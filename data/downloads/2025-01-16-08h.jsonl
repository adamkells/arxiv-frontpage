{"created":"2025-01-15 18:59:39","title":"Octopus: Scalable Low-Cost CXL Memory Pooling","abstract":"Compute Express Link (CXL) is widely-supported interconnect standard that promises to enable memory disaggregation in data centers. CXL allows for memory pooling, which can be used to create a shared memory space across multiple servers. However, CXL does not specify how to actually build a memory pool. Existing proposals for CXL memory pools are expensive, as they require CXL switches or large multi-headed devices. In this paper, we propose a new design for CXL memory pools that is cost-effective. We call these designs Octopus topologies. Our design uses small CXL devices that can be made cheaply and offer fast access latencies. Specifically, we propose asymmetric CXL topologies where hosts connect to different sets of CXL devices. This enables pooling and sharing memory across multiple hosts even as each individual CXL device is only connected to a small number of hosts. Importantly, this uses hardware that is readily available today. We also show the trade-off in terms of CXL pod size and cost overhead per host. Octopus improves the Pareto frontier defined by prior policies, e.g., offering to connect 3x as many hosts at 17% lower cost per host.","sentences":["Compute Express Link (CXL) is widely-supported interconnect standard that promises to enable memory disaggregation in data centers.","CXL allows for memory pooling, which can be used to create a shared memory space across multiple servers.","However, CXL does not specify how to actually build a memory pool.","Existing proposals for CXL memory pools are expensive, as they require CXL switches or large multi-headed devices.","In this paper, we propose a new design for CXL memory pools that is cost-effective.","We call these designs Octopus topologies.","Our design uses small CXL devices that can be made cheaply and offer fast access latencies.","Specifically, we propose asymmetric CXL topologies where hosts connect to different sets of CXL devices.","This enables pooling and sharing memory across multiple hosts even as each individual CXL device is only connected to a small number of hosts.","Importantly, this uses hardware that is readily available today.","We also show the trade-off in terms of CXL pod size and cost overhead per host.","Octopus improves the Pareto frontier defined by prior policies, e.g., offering to connect 3x as many hosts at 17% lower cost per host."],"url":"http://arxiv.org/abs/2501.09020v1"}
{"created":"2025-01-15 18:48:38","title":"SimGen: A Diffusion-Based Framework for Simultaneous Surgical Image and Segmentation Mask Generation","abstract":"Acquiring and annotating surgical data is often resource-intensive, ethical constraining, and requiring significant expert involvement. While generative AI models like text-to-image can alleviate data scarcity, incorporating spatial annotations, such as segmentation masks, is crucial for precision-driven surgical applications, simulation, and education. This study introduces both a novel task and method, SimGen, for Simultaneous Image and Mask Generation. SimGen is a diffusion model based on the DDPM framework and Residual U-Net, designed to jointly generate high-fidelity surgical images and their corresponding segmentation masks. The model leverages cross-correlation priors to capture dependencies between continuous image and discrete mask distributions. Additionally, a Canonical Fibonacci Lattice (CFL) is employed to enhance class separability and uniformity in the RGB space of the masks. SimGen delivers high-fidelity images and accurate segmentation masks, outperforming baselines across six public datasets assessed on image and semantic inception distance metrics. Ablation study shows that the CFL improves mask quality and spatial separation. Downstream experiments suggest generated image-mask pairs are usable if regulations limit human data release for research. This work offers a cost-effective solution for generating paired surgical images and complex labels, advancing surgical AI development by reducing the need for expensive manual annotations.","sentences":["Acquiring and annotating surgical data is often resource-intensive, ethical constraining, and requiring significant expert involvement.","While generative AI models like text-to-image can alleviate data scarcity, incorporating spatial annotations, such as segmentation masks, is crucial for precision-driven surgical applications, simulation, and education.","This study introduces both a novel task and method, SimGen, for Simultaneous Image and Mask Generation.","SimGen is a diffusion model based on the DDPM framework and Residual U-Net, designed to jointly generate high-fidelity surgical images and their corresponding segmentation masks.","The model leverages cross-correlation priors to capture dependencies between continuous image and discrete mask distributions.","Additionally, a Canonical Fibonacci Lattice (CFL) is employed to enhance class separability and uniformity in the RGB space of the masks.","SimGen delivers high-fidelity images and accurate segmentation masks, outperforming baselines across six public datasets assessed on image and semantic inception distance metrics.","Ablation study shows that the CFL improves mask quality and spatial separation.","Downstream experiments suggest generated image-mask pairs are usable if regulations limit human data release for research.","This work offers a cost-effective solution for generating paired surgical images and complex labels, advancing surgical AI development by reducing the need for expensive manual annotations."],"url":"http://arxiv.org/abs/2501.09008v1"}
{"created":"2025-01-15 18:43:50","title":"Lightweight Security for Ambient-Powered Programmable Reflections with Reconfigurable Intelligent Surfaces","abstract":"Ambient Internet-of-Things (AIoT) form a new class of emerging technology that promises to deliver pervasive wireless connectivity to previously disconnected devices and products, assisting dependent industries (for example, supply chain, clothing, remote surveillance, climate monitoring, and sensors) to obtain granular real-time service visibility. Such ultra-low complexity and power consumption devices, that are either battery-less or have the capability for limited energy storage, can provide data feeds about the condition of any aspect (e.g., an environment or an item) that is being monitored, enabling proactive or reactive control by any application server. Although the security of data involving AIoT devices is critical for key decisions of any dependent operational system, the implementation of resource intensive cryptographic algorithms and other security mechanisms becomes nearly infeasible, or very challenging, due to the device energy and computational limitations. In this article, we present a lightweight security solution that enables confidentiality, integrity, and privacy protection in wireless links including AIoT. We consider, as a case study, an ambient-powered Reconfigurable Intelligent Surface (RIS) that harvests energy from its incident radio waves to realize programmable reflective beamforming, enabling the communication between a Base Station (BS) and end-user terminals. The proposed lightweight security solution is applied to the control channel between the BS and the RIS controller which is responsible for the metasurface's dynamic management and phase configuration optimization.","sentences":["Ambient Internet-of-Things (AIoT) form a new class of emerging technology that promises to deliver pervasive wireless connectivity to previously disconnected devices and products, assisting dependent industries (for example, supply chain, clothing, remote surveillance, climate monitoring, and sensors) to obtain granular real-time service visibility.","Such ultra-low complexity and power consumption devices, that are either battery-less or have the capability for limited energy storage, can provide data feeds about the condition of any aspect (e.g., an environment or an item) that is being monitored, enabling proactive or reactive control by any application server.","Although the security of data involving AIoT devices is critical for key decisions of any dependent operational system, the implementation of resource intensive cryptographic algorithms and other security mechanisms becomes nearly infeasible, or very challenging, due to the device energy and computational limitations.","In this article, we present a lightweight security solution that enables confidentiality, integrity, and privacy protection in wireless links including AIoT.","We consider, as a case study, an ambient-powered Reconfigurable Intelligent Surface (RIS) that harvests energy from its incident radio waves to realize programmable reflective beamforming, enabling the communication between a Base Station (BS) and end-user terminals.","The proposed lightweight security solution is applied to the control channel between the BS and the RIS controller which is responsible for the metasurface's dynamic management and phase configuration optimization."],"url":"http://arxiv.org/abs/2501.09005v1"}
{"created":"2025-01-15 18:37:08","title":"Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails","abstract":"As Large Language Models (LLMs) and generative AI become increasingly widespread, concerns about content safety have grown in parallel. Currently, there is a clear lack of high-quality, human-annotated datasets that address the full spectrum of LLM-related safety risks and are usable for commercial applications. To bridge this gap, we propose a comprehensive and adaptable taxonomy for categorizing safety risks, structured into 12 top-level hazard categories with an extension to 9 fine-grained subcategories. This taxonomy is designed to meet the diverse requirements of downstream users, offering more granular and flexible tools for managing various risk types. Using a hybrid data generation pipeline that combines human annotations with a multi-LLM \"jury\" system to assess the safety of responses, we obtain Aegis 2.0, a carefully curated collection of 34,248 samples of human-LLM interactions, annotated according to our proposed taxonomy. To validate its effectiveness, we demonstrate that several lightweight models, trained using parameter-efficient techniques on Aegis 2.0, achieve performance competitive with leading safety models fully fine-tuned on much larger, non-commercial datasets. In addition, we introduce a novel training blend that combines safety with topic following data.This approach enhances the adaptability of guard models, enabling them to generalize to new risk categories defined during inference. We plan to open-source Aegis 2.0 data and models to the research community to aid in the safety guardrailing of LLMs.","sentences":["As Large Language Models (LLMs) and generative AI become increasingly widespread, concerns about content safety have grown in parallel.","Currently, there is a clear lack of high-quality, human-annotated datasets that address the full spectrum of LLM-related safety risks and are usable for commercial applications.","To bridge this gap, we propose a comprehensive and adaptable taxonomy for categorizing safety risks, structured into 12 top-level hazard categories with an extension to 9 fine-grained subcategories.","This taxonomy is designed to meet the diverse requirements of downstream users, offering more granular and flexible tools for managing various risk types.","Using a hybrid data generation pipeline that combines human annotations with a multi-LLM \"jury\" system to assess the safety of responses, we obtain Aegis 2.0, a carefully curated collection of 34,248 samples of human-LLM interactions, annotated according to our proposed taxonomy.","To validate its effectiveness, we demonstrate that several lightweight models, trained using parameter-efficient techniques on Aegis 2.0, achieve performance competitive with leading safety models fully fine-tuned on much larger, non-commercial datasets.","In addition, we introduce a novel training blend that combines safety with topic following data.","This approach enhances the adaptability of guard models, enabling them to generalize to new risk categories defined during inference.","We plan to open-source Aegis 2.0 data and models to the research community to aid in the safety guardrailing of LLMs."],"url":"http://arxiv.org/abs/2501.09004v1"}
{"created":"2025-01-15 18:23:33","title":"VECT-GAN: A variationally encoded generative model for overcoming data scarcity in pharmaceutical science","abstract":"Data scarcity in pharmaceutical research has led to reliance on labour-intensive trial and error approaches for development rather than data driven methods. While Machine Learning offers a solution, existing datasets are often small and noisy, limiting their utility. To address this, we developed a Variationally Encoded Conditional Tabular Generative Adversarial Network (VECT GAN), a novel generative model specifically designed for augmenting small, noisy datasets. We introduce a pipeline where data is augmented before regression model development and demonstrate that this consistently and significantly improves performance over other state of the art tabular generative models. We apply this pipeline across six pharmaceutical datasets, and highlight its real-world applicability by developing novel polymers with medically desirable mucoadhesive properties, which we made and experimentally characterised. Additionally, we pre-train the model on the ChEMBL database of drug-like molecules, leveraging knowledge distillation to enhance its generalisability, making it readily available for use on pharmaceutical datasets containing small molecules, which is an extremely common pharmaceutical task. We demonstrate the power of synthetic data for regularising small tabular datasets, highlighting its potential to become standard practice in pharmaceutical model development, and make our method, including VECT GAN pretrained on ChEMBL available as a pip package.","sentences":["Data scarcity in pharmaceutical research has led to reliance on labour-intensive trial and error approaches for development rather than data driven methods.","While Machine Learning offers a solution, existing datasets are often small and noisy, limiting their utility.","To address this, we developed a Variationally Encoded Conditional Tabular Generative Adversarial Network (VECT GAN), a novel generative model specifically designed for augmenting small, noisy datasets.","We introduce a pipeline where data is augmented before regression model development and demonstrate that this consistently and significantly improves performance over other state of the art tabular generative models.","We apply this pipeline across six pharmaceutical datasets, and highlight its real-world applicability by developing novel polymers with medically desirable mucoadhesive properties, which we made and experimentally characterised.","Additionally, we pre-train the model on the ChEMBL database of drug-like molecules, leveraging knowledge distillation to enhance its generalisability, making it readily available for use on pharmaceutical datasets containing small molecules, which is an extremely common pharmaceutical task.","We demonstrate the power of synthetic data for regularising small tabular datasets, highlighting its potential to become standard practice in pharmaceutical model development, and make our method, including VECT GAN pretrained on ChEMBL available as a pip package."],"url":"http://arxiv.org/abs/2501.08995v1"}
{"created":"2025-01-15 17:47:57","title":"Development and Validation of the Provider Documentation Summarization Quality Instrument for Large Language Models","abstract":"As Large Language Models (LLMs) are integrated into electronic health record (EHR) workflows, validated instruments are essential to evaluate their performance before implementation. Existing instruments for provider documentation quality are often unsuitable for the complexities of LLM-generated text and lack validation on real-world data. The Provider Documentation Summarization Quality Instrument (PDSQI-9) was developed to evaluate LLM-generated clinical summaries. Multi-document summaries were generated from real-world EHR data across multiple specialties using several LLMs (GPT-4o, Mixtral 8x7b, and Llama 3-8b). Validation included Pearson correlation for substantive validity, factor analysis and Cronbach's alpha for structural validity, inter-rater reliability (ICC and Krippendorff's alpha) for generalizability, a semi-Delphi process for content validity, and comparisons of high- versus low-quality summaries for discriminant validity. Seven physician raters evaluated 779 summaries and answered 8,329 questions, achieving over 80% power for inter-rater reliability. The PDSQI-9 demonstrated strong internal consistency (Cronbach's alpha = 0.879; 95% CI: 0.867-0.891) and high inter-rater reliability (ICC = 0.867; 95% CI: 0.867-0.868), supporting structural validity and generalizability. Factor analysis identified a 4-factor model explaining 58% of the variance, representing organization, clarity, accuracy, and utility. Substantive validity was supported by correlations between note length and scores for Succinct (rho = -0.200, p = 0.029) and Organized (rho = -0.190, p = 0.037). Discriminant validity distinguished high- from low-quality summaries (p < 0.001). The PDSQI-9 demonstrates robust construct validity, supporting its use in clinical practice to evaluate LLM-generated summaries and facilitate safer integration of LLMs into healthcare workflows.","sentences":["As Large Language Models (LLMs) are integrated into electronic health record (EHR) workflows, validated instruments are essential to evaluate their performance before implementation.","Existing instruments for provider documentation quality are often unsuitable for the complexities of LLM-generated text and lack validation on real-world data.","The Provider Documentation Summarization Quality Instrument (PDSQI-9) was developed to evaluate LLM-generated clinical summaries.","Multi-document summaries were generated from real-world EHR data across multiple specialties using several LLMs (GPT-4o, Mixtral 8x7b, and Llama 3-8b).","Validation included Pearson correlation for substantive validity, factor analysis and Cronbach's alpha for structural validity, inter-rater reliability (ICC and Krippendorff's alpha) for generalizability, a semi-Delphi process for content validity, and comparisons of high- versus low-quality summaries for discriminant validity.","Seven physician raters evaluated 779 summaries and answered 8,329 questions, achieving over 80% power for inter-rater reliability.","The PDSQI-9 demonstrated strong internal consistency (Cronbach's alpha = 0.879; 95% CI: 0.867-0.891) and high inter-rater reliability (ICC = 0.867; 95% CI: 0.867-0.868), supporting structural validity and generalizability.","Factor analysis identified a 4-factor model explaining 58% of the variance, representing organization, clarity, accuracy, and utility.","Substantive validity was supported by correlations between note length and scores for Succinct (rho = -0.200, p = 0.029) and Organized (rho = -0.190, p = 0.037).","Discriminant validity distinguished high- from low-quality summaries (p < 0.001).","The PDSQI-9 demonstrates robust construct validity, supporting its use in clinical practice to evaluate LLM-generated summaries and facilitate safer integration of LLMs into healthcare workflows."],"url":"http://arxiv.org/abs/2501.08977v1"}
{"created":"2025-01-15 17:36:56","title":"Learning to Extract Cross-Domain Aspects and Understanding Sentiments Using Large Language Models","abstract":"Aspect-based sentiment analysis (ASBA) is a refined approach to sentiment analysis that aims to extract and classify sentiments based on specific aspects or features of a product, service, or entity. Unlike traditional sentiment analysis, which assigns a general sentiment score to entire reviews or texts, ABSA focuses on breaking down the text into individual components or aspects (e.g., quality, price, service) and evaluating the sentiment towards each. This allows for a more granular level of understanding of customer opinions, enabling businesses to pinpoint specific areas of strength and improvement. The process involves several key steps, including aspect extraction, sentiment classification, and aspect-level sentiment aggregation for a review paragraph or any other form that the users have provided. ABSA has significant applications in areas such as product reviews, social media monitoring, customer feedback analysis, and market research. By leveraging techniques from natural language processing (NLP) and machine learning, ABSA facilitates the extraction of valuable insights, enabling companies to make data-driven decisions that enhance customer satisfaction and optimize offerings. As ABSA evolves, it holds the potential to greatly improve personalized customer experiences by providing a deeper understanding of sentiment across various product aspects. In this work, we have analyzed the strength of LLMs for a complete cross-domain aspect-based sentiment analysis with the aim of defining the framework for certain products and using it for other similar situations. We argue that it is possible to that at an effectiveness of 92\\% accuracy for the Aspect Based Sentiment Analysis dataset of SemEval-2015 Task 12.","sentences":["Aspect-based sentiment analysis (ASBA) is a refined approach to sentiment analysis that aims to extract and classify sentiments based on specific aspects or features of a product, service, or entity.","Unlike traditional sentiment analysis, which assigns a general sentiment score to entire reviews or texts, ABSA focuses on breaking down the text into individual components or aspects (e.g., quality, price, service) and evaluating the sentiment towards each.","This allows for a more granular level of understanding of customer opinions, enabling businesses to pinpoint specific areas of strength and improvement.","The process involves several key steps, including aspect extraction, sentiment classification, and aspect-level sentiment aggregation for a review paragraph or any other form that the users have provided.","ABSA has significant applications in areas such as product reviews, social media monitoring, customer feedback analysis, and market research.","By leveraging techniques from natural language processing (NLP) and machine learning, ABSA facilitates the extraction of valuable insights, enabling companies to make data-driven decisions that enhance customer satisfaction and optimize offerings.","As ABSA evolves, it holds the potential to greatly improve personalized customer experiences by providing a deeper understanding of sentiment across various product aspects.","In this work, we have analyzed the strength of LLMs for a complete cross-domain aspect-based sentiment analysis with the aim of defining the framework for certain products and using it for other similar situations.","We argue that it is possible to that at an effectiveness of 92\\% accuracy for the Aspect Based Sentiment Analysis dataset of SemEval-2015 Task 12."],"url":"http://arxiv.org/abs/2501.08974v1"}
{"created":"2025-01-15 17:28:53","title":"Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography","abstract":"We often interact with untrusted parties. Prioritization of privacy can limit the effectiveness of these interactions, as achieving certain goals necessitates sharing private data. Traditionally, addressing this challenge has involved either seeking trusted intermediaries or constructing cryptographic protocols that restrict how much data is revealed, such as multi-party computations or zero-knowledge proofs. While significant advances have been made in scaling cryptographic approaches, they remain limited in terms of the size and complexity of applications they can be used for. In this paper, we argue that capable machine learning models can fulfill the role of a trusted third party, thus enabling secure computations for applications that were previously infeasible. In particular, we describe Trusted Capable Model Environments (TCMEs) as an alternative approach for scaling secure computation, where capable machine learning model(s) interact under input/output constraints, with explicit information flow control and explicit statelessness. This approach aims to achieve a balance between privacy and computational efficiency, enabling private inference where classical cryptographic solutions are currently infeasible. We describe a number of use cases that are enabled by TCME, and show that even some simple classic cryptographic problems can already be solved with TCME. Finally, we outline current limitations and discuss the path forward in implementing them.","sentences":["We often interact with untrusted parties.","Prioritization of privacy can limit the effectiveness of these interactions, as achieving certain goals necessitates sharing private data.","Traditionally, addressing this challenge has involved either seeking trusted intermediaries or constructing cryptographic protocols that restrict how much data is revealed, such as multi-party computations or zero-knowledge proofs.","While significant advances have been made in scaling cryptographic approaches, they remain limited in terms of the size and complexity of applications they can be used for.","In this paper, we argue that capable machine learning models can fulfill the role of a trusted third party, thus enabling secure computations for applications that were previously infeasible.","In particular, we describe Trusted Capable Model Environments (TCMEs) as an alternative approach for scaling secure computation, where capable machine learning model(s) interact under input/output constraints, with explicit information flow control and explicit statelessness.","This approach aims to achieve a balance between privacy and computational efficiency, enabling private inference where classical cryptographic solutions are currently infeasible.","We describe a number of use cases that are enabled by TCME, and show that even some simple classic cryptographic problems can already be solved with TCME.","Finally, we outline current limitations and discuss the path forward in implementing them."],"url":"http://arxiv.org/abs/2501.08970v1"}
{"created":"2025-01-15 17:18:46","title":"An analysis of data variation and bias in image-based dermatological datasets for machine learning classification","abstract":"AI algorithms have become valuable in aiding professionals in healthcare. The increasing confidence obtained by these models is helpful in critical decision demands. In clinical dermatology, classification models can detect malignant lesions on patients' skin using only RGB images as input. However, most learning-based methods employ data acquired from dermoscopic datasets on training, which are large and validated by a gold standard. Clinical models aim to deal with classification on users' smartphone cameras that do not contain the corresponding resolution provided by dermoscopy. Also, clinical applications bring new challenges. It can contain captures from uncontrolled environments, skin tone variations, viewpoint changes, noises in data and labels, and unbalanced classes. A possible alternative would be to use transfer learning to deal with the clinical images. However, as the number of samples is low, it can cause degradations on the model's performance; the source distribution used in training differs from the test set. This work aims to evaluate the gap between dermoscopic and clinical samples and understand how the dataset variations impact training. It assesses the main differences between distributions that disturb the model's prediction. Finally, from experiments on different architectures, we argue how to combine the data from divergent distributions, decreasing the impact on the model's final accuracy.","sentences":["AI algorithms have become valuable in aiding professionals in healthcare.","The increasing confidence obtained by these models is helpful in critical decision demands.","In clinical dermatology, classification models can detect malignant lesions on patients' skin using only RGB images as input.","However, most learning-based methods employ data acquired from dermoscopic datasets on training, which are large and validated by a gold standard.","Clinical models aim to deal with classification on users' smartphone cameras that do not contain the corresponding resolution provided by dermoscopy.","Also, clinical applications bring new challenges.","It can contain captures from uncontrolled environments, skin tone variations, viewpoint changes, noises in data and labels, and unbalanced classes.","A possible alternative would be to use transfer learning to deal with the clinical images.","However, as the number of samples is low, it can cause degradations on the model's performance; the source distribution used in training differs from the test set.","This work aims to evaluate the gap between dermoscopic and clinical samples and understand how the dataset variations impact training.","It assesses the main differences between distributions that disturb the model's prediction.","Finally, from experiments on different architectures, we argue how to combine the data from divergent distributions, decreasing the impact on the model's final accuracy."],"url":"http://arxiv.org/abs/2501.08962v1"}
{"created":"2025-01-15 16:49:32","title":"Taint Analysis for Graph APIs Focusing on Broken Access Control","abstract":"Graph APIs are capable of flexibly retrieving or manipulating graph-structured data over the web. This rather novel type of APIs presents new challenges when it comes to properly securing the APIs against the usual web application security risks, e.g., broken access control. A prominent security testing approach is taint analysis, which traces tainted, i.e., security-relevant, data from sources (where tainted data is inserted) to sinks (where the use of tainted data may lead to a security risk), over the information flow in an application.   We present a first systematic approach to static and dynamic taint analysis for Graph APIs focusing on broken access control. The approach comprises the following. We taint nodes in the Graph API if they represent data requiring specific privileges in order to be retrieved or manipulated, and identify API calls which are related to sources and sinks. Then, we statically analyze whether tainted information flow between API source and sink calls occurs. To this end, we model the API calls using graph transformation rules. We subsequently use critical pair analysis to automatically analyze potential dependencies between rules representing source calls and rules representing sink calls. We distinguish direct from indirect tainted information flow and argue under which conditions the CPA is able to detect not only direct, but also indirect tainted flow. The static taint analysis (i) identifies flows that need to be further reviewed, since tainted nodes may be created by an API call and used or manipulated by another API call later without having the necessary privileges, and (ii) can be used to systematically design dynamic security tests for broken access control. The dynamic taint analysis checks if potential broken access control risks detected during the static taint analysis really occur. We apply the approach to a part of the GitHub GraphQL API.","sentences":["Graph APIs are capable of flexibly retrieving or manipulating graph-structured data over the web.","This rather novel type of APIs presents new challenges when it comes to properly securing the APIs against the usual web application security risks, e.g., broken access control.","A prominent security testing approach is taint analysis, which traces tainted, i.e., security-relevant, data from sources (where tainted data is inserted) to sinks (where the use of tainted data may lead to a security risk), over the information flow in an application.   ","We present a first systematic approach to static and dynamic taint analysis for Graph APIs focusing on broken access control.","The approach comprises the following.","We taint nodes in the Graph API if they represent data requiring specific privileges in order to be retrieved or manipulated, and identify API calls which are related to sources and sinks.","Then, we statically analyze whether tainted information flow between API source and sink calls occurs.","To this end, we model the API calls using graph transformation rules.","We subsequently use critical pair analysis to automatically analyze potential dependencies between rules representing source calls and rules representing sink calls.","We distinguish direct from indirect tainted information flow and argue under which conditions the CPA is able to detect not only direct, but also indirect tainted flow.","The static taint analysis (i) identifies flows that need to be further reviewed, since tainted nodes may be created by an API call and used or manipulated by another API call later without having the necessary privileges, and (ii) can be used to systematically design dynamic security tests for broken access control.","The dynamic taint analysis checks if potential broken access control risks detected during the static taint analysis really occur.","We apply the approach to a part of the GitHub GraphQL API."],"url":"http://arxiv.org/abs/2501.08947v1"}
{"created":"2025-01-15 16:49:22","title":"Applying General Turn-taking Models to Conversational Human-Robot Interaction","abstract":"Turn-taking is a fundamental aspect of conversation, but current Human-Robot Interaction (HRI) systems often rely on simplistic, silence-based models, leading to unnatural pauses and interruptions. This paper investigates, for the first time, the application of general turn-taking models, specifically TurnGPT and Voice Activity Projection (VAP), to improve conversational dynamics in HRI. These models are trained on human-human dialogue data using self-supervised learning objectives, without requiring domain-specific fine-tuning. We propose methods for using these models in tandem to predict when a robot should begin preparing responses, take turns, and handle potential interruptions. We evaluated the proposed system in a within-subject study against a traditional baseline system, using the Furhat robot with 39 adults in a conversational setting, in combination with a large language model for autonomous response generation. The results show that participants significantly prefer the proposed system, and it significantly reduces response delays and interruptions.","sentences":["Turn-taking is a fundamental aspect of conversation, but current Human-Robot Interaction (HRI) systems often rely on simplistic, silence-based models, leading to unnatural pauses and interruptions.","This paper investigates, for the first time, the application of general turn-taking models, specifically TurnGPT and Voice Activity Projection (VAP), to improve conversational dynamics in HRI.","These models are trained on human-human dialogue data using self-supervised learning objectives, without requiring domain-specific fine-tuning.","We propose methods for using these models in tandem to predict when a robot should begin preparing responses, take turns, and handle potential interruptions.","We evaluated the proposed system in a within-subject study against a traditional baseline system, using the Furhat robot with 39 adults in a conversational setting, in combination with a large language model for autonomous response generation.","The results show that participants significantly prefer the proposed system, and it significantly reduces response delays and interruptions."],"url":"http://arxiv.org/abs/2501.08946v1"}
{"created":"2025-01-15 16:34:20","title":"Visual WetlandBirds Dataset: Bird Species Identification and Behavior Recognition in Videos","abstract":"The current biodiversity loss crisis makes animal monitoring a relevant field of study. In light of this, data collected through monitoring can provide essential insights, and information for decision-making aimed at preserving global biodiversity. Despite the importance of such data, there is a notable scarcity of datasets featuring videos of birds, and none of the existing datasets offer detailed annotations of bird behaviors in video format. In response to this gap, our study introduces the first fine-grained video dataset specifically designed for bird behavior detection and species classification. This dataset addresses the need for comprehensive bird video datasets and provides detailed data on bird actions, facilitating the development of deep learning models to recognize these, similar to the advancements made in human action recognition. The proposed dataset comprises 178 videos recorded in Spanish wetlands, capturing 13 different bird species performing 7 distinct behavior classes. In addition, we also present baseline results using state of the art models on two tasks: bird behavior recognition and species classification.","sentences":["The current biodiversity loss crisis makes animal monitoring a relevant field of study.","In light of this, data collected through monitoring can provide essential insights, and information for decision-making aimed at preserving global biodiversity.","Despite the importance of such data, there is a notable scarcity of datasets featuring videos of birds, and none of the existing datasets offer detailed annotations of bird behaviors in video format.","In response to this gap, our study introduces the first fine-grained video dataset specifically designed for bird behavior detection and species classification.","This dataset addresses the need for comprehensive bird video datasets and provides detailed data on bird actions, facilitating the development of deep learning models to recognize these, similar to the advancements made in human action recognition.","The proposed dataset comprises 178 videos recorded in Spanish wetlands, capturing 13 different bird species performing 7 distinct behavior classes.","In addition, we also present baseline results using state of the art models on two tasks: bird behavior recognition and species classification."],"url":"http://arxiv.org/abs/2501.08931v1"}
{"created":"2025-01-15 16:30:05","title":"Learning Joint Denoising, Demosaicing, and Compression from the Raw Natural Image Noise Dataset","abstract":"This paper introduces the Raw Natural Image Noise Dataset (RawNIND), a diverse collection of paired raw images designed to support the development of denoising models that generalize across sensors, image development workflows, and styles. Two denoising methods are proposed: one operates directly on raw Bayer data, leveraging computational efficiency, while the other processes linear RGB images for improved generalization to different sensors, with both preserving flexibility for subsequent development. Both methods outperform traditional approaches which rely on developed images. Additionally, the integration of denoising and compression at the raw data level significantly enhances rate-distortion performance and computational efficiency. These findings suggest a paradigm shift toward raw data workflows for efficient and flexible image processing.","sentences":["This paper introduces the Raw Natural Image Noise Dataset (RawNIND), a diverse collection of paired raw images designed to support the development of denoising models that generalize across sensors, image development workflows, and styles.","Two denoising methods are proposed: one operates directly on raw Bayer data, leveraging computational efficiency, while the other processes linear RGB images for improved generalization to different sensors, with both preserving flexibility for subsequent development.","Both methods outperform traditional approaches which rely on developed images.","Additionally, the integration of denoising and compression at the raw data level significantly enhances rate-distortion performance and computational efficiency.","These findings suggest a paradigm shift toward raw data workflows for efficient and flexible image processing."],"url":"http://arxiv.org/abs/2501.08924v1"}
{"created":"2025-01-15 16:06:10","title":"Leveraging Large Language Models as Knowledge-Driven Agents for Reliable Retrosynthesis Planning","abstract":"Identifying reliable synthesis pathways in materials chemistry is a complex task, particularly in polymer science, due to the intricate and often non-unique nomenclature of macromolecules. To address this challenge, we propose an agent system that integrates large language models (LLMs) and knowledge graphs (KGs). By leveraging LLMs' powerful capabilities for extracting and recognizing chemical substance names, and storing the extracted data in a structured knowledge graph, our system fully automates the retrieval of relevant literatures, extraction of reaction data, database querying, construction of retrosynthetic pathway trees, further expansion through the retrieval of additional literature and recommendation of optimal reaction pathways. A novel Multi-branched Reaction Pathway Search (MBRPS) algorithm enables the exploration of all pathways, with a particular focus on multi-branched ones, helping LLMs overcome weak reasoning in multi-branched paths. This work represents the first attempt to develop a fully automated retrosynthesis planning agent tailored specially for macromolecules powered by LLMs. Applied to polyimide synthesis, our new approach constructs a retrosynthetic pathway tree with hundreds of pathways and recommends optimized routes, including both known and novel pathways, demonstrating its effectiveness and potential for broader applications.","sentences":["Identifying reliable synthesis pathways in materials chemistry is a complex task, particularly in polymer science, due to the intricate and often non-unique nomenclature of macromolecules.","To address this challenge, we propose an agent system that integrates large language models (LLMs) and knowledge graphs (KGs).","By leveraging LLMs' powerful capabilities for extracting and recognizing chemical substance names, and storing the extracted data in a structured knowledge graph, our system fully automates the retrieval of relevant literatures, extraction of reaction data, database querying, construction of retrosynthetic pathway trees, further expansion through the retrieval of additional literature and recommendation of optimal reaction pathways.","A novel Multi-branched Reaction Pathway Search (MBRPS) algorithm enables the exploration of all pathways, with a particular focus on multi-branched ones, helping LLMs overcome weak reasoning in multi-branched paths.","This work represents the first attempt to develop a fully automated retrosynthesis planning agent tailored specially for macromolecules powered by LLMs.","Applied to polyimide synthesis, our new approach constructs a retrosynthetic pathway tree with hundreds of pathways and recommends optimized routes, including both known and novel pathways, demonstrating its effectiveness and potential for broader applications."],"url":"http://arxiv.org/abs/2501.08897v1"}
{"created":"2025-01-15 15:58:16","title":"A Two-Stage Pretraining-Finetuning Framework for Treatment Effect Estimation with Unmeasured Confounding","abstract":"Estimating the conditional average treatment effect (CATE) from observational data plays a crucial role in areas such as e-commerce, healthcare, and economics. Existing studies mainly rely on the strong ignorability assumption that there are no unmeasured confounders, whose presence cannot be tested from observational data and can invalidate any causal conclusion. In contrast, data collected from randomized controlled trials (RCT) do not suffer from confounding, but are usually limited by a small sample size. In this paper, we propose a two-stage pretraining-finetuning (TSPF) framework using both large-scale observational data and small-scale RCT data to estimate the CATE in the presence of unmeasured confounding. In the first stage, a foundational representation of covariates is trained to estimate counterfactual outcomes through large-scale observational data. In the second stage, we propose to train an augmented representation of the covariates, which is concatenated to the foundational representation obtained in the first stage to adjust for the unmeasured confounding. To avoid overfitting caused by the small-scale RCT data in the second stage, we further propose a partial parameter initialization approach, rather than training a separate network. The superiority of our approach is validated on two public datasets with extensive experiments. The code is available at https://github.com/zhouchuanCN/KDD25-TSPF.","sentences":["Estimating the conditional average treatment effect (CATE) from observational data plays a crucial role in areas such as e-commerce, healthcare, and economics.","Existing studies mainly rely on the strong ignorability assumption that there are no unmeasured confounders, whose presence cannot be tested from observational data and can invalidate any causal conclusion.","In contrast, data collected from randomized controlled trials (RCT) do not suffer from confounding, but are usually limited by a small sample size.","In this paper, we propose a two-stage pretraining-finetuning (TSPF) framework using both large-scale observational data and small-scale RCT data to estimate the CATE in the presence of unmeasured confounding.","In the first stage, a foundational representation of covariates is trained to estimate counterfactual outcomes through large-scale observational data.","In the second stage, we propose to train an augmented representation of the covariates, which is concatenated to the foundational representation obtained in the first stage to adjust for the unmeasured confounding.","To avoid overfitting caused by the small-scale RCT data in the second stage, we further propose a partial parameter initialization approach, rather than training a separate network.","The superiority of our approach is validated on two public datasets with extensive experiments.","The code is available at https://github.com/zhouchuanCN/KDD25-TSPF."],"url":"http://arxiv.org/abs/2501.08888v1"}
{"created":"2025-01-15 15:49:46","title":"Incrementally Learning Multiple Diverse Data Domains via Multi-Source Dynamic Expansion Model","abstract":"Continual Learning seeks to develop a model capable of incrementally assimilating new information while retaining prior knowledge. However, current research predominantly addresses a straightforward learning context, wherein all data samples originate from a singular data domain. This paper shifts focus to a more complex and realistic learning environment, characterized by data samples sourced from multiple distinct domains. We tackle this intricate learning challenge by introducing a novel methodology, termed the Multi-Source Dynamic Expansion Model (MSDEM), which leverages various pre-trained models as backbones and progressively establishes new experts based on them to adapt to emerging tasks. Additionally, we propose an innovative dynamic expandable attention mechanism designed to selectively harness knowledge from multiple backbones, thereby accelerating the new task learning. Moreover, we introduce a dynamic graph weight router that strategically reuses all previously acquired parameters and representations for new task learning, maximizing the positive knowledge transfer effect, which further improves generalization performance. We conduct a comprehensive series of experiments, and the empirical findings indicate that our proposed approach achieves state-of-the-art performance.","sentences":["Continual Learning seeks to develop a model capable of incrementally assimilating new information while retaining prior knowledge.","However, current research predominantly addresses a straightforward learning context, wherein all data samples originate from a singular data domain.","This paper shifts focus to a more complex and realistic learning environment, characterized by data samples sourced from multiple distinct domains.","We tackle this intricate learning challenge by introducing a novel methodology, termed the Multi-Source Dynamic Expansion Model (MSDEM), which leverages various pre-trained models as backbones and progressively establishes new experts based on them to adapt to emerging tasks.","Additionally, we propose an innovative dynamic expandable attention mechanism designed to selectively harness knowledge from multiple backbones, thereby accelerating the new task learning.","Moreover, we introduce a dynamic graph weight router that strategically reuses all previously acquired parameters and representations for new task learning, maximizing the positive knowledge transfer effect, which further improves generalization performance.","We conduct a comprehensive series of experiments, and the empirical findings indicate that our proposed approach achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2501.08878v1"}
{"created":"2025-01-15 15:38:56","title":"Silent Abandonment in Text-Based Contact Centers: Identifying, Quantifying, and Mitigating its Operational Impacts","abstract":"In the quest to improve services, companies offer customers the option to interact with agents via texting. Such contact centers face unique challenges compared to traditional call centers, as measuring customer experience proxies like abandonment and patience involves uncertainty. A key source of this uncertainty is silent abandonment, where customers leave without notifying the system, wasting agent time and leaving their status unclear. Silent abandonment also obscures whether a customer was served or left. Our goals are to measure the magnitude of silent abandonment and mitigate its effects. Classification models show that 3%-70% of customers across 17 companies abandon silently. In one study, 71.3% of abandoning customers did so silently, reducing agent efficiency by 3.2% and system capacity by 15.3%, incurring $5,457 in annual costs per agent. We develop an expectation-maximization (EM) algorithm to estimate customer patience under uncertainty and identify influencing covariates. We find that companies should use classification models to estimate abandonment scope and our EM algorithm to assess patience. We suggest strategies to operationally mitigate the impact of silent abandonment by predicting suspected silent-abandonment behavior or changing service design. Specifically, we show that while allowing customers to write while waiting in the queue creates a missing data challenge, it also significantly increases patience and reduces service time, leading to reduced abandonment and lower staffing requirements.","sentences":["In the quest to improve services, companies offer customers the option to interact with agents via texting.","Such contact centers face unique challenges compared to traditional call centers, as measuring customer experience proxies like abandonment and patience involves uncertainty.","A key source of this uncertainty is silent abandonment, where customers leave without notifying the system, wasting agent time and leaving their status unclear.","Silent abandonment also obscures whether a customer was served or left.","Our goals are to measure the magnitude of silent abandonment and mitigate its effects.","Classification models show that 3%-70% of customers across 17 companies abandon silently.","In one study, 71.3% of abandoning customers did so silently, reducing agent efficiency by 3.2% and system capacity by 15.3%, incurring $5,457 in annual costs per agent.","We develop an expectation-maximization (EM) algorithm to estimate customer patience under uncertainty and identify influencing covariates.","We find that companies should use classification models to estimate abandonment scope and our EM algorithm to assess patience.","We suggest strategies to operationally mitigate the impact of silent abandonment by predicting suspected silent-abandonment behavior or changing service design.","Specifically, we show that while allowing customers to write while waiting in the queue creates a missing data challenge, it also significantly increases patience and reduces service time, leading to reduced abandonment and lower staffing requirements."],"url":"http://arxiv.org/abs/2501.08869v1"}
{"created":"2025-01-15 15:30:31","title":"The geometry of moral decision making","abstract":"We show how (resource) bounded rationality can be understood as the interplay of two fundamental moral principles: deontology and utilitarianism. In particular, we interpret deontology as a regularisation function in an optimal control problem, coupled with a free parameter, the inverse temperature, to shield the individual from expected utility. We discuss the information geometry of bounded rationality and aspects of its relation to rate distortion theory. A central role is played by Markov kernels and regular conditional probability, which are also studied geometrically. A gradient equation is used to determine the utility expansion path. Finally, the framework is applied to the analysis of a disutility model of the restriction of constitutional rights that we derive from legal doctrine. The methods discussed here are also relevant to the theory of autonomous agents.","sentences":["We show how (resource) bounded rationality can be understood as the interplay of two fundamental moral principles: deontology and utilitarianism.","In particular, we interpret deontology as a regularisation function in an optimal control problem, coupled with a free parameter, the inverse temperature, to shield the individual from expected utility.","We discuss the information geometry of bounded rationality and aspects of its relation to rate distortion theory.","A central role is played by Markov kernels and regular conditional probability, which are also studied geometrically.","A gradient equation is used to determine the utility expansion path.","Finally, the framework is applied to the analysis of a disutility model of the restriction of constitutional rights that we derive from legal doctrine.","The methods discussed here are also relevant to the theory of autonomous agents."],"url":"http://arxiv.org/abs/2501.08865v1"}
{"created":"2025-01-15 15:22:57","title":"ARMOR: Shielding Unlearnable Examples against Data Augmentation","abstract":"Private data, when published online, may be collected by unauthorized parties to train deep neural networks (DNNs). To protect privacy, defensive noises can be added to original samples to degrade their learnability by DNNs. Recently, unlearnable examples are proposed to minimize the training loss such that the model learns almost nothing. However, raw data are often pre-processed before being used for training, which may restore the private information of protected data. In this paper, we reveal the data privacy violation induced by data augmentation, a commonly used data pre-processing technique to improve model generalization capability, which is the first of its kind as far as we are concerned. We demonstrate that data augmentation can significantly raise the accuracy of the model trained on unlearnable examples from 21.3% to 66.1%. To address this issue, we propose a defense framework, dubbed ARMOR, to protect data privacy from potential breaches of data augmentation. To overcome the difficulty of having no access to the model training process, we design a non-local module-assisted surrogate model that better captures the effect of data augmentation. In addition, we design a surrogate augmentation selection strategy that maximizes distribution alignment between augmented and non-augmented samples, to choose the optimal augmentation strategy for each class. We also use a dynamic step size adjustment algorithm to enhance the defensive noise generation process. Extensive experiments are conducted on 4 datasets and 5 data augmentation methods to verify the performance of ARMOR. Comparisons with 6 state-of-the-art defense methods have demonstrated that ARMOR can preserve the unlearnability of protected private data under data augmentation. ARMOR reduces the test accuracy of the model trained on augmented protected samples by as much as 60% more than baselines.","sentences":["Private data, when published online, may be collected by unauthorized parties to train deep neural networks (DNNs).","To protect privacy, defensive noises can be added to original samples to degrade their learnability by DNNs.","Recently, unlearnable examples are proposed to minimize the training loss such that the model learns almost nothing.","However, raw data are often pre-processed before being used for training, which may restore the private information of protected data.","In this paper, we reveal the data privacy violation induced by data augmentation, a commonly used data pre-processing technique to improve model generalization capability, which is the first of its kind as far as we are concerned.","We demonstrate that data augmentation can significantly raise the accuracy of the model trained on unlearnable examples from 21.3% to 66.1%.","To address this issue, we propose a defense framework, dubbed ARMOR, to protect data privacy from potential breaches of data augmentation.","To overcome the difficulty of having no access to the model training process, we design a non-local module-assisted surrogate model that better captures the effect of data augmentation.","In addition, we design a surrogate augmentation selection strategy that maximizes distribution alignment between augmented and non-augmented samples, to choose the optimal augmentation strategy for each class.","We also use a dynamic step size adjustment algorithm to enhance the defensive noise generation process.","Extensive experiments are conducted on 4 datasets and 5 data augmentation methods to verify the performance of ARMOR.","Comparisons with 6 state-of-the-art defense methods have demonstrated that ARMOR can preserve the unlearnability of protected private data under data augmentation.","ARMOR reduces the test accuracy of the model trained on augmented protected samples by as much as 60% more than baselines."],"url":"http://arxiv.org/abs/2501.08862v1"}
{"created":"2025-01-15 15:05:49","title":"Digital Phenotyping for Adolescent Mental Health: A Feasibility Study Employing Machine Learning to Predict Mental Health Risk From Active and Passive Smartphone Data","abstract":"Background: Adolescents are particularly vulnerable to mental disorders, with over 75% of cases manifesting before the age of 25. Research indicates that only 18 to 34% of young people experiencing high levels of depression or anxiety symptoms seek support. Digital tools leveraging smartphones offer scalable and early intervention opportunities. Objective: Using a novel machine learning framework, this study evaluated the feasibility of integrating active and passive smartphone data to predict mental disorders in non-clinical adolescents. Specifically, we investigated the utility of the Mindcraft app in predicting risks for internalising and externalising disorders, eating disorders, insomnia and suicidal ideation. Methods: Participants (N=103; mean age 16.1 years) were recruited from three London schools. Participants completed the Strengths and Difficulties Questionnaire, the Eating Disorders-15 Questionnaire, Sleep Condition Indicator Questionnaire and indicated the presence/absence of suicidal ideation. They used the Mindcraft app for 14 days, contributing active data via self-reports and passive data from smartphone sensors. A contrastive pretraining phase was applied to enhance user-specific feature stability, followed by supervised fine-tuning. The model evaluation employed leave-one-subject-out cross-validation using balanced accuracy as the primary metric. Results: The integration of active and passive data achieved superior performance compared to individual data sources, with mean balanced accuracies of 0.71 for SDQ-High risk, 0.67 for insomnia, 0.77 for suicidal ideation and 0.70 for eating disorders. The contrastive learning framework stabilised daily behavioural representations, enhancing predictive robustness. This study demonstrates the potential of integrating active and passive smartphone data with advanced machine-learning techniques for predicting mental health risks.","sentences":["Background: Adolescents are particularly vulnerable to mental disorders, with over 75% of cases manifesting before the age of 25.","Research indicates that only 18 to 34% of young people experiencing high levels of depression or anxiety symptoms seek support.","Digital tools leveraging smartphones offer scalable and early intervention opportunities.","Objective: Using a novel machine learning framework, this study evaluated the feasibility of integrating active and passive smartphone data to predict mental disorders in non-clinical adolescents.","Specifically, we investigated the utility of the Mindcraft app in predicting risks for internalising and externalising disorders, eating disorders, insomnia and suicidal ideation.","Methods: Participants (N=103; mean age 16.1 years) were recruited from three London schools.","Participants completed the Strengths and Difficulties Questionnaire, the Eating Disorders-15 Questionnaire, Sleep Condition Indicator Questionnaire and indicated the presence/absence of suicidal ideation.","They used the Mindcraft app for 14 days, contributing active data via self-reports and passive data from smartphone sensors.","A contrastive pretraining phase was applied to enhance user-specific feature stability, followed by supervised fine-tuning.","The model evaluation employed leave-one-subject-out cross-validation using balanced accuracy as the primary metric.","Results: The integration of active and passive data achieved superior performance compared to individual data sources, with mean balanced accuracies of 0.71 for SDQ-High risk, 0.67 for insomnia, 0.77 for suicidal ideation and 0.70 for eating disorders.","The contrastive learning framework stabilised daily behavioural representations, enhancing predictive robustness.","This study demonstrates the potential of integrating active and passive smartphone data with advanced machine-learning techniques for predicting mental health risks."],"url":"http://arxiv.org/abs/2501.08851v1"}
{"created":"2025-01-15 15:04:10","title":"Graph Counterfactual Explainable AI via Latent Space Traversal","abstract":"Explaining the predictions of a deep neural network is a nontrivial task, yet high-quality explanations for predictions are often a prerequisite for practitioners to trust these models. Counterfactual explanations aim to explain predictions by finding the ''nearest'' in-distribution alternative input whose prediction changes in a pre-specified way. However, it remains an open question how to define this nearest alternative input, whose solution depends on both the domain (e.g. images, graphs, tabular data, etc.) and the specific application considered. For graphs, this problem is complicated i) by their discrete nature, as opposed to the continuous nature of state-of-the-art graph classifiers; and ii) by the node permutation group acting on the graphs. We propose a method to generate counterfactual explanations for any differentiable black-box graph classifier, utilizing a case-specific permutation equivariant graph variational autoencoder. We generate counterfactual explanations in a continuous fashion by traversing the latent space of the autoencoder across the classification boundary of the classifier, allowing for seamless integration of discrete graph structure and continuous graph attributes. We empirically validate the approach on three graph datasets, showing that our model is consistently high-performing and more robust than the baselines.","sentences":["Explaining the predictions of a deep neural network is a nontrivial task, yet high-quality explanations for predictions are often a prerequisite for practitioners to trust these models.","Counterfactual explanations aim to explain predictions by finding the ''nearest'' in-distribution alternative input whose prediction changes in a pre-specified way.","However, it remains an open question how to define this nearest alternative input, whose solution depends on both the domain (e.g. images, graphs, tabular data, etc.) and the specific application considered.","For graphs, this problem is complicated i) by their discrete nature, as opposed to the continuous nature of state-of-the-art graph classifiers; and ii) by the node permutation group acting on the graphs.","We propose a method to generate counterfactual explanations for any differentiable black-box graph classifier, utilizing a case-specific permutation equivariant graph variational autoencoder.","We generate counterfactual explanations in a continuous fashion by traversing the latent space of the autoencoder across the classification boundary of the classifier, allowing for seamless integration of discrete graph structure and continuous graph attributes.","We empirically validate the approach on three graph datasets, showing that our model is consistently high-performing and more robust than the baselines."],"url":"http://arxiv.org/abs/2501.08850v1"}
{"created":"2025-01-15 14:59:00","title":"Automatic tuning of communication protocols for vehicular ad hoc networks using metaheuristics","abstract":"The emerging field of vehicular ad hoc networks (VANETs) deals with a set of communicating vehicles which are able to spontaneously interconnect without any pre-existing infrastructure. In such kind of networks, it is crucial to make an optimal configuration of the communication protocols previously to the final network deployment. This way, a human designer can obtain an optimal QoS of the network beforehand. The problem we consider in this work lies in configuring the File Transfer protocol Configuration (FTC) with the aim of optimizing the transmission time, the number of lost packets, and the amount of data transferred in realistic VANET scenarios. We face the FTC with five representative state-of-the-art optimization techniques and compare their performance. These algorithms are: Particle Swarm Optimization (PSO), Differential Evolution (DE), Genetic Algorithm (GA), Evolutionary Strategy (ES), and Simulated Annealing (SA). For our tests, two typical environment instances of VANETs for Urban and Highway scenarios have been defined. The experiments using ns- 2 (a well-known realistic VANET simulator) reveal that PSO outperforms all the compared algorithms for both studied VANET instances.","sentences":["The emerging field of vehicular ad hoc networks (VANETs) deals with a set of communicating vehicles which are able to spontaneously interconnect without any pre-existing infrastructure.","In such kind of networks, it is crucial to make an optimal configuration of the communication protocols previously to the final network deployment.","This way, a human designer can obtain an optimal QoS of the network beforehand.","The problem we consider in this work lies in configuring the File Transfer protocol Configuration (FTC) with the aim of optimizing the transmission time, the number of lost packets, and the amount of data transferred in realistic VANET scenarios.","We face the FTC with five representative state-of-the-art optimization techniques and compare their performance.","These algorithms are: Particle Swarm Optimization (PSO), Differential Evolution (DE), Genetic Algorithm (GA), Evolutionary Strategy (ES), and Simulated Annealing (SA).","For our tests, two typical environment instances of VANETs for Urban and Highway scenarios have been defined.","The experiments using ns- 2 (a well-known realistic VANET simulator) reveal that PSO outperforms all the compared algorithms for both studied VANET instances."],"url":"http://arxiv.org/abs/2501.08847v1"}
{"created":"2025-01-15 14:58:41","title":"Beating Competitive Ratio 4 for Graphic Matroid Secretary","abstract":"One of the classic problems in online decision-making is the *secretary problem* where to goal is to maximize the probability of choosing the largest number from a randomly ordered sequence. A natural extension allows selecting multiple values under a combinatorial constraint. Babaioff, Immorlica, Kempe, and Kleinberg (SODA'07, JACM'18) introduced the *matroid secretary conjecture*, suggesting an $O(1)$-competitive algorithm exists for matroids. Many works since have attempted to obtain algorithms for both general matroids and specific classes of matroids. The ultimate goal is to obtain an $e$-competitive algorithm, and the *strong matroid secretary conjecture* states that this is possible for general matroids.   A key class of matroids is the *graphic matroid*, where a set of graph edges is independent if it contains no cycle. The rich combinatorial structure of graphs makes them a natural first step towards solving a problem for general matroids. Babaioff et al. (SODA'07, JACM'18) first studied the graphic matroid setting, achieving a $16$-competitive algorithm. Subsequent works have improved the competitive ratio, most recently to 4 by Soto, Turkieltaub, and Verdugo (SODA'18).   We break this $4$-competitive barrier, presenting a new algorithm with a competitive ratio of $3.95$. For simple graphs, we further improve this to $3.77$. Intuitively, solving the problem for simple graphs is easier since they lack length-two cycles. A natural question is whether a ratio arbitrarily close to $e$ can be achieved by assuming sufficiently large girth.   We answer this affirmatively, showing a competitive ratio arbitrarily close to $e$ even for constant girth values, supporting the strong matroid secretary conjecture. We also prove this bound is tight: for any constant $g$, no algorithm can achieve a ratio better than $e$ even when the graph has girth at least $g$.","sentences":["One of the classic problems in online decision-making is the *secretary problem* where to goal is to maximize the probability of choosing the largest number from a randomly ordered sequence.","A natural extension allows selecting multiple values under a combinatorial constraint.","Babaioff, Immorlica, Kempe, and Kleinberg (SODA'07, JACM'18) introduced the *matroid secretary conjecture*, suggesting an $O(1)$-competitive algorithm exists for matroids.","Many works since have attempted to obtain algorithms for both general matroids and specific classes of matroids.","The ultimate goal is to obtain an $e$-competitive algorithm, and the *strong matroid secretary conjecture* states that this is possible for general matroids.   ","A key class of matroids is the *graphic matroid*, where a set of graph edges is independent if it contains no cycle.","The rich combinatorial structure of graphs makes them a natural first step towards solving a problem for general matroids.","Babaioff et al.","(SODA'07, JACM'18) first studied the graphic matroid setting, achieving a $16$-competitive algorithm.","Subsequent works have improved the competitive ratio, most recently to 4 by Soto, Turkieltaub, and Verdugo (SODA'18).   ","We break this $4$-competitive barrier, presenting a new algorithm with a competitive ratio of $3.95$. For simple graphs, we further improve this to $3.77$. Intuitively, solving the problem for simple graphs is easier since they lack length-two cycles.","A natural question is whether a ratio arbitrarily close to $e$ can be achieved by assuming sufficiently large girth.   ","We answer this affirmatively, showing a competitive ratio arbitrarily close to $e$ even for constant girth values, supporting the strong matroid secretary conjecture.","We also prove this bound is tight: for any constant $g$, no algorithm can achieve a ratio better than $e$ even when the graph has girth at least $g$."],"url":"http://arxiv.org/abs/2501.08846v1"}
{"created":"2025-01-15 14:19:03","title":"A Closer Look at the Learnability of Out-of-Distribution (OOD) Detection","abstract":"Machine learning algorithms often encounter different or \"out-of-distribution\" (OOD) data at deployment time, and OOD detection is frequently employed to detect these examples. While it works reasonably well in practice, existing theoretical results on OOD detection are highly pessimistic. In this work, we take a closer look at this problem, and make a distinction between uniform and non-uniform learnability, following PAC learning theory. We characterize under what conditions OOD detection is uniformly and non-uniformly learnable, and we show that in several cases, non-uniform learnability turns a number of negative results into positive. In all cases where OOD detection is learnable, we provide concrete learning algorithms and a sample-complexity analysis.","sentences":["Machine learning algorithms often encounter different or \"out-of-distribution\" (OOD) data at deployment time, and OOD detection is frequently employed to detect these examples.","While it works reasonably well in practice, existing theoretical results on OOD detection are highly pessimistic.","In this work, we take a closer look at this problem, and make a distinction between uniform and non-uniform learnability, following PAC learning theory.","We characterize under what conditions OOD detection is uniformly and non-uniformly learnable, and we show that in several cases, non-uniform learnability turns a number of negative results into positive.","In all cases where OOD detection is learnable, we provide concrete learning algorithms and a sample-complexity analysis."],"url":"http://arxiv.org/abs/2501.08821v1"}
{"created":"2025-01-15 14:12:59","title":"IDEA: Image Description Enhanced CLIP-Adapter","abstract":"CLIP (Contrastive Language-Image Pre-training) has attained great success in pattern recognition and computer vision. Transferring CLIP to downstream tasks (e.g. zero- or few-shot classification) is a hot topic in multimodal learning. However, current studies primarily focus on either prompt learning for text or adapter tuning for vision, without fully exploiting the complementary information and correlations among image-text pairs. In this paper, we propose an Image Description Enhanced CLIP-Adapter (IDEA) method to adapt CLIP to few-shot image classification tasks. This method captures fine-grained features by leveraging both visual features and textual descriptions of images. IDEA is a training-free method for CLIP, and it can be comparable to or even exceeds state-of-the-art models on multiple tasks. Furthermore, we introduce Trainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable components (i.e., a projector and a learnable latent space), further enhancing the model's performance and achieving SOTA results on 11 datasets. As one important contribution, we employ the Llama model and design a comprehensive pipeline to generate textual descriptions for images of 11 datasets, resulting in a total of 1,637,795 image-text pairs, named \"IMD-11\". Our code and data are released at https://github.com/FourierAI/IDEA.","sentences":["CLIP (Contrastive Language-Image Pre-training) has attained great success in pattern recognition and computer vision.","Transferring CLIP to downstream tasks (e.g. zero- or few-shot classification) is a hot topic in multimodal learning.","However, current studies primarily focus on either prompt learning for text or adapter tuning for vision, without fully exploiting the complementary information and correlations among image-text pairs.","In this paper, we propose an Image Description Enhanced CLIP-Adapter (IDEA) method to adapt CLIP to few-shot image classification tasks.","This method captures fine-grained features by leveraging both visual features and textual descriptions of images.","IDEA is a training-free method for CLIP, and it can be comparable to or even exceeds state-of-the-art models on multiple tasks.","Furthermore, we introduce Trainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable components (i.e., a projector and a learnable latent space), further enhancing the model's performance and achieving SOTA results on 11 datasets.","As one important contribution, we employ the Llama model and design a comprehensive pipeline to generate textual descriptions for images of 11 datasets, resulting in a total of 1,637,795 image-text pairs, named \"IMD-11\".","Our code and data are released at https://github.com/FourierAI/IDEA."],"url":"http://arxiv.org/abs/2501.08816v1"}
{"created":"2025-01-15 14:12:38","title":"SAIF: A Comprehensive Framework for Evaluating the Risks of Generative AI in the Public Sector","abstract":"The rapid adoption of generative AI in the public sector, encompassing diverse applications ranging from automated public assistance to welfare services and immigration processes, highlights its transformative potential while underscoring the pressing need for thorough risk assessments. Despite its growing presence, evaluations of risks associated with AI-driven systems in the public sector remain insufficiently explored. Building upon an established taxonomy of AI risks derived from diverse government policies and corporate guidelines, we investigate the critical risks posed by generative AI in the public sector while extending the scope to account for its multimodal capabilities. In addition, we propose a Systematic dAta generatIon Framework for evaluating the risks of generative AI (SAIF). SAIF involves four key stages: breaking down risks, designing scenarios, applying jailbreak methods, and exploring prompt types. It ensures the systematic and consistent generation of prompt data, facilitating a comprehensive evaluation while providing a solid foundation for mitigating the risks. Furthermore, SAIF is designed to accommodate emerging jailbreak methods and evolving prompt types, thereby enabling effective responses to unforeseen risk scenarios. We believe that this study can play a crucial role in fostering the safe and responsible integration of generative AI into the public sector.","sentences":["The rapid adoption of generative AI in the public sector, encompassing diverse applications ranging from automated public assistance to welfare services and immigration processes, highlights its transformative potential while underscoring the pressing need for thorough risk assessments.","Despite its growing presence, evaluations of risks associated with AI-driven systems in the public sector remain insufficiently explored.","Building upon an established taxonomy of AI risks derived from diverse government policies and corporate guidelines, we investigate the critical risks posed by generative AI in the public sector while extending the scope to account for its multimodal capabilities.","In addition, we propose a Systematic dAta generatIon Framework for evaluating the risks of generative AI (SAIF).","SAIF involves four key stages: breaking down risks, designing scenarios, applying jailbreak methods, and exploring prompt types.","It ensures the systematic and consistent generation of prompt data, facilitating a comprehensive evaluation while providing a solid foundation for mitigating the risks.","Furthermore, SAIF is designed to accommodate emerging jailbreak methods and evolving prompt types, thereby enabling effective responses to unforeseen risk scenarios.","We believe that this study can play a crucial role in fostering the safe and responsible integration of generative AI into the public sector."],"url":"http://arxiv.org/abs/2501.08814v1"}
{"created":"2025-01-15 14:03:27","title":"Multi-visual modality micro drone-based structural damage detection","abstract":"Accurate detection and resilience of object detectors in structural damage detection are important in ensuring the continuous use of civil infrastructure. However, achieving robustness in object detectors remains a persistent challenge, impacting their ability to generalize effectively. This study proposes DetectorX, a robust framework for structural damage detection coupled with a micro drone. DetectorX addresses the challenges of object detector robustness by incorporating two innovative modules: a stem block and a spiral pooling technique. The stem block introduces a dynamic visual modality by leveraging the outputs of two Deep Convolutional Neural Network (DCNN) models. The framework employs the proposed event-based reward reinforcement learning to constrain the actions of a parent and child DCNN model leading to a reward. This results in the induction of two dynamic visual modalities alongside the Red, Green, and Blue (RGB) data. This enhancement significantly augments DetectorX's perception and adaptability in diverse environmental situations. Further, a spiral pooling technique, an online image augmentation method, strengthens the framework by increasing feature representations by concatenating spiraled and average/max pooled features. In three extensive experiments: (1) comparative and (2) robustness, which use the Pacific Earthquake Engineering Research Hub ImageNet dataset, and (3) field-experiment, DetectorX performed satisfactorily across varying metrics, including precision (0.88), recall (0.84), average precision (0.91), mean average precision (0.76), and mean average recall (0.73), compared to the competing detectors including You Only Look Once X-medium (YOLOX-m) and others. The study's findings indicate that DetectorX can provide satisfactory results and demonstrate resilience in challenging environments.","sentences":["Accurate detection and resilience of object detectors in structural damage detection are important in ensuring the continuous use of civil infrastructure.","However, achieving robustness in object detectors remains a persistent challenge, impacting their ability to generalize effectively.","This study proposes DetectorX, a robust framework for structural damage detection coupled with a micro drone.","DetectorX addresses the challenges of object detector robustness by incorporating two innovative modules: a stem block and a spiral pooling technique.","The stem block introduces a dynamic visual modality by leveraging the outputs of two Deep Convolutional Neural Network (DCNN) models.","The framework employs the proposed event-based reward reinforcement learning to constrain the actions of a parent and child DCNN model leading to a reward.","This results in the induction of two dynamic visual modalities alongside the Red, Green, and Blue (RGB) data.","This enhancement significantly augments DetectorX's perception and adaptability in diverse environmental situations.","Further, a spiral pooling technique, an online image augmentation method, strengthens the framework by increasing feature representations by concatenating spiraled and average/max pooled features.","In three extensive experiments: (1) comparative and (2) robustness, which use the Pacific Earthquake Engineering Research Hub ImageNet dataset, and (3) field-experiment, DetectorX performed satisfactorily across varying metrics, including precision (0.88), recall (0.84), average precision (0.91), mean average precision (0.76), and mean average recall (0.73), compared to the competing detectors including You Only Look Once X-medium (YOLOX-m) and others.","The study's findings indicate that DetectorX can provide satisfactory results and demonstrate resilience in challenging environments."],"url":"http://arxiv.org/abs/2501.08807v1"}
{"created":"2025-01-15 13:46:33","title":"Exploring ChatGPT for Face Presentation Attack Detection in Zero and Few-Shot in-Context Learning","abstract":"This study highlights the potential of ChatGPT (specifically GPT-4o) as a competitive alternative for Face Presentation Attack Detection (PAD), outperforming several PAD models, including commercial solutions, in specific scenarios. Our results show that GPT-4o demonstrates high consistency, particularly in few-shot in-context learning, where its performance improves as more examples are provided (reference data). We also observe that detailed prompts enable the model to provide scores reliably, a behavior not observed with concise prompts. Additionally, explanation-seeking prompts slightly enhance the model's performance by improving its interpretability. Remarkably, the model exhibits emergent reasoning capabilities, correctly predicting the attack type (print or replay) with high accuracy in few-shot scenarios, despite not being explicitly instructed to classify attack types. Despite these strengths, GPT-4o faces challenges in zero-shot tasks, where its performance is limited compared to specialized PAD systems. Experiments were conducted on a subset of the SOTERIA dataset, ensuring compliance with data privacy regulations by using only data from consenting individuals. These findings underscore GPT-4o's promise in PAD applications, laying the groundwork for future research to address broader data privacy concerns and improve cross-dataset generalization. Code available here: https://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad","sentences":["This study highlights the potential of ChatGPT (specifically GPT-4o) as a competitive alternative for Face Presentation Attack Detection (PAD), outperforming several PAD models, including commercial solutions, in specific scenarios.","Our results show that GPT-4o demonstrates high consistency, particularly in few-shot in-context learning, where its performance improves as more examples are provided (reference data).","We also observe that detailed prompts enable the model to provide scores reliably, a behavior not observed with concise prompts.","Additionally, explanation-seeking prompts slightly enhance the model's performance by improving its interpretability.","Remarkably, the model exhibits emergent reasoning capabilities, correctly predicting the attack type (print or replay) with high accuracy in few-shot scenarios, despite not being explicitly instructed to classify attack types.","Despite these strengths, GPT-4o faces challenges in zero-shot tasks, where its performance is limited compared to specialized PAD systems.","Experiments were conducted on a subset of the SOTERIA dataset, ensuring compliance with data privacy regulations by using only data from consenting individuals.","These findings underscore GPT-4o's promise in PAD applications, laying the groundwork for future research to address broader data privacy concerns and improve cross-dataset generalization.","Code available here: https://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad"],"url":"http://arxiv.org/abs/2501.08799v1"}
{"created":"2025-01-15 13:01:47","title":"Deep learning for temporal super-resolution 4D Flow MRI","abstract":"4D Flow Magnetic Resonance Imaging (4D Flow MRI) is a non-invasive technique for volumetric, time-resolved blood flow quantification. However, apparent trade-offs between acquisition time, image noise, and resolution limit clinical applicability. In particular, in regions of highly transient flow, coarse temporal resolution can hinder accurate capture of physiologically relevant flow variations. To overcome these issues, post-processing techniques using deep learning have shown promising results to enhance resolution post-scan using so-called super-resolution networks. However, while super-resolution has been focusing on spatial upsampling, temporal super-resolution remains largely unexplored. The aim of this study was therefore to implement and evaluate a residual network for temporal super-resolution 4D Flow MRI. To achieve this, an existing spatial network (4DFlowNet) was re-designed for temporal upsampling, adapting input dimensions, and optimizing internal layer structures. Training and testing were performed using synthetic 4D Flow MRI data originating from patient-specific in-silico models, as well as using in-vivo datasets. Overall, excellent performance was achieved with input velocities effectively denoised and temporally upsampled, with a mean absolute error (MAE) of 1.0 cm/s in an unseen in-silico setting, outperforming deterministic alternatives (linear interpolation MAE = 2.3 cm/s, sinc interpolation MAE = 2.6 cm/s). Further, the network synthesized high-resolution temporal information from unseen low-resolution in-vivo data, with strong correlation observed at peak flow frames. As such, our results highlight the potential of utilizing data-driven neural networks for temporal super-resolution 4D Flow MRI, enabling high-frame-rate flow quantification without extending acquisition times beyond clinically acceptable limits.","sentences":["4D Flow Magnetic Resonance Imaging (4D Flow MRI) is a non-invasive technique for volumetric, time-resolved blood flow quantification.","However, apparent trade-offs between acquisition time, image noise, and resolution limit clinical applicability.","In particular, in regions of highly transient flow, coarse temporal resolution can hinder accurate capture of physiologically relevant flow variations.","To overcome these issues, post-processing techniques using deep learning have shown promising results to enhance resolution post-scan using so-called super-resolution networks.","However, while super-resolution has been focusing on spatial upsampling, temporal super-resolution remains largely unexplored.","The aim of this study was therefore to implement and evaluate a residual network for temporal super-resolution 4D Flow MRI.","To achieve this, an existing spatial network (4DFlowNet) was re-designed for temporal upsampling, adapting input dimensions, and optimizing internal layer structures.","Training and testing were performed using synthetic 4D Flow MRI data originating from patient-specific in-silico models, as well as using in-vivo datasets.","Overall, excellent performance was achieved with input velocities effectively denoised and temporally upsampled, with a mean absolute error (MAE) of 1.0 cm/s in an unseen in-silico setting, outperforming deterministic alternatives (linear interpolation MAE = 2.3 cm/s, sinc interpolation MAE = 2.6 cm/s).","Further, the network synthesized high-resolution temporal information from unseen low-resolution in-vivo data, with strong correlation observed at peak flow frames.","As such, our results highlight the potential of utilizing data-driven neural networks for temporal super-resolution 4D Flow MRI, enabling high-frame-rate flow quantification without extending acquisition times beyond clinically acceptable limits."],"url":"http://arxiv.org/abs/2501.08780v1"}
{"created":"2025-01-15 12:56:38","title":"Adaptive Approximation Schemes for Matching Queues","abstract":"We study a continuous-time, infinite-horizon dynamic matching problem. Suppliers arrive according to a Poisson process; while waiting, they may abandon the queue at a uniform rate. Customers on the other side of the network must be matched upon arrival. The objective is to minimize the expected long-term average cost subject to a throughput constraint on the total match rate.   Previous literature on dynamic matching focuses on \"static\" policies, where the matching decisions do not depend explicitly on the state of the supplier queues, achieving constant-factor approximations. By contrast, we design \"adaptive\" policies, which leverage queue length information, and obtain near-optimal polynomial-time algorithms for several classes of instances.   First, we develop a bi-criteria Fully Polynomial-time Approximation Scheme (FPTAS) for dynamic matching on networks with a constant number of queues -- that computes a $(1-\\epsilon)$-approximation of the optimal policy in time polynomial in both the input size and $1/\\epsilon$. Using this algorithm as a subroutine, we obtain an FPTAS for dynamic matching on Euclidean networks of fixed dimension. A key new technique is a hybrid LP relaxation, which combines static and state-dependent LP approximations of the queue dynamics, after a decomposition of the network.   Constant-size networks are motivated by deceased organ donation schemes, where the supply types can be divided according to blood and tissue types. The Euclidean case is of interest in ride-hailing and spatial service platforms, where the goal is to fulfill as many trips as possible while minimizing driving distances.","sentences":["We study a continuous-time, infinite-horizon dynamic matching problem.","Suppliers arrive according to a Poisson process; while waiting, they may abandon the queue at a uniform rate.","Customers on the other side of the network must be matched upon arrival.","The objective is to minimize the expected long-term average cost subject to a throughput constraint on the total match rate.   ","Previous literature on dynamic matching focuses on \"static\" policies, where the matching decisions do not depend explicitly on the state of the supplier queues, achieving constant-factor approximations.","By contrast, we design \"adaptive\" policies, which leverage queue length information, and obtain near-optimal polynomial-time algorithms for several classes of instances.   ","First, we develop a bi-criteria Fully Polynomial-time Approximation Scheme (FPTAS) for dynamic matching on networks with a constant number of queues -- that computes a $(1-\\epsilon)$-approximation of the optimal policy in time polynomial in both the input size and $1/\\epsilon$. Using this algorithm as a subroutine, we obtain an FPTAS for dynamic matching on Euclidean networks of fixed dimension.","A key new technique is a hybrid LP relaxation, which combines static and state-dependent LP approximations of the queue dynamics, after a decomposition of the network.   ","Constant-size networks are motivated by deceased organ donation schemes, where the supply types can be divided according to blood and tissue types.","The Euclidean case is of interest in ride-hailing and spatial service platforms, where the goal is to fulfill as many trips as possible while minimizing driving distances."],"url":"http://arxiv.org/abs/2501.08775v1"}
{"created":"2025-01-15 12:44:52","title":"Admitting Ignorance Helps the Video Question Answering Models to Answer","abstract":"Significant progress has been made in the field of video question answering (VideoQA) thanks to deep learning and large-scale pretraining. Despite the presence of sophisticated model structures and powerful video-text foundation models, most existing methods focus solely on maximizing the correlation between answers and video-question pairs during training. We argue that these models often establish shortcuts, resulting in spurious correlations between questions and answers, especially when the alignment between video and text data is suboptimal. To address these spurious correlations, we propose a novel training framework in which the model is compelled to acknowledge its ignorance when presented with an intervened question, rather than making guesses solely based on superficial question-answer correlations. We introduce methodologies for intervening in questions, utilizing techniques such as displacement and perturbation, and design frameworks for the model to admit its lack of knowledge in both multi-choice VideoQA and open-ended settings. In practice, we integrate a state-of-the-art model into our framework to validate its effectiveness. The results clearly demonstrate that our framework can significantly enhance the performance of VideoQA models with minimal structural modifications.","sentences":["Significant progress has been made in the field of video question answering (VideoQA) thanks to deep learning and large-scale pretraining.","Despite the presence of sophisticated model structures and powerful video-text foundation models, most existing methods focus solely on maximizing the correlation between answers and video-question pairs during training.","We argue that these models often establish shortcuts, resulting in spurious correlations between questions and answers, especially when the alignment between video and text data is suboptimal.","To address these spurious correlations, we propose a novel training framework in which the model is compelled to acknowledge its ignorance when presented with an intervened question, rather than making guesses solely based on superficial question-answer correlations.","We introduce methodologies for intervening in questions, utilizing techniques such as displacement and perturbation, and design frameworks for the model to admit its lack of knowledge in both multi-choice VideoQA and open-ended settings.","In practice, we integrate a state-of-the-art model into our framework to validate its effectiveness.","The results clearly demonstrate that our framework can significantly enhance the performance of VideoQA models with minimal structural modifications."],"url":"http://arxiv.org/abs/2501.08771v1"}
{"created":"2025-01-15 12:42:09","title":"Enhanced Large Language Models for Effective Screening of Depression and Anxiety","abstract":"Depressive and anxiety disorders are widespread, necessitating timely identification and management. Recent advances in Large Language Models (LLMs) offer potential solutions, yet high costs and ethical concerns about training data remain challenges. This paper introduces a pipeline for synthesizing clinical interviews, resulting in 1,157 interactive dialogues (PsyInterview), and presents EmoScan, an LLM-based emotional disorder screening system. EmoScan distinguishes between coarse (e.g., anxiety or depressive disorders) and fine disorders (e.g., major depressive disorders) and conducts high-quality interviews. Evaluations showed that EmoScan exceeded the performance of base models and other LLMs like GPT-4 in screening emotional disorders (F1-score=0.7467). It also delivers superior explanations (BERTScore=0.9408) and demonstrates robust generalizability (F1-score of 0.67 on an external dataset). Furthermore, EmoScan outperforms baselines in interviewing skills, as validated by automated ratings and human evaluations. This work highlights the importance of scalable data-generative pipelines for developing effective mental health LLM tools.","sentences":["Depressive and anxiety disorders are widespread, necessitating timely identification and management.","Recent advances in Large Language Models (LLMs) offer potential solutions, yet high costs and ethical concerns about training data remain challenges.","This paper introduces a pipeline for synthesizing clinical interviews, resulting in 1,157 interactive dialogues (PsyInterview), and presents EmoScan, an LLM-based emotional disorder screening system.","EmoScan distinguishes between coarse (e.g., anxiety or depressive disorders) and fine disorders (e.g., major depressive disorders) and conducts high-quality interviews.","Evaluations showed that EmoScan exceeded the performance of base models and other LLMs like GPT-4 in screening emotional disorders (F1-score=0.7467).","It also delivers superior explanations (BERTScore=0.9408) and demonstrates robust generalizability (F1-score of 0.67 on an external dataset).","Furthermore, EmoScan outperforms baselines in interviewing skills, as validated by automated ratings and human evaluations.","This work highlights the importance of scalable data-generative pipelines for developing effective mental health LLM tools."],"url":"http://arxiv.org/abs/2501.08769v1"}
{"created":"2025-01-15 12:33:11","title":"Few-Shot Learner Generalizes Across AI-Generated Image Detection","abstract":"Current fake image detectors trained on large synthetic image datasets perform satisfactorily on limited studied generative models. However, they suffer a notable performance decline over unseen models. Besides, collecting adequate training data from online generative models is often expensive or infeasible. To overcome these issues, we propose Few-Shot Detector (FSD), a novel AI-generated image detector which learns a specialized metric space to effectively distinguish unseen fake images by utilizing very few samples. Experiments show FSD achieves state-of-the-art performance by $+7.4\\%$ average ACC on GenImage dataset. More importantly, our method is better capable of capturing the intra-category common features in unseen images without further training.","sentences":["Current fake image detectors trained on large synthetic image datasets perform satisfactorily on limited studied generative models.","However, they suffer a notable performance decline over unseen models.","Besides, collecting adequate training data from online generative models is often expensive or infeasible.","To overcome these issues, we propose Few-Shot Detector (FSD), a novel AI-generated image detector which learns a specialized metric space to effectively distinguish unseen fake images by utilizing very few samples.","Experiments show FSD achieves state-of-the-art performance by $+7.4\\%$ average ACC on GenImage dataset.","More importantly, our method is better capable of capturing the intra-category common features in unseen images without further training."],"url":"http://arxiv.org/abs/2501.08763v1"}
{"created":"2025-01-15 12:14:02","title":"Can Millimeter-Wave Support Interactive Extended Reality Under Rapid Rotational Motion?","abstract":"Using Millimeter-Wave (mmWave) wireless communications is often named as the prime enabler for mobile interactive Extended Reality (XR), as it offers multi-gigabit data rates at millisecond-range latency. To achieve this, mmWave nodes must focus their energy towards each other, which is especially challenging in XR scenarios, where the transceiver on the user's XR device may rotate rapidly. To evaluate the feasibility of mmWave XR, we present the first throughput and latency evaluation of state-of-the-art mmWave hardware under rapid rotational motion, for different PHY and MAC-layer parameter configurations. We show that this parameter configuration has a significant impact on performance, and that specialized beamforming approaches for rapid rotational motion may be necessary to enable uninterrupted, high-quality mobile interactive XR experiences.","sentences":["Using Millimeter-Wave (mmWave) wireless communications is often named as the prime enabler for mobile interactive Extended Reality (XR), as it offers multi-gigabit data rates at millisecond-range latency.","To achieve this, mmWave nodes must focus their energy towards each other, which is especially challenging in XR scenarios, where the transceiver on the user's XR device may rotate rapidly.","To evaluate the feasibility of mmWave XR, we present the first throughput and latency evaluation of state-of-the-art mmWave hardware under rapid rotational motion, for different PHY and MAC-layer parameter configurations.","We show that this parameter configuration has a significant impact on performance, and that specialized beamforming approaches for rapid rotational motion may be necessary to enable uninterrupted, high-quality mobile interactive XR experiences."],"url":"http://arxiv.org/abs/2501.08751v1"}
{"created":"2025-01-15 11:34:56","title":"MeshMask: Physics-Based Simulations with Masked Graph Neural Networks","abstract":"We introduce a novel masked pre-training technique for graph neural networks (GNNs) applied to computational fluid dynamics (CFD) problems. By randomly masking up to 40\\% of input mesh nodes during pre-training, we force the model to learn robust representations of complex fluid dynamics. We pair this masking strategy with an asymmetric encoder-decoder architecture and gated multi-layer perceptrons to further enhance performance. The proposed method achieves state-of-the-art results on seven CFD datasets, including a new challenging dataset of 3D intracranial aneurysm simulations with over 250,000 nodes per mesh. Moreover, it significantly improves model performance and training efficiency across such diverse range of fluid simulation tasks. We demonstrate improvements of up to 60\\% in long-term prediction accuracy compared to previous best models, while maintaining similar computational costs. Notably, our approach enables effective pre-training on multiple datasets simultaneously, significantly reducing the time and data required to achieve high performance on new tasks. Through extensive ablation studies, we provide insights into the optimal masking ratio, architectural choices, and training strategies.","sentences":["We introduce a novel masked pre-training technique for graph neural networks (GNNs) applied to computational fluid dynamics (CFD) problems.","By randomly masking up to 40\\% of input mesh nodes during pre-training, we force the model to learn robust representations of complex fluid dynamics.","We pair this masking strategy with an asymmetric encoder-decoder architecture and gated multi-layer perceptrons to further enhance performance.","The proposed method achieves state-of-the-art results on seven CFD datasets, including a new challenging dataset of 3D intracranial aneurysm simulations with over 250,000 nodes per mesh.","Moreover, it significantly improves model performance and training efficiency across such diverse range of fluid simulation tasks.","We demonstrate improvements of up to 60\\% in long-term prediction accuracy compared to previous best models, while maintaining similar computational costs.","Notably, our approach enables effective pre-training on multiple datasets simultaneously, significantly reducing the time and data required to achieve high performance on new tasks.","Through extensive ablation studies, we provide insights into the optimal masking ratio, architectural choices, and training strategies."],"url":"http://arxiv.org/abs/2501.08738v1"}
{"created":"2025-01-15 11:33:52","title":"Resource-Constrained Federated Continual Learning: What Does Matter?","abstract":"Federated Continual Learning (FCL) aims to enable sequentially privacy-preserving model training on streams of incoming data that vary in edge devices by preserving previous knowledge while adapting to new data. Current FCL literature focuses on restricted data privacy and access to previously seen data while imposing no constraints on the training overhead. This is unreasonable for FCL applications in real-world scenarios, where edge devices are primarily constrained by resources such as storage, computational budget, and label rate. We revisit this problem with a large-scale benchmark and analyze the performance of state-of-the-art FCL approaches under different resource-constrained settings. Various typical FCL techniques and six datasets in two incremental learning scenarios (Class-IL and Domain-IL) are involved in our experiments. Through extensive experiments amounting to a total of over 1,000+ GPU hours, we find that, under limited resource-constrained settings, existing FCL approaches, with no exception, fail to achieve the expected performance. Our conclusions are consistent in the sensitivity analysis. This suggests that most existing FCL methods are particularly too resource-dependent for real-world deployment. Moreover, we study the performance of typical FCL techniques with resource constraints and shed light on future research directions in FCL.","sentences":["Federated Continual Learning (FCL) aims to enable sequentially privacy-preserving model training on streams of incoming data that vary in edge devices by preserving previous knowledge while adapting to new data.","Current FCL literature focuses on restricted data privacy and access to previously seen data while imposing no constraints on the training overhead.","This is unreasonable for FCL applications in real-world scenarios, where edge devices are primarily constrained by resources such as storage, computational budget, and label rate.","We revisit this problem with a large-scale benchmark and analyze the performance of state-of-the-art FCL approaches under different resource-constrained settings.","Various typical FCL techniques and six datasets in two incremental learning scenarios (Class-IL and Domain-IL) are involved in our experiments.","Through extensive experiments amounting to a total of over 1,000+ GPU hours, we find that, under limited resource-constrained settings, existing FCL approaches, with no exception, fail to achieve the expected performance.","Our conclusions are consistent in the sensitivity analysis.","This suggests that most existing FCL methods are particularly too resource-dependent for real-world deployment.","Moreover, we study the performance of typical FCL techniques with resource constraints and shed light on future research directions in FCL."],"url":"http://arxiv.org/abs/2501.08737v1"}
{"created":"2025-01-15 11:29:26","title":"Holoview: Interactive 3D visualization of medical data in AR","abstract":"We introduce HoloView, an innovative augmented reality (AR) system that enhances interactive learning of human anatomical structures through immersive visualization. Combining advanced rendering techniques with intuitive gesture-based interactions, HoloView provides a comprehensive technical solution for medical education. The system architecture features a distributed rendering pipeline that offloads stereoscopic computations to a remote server, optimizing performance and enabling high-quality visualization on less powerful devices. To prioritize visual quality in the user's direct line of sight while reducing computational load, we implement foveated rendering optimization, enhancing the immersive experience. Additionally, a hybrid surface-volume rendering technique is used to achieve faster rendering speeds without sacrificing visual fidelity. Complemented by a carefully designed user interface and gesture-based interaction system, HoloView allows users to naturally manipulate holographic content and seamlessly navigate the learning environment. HoloView significantly facilitates anatomical structure visualization and promotes an engaging, user-centric learning experience.","sentences":["We introduce HoloView, an innovative augmented reality (AR) system that enhances interactive learning of human anatomical structures through immersive visualization.","Combining advanced rendering techniques with intuitive gesture-based interactions, HoloView provides a comprehensive technical solution for medical education.","The system architecture features a distributed rendering pipeline that offloads stereoscopic computations to a remote server, optimizing performance and enabling high-quality visualization on less powerful devices.","To prioritize visual quality in the user's direct line of sight while reducing computational load, we implement foveated rendering optimization, enhancing the immersive experience.","Additionally, a hybrid surface-volume rendering technique is used to achieve faster rendering speeds without sacrificing visual fidelity.","Complemented by a carefully designed user interface and gesture-based interaction system, HoloView allows users to naturally manipulate holographic content and seamlessly navigate the learning environment.","HoloView significantly facilitates anatomical structure visualization and promotes an engaging, user-centric learning experience."],"url":"http://arxiv.org/abs/2501.08736v1"}
{"created":"2025-01-15 11:11:38","title":"GRAPPA - A Hybrid Graph Neural Network for Predicting Pure Component Vapor Pressures","abstract":"Although the pure component vapor pressure is one of the most important properties for designing chemical processes, no broadly applicable, sufficiently accurate, and open-source prediction method has been available. To overcome this, we have developed GRAPPA - a hybrid graph neural network for predicting vapor pressures of pure components. GRAPPA enables the prediction of the vapor pressure curve of basically any organic molecule, requiring only the molecular structure as input. The new model consists of three parts: A graph attention network for the message passing step, a pooling function that captures long-range interactions, and a prediction head that yields the component-specific parameters of the Antoine equation, from which the vapor pressure can readily and consistently be calculated for any temperature. We have trained and evaluated GRAPPA on experimental vapor pressure data of almost 25,000 pure components. We found excellent prediction accuracy for unseen components, outperforming state-of-the-art group contribution methods and other machine learning approaches in applicability and accuracy. The trained model and its code are fully disclosed, and GRAPPA is directly applicable via the interactive website ml-prop.mv.rptu.de.","sentences":["Although the pure component vapor pressure is one of the most important properties for designing chemical processes, no broadly applicable, sufficiently accurate, and open-source prediction method has been available.","To overcome this, we have developed GRAPPA - a hybrid graph neural network for predicting vapor pressures of pure components.","GRAPPA enables the prediction of the vapor pressure curve of basically any organic molecule, requiring only the molecular structure as input.","The new model consists of three parts: A graph attention network for the message passing step, a pooling function that captures long-range interactions, and a prediction head that yields the component-specific parameters of the Antoine equation, from which the vapor pressure can readily and consistently be calculated for any temperature.","We have trained and evaluated GRAPPA on experimental vapor pressure data of almost 25,000 pure components.","We found excellent prediction accuracy for unseen components, outperforming state-of-the-art group contribution methods and other machine learning approaches in applicability and accuracy.","The trained model and its code are fully disclosed, and GRAPPA is directly applicable via the interactive website ml-prop.mv.rptu.de."],"url":"http://arxiv.org/abs/2501.08729v1"}
{"created":"2025-01-15 11:05:25","title":"Multilingual Email Phishing Attacks Detection using OSINT and Machine Learning","abstract":"Email phishing remains a prevalent cyber threat, targeting victims to extract sensitive information or deploy malicious software. This paper explores the integration of open-source intelligence (OSINT) tools and machine learning (ML) models to enhance phishing detection across multilingual datasets. Using Nmap and theHarvester, this study extracted 17 features, including domain names, IP addresses, and open ports, to improve detection accuracy. Multilingual email datasets, including English and Arabic, were analyzed to address the limitations of ML models trained predominantly on English data. Experiments with five classification algorithms: Decision Tree, Random Forest, Support Vector Machine, XGBoost, and Multinomial Na\\\"ive Bayes. It revealed that Random Forest achieved the highest performance, with an accuracy of 97.37% for both English and Arabic datasets. For OSINT-enhanced datasets, the model demonstrated an improvement in accuracy compared to baseline models without OSINT features. These findings highlight the potential of combining OSINT tools with advanced ML models to detect phishing emails more effectively across diverse languages and contexts. This study contributes an approach to phishing detection by incorporating OSINT features and evaluating their impact on multilingual datasets, addressing a critical gap in cybersecurity research.","sentences":["Email phishing remains a prevalent cyber threat, targeting victims to extract sensitive information or deploy malicious software.","This paper explores the integration of open-source intelligence (OSINT) tools and machine learning (ML) models to enhance phishing detection across multilingual datasets.","Using Nmap and theHarvester, this study extracted 17 features, including domain names, IP addresses, and open ports, to improve detection accuracy.","Multilingual email datasets, including English and Arabic, were analyzed to address the limitations of ML models trained predominantly on English data.","Experiments with five classification algorithms: Decision Tree, Random Forest, Support Vector Machine, XGBoost, and Multinomial Na\\\"ive Bayes.","It revealed that Random Forest achieved the highest performance, with an accuracy of 97.37% for both English and Arabic datasets.","For OSINT-enhanced datasets, the model demonstrated an improvement in accuracy compared to baseline models without OSINT features.","These findings highlight the potential of combining OSINT tools with advanced ML models to detect phishing emails more effectively across diverse languages and contexts.","This study contributes an approach to phishing detection by incorporating OSINT features and evaluating their impact on multilingual datasets, addressing a critical gap in cybersecurity research."],"url":"http://arxiv.org/abs/2501.08723v1"}
{"created":"2025-01-15 10:58:32","title":"$\\texttt{InfoHier}$: Hierarchical Information Extraction via Encoding and Embedding","abstract":"Analyzing large-scale datasets, especially involving complex and high-dimensional data like images, is particularly challenging. While self-supervised learning (SSL) has proven effective for learning representations from unlabelled data, it typically focuses on flat, non-hierarchical structures, missing the multi-level relationships present in many real-world datasets. Hierarchical clustering (HC) can uncover these relationships by organizing data into a tree-like structure, but it often relies on rigid similarity metrics that struggle to capture the complexity of diverse data types. To address these we envision $\\texttt{InfoHier}$, a framework that combines SSL with HC to jointly learn robust latent representations and hierarchical structures. This approach leverages SSL to provide adaptive representations, enhancing HC's ability to capture complex patterns. Simultaneously, it integrates HC loss to refine SSL training, resulting in representations that are more attuned to the underlying information hierarchy. $\\texttt{InfoHier}$ has the potential to improve the expressiveness and performance of both clustering and representation learning, offering significant benefits for data analysis, management, and information retrieval.","sentences":["Analyzing large-scale datasets, especially involving complex and high-dimensional data like images, is particularly challenging.","While self-supervised learning (SSL) has proven effective for learning representations from unlabelled data, it typically focuses on flat, non-hierarchical structures, missing the multi-level relationships present in many real-world datasets.","Hierarchical clustering (HC) can uncover these relationships by organizing data into a tree-like structure, but it often relies on rigid similarity metrics that struggle to capture the complexity of diverse data types.","To address these we envision $\\texttt{InfoHier}$, a framework that combines SSL with HC to jointly learn robust latent representations and hierarchical structures.","This approach leverages SSL to provide adaptive representations, enhancing HC's ability to capture complex patterns.","Simultaneously, it integrates HC loss to refine SSL training, resulting in representations that are more attuned to the underlying information hierarchy.","$\\texttt{InfoHier}$ has the potential to improve the expressiveness and performance of both clustering and representation learning, offering significant benefits for data analysis, management, and information retrieval."],"url":"http://arxiv.org/abs/2501.08717v1"}
{"created":"2025-01-15 10:57:55","title":"The Inherent Limits of Pretrained LLMs: The Unexpected Convergence of Instruction Tuning and In-Context Learning Capabilities","abstract":"Large Language Models (LLMs), trained on extensive web-scale corpora, have demonstrated remarkable abilities across diverse tasks, especially as they are scaled up. Nevertheless, even state-of-the-art models struggle in certain cases, sometimes failing at problems solvable by young children, indicating that traditional notions of task complexity are insufficient for explaining LLM capabilities. However, exploring LLM capabilities is complicated by the fact that most widely-used models are also \"instruction-tuned\" to respond appropriately to prompts. With the goal of disentangling the factors influencing LLM performance, we investigate whether instruction-tuned models possess fundamentally different capabilities from base models that are prompted using in-context examples. Through extensive experiments across various model families, scales and task types, which included instruction tuning 90 different LLMs, we demonstrate that the performance of instruction-tuned models is significantly correlated with the in-context performance of their base counterparts. By clarifying what instruction-tuning contributes, we extend prior research into in-context learning, which suggests that base models use priors from pretraining data to solve tasks. Specifically, we extend this understanding to instruction-tuned models, suggesting that their pretraining data similarly sets a limiting boundary on the tasks they can solve, with the added influence of the instruction-tuning dataset.","sentences":["Large Language Models (LLMs), trained on extensive web-scale corpora, have demonstrated remarkable abilities across diverse tasks, especially as they are scaled up.","Nevertheless, even state-of-the-art models struggle in certain cases, sometimes failing at problems solvable by young children, indicating that traditional notions of task complexity are insufficient for explaining LLM capabilities.","However, exploring LLM capabilities is complicated by the fact that most widely-used models are also \"instruction-tuned\" to respond appropriately to prompts.","With the goal of disentangling the factors influencing LLM performance, we investigate whether instruction-tuned models possess fundamentally different capabilities from base models that are prompted using in-context examples.","Through extensive experiments across various model families, scales and task types, which included instruction tuning 90 different LLMs, we demonstrate that the performance of instruction-tuned models is significantly correlated with the in-context performance of their base counterparts.","By clarifying what instruction-tuning contributes, we extend prior research into in-context learning, which suggests that base models use priors from pretraining data to solve tasks.","Specifically, we extend this understanding to instruction-tuned models, suggesting that their pretraining data similarly sets a limiting boundary on the tasks they can solve, with the added influence of the instruction-tuning dataset."],"url":"http://arxiv.org/abs/2501.08716v1"}
{"created":"2025-01-15 10:50:54","title":"Disentangled Interleaving Variational Encoding","abstract":"Conflicting objectives present a considerable challenge in interleaving multi-task learning, necessitating the need for meticulous design and balance to ensure effective learning of a representative latent data space across all tasks without mutual negative impact. Drawing inspiration from the concept of marginal and conditional probability distributions in probability theory, we design a principled and well-founded approach to disentangle the original input into marginal and conditional probability distributions in the latent space of a variational autoencoder. Our proposed model, Deep Disentangled Interleaving Variational Encoding (DeepDIVE) learns disentangled features from the original input to form clusters in the embedding space and unifies these features via the cross-attention mechanism in the fusion stage. We theoretically prove that combining the objectives for reconstruction and forecasting fully captures the lower bound and mathematically derive a loss function for disentanglement using Na\\\"ive Bayes. Under the assumption that the prior is a mixture of log-concave distributions, we also establish that the Kullback-Leibler divergence between the prior and the posterior is upper bounded by a function minimized by the minimizer of the cross entropy loss, informing our adoption of radial basis functions (RBF) and cross entropy with interleaving training for DeepDIVE to provide a justified basis for convergence. Experiments on two public datasets show that DeepDIVE disentangles the original input and yields forecast accuracies better than the original VAE and comparable to existing state-of-the-art baselines.","sentences":["Conflicting objectives present a considerable challenge in interleaving multi-task learning, necessitating the need for meticulous design and balance to ensure effective learning of a representative latent data space across all tasks without mutual negative impact.","Drawing inspiration from the concept of marginal and conditional probability distributions in probability theory, we design a principled and well-founded approach to disentangle the original input into marginal and conditional probability distributions in the latent space of a variational autoencoder.","Our proposed model, Deep Disentangled Interleaving Variational Encoding (DeepDIVE) learns disentangled features from the original input to form clusters in the embedding space and unifies these features via the cross-attention mechanism in the fusion stage.","We theoretically prove that combining the objectives for reconstruction and forecasting fully captures the lower bound and mathematically derive a loss function for disentanglement using Na\\\"ive Bayes.","Under the assumption that the prior is a mixture of log-concave distributions, we also establish that the Kullback-Leibler divergence between the prior and the posterior is upper bounded by a function minimized by the minimizer of the cross entropy loss, informing our adoption of radial basis functions (RBF) and cross entropy with interleaving training for DeepDIVE to provide a justified basis for convergence.","Experiments on two public datasets show that DeepDIVE disentangles the original input and yields forecast accuracies better than the original VAE and comparable to existing state-of-the-art baselines."],"url":"http://arxiv.org/abs/2501.08710v1"}
{"created":"2025-01-15 10:09:38","title":"Deep Learning-Based Feature Fusion for Emotion Analysis and Suicide Risk Differentiation in Chinese Psychological Support Hotlines","abstract":"Mental health is a critical global public health issue, and psychological support hotlines play a pivotal role in providing mental health assistance and identifying suicide risks at an early stage. However, the emotional expressions conveyed during these calls remain underexplored in current research. This study introduces a method that combines pitch acoustic features with deep learning-based features to analyze and understand emotions expressed during hotline interactions. Using data from China's largest psychological support hotline, our method achieved an F1-score of 79.13% for negative binary emotion classification.Additionally, the proposed approach was validated on an open dataset for multi-class emotion classification,where it demonstrated better performance compared to the state-of-the-art methods. To explore its clinical relevance, we applied the model to analysis the frequency of negative emotions and the rate of emotional change in the conversation, comparing 46 subjects with suicidal behavior to those without. While the suicidal group exhibited more frequent emotional changes than the non-suicidal group, the difference was not statistically significant.Importantly, our findings suggest that emotional fluctuation intensity and frequency could serve as novel features for psychological assessment scales and suicide risk prediction.The proposed method provides valuable insights into emotional dynamics and has the potential to advance early intervention and improve suicide prevention strategies through integration with clinical tools and assessments The source code is publicly available at https://github.com/Sco-field/Speechemotionrecognition/tree/main.","sentences":["Mental health is a critical global public health issue, and psychological support hotlines play a pivotal role in providing mental health assistance and identifying suicide risks at an early stage.","However, the emotional expressions conveyed during these calls remain underexplored in current research.","This study introduces a method that combines pitch acoustic features with deep learning-based features to analyze and understand emotions expressed during hotline interactions.","Using data from China's largest psychological support hotline, our method achieved an F1-score of 79.13% for negative binary emotion classification.","Additionally, the proposed approach was validated on an open dataset for multi-class emotion classification,where it demonstrated better performance compared to the state-of-the-art methods.","To explore its clinical relevance, we applied the model to analysis the frequency of negative emotions and the rate of emotional change in the conversation, comparing 46 subjects with suicidal behavior to those without.","While the suicidal group exhibited more frequent emotional changes than the non-suicidal group, the difference was not statistically significant.","Importantly, our findings suggest that emotional fluctuation intensity and frequency could serve as novel features for psychological assessment scales and suicide risk prediction.","The proposed method provides valuable insights into emotional dynamics and has the potential to advance early intervention and improve suicide prevention strategies through integration with clinical tools and assessments The source code is publicly available at https://github.com/Sco-field/Speechemotionrecognition/tree/main."],"url":"http://arxiv.org/abs/2501.08696v1"}
{"created":"2025-01-15 09:50:19","title":"Adaptive Data Augmentation with NaturalSpeech3 for Far-field Speaker Verification","abstract":"The scarcity of speaker-annotated far-field speech presents a significant challenge in developing high-performance far-field speaker verification (SV) systems. While data augmentation using large-scale near-field speech has been a common strategy to address this limitation, the mismatch in acoustic environments between near-field and far-field speech significantly hinders the improvement of far-field SV effectiveness. In this paper, we propose an adaptive speech augmentation approach leveraging NaturalSpeech3, a pre-trained foundation text-to-speech (TTS) model, to convert near-field speech into far-field speech by incorporating far-field acoustic ambient noise for data augmentation. Specifically, we utilize FACodec from NaturalSpeech3 to decompose the speech waveform into distinct embedding subspaces-content, prosody, speaker, and residual (acoustic details) embeddings-and reconstruct the speech waveform from these disentangled representations. In our method, the prosody, content, and residual embeddings of far-field speech are combined with speaker embeddings from near-field speech to generate augmented pseudo far-field speech that maintains the speaker identity from the out-domain near-field speech while preserving the acoustic environment of the in-domain far-field speech. This approach not only serves as an effective strategy for augmenting training data for far-field speaker verification but also extends to cross-data augmentation for enrollment and test speech in evaluation trials.Experimental results on FFSVC demonstrate that the adaptive data augmentation method significantly outperforms traditional approaches, such as random noise addition and reverberation, as well as other competitive data augmentation strategies.","sentences":["The scarcity of speaker-annotated far-field speech presents a significant challenge in developing high-performance far-field speaker verification (SV) systems.","While data augmentation using large-scale near-field speech has been a common strategy to address this limitation, the mismatch in acoustic environments between near-field and far-field speech significantly hinders the improvement of far-field SV effectiveness.","In this paper, we propose an adaptive speech augmentation approach leveraging NaturalSpeech3, a pre-trained foundation text-to-speech (TTS) model, to convert near-field speech into far-field speech by incorporating far-field acoustic ambient noise for data augmentation.","Specifically, we utilize FACodec from NaturalSpeech3 to decompose the speech waveform into distinct embedding subspaces-content, prosody, speaker, and residual (acoustic details) embeddings-and reconstruct the speech waveform from these disentangled representations.","In our method, the prosody, content, and residual embeddings of far-field speech are combined with speaker embeddings from near-field speech to generate augmented pseudo far-field speech that maintains the speaker identity from the out-domain near-field speech while preserving the acoustic environment of the in-domain far-field speech.","This approach not only serves as an effective strategy for augmenting training data for far-field speaker verification but also extends to cross-data augmentation for enrollment and test speech in evaluation trials.","Experimental results on FFSVC demonstrate that the adaptive data augmentation method significantly outperforms traditional approaches, such as random noise addition and reverberation, as well as other competitive data augmentation strategies."],"url":"http://arxiv.org/abs/2501.08691v1"}
{"created":"2025-01-15 09:08:05","title":"Investigating Parameter-Efficiency of Hybrid QuGANs Based on Geometric Properties of Generated Sea Route Graphs","abstract":"The demand for artificially generated data for the development, training and testing of new algorithms is omnipresent. Quantum computing (QC), does offer the hope that its inherent probabilistic functionality can be utilised in this field of generative artificial intelligence. In this study, we use quantum-classical hybrid generative adversarial networks (QuGANs) to artificially generate graphs of shipping routes. We create a training dataset based on real shipping data and investigate to what extent QuGANs are able to learn and reproduce inherent distributions and geometric features of this data. We compare hybrid QuGANs with classical Generative Adversarial Networks (GANs), with a special focus on their parameter efficiency. Our results indicate that QuGANs are indeed able to quickly learn and represent underlying geometric properties and distributions, although they seem to have difficulties in introducing variance into the sampled data. Compared to classical GANs of greater size, measured in the number of parameters used, some QuGANs show similar result quality. Our reference to concrete use cases, such as the generation of shipping data, provides an illustrative example and demonstrate the potential and diversity in which QC can be used.","sentences":["The demand for artificially generated data for the development, training and testing of new algorithms is omnipresent.","Quantum computing (QC), does offer the hope that its inherent probabilistic functionality can be utilised in this field of generative artificial intelligence.","In this study, we use quantum-classical hybrid generative adversarial networks (QuGANs) to artificially generate graphs of shipping routes.","We create a training dataset based on real shipping data and investigate to what extent QuGANs are able to learn and reproduce inherent distributions and geometric features of this data.","We compare hybrid QuGANs with classical Generative Adversarial Networks (GANs), with a special focus on their parameter efficiency.","Our results indicate that QuGANs are indeed able to quickly learn and represent underlying geometric properties and distributions, although they seem to have difficulties in introducing variance into the sampled data.","Compared to classical GANs of greater size, measured in the number of parameters used, some QuGANs show similar result quality.","Our reference to concrete use cases, such as the generation of shipping data, provides an illustrative example and demonstrate the potential and diversity in which QC can be used."],"url":"http://arxiv.org/abs/2501.08678v1"}
{"created":"2025-01-15 09:04:56","title":"GS-LIVO: Real-Time LiDAR, Inertial, and Visual Multi-sensor Fused Odometry with Gaussian Mapping","abstract":"In recent years, 3D Gaussian splatting (3D-GS) has emerged as a novel scene representation approach. However, existing vision-only 3D-GS methods often rely on hand-crafted heuristics for point-cloud densification and face challenges in handling occlusions and high GPU memory and computation consumption. LiDAR-Inertial-Visual (LIV) sensor configuration has demonstrated superior performance in localization and dense mapping by leveraging complementary sensing characteristics: rich texture information from cameras, precise geometric measurements from LiDAR, and high-frequency motion data from IMU. Inspired by this, we propose a novel real-time Gaussian-based simultaneous localization and mapping (SLAM) system. Our map system comprises a global Gaussian map and a sliding window of Gaussians, along with an IESKF-based odometry. The global Gaussian map consists of hash-indexed voxels organized in a recursive octree, effectively covering sparse spatial volumes while adapting to different levels of detail and scales. The Gaussian map is initialized through multi-sensor fusion and optimized with photometric gradients. Our system incrementally maintains a sliding window of Gaussians, significantly reducing GPU computation and memory consumption by only optimizing the map within the sliding window. Moreover, we implement a tightly coupled multi-sensor fusion odometry with an iterative error state Kalman filter (IESKF), leveraging real-time updating and rendering of the Gaussian map. Our system represents the first real-time Gaussian-based SLAM framework deployable on resource-constrained embedded systems, demonstrated on the NVIDIA Jetson Orin NX platform. The framework achieves real-time performance while maintaining robust multi-sensor fusion capabilities. All implementation algorithms, hardware designs, and CAD models will be publicly available.","sentences":["In recent years, 3D Gaussian splatting (3D-GS) has emerged as a novel scene representation approach.","However, existing vision-only 3D-GS methods often rely on hand-crafted heuristics for point-cloud densification and face challenges in handling occlusions and high GPU memory and computation consumption.","LiDAR-Inertial-Visual (LIV) sensor configuration has demonstrated superior performance in localization and dense mapping by leveraging complementary sensing characteristics: rich texture information from cameras, precise geometric measurements from LiDAR, and high-frequency motion data from IMU.","Inspired by this, we propose a novel real-time Gaussian-based simultaneous localization and mapping (SLAM) system.","Our map system comprises a global Gaussian map and a sliding window of Gaussians, along with an IESKF-based odometry.","The global Gaussian map consists of hash-indexed voxels organized in a recursive octree, effectively covering sparse spatial volumes while adapting to different levels of detail and scales.","The Gaussian map is initialized through multi-sensor fusion and optimized with photometric gradients.","Our system incrementally maintains a sliding window of Gaussians, significantly reducing GPU computation and memory consumption by only optimizing the map within the sliding window.","Moreover, we implement a tightly coupled multi-sensor fusion odometry with an iterative error state Kalman filter (IESKF), leveraging real-time updating and rendering of the Gaussian map.","Our system represents the first real-time Gaussian-based SLAM framework deployable on resource-constrained embedded systems, demonstrated on the NVIDIA Jetson Orin NX platform.","The framework achieves real-time performance while maintaining robust multi-sensor fusion capabilities.","All implementation algorithms, hardware designs, and CAD models will be publicly available."],"url":"http://arxiv.org/abs/2501.08672v1"}
{"created":"2025-01-15 09:04:30","title":"Augmenting Smart Contract Decompiler Output through Fine-grained Dependency Analysis and LLM-facilitated Semantic Recovery","abstract":"Decompiler is a specialized type of reverse engineering tool extensively employed in program analysis tasks, particularly in program comprehension and vulnerability detection. However, current Solidity smart contract decompilers face significant limitations in reconstructing the original source code. In particular, the bottleneck of SOTA decompilers lies in inaccurate method identification, incorrect variable type recovery, and missing contract attributes. These deficiencies hinder downstream tasks and understanding of the program logic. To address these challenges, we propose SmartHalo, a new framework that enhances decompiler output by combining static analysis (SA) and large language models (LLM). SmartHalo leverages the complementary strengths of SA's accuracy in control and data flow analysis and LLM's capability in semantic prediction. More specifically, \\system{} constructs a new data structure - Dependency Graph (DG), to extract semantic dependencies via static analysis. Then, it takes DG to create prompts for LLM optimization. Finally, the correctness of LLM outputs is validated through symbolic execution and formal verification. Evaluation on a dataset consisting of 465 randomly selected smart contract methods shows that SmartHalo significantly improves the quality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse). Notably, integrating GPT-4o with SmartHalo further enhances its performance, achieving precision rates of 87.39% for method boundaries, 90.39% for variable types, and 80.65% for contract attributes.","sentences":["Decompiler is a specialized type of reverse engineering tool extensively employed in program analysis tasks, particularly in program comprehension and vulnerability detection.","However, current Solidity smart contract decompilers face significant limitations in reconstructing the original source code.","In particular, the bottleneck of SOTA decompilers lies in inaccurate method identification, incorrect variable type recovery, and missing contract attributes.","These deficiencies hinder downstream tasks and understanding of the program logic.","To address these challenges, we propose SmartHalo, a new framework that enhances decompiler output by combining static analysis (SA) and large language models (LLM).","SmartHalo leverages the complementary strengths of SA's accuracy in control and data flow analysis and LLM's capability in semantic prediction.","More specifically, \\system{} constructs a new data structure - Dependency Graph (DG), to extract semantic dependencies via static analysis.","Then, it takes DG to create prompts for LLM optimization.","Finally, the correctness of LLM outputs is validated through symbolic execution and formal verification.","Evaluation on a dataset consisting of 465 randomly selected smart contract methods shows that SmartHalo significantly improves the quality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse).","Notably, integrating GPT-4o with SmartHalo further enhances its performance, achieving precision rates of 87.39% for method boundaries, 90.39% for variable types, and 80.65% for contract attributes."],"url":"http://arxiv.org/abs/2501.08670v1"}
{"created":"2025-01-15 09:04:19","title":"SPEQ: Stabilization Phases for Efficient Q-Learning in High Update-To-Data Ratio Reinforcement Learning","abstract":"A key challenge in Deep Reinforcement Learning is sample efficiency, especially in real-world applications where collecting environment interactions is expensive or risky. Recent off-policy algorithms improve sample efficiency by increasing the Update-To-Data (UTD) ratio and performing more gradient updates per environment interaction. While this improves sample efficiency, it significantly increases computational cost due to the higher number of gradient updates required. In this paper we propose a sample-efficient method to improve computational efficiency by separating training into distinct learning phases in order to exploit gradient updates more effectively. Our approach builds on top of the Dropout Q-Functions (DroQ) algorithm and alternates between an online, low UTD ratio training phase, and an offline stabilization phase. During the stabilization phase, we fine-tune the Q-functions without collecting new environment interactions. This process improves the effectiveness of the replay buffer and reduces computational overhead. Our experimental results on continuous control problems show that our method achieves results comparable to state-of-the-art, high UTD ratio algorithms while requiring 56\\% fewer gradient updates and 50\\% less training time than DroQ. Our approach offers an effective and computationally economical solution while maintaining the same sample efficiency as the more costly, high UTD ratio state-of-the-art.","sentences":["A key challenge in Deep Reinforcement Learning is sample efficiency, especially in real-world applications where collecting environment interactions is expensive or risky.","Recent off-policy algorithms improve sample efficiency by increasing the Update-To-Data (UTD) ratio and performing more gradient updates per environment interaction.","While this improves sample efficiency, it significantly increases computational cost due to the higher number of gradient updates required.","In this paper we propose a sample-efficient method to improve computational efficiency by separating training into distinct learning phases in order to exploit gradient updates more effectively.","Our approach builds on top of the Dropout Q-Functions (DroQ) algorithm and alternates between an online, low UTD ratio training phase, and an offline stabilization phase.","During the stabilization phase, we fine-tune the Q-functions without collecting new environment interactions.","This process improves the effectiveness of the replay buffer and reduces computational overhead.","Our experimental results on continuous control problems show that our method achieves results comparable to state-of-the-art, high UTD ratio algorithms while requiring 56\\% fewer gradient updates and 50\\% less training time than DroQ. Our approach offers an effective and computationally economical solution while maintaining the same sample efficiency as the more costly, high UTD ratio state-of-the-art."],"url":"http://arxiv.org/abs/2501.08669v1"}
{"created":"2025-01-15 09:00:32","title":"A Survey on Facial Image Privacy Preservation in Cloud-Based Services","abstract":"Facial recognition models are increasingly employed by commercial enterprises, government agencies, and cloud service providers for identity verification, consumer services, and surveillance. These models are often trained using vast amounts of facial data processed and stored in cloud-based platforms, raising significant privacy concerns. Users' facial images may be exploited without their consent, leading to potential data breaches and misuse. This survey presents a comprehensive review of current methods aimed at preserving facial image privacy in cloud-based services. We categorize these methods into two primary approaches: image obfuscation-based protection and adversarial perturbation-based protection. We provide an in-depth analysis of both categories, offering qualitative and quantitative comparisons of their effectiveness. Additionally, we highlight unresolved challenges and propose future research directions to improve privacy preservation in cloud computing environments.","sentences":["Facial recognition models are increasingly employed by commercial enterprises, government agencies, and cloud service providers for identity verification, consumer services, and surveillance.","These models are often trained using vast amounts of facial data processed and stored in cloud-based platforms, raising significant privacy concerns.","Users' facial images may be exploited without their consent, leading to potential data breaches and misuse.","This survey presents a comprehensive review of current methods aimed at preserving facial image privacy in cloud-based services.","We categorize these methods into two primary approaches: image obfuscation-based protection and adversarial perturbation-based protection.","We provide an in-depth analysis of both categories, offering qualitative and quantitative comparisons of their effectiveness.","Additionally, we highlight unresolved challenges and propose future research directions to improve privacy preservation in cloud computing environments."],"url":"http://arxiv.org/abs/2501.08665v1"}
{"created":"2025-01-15 08:59:52","title":"Efficient Shape Reconfiguration by Hybrid Programmable Matter","abstract":"Shape formation is one of the most thoroughly studied problems in most algorithmic models of programmable matter. However, few existing shape formation algorithms utilize similarities between an initial configuration and a desired target shape. In the hybrid model, an active agent with the computational capabilities of a deterministic finite automaton can form shapes by lifting and placing passive tiles on the triangular lattice. We study the shape reconfiguration problem where the agent needs to move all tiles in an input shape to so-called target nodes, which are distinguishable from other nodes by the agent. We present a worst-case optimal $O(mn)$ algorithm for simply connected target shapes and an $O(n^4)$ algorithm for a large class of target shapes that may contain holes, where $m$ is the initial number of unoccupied target nodes and $n$ is the total number of tiles.","sentences":["Shape formation is one of the most thoroughly studied problems in most algorithmic models of programmable matter.","However, few existing shape formation algorithms utilize similarities between an initial configuration and a desired target shape.","In the hybrid model, an active agent with the computational capabilities of a deterministic finite automaton can form shapes by lifting and placing passive tiles on the triangular lattice.","We study the shape reconfiguration problem where the agent needs to move all tiles in an input shape to so-called target nodes, which are distinguishable from other nodes by the agent.","We present a worst-case optimal $O(mn)$ algorithm for simply connected target shapes and an $O(n^4)$ algorithm for a large class of target shapes that may contain holes, where $m$ is the initial number of unoccupied target nodes and $n$ is the total number of tiles."],"url":"http://arxiv.org/abs/2501.08663v1"}
{"created":"2025-01-15 08:50:52","title":"BRIGHT-VO: Brightness-Guided Hybrid Transformer for Visual Odometry with Multi-modality Refinement Module","abstract":"Visual odometry (VO) plays a crucial role in autonomous driving, robotic navigation, and other related tasks by estimating the position and orientation of a camera based on visual input. Significant progress has been made in data-driven VO methods, particularly those leveraging deep learning techniques to extract image features and estimate camera poses. However, these methods often struggle in low-light conditions because of the reduced visibility of features and the increased difficulty of matching keypoints. To address this limitation, we introduce BrightVO, a novel VO model based on Transformer architecture, which not only performs front-end visual feature extraction, but also incorporates a multi-modality refinement module in the back-end that integrates Inertial Measurement Unit (IMU) data. Using pose graph optimization, this module iteratively refines pose estimates to reduce errors and improve both accuracy and robustness. Furthermore, we create a synthetic low-light dataset, KiC4R, which includes a variety of lighting conditions to facilitate the training and evaluation of VO frameworks in challenging environments. Experimental results demonstrate that BrightVO achieves state-of-the-art performance on both the KiC4R dataset and the KITTI benchmarks. Specifically, it provides an average improvement of 20% in pose estimation accuracy in normal outdoor environments and 259% in low-light conditions, outperforming existing methods. For widespread use and further development, the research work is fully open-source at https://github.com/Anastasiawd/BrightVO.","sentences":["Visual odometry (VO) plays a crucial role in autonomous driving, robotic navigation, and other related tasks by estimating the position and orientation of a camera based on visual input.","Significant progress has been made in data-driven VO methods, particularly those leveraging deep learning techniques to extract image features and estimate camera poses.","However, these methods often struggle in low-light conditions because of the reduced visibility of features and the increased difficulty of matching keypoints.","To address this limitation, we introduce BrightVO, a novel VO model based on Transformer architecture, which not only performs front-end visual feature extraction, but also incorporates a multi-modality refinement module in the back-end that integrates Inertial Measurement Unit (IMU) data.","Using pose graph optimization, this module iteratively refines pose estimates to reduce errors and improve both accuracy and robustness.","Furthermore, we create a synthetic low-light dataset, KiC4R, which includes a variety of lighting conditions to facilitate the training and evaluation of VO frameworks in challenging environments.","Experimental results demonstrate that BrightVO achieves state-of-the-art performance on both the KiC4R dataset and the KITTI benchmarks.","Specifically, it provides an average improvement of 20% in pose estimation accuracy in normal outdoor environments and 259% in low-light conditions, outperforming existing methods.","For widespread use and further development, the research work is fully open-source at https://github.com/Anastasiawd/BrightVO."],"url":"http://arxiv.org/abs/2501.08659v1"}
{"created":"2025-01-15 08:43:48","title":"StereoGen: High-quality Stereo Image Generation from a Single Image","abstract":"State-of-the-art supervised stereo matching methods have achieved amazing results on various benchmarks. However, these data-driven methods suffer from generalization to real-world scenarios due to the lack of real-world annotated data. In this paper, we propose StereoGen, a novel pipeline for high-quality stereo image generation. This pipeline utilizes arbitrary single images as left images and pseudo disparities generated by a monocular depth estimation model to synthesize high-quality corresponding right images. Unlike previous methods that fill the occluded area in warped right images using random backgrounds or using convolutions to take nearby pixels selectively, we fine-tune a diffusion inpainting model to recover the background. Images generated by our model possess better details and undamaged semantic structures. Besides, we propose Training-free Confidence Generation and Adaptive Disparity Selection. The former suppresses the negative effect of harmful pseudo ground truth during stereo training, while the latter helps generate a wider disparity distribution and better synthetic images. Experiments show that models trained under our pipeline achieve state-of-the-art zero-shot generalization results among all published methods. The code will be available upon publication of the paper.","sentences":["State-of-the-art supervised stereo matching methods have achieved amazing results on various benchmarks.","However, these data-driven methods suffer from generalization to real-world scenarios due to the lack of real-world annotated data.","In this paper, we propose StereoGen, a novel pipeline for high-quality stereo image generation.","This pipeline utilizes arbitrary single images as left images and pseudo disparities generated by a monocular depth estimation model to synthesize high-quality corresponding right images.","Unlike previous methods that fill the occluded area in warped right images using random backgrounds or using convolutions to take nearby pixels selectively, we fine-tune a diffusion inpainting model to recover the background.","Images generated by our model possess better details and undamaged semantic structures.","Besides, we propose Training-free Confidence Generation and Adaptive Disparity Selection.","The former suppresses the negative effect of harmful pseudo ground truth during stereo training, while the latter helps generate a wider disparity distribution and better synthetic images.","Experiments show that models trained under our pipeline achieve state-of-the-art zero-shot generalization results among all published methods.","The code will be available upon publication of the paper."],"url":"http://arxiv.org/abs/2501.08654v1"}
{"created":"2025-01-15 08:38:07","title":"Fine-grained Spatio-temporal Event Prediction with Self-adaptive Anchor Graph","abstract":"Event prediction tasks often handle spatio-temporal data distributed in a large spatial area. Different regions in the area exhibit different characteristics while having latent correlations. This spatial heterogeneity and correlations greatly affect the spatio-temporal distributions of event occurrences, which has not been addressed by state-of-the-art models. Learning spatial dependencies of events in a continuous space is challenging due to its fine granularity and a lack of prior knowledge. In this work, we propose a novel Graph Spatio-Temporal Point Process (GSTPP) model for fine-grained event prediction. It adopts an encoder-decoder architecture that jointly models the state dynamics of spatially localized regions using neural Ordinary Differential Equations (ODEs). The state evolution is built on the foundation of a novel Self-Adaptive Anchor Graph (SAAG) that captures spatial dependencies. By adaptively localizing the anchor nodes in the space and jointly constructing the correlation edges between them, the SAAG enhances the model's ability of learning complex spatial event patterns. The proposed GSTPP model greatly improves the accuracy of fine-grained event prediction. Extensive experimental results show that our method greatly improves the prediction accuracy over existing spatio-temporal event prediction approaches.","sentences":["Event prediction tasks often handle spatio-temporal data distributed in a large spatial area.","Different regions in the area exhibit different characteristics while having latent correlations.","This spatial heterogeneity and correlations greatly affect the spatio-temporal distributions of event occurrences, which has not been addressed by state-of-the-art models.","Learning spatial dependencies of events in a continuous space is challenging due to its fine granularity and a lack of prior knowledge.","In this work, we propose a novel Graph Spatio-Temporal Point Process (GSTPP) model for fine-grained event prediction.","It adopts an encoder-decoder architecture that jointly models the state dynamics of spatially localized regions using neural Ordinary Differential Equations (ODEs).","The state evolution is built on the foundation of a novel Self-Adaptive Anchor Graph (SAAG) that captures spatial dependencies.","By adaptively localizing the anchor nodes in the space and jointly constructing the correlation edges between them, the SAAG enhances the model's ability of learning complex spatial event patterns.","The proposed GSTPP model greatly improves the accuracy of fine-grained event prediction.","Extensive experimental results show that our method greatly improves the prediction accuracy over existing spatio-temporal event prediction approaches."],"url":"http://arxiv.org/abs/2501.08653v1"}
{"created":"2025-01-15 08:19:13","title":"Learnable Sparsification of Die-to-Die Communication via Spike-Based Encoding","abstract":"Efficient communication is central to both biological and artificial intelligence (AI) systems. In biological brains, the challenge of long-range communication across regions is addressed through sparse, spike-based signaling, minimizing energy consumption and latency. In contrast, modern AI workloads, which keep scaling ever larger across distributed compute systems, are increasingly constrained by bandwidth limitations, creating bottlenecks that hinder scalability and energy efficiency. Inspired by the brain's efficient communication strategies, we propose SNAP, a hybrid neural network architecture combining spiking neural networks (SNNs) and artificial neural networks (ANNs) to address these challenges. SNAP integrates SNNs at bandwidth-constrained regions, such as chip boundaries, where spike-based encoding reduces data transfer overhead. Within each chip, dense ANN computations are maintained to preserve high throughput, accuracy, and robustness.   Historically, SNNs have faced difficulties scaling up, with limitations in task-specific performance and reliance on specialized hardware to exploit sparsity. SNAP overcomes these barriers through an algorithm-architecture co-design leveraging learnable sparsity for die-to-die communication while limiting spiking layers to specific network partitions. This composable design integrates spike-based and non-spiking pathways, making it adaptable to diverse deep learning workloads. Our evaluations on language processing and computer vision tasks demonstrate up to 5.3x energy efficiency improvements and 15.2x reductions in inference latency, outperforming both traditional SNNs and non-spiking models. We find that as model resources scale, SNAP's improvement margins grow. By addressing the critical bottleneck of inter-chip communication, SNAP offers a scalable, biologically inspired pathway to more efficient AI systems.","sentences":["Efficient communication is central to both biological and artificial intelligence (AI) systems.","In biological brains, the challenge of long-range communication across regions is addressed through sparse, spike-based signaling, minimizing energy consumption and latency.","In contrast, modern AI workloads, which keep scaling ever larger across distributed compute systems, are increasingly constrained by bandwidth limitations, creating bottlenecks that hinder scalability and energy efficiency.","Inspired by the brain's efficient communication strategies, we propose SNAP, a hybrid neural network architecture combining spiking neural networks (SNNs) and artificial neural networks (ANNs) to address these challenges.","SNAP integrates SNNs at bandwidth-constrained regions, such as chip boundaries, where spike-based encoding reduces data transfer overhead.","Within each chip, dense ANN computations are maintained to preserve high throughput, accuracy, and robustness.   ","Historically, SNNs have faced difficulties scaling up, with limitations in task-specific performance and reliance on specialized hardware to exploit sparsity.","SNAP overcomes these barriers through an algorithm-architecture co-design leveraging learnable sparsity for die-to-die communication while limiting spiking layers to specific network partitions.","This composable design integrates spike-based and non-spiking pathways, making it adaptable to diverse deep learning workloads.","Our evaluations on language processing and computer vision tasks demonstrate up to 5.3x energy efficiency improvements and 15.2x reductions in inference latency, outperforming both traditional SNNs and non-spiking models.","We find that as model resources scale, SNAP's improvement margins grow.","By addressing the critical bottleneck of inter-chip communication, SNAP offers a scalable, biologically inspired pathway to more efficient AI systems."],"url":"http://arxiv.org/abs/2501.08645v1"}
{"created":"2025-01-15 08:12:25","title":"Extension of indoor mmW link radio coverage in non line-of-sight conditions","abstract":"In future wireless communication systems, millimeter waves (mmWaves) will play an important role in meeting high data rates. However, due to their short wavelengths, these mmWaves present high propagation losses and are highly attenuated by blocking. In this chapter, we seek to increase the indoor radio coverage at 60 GHz in non line-of-sight (NLOS) environments. Firstly, a metallic passive reflector is used in an L-shaped corridor. Secondly, an array of grooved metallic antennas of size 20 cm x 20 cm (corresponding to 80 grooves) is used in a T-shaped corridor. Next, the study focuses on the blockage losses caused by the human body. The results obtained in these different configurations show that it is possible to use beamforming to exploit a reflected path when the direct path is blocked.","sentences":["In future wireless communication systems, millimeter waves (mmWaves) will play an important role in meeting high data rates.","However, due to their short wavelengths, these mmWaves present high propagation losses and are highly attenuated by blocking.","In this chapter, we seek to increase the indoor radio coverage at 60 GHz in non line-of-sight (NLOS) environments.","Firstly, a metallic passive reflector is used in an L-shaped corridor.","Secondly, an array of grooved metallic antennas of size 20 cm x 20 cm (corresponding to 80 grooves) is used in a T-shaped corridor.","Next, the study focuses on the blockage losses caused by the human body.","The results obtained in these different configurations show that it is possible to use beamforming to exploit a reflected path when the direct path is blocked."],"url":"http://arxiv.org/abs/2501.08644v1"}
{"created":"2025-01-15 08:04:44","title":"Detecting Wildfire Flame and Smoke through Edge Computing using Transfer Learning Enhanced Deep Learning Models","abstract":"Autonomous unmanned aerial vehicles (UAVs) integrated with edge computing capabilities empower real-time data processing directly on the device, dramatically reducing latency in critical scenarios such as wildfire detection. This study underscores Transfer Learning's (TL) significance in boosting the performance of object detectors for identifying wildfire smoke and flames, especially when trained on limited datasets, and investigates the impact TL has on edge computing metrics. With the latter focusing how TL-enhanced You Only Look Once (YOLO) models perform in terms of inference time, power usage, and energy consumption when using edge computing devices. This study utilizes the Aerial Fire and Smoke Essential (AFSE) dataset as the target, with the Flame and Smoke Detection Dataset (FASDD) and the Microsoft Common Objects in Context (COCO) dataset serving as source datasets. We explore a two-stage cascaded TL method, utilizing D-Fire or FASDD as initial stage target datasets and AFSE as the subsequent stage. Through fine-tuning, TL significantly enhances detection precision, achieving up to 79.2% mean Average Precision (mAP@0.5), reduces training time, and increases model generalizability across the AFSE dataset. However, cascaded TL yielded no notable improvements and TL alone did not benefit the edge computing metrics evaluated. Lastly, this work found that YOLOv5n remains a powerful model when lacking hardware acceleration, finding that YOLOv5n can process images nearly twice as fast as its newer counterpart, YOLO11n. Overall, the results affirm TL's role in augmenting the accuracy of object detectors while also illustrating that additional enhancements are needed to improve edge computing performance.","sentences":["Autonomous unmanned aerial vehicles (UAVs) integrated with edge computing capabilities empower real-time data processing directly on the device, dramatically reducing latency in critical scenarios such as wildfire detection.","This study underscores Transfer Learning's (TL) significance in boosting the performance of object detectors for identifying wildfire smoke and flames, especially when trained on limited datasets, and investigates the impact TL has on edge computing metrics.","With the latter focusing how TL-enhanced You Only Look Once (YOLO) models perform in terms of inference time, power usage, and energy consumption when using edge computing devices.","This study utilizes the Aerial Fire and Smoke Essential (AFSE) dataset as the target, with the Flame and Smoke Detection Dataset (FASDD) and the Microsoft Common Objects in Context (COCO) dataset serving as source datasets.","We explore a two-stage cascaded TL method, utilizing D-Fire or FASDD as initial stage target datasets and AFSE as the subsequent stage.","Through fine-tuning, TL significantly enhances detection precision, achieving up to 79.2% mean Average Precision (mAP@0.5), reduces training time, and increases model generalizability across the AFSE dataset.","However, cascaded TL yielded no notable improvements and TL alone did not benefit the edge computing metrics evaluated.","Lastly, this work found that YOLOv5n remains a powerful model when lacking hardware acceleration, finding that YOLOv5n can process images nearly twice as fast as its newer counterpart, YOLO11n.","Overall, the results affirm TL's role in augmenting the accuracy of object detectors while also illustrating that additional enhancements are needed to improve edge computing performance."],"url":"http://arxiv.org/abs/2501.08639v1"}
{"created":"2025-01-15 07:18:51","title":"Transformer-based Multivariate Time Series Anomaly Localization","abstract":"With the growing complexity of Cyber-Physical Systems (CPS) and the integration of Internet of Things (IoT), the use of sensors for online monitoring generates large volume of multivariate time series (MTS) data. Consequently, the need for robust anomaly diagnosis in MTS is paramount to maintaining system reliability and safety. While significant advancements have been made in anomaly detection, localization remains a largely underexplored area, though crucial for intelligent decision-making. This paper introduces a novel transformer-based model for unsupervised anomaly diagnosis in MTS, with a focus on improving localization performance, through an in-depth analysis of the self-attention mechanism's learning behavior under both normal and anomalous conditions. We formulate the anomaly localization problem as a three-stage process: time-step, window, and segment-based. This leads to the development of the Space-Time Anomaly Score (STAS), a new metric inspired by the connection between transformer latent representations and space-time statistical models. STAS is designed to capture individual anomaly behaviors and inter-series dependencies, delivering enhanced localization performance. Additionally, the Statistical Feature Anomaly Score (SFAS) complements STAS by analyzing statistical features around anomalies, with their combination helping to reduce false alarms. Experiments on real world and synthetic datasets illustrate the model's superiority over state-of-the-art methods in both detection and localization tasks.","sentences":["With the growing complexity of Cyber-Physical Systems (CPS) and the integration of Internet of Things (IoT), the use of sensors for online monitoring generates large volume of multivariate time series (MTS) data.","Consequently, the need for robust anomaly diagnosis in MTS is paramount to maintaining system reliability and safety.","While significant advancements have been made in anomaly detection, localization remains a largely underexplored area, though crucial for intelligent decision-making.","This paper introduces a novel transformer-based model for unsupervised anomaly diagnosis in MTS, with a focus on improving localization performance, through an in-depth analysis of the self-attention mechanism's learning behavior under both normal and anomalous conditions.","We formulate the anomaly localization problem as a three-stage process: time-step, window, and segment-based.","This leads to the development of the Space-Time Anomaly Score (STAS), a new metric inspired by the connection between transformer latent representations and space-time statistical models.","STAS is designed to capture individual anomaly behaviors and inter-series dependencies, delivering enhanced localization performance.","Additionally, the Statistical Feature Anomaly Score (SFAS) complements STAS by analyzing statistical features around anomalies, with their combination helping to reduce false alarms.","Experiments on real world and synthetic datasets illustrate the model's superiority over state-of-the-art methods in both detection and localization tasks."],"url":"http://arxiv.org/abs/2501.08628v1"}
{"created":"2025-01-15 07:07:48","title":"A Learning Algorithm That Attains the Human Optimum in a Repeated Human-Machine Interaction Game","abstract":"When humans interact with learning-based control systems, a common goal is to minimize a cost function known only to the human. For instance, an exoskeleton may adapt its assistance in an effort to minimize the human's metabolic cost-of-transport. Conventional approaches to synthesizing the learning algorithm solve an inverse problem to infer the human's cost. However, these problems can be ill-posed, hard to solve, or sensitive to problem data. Here we show a game-theoretic learning algorithm that works solely by observing human actions to find the cost minimum, avoiding the need to solve an inverse problem. We evaluate the performance of our algorithm in an extensive set of human subjects experiments, demonstrating consistent convergence to the minimum of a prescribed human cost function in scalar and multidimensional instantiations of the game. We conclude by outlining future directions for theoretical and empirical extensions of our results.","sentences":["When humans interact with learning-based control systems, a common goal is to minimize a cost function known only to the human.","For instance, an exoskeleton may adapt its assistance in an effort to minimize the human's metabolic cost-of-transport.","Conventional approaches to synthesizing the learning algorithm solve an inverse problem to infer the human's cost.","However, these problems can be ill-posed, hard to solve, or sensitive to problem data.","Here we show a game-theoretic learning algorithm that works solely by observing human actions to find the cost minimum, avoiding the need to solve an inverse problem.","We evaluate the performance of our algorithm in an extensive set of human subjects experiments, demonstrating consistent convergence to the minimum of a prescribed human cost function in scalar and multidimensional instantiations of the game.","We conclude by outlining future directions for theoretical and empirical extensions of our results."],"url":"http://arxiv.org/abs/2501.08626v1"}
{"created":"2025-01-15 06:35:39","title":"CT-PatchTST: Channel-Time Patch Time-Series Transformer for Long-Term Renewable Energy Forecasting","abstract":"Accurately predicting renewable energy output is crucial for the efficient integration of solar and wind power into modern energy systems. This study develops and evaluates an advanced deep learning model, Channel-Time Patch Time-Series Transformer (CT-PatchTST), to forecast the power output of photovoltaic and wind energy systems using annual offshore wind power, onshore wind power, and solar power generation data from Denmark. While the original Patch Time-Series Transformer(PatchTST) model employs a channel-independent (CI) approach, it tends to overlook inter-channel relationships during training, potentially leading to a loss of critical information. To address this limitation and further leverage the benefits of increased data granularity brought by CI, we propose CT-PatchTST. This enhanced model improves the processing of inter-channel information while maintaining the advantages of the channel-independent approach. The predictive performance of CT-PatchTST is rigorously analyzed, demonstrating its ability to provide precise and reliable energy forecasts. This work contributes to improving the predictability of renewable energy systems, supporting their broader adoption and integration into energy grids.","sentences":["Accurately predicting renewable energy output is crucial for the efficient integration of solar and wind power into modern energy systems.","This study develops and evaluates an advanced deep learning model, Channel-Time Patch Time-Series Transformer (CT-PatchTST), to forecast the power output of photovoltaic and wind energy systems using annual offshore wind power, onshore wind power, and solar power generation data from Denmark.","While the original Patch Time-Series Transformer(PatchTST) model employs a channel-independent (CI) approach, it tends to overlook inter-channel relationships during training, potentially leading to a loss of critical information.","To address this limitation and further leverage the benefits of increased data granularity brought by CI, we propose CT-PatchTST.","This enhanced model improves the processing of inter-channel information while maintaining the advantages of the channel-independent approach.","The predictive performance of CT-PatchTST is rigorously analyzed, demonstrating its ability to provide precise and reliable energy forecasts.","This work contributes to improving the predictability of renewable energy systems, supporting their broader adoption and integration into energy grids."],"url":"http://arxiv.org/abs/2501.08620v1"}
{"created":"2025-01-15 06:30:26","title":"Towards Aligned Data Forgetting via Twin Machine Unlearning","abstract":"Modern privacy regulations have spurred the evolution of machine unlearning, a technique enabling a trained model to efficiently forget specific training data. In prior unlearning methods, the concept of \"data forgetting\" is often interpreted and implemented as achieving zero classification accuracy on such data. Nevertheless, the authentic aim of machine unlearning is to achieve alignment between the unlearned model and the gold model, i.e., encouraging them to have identical classification accuracy. On the other hand, the gold model often exhibits non-zero classification accuracy due to its generalization ability. To achieve aligned data forgetting, we propose a Twin Machine Unlearning (TMU) approach, where a twin unlearning problem is defined corresponding to the original unlearning problem. Consequently, the generalization-label predictor trained on the twin problem can be transferred to the original problem, facilitating aligned data forgetting. Comprehensive empirical experiments illustrate that our approach significantly enhances the alignment between the unlearned model and the gold model.","sentences":["Modern privacy regulations have spurred the evolution of machine unlearning, a technique enabling a trained model to efficiently forget specific training data.","In prior unlearning methods, the concept of \"data forgetting\" is often interpreted and implemented as achieving zero classification accuracy on such data.","Nevertheless, the authentic aim of machine unlearning is to achieve alignment between the unlearned model and the gold model, i.e., encouraging them to have identical classification accuracy.","On the other hand, the gold model often exhibits non-zero classification accuracy due to its generalization ability.","To achieve aligned data forgetting, we propose a Twin Machine Unlearning (TMU) approach, where a twin unlearning problem is defined corresponding to the original unlearning problem.","Consequently, the generalization-label predictor trained on the twin problem can be transferred to the original problem, facilitating aligned data forgetting.","Comprehensive empirical experiments illustrate that our approach significantly enhances the alignment between the unlearned model and the gold model."],"url":"http://arxiv.org/abs/2501.08615v1"}
{"created":"2025-01-15 06:17:06","title":"Multi-view Correlation-aware Network Traffic Detection on Flow Hypergraph","abstract":"As the Internet rapidly expands, the increasing complexity and diversity of network activities pose significant challenges to effective network governance and security regulation. Network traffic, which serves as a crucial data carrier of network activities, has become indispensable in this process. Network traffic detection aims to monitor, analyze, and evaluate the data flows transmitted across the network to ensure network security and optimize performance. However, existing network traffic detection methods generally suffer from several limitations: 1) a narrow focus on characterizing traffic features from a single perspective; 2) insufficient exploration of discriminative features for different traffic; 3) poor generalization to different traffic scenarios. To address these issues, we propose a multi-view correlation-aware framework named FlowID for network traffic detection. FlowID captures multi-view traffic features via temporal and interaction awareness, while a hypergraph encoder further explores higher-order relationships between flows. To overcome the challenges of data imbalance and label scarcity, we design a dual-contrastive proxy task, enhancing the framework's ability to differentiate between various traffic flows through traffic-to-traffic and group-to-group contrast. Extensive experiments on five real-world datasets demonstrate that FlowID significantly outperforms existing methods in accuracy, robustness, and generalization across diverse network scenarios, particularly in detecting malicious traffic.","sentences":["As the Internet rapidly expands, the increasing complexity and diversity of network activities pose significant challenges to effective network governance and security regulation.","Network traffic, which serves as a crucial data carrier of network activities, has become indispensable in this process.","Network traffic detection aims to monitor, analyze, and evaluate the data flows transmitted across the network to ensure network security and optimize performance.","However, existing network traffic detection methods generally suffer from several limitations: 1) a narrow focus on characterizing traffic features from a single perspective; 2) insufficient exploration of discriminative features for different traffic; 3) poor generalization to different traffic scenarios.","To address these issues, we propose a multi-view correlation-aware framework named FlowID for network traffic detection.","FlowID captures multi-view traffic features via temporal and interaction awareness, while a hypergraph encoder further explores higher-order relationships between flows.","To overcome the challenges of data imbalance and label scarcity, we design a dual-contrastive proxy task, enhancing the framework's ability to differentiate between various traffic flows through traffic-to-traffic and group-to-group contrast.","Extensive experiments on five real-world datasets demonstrate that FlowID significantly outperforms existing methods in accuracy, robustness, and generalization across diverse network scenarios, particularly in detecting malicious traffic."],"url":"http://arxiv.org/abs/2501.08610v1"}
{"created":"2025-01-15 06:15:15","title":"Computerized Assessment of Motor Imitation for Distinguishing Autism in Video (CAMI-2DNet)","abstract":"Motor imitation impairments are commonly reported in individuals with autism spectrum conditions (ASCs), suggesting that motor imitation could be used as a phenotype for addressing autism heterogeneity. Traditional methods for assessing motor imitation are subjective, labor-intensive, and require extensive human training. Modern Computerized Assessment of Motor Imitation (CAMI) methods, such as CAMI-3D for motion capture data and CAMI-2D for video data, are less subjective. However, they rely on labor-intensive data normalization and cleaning techniques, and human annotations for algorithm training. To address these challenges, we propose CAMI-2DNet, a scalable and interpretable deep learning-based approach to motor imitation assessment in video data, which eliminates the need for data normalization, cleaning and annotation. CAMI-2DNet uses an encoder-decoder architecture to map a video to a motion encoding that is disentangled from nuisance factors such as body shape and camera views. To learn a disentangled representation, we employ synthetic data generated by motion retargeting of virtual characters through the reshuffling of motion, body shape, and camera views, as well as real participant data. To automatically assess how well an individual imitates an actor, we compute a similarity score between their motion encodings, and use it to discriminate individuals with ASCs from neurotypical (NT) individuals. Our comparative analysis demonstrates that CAMI-2DNet has a strong correlation with human scores while outperforming CAMI-2D in discriminating ASC vs NT children. Moreover, CAMI-2DNet performs comparably to CAMI-3D while offering greater practicality by operating directly on video data and without the need for ad-hoc data normalization and human annotations.","sentences":["Motor imitation impairments are commonly reported in individuals with autism spectrum conditions (ASCs), suggesting that motor imitation could be used as a phenotype for addressing autism heterogeneity.","Traditional methods for assessing motor imitation are subjective, labor-intensive, and require extensive human training.","Modern Computerized Assessment of Motor Imitation (CAMI) methods, such as CAMI-3D for motion capture data and CAMI-2D for video data, are less subjective.","However, they rely on labor-intensive data normalization and cleaning techniques, and human annotations for algorithm training.","To address these challenges, we propose CAMI-2DNet, a scalable and interpretable deep learning-based approach to motor imitation assessment in video data, which eliminates the need for data normalization, cleaning and annotation.","CAMI-2DNet uses an encoder-decoder architecture to map a video to a motion encoding that is disentangled from nuisance factors such as body shape and camera views.","To learn a disentangled representation, we employ synthetic data generated by motion retargeting of virtual characters through the reshuffling of motion, body shape, and camera views, as well as real participant data.","To automatically assess how well an individual imitates an actor, we compute a similarity score between their motion encodings, and use it to discriminate individuals with ASCs from neurotypical (NT) individuals.","Our comparative analysis demonstrates that CAMI-2DNet has a strong correlation with human scores while outperforming CAMI-2D in discriminating ASC vs NT children.","Moreover, CAMI-2DNet performs comparably to CAMI-3D while offering greater practicality by operating directly on video data and without the need for ad-hoc data normalization and human annotations."],"url":"http://arxiv.org/abs/2501.08609v1"}
{"created":"2025-01-15 05:52:55","title":"Double reflections Assisted RIS Deployment and Energy-efficient Group Selection in mmWaves D2D Communication","abstract":"Reconfigurable intelligent surfaces (RISs) offer a viable way to improve the performance of multi-hop device-to-device (D2D) communication. However, due to the substantial propagation and penetration losses of the millimeter waves (mmWaves), a direct line of sight (LoS) link and close proximity of a device pair are required for a high data rate. Static obstacles like trees and buildings can easily impede the direct LoS connectivity between a device pair. Hence, RIS placement plays a crucial role in establishing an indirect LoS link between them. Therefore, in this work, we propose a set cover-based RIS deployment strategy for both single and double RIS-assisted D2D communication. In particular, we have demonstrated that permitting reflections via two consecutive RISs can greatly lower the RIS density in the environment, preventing resource waste and enabling the service of more obstructed device pairs. After the RIS deployment, for information transfer, we also propose an energy-efficient group selection criteria. Moreover, we prove that sometimes double reflections are more beneficial than single reflection, which is counter-intuitive. Numerical results show that our approach outperforms a random and a recent deployment strategy.","sentences":["Reconfigurable intelligent surfaces (RISs) offer a viable way to improve the performance of multi-hop device-to-device (D2D) communication.","However, due to the substantial propagation and penetration losses of the millimeter waves (mmWaves), a direct line of sight (LoS) link and close proximity of a device pair are required for a high data rate.","Static obstacles like trees and buildings can easily impede the direct LoS connectivity between a device pair.","Hence, RIS placement plays a crucial role in establishing an indirect LoS link between them.","Therefore, in this work, we propose a set cover-based RIS deployment strategy for both single and double RIS-assisted D2D communication.","In particular, we have demonstrated that permitting reflections via two consecutive RISs can greatly lower the RIS density in the environment, preventing resource waste and enabling the service of more obstructed device pairs.","After the RIS deployment, for information transfer, we also propose an energy-efficient group selection criteria.","Moreover, we prove that sometimes double reflections are more beneficial than single reflection, which is counter-intuitive.","Numerical results show that our approach outperforms a random and a recent deployment strategy."],"url":"http://arxiv.org/abs/2501.08599v1"}
{"created":"2025-01-15 05:20:01","title":"OpenMLDB: A Real-Time Relational Data Feature Computation System for Online ML","abstract":"Efficient and consistent feature computation is crucial for a wide range of online ML applications. Typically, feature computation is divided into two distinct phases, i.e., offline stage for model training and online stage for model serving. These phases often rely on execution engines with different interface languages and function implementations, causing significant inconsistencies. Moreover, many online ML features involve complex time-series computations (e.g., functions over varied-length table windows) that differ from standard streaming and analytical queries. Existing data processing systems (e.g., Spark, Flink, DuckDB) often incur multi-second latencies for these computations, making them unsuitable for real-time online ML applications that demand timely feature updates.   This paper presents OpenMLDB, a feature computation system deployed in 4Paradigm's SageOne platform and over 100 real scenarios. Technically, OpenMLDB first employs a unified query plan generator for consistent computation results across the offline and online stages, significantly reducing feature deployment overhead. Second, OpenMLDB provides an online execution engine that resolves performance bottlenecks caused by long window computations (via pre-aggregation) and multi-table window unions (via data self-adjusting). It also provides a high-performance offline execution engine with window parallel optimization and time-aware data skew resolving. Third, OpenMLDB features a compact data format and stream-focused indexing to maximize memory usage and accelerate data access. Evaluations in testing and real workloads reveal significant performance improvements and resource savings compared to the baseline systems. The open community of OpenMLDB now has over 150 contributors and gained 1.6k stars on GitHub.","sentences":["Efficient and consistent feature computation is crucial for a wide range of online ML applications.","Typically, feature computation is divided into two distinct phases, i.e., offline stage for model training and online stage for model serving.","These phases often rely on execution engines with different interface languages and function implementations, causing significant inconsistencies.","Moreover, many online ML features involve complex time-series computations (e.g., functions over varied-length table windows) that differ from standard streaming and analytical queries.","Existing data processing systems (e.g., Spark, Flink, DuckDB) often incur multi-second latencies for these computations, making them unsuitable for real-time online ML applications that demand timely feature updates.   ","This paper presents OpenMLDB, a feature computation system deployed in 4Paradigm's SageOne platform and over 100 real scenarios.","Technically, OpenMLDB first employs a unified query plan generator for consistent computation results across the offline and online stages, significantly reducing feature deployment overhead.","Second, OpenMLDB provides an online execution engine that resolves performance bottlenecks caused by long window computations (via pre-aggregation) and multi-table window unions (via data self-adjusting).","It also provides a high-performance offline execution engine with window parallel optimization and time-aware data skew resolving.","Third, OpenMLDB features a compact data format and stream-focused indexing to maximize memory usage and accelerate data access.","Evaluations in testing and real workloads reveal significant performance improvements and resource savings compared to the baseline systems.","The open community of OpenMLDB now has over 150 contributors and gained 1.6k stars on GitHub."],"url":"http://arxiv.org/abs/2501.08591v1"}
{"created":"2025-01-15 05:17:38","title":"Molecular Graph Contrastive Learning with Line Graph","abstract":"Trapped by the label scarcity in molecular property prediction and drug design, graph contrastive learning (GCL) came forward. Leading contrastive learning works show two kinds of view generators, that is, random or learnable data corruption and domain knowledge incorporation. While effective, the two ways also lead to molecular semantics altering and limited generalization capability, respectively. To this end, we relate the \\textbf{L}in\\textbf{E} graph with \\textbf{MO}lecular graph co\\textbf{N}trastive learning and propose a novel method termed \\textit{LEMON}. Specifically, by contrasting the given graph with the corresponding line graph, the graph encoder can freely encode the molecular semantics without omission. Furthermore, we present a new patch with edge attribute fusion and two local contrastive losses enhance information transmission and tackle hard negative samples. Compared with state-of-the-art (SOTA) methods for view generation, superior performance on molecular property prediction suggests the effectiveness of our proposed framework.","sentences":["Trapped by the label scarcity in molecular property prediction and drug design, graph contrastive learning (GCL) came forward.","Leading contrastive learning works show two kinds of view generators, that is, random or learnable data corruption and domain knowledge incorporation.","While effective, the two ways also lead to molecular semantics altering and limited generalization capability, respectively.","To this end, we relate the \\textbf{L}in\\textbf{E} graph with \\textbf{MO}lecular graph co\\textbf{N}trastive learning and propose a novel method termed \\textit{LEMON}.","Specifically, by contrasting the given graph with the corresponding line graph, the graph encoder can freely encode the molecular semantics without omission.","Furthermore, we present a new patch with edge attribute fusion and two local contrastive losses enhance information transmission and tackle hard negative samples.","Compared with state-of-the-art (SOTA) methods for view generation, superior performance on molecular property prediction suggests the effectiveness of our proposed framework."],"url":"http://arxiv.org/abs/2501.08589v1"}
{"created":"2025-01-15 05:01:14","title":"Normalize Then Propagate: Efficient Homophilous Regularization for Few-shot Semi-Supervised Node Classification","abstract":"Graph Neural Networks (GNNs) have demonstrated remarkable ability in semi-supervised node classification. However, most existing GNNs rely heavily on a large amount of labeled data for training, which is labor-intensive and requires extensive domain knowledge. In this paper, we first analyze the restrictions of GNNs generalization from the perspective of supervision signals in the context of few-shot semi-supervised node classification. To address these challenges, we propose a novel algorithm named NormProp, which utilizes the homophily assumption of unlabeled nodes to generate additional supervision signals, thereby enhancing the generalization against label scarcity. The key idea is to efficiently capture both the class information and the consistency of aggregation during message passing, via decoupling the direction and Euclidean norm of node representations. Moreover, we conduct a theoretical analysis to determine the upper bound of Euclidean norm, and then propose homophilous regularization to constraint the consistency of unlabeled nodes. Extensive experiments demonstrate that NormProp achieve state-of-the-art performance under low-label rate scenarios with low computational complexity.","sentences":["Graph Neural Networks (GNNs) have demonstrated remarkable ability in semi-supervised node classification.","However, most existing GNNs rely heavily on a large amount of labeled data for training, which is labor-intensive and requires extensive domain knowledge.","In this paper, we first analyze the restrictions of GNNs generalization from the perspective of supervision signals in the context of few-shot semi-supervised node classification.","To address these challenges, we propose a novel algorithm named NormProp, which utilizes the homophily assumption of unlabeled nodes to generate additional supervision signals, thereby enhancing the generalization against label scarcity.","The key idea is to efficiently capture both the class information and the consistency of aggregation during message passing, via decoupling the direction and Euclidean norm of node representations.","Moreover, we conduct a theoretical analysis to determine the upper bound of Euclidean norm, and then propose homophilous regularization to constraint the consistency of unlabeled nodes.","Extensive experiments demonstrate that NormProp achieve state-of-the-art performance under low-label rate scenarios with low computational complexity."],"url":"http://arxiv.org/abs/2501.08581v1"}
{"created":"2025-01-15 04:59:49","title":"What Limits LLM-based Human Simulation: LLMs or Our Design?","abstract":"We argue that advancing LLM-based human simulation requires addressing both LLM's inherent limitations and simulation framework design challenges. Recent studies have revealed significant gaps between LLM-based human simulations and real-world observations, highlighting these dual challenges. To address these gaps, we present a comprehensive analysis of LLM limitations and our design issues, proposing targeted solutions for both aspects. Furthermore, we explore future directions that address both challenges simultaneously, particularly in data collection, LLM generation, and evaluation. To support further research in this field, we provide a curated collection of LLM-based human simulation resources.\\footnote{https://github.com/Persdre/llm-human-simulation}","sentences":["We argue that advancing LLM-based human simulation requires addressing both LLM's inherent limitations and simulation framework design challenges.","Recent studies have revealed significant gaps between LLM-based human simulations and real-world observations, highlighting these dual challenges.","To address these gaps, we present a comprehensive analysis of LLM limitations and our design issues, proposing targeted solutions for both aspects.","Furthermore, we explore future directions that address both challenges simultaneously, particularly in data collection, LLM generation, and evaluation.","To support further research in this field, we provide a curated collection of LLM-based human simulation resources.\\footnote{https://github.com/Persdre/llm-human-simulation}"],"url":"http://arxiv.org/abs/2501.08579v1"}
{"created":"2025-01-15 04:56:26","title":"Scalable and High-Quality Neural Implicit Representation for 3D Reconstruction","abstract":"Various SDF-based neural implicit surface reconstruction methods have been proposed recently, and have demonstrated remarkable modeling capabilities. However, due to the global nature and limited representation ability of a single network, existing methods still suffer from many drawbacks, such as limited accuracy and scale of the reconstruction. In this paper, we propose a versatile, scalable and high-quality neural implicit representation to address these issues. We integrate a divide-and-conquer approach into the neural SDF-based reconstruction. Specifically, we model the object or scene as a fusion of multiple independent local neural SDFs with overlapping regions. The construction of our representation involves three key steps: (1) constructing the distribution and overlap relationship of the local radiance fields based on object structure or data distribution, (2) relative pose registration for adjacent local SDFs, and (3) SDF blending. Thanks to the independent representation of each local region, our approach can not only achieve high-fidelity surface reconstruction, but also enable scalable scene reconstruction. Extensive experimental results demonstrate the effectiveness and practicality of our proposed method.","sentences":["Various SDF-based neural implicit surface reconstruction methods have been proposed recently, and have demonstrated remarkable modeling capabilities.","However, due to the global nature and limited representation ability of a single network, existing methods still suffer from many drawbacks, such as limited accuracy and scale of the reconstruction.","In this paper, we propose a versatile, scalable and high-quality neural implicit representation to address these issues.","We integrate a divide-and-conquer approach into the neural SDF-based reconstruction.","Specifically, we model the object or scene as a fusion of multiple independent local neural SDFs with overlapping regions.","The construction of our representation involves three key steps: (1) constructing the distribution and overlap relationship of the local radiance fields based on object structure or data distribution, (2) relative pose registration for adjacent local SDFs, and (3) SDF blending.","Thanks to the independent representation of each local region, our approach can not only achieve high-fidelity surface reconstruction, but also enable scalable scene reconstruction.","Extensive experimental results demonstrate the effectiveness and practicality of our proposed method."],"url":"http://arxiv.org/abs/2501.08577v1"}
{"created":"2025-01-15 04:51:10","title":"GOTLoc: General Outdoor Text-based Localization Using Scene Graph Retrieval with OpenStreetMap","abstract":"We propose GOTLoc, a robust localization method capable of operating even in outdoor environments where GPS signals are unavailable. The method achieves this robust localization by leveraging comparisons between scene graphs generated from text descriptions and maps. Existing text-based localization studies typically represent maps as point clouds and identify the most similar scenes by comparing embeddings of text and point cloud data. However, point cloud maps have limited scalability as it is impractical to pre-generate maps for all outdoor spaces. Furthermore, their large data size makes it challenging to store and utilize them directly on actual robots. To address these issues, GOTLoc leverages compact data structures, such as scene graphs, to store spatial information, enabling individual robots to carry and utilize large amounts of map data. Additionally, by utilizing publicly available map data, such as OpenStreetMap, which provides global information on outdoor spaces, we eliminate the need for additional effort to create custom map data. For performance evaluation, we utilized the KITTI360Pose dataset in conjunction with corresponding OpenStreetMap data to compare the proposed method with existing approaches. Our results demonstrate that the proposed method achieves accuracy comparable to algorithms relying on point cloud maps. Moreover, in city-scale tests, GOTLoc required significantly less storage compared to point cloud-based methods and completed overall processing within a few seconds, validating its applicability to real-world robotics. Our code is available at https://github.com/donghwijung/GOTLoc.","sentences":["We propose GOTLoc, a robust localization method capable of operating even in outdoor environments where GPS signals are unavailable.","The method achieves this robust localization by leveraging comparisons between scene graphs generated from text descriptions and maps.","Existing text-based localization studies typically represent maps as point clouds and identify the most similar scenes by comparing embeddings of text and point cloud data.","However, point cloud maps have limited scalability as it is impractical to pre-generate maps for all outdoor spaces.","Furthermore, their large data size makes it challenging to store and utilize them directly on actual robots.","To address these issues, GOTLoc leverages compact data structures, such as scene graphs, to store spatial information, enabling individual robots to carry and utilize large amounts of map data.","Additionally, by utilizing publicly available map data, such as OpenStreetMap, which provides global information on outdoor spaces, we eliminate the need for additional effort to create custom map data.","For performance evaluation, we utilized the KITTI360Pose dataset in conjunction with corresponding OpenStreetMap data to compare the proposed method with existing approaches.","Our results demonstrate that the proposed method achieves accuracy comparable to algorithms relying on point cloud maps.","Moreover, in city-scale tests, GOTLoc required significantly less storage compared to point cloud-based methods and completed overall processing within a few seconds, validating its applicability to real-world robotics.","Our code is available at https://github.com/donghwijung/GOTLoc."],"url":"http://arxiv.org/abs/2501.08575v1"}
{"created":"2025-01-15 04:32:41","title":"Information Entropy Invariance: Enhancing Length Extrapolation in Attention Mechanisms","abstract":"Improving the length extrapolation capabilities of Large Language Models (LLMs) remains a critical challenge in natural language processing. Many recent efforts have focused on modifying the scaled dot-product attention mechanism, and often introduce scaled temperatures without rigorous theoretical justification. To fill this gap, we introduce a novel approach based on information entropy invariance. We propose two new scaled temperatures to enhance length extrapolation. First, a training-free method InfoScale is designed for dot-product attention, and preserves focus on original tokens during length extrapolation by ensuring information entropy remains consistent. Second, we theoretically analyze the impact of scaling (CosScale) on cosine attention. Experimental data demonstrates that combining InfoScale and CosScale achieves state-of-the-art performance on the GAU-{\\alpha} model with a context window extended to 64 times the training length, and outperforms seven existing methods. Our analysis reveals that significantly increasing CosScale approximates windowed attention, and highlights the significance of attention score dilution as a key challenge in long-range context handling. The code and data are available at https://github.com/HT-NEKO/InfoScale.","sentences":["Improving the length extrapolation capabilities of Large Language Models (LLMs) remains a critical challenge in natural language processing.","Many recent efforts have focused on modifying the scaled dot-product attention mechanism, and often introduce scaled temperatures without rigorous theoretical justification.","To fill this gap, we introduce a novel approach based on information entropy invariance.","We propose two new scaled temperatures to enhance length extrapolation.","First, a training-free method InfoScale is designed for dot-product attention, and preserves focus on original tokens during length extrapolation by ensuring information entropy remains consistent.","Second, we theoretically analyze the impact of scaling (CosScale) on cosine attention.","Experimental data demonstrates that combining InfoScale and CosScale achieves state-of-the-art performance on the GAU-{\\alpha} model with a context window extended to 64 times the training length, and outperforms seven existing methods.","Our analysis reveals that significantly increasing CosScale approximates windowed attention, and highlights the significance of attention score dilution as a key challenge in long-range context handling.","The code and data are available at https://github.com/HT-NEKO/InfoScale."],"url":"http://arxiv.org/abs/2501.08570v1"}
{"created":"2025-01-15 04:17:48","title":"Towards Lightweight and Stable Zero-shot TTS with Self-distilled Representation Disentanglement","abstract":"Zero-shot Text-To-Speech (TTS) synthesis shows great promise for personalized voice customization through voice cloning. However, current methods for achieving zero-shot TTS heavily rely on large model scales and extensive training datasets to ensure satisfactory performance and generalizability across various speakers. This raises concerns regarding both deployment costs and data security. In this paper, we present a lightweight and stable zero-shot TTS system. We introduce a novel TTS architecture designed to effectively model linguistic content and various speaker attributes from source speech and prompt speech, respectively. Furthermore, we present a two-stage self-distillation framework that constructs parallel data pairs for effectively disentangling linguistic content and speakers from the perspective of training data. Extensive experiments show that our system exhibits excellent performance and superior stability on the zero-shot TTS tasks. Moreover, it shows markedly superior computational efficiency, with RTFs of 0.13 and 0.012 on the CPU and GPU, respectively.","sentences":["Zero-shot Text-To-Speech (TTS) synthesis shows great promise for personalized voice customization through voice cloning.","However, current methods for achieving zero-shot TTS heavily rely on large model scales and extensive training datasets to ensure satisfactory performance and generalizability across various speakers.","This raises concerns regarding both deployment costs and data security.","In this paper, we present a lightweight and stable zero-shot TTS system.","We introduce a novel TTS architecture designed to effectively model linguistic content and various speaker attributes from source speech and prompt speech, respectively.","Furthermore, we present a two-stage self-distillation framework that constructs parallel data pairs for effectively disentangling linguistic content and speakers from the perspective of training data.","Extensive experiments show that our system exhibits excellent performance and superior stability on the zero-shot TTS tasks.","Moreover, it shows markedly superior computational efficiency, with RTFs of 0.13 and 0.012 on the CPU and GPU, respectively."],"url":"http://arxiv.org/abs/2501.08566v1"}
{"created":"2025-01-15 04:07:06","title":"MIAFEx: An Attention-based Feature Extraction Method for Medical Image Classification","abstract":"Feature extraction techniques are crucial in medical image classification; however, classical feature extractors in addition to traditional machine learning classifiers often exhibit significant limitations in providing sufficient discriminative information for complex image sets. While Convolutional Neural Networks (CNNs) and Vision Transformer (ViT) have shown promise in feature extraction, they are prone to overfitting due to the inherent characteristics of medical imaging data, including small sample sizes or high intra-class variance. In this work, the Medical Image Attention-based Feature Extractor (MIAFEx) is proposed, a novel method that employs a learnable refinement mechanism to enhance the classification token within the Transformer encoder architecture. This mechanism adjusts the token based on learned weights, improving the extraction of salient features and enhancing the model's adaptability to the challenges presented by medical imaging data. The MIAFEx output features quality is compared against classical feature extractors using traditional and hybrid classifiers. Also, the performance of these features is compared against modern CNN and ViT models in classification tasks, demonstrating its superiority in accuracy and robustness across multiple complex classification medical imaging datasets. This advantage is particularly pronounced in scenarios with limited training data, where traditional and modern models often struggle to generalize effectively. The source code of this proposal can be found at https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFEx","sentences":["Feature extraction techniques are crucial in medical image classification; however, classical feature extractors in addition to traditional machine learning classifiers often exhibit significant limitations in providing sufficient discriminative information for complex image sets.","While Convolutional Neural Networks (CNNs) and Vision Transformer (ViT) have shown promise in feature extraction, they are prone to overfitting due to the inherent characteristics of medical imaging data, including small sample sizes or high intra-class variance.","In this work, the Medical Image Attention-based Feature Extractor (MIAFEx) is proposed, a novel method that employs a learnable refinement mechanism to enhance the classification token within the Transformer encoder architecture.","This mechanism adjusts the token based on learned weights, improving the extraction of salient features and enhancing the model's adaptability to the challenges presented by medical imaging data.","The MIAFEx output features quality is compared against classical feature extractors using traditional and hybrid classifiers.","Also, the performance of these features is compared against modern CNN and ViT models in classification tasks, demonstrating its superiority in accuracy and robustness across multiple complex classification medical imaging datasets.","This advantage is particularly pronounced in scenarios with limited training data, where traditional and modern models often struggle to generalize effectively.","The source code of this proposal can be found at https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFEx"],"url":"http://arxiv.org/abs/2501.08562v1"}
{"created":"2025-01-15 03:00:57","title":"Knowledge prompt chaining for semantic modeling","abstract":"The task of building semantics for structured data such as CSV, JSON, and XML files is highly relevant in the knowledge representation field. Even though we have a vast of structured data on the internet, mapping them to domain ontologies to build semantics for them is still very challenging as it requires the construction model to understand and learn graph-structured knowledge. Otherwise, the task will require human beings' effort and cost. In this paper, we proposed a novel automatic semantic modeling framework: Knowledge Prompt Chaining. It can serialize the graph-structured knowledge and inject it into the LLMs properly in a Prompt Chaining architecture. Through this knowledge injection and prompting chaining, the model in our framework can learn the structure information and latent space of the graph and generate the semantic labels and semantic graphs following the chains' insturction naturally. Based on experimental results, our method achieves better performance than existing leading techniques, despite using reduced structured input data.","sentences":["The task of building semantics for structured data such as CSV, JSON, and XML files is highly relevant in the knowledge representation field.","Even though we have a vast of structured data on the internet, mapping them to domain ontologies to build semantics for them is still very challenging as it requires the construction model to understand and learn graph-structured knowledge.","Otherwise, the task will require human beings' effort and cost.","In this paper, we proposed a novel automatic semantic modeling framework: Knowledge Prompt Chaining.","It can serialize the graph-structured knowledge and inject it into the LLMs properly in a Prompt Chaining architecture.","Through this knowledge injection and prompting chaining, the model in our framework can learn the structure information and latent space of the graph and generate the semantic labels and semantic graphs following the chains' insturction naturally.","Based on experimental results, our method achieves better performance than existing leading techniques, despite using reduced structured input data."],"url":"http://arxiv.org/abs/2501.08540v1"}
{"created":"2025-01-15 02:59:21","title":"Research on stock price forecast of general electric based on mixed CNN-LSTM model","abstract":"Accurate stock price prediction is crucial for investors and financial institutions, yet the complexity of the stock market makes it highly challenging. This study aims to construct an effective model to enhance the prediction ability of General Electric's stock price trend. The CNN - LSTM model is adopted, combining the feature extraction ability of CNN with the long - term dependency handling ability of LSTM, and the Adam optimizer is used to adjust the parameters. In the data preparation stage, historical trading data of General Electric's stock is collected. After cleaning, handling missing values, and feature engineering, features with strong correlations to the closing price are selected and dimensionality reduction is performed. During model training, the data is divided into training, validation, and testing sets in a ratio of 7:2:1. The Stochastic Gradient Descent algorithm is used with a dynamic learning rate adjustment and L2 regularization, and the Mean Squared Error is used as the loss function, evaluated by variance, R - squared score, and maximum error. Experimental results show that the model loss decreases steadily, and the predicted values align well with the actual values, providing a powerful tool for investment decisions. However, the model's performance in real - time and extreme market conditions remains to be tested, and future improvements could consider incorporating more data sources.","sentences":["Accurate stock price prediction is crucial for investors and financial institutions, yet the complexity of the stock market makes it highly challenging.","This study aims to construct an effective model to enhance the prediction ability of General Electric's stock price trend.","The CNN - LSTM model is adopted, combining the feature extraction ability of CNN with the long - term dependency handling ability of LSTM, and the Adam optimizer is used to adjust the parameters.","In the data preparation stage, historical trading data of General Electric's stock is collected.","After cleaning, handling missing values, and feature engineering, features with strong correlations to the closing price are selected and dimensionality reduction is performed.","During model training, the data is divided into training, validation, and testing sets in a ratio of 7:2:1.","The Stochastic Gradient Descent algorithm is used with a dynamic learning rate adjustment and L2 regularization, and the Mean Squared Error is used as the loss function, evaluated by variance, R - squared score, and maximum error.","Experimental results show that the model loss decreases steadily, and the predicted values align well with the actual values, providing a powerful tool for investment decisions.","However, the model's performance in real - time and extreme market conditions remains to be tested, and future improvements could consider incorporating more data sources."],"url":"http://arxiv.org/abs/2501.08539v1"}
{"created":"2025-01-15 02:37:28","title":"Dynamic Portfolio Optimization via Augmented DDPG with Quantum Price Levels-Based Trading Strategy","abstract":"With the development of deep learning, Dynamic Portfolio Optimization (DPO) problem has received a lot of attention in recent years, not only in the field of finance but also in the field of deep learning. Some advanced research in recent years has proposed the application of Deep Reinforcement Learning (DRL) to the DPO problem, which demonstrated to be more advantageous than supervised learning in solving the DPO problem. However, there are still certain unsolved issues: 1) DRL algorithms usually have the problems of slow learning speed and high sample complexity, which is especially problematic when dealing with complex financial data. 2) researchers use DRL simply for the purpose of obtaining high returns, but pay little attention to the problem of risk control and trading strategy, which will affect the stability of model returns. In order to address these issues, in this study we revamped the intrinsic structure of the model based on the Deep Deterministic Policy Gradient (DDPG) and proposed the Augmented DDPG model. Besides, we also proposed an innovative risk control strategy based on Quantum Price Levels (QPLs) derived from Quantum Finance Theory (QFT). Our experimental results revealed that our model has better profitability as well as risk control ability with less sample complexity in the DPO problem compared to the baseline models.","sentences":["With the development of deep learning, Dynamic Portfolio Optimization (DPO) problem has received a lot of attention in recent years, not only in the field of finance but also in the field of deep learning.","Some advanced research in recent years has proposed the application of Deep Reinforcement Learning (DRL) to the DPO problem, which demonstrated to be more advantageous than supervised learning in solving the DPO problem.","However, there are still certain unsolved issues: 1) DRL algorithms usually have the problems of slow learning speed and high sample complexity, which is especially problematic when dealing with complex financial data.","2) researchers use DRL simply for the purpose of obtaining high returns, but pay little attention to the problem of risk control and trading strategy, which will affect the stability of model returns.","In order to address these issues, in this study we revamped the intrinsic structure of the model based on the Deep Deterministic Policy Gradient (DDPG) and proposed the Augmented DDPG model.","Besides, we also proposed an innovative risk control strategy based on Quantum Price Levels (QPLs) derived from Quantum Finance Theory (QFT).","Our experimental results revealed that our model has better profitability as well as risk control ability with less sample complexity in the DPO problem compared to the baseline models."],"url":"http://arxiv.org/abs/2501.08528v1"}
{"created":"2025-01-15 02:17:38","title":"Mitigating Domain Shift in Federated Learning via Intra- and Inter-Domain Prototypes","abstract":"Federated Learning (FL) has emerged as a decentralized machine learning technique, allowing clients to train a global model collaboratively without sharing private data. However, most FL studies ignore the crucial challenge of heterogeneous domains where each client has a distinct feature distribution, which is common in real-world scenarios. Prototype learning, which leverages the mean feature vectors within the same classes, has become a prominent solution for federated learning under domain skew. However, existing federated prototype learning methods only consider inter-domain prototypes on the server and overlook intra-domain characteristics. In this work, we introduce a novel federated prototype learning method, namely I$^2$PFL, which incorporates $\\textbf{I}$ntra-domain and $\\textbf{I}$nter-domain $\\textbf{P}$rototypes, to mitigate domain shifts and learn a generalized global model across multiple domains in federated learning. To construct intra-domain prototypes, we propose feature alignment with MixUp-based augmented prototypes to capture the diversity of local domains and enhance the generalization of local features. Additionally, we introduce a reweighting mechanism for inter-domain prototypes to generate generalized prototypes to provide inter-domain knowledge and reduce domain skew across multiple clients. Extensive experiments on the Digits, Office-10, and PACS datasets illustrate the superior performance of our method compared to other baselines.","sentences":["Federated Learning (FL) has emerged as a decentralized machine learning technique, allowing clients to train a global model collaboratively without sharing private data.","However, most FL studies ignore the crucial challenge of heterogeneous domains where each client has a distinct feature distribution, which is common in real-world scenarios.","Prototype learning, which leverages the mean feature vectors within the same classes, has become a prominent solution for federated learning under domain skew.","However, existing federated prototype learning methods only consider inter-domain prototypes on the server and overlook intra-domain characteristics.","In this work, we introduce a novel federated prototype learning method, namely I$^2$PFL, which incorporates $\\textbf{I}$ntra-domain and $\\textbf{I}$nter-domain $\\textbf{P}$rototypes, to mitigate domain shifts and learn a generalized global model across multiple domains in federated learning.","To construct intra-domain prototypes, we propose feature alignment with MixUp-based augmented prototypes to capture the diversity of local domains and enhance the generalization of local features.","Additionally, we introduce a reweighting mechanism for inter-domain prototypes to generate generalized prototypes to provide inter-domain knowledge and reduce domain skew across multiple clients.","Extensive experiments on the Digits, Office-10, and PACS datasets illustrate the superior performance of our method compared to other baselines."],"url":"http://arxiv.org/abs/2501.08521v1"}
{"created":"2025-01-15 01:59:24","title":"Learning Hyperplane Tree: A Piecewise Linear and Fully Interpretable Decision-making Framework","abstract":"This paper introduces a novel tree-based model, Learning Hyperplane Tree (LHT), which outperforms state-of-the-art (SOTA) tree models for classification tasks on several public datasets. The structure of LHT is simple and efficient: it partitions the data using several hyperplanes to progressively distinguish between target and non-target class samples. Although the separation is not perfect at each stage, LHT effectively improves the distinction through successive partitions. During testing, a sample is classified by evaluating the hyperplanes defined in the branching blocks and traversing down the tree until it reaches the corresponding leaf block. The class of the test sample is then determined using the piecewise linear membership function defined in the leaf blocks, which is derived through least-squares fitting and fuzzy logic. LHT is highly transparent and interpretable--at each branching block, the contribution of each feature to the classification can be clearly observed.","sentences":["This paper introduces a novel tree-based model, Learning Hyperplane Tree (LHT), which outperforms state-of-the-art (SOTA) tree models for classification tasks on several public datasets.","The structure of LHT is simple and efficient: it partitions the data using several hyperplanes to progressively distinguish between target and non-target class samples.","Although the separation is not perfect at each stage, LHT effectively improves the distinction through successive partitions.","During testing, a sample is classified by evaluating the hyperplanes defined in the branching blocks and traversing down the tree until it reaches the corresponding leaf block.","The class of the test sample is then determined using the piecewise linear membership function defined in the leaf blocks, which is derived through least-squares fitting and fuzzy logic.","LHT is highly transparent and interpretable--at each branching block, the contribution of each feature to the classification can be clearly observed."],"url":"http://arxiv.org/abs/2501.08515v1"}
{"created":"2025-01-15 00:56:59","title":"Exploring the Efficacy of Meta-Learning: Unveiling Superior Data Diversity Utilization of MAML Over Pre-training","abstract":"Currently, data and model size dominate the narrative in the training of super-large, powerful models. However, there has been a lack of exploration on the effect of other attributes of the training dataset on model performance. We hypothesize that dataset diversity can impact the performance of vision models. Our study shows positive correlations between test set accuracy and data diversity, providing an argument for furthering the research of dataset attributes beyond size. We analyzed pre-training and model-agnostic meta-learning methods on twelve popular visual datasets (e.g., Omniglot, CIFAR-FS, Aircraft) and five model configurations, including MAML variants with different numbers of inner gradient steps and supervised learning. We show moderate to strong positive correlations (R-squared: 0.15-0.42) between accuracy and data diversity and weaker but significant correlations (R-squared: ~0.2) between loss and diversity. These findings support our hypothesis and demonstrate a promising way for a deeper exploration of how formal data diversity influences model performance. This initial study highlights the potential of (Task2Vec) data diversity as a valuable measure in the rapidly evolving field of large-scale learning and emphasizes that understanding the dataset is key to building more powerful and generalizable models.","sentences":["Currently, data and model size dominate the narrative in the training of super-large, powerful models.","However, there has been a lack of exploration on the effect of other attributes of the training dataset on model performance.","We hypothesize that dataset diversity can impact the performance of vision models.","Our study shows positive correlations between test set accuracy and data diversity, providing an argument for furthering the research of dataset attributes beyond size.","We analyzed pre-training and model-agnostic meta-learning methods on twelve popular visual datasets (e.g., Omniglot, CIFAR-FS, Aircraft) and five model configurations, including MAML variants with different numbers of inner gradient steps and supervised learning.","We show moderate to strong positive correlations (R-squared: 0.15-0.42) between accuracy and data diversity and weaker but significant correlations (R-squared: ~0.2) between loss and diversity.","These findings support our hypothesis and demonstrate a promising way for a deeper exploration of how formal data diversity influences model performance.","This initial study highlights the potential of (Task2Vec) data diversity as a valuable measure in the rapidly evolving field of large-scale learning and emphasizes that understanding the dataset is key to building more powerful and generalizable models."],"url":"http://arxiv.org/abs/2501.08506v1"}
{"created":"2025-01-15 00:39:21","title":"Adapting Whisper for Regional Dialects: Enhancing Public Services for Vulnerable Populations in the United Kingdom","abstract":"We collect novel data in the public service domain to evaluate the capability of the state-of-the-art automatic speech recognition (ASR) models in capturing regional differences in accents in the United Kingdom (UK), specifically focusing on two accents from Scotland with distinct dialects. This study addresses real-world problems where biased ASR models can lead to miscommunication in public services, disadvantaging individuals with regional accents particularly those in vulnerable populations. We first examine the out-of-the-box performance of the Whisper large-v3 model on a baseline dataset and our data. We then explore the impact of fine-tuning Whisper on the performance in the two UK regions and investigate the effectiveness of existing model evaluation techniques for our real-world application through manual inspection of model errors. We observe that the Whisper model has a higher word error rate (WER) on our test datasets compared to the baseline data and fine-tuning on a given data improves performance on the test dataset with the same domain and accent. The fine-tuned models also appear to show improved performance when applied to the test data outside of the region it was trained on suggesting that fine-tuned models may be transferable within parts of the UK. Our manual analysis of model outputs reveals the benefits and drawbacks of using WER as an evaluation metric and fine-tuning to adapt to regional dialects.","sentences":["We collect novel data in the public service domain to evaluate the capability of the state-of-the-art automatic speech recognition (ASR) models in capturing regional differences in accents in the United Kingdom (UK), specifically focusing on two accents from Scotland with distinct dialects.","This study addresses real-world problems where biased ASR models can lead to miscommunication in public services, disadvantaging individuals with regional accents particularly those in vulnerable populations.","We first examine the out-of-the-box performance of the Whisper large-v3 model on a baseline dataset and our data.","We then explore the impact of fine-tuning Whisper on the performance in the two UK regions and investigate the effectiveness of existing model evaluation techniques for our real-world application through manual inspection of model errors.","We observe that the Whisper model has a higher word error rate (WER) on our test datasets compared to the baseline data and fine-tuning on a given data improves performance on the test dataset with the same domain and accent.","The fine-tuned models also appear to show improved performance when applied to the test data outside of the region it was trained on suggesting that fine-tuned models may be transferable within parts of the UK.","Our manual analysis of model outputs reveals the benefits and drawbacks of using WER as an evaluation metric and fine-tuning to adapt to regional dialects."],"url":"http://arxiv.org/abs/2501.08502v1"}
{"created":"2025-01-15 00:36:53","title":"Visual Network Analysis in Immersive Environments: A Survey","abstract":"The increasing complexity and volume of network data demand effective analysis approaches, with visual exploration proving particularly beneficial. Immersive technologies, such as augmented reality, virtual reality, and large display walls, have enabled the emerging field of immersive analytics, offering new opportunities to enhance user engagement, spatial awareness, and problem-solving. A growing body of work explores immersive environments for network visualisation, ranging from design studies to fully integrated applications across various domains. Despite these advancements, the field remains fragmented, with diverse methodologies, hardware setups, and evaluation criteria, often lacking clear connections to prior work. This fragmentation complicates the comparability and generalisability of findings. To address this, we present a structured survey of visual network analysis in immersive environments. We systematically categorise and analyse existing approaches, revealing connections and coverage within the design space. By synthesising findings of experiments and evaluating current applications, we identify key achievements, challenges, and research gaps. Additionally, we provide an interactive online resource for exploring and updating results, aiming to guide researchers and practitioners in advancing the field. This work provides a comprehensive overview of the research landscape and proposes actionable insights to foster innovation in immersive network analysis.","sentences":["The increasing complexity and volume of network data demand effective analysis approaches, with visual exploration proving particularly beneficial.","Immersive technologies, such as augmented reality, virtual reality, and large display walls, have enabled the emerging field of immersive analytics, offering new opportunities to enhance user engagement, spatial awareness, and problem-solving.","A growing body of work explores immersive environments for network visualisation, ranging from design studies to fully integrated applications across various domains.","Despite these advancements, the field remains fragmented, with diverse methodologies, hardware setups, and evaluation criteria, often lacking clear connections to prior work.","This fragmentation complicates the comparability and generalisability of findings.","To address this, we present a structured survey of visual network analysis in immersive environments.","We systematically categorise and analyse existing approaches, revealing connections and coverage within the design space.","By synthesising findings of experiments and evaluating current applications, we identify key achievements, challenges, and research gaps.","Additionally, we provide an interactive online resource for exploring and updating results, aiming to guide researchers and practitioners in advancing the field.","This work provides a comprehensive overview of the research landscape and proposes actionable insights to foster innovation in immersive network analysis."],"url":"http://arxiv.org/abs/2501.08500v1"}
{"created":"2025-01-15 00:00:01","title":"Addressing Intersectionality, Explainability, and Ethics in AI-Driven Diagnostics: A Rebuttal and Call for Transdiciplinary Action","abstract":"The increasing integration of artificial intelligence (AI) into medical diagnostics necessitates a critical examination of its ethical and practical implications. While the prioritization of diagnostic accuracy, as advocated by Sabuncu et al. (2025), is essential, this approach risks oversimplifying complex socio-ethical issues, including fairness, privacy, and intersectionality. This rebuttal emphasizes the dangers of reducing multifaceted health disparities to quantifiable metrics and advocates for a more transdisciplinary approach. By incorporating insights from social sciences, ethics, and public health, AI systems can address the compounded effects of intersecting identities and safeguard sensitive data. Additionally, explainability and interpretability must be central to AI design, fostering trust and accountability. This paper calls for a framework that balances accuracy with fairness, privacy, and inclusivity to ensure AI-driven diagnostics serve diverse populations equitably and ethically.","sentences":["The increasing integration of artificial intelligence (AI) into medical diagnostics necessitates a critical examination of its ethical and practical implications.","While the prioritization of diagnostic accuracy, as advocated by Sabuncu et al. (2025), is essential, this approach risks oversimplifying complex socio-ethical issues, including fairness, privacy, and intersectionality.","This rebuttal emphasizes the dangers of reducing multifaceted health disparities to quantifiable metrics and advocates for a more transdisciplinary approach.","By incorporating insights from social sciences, ethics, and public health, AI systems can address the compounded effects of intersecting identities and safeguard sensitive data.","Additionally, explainability and interpretability must be central to AI design, fostering trust and accountability.","This paper calls for a framework that balances accuracy with fairness, privacy, and inclusivity to ensure AI-driven diagnostics serve diverse populations equitably and ethically."],"url":"http://arxiv.org/abs/2501.08497v1"}
{"created":"2025-01-14 23:59:23","title":"Quantifying the Importance of Data Alignment in Downstream Model Performance","abstract":"Contrary to the conventional emphasis on dataset size, we explore the role of data alignment -- an often overlooked aspect of data quality -- in training capable Large Language Models (LLMs). To do so, we use the Task2Vec-based alignment coefficient, a quantitative measure of the similarity between two datasets, to quantify the impact of alignment between training data and evaluation data on downstream performance. In particular, we conduct controlled \\textit{interventional} experiments for two settings: 1. the impact of increased alignment coefficients between various pre-training (pt) against evaluation datasets, and 2. the impact of increased alignment coefficients between domain specific fine-tuning (ft) against domain specific evaluation. The domain specific task we explore is Autoformalization -- the machine translation task between natural language and code for formal verification. In both settings, we find a strong, predictable negative correlation between the alignment coefficient of a model's training and evaluation data and the model's loss/perplexity on the respective downstream task. These findings suggest a re-evaluation of LLM training approaches, demonstrating the relevance of data alignment compared to data quantity, especially in specialized downstream tasks such as Autoformalization.","sentences":["Contrary to the conventional emphasis on dataset size, we explore the role of data alignment -- an often overlooked aspect of data quality -- in training capable Large Language Models (LLMs).","To do so, we use the Task2Vec-based alignment coefficient, a quantitative measure of the similarity between two datasets, to quantify the impact of alignment between training data and evaluation data on downstream performance.","In particular, we conduct controlled \\textit{interventional} experiments for two settings: 1.","the impact of increased alignment coefficients between various pre-training (pt) against evaluation datasets, and 2.","the impact of increased alignment coefficients between domain specific fine-tuning (ft) against domain specific evaluation.","The domain specific task we explore is Autoformalization -- the machine translation task between natural language and code for formal verification.","In both settings, we find a strong, predictable negative correlation between the alignment coefficient of a model's training and evaluation data and the model's loss/perplexity on the respective downstream task.","These findings suggest a re-evaluation of LLM training approaches, demonstrating the relevance of data alignment compared to data quantity, especially in specialized downstream tasks such as Autoformalization."],"url":"http://arxiv.org/abs/2501.08496v1"}
{"created":"2025-01-14 23:13:14","title":"CORD: Co-design of Resource Allocation and Deadline Decomposition with Generative Profiling","abstract":"As multicore hardware is becoming increasingly common in real-time systems, traditional scheduling techniques that assume a single worst-case execution time for a task are no longer adequate, since they ignore the impact of shared resources on execution time. When tasks execute concurrently on different cores, their execution times often vary substantially with their allocated budgets of shared resources, such as cache and memory bandwidth. Even under a specific resource allocation, the resource use pattern of a task also changes with time during a job execution. It is therefore important to consider the relationship between multicore resources and execution time in task modeling and scheduling algorithm design.   In this paper, we propose a much more precise execution model for DAG-based real-time tasks that captures the time-varying resource use characteristics of a task under different budgets of shared resources. We present a generative resource profiling algorithm that efficiently predicts, from limited measurement data, the resource profile of a task at any time during its execution under a given resource budget. The generative profiles can then be used to construct the execution models for tasks, using which one can make informed resource allocation decisions. We further introduce a multicore resource allocation and deadline decomposition co-design technique for DAG-based tasks that leverages the generated execution models to jointly allocate resources and deadlines to subtasks, to maximize resource efficiency and schedulability. Our evaluation results show that our generative profiling algorithm achieves high accuracy while being efficient, and that our co-allocation technique substantially improves schedulability compared to a state-of-the-art deadline decomposition method.","sentences":["As multicore hardware is becoming increasingly common in real-time systems, traditional scheduling techniques that assume a single worst-case execution time for a task are no longer adequate, since they ignore the impact of shared resources on execution time.","When tasks execute concurrently on different cores, their execution times often vary substantially with their allocated budgets of shared resources, such as cache and memory bandwidth.","Even under a specific resource allocation, the resource use pattern of a task also changes with time during a job execution.","It is therefore important to consider the relationship between multicore resources and execution time in task modeling and scheduling algorithm design.   ","In this paper, we propose a much more precise execution model for DAG-based real-time tasks that captures the time-varying resource use characteristics of a task under different budgets of shared resources.","We present a generative resource profiling algorithm that efficiently predicts, from limited measurement data, the resource profile of a task at any time during its execution under a given resource budget.","The generative profiles can then be used to construct the execution models for tasks, using which one can make informed resource allocation decisions.","We further introduce a multicore resource allocation and deadline decomposition co-design technique for DAG-based tasks that leverages the generated execution models to jointly allocate resources and deadlines to subtasks, to maximize resource efficiency and schedulability.","Our evaluation results show that our generative profiling algorithm achieves high accuracy while being efficient, and that our co-allocation technique substantially improves schedulability compared to a state-of-the-art deadline decomposition method."],"url":"http://arxiv.org/abs/2501.08484v1"}
{"created":"2025-01-14 22:44:23","title":"Skyrise: Exploiting Serverless Cloud Infrastructure for Elastic Data Processing","abstract":"Serverless computing offers elasticity unmatched by conventional server-based cloud infrastructure. Although modern data processing systems embrace serverless storage, such as Amazon S3, they continue to manage their compute resources as servers. This is challenging for unpredictable workloads, leaving clusters often underutilized. Recent research shows the potential of serverless compute resources, such as cloud functions, for elastic data processing, but also sees limitations in performance robustness and cost efficiency for long running workloads. These challenges require holistic approaches across the system stack. However, to the best of our knowledge, there is no end-to-end data processing system built entirely on serverless infrastructure. In this paper, we present Skyrise, our effort towards building the first fully serverless SQL query processor. Skyrise exploits the elasticity of its underlying infrastructure, while alleviating the inherent limitations with a number of adaptive and cost-aware techniques. We show that both Skyrise's performance and cost are competitive to other cloud data systems for terabyte-scale queries of the analytical TPC-H benchmark.","sentences":["Serverless computing offers elasticity unmatched by conventional server-based cloud infrastructure.","Although modern data processing systems embrace serverless storage, such as Amazon S3, they continue to manage their compute resources as servers.","This is challenging for unpredictable workloads, leaving clusters often underutilized.","Recent research shows the potential of serverless compute resources, such as cloud functions, for elastic data processing, but also sees limitations in performance robustness and cost efficiency for long running workloads.","These challenges require holistic approaches across the system stack.","However, to the best of our knowledge, there is no end-to-end data processing system built entirely on serverless infrastructure.","In this paper, we present Skyrise, our effort towards building the first fully serverless SQL query processor.","Skyrise exploits the elasticity of its underlying infrastructure, while alleviating the inherent limitations with a number of adaptive and cost-aware techniques.","We show that both Skyrise's performance and cost are competitive to other cloud data systems for terabyte-scale queries of the analytical TPC-H benchmark."],"url":"http://arxiv.org/abs/2501.08479v1"}
{"created":"2025-01-14 22:36:11","title":"Benchmarking Classical, Deep, and Generative Models for Human Activity Recognition","abstract":"Human Activity Recognition (HAR) has gained significant importance with the growing use of sensor-equipped devices and large datasets. This paper evaluates the performance of three categories of models : classical machine learning, deep learning architectures, and Restricted Boltzmann Machines (RBMs) using five key benchmark datasets of HAR (UCI-HAR, OPPORTUNITY, PAMAP2, WISDM, and Berkeley MHAD). We assess various models, including Decision Trees, Random Forests, Convolutional Neural Networks (CNN), and Deep Belief Networks (DBNs), using metrics such as accuracy, precision, recall, and F1-score for a comprehensive comparison. The results show that CNN models offer superior performance across all datasets, especially on the Berkeley MHAD. Classical models like Random Forest do well on smaller datasets but face challenges with larger, more complex data. RBM-based models also show notable potential, particularly for feature learning. This paper offers a detailed comparison to help researchers choose the most suitable model for HAR tasks.","sentences":["Human Activity Recognition (HAR) has gained significant importance with the growing use of sensor-equipped devices and large datasets.","This paper evaluates the performance of three categories of models : classical machine learning, deep learning architectures, and Restricted Boltzmann Machines (RBMs) using five key benchmark datasets of HAR (UCI-HAR, OPPORTUNITY, PAMAP2, WISDM, and Berkeley MHAD).","We assess various models, including Decision Trees, Random Forests, Convolutional Neural Networks (CNN), and Deep Belief Networks (DBNs), using metrics such as accuracy, precision, recall, and F1-score for a comprehensive comparison.","The results show that CNN models offer superior performance across all datasets, especially on the Berkeley MHAD.","Classical models like Random Forest do well on smaller datasets but face challenges with larger, more complex data.","RBM-based models also show notable potential, particularly for feature learning.","This paper offers a detailed comparison to help researchers choose the most suitable model for HAR tasks."],"url":"http://arxiv.org/abs/2501.08471v1"}
{"created":"2025-01-14 22:27:48","title":"Selective Attention Merging for low resource tasks: A case study of Child ASR","abstract":"While Speech Foundation Models (SFMs) excel in various speech tasks, their performance for low-resource tasks such as child Automatic Speech Recognition (ASR) is hampered by limited pretraining data. To address this, we explore different model merging techniques to leverage knowledge from models trained on larger, more diverse speech corpora. This paper also introduces Selective Attention (SA) Merge, a novel method that selectively merges task vectors from attention matrices to enhance SFM performance on low-resource tasks. Experiments on the MyST database show significant reductions in relative word error rate of up to 14%, outperforming existing model merging and data augmentation techniques. By combining data augmentation techniques with SA Merge, we achieve a new state-of-the-art WER of 8.69 on the MyST database for the Whisper-small model, highlighting the potential of SA Merge for improving low-resource ASR.","sentences":["While Speech Foundation Models (SFMs) excel in various speech tasks, their performance for low-resource tasks such as child Automatic Speech Recognition (ASR) is hampered by limited pretraining data.","To address this, we explore different model merging techniques to leverage knowledge from models trained on larger, more diverse speech corpora.","This paper also introduces Selective Attention (SA) Merge, a novel method that selectively merges task vectors from attention matrices to enhance SFM performance on low-resource tasks.","Experiments on the MyST database show significant reductions in relative word error rate of up to 14%, outperforming existing model merging and data augmentation techniques.","By combining data augmentation techniques with SA Merge, we achieve a new state-of-the-art WER of 8.69 on the MyST database for the Whisper-small model, highlighting the potential of SA Merge for improving low-resource ASR."],"url":"http://arxiv.org/abs/2501.08468v1"}
{"created":"2025-01-14 22:23:11","title":"Predicting Performance of Object Detection Models in Electron Microscopy Using Random Forests","abstract":"Quantifying prediction uncertainty when applying object detection models to new, unlabeled datasets is critical in applied machine learning. This study introduces an approach to estimate the performance of deep learning-based object detection models for quantifying defects in transmission electron microscopy (TEM) images, focusing on detecting irradiation-induced cavities in TEM images of metal alloys. We developed a random forest regression model that predicts the object detection F1 score, a statistical metric used to evaluate the ability to accurately locate and classify objects of interest. The random forest model uses features extracted from the predictions of the object detection model whose uncertainty is being quantified, enabling fast prediction on new, unlabeled images. The mean absolute error (MAE) for predicting F1 of the trained model on test data is 0.09, and the $R^2$ score is 0.77, indicating there is a significant correlation between the random forest regression model predicted and true defect detection F1 scores. The approach is shown to be robust across three distinct TEM image datasets with varying imaging and material domains. Our approach enables users to estimate the reliability of a defect detection and segmentation model predictions and assess the applicability of the model to their specific datasets, providing valuable information about possible domain shifts and whether the model needs to be fine-tuned or trained on additional data to be maximally effective for the desired use case.","sentences":["Quantifying prediction uncertainty when applying object detection models to new, unlabeled datasets is critical in applied machine learning.","This study introduces an approach to estimate the performance of deep learning-based object detection models for quantifying defects in transmission electron microscopy (TEM) images, focusing on detecting irradiation-induced cavities in TEM images of metal alloys.","We developed a random forest regression model that predicts the object detection F1 score, a statistical metric used to evaluate the ability to accurately locate and classify objects of interest.","The random forest model uses features extracted from the predictions of the object detection model whose uncertainty is being quantified, enabling fast prediction on new, unlabeled images.","The mean absolute error (MAE) for predicting F1 of the trained model on test data is 0.09, and the $R^2$ score is 0.77, indicating there is a significant correlation between the random forest regression model predicted and true defect detection F1 scores.","The approach is shown to be robust across three distinct TEM image datasets with varying imaging and material domains.","Our approach enables users to estimate the reliability of a defect detection and segmentation model predictions and assess the applicability of the model to their specific datasets, providing valuable information about possible domain shifts and whether the model needs to be fine-tuned or trained on additional data to be maximally effective for the desired use case."],"url":"http://arxiv.org/abs/2501.08465v1"}
{"created":"2025-01-14 22:20:55","title":"Time series forecasting for multidimensional telemetry data using GAN and BiLSTM in a Digital Twin","abstract":"The research related to digital twins has been increasing in recent years. Besides the mirroring of the physical word into the digital, there is the need of providing services related to the data collected and transferred to the virtual world. One of these services is the forecasting of physical part future behavior, that could lead to applications, like preventing harmful events or designing improvements to get better performance. One strategy used to predict any system operation it is the use of time series models like ARIMA or LSTM, and improvements were implemented using these algorithms. Recently, deep learning techniques based on generative models such as Generative Adversarial Networks (GANs) have been proposed to create time series and the use of LSTM has gained more relevance in time series forecasting, but both have limitations that restrict the forecasting results. Another issue found in the literature is the challenge of handling multivariate environments/applications in time series generation. Therefore, new methods need to be studied in order to fill these gaps and, consequently, provide better resources for creating useful digital twins. In this proposal, it is going to be studied the integration of a BiLSTM layer with a time series obtained by GAN in order to improve the forecasting of all the features provided by the dataset in terms of accuracy and, consequently, improving behaviour prediction.","sentences":["The research related to digital twins has been increasing in recent years.","Besides the mirroring of the physical word into the digital, there is the need of providing services related to the data collected and transferred to the virtual world.","One of these services is the forecasting of physical part future behavior, that could lead to applications, like preventing harmful events or designing improvements to get better performance.","One strategy used to predict any system operation it is the use of time series models like ARIMA or LSTM, and improvements were implemented using these algorithms.","Recently, deep learning techniques based on generative models such as Generative Adversarial Networks (GANs) have been proposed to create time series and the use of LSTM has gained more relevance in time series forecasting, but both have limitations that restrict the forecasting results.","Another issue found in the literature is the challenge of handling multivariate environments/applications in time series generation.","Therefore, new methods need to be studied in order to fill these gaps and, consequently, provide better resources for creating useful digital twins.","In this proposal, it is going to be studied the integration of a BiLSTM layer with a time series obtained by GAN in order to improve the forecasting of all the features provided by the dataset in terms of accuracy and, consequently, improving behaviour prediction."],"url":"http://arxiv.org/abs/2501.08464v1"}
{"created":"2025-01-14 22:02:38","title":"Large Language Models For Text Classification: Case Study And Comprehensive Review","abstract":"Unlocking the potential of Large Language Models (LLMs) in data classification represents a promising frontier in natural language processing. In this work, we evaluate the performance of different LLMs in comparison with state-of-the-art deep-learning and machine-learning models, in two different classification scenarios: i) the classification of employees' working locations based on job reviews posted online (multiclass classification), and 2) the classification of news articles as fake or not (binary classification). Our analysis encompasses a diverse range of language models differentiating in size, quantization, and architecture. We explore the impact of alternative prompting techniques and evaluate the models based on the weighted F1-score. Also, we examine the trade-off between performance (F1-score) and time (inference response time) for each language model to provide a more nuanced understanding of each model's practical applicability. Our work reveals significant variations in model responses based on the prompting strategies. We find that LLMs, particularly Llama3 and GPT-4, can outperform traditional methods in complex classification tasks, such as multiclass classification, though at the cost of longer inference times. In contrast, simpler ML models offer better performance-to-time trade-offs in simpler binary classification tasks.","sentences":["Unlocking the potential of Large Language Models (LLMs) in data classification represents a promising frontier in natural language processing.","In this work, we evaluate the performance of different LLMs in comparison with state-of-the-art deep-learning and machine-learning models, in two different classification scenarios: i) the classification of employees' working locations based on job reviews posted online (multiclass classification), and 2) the classification of news articles as fake or not (binary classification).","Our analysis encompasses a diverse range of language models differentiating in size, quantization, and architecture.","We explore the impact of alternative prompting techniques and evaluate the models based on the weighted F1-score.","Also, we examine the trade-off between performance (F1-score) and time (inference response time) for each language model to provide a more nuanced understanding of each model's practical applicability.","Our work reveals significant variations in model responses based on the prompting strategies.","We find that LLMs, particularly Llama3 and GPT-4, can outperform traditional methods in complex classification tasks, such as multiclass classification, though at the cost of longer inference times.","In contrast, simpler ML models offer better performance-to-time trade-offs in simpler binary classification tasks."],"url":"http://arxiv.org/abs/2501.08457v1"}
{"created":"2025-01-14 21:55:37","title":"Tag&Tab: Pretraining Data Detection in Large Language Models Using Keyword-Based Membership Inference Attack","abstract":"Large language models (LLMs) have become essential digital task assistance tools. Their training relies heavily on the collection of vast amounts of data, which may include copyright-protected or sensitive information. Recent studies on the detection of pretraining data in LLMs have primarily focused on sentence-level or paragraph-level membership inference attacks (MIAs), usually involving probability analysis of the target model prediction tokens. However, the proposed methods often demonstrate poor performance, specifically in terms of accuracy, failing to account for the semantic importance of textual content and word significance. To address these shortcomings, we propose Tag&Tab, a novel approach for detecting data that has been used as part of the LLM pretraining. Our method leverages advanced natural language processing (NLP) techniques to tag keywords in the input text - a process we term Tagging. Then, the LLM is used to obtain the probabilities of these keywords and calculate their average log-likelihood to determine input text membership, a process we refer to as Tabbing. Our experiments on three benchmark datasets (BookMIA, MIMIR, and the Pile) and several open-source LLMs of varying sizes demonstrate an average increase in the AUC scores ranging from 4.1% to 12.1% over state-of-the-art methods. Tag&Tab not only sets a new standard for data leakage detection in LLMs, but its outstanding performance is a testament to the importance of words in MIAs on LLMs.","sentences":["Large language models (LLMs) have become essential digital task assistance tools.","Their training relies heavily on the collection of vast amounts of data, which may include copyright-protected or sensitive information.","Recent studies on the detection of pretraining data in LLMs have primarily focused on sentence-level or paragraph-level membership inference attacks (MIAs), usually involving probability analysis of the target model prediction tokens.","However, the proposed methods often demonstrate poor performance, specifically in terms of accuracy, failing to account for the semantic importance of textual content and word significance.","To address these shortcomings, we propose Tag&Tab, a novel approach for detecting data that has been used as part of the LLM pretraining.","Our method leverages advanced natural language processing (NLP) techniques to tag keywords in the input text - a process we term Tagging.","Then, the LLM is used to obtain the probabilities of these keywords and calculate their average log-likelihood to determine input text membership, a process we refer to as Tabbing.","Our experiments on three benchmark datasets (BookMIA, MIMIR, and the Pile) and several open-source LLMs of varying sizes demonstrate an average increase in the AUC scores ranging from 4.1% to 12.1% over state-of-the-art methods.","Tag&Tab not only sets a new standard for data leakage detection in LLMs, but its outstanding performance is a testament to the importance of words in MIAs on LLMs."],"url":"http://arxiv.org/abs/2501.08454v1"}
{"created":"2025-01-14 21:53:11","title":"Vchitect-2.0: Parallel Transformer for Scaling Up Video Diffusion Models","abstract":"We present Vchitect-2.0, a parallel transformer architecture designed to scale up video diffusion models for large-scale text-to-video generation. The overall Vchitect-2.0 system has several key designs. (1) By introducing a novel Multimodal Diffusion Block, our approach achieves consistent alignment between text descriptions and generated video frames, while maintaining temporal coherence across sequences. (2) To overcome memory and computational bottlenecks, we propose a Memory-efficient Training framework that incorporates hybrid parallelism and other memory reduction techniques, enabling efficient training of long video sequences on distributed systems. (3) Additionally, our enhanced data processing pipeline ensures the creation of Vchitect T2V DataVerse, a high-quality million-scale training dataset through rigorous annotation and aesthetic evaluation. Extensive benchmarking demonstrates that Vchitect-2.0 outperforms existing methods in video quality, training efficiency, and scalability, serving as a suitable base for high-fidelity video generation.","sentences":["We present Vchitect-2.0, a parallel transformer architecture designed to scale up video diffusion models for large-scale text-to-video generation.","The overall Vchitect-2.0 system has several key designs.","(1) By introducing a novel Multimodal Diffusion Block, our approach achieves consistent alignment between text descriptions and generated video frames, while maintaining temporal coherence across sequences.","(2) To overcome memory and computational bottlenecks, we propose a Memory-efficient Training framework that incorporates hybrid parallelism and other memory reduction techniques, enabling efficient training of long video sequences on distributed systems.","(3) Additionally, our enhanced data processing pipeline ensures the creation of Vchitect T2V DataVerse, a high-quality million-scale training dataset through rigorous annotation and aesthetic evaluation.","Extensive benchmarking demonstrates that Vchitect-2.0 outperforms existing methods in video quality, training efficiency, and scalability, serving as a suitable base for high-fidelity video generation."],"url":"http://arxiv.org/abs/2501.08453v1"}
{"created":"2025-01-14 21:38:01","title":"A Refreshment Stirred, Not Shaken (II): Invariant-Preserving Deployments of Differential Privacy for the US Decennial Census","abstract":"Through the lens of the system of differential privacy specifications developed in Part I of a trio of articles, this second paper examines two statistical disclosure control (SDC) methods for the United States Decennial Census: the Permutation Swapping Algorithm (PSA), which is similar to the 2010 Census's disclosure avoidance system (DAS), and the TopDown Algorithm (TDA), which was used in the 2020 DAS. To varying degrees, both methods leave unaltered some statistics of the confidential data $\\unicode{x2013}$ which are called the method's invariants $\\unicode{x2013}$ and hence neither can be readily reconciled with differential privacy (DP), at least as it was originally conceived. Nevertheless, we establish that the PSA satisfies $\\varepsilon$-DP subject to the invariants it necessarily induces, thereby showing that this traditional SDC method can in fact still be understood within our more-general system of DP specifications. By a similar modification to $\\rho$-zero concentrated DP, we also provide a DP specification for the TDA. Finally, as a point of comparison, we consider the counterfactual scenario in which the PSA was adopted for the 2020 Census, resulting in a reduction in the nominal privacy loss, but at the cost of releasing many more invariants. Therefore, while our results explicate the mathematical guarantees of SDC provided by the PSA, the TDA and the 2020 DAS in general, care must be taken in their translation to actual privacy protection $\\unicode{x2013}$ just as is the case for any DP deployment.","sentences":["Through the lens of the system of differential privacy specifications developed in Part I of a trio of articles, this second paper examines two statistical disclosure control (SDC) methods for the United States Decennial Census: the Permutation Swapping Algorithm (PSA), which is similar to the 2010 Census's disclosure avoidance system (DAS), and the TopDown Algorithm (TDA), which was used in the 2020 DAS.","To varying degrees, both methods leave unaltered some statistics of the confidential data $\\unicode{x2013}$ which are called the method's invariants $\\unicode{x2013}$ and hence neither can be readily reconciled with differential privacy (DP), at least as it was originally conceived.","Nevertheless, we establish that the PSA satisfies $\\varepsilon$-DP subject to the invariants it necessarily induces, thereby showing that this traditional SDC method can in fact still be understood within our more-general system of DP specifications.","By a similar modification to $\\rho$-zero concentrated DP, we also provide a DP specification for the TDA.","Finally, as a point of comparison, we consider the counterfactual scenario in which the PSA was adopted for the 2020 Census, resulting in a reduction in the nominal privacy loss, but at the cost of releasing many more invariants.","Therefore, while our results explicate the mathematical guarantees of SDC provided by the PSA, the TDA and the 2020 DAS in general, care must be taken in their translation to actual privacy protection $\\unicode{x2013}$ just as is the case for any DP deployment."],"url":"http://arxiv.org/abs/2501.08449v1"}
{"created":"2025-01-14 21:34:34","title":"Poseidon: A ViT-based Architecture for Multi-Frame Pose Estimation with Adaptive Frame Weighting and Multi-Scale Feature Fusion","abstract":"Human pose estimation, a vital task in computer vision, involves detecting and localising human joints in images and videos. While single-frame pose estimation has seen significant progress, it often fails to capture the temporal dynamics for understanding complex, continuous movements. We propose Poseidon, a novel multi-frame pose estimation architecture that extends the ViTPose model by integrating temporal information for enhanced accuracy and robustness to address these limitations. Poseidon introduces key innovations: (1) an Adaptive Frame Weighting (AFW) mechanism that dynamically prioritises frames based on their relevance, ensuring that the model focuses on the most informative data; (2) a Multi-Scale Feature Fusion (MSFF) module that aggregates features from different backbone layers to capture both fine-grained details and high-level semantics; and (3) a Cross-Attention module for effective information exchange between central and contextual frames, enhancing the model's temporal coherence. The proposed architecture improves performance in complex video scenarios and offers scalability and computational efficiency suitable for real-world applications. Our approach achieves state-of-the-art performance on the PoseTrack21 and PoseTrack18 datasets, achieving mAP scores of 88.3 and 87.8, respectively, outperforming existing methods.","sentences":["Human pose estimation, a vital task in computer vision, involves detecting and localising human joints in images and videos.","While single-frame pose estimation has seen significant progress, it often fails to capture the temporal dynamics for understanding complex, continuous movements.","We propose Poseidon, a novel multi-frame pose estimation architecture that extends the ViTPose model by integrating temporal information for enhanced accuracy and robustness to address these limitations.","Poseidon introduces key innovations: (1) an Adaptive Frame Weighting (AFW) mechanism that dynamically prioritises frames based on their relevance, ensuring that the model focuses on the most informative data; (2) a Multi-Scale Feature Fusion (MSFF) module that aggregates features from different backbone layers to capture both fine-grained details and high-level semantics; and (3) a Cross-Attention module for effective information exchange between central and contextual frames, enhancing the model's temporal coherence.","The proposed architecture improves performance in complex video scenarios and offers scalability and computational efficiency suitable for real-world applications.","Our approach achieves state-of-the-art performance on the PoseTrack21 and PoseTrack18 datasets, achieving mAP scores of 88.3 and 87.8, respectively, outperforming existing methods."],"url":"http://arxiv.org/abs/2501.08446v1"}
{"created":"2025-01-14 20:44:17","title":"Physics-informed neural networks for phase-resolved data assimilation and prediction of nonlinear ocean waves","abstract":"The assimilation and prediction of phase-resolved surface gravity waves are critical challenges in ocean science and engineering. Potential flow theory (PFT) has been widely employed to develop wave models and numerical techniques for wave prediction. However, traditional wave prediction methods are often limited. For example, most simplified wave models have a limited ability to capture strong wave nonlinearity, while fully nonlinear PFT solvers often fail to meet the speed requirements of engineering applications. This computational inefficiency also hinders the development of effective data assimilation techniques, which are required to reconstruct spatial wave information from sparse measurements to initialize the wave prediction. To address these challenges, we propose a novel solver method that leverages physics-informed neural networks (PINNs) that parameterize PFT solutions as neural networks. This provides a computationally inexpensive way to assimilate and predict wave data. The proposed PINN framework is validated through comparisons with analytical linear PFT solutions and experimental data collected in a laboratory wave flume. The results demonstrate that our approach accurately captures and predicts irregular, nonlinear, and dispersive wave surface dynamics. Moreover, the PINN can infer the fully nonlinear velocity potential throughout the entire fluid volume solely from surface elevation measurements, enabling the calculation of fluid velocities that are difficult to measure experimentally.","sentences":["The assimilation and prediction of phase-resolved surface gravity waves are critical challenges in ocean science and engineering.","Potential flow theory (PFT) has been widely employed to develop wave models and numerical techniques for wave prediction.","However, traditional wave prediction methods are often limited.","For example, most simplified wave models have a limited ability to capture strong wave nonlinearity, while fully nonlinear PFT solvers often fail to meet the speed requirements of engineering applications.","This computational inefficiency also hinders the development of effective data assimilation techniques, which are required to reconstruct spatial wave information from sparse measurements to initialize the wave prediction.","To address these challenges, we propose a novel solver method that leverages physics-informed neural networks (PINNs) that parameterize PFT solutions as neural networks.","This provides a computationally inexpensive way to assimilate and predict wave data.","The proposed PINN framework is validated through comparisons with analytical linear PFT solutions and experimental data collected in a laboratory wave flume.","The results demonstrate that our approach accurately captures and predicts irregular, nonlinear, and dispersive wave surface dynamics.","Moreover, the PINN can infer the fully nonlinear velocity potential throughout the entire fluid volume solely from surface elevation measurements, enabling the calculation of fluid velocities that are difficult to measure experimentally."],"url":"http://arxiv.org/abs/2501.08430v1"}
{"created":"2025-01-14 20:38:30","title":"Physics-Informed Latent Neural Operator for Real-time Predictions of Complex Physical Systems","abstract":"Deep operator network (DeepONet) has shown great promise as a surrogate model for systems governed by partial differential equations (PDEs), learning mappings between infinite-dimensional function spaces with high accuracy. However, achieving low generalization errors often requires highly overparameterized networks, posing significant challenges for large-scale, complex systems. To address these challenges, latent DeepONet was proposed, introducing a two-step approach: first, a reduced-order model is used to learn a low-dimensional latent space, followed by operator learning on this latent space. While effective, this method is inherently data-driven, relying on large datasets and making it difficult to incorporate governing physics into the framework. Additionally, the decoupled nature of these steps prevents end-to-end optimization and the ability to handle data scarcity. This work introduces PI-Latent-NO, a physics-informed latent operator learning framework that overcomes these limitations. Our architecture employs two coupled DeepONets in an end-to-end training scheme: the first, termed Latent-DeepONet, identifies and learns the low-dimensional latent space, while the second, Reconstruction-DeepONet, maps the latent representations back to the original physical space. By integrating governing physics directly into the training process, our approach requires significantly fewer data samples while achieving high accuracy. Furthermore, the framework is computationally and memory efficient, exhibiting nearly constant scaling behavior on a single GPU and demonstrating the potential for further efficiency gains with distributed training. We validate the proposed method on high-dimensional parametric PDEs, demonstrating its effectiveness as a proof of concept and its potential scalability for large-scale systems.","sentences":["Deep operator network (DeepONet) has shown great promise as a surrogate model for systems governed by partial differential equations (PDEs), learning mappings between infinite-dimensional function spaces with high accuracy.","However, achieving low generalization errors often requires highly overparameterized networks, posing significant challenges for large-scale, complex systems.","To address these challenges, latent DeepONet was proposed, introducing a two-step approach: first, a reduced-order model is used to learn a low-dimensional latent space, followed by operator learning on this latent space.","While effective, this method is inherently data-driven, relying on large datasets and making it difficult to incorporate governing physics into the framework.","Additionally, the decoupled nature of these steps prevents end-to-end optimization and the ability to handle data scarcity.","This work introduces PI-Latent-NO, a physics-informed latent operator learning framework that overcomes these limitations.","Our architecture employs two coupled DeepONets in an end-to-end training scheme: the first, termed Latent-DeepONet, identifies and learns the low-dimensional latent space, while the second, Reconstruction-DeepONet, maps the latent representations back to the original physical space.","By integrating governing physics directly into the training process, our approach requires significantly fewer data samples while achieving high accuracy.","Furthermore, the framework is computationally and memory efficient, exhibiting nearly constant scaling behavior on a single GPU and demonstrating the potential for further efficiency gains with distributed training.","We validate the proposed method on high-dimensional parametric PDEs, demonstrating its effectiveness as a proof of concept and its potential scalability for large-scale systems."],"url":"http://arxiv.org/abs/2501.08428v1"}
{"created":"2025-01-14 20:38:15","title":"Causal vs. Anticausal merging of predictors","abstract":"We study the differences arising from merging predictors in the causal and anticausal directions using the same data. In particular we study the asymmetries that arise in a simple model where we merge the predictors using one binary variable as target and two continuous variables as predictors. We use Causal Maximum Entropy (CMAXENT) as inductive bias to merge the predictors, however, we expect similar differences to hold also when we use other merging methods that take into account asymmetries between cause and effect. We show that if we observe all bivariate distributions, the CMAXENT solution reduces to a logistic regression in the causal direction and Linear Discriminant Analysis (LDA) in the anticausal direction. Furthermore, we study how the decision boundaries of these two solutions differ whenever we observe only some of the bivariate distributions implications for Out-Of-Variable (OOV) generalisation.","sentences":["We study the differences arising from merging predictors in the causal and anticausal directions using the same data.","In particular we study the asymmetries that arise in a simple model where we merge the predictors using one binary variable as target and two continuous variables as predictors.","We use Causal Maximum Entropy (CMAXENT) as inductive bias to merge the predictors, however, we expect similar differences to hold also when we use other merging methods that take into account asymmetries between cause and effect.","We show that if we observe all bivariate distributions, the CMAXENT solution reduces to a logistic regression in the causal direction and Linear Discriminant Analysis (LDA) in the anticausal direction.","Furthermore, we study how the decision boundaries of these two solutions differ whenever we observe only some of the bivariate distributions implications for Out-Of-Variable (OOV) generalisation."],"url":"http://arxiv.org/abs/2501.08426v1"}
{"created":"2025-01-14 20:08:16","title":"Ensemble of Large Language Models for Curated Labeling and Rating of Free-text Data","abstract":"Free-text responses are commonly collected in psychological studies, providing rich qualitative insights that quantitative measures may not capture. Labeling curated topics of research interest in free-text data by multiple trained human coders is typically labor-intensive and time-consuming. Though large language models (LLMs) excel in language processing, LLM-assisted labeling techniques relying on closed-source LLMs cannot be directly applied to free-text data, without explicit consent for external use.   In this study, we propose a framework of assembling locally-deployable LLMs to enhance the labeling of predetermined topics in free-text data under privacy constraints. Analogous to annotation by multiple human raters, this framework leverages the heterogeneity of diverse open-source LLMs. The ensemble approach seeks a balance between the agreement and disagreement across LLMs, guided by a relevancy scoring methodology that utilizes embedding distances between topic descriptions and LLMs' reasoning. We evaluated the ensemble approach using both publicly accessible Reddit data from eating disorder related forums, and free-text responses from eating disorder patients, both complemented by human annotations.   We found that: (1) there is heterogeneity in the performance of labeling among same-sized LLMs, with some showing low sensitivity but high precision, while others exhibit high sensitivity but low precision. (2) Compared to individual LLMs, the ensemble of LLMs achieved the highest accuracy and optimal precision-sensitivity trade-off in predicting human annotations. (3) The relevancy scores across LLMs showed greater agreement than dichotomous labels, indicating that the relevancy scoring method effectively mitigates the heterogeneity in LLMs' labeling.","sentences":["Free-text responses are commonly collected in psychological studies, providing rich qualitative insights that quantitative measures may not capture.","Labeling curated topics of research interest in free-text data by multiple trained human coders is typically labor-intensive and time-consuming.","Though large language models (LLMs) excel in language processing, LLM-assisted labeling techniques relying on closed-source LLMs cannot be directly applied to free-text data, without explicit consent for external use.   ","In this study, we propose a framework of assembling locally-deployable LLMs to enhance the labeling of predetermined topics in free-text data under privacy constraints.","Analogous to annotation by multiple human raters, this framework leverages the heterogeneity of diverse open-source LLMs.","The ensemble approach seeks a balance between the agreement and disagreement across LLMs, guided by a relevancy scoring methodology that utilizes embedding distances between topic descriptions and LLMs' reasoning.","We evaluated the ensemble approach using both publicly accessible Reddit data from eating disorder related forums, and free-text responses from eating disorder patients, both complemented by human annotations.   ","We found that: (1) there is heterogeneity in the performance of labeling among same-sized LLMs, with some showing low sensitivity but high precision, while others exhibit high sensitivity but low precision.","(2) Compared to individual LLMs, the ensemble of LLMs achieved the highest accuracy and optimal precision-sensitivity trade-off in predicting human annotations.","(3) The relevancy scores across LLMs showed greater agreement than dichotomous labels, indicating that the relevancy scoring method effectively mitigates the heterogeneity in LLMs' labeling."],"url":"http://arxiv.org/abs/2501.08413v1"}
{"created":"2025-01-14 19:59:59","title":"BiDepth Multimodal Neural Network: Bidirectional Depth Deep Learning Arcitecture for Spatial-Temporal Prediction","abstract":"Accurate prediction of spatial-temporal (ST) information in dynamic systems, such as urban mobility and weather patterns, is a crucial yet challenging problem. The complexity stems from the intricate interplay between spatial proximity and temporal relevance, where both long-term trends and short-term fluctuations are present in convoluted patterns. Existing approaches, including traditional statistical methods and conventional neural networks, may provide inaccurate results due to the lack of an effective mechanism that simultaneously incorporates information at variable temporal depths while maintaining spatial context, resulting in a trade-off between comprehensive long-term historical analysis and responsiveness to short-term new information. To bridge this gap, this paper proposes the BiDepth Multimodal Neural Network (BDMNN) with bidirectional depth modulation that enables a comprehensive understanding of both long-term seasonality and short-term fluctuations, adapting to the complex ST context. Case studies with real-world public data demonstrate significant improvements in prediction accuracy, with a 12% reduction in Mean Squared Error for urban traffic prediction and a 15% improvement in rain precipitation forecasting compared to state-of-the-art benchmarks, without demanding extra computational resources.","sentences":["Accurate prediction of spatial-temporal (ST) information in dynamic systems, such as urban mobility and weather patterns, is a crucial yet challenging problem.","The complexity stems from the intricate interplay between spatial proximity and temporal relevance, where both long-term trends and short-term fluctuations are present in convoluted patterns.","Existing approaches, including traditional statistical methods and conventional neural networks, may provide inaccurate results due to the lack of an effective mechanism that simultaneously incorporates information at variable temporal depths while maintaining spatial context, resulting in a trade-off between comprehensive long-term historical analysis and responsiveness to short-term new information.","To bridge this gap, this paper proposes the BiDepth Multimodal Neural Network (BDMNN) with bidirectional depth modulation that enables a comprehensive understanding of both long-term seasonality and short-term fluctuations, adapting to the complex ST context.","Case studies with real-world public data demonstrate significant improvements in prediction accuracy, with a 12% reduction in Mean Squared Error for urban traffic prediction and a 15% improvement in rain precipitation forecasting compared to state-of-the-art benchmarks, without demanding extra computational resources."],"url":"http://arxiv.org/abs/2501.08411v1"}
{"created":"2025-01-14 19:56:43","title":"Leveraging 2D Masked Reconstruction for Domain Adaptation of 3D Pose Estimation","abstract":"RGB-based 3D pose estimation methods have been successful with the development of deep learning and the emergence of high-quality 3D pose datasets. However, most existing methods do not operate well for testing images whose distribution is far from that of training data. However, most existing methods do not operate well for testing images whose distribution is far from that of training data. This problem might be alleviated by involving diverse data during training, however it is non-trivial to collect such diverse data with corresponding labels (i.e. 3D pose). In this paper, we introduced an unsupervised domain adaptation framework for 3D pose estimation that utilizes the unlabeled data in addition to labeled data via masked image modeling (MIM) framework. Foreground-centric reconstruction and attention regularization are further proposed to increase the effectiveness of unlabeled data usage. Experiments are conducted on the various datasets in human and hand pose estimation tasks, especially using the cross-domain scenario. We demonstrated the effectiveness of ours by achieving the state-of-the-art accuracy on all datasets.","sentences":["RGB-based 3D pose estimation methods have been successful with the development of deep learning and the emergence of high-quality 3D pose datasets.","However, most existing methods do not operate well for testing images whose distribution is far from that of training data.","However, most existing methods do not operate well for testing images whose distribution is far from that of training data.","This problem might be alleviated by involving diverse data during training, however it is non-trivial to collect such diverse data with corresponding labels (i.e. 3D pose).","In this paper, we introduced an unsupervised domain adaptation framework for 3D pose estimation that utilizes the unlabeled data in addition to labeled data via masked image modeling (MIM) framework.","Foreground-centric reconstruction and attention regularization are further proposed to increase the effectiveness of unlabeled data usage.","Experiments are conducted on the various datasets in human and hand pose estimation tasks, especially using the cross-domain scenario.","We demonstrated the effectiveness of ours by achieving the state-of-the-art accuracy on all datasets."],"url":"http://arxiv.org/abs/2501.08408v1"}
{"created":"2025-01-14 19:37:07","title":"Navigating Gender Disparities in Communication Research Leadership: Academic Recognition, Career Development, and Compensation","abstract":"This study examines gender disparities in communication research through citation metrics, authorship patterns, team composition, and faculty salaries. Using data from 62,359 papers across 121 communication journals, we find that while female authors are increasingly represented, citation gaps persist, with sole-authored papers by women receiving fewer citations than those by men, especially in smaller teams. Team composition analysis reveals a tendency toward gender homophily, with single-gender teams being more common. In top U.S. communication journals, female authors face underrepresentation and citation disparities favoring male authors. Salary analysis from leading U.S. public universities shows that female faculty earn lower salaries at the Assistant Professor level, though disparities lessen at higher ranks. These findings highlight the need for greater efforts to promote gender equity through inclusive collaboration, equitable citation practices, and fair compensation.","sentences":["This study examines gender disparities in communication research through citation metrics, authorship patterns, team composition, and faculty salaries.","Using data from 62,359 papers across 121 communication journals, we find that while female authors are increasingly represented, citation gaps persist, with sole-authored papers by women receiving fewer citations than those by men, especially in smaller teams.","Team composition analysis reveals a tendency toward gender homophily, with single-gender teams being more common.","In top U.S. communication journals, female authors face underrepresentation and citation disparities favoring male authors.","Salary analysis from leading U.S. public universities shows that female faculty earn lower salaries at the Assistant Professor level, though disparities lessen at higher ranks.","These findings highlight the need for greater efforts to promote gender equity through inclusive collaboration, equitable citation practices, and fair compensation."],"url":"http://arxiv.org/abs/2501.08401v1"}
{"created":"2025-01-14 18:40:33","title":"3D Gaussian Splatting with Normal Information for Mesh Extraction and Improved Rendering","abstract":"Differentiable 3D Gaussian splatting has emerged as an efficient and flexible rendering technique for representing complex scenes from a collection of 2D views and enabling high-quality real-time novel-view synthesis. However, its reliance on photometric losses can lead to imprecisely reconstructed geometry and extracted meshes, especially in regions with high curvature or fine detail. We propose a novel regularization method using the gradients of a signed distance function estimated from the Gaussians, to improve the quality of rendering while also extracting a surface mesh. The regularizing normal supervision facilitates better rendering and mesh reconstruction, which is crucial for downstream applications in video generation, animation, AR-VR and gaming. We demonstrate the effectiveness of our approach on datasets such as Mip-NeRF360, Tanks and Temples, and Deep-Blending. Our method scores higher on photorealism metrics compared to other mesh extracting rendering methods without compromising mesh quality.","sentences":["Differentiable 3D Gaussian splatting has emerged as an efficient and flexible rendering technique for representing complex scenes from a collection of 2D views and enabling high-quality real-time novel-view synthesis.","However, its reliance on photometric losses can lead to imprecisely reconstructed geometry and extracted meshes, especially in regions with high curvature or fine detail.","We propose a novel regularization method using the gradients of a signed distance function estimated from the Gaussians, to improve the quality of rendering while also extracting a surface mesh.","The regularizing normal supervision facilitates better rendering and mesh reconstruction, which is crucial for downstream applications in video generation, animation, AR-VR and gaming.","We demonstrate the effectiveness of our approach on datasets such as Mip-NeRF360, Tanks and Temples, and Deep-Blending.","Our method scores higher on photorealism metrics compared to other mesh extracting rendering methods without compromising mesh quality."],"url":"http://arxiv.org/abs/2501.08370v1"}
