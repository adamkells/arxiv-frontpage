{"created":"2025-02-20 18:59:42","title":"Benchmarking Multimodal RAG through a Chart-based Document Question-Answering Generation Framework","abstract":"Multimodal Retrieval-Augmented Generation (MRAG) enhances reasoning capabilities by integrating external knowledge. However, existing benchmarks primarily focus on simple image-text interactions, overlooking complex visual formats like charts that are prevalent in real-world applications. In this work, we introduce a novel task, Chart-based MRAG, to address this limitation. To semi-automatically generate high-quality evaluation samples, we propose CHARt-based document question-answering GEneration (CHARGE), a framework that produces evaluation data through structured keypoint extraction, crossmodal verification, and keypoint-based generation. By combining CHARGE with expert validation, we construct Chart-MRAG Bench, a comprehensive benchmark for chart-based MRAG evaluation, featuring 4,738 question-answering pairs across 8 domains from real-world documents. Our evaluation reveals three critical limitations in current approaches: (1) unified multimodal embedding retrieval methods struggles in chart-based scenarios, (2) even with ground-truth retrieval, state-of-the-art MLLMs achieve only 58.19% Correctness and 73.87% Coverage scores, and (3) MLLMs demonstrate consistent text-over-visual modality bias during Chart-based MRAG reasoning. The CHARGE and Chart-MRAG Bench are released at https://github.com/Nomothings/CHARGE.git.","sentences":["Multimodal Retrieval-Augmented Generation (MRAG) enhances reasoning capabilities by integrating external knowledge.","However, existing benchmarks primarily focus on simple image-text interactions, overlooking complex visual formats like charts that are prevalent in real-world applications.","In this work, we introduce a novel task, Chart-based MRAG, to address this limitation.","To semi-automatically generate high-quality evaluation samples, we propose CHARt-based document question-answering GEneration (CHARGE), a framework that produces evaluation data through structured keypoint extraction, crossmodal verification, and keypoint-based generation.","By combining CHARGE with expert validation, we construct Chart-MRAG Bench, a comprehensive benchmark for chart-based MRAG evaluation, featuring 4,738 question-answering pairs across 8 domains from real-world documents.","Our evaluation reveals three critical limitations in current approaches: (1) unified multimodal embedding retrieval methods struggles in chart-based scenarios, (2) even with ground-truth retrieval, state-of-the-art MLLMs achieve only 58.19% Correctness and 73.87% Coverage scores, and (3) MLLMs demonstrate consistent text-over-visual modality bias during Chart-based MRAG reasoning.","The CHARGE and Chart-MRAG Bench are released at https://github.com/Nomothings/CHARGE.git."],"url":"http://arxiv.org/abs/2502.14864v1"}
{"created":"2025-02-20 18:58:07","title":"Prompt-to-Leaderboard","abstract":"Large language model (LLM) evaluations typically rely on aggregated metrics like accuracy or human preference, averaging across users and prompts. This averaging obscures user- and prompt-specific variations in model performance. To address this, we propose Prompt-to-Leaderboard (P2L), a method that produces leaderboards specific to a prompt. The core idea is to train an LLM taking natural language prompts as input to output a vector of Bradley-Terry coefficients which are then used to predict the human preference vote. The resulting prompt-dependent leaderboards allow for unsupervised task-specific evaluation, optimal routing of queries to models, personalization, and automated evaluation of model strengths and weaknesses. Data from Chatbot Arena suggest that P2L better captures the nuanced landscape of language model performance than the averaged leaderboard. Furthermore, our findings suggest that P2L's ability to produce prompt-specific evaluations follows a power law scaling similar to that observed in LLMs themselves. In January 2025, the router we trained based on this methodology achieved the \\#1 spot in the Chatbot Arena leaderboard. Our code is available at this GitHub link: https://github.com/lmarena/p2l.","sentences":["Large language model (LLM) evaluations typically rely on aggregated metrics like accuracy or human preference, averaging across users and prompts.","This averaging obscures user- and prompt-specific variations in model performance.","To address this, we propose Prompt-to-Leaderboard (P2L), a method that produces leaderboards specific to a prompt.","The core idea is to train an LLM taking natural language prompts as input to output a vector of Bradley-Terry coefficients which are then used to predict the human preference vote.","The resulting prompt-dependent leaderboards allow for unsupervised task-specific evaluation, optimal routing of queries to models, personalization, and automated evaluation of model strengths and weaknesses.","Data from Chatbot Arena suggest that P2L better captures the nuanced landscape of language model performance than the averaged leaderboard.","Furthermore, our findings suggest that P2L's ability to produce prompt-specific evaluations follows a power law scaling similar to that observed in LLMs themselves.","In January 2025, the router we trained based on this methodology achieved the \\#1 spot in the Chatbot Arena leaderboard.","Our code is available at this GitHub link: https://github.com/lmarena/p2l."],"url":"http://arxiv.org/abs/2502.14855v1"}
{"created":"2025-02-20 18:58:03","title":"CLIPPER: Compression enables long-context synthetic data generation","abstract":"LLM developers are increasingly reliant on synthetic data, but generating high-quality data for complex long-context reasoning tasks remains challenging. We introduce CLIPPER, a compression-based approach for generating synthetic data tailored to narrative claim verification - a task that requires reasoning over a book to verify a given claim. Instead of generating claims directly from the raw text of the book, which results in artifact-riddled claims, CLIPPER first compresses the book into chapter outlines and book summaries and then uses these intermediate representations to generate complex claims and corresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces claims that are more valid, grounded, and complex. Using CLIPPER, we construct a dataset of 19K synthetic book claims paired with their source texts and chain-of-thought reasoning, and use it to fine-tune three open-weight models. Our best model achieves breakthrough results on narrative claim verification (from 28% to 76% accuracy on our test set) and sets a new state-of-the-art for sub-10B models on the NoCha leaderboard. Further analysis shows that our models generate more detailed and grounded chain-of-thought reasoning while also improving performance on other narrative understanding tasks (e.g., NarrativeQA).","sentences":["LLM developers are increasingly reliant on synthetic data, but generating high-quality data for complex long-context reasoning tasks remains challenging.","We introduce CLIPPER, a compression-based approach for generating synthetic data tailored to narrative claim verification - a task that requires reasoning over a book to verify a given claim.","Instead of generating claims directly from the raw text of the book, which results in artifact-riddled claims, CLIPPER first compresses the book into chapter outlines and book summaries and then uses these intermediate representations to generate complex claims and corresponding chain-of-thoughts.","Compared to naive approaches, CLIPPER produces claims that are more valid, grounded, and complex.","Using CLIPPER, we construct a dataset of 19K synthetic book claims paired with their source texts and chain-of-thought reasoning, and use it to fine-tune three open-weight models.","Our best model achieves breakthrough results on narrative claim verification (from 28% to 76% accuracy on our test set) and sets a new state-of-the-art for sub-10B models on the NoCha leaderboard.","Further analysis shows that our models generate more detailed and grounded chain-of-thought reasoning while also improving performance on other narrative understanding tasks (e.g., NarrativeQA)."],"url":"http://arxiv.org/abs/2502.14854v1"}
{"created":"2025-02-20 18:56:03","title":"GATE: Graph-based Adaptive Tool Evolution Across Diverse Tasks","abstract":"Large Language Models (LLMs) have shown great promise in tool-making, yet existing frameworks often struggle to efficiently construct reliable toolsets and are limited to single-task settings. To address these challenges, we propose GATE (Graph-based Adaptive Tool Evolution), an adaptive framework that dynamically constructs and evolves a hierarchical graph of reusable tools across multiple scenarios. We evaluate GATE on open-ended tasks (Minecraft), agent-based tasks (TextCraft, DABench), and code generation tasks (MATH, Date, TabMWP). Our results show that GATE achieves up to 4.3x faster milestone completion in Minecraft compared to the previous SOTA, and provides an average improvement of 9.23% over existing tool-making methods in code generation tasks and 10.03% in agent tasks. GATE demonstrates the power of adaptive evolution, balancing tool quantity, complexity, and functionality while maintaining high efficiency. Code and data are available at \\url{https://github.com/ayanami2003/GATE}.","sentences":["Large Language Models (LLMs) have shown great promise in tool-making, yet existing frameworks often struggle to efficiently construct reliable toolsets and are limited to single-task settings.","To address these challenges, we propose GATE (Graph-based Adaptive Tool Evolution), an adaptive framework that dynamically constructs and evolves a hierarchical graph of reusable tools across multiple scenarios.","We evaluate GATE on open-ended tasks (Minecraft), agent-based tasks (TextCraft, DABench), and code generation tasks (MATH, Date, TabMWP).","Our results show that GATE achieves up to 4.3x faster milestone completion in Minecraft compared to the previous SOTA, and provides an average improvement of 9.23% over existing tool-making methods in code generation tasks and 10.03% in agent tasks.","GATE demonstrates the power of adaptive evolution, balancing tool quantity, complexity, and functionality while maintaining high efficiency.","Code and data are available at \\url{https://github.com/ayanami2003/GATE}."],"url":"http://arxiv.org/abs/2502.14848v1"}
{"created":"2025-02-20 18:55:30","title":"Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation","abstract":"Reasoning about images with rich text, such as charts and documents, is a critical application of vision-language models (VLMs). However, VLMs often struggle in these domains due to the scarcity of diverse text-rich vision-language data. To address this challenge, we present CoSyn, a framework that leverages the coding capabilities of text-only large language models (LLMs) to automatically create synthetic text-rich multimodal data. Given input text describing a target domain (e.g., \"nutrition fact labels\"), CoSyn prompts an LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic images. With the underlying code as textual representations of the synthetic images, CoSyn can generate high-quality instruction-tuning data, again relying on a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K images and 2.7M rows of vision-language instruction-tuning data. Comprehensive experiments on seven benchmarks demonstrate that models trained on our synthetic data achieve state-of-the-art performance among competitive open-source models, including Llama 3.2, and surpass proprietary models such as GPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing data, enabling VLMs to ground information within input images, showcasing its potential for developing multimodal agents capable of acting in real-world environments.","sentences":["Reasoning about images with rich text, such as charts and documents, is a critical application of vision-language models (VLMs).","However, VLMs often struggle in these domains due to the scarcity of diverse text-rich vision-language data.","To address this challenge, we present CoSyn, a framework that leverages the coding capabilities of text-only large language models (LLMs) to automatically create synthetic text-rich multimodal data.","Given input text describing a target domain (e.g., \"nutrition fact labels\"), CoSyn prompts an LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic images.","With the underlying code as textual representations of the synthetic images, CoSyn can generate high-quality instruction-tuning data, again relying on a text-only LLM.","Using CoSyn, we constructed a dataset comprising 400K images and 2.7M rows of vision-language instruction-tuning data.","Comprehensive experiments on seven benchmarks demonstrate that models trained on our synthetic data achieve state-of-the-art performance among competitive open-source models, including Llama 3.2, and surpass proprietary models such as GPT-4V and Gemini 1.5 Flash.","Furthermore, CoSyn can produce synthetic pointing data, enabling VLMs to ground information within input images, showcasing its potential for developing multimodal agents capable of acting in real-world environments."],"url":"http://arxiv.org/abs/2502.14846v1"}
{"created":"2025-02-20 18:52:24","title":"Spatial Distribution-Shift Aware Knowledge-Guided Machine Learning","abstract":"Given inputs of diverse soil characteristics and climate data gathered from various regions, we aimed to build a model to predict accurate land emissions. The problem is important since accurate quantification of the carbon cycle in agroecosystems is crucial for mitigating climate change and ensuring sustainable food production. Predicting accurate land emissions is challenging since calibrating the heterogeneous nature of soil properties, moisture, and environmental conditions is hard at decision-relevant scales. Traditional approaches do not adequately estimate land emissions due to location-independent parameters failing to leverage the spatial heterogeneity and also require large datasets. To overcome these limitations, we proposed Spatial Distribution-Shift Aware Knowledge-Guided Machine Learning (SDSA-KGML), which leverages location-dependent parameters that account for significant spatial heterogeneity in soil moisture from multiple sites within the same region. Experimental results demonstrate that SDSA-KGML models achieve higher local accuracy for the specified states in the Midwest Region.","sentences":["Given inputs of diverse soil characteristics and climate data gathered from various regions, we aimed to build a model to predict accurate land emissions.","The problem is important since accurate quantification of the carbon cycle in agroecosystems is crucial for mitigating climate change and ensuring sustainable food production.","Predicting accurate land emissions is challenging since calibrating the heterogeneous nature of soil properties, moisture, and environmental conditions is hard at decision-relevant scales.","Traditional approaches do not adequately estimate land emissions due to location-independent parameters failing to leverage the spatial heterogeneity and also require large datasets.","To overcome these limitations, we proposed Spatial Distribution-Shift Aware Knowledge-Guided Machine Learning (SDSA-KGML), which leverages location-dependent parameters that account for significant spatial heterogeneity in soil moisture from multiple sites within the same region.","Experimental results demonstrate that SDSA-KGML models achieve higher local accuracy for the specified states in the Midwest Region."],"url":"http://arxiv.org/abs/2502.14840v1"}
{"created":"2025-02-20 18:51:12","title":"Revealing and Mitigating Over-Attention in Knowledge Editing","abstract":"Large Language Models have demonstrated superior performance across a wide range of tasks, but they still exhibit undesirable errors due to incorrect knowledge learned from the training data. To avoid this, knowledge editing methods emerged to precisely edit the specific model knowledge via efficiently modifying a very small percentage of parameters. % However, those methods can lead to the problem of Specificity Failure: when the content related to the edited knowledge occurs in the context, it can inadvertently corrupt other pre-existing knowledge. However, those methods can lead to the problem of Specificity Failure, where the existing knowledge and capabilities are severely degraded due to editing. Our preliminary indicates that Specificity Failure primarily stems from the model's attention heads assigning excessive attention scores to entities related to the edited knowledge, thereby unduly focusing on specific snippets within the context, which we denote as the Attention Drift phenomenon. To mitigate such Attention Drift issue, we introduce a simple yet effective method Selective Attention Drift Restriction}(SADR), which introduces an additional regularization term during the knowledge editing process to restrict changes in the attention weight distribution, thereby preventing undue focus on the edited entity. Experiments on five frequently used strong LLMs demonstrate the effectiveness of our method, where SADR can significantly mitigate Specificity Failure in the predominant knowledge editing tasks.","sentences":["Large Language Models have demonstrated superior performance across a wide range of tasks, but they still exhibit undesirable errors due to incorrect knowledge learned from the training data.","To avoid this, knowledge editing methods emerged to precisely edit the specific model knowledge via efficiently modifying a very small percentage of parameters.","% However, those methods can lead to the problem of Specificity Failure: when the content related to the edited knowledge occurs in the context, it can inadvertently corrupt other pre-existing knowledge.","However, those methods can lead to the problem of Specificity Failure, where the existing knowledge and capabilities are severely degraded due to editing.","Our preliminary indicates that Specificity Failure primarily stems from the model's attention heads assigning excessive attention scores to entities related to the edited knowledge, thereby unduly focusing on specific snippets within the context, which we denote as the Attention Drift phenomenon.","To mitigate such Attention Drift issue, we introduce a simple yet effective method Selective Attention Drift Restriction}(SADR), which introduces an additional regularization term during the knowledge editing process to restrict changes in the attention weight distribution, thereby preventing undue focus on the edited entity.","Experiments on five frequently used strong LLMs demonstrate the effectiveness of our method, where SADR can significantly mitigate Specificity Failure in the predominant knowledge editing tasks."],"url":"http://arxiv.org/abs/2502.14838v1"}
{"created":"2025-02-20 18:50:42","title":"Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs","abstract":"Multi-head Latent Attention (MLA) is an innovative architecture proposed by DeepSeek, designed to ensure efficient and economical inference by significantly compressing the Key-Value (KV) cache into a latent vector. Compared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its variants such as Grouped-Query Attention (GQA) exhibit significant cost disadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA without pre-training from scratch is both meaningful and challenging. This paper proposes the first data-efficient fine-tuning method for transitioning from MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE, we remove RoPE from dimensions of queries and keys that contribute less to the attention scores, for low-rank approximation, we introduce joint SVD approximations based on the pre-trained parameters of keys and values. These carefully designed strategies enable MHA2MLA to recover performance using only a small fraction (0.3% to 0.6%) of the data, significantly reducing inference costs while seamlessly integrating with compression techniques such as KV cache quantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%, with only a 0.5% drop in LongBench performance.","sentences":["Multi-head Latent Attention (MLA) is an innovative architecture proposed by DeepSeek, designed to ensure efficient and economical inference by significantly compressing the Key-Value (KV) cache into a latent vector.","Compared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its variants such as Grouped-Query Attention (GQA) exhibit significant cost disadvantages.","Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA without pre-training from scratch is both meaningful and challenging.","This paper proposes the first data-efficient fine-tuning method for transitioning from MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE, we remove RoPE from dimensions of queries and keys that contribute less to the attention scores, for low-rank approximation, we introduce joint SVD approximations based on the pre-trained parameters of keys and values.","These carefully designed strategies enable MHA2MLA to recover performance using only a small fraction (0.3% to 0.6%) of the data, significantly reducing inference costs while seamlessly integrating with compression techniques such as KV cache quantization.","For example, the KV cache size of Llama2-7B is reduced by 92.19%, with only a 0.5% drop in LongBench performance."],"url":"http://arxiv.org/abs/2502.14837v1"}
{"created":"2025-02-20 18:47:36","title":"LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models","abstract":"Existing Large Vision-Language Models (LVLMs) can process inputs with context lengths up to 128k visual and text tokens, yet they struggle to generate coherent outputs beyond 1,000 words. We find that the primary limitation is the absence of long output examples during supervised fine-tuning (SFT). To tackle this issue, we introduce LongWriter-V-22k, a SFT dataset comprising 22,158 examples, each with multiple input images, an instruction, and corresponding outputs ranging from 0 to 10,000 words. Moreover, to achieve long outputs that maintain high-fidelity to the input images, we employ Direct Preference Optimization (DPO) to the SFT model. Given the high cost of collecting human feedback for lengthy outputs (e.g., 3,000 words), we propose IterDPO, which breaks long outputs into segments and uses iterative corrections to form preference pairs with the original outputs. Additionally, we develop MMLongBench-Write, a benchmark featuring six tasks to evaluate the long-generation capabilities of VLMs. Our 7B parameter model, trained with LongWriter-V-22k and IterDPO, achieves impressive performance on this benchmark, outperforming larger proprietary models like GPT-4o. Code and data: https://github.com/THU-KEG/LongWriter-V","sentences":["Existing Large Vision-Language Models (LVLMs) can process inputs with context lengths up to 128k visual and text tokens, yet they struggle to generate coherent outputs beyond 1,000 words.","We find that the primary limitation is the absence of long output examples during supervised fine-tuning (SFT).","To tackle this issue, we introduce LongWriter-V-22k, a SFT dataset comprising 22,158 examples, each with multiple input images, an instruction, and corresponding outputs ranging from 0 to 10,000 words.","Moreover, to achieve long outputs that maintain high-fidelity to the input images, we employ Direct Preference Optimization (DPO) to the SFT model.","Given the high cost of collecting human feedback for lengthy outputs (e.g., 3,000 words), we propose IterDPO, which breaks long outputs into segments and uses iterative corrections to form preference pairs with the original outputs.","Additionally, we develop MMLongBench-Write, a benchmark featuring six tasks to evaluate the long-generation capabilities of VLMs.","Our 7B parameter model, trained with LongWriter-V-22k and IterDPO, achieves impressive performance on this benchmark, outperforming larger proprietary models like GPT-4o.","Code and data: https://github.com/THU-KEG/LongWriter-V"],"url":"http://arxiv.org/abs/2502.14834v1"}
{"created":"2025-02-20 18:45:43","title":"Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs","abstract":"While large language models demonstrate remarkable capabilities at task-specific applications through fine-tuning, extending these benefits across diverse languages is essential for broad accessibility. However, effective cross-lingual transfer is hindered by LLM performance gaps across languages and the scarcity of fine-tuning data in many languages. Through analysis of LLM internal representations from over 1,000+ language pairs, we discover that middle layers exhibit the strongest potential for cross-lingual alignment. Building on this finding, we propose a middle-layer alignment objective integrated into task-specific training. Our experiments on slot filling, machine translation, and structured text generation show consistent improvements in cross-lingual transfer, especially to lower-resource languages. The method is robust to the choice of alignment languages and generalizes to languages unseen during alignment. Furthermore, we show that separately trained alignment modules can be merged with existing task-specific modules, improving cross-lingual capabilities without full re-training. Our code is publicly available (https://github.com/dannigt/mid-align).","sentences":["While large language models demonstrate remarkable capabilities at task-specific applications through fine-tuning, extending these benefits across diverse languages is essential for broad accessibility.","However, effective cross-lingual transfer is hindered by LLM performance gaps across languages and the scarcity of fine-tuning data in many languages.","Through analysis of LLM internal representations from over 1,000+ language pairs, we discover that middle layers exhibit the strongest potential for cross-lingual alignment.","Building on this finding, we propose a middle-layer alignment objective integrated into task-specific training.","Our experiments on slot filling, machine translation, and structured text generation show consistent improvements in cross-lingual transfer, especially to lower-resource languages.","The method is robust to the choice of alignment languages and generalizes to languages unseen during alignment.","Furthermore, we show that separately trained alignment modules can be merged with existing task-specific modules, improving cross-lingual capabilities without full re-training.","Our code is publicly available (https://github.com/dannigt/mid-align)."],"url":"http://arxiv.org/abs/2502.14830v1"}
{"created":"2025-02-20 18:42:58","title":"A Survey of Model Architectures in Information Retrieval","abstract":"This survey examines the evolution of model architectures in information retrieval (IR), focusing on two key aspects: backbone models for feature extraction and end-to-end system architectures for relevance estimation. The review intentionally separates architectural considerations from training methodologies to provide a focused analysis of structural innovations in IR systems.We trace the development from traditional term-based methods to modern neural approaches, particularly highlighting the impact of transformer-based models and subsequent large language models (LLMs). We conclude by discussing emerging challenges and future directions, including architectural optimizations for performance and scalability, handling of multimodal, multilingual data, and adaptation to novel application domains beyond traditional search paradigms.","sentences":["This survey examines the evolution of model architectures in information retrieval (IR), focusing on two key aspects: backbone models for feature extraction and end-to-end system architectures for relevance estimation.","The review intentionally separates architectural considerations from training methodologies to provide a focused analysis of structural innovations in IR systems.","We trace the development from traditional term-based methods to modern neural approaches, particularly highlighting the impact of transformer-based models and subsequent large language models (LLMs).","We conclude by discussing emerging challenges and future directions, including architectural optimizations for performance and scalability, handling of multimodal, multilingual data, and adaptation to novel application domains beyond traditional search paradigms."],"url":"http://arxiv.org/abs/2502.14822v1"}
{"created":"2025-02-20 18:41:48","title":"eC-Tab2Text: Aspect-Based Text Generation from e-Commerce Product Tables","abstract":"Large Language Models (LLMs) have demonstrated exceptional versatility across diverse domains, yet their application in e-commerce remains underexplored due to a lack of domain-specific datasets. To address this gap, we introduce eC-Tab2Text, a novel dataset designed to capture the intricacies of e-commerce, including detailed product attributes and user-specific queries. Leveraging eC-Tab2Text, we focus on text generation from product tables, enabling LLMs to produce high-quality, attribute-specific product reviews from structured tabular data. Fine-tuned models were rigorously evaluated using standard Table2Text metrics, alongside correctness, faithfulness, and fluency assessments. Our results demonstrate substantial improvements in generating contextually accurate reviews, highlighting the transformative potential of tailored datasets and fine-tuning methodologies in optimizing e-commerce workflows. This work highlights the potential of LLMs in e-commerce workflows and the essential role of domain-specific datasets in tailoring them to industry-specific challenges.","sentences":["Large Language Models (LLMs) have demonstrated exceptional versatility across diverse domains, yet their application in e-commerce remains underexplored due to a lack of domain-specific datasets.","To address this gap, we introduce eC-Tab2Text, a novel dataset designed to capture the intricacies of e-commerce, including detailed product attributes and user-specific queries.","Leveraging eC-Tab2Text, we focus on text generation from product tables, enabling LLMs to produce high-quality, attribute-specific product reviews from structured tabular data.","Fine-tuned models were rigorously evaluated using standard Table2Text metrics, alongside correctness, faithfulness, and fluency assessments.","Our results demonstrate substantial improvements in generating contextually accurate reviews, highlighting the transformative potential of tailored datasets and fine-tuning methodologies in optimizing e-commerce workflows.","This work highlights the potential of LLMs in e-commerce workflows and the essential role of domain-specific datasets in tailoring them to industry-specific challenges."],"url":"http://arxiv.org/abs/2502.14820v1"}
{"created":"2025-02-20 18:39:41","title":"Learning from Reward-Free Offline Data: A Case for Planning with Latent Dynamics Models","abstract":"A long-standing goal in AI is to build agents that can solve a variety of tasks across different environments, including previously unseen ones. Two dominant approaches tackle this challenge: (i) reinforcement learning (RL), which learns policies through trial and error, and (ii) optimal control, which plans actions using a learned or known dynamics model. However, their relative strengths and weaknesses remain underexplored in the setting where agents must learn from offline trajectories without reward annotations. In this work, we systematically analyze the performance of different RL and control-based methods under datasets of varying quality. On the RL side, we consider goal-conditioned and zero-shot approaches. On the control side, we train a latent dynamics model using the Joint Embedding Predictive Architecture (JEPA) and use it for planning. We study how dataset properties-such as data diversity, trajectory quality, and environment variability-affect the performance of these approaches. Our results show that model-free RL excels when abundant, high-quality data is available, while model-based planning excels in generalization to novel environment layouts, trajectory stitching, and data-efficiency. Notably, planning with a latent dynamics model emerges as a promising approach for zero-shot generalization from suboptimal data.","sentences":["A long-standing goal in AI is to build agents that can solve a variety of tasks across different environments, including previously unseen ones.","Two dominant approaches tackle this challenge: (i) reinforcement learning (RL), which learns policies through trial and error, and (ii) optimal control, which plans actions using a learned or known dynamics model.","However, their relative strengths and weaknesses remain underexplored in the setting where agents must learn from offline trajectories without reward annotations.","In this work, we systematically analyze the performance of different RL and control-based methods under datasets of varying quality.","On the RL side, we consider goal-conditioned and zero-shot approaches.","On the control side, we train a latent dynamics model using the Joint Embedding Predictive Architecture (JEPA) and use it for planning.","We study how dataset properties-such as data diversity, trajectory quality, and environment variability-affect the performance of these approaches.","Our results show that model-free RL excels when abundant, high-quality data is available, while model-based planning excels in generalization to novel environment layouts, trajectory stitching, and data-efficiency.","Notably, planning with a latent dynamics model emerges as a promising approach for zero-shot generalization from suboptimal data."],"url":"http://arxiv.org/abs/2502.14819v1"}
{"created":"2025-02-20 18:34:30","title":"Byzantine Game Theory: Sun Tzus Boxes","abstract":"We introduce the Byzantine Selection Problem, living at the intersection of game theory and fault-tolerant distributed computing. Here, an event organizer is presented with a group of $n$ agents, and wants to select $\\ell < n$ of them to form a team. For these purposes, each agent $i$ self-reports a positive skill value $v_i$, and a team's value is the sum of its members' skill values. Ideally, the value of the team should be as large as possible, which can be easily achieved by selecting agents with the highest $\\ell$ skill values. However, an unknown subset of at most $t < n$ agents are byzantine and hence not to be trusted, rendering their true skill values as $0$. In the spirit of the distributed computing literature, the identity of the byzantine agents is not random but instead chosen by an adversary aiming to minimize the value of the chosen team. Can we still select a team with good guarantees in this adversarial setting? As it turns out, deterministically, it remains optimal to select agents with the highest $\\ell$ values. Yet, if $t \\geq \\ell$, the adversary can choose to make all selected agents byzantine, leading to a team of value zero. To provide meaningful guarantees, one hence needs to allow for randomization, in which case the expected value of the selected team needs to be maximized, assuming again that the adversary plays to minimize it. For this case, we provide linear-time randomized algorithms that maximize the expected value of the selected team.","sentences":["We introduce the Byzantine Selection Problem, living at the intersection of game theory and fault-tolerant distributed computing.","Here, an event organizer is presented with a group of $n$ agents, and wants to select $\\ell < n$ of them to form a team.","For these purposes, each agent $i$ self-reports a positive skill value $v_i$, and a team's value is the sum of its members' skill values.","Ideally, the value of the team should be as large as possible, which can be easily achieved by selecting agents with the highest $\\ell$ skill values.","However, an unknown subset of at most $t < n$ agents are byzantine and hence not to be trusted, rendering their true skill values as $0$. In the spirit of the distributed computing literature, the identity of the byzantine agents is not random but instead chosen by an adversary aiming to minimize the value of the chosen team.","Can we still select a team with good guarantees in this adversarial setting?","As it turns out, deterministically, it remains optimal to select agents with the highest $\\ell$ values.","Yet, if $t \\geq \\ell$, the adversary can choose to make all selected agents byzantine, leading to a team of value zero.","To provide meaningful guarantees, one hence needs to allow for randomization, in which case the expected value of the selected team needs to be maximized, assuming again that the adversary plays to minimize it.","For this case, we provide linear-time randomized algorithms that maximize the expected value of the selected team."],"url":"http://arxiv.org/abs/2502.14812v1"}
{"created":"2025-02-20 18:32:02","title":"PREM: Privately Answering Statistical Queries with Relative Error","abstract":"We introduce $\\mathsf{PREM}$ (Private Relative Error Multiplicative weight update), a new framework for generating synthetic data that achieves a relative error guarantee for statistical queries under $(\\varepsilon, \\delta)$ differential privacy (DP). Namely, for a domain ${\\cal X}$, a family ${\\cal F}$ of queries $f : {\\cal X} \\to \\{0, 1\\}$, and $\\zeta > 0$, our framework yields a mechanism that on input dataset $D \\in {\\cal X}^n$ outputs a synthetic dataset $\\widehat{D} \\in {\\cal X}^n$ such that all statistical queries in ${\\cal F}$ on $D$, namely $\\sum_{x \\in D} f(x)$ for $f \\in {\\cal F}$, are within a $1 \\pm \\zeta$ multiplicative factor of the corresponding value on $\\widehat{D}$ up to an additive error that is polynomial in $\\log |{\\cal F}|$, $\\log |{\\cal X}|$, $\\log n$, $\\log(1/\\delta)$, $1/\\varepsilon$, and $1/\\zeta$. In contrast, any $(\\varepsilon, \\delta)$-DP mechanism is known to require worst-case additive error that is polynomial in at least one of $n, |{\\cal F}|$, or $|{\\cal X}|$. We complement our algorithm with nearly matching lower bounds.","sentences":["We introduce $\\mathsf{PREM}$ (Private Relative Error Multiplicative weight update), a new framework for generating synthetic data that achieves a relative error guarantee for statistical queries under $(\\varepsilon, \\delta)$ differential privacy (DP).","Namely, for a domain ${\\cal X}$, a family ${\\cal F}$ of queries $f : {\\cal X} \\to \\{0, 1\\}$, and $\\zeta > 0$, our framework yields a mechanism that on input dataset $D \\in {\\cal X}^n$ outputs a synthetic dataset $\\widehat{D} \\in {\\cal X}^n$ such that all statistical queries in ${\\cal F}$ on $D$, namely $\\sum_{x \\in D} f(x)$ for $f \\in {\\cal F}$, are within a $1 \\pm \\zeta$ multiplicative factor of the corresponding value on $\\widehat{D}$ up to an additive error that is polynomial in $\\log |{\\cal F}|$, $\\log |{\\cal X}|$, $\\log n$, $\\log(1/\\delta)$, $1/\\varepsilon$, and $1/\\zeta$. In contrast, any $(\\varepsilon, \\delta)$-DP mechanism is known to require worst-case additive error that is polynomial in at least one of $n, |{\\cal F}|$, or $|{\\cal X}|$.","We complement our algorithm with nearly matching lower bounds."],"url":"http://arxiv.org/abs/2502.14809v1"}
{"created":"2025-02-20 18:26:39","title":"Planning, scheduling, and execution on the Moon: the CADRE technology demonstration mission","abstract":"NASA's Cooperative Autonomous Distributed Robotic Exploration (CADRE) mission, slated for flight to the Moon's Reiner Gamma region in 2025/2026, is designed to demonstrate multi-agent autonomous exploration of the Lunar surface and sub-surface. A team of three robots and a base station will autonomously explore a region near the lander, collecting the data required for 3D reconstruction of the surface with no human input; and then autonomously perform distributed sensing with multi-static ground penetrating radars (GPR), driving in formation while performing coordinated radar soundings to create a map of the subsurface. At the core of CADRE's software architecture is a novel autonomous, distributed planning, scheduling, and execution (PS&E) system. The system coordinates the robots' activities, planning and executing tasks that require multiple robots' participation while ensuring that each individual robot's thermal and power resources stay within prescribed bounds, and respecting ground-prescribed sleep-wake cycles. The system uses a centralized-planning, distributed-execution paradigm, and a leader election mechanism ensures robustness to failures of individual agents. In this paper, we describe the architecture of CADRE's PS&E system; discuss its design rationale; and report on verification and validation (V&V) testing of the system on CADRE's hardware in preparation for deployment on the Moon.","sentences":["NASA's Cooperative Autonomous Distributed Robotic Exploration (CADRE) mission, slated for flight to the Moon's Reiner Gamma region in 2025/2026, is designed to demonstrate multi-agent autonomous exploration of the Lunar surface and sub-surface.","A team of three robots and a base station will autonomously explore a region near the lander, collecting the data required for 3D reconstruction of the surface with no human input; and then autonomously perform distributed sensing with multi-static ground penetrating radars (GPR), driving in formation while performing coordinated radar soundings to create a map of the subsurface.","At the core of CADRE's software architecture is a novel autonomous, distributed planning, scheduling, and execution (PS&E) system.","The system coordinates the robots' activities, planning and executing tasks that require multiple robots' participation while ensuring that each individual robot's thermal and power resources stay within prescribed bounds, and respecting ground-prescribed sleep-wake cycles.","The system uses a centralized-planning, distributed-execution paradigm, and a leader election mechanism ensures robustness to failures of individual agents.","In this paper, we describe the architecture of CADRE's PS&E system; discuss its design rationale; and report on verification and validation (V&V) testing of the system on CADRE's hardware in preparation for deployment on the Moon."],"url":"http://arxiv.org/abs/2502.14803v1"}
{"created":"2025-02-20 18:26:02","title":"From RAG to Memory: Non-Parametric Continual Learning for Large Language Models","abstract":"Our ability to continuously acquire, organize, and leverage knowledge is a key feature of human intelligence that AI systems must approximate to unlock their full potential. Given the challenges in continual learning with large language models (LLMs), retrieval-augmented generation (RAG) has become the dominant way to introduce new information. However, its reliance on vector retrieval hinders its ability to mimic the dynamic and interconnected nature of human long-term memory. Recent RAG approaches augment vector embeddings with various structures like knowledge graphs to address some of these gaps, namely sense-making and associativity. However, their performance on more basic factual memory tasks drops considerably below standard RAG. We address this unintended deterioration and propose HippoRAG 2, a framework that outperforms standard RAG comprehensively on factual, sense-making, and associative memory tasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in HippoRAG and enhances it with deeper passage integration and more effective online use of an LLM. This combination pushes this RAG system closer to the effectiveness of human long-term memory, achieving a 7% improvement in associative memory tasks over the state-of-the-art embedding model while also exhibiting superior factual knowledge and sense-making memory capabilities. This work paves the way for non-parametric continual learning for LLMs. Our code and data will be released at https://github.com/OSU-NLP-Group/HippoRAG.","sentences":["Our ability to continuously acquire, organize, and leverage knowledge is a key feature of human intelligence that AI systems must approximate to unlock their full potential.","Given the challenges in continual learning with large language models (LLMs), retrieval-augmented generation (RAG) has become the dominant way to introduce new information.","However, its reliance on vector retrieval hinders its ability to mimic the dynamic and interconnected nature of human long-term memory.","Recent RAG approaches augment vector embeddings with various structures like knowledge graphs to address some of these gaps, namely sense-making and associativity.","However, their performance on more basic factual memory tasks drops considerably below standard RAG.","We address this unintended deterioration and propose HippoRAG 2, a framework that outperforms standard RAG comprehensively on factual, sense-making, and associative memory tasks.","HippoRAG 2 builds upon the Personalized PageRank algorithm used in HippoRAG and enhances it with deeper passage integration and more effective online use of an LLM.","This combination pushes this RAG system closer to the effectiveness of human long-term memory, achieving a 7% improvement in associative memory tasks over the state-of-the-art embedding model while also exhibiting superior factual knowledge and sense-making memory capabilities.","This work paves the way for non-parametric continual learning for LLMs.","Our code and data will be released at https://github.com/OSU-NLP-Group/HippoRAG."],"url":"http://arxiv.org/abs/2502.14802v1"}
{"created":"2025-02-20 18:22:44","title":"AVD2: Accident Video Diffusion for Accident Video Description","abstract":"Traffic accidents present complex challenges for autonomous driving, often featuring unpredictable scenarios that hinder accurate system interpretation and responses.Nonetheless, prevailing methodologies fall short in elucidating the causes of accidents and proposing preventive measures due to the paucity of training data specific to accident scenarios.In this work, we introduce AVD2 (Accident Video Diffusion for Accident Video Description), a novel framework that enhances accident scene understanding by generating accident videos that aligned with detailed natural language descriptions and reasoning, resulting in the contributed EMM-AU (Enhanced Multi-Modal Accident Video Understanding) dataset. Empirical results reveal that the integration of the EMM-AU dataset establishes state-of-the-art performance across both automated metrics and human evaluations, markedly advancing the domains of accident analysis and prevention. Project resources are available at https://an-answer-tree.github.io","sentences":["Traffic accidents present complex challenges for autonomous driving, often featuring unpredictable scenarios that hinder accurate system interpretation and responses.","Nonetheless, prevailing methodologies fall short in elucidating the causes of accidents and proposing preventive measures due to the paucity of training data specific to accident scenarios.","In this work, we introduce AVD2 (Accident Video Diffusion for Accident Video Description), a novel framework that enhances accident scene understanding by generating accident videos that aligned with detailed natural language descriptions and reasoning, resulting in the contributed EMM-AU (Enhanced Multi-Modal Accident Video Understanding) dataset.","Empirical results reveal that the integration of the EMM-AU dataset establishes state-of-the-art performance across both automated metrics and human evaluations, markedly advancing the domains of accident analysis and prevention.","Project resources are available at https://an-answer-tree.github.io"],"url":"http://arxiv.org/abs/2502.14801v1"}
{"created":"2025-02-20 18:17:11","title":"Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration","abstract":"This paper addresses the limitations of current humanoid robot control frameworks, which primarily rely on reactive mechanisms and lack autonomous interaction capabilities due to data scarcity. We propose Humanoid-VLA, a novel framework that integrates language understanding, egocentric scene perception, and motion control, enabling universal humanoid control. Humanoid-VLA begins with language-motion pre-alignment using non-egocentric human motion datasets paired with textual descriptions, allowing the model to learn universal motion patterns and action semantics. We then incorporate egocentric visual context through a parameter efficient video-conditioned fine-tuning, enabling context-aware motion generation. Furthermore, we introduce a self-supervised data augmentation strategy that automatically generates pseudoannotations directly derived from motion data. This process converts raw motion sequences into informative question-answer pairs, facilitating the effective use of large-scale unlabeled video data. Built upon whole-body control architectures, extensive experiments show that Humanoid-VLA achieves object interaction and environment exploration tasks with enhanced contextual awareness, demonstrating a more human-like capacity for adaptive and intelligent engagement.","sentences":["This paper addresses the limitations of current humanoid robot control frameworks, which primarily rely on reactive mechanisms and lack autonomous interaction capabilities due to data scarcity.","We propose Humanoid-VLA, a novel framework that integrates language understanding, egocentric scene perception, and motion control, enabling universal humanoid control.","Humanoid-VLA begins with language-motion pre-alignment using non-egocentric human motion datasets paired with textual descriptions, allowing the model to learn universal motion patterns and action semantics.","We then incorporate egocentric visual context through a parameter efficient video-conditioned fine-tuning, enabling context-aware motion generation.","Furthermore, we introduce a self-supervised data augmentation strategy that automatically generates pseudoannotations directly derived from motion data.","This process converts raw motion sequences into informative question-answer pairs, facilitating the effective use of large-scale unlabeled video data.","Built upon whole-body control architectures, extensive experiments show that Humanoid-VLA achieves object interaction and environment exploration tasks with enhanced contextual awareness, demonstrating a more human-like capacity for adaptive and intelligent engagement."],"url":"http://arxiv.org/abs/2502.14795v1"}
{"created":"2025-02-20 18:11:38","title":"Rapid Word Learning Through Meta In-Context Learning","abstract":"Humans can quickly learn a new word from a few illustrative examples, and then systematically and flexibly use it in novel contexts. Yet the abilities of current language models for few-shot word learning, and methods for improving these abilities, are underexplored. In this study, we introduce a novel method, Meta-training for IN-context learNing Of Words (Minnow). This method trains language models to generate new examples of a word's usage given a few in-context examples, using a special placeholder token to represent the new word. This training is repeated on many new words to develop a general word-learning ability. We find that training models from scratch with Minnow on human-scale child-directed language enables strong few-shot word learning, comparable to a large language model (LLM) pre-trained on orders of magnitude more data. Furthermore, through discriminative and generative evaluations, we demonstrate that finetuning pre-trained LLMs with Minnow improves their ability to discriminate between new words, identify syntactic categories of new words, and generate reasonable new usages and definitions for new words, based on one or a few in-context examples. These findings highlight the data efficiency of Minnow and its potential to improve language model performance in word learning tasks.","sentences":["Humans can quickly learn a new word from a few illustrative examples, and then systematically and flexibly use it in novel contexts.","Yet the abilities of current language models for few-shot word learning, and methods for improving these abilities, are underexplored.","In this study, we introduce a novel method, Meta-training for IN-context learNing Of Words (Minnow).","This method trains language models to generate new examples of a word's usage given a few in-context examples, using a special placeholder token to represent the new word.","This training is repeated on many new words to develop a general word-learning ability.","We find that training models from scratch with Minnow on human-scale child-directed language enables strong few-shot word learning, comparable to a large language model (LLM) pre-trained on orders of magnitude more data.","Furthermore, through discriminative and generative evaluations, we demonstrate that finetuning pre-trained LLMs with Minnow improves their ability to discriminate between new words, identify syntactic categories of new words, and generate reasonable new usages and definitions for new words, based on one or a few in-context examples.","These findings highlight the data efficiency of Minnow and its potential to improve language model performance in word learning tasks."],"url":"http://arxiv.org/abs/2502.14791v1"}
{"created":"2025-02-20 18:08:29","title":"SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features","abstract":"We introduce SigLIP 2, a family of new multilingual vision-language encoders that build on the success of the original SigLIP. In this second iteration, we extend the original image-text training objective with several prior, independently developed techniques into a unified recipe -- this includes captioning-based pretraining, self-supervised losses (self-distillation, masked prediction) and online data curation. With these changes, SigLIP 2 models outperform their SigLIP counterparts at all model scales in core capabilities, including zero-shot classification, image-text retrieval, and transfer performance when extracting visual representations for Vision-Language Models (VLMs). Furthermore, the new training recipe leads to significant improvements on localization and dense prediction tasks. We also train variants which support multiple resolutions and preserve the input's native aspect ratio. Finally, we train on a more diverse data-mixture that includes de-biasing techniques, leading to much better multilingual understanding and improved fairness. To allow users to trade off inference cost with performance, we release model checkpoints at four sizes: ViT-B (86M), L (303M), So400m (400M), and g (1B).","sentences":["We introduce SigLIP 2, a family of new multilingual vision-language encoders that build on the success of the original SigLIP.","In this second iteration, we extend the original image-text training objective with several prior, independently developed techniques into a unified recipe -- this includes captioning-based pretraining, self-supervised losses (self-distillation, masked prediction) and online data curation.","With these changes, SigLIP 2 models outperform their SigLIP counterparts at all model scales in core capabilities, including zero-shot classification, image-text retrieval, and transfer performance when extracting visual representations for Vision-Language Models (VLMs).","Furthermore, the new training recipe leads to significant improvements on localization and dense prediction tasks.","We also train variants which support multiple resolutions and preserve the input's native aspect ratio.","Finally, we train on a more diverse data-mixture that includes de-biasing techniques, leading to much better multilingual understanding and improved fairness.","To allow users to trade off inference cost with performance, we release model checkpoints at four sizes: ViT-B (86M), L (303M), So400m (400M), and g (1B)."],"url":"http://arxiv.org/abs/2502.14786v1"}
{"created":"2025-02-20 18:05:34","title":"Real-Time Device Reach Forecasting Using HLL and MinHash Data Sketches","abstract":"Predicting the right number of TVs (Device Reach) in real-time based on a user-specified targeting attributes is imperative for running multi-million dollar ADs business. The traditional approach of SQL queries to join billions of records across multiple targeting dimensions is extremely slow. As a workaround, many applications will have an offline process to crunch these numbers and present the results after many hours. In our case, the solution was an offline process taking 24 hours to onboard a customer resulting in a potential loss of business. To solve this problem, we have built a new real-time prediction system using MinHash and HyperLogLog (HLL) data sketches to compute the device reach at runtime when a user makes a request. However, existing MinHash implementations do not solve the complex problem of multilevel aggregation and intersection. This work will show how we have solved this problem, in addition, we have improved MinHash algorithm to run 4 times faster using Single Instruction Multiple Data (SIMD) vectorized operations for high speed and accuracy with constant space to process billions of records. Finally, by experiments, we prove that the results are as accurate as traditional offline prediction system with an acceptable error rate of 5%.","sentences":["Predicting the right number of TVs (Device Reach) in real-time based on a user-specified targeting attributes is imperative for running multi-million dollar ADs business.","The traditional approach of SQL queries to join billions of records across multiple targeting dimensions is extremely slow.","As a workaround, many applications will have an offline process to crunch these numbers and present the results after many hours.","In our case, the solution was an offline process taking 24 hours to onboard a customer resulting in a potential loss of business.","To solve this problem, we have built a new real-time prediction system using MinHash and HyperLogLog (HLL) data sketches to compute the device reach at runtime when a user makes a request.","However, existing MinHash implementations do not solve the complex problem of multilevel aggregation and intersection.","This work will show how we have solved this problem, in addition, we have improved MinHash algorithm to run 4 times faster using Single Instruction Multiple Data (SIMD) vectorized operations for high speed and accuracy with constant space to process billions of records.","Finally, by experiments, we prove that the results are as accurate as traditional offline prediction system with an acceptable error rate of 5%."],"url":"http://arxiv.org/abs/2502.14785v1"}
{"created":"2025-02-20 18:01:41","title":"ReVision: A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented Visual Instruction Rewriting","abstract":"Efficient and privacy-preserving multimodal interaction is essential as AR, VR, and modern smartphones with powerful cameras become primary interfaces for human-computer communication. Existing powerful large vision-language models (VLMs) enabling multimodal interaction often rely on cloud-based processing, raising significant concerns about (1) visual privacy by transmitting sensitive vision data to servers, and (2) their limited real-time, on-device usability. This paper explores Visual Instruction Rewriting, a novel approach that transforms multimodal instructions into text-only commands, allowing seamless integration of lightweight on-device instruction rewriter VLMs (250M parameters) with existing conversational AI systems, enhancing vision data privacy. To achieve this, we present a dataset of over 39,000 examples across 14 domains and develop a compact VLM, pretrained on image captioning datasets and fine-tuned for instruction rewriting. Experimental results, evaluated through NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic parsing analysis, demonstrate that even a quantized version of the model (<500MB storage footprint) can achieve effective instruction rewriting, thus enabling privacy-focused, multimodal AI applications.","sentences":["Efficient and privacy-preserving multimodal interaction is essential as AR, VR, and modern smartphones with powerful cameras become primary interfaces for human-computer communication.","Existing powerful large vision-language models (VLMs) enabling multimodal interaction often rely on cloud-based processing, raising significant concerns about (1) visual privacy by transmitting sensitive vision data to servers, and (2) their limited real-time, on-device usability.","This paper explores Visual Instruction Rewriting, a novel approach that transforms multimodal instructions into text-only commands, allowing seamless integration of lightweight on-device instruction rewriter VLMs (250M parameters) with existing conversational AI systems, enhancing vision data privacy.","To achieve this, we present a dataset of over 39,000 examples across 14 domains and develop a compact VLM, pretrained on image captioning datasets and fine-tuned for instruction rewriting.","Experimental results, evaluated through NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic parsing analysis, demonstrate that even a quantized version of the model (<500MB storage footprint) can achieve effective instruction rewriting, thus enabling privacy-focused, multimodal AI applications."],"url":"http://arxiv.org/abs/2502.14780v1"}
{"created":"2025-02-20 17:59:59","title":"Harnessing PDF Data for Improving Japanese Large Multimodal Models","abstract":"Large Multimodal Models (LMMs) have demonstrated strong performance in English, but their effectiveness in Japanese remains limited due to the lack of high-quality training data. Current Japanese LMMs often rely on translated English datasets, restricting their ability to capture Japan-specific cultural knowledge. To address this, we explore the potential of Japanese PDF data as a training resource, an area that remains largely underutilized. We introduce a fully automated pipeline that leverages pretrained models to extract image-text pairs from PDFs through layout analysis, OCR, and vision-language pairing, removing the need for manual annotation. Additionally, we construct instruction data from extracted image-text pairs to enrich the training data. To evaluate the effectiveness of PDF-derived data, we train Japanese LMMs and assess their performance on the Japanese LMM Benchmark. Our results demonstrate substantial improvements, with performance gains ranging from 3.9% to 13.8% on Heron-Bench. Further analysis highlights the impact of PDF-derived data on various factors, such as model size and language models, reinforcing its value as a multimodal resource for Japanese LMMs. We plan to make the source code and data publicly available upon acceptance.","sentences":["Large Multimodal Models (LMMs) have demonstrated strong performance in English, but their effectiveness in Japanese remains limited due to the lack of high-quality training data.","Current Japanese LMMs often rely on translated English datasets, restricting their ability to capture Japan-specific cultural knowledge.","To address this, we explore the potential of Japanese PDF data as a training resource, an area that remains largely underutilized.","We introduce a fully automated pipeline that leverages pretrained models to extract image-text pairs from PDFs through layout analysis, OCR, and vision-language pairing, removing the need for manual annotation.","Additionally, we construct instruction data from extracted image-text pairs to enrich the training data.","To evaluate the effectiveness of PDF-derived data, we train Japanese LMMs and assess their performance on the Japanese LMM Benchmark.","Our results demonstrate substantial improvements, with performance gains ranging from 3.9% to 13.8% on Heron-Bench.","Further analysis highlights the impact of PDF-derived data on various factors, such as model size and language models, reinforcing its value as a multimodal resource for Japanese LMMs.","We plan to make the source code and data publicly available upon acceptance."],"url":"http://arxiv.org/abs/2502.14778v1"}
{"created":"2025-02-20 17:59:55","title":"Making Universal Policies Universal","abstract":"The development of a generalist agent capable of solving a wide range of sequential decision-making tasks remains a significant challenge. We address this problem in a cross-agent setup where agents share the same observation space but differ in their action spaces. Our approach builds on the universal policy framework, which decouples policy learning into two stages: a diffusion-based planner that generates observation sequences and an inverse dynamics model that assigns actions to these plans. We propose a method for training the planner on a joint dataset composed of trajectories from all agents. This method offers the benefit of positive transfer by pooling data from different agents, while the primary challenge lies in adapting shared plans to each agent's unique constraints. We evaluate our approach on the BabyAI environment, covering tasks of varying complexity, and demonstrate positive transfer across agents. Additionally, we examine the planner's generalisation ability to unseen agents and compare our method to traditional imitation learning approaches. By training on a pooled dataset from multiple agents, our universal policy achieves an improvement of up to $42.20\\%$ in task completion accuracy compared to a policy trained on a dataset from a single agent.","sentences":["The development of a generalist agent capable of solving a wide range of sequential decision-making tasks remains a significant challenge.","We address this problem in a cross-agent setup where agents share the same observation space but differ in their action spaces.","Our approach builds on the universal policy framework, which decouples policy learning into two stages: a diffusion-based planner that generates observation sequences and an inverse dynamics model that assigns actions to these plans.","We propose a method for training the planner on a joint dataset composed of trajectories from all agents.","This method offers the benefit of positive transfer by pooling data from different agents, while the primary challenge lies in adapting shared plans to each agent's unique constraints.","We evaluate our approach on the BabyAI environment, covering tasks of varying complexity, and demonstrate positive transfer across agents.","Additionally, we examine the planner's generalisation ability to unseen agents and compare our method to traditional imitation learning approaches.","By training on a pooled dataset from multiple agents, our universal policy achieves an improvement of up to $42.20\\%$ in task completion accuracy compared to a policy trained on a dataset from a single agent."],"url":"http://arxiv.org/abs/2502.14777v1"}
{"created":"2025-02-20 17:53:13","title":"Efficient Multivariate Robust Mean Estimation Under Mean-Shift Contamination","abstract":"We study the algorithmic problem of robust mean estimation of an identity covariance Gaussian in the presence of mean-shift contamination. In this contamination model, we are given a set of points in $\\mathbb{R}^d$ generated i.i.d. via the following process. For a parameter $\\alpha<1/2$, the $i$-th sample $x_i$ is obtained as follows: with probability $1-\\alpha$, $x_i$ is drawn from $\\mathcal{N}(\\mu, I)$, where $\\mu \\in \\mathbb{R}^d$ is the target mean; and with probability $\\alpha$, $x_i$ is drawn from $\\mathcal{N}(z_i, I)$, where $z_i$ is unknown and potentially arbitrary. Prior work characterized the information-theoretic limits of this task. Specifically, it was shown that, in contrast to Huber contamination, in the presence of mean-shift contamination consistent estimation is possible. On the other hand, all known robust estimators in the mean-shift model have running times exponential in the dimension. Here we give the first computationally efficient algorithm for high-dimensional robust mean estimation with mean-shift contamination that can tolerate a constant fraction of outliers. In particular, our algorithm has near-optimal sample complexity, runs in sample-polynomial time, and approximates the target mean to any desired accuracy. Conceptually, our result contributes to a growing body of work that studies inference with respect to natural noise models lying in between fully adversarial and random settings.","sentences":["We study the algorithmic problem of robust mean estimation of an identity covariance Gaussian in the presence of mean-shift contamination.","In this contamination model, we are given a set of points in $\\mathbb{R}^d$ generated i.i.d. via the following process.","For a parameter $\\alpha<1/2$, the $i$-th sample $x_i$ is obtained as follows: with probability $1-\\alpha$, $x_i$ is drawn from $\\mathcal{N}(\\mu, I)$, where $\\mu \\in \\mathbb{R}^d$ is the target mean; and with probability $\\alpha$, $x_i$ is drawn from $\\mathcal{N}(z_i, I)$, where $z_i$ is unknown and potentially arbitrary.","Prior work characterized the information-theoretic limits of this task.","Specifically, it was shown that, in contrast to Huber contamination, in the presence of mean-shift contamination consistent estimation is possible.","On the other hand, all known robust estimators in the mean-shift model have running times exponential in the dimension.","Here we give the first computationally efficient algorithm for high-dimensional robust mean estimation with mean-shift contamination that can tolerate a constant fraction of outliers.","In particular, our algorithm has near-optimal sample complexity, runs in sample-polynomial time, and approximates the target mean to any desired accuracy.","Conceptually, our result contributes to a growing body of work that studies inference with respect to natural noise models lying in between fully adversarial and random settings."],"url":"http://arxiv.org/abs/2502.14772v1"}
{"created":"2025-02-20 17:49:26","title":"Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning","abstract":"Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in large reasoning models. To analyze reasoning dynamics, we use synthetic logic puzzles as training data due to their controllable complexity and straightforward answer verification. We make some key technical contributions that lead to effective and stable RL training: a system prompt that emphasizes the thinking and answering process, a stringent format reward function that penalizes outputs for taking shortcuts, and a straightforward training recipe that achieves stable convergence. Our 7B model develops advanced reasoning skills-such as reflection, verification, and summarization-that are absent from the logic corpus. Remarkably, after training on just 5K logic problems, it demonstrates generalization abilities to the challenging math benchmarks AIME and AMC.","sentences":["Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in large reasoning models.","To analyze reasoning dynamics, we use synthetic logic puzzles as training data due to their controllable complexity and straightforward answer verification.","We make some key technical contributions that lead to effective and stable RL training: a system prompt that emphasizes the thinking and answering process, a stringent format reward function that penalizes outputs for taking shortcuts, and a straightforward training recipe that achieves stable convergence.","Our 7B model develops advanced reasoning skills-such as reflection, verification, and summarization-that are absent from the logic corpus.","Remarkably, after training on just 5K logic problems, it demonstrates generalization abilities to the challenging math benchmarks AIME and AMC."],"url":"http://arxiv.org/abs/2502.14768v1"}
{"created":"2025-02-20 17:38:31","title":"The illusion of households as entities in social networks","abstract":"Data recording connections between people in communities and villages are collected and analyzed in various ways, most often as either networks of individuals or as networks of households. These two networks can differ in substantial ways. The methodological choice of which network to study, therefore, is an important aspect in both study design and data analysis. In this work, we consider various key differences between household and individual social network structure, and ways in which the networks cannot be used interchangeably. In addition to formalizing the choices for representing each network, we explore the consequences of how the results of social network analysis change depending on the choice between studying the individual and household network -- from determining whether networks are assortative or disassortative to the ranking of influence-maximizing nodes. As our main contribution, we draw upon related work to propose a set of systematic recommendations for determining the relevant network representation to study. Our recommendations include assessing a series of entitativity criteria and relating these criteria to theories and observations about patterns and norms in social dynamics at the household level: notably, how information spreads within households and how power structures and gender roles affect this spread. We draw upon the definition of an illusion of entitativity to identify cases wherein grouping people into households does not satisfy these criteria or adequately represent given cultural or experimental contexts. Given the widespread use of social network data for studying communities, there is broad impact in understanding which network to study and the consequences of that decision. We hope that this work gives guidance to practitioners and researchers collecting and studying social network data.","sentences":["Data recording connections between people in communities and villages are collected and analyzed in various ways, most often as either networks of individuals or as networks of households.","These two networks can differ in substantial ways.","The methodological choice of which network to study, therefore, is an important aspect in both study design and data analysis.","In this work, we consider various key differences between household and individual social network structure, and ways in which the networks cannot be used interchangeably.","In addition to formalizing the choices for representing each network, we explore the consequences of how the results of social network analysis change depending on the choice between studying the individual and household network -- from determining whether networks are assortative or disassortative to the ranking of influence-maximizing nodes.","As our main contribution, we draw upon related work to propose a set of systematic recommendations for determining the relevant network representation to study.","Our recommendations include assessing a series of entitativity criteria and relating these criteria to theories and observations about patterns and norms in social dynamics at the household level: notably, how information spreads within households and how power structures and gender roles affect this spread.","We draw upon the definition of an illusion of entitativity to identify cases wherein grouping people into households does not satisfy these criteria or adequately represent given cultural or experimental contexts.","Given the widespread use of social network data for studying communities, there is broad impact in understanding which network to study and the consequences of that decision.","We hope that this work gives guidance to practitioners and researchers collecting and studying social network data."],"url":"http://arxiv.org/abs/2502.14764v1"}
{"created":"2025-02-20 17:36:19","title":"User Awareness and Perspectives Survey on Privacy, Security and Usability of Auditory Prostheses","abstract":"According to the World Health Organization, over 466 million people worldwide suffer from disabling hearing loss, with approximately 34 million of these being children. Hearing aids (HA) and cochlear implants (CI) have become indispensable tools for restoring hearing and enhancing the quality of life for individuals with hearing impairments. Clinical research and consumer studies indicate that users of HAs and CIs report significant improvements in their daily lives, including enhanced communication abilities and social engagement and reduced psychological stress. Modern auditory prosthetic devices are more advanced and interconnected with digital networks to add functionality, such as streaming audio directly from smartphones and other devices, remote adjustments by audiologists, integration with smart home systems, and access to artificial intelligence-driven sound enhancement features. With this interconnectivity, issues surrounding data privacy and security have become increasingly pertinent. There is limited research on the usability perceptions of current HA and CI models from the perspective of end-users. In addition, no studies have investigated consumer mental models during the purchasing process, particularly which factors they prioritize when selecting a device. In this study, we assessed participants' satisfaction levels with various features of their auditory prostheses. This work contributes to the field by addressing gaps in user perceptions of HA and CI usability, identifying key factors in consumer purchasing decisions, and highlighting the need for improved privacy and security awareness and education among users.","sentences":["According to the World Health Organization, over 466 million people worldwide suffer from disabling hearing loss, with approximately 34 million of these being children.","Hearing aids (HA) and cochlear implants (CI) have become indispensable tools for restoring hearing and enhancing the quality of life for individuals with hearing impairments.","Clinical research and consumer studies indicate that users of HAs and CIs report significant improvements in their daily lives, including enhanced communication abilities and social engagement and reduced psychological stress.","Modern auditory prosthetic devices are more advanced and interconnected with digital networks to add functionality, such as streaming audio directly from smartphones and other devices, remote adjustments by audiologists, integration with smart home systems, and access to artificial intelligence-driven sound enhancement features.","With this interconnectivity, issues surrounding data privacy and security have become increasingly pertinent.","There is limited research on the usability perceptions of current HA and CI models from the perspective of end-users.","In addition, no studies have investigated consumer mental models during the purchasing process, particularly which factors they prioritize when selecting a device.","In this study, we assessed participants' satisfaction levels with various features of their auditory prostheses.","This work contributes to the field by addressing gaps in user perceptions of HA and CI usability, identifying key factors in consumer purchasing decisions, and highlighting the need for improved privacy and security awareness and education among users."],"url":"http://arxiv.org/abs/2502.14761v1"}
{"created":"2025-02-20 17:19:41","title":"Large Language Models Struggle to Describe the Haystack without Human Help: Human-in-the-loop Evaluation of LLMs","abstract":"A common use of NLP is to facilitate the understanding of large document collections, with a shift from using traditional topic models to Large Language Models. Yet the effectiveness of using LLM for large corpus understanding in real-world applications remains under-explored. This study measures the knowledge users acquire with unsupervised, supervised LLM-based exploratory approaches or traditional topic models on two datasets. While LLM-based methods generate more human-readable topics and show higher average win probabilities than traditional models for data exploration, they produce overly generic topics for domain-specific datasets that do not easily allow users to learn much about the documents. Adding human supervision to the LLM generation process improves data exploration by mitigating hallucination and over-genericity but requires greater human effort. In contrast, traditional. models like Latent Dirichlet Allocation (LDA) remain effective for exploration but are less user-friendly. We show that LLMs struggle to describe the haystack of large corpora without human help, particularly domain-specific data, and face scaling and hallucination limitations due to context length constraints. Dataset available at https://huggingface. co/datasets/zli12321/Bills.","sentences":["A common use of NLP is to facilitate the understanding of large document collections, with a shift from using traditional topic models to Large Language Models.","Yet the effectiveness of using LLM for large corpus understanding in real-world applications remains under-explored.","This study measures the knowledge users acquire with unsupervised, supervised LLM-based exploratory approaches or traditional topic models on two datasets.","While LLM-based methods generate more human-readable topics and show higher average win probabilities than traditional models for data exploration, they produce overly generic topics for domain-specific datasets that do not easily allow users to learn much about the documents.","Adding human supervision to the LLM generation process improves data exploration by mitigating hallucination and over-genericity but requires greater human effort.","In contrast, traditional. models like Latent Dirichlet Allocation (LDA) remain effective for exploration but are less user-friendly.","We show that LLMs struggle to describe the haystack of large corpora without human help, particularly domain-specific data, and face scaling and hallucination limitations due to context length constraints.","Dataset available at https://huggingface. co/datasets/zli12321/Bills."],"url":"http://arxiv.org/abs/2502.14748v1"}
{"created":"2025-02-20 17:16:10","title":"SQL4NN: Validation and expressive querying of models as data","abstract":"We consider machine learning models, learned from data, to be an important, intensional, kind of data in themselves. As such, various analysis tasks on models can be thought of as queries over this intensional data, often combined with extensional data such as data for training or validation. We demonstrate that relational database systems and SQL can actually be well suited for many such tasks.","sentences":["We consider machine learning models, learned from data, to be an important, intensional, kind of data in themselves.","As such, various analysis tasks on models can be thought of as queries over this intensional data, often combined with extensional data such as data for training or validation.","We demonstrate that relational database systems and SQL can actually be well suited for many such tasks."],"url":"http://arxiv.org/abs/2502.14745v1"}
{"created":"2025-02-20 17:10:11","title":"Reinforcement Learning with Graph Attention for Routing and Wavelength Assignment with Lightpath Reuse","abstract":"Many works have investigated reinforcement learning (RL) for routing and spectrum assignment on flex-grid networks but only one work to date has examined RL for fixed-grid with flex-rate transponders, despite production systems using this paradigm. Flex-rate transponders allow existing lightpaths to accommodate new services, a task we term routing and wavelength assignment with lightpath reuse (RWA-LR). We re-examine this problem and present a thorough benchmarking of heuristic algorithms for RWA-LR, which are shown to have 6% increased throughput when candidate paths are ordered by number of hops, rather than total length. We train an RL agent for RWA-LR with graph attention networks for the policy and value functions to exploit the graph-structured data. We provide details of our methodology and open source all of our code for reproduction. We outperform the previous state-of-the-art RL approach by 2.5% (17.4 Tbps mean additional throughput) and the best heuristic by 1.2% (8.5 Tbps mean additional throughput). This marginal gain highlights the difficulty in learning effective RL policies on long horizon resource allocation tasks.","sentences":["Many works have investigated reinforcement learning (RL) for routing and spectrum assignment on flex-grid networks but only one work to date has examined RL for fixed-grid with flex-rate transponders, despite production systems using this paradigm.","Flex-rate transponders allow existing lightpaths to accommodate new services, a task we term routing and wavelength assignment with lightpath reuse (RWA-LR).","We re-examine this problem and present a thorough benchmarking of heuristic algorithms for RWA-LR, which are shown to have 6% increased throughput when candidate paths are ordered by number of hops, rather than total length.","We train an RL agent for RWA-LR with graph attention networks for the policy and value functions to exploit the graph-structured data.","We provide details of our methodology and open source all of our code for reproduction.","We outperform the previous state-of-the-art RL approach by 2.5% (17.4 Tbps mean additional throughput) and the best heuristic by 1.2% (8.5 Tbps mean additional throughput).","This marginal gain highlights the difficulty in learning effective RL policies on long horizon resource allocation tasks."],"url":"http://arxiv.org/abs/2502.14741v1"}
{"created":"2025-02-20 16:48:14","title":"Multi-dataset synergistic in supervised learning to pre-label structural components in point clouds from shell construction scenes","abstract":"The significant effort required to annotate data for new training datasets hinders computer vision research and machine learning in the construction industry. This work explores adapting standard datasets and the latest transformer model architectures for point cloud semantic segmentation in the context of shell construction sites. Unlike common approaches focused on object segmentation of building interiors and furniture, this study addressed the challenges of segmenting complex structural components in Architecture, Engineering, and Construction (AEC). We establish a baseline through supervised training and a custom validation dataset, evaluate the cross-domain inference with large-scale indoor datasets, and utilize transfer learning to maximize segmentation performance with minimal new data. The findings indicate that with minimal fine-tuning, pre-trained transformer architectures offer an effective strategy for building component segmentation. Our results are promising for automating the annotation of new, previously unseen data when creating larger training resources and for the segmentation of frequently recurring objects.","sentences":["The significant effort required to annotate data for new training datasets hinders computer vision research and machine learning in the construction industry.","This work explores adapting standard datasets and the latest transformer model architectures for point cloud semantic segmentation in the context of shell construction sites.","Unlike common approaches focused on object segmentation of building interiors and furniture, this study addressed the challenges of segmenting complex structural components in Architecture, Engineering, and Construction (AEC).","We establish a baseline through supervised training and a custom validation dataset, evaluate the cross-domain inference with large-scale indoor datasets, and utilize transfer learning to maximize segmentation performance with minimal new data.","The findings indicate that with minimal fine-tuning, pre-trained transformer architectures offer an effective strategy for building component segmentation.","Our results are promising for automating the annotation of new, previously unseen data when creating larger training resources and for the segmentation of frequently recurring objects."],"url":"http://arxiv.org/abs/2502.14721v1"}
{"created":"2025-02-20 16:34:46","title":"Data-Efficient Pretraining with Group-Level Data Influence Modeling","abstract":"Data-efficient pretraining has shown tremendous potential to elevate scaling laws. This paper argues that effective pretraining data should be curated at the group level, treating a set of data points as a whole rather than as independent contributors. To achieve that, we propose Group-Level Data Influence Modeling (Group-MATES), a novel data-efficient pretraining method that captures and optimizes group-level data utility. Specifically, Group-MATES collects oracle group-level influences by locally probing the pretraining model with data sets. It then fine-tunes a relational data influence model to approximate oracles as relationship-weighted aggregations of individual influences. The fine-tuned model selects the data subset by maximizing its group-level influence prediction, with influence-aware clustering to enable efficient inference. Experiments on the DCLM benchmark demonstrate that Group-MATES achieves a 10% relative core score improvement on 22 downstream tasks over DCLM-Baseline and 5% over individual-influence-based methods, establishing a new state-of-the-art. Further analyses highlight the effectiveness of relational data influence models in capturing intricate interactions between data points.","sentences":["Data-efficient pretraining has shown tremendous potential to elevate scaling laws.","This paper argues that effective pretraining data should be curated at the group level, treating a set of data points as a whole rather than as independent contributors.","To achieve that, we propose Group-Level Data Influence Modeling (Group-MATES), a novel data-efficient pretraining method that captures and optimizes group-level data utility.","Specifically, Group-MATES collects oracle group-level influences by locally probing the pretraining model with data sets.","It then fine-tunes a relational data influence model to approximate oracles as relationship-weighted aggregations of individual influences.","The fine-tuned model selects the data subset by maximizing its group-level influence prediction, with influence-aware clustering to enable efficient inference.","Experiments on the DCLM benchmark demonstrate that Group-MATES achieves a 10% relative core score improvement on 22 downstream tasks over DCLM-Baseline and 5% over individual-influence-based methods, establishing a new state-of-the-art.","Further analyses highlight the effectiveness of relational data influence models in capturing intricate interactions between data points."],"url":"http://arxiv.org/abs/2502.14709v1"}
{"created":"2025-02-20 16:29:37","title":"Not All Data are Good Labels: On the Self-supervised Labeling for Time Series Forecasting","abstract":"Time Series Forecasting (TSF) is a crucial task in various domains, yet existing TSF models rely heavily on high-quality data and insufficiently exploit all available data. This paper explores a novel self-supervised approach to re-label time series datasets by inherently constructing candidate datasets. During the optimization of a simple reconstruction network, intermediates are used as pseudo labels in a self-supervised paradigm, improving generalization for any predictor. We introduce the Self-Correction with Adaptive Mask (SCAM), which discards overfitted components and selectively replaces them with pseudo labels generated from reconstructions. Additionally, we incorporate Spectral Norm Regularization (SNR) to further suppress overfitting from a loss landscape perspective. Our experiments on eleven real-world datasets demonstrate that SCAM consistently improves the performance of various backbone models. This work offers a new perspective on constructing datasets and enhancing the generalization of TSF models through self-supervised learning.","sentences":["Time Series Forecasting (TSF) is a crucial task in various domains, yet existing TSF models rely heavily on high-quality data and insufficiently exploit all available data.","This paper explores a novel self-supervised approach to re-label time series datasets by inherently constructing candidate datasets.","During the optimization of a simple reconstruction network, intermediates are used as pseudo labels in a self-supervised paradigm, improving generalization for any predictor.","We introduce the Self-Correction with Adaptive Mask (SCAM), which discards overfitted components and selectively replaces them with pseudo labels generated from reconstructions.","Additionally, we incorporate Spectral Norm Regularization (SNR) to further suppress overfitting from a loss landscape perspective.","Our experiments on eleven real-world datasets demonstrate that SCAM consistently improves the performance of various backbone models.","This work offers a new perspective on constructing datasets and enhancing the generalization of TSF models through self-supervised learning."],"url":"http://arxiv.org/abs/2502.14704v1"}
{"created":"2025-02-20 16:24:53","title":"Counter Pools: Counter Representation for Efficient Stream Processing","abstract":"Due to the large data volume and number of distinct elements, space is often the bottleneck of many stream processing systems. The data structures used by these systems often consist of counters whose optimization yields significant memory savings. The challenge lies in balancing the size of the counters: too small, and they overflow; too large, and memory capacity limits their number.   In this work, we suggest an efficient encoding scheme that sizes each counter according to its needs. Our approach uses fixed-sized pools of memory (e.g., a single memory word or 64 bits), where each pool manages a small number of counters. We pay special attention to performance and demonstrate considerable improvements for various streaming algorithms and workload characteristics.","sentences":["Due to the large data volume and number of distinct elements, space is often the bottleneck of many stream processing systems.","The data structures used by these systems often consist of counters whose optimization yields significant memory savings.","The challenge lies in balancing the size of the counters: too small, and they overflow; too large, and memory capacity limits their number.   ","In this work, we suggest an efficient encoding scheme that sizes each counter according to its needs.","Our approach uses fixed-sized pools of memory (e.g., a single memory word or 64 bits), where each pool manages a small number of counters.","We pay special attention to performance and demonstrate considerable improvements for various streaming algorithms and workload characteristics."],"url":"http://arxiv.org/abs/2502.14699v1"}
{"created":"2025-02-20 16:22:40","title":"General Uncertainty Estimation with Delta Variances","abstract":"Decision makers may suffer from uncertainty induced by limited data. This may be mitigated by accounting for epistemic uncertainty, which is however challenging to estimate efficiently for large neural networks. To this extent we investigate Delta Variances, a family of algorithms for epistemic uncertainty quantification, that is computationally efficient and convenient to implement. It can be applied to neural networks and more general functions composed of neural networks. As an example we consider a weather simulator with a neural-network-based step function inside -- here Delta Variances empirically obtain competitive results at the cost of a single gradient computation. The approach is convenient as it requires no changes to the neural network architecture or training procedure. We discuss multiple ways to derive Delta Variances theoretically noting that special cases recover popular techniques and present a unified perspective on multiple related methods. Finally we observe that this general perspective gives rise to a natural extension and empirically show its benefit.","sentences":["Decision makers may suffer from uncertainty induced by limited data.","This may be mitigated by accounting for epistemic uncertainty, which is however challenging to estimate efficiently for large neural networks.","To this extent we investigate Delta Variances, a family of algorithms for epistemic uncertainty quantification, that is computationally efficient and convenient to implement.","It can be applied to neural networks and more general functions composed of neural networks.","As an example we consider a weather simulator with a neural-network-based step function inside -- here Delta Variances empirically obtain competitive results at the cost of a single gradient computation.","The approach is convenient as it requires no changes to the neural network architecture or training procedure.","We discuss multiple ways to derive Delta Variances theoretically noting that special cases recover popular techniques and present a unified perspective on multiple related methods.","Finally we observe that this general perspective gives rise to a natural extension and empirically show its benefit."],"url":"http://arxiv.org/abs/2502.14698v1"}
{"created":"2025-02-20 16:19:17","title":"Revisiting Near-Far Field Boundary in Dual-Polarized XL-MIMO Systems","abstract":"Extremely large-scale multiple-input multiple-output (XL-MIMO) is expected to be an important technology in future sixth generation (6G) networks. Compared with conventional single-polarized XL-MIMO, where signals are transmitted and received in only one polarization direction, dual-polarized XL-MIMO systems achieve higher data rate by improving multiplexing performances, and thus are the focus of this paper. Due to enlarged aperture, near-field regions become non-negligible in XL-MIMO communications, necessitating accurate near-far field boundary characterizations. However, existing boundaries developed for single-polarized systems only consider phase or power differences across array elements while irrespective of cross-polarization discrimination (XPD) variances in dual-polarized XL-MIMO systems, deteriorating transmit covariance optimization performances. In this paper, we revisit near-far field boundaries for dual-polarized XL-MIMO systems by taking XPD differences into account, which faces the following challenge. Unlike existing near-far field boundaries, which only need to consider co-polarized channel components, deriving boundaries for dual-polarized XL-MIMO systems requires modeling joint effects of co-polarized and cross-polarized components. To address this issue, we model XPD variations across antennas and introduce a non-uniform XPD distance to complement existing near-far field boundaries. Based on the new distance criterion, we propose an efficient scheme to optimize transmit covariance. Numerical results validate our analysis and demonstrate the proposed algorithm's effectiveness.","sentences":["Extremely large-scale multiple-input multiple-output (XL-MIMO) is expected to be an important technology in future sixth generation (6G) networks.","Compared with conventional single-polarized XL-MIMO, where signals are transmitted and received in only one polarization direction, dual-polarized XL-MIMO systems achieve higher data rate by improving multiplexing performances, and thus are the focus of this paper.","Due to enlarged aperture, near-field regions become non-negligible in XL-MIMO communications, necessitating accurate near-far field boundary characterizations.","However, existing boundaries developed for single-polarized systems only consider phase or power differences across array elements while irrespective of cross-polarization discrimination (XPD) variances in dual-polarized XL-MIMO systems, deteriorating transmit covariance optimization performances.","In this paper, we revisit near-far field boundaries for dual-polarized XL-MIMO systems by taking XPD differences into account, which faces the following challenge.","Unlike existing near-far field boundaries, which only need to consider co-polarized channel components, deriving boundaries for dual-polarized XL-MIMO systems requires modeling joint effects of co-polarized and cross-polarized components.","To address this issue, we model XPD variations across antennas and introduce a non-uniform XPD distance to complement existing near-far field boundaries.","Based on the new distance criterion, we propose an efficient scheme to optimize transmit covariance.","Numerical results validate our analysis and demonstrate the proposed algorithm's effectiveness."],"url":"http://arxiv.org/abs/2502.14694v1"}
{"created":"2025-02-20 16:13:17","title":"SegAug: CTC-Aligned Segmented Augmentation For Robust RNN-Transducer Based Speech Recognition","abstract":"RNN-Transducer (RNN-T) is a widely adopted architecture in speech recognition, integrating acoustic and language modeling in an end-to-end framework. However, the RNN-T predictor tends to over-rely on consecutive word dependencies in training data, leading to high deletion error rates, particularly with less common or out-of-domain phrases. Existing solutions, such as regularization and data augmentation, often compromise other aspects of performance. We propose SegAug, an alignment-based augmentation technique that generates contextually varied audio-text pairs with low sentence-level semantics. This method encourages the model to focus more on acoustic features while diversifying the learned textual patterns of its internal language model, thereby reducing deletion errors and enhancing overall performance. Evaluations on the LibriSpeech and Tedlium-v3 datasets demonstrate a relative WER reduction of up to 12.5% on small-scale and 6.9% on large-scale settings. Notably, most of the improvement stems from reduced deletion errors, with relative reductions of 45.4% and 18.5%, respectively. These results highlight SegAug's effectiveness in improving RNN-T's robustness, offering a promising solution for enhancing speech recognition performance across diverse and challenging scenarios.","sentences":["RNN-Transducer (RNN-T) is a widely adopted architecture in speech recognition, integrating acoustic and language modeling in an end-to-end framework.","However, the RNN-T predictor tends to over-rely on consecutive word dependencies in training data, leading to high deletion error rates, particularly with less common or out-of-domain phrases.","Existing solutions, such as regularization and data augmentation, often compromise other aspects of performance.","We propose SegAug, an alignment-based augmentation technique that generates contextually varied audio-text pairs with low sentence-level semantics.","This method encourages the model to focus more on acoustic features while diversifying the learned textual patterns of its internal language model, thereby reducing deletion errors and enhancing overall performance.","Evaluations on the LibriSpeech and Tedlium-v3 datasets demonstrate a relative WER reduction of up to 12.5% on small-scale and 6.9% on large-scale settings.","Notably, most of the improvement stems from reduced deletion errors, with relative reductions of 45.4% and 18.5%, respectively.","These results highlight SegAug's effectiveness in improving RNN-T's robustness, offering a promising solution for enhancing speech recognition performance across diverse and challenging scenarios."],"url":"http://arxiv.org/abs/2502.14685v1"}
{"created":"2025-02-20 16:10:18","title":"seqKAN: Sequence processing with Kolmogorov-Arnold Networks","abstract":"Kolmogorov-Arnold Networks (KANs) have been recently proposed as a machine learning framework that is more interpretable and controllable than the multi-layer perceptron. Various network architectures have been proposed within the KAN framework targeting different tasks and application domains, including sequence processing.   This paper proposes seqKAN, a new KAN architecture for sequence processing. Although multiple sequence processing KAN architectures have already been proposed, we argue that seqKAN is more faithful to the core concept of the KAN framework. Furthermore, we empirically demonstrate that it achieves better results.   The empirical evaluation is performed on generated data from a complex physics problem on an interpolation and an extrapolation task. Using this dataset we compared seqKAN against a prior KAN network for timeseries prediction, recurrent deep networks, and symbolic regression. seqKAN substantially outperforms all architectures, particularly on the extrapolation dataset, while also being the most transparent.","sentences":["Kolmogorov-Arnold Networks (KANs) have been recently proposed as a machine learning framework that is more interpretable and controllable than the multi-layer perceptron.","Various network architectures have been proposed within the KAN framework targeting different tasks and application domains, including sequence processing.   ","This paper proposes seqKAN, a new KAN architecture for sequence processing.","Although multiple sequence processing KAN architectures have already been proposed, we argue that seqKAN is more faithful to the core concept of the KAN framework.","Furthermore, we empirically demonstrate that it achieves better results.   ","The empirical evaluation is performed on generated data from a complex physics problem on an interpolation and an extrapolation task.","Using this dataset we compared seqKAN against a prior KAN network for timeseries prediction, recurrent deep networks, and symbolic regression.","seqKAN substantially outperforms all architectures, particularly on the extrapolation dataset, while also being the most transparent."],"url":"http://arxiv.org/abs/2502.14681v1"}
{"created":"2025-02-20 16:09:57","title":"Disentangled Latent Spaces for Reduced Order Models using Deterministic Autoencoders","abstract":"Data-driven reduced-order models based on autoencoders generally lack interpretability compared to classical methods such as the proper orthogonal decomposition. More interpretability can be gained by disentangling the latent variables and analyzing the resulting modes. For this purpose, probabilistic $\\beta$-variational autoencoders ($\\beta$-VAEs) are frequently used in computational fluid dynamics and other simulation sciences. Using a benchmark periodic flow dataset, we show that competitive results can be achieved using non-probabilistic autoencoder approaches that either promote orthogonality or penalize correlation between latent variables. Compared to probabilistic autoencoders, these approaches offer more robustness with respect to the choice of hyperparameters entering the loss function. We further demonstrate the ability of a non-probabilistic approach to identify a reduced number of active latent variables by introducing a correlation penalty, a function also known from the use of $\\beta$-VAE. The investigated probabilistic and non-probabilistic autoencoder models are finally used for the dimensionality reduction of aircraft ditching loads, which serves as an industrial application in this work.","sentences":["Data-driven reduced-order models based on autoencoders generally lack interpretability compared to classical methods such as the proper orthogonal decomposition.","More interpretability can be gained by disentangling the latent variables and analyzing the resulting modes.","For this purpose, probabilistic $\\beta$-variational autoencoders ($\\beta$-VAEs) are frequently used in computational fluid dynamics and other simulation sciences.","Using a benchmark periodic flow dataset, we show that competitive results can be achieved using non-probabilistic autoencoder approaches that either promote orthogonality or penalize correlation between latent variables.","Compared to probabilistic autoencoders, these approaches offer more robustness with respect to the choice of hyperparameters entering the loss function.","We further demonstrate the ability of a non-probabilistic approach to identify a reduced number of active latent variables by introducing a correlation penalty, a function also known from the use of $\\beta$-VAE.","The investigated probabilistic and non-probabilistic autoencoder models are finally used for the dimensionality reduction of aircraft ditching loads, which serves as an industrial application in this work."],"url":"http://arxiv.org/abs/2502.14679v1"}
{"created":"2025-02-20 16:09:27","title":"Data-Constrained Synthesis of Training Data for De-Identification","abstract":"Many sensitive domains -- such as the clinical domain -- lack widely available datasets due to privacy risks. The increasing generative capabilities of large language models (LLMs) have made synthetic datasets a viable path forward. In this study, we domain-adapt LLMs to the clinical domain and generate synthetic clinical texts that are machine-annotated with tags for personally identifiable information using capable encoder-based NER models. The synthetic corpora are then used to train synthetic NER models. The results show that training NER models using synthetic corpora incurs only a small drop in predictive performance. The limits of this process are investigated in a systematic ablation study -- using both Swedish and Spanish data. Our analysis shows that smaller datasets can be sufficient for domain-adapting LLMs for data synthesis. Instead, the effectiveness of this process is almost entirely contingent on the performance of the machine-annotating NER models trained using the original data.","sentences":["Many sensitive domains -- such as the clinical domain -- lack widely available datasets due to privacy risks.","The increasing generative capabilities of large language models (LLMs) have made synthetic datasets a viable path forward.","In this study, we domain-adapt LLMs to the clinical domain and generate synthetic clinical texts that are machine-annotated with tags for personally identifiable information using capable encoder-based NER models.","The synthetic corpora are then used to train synthetic NER models.","The results show that training NER models using synthetic corpora incurs only a small drop in predictive performance.","The limits of this process are investigated in a systematic ablation study -- using both Swedish and Spanish data.","Our analysis shows that smaller datasets can be sufficient for domain-adapting LLMs for data synthesis.","Instead, the effectiveness of this process is almost entirely contingent on the performance of the machine-annotating NER models trained using the original data."],"url":"http://arxiv.org/abs/2502.14677v1"}
{"created":"2025-02-20 15:58:25","title":"InstructAgent: Building User Controllable Recommender via LLM Agent","abstract":"Traditional recommender systems usually take the user-platform paradigm, where users are directly exposed under the control of the platform's recommendation algorithms. However, the defect of recommendation algorithms may put users in very vulnerable positions under this paradigm. First, many sophisticated models are often designed with commercial objectives in mind, focusing on the platform's benefits, which may hinder their ability to protect and capture users' true interests. Second, these models are typically optimized using data from all users, which may overlook individual user's preferences. Due to these shortcomings, users may experience several disadvantages under the traditional user-platform direct exposure paradigm, such as lack of control over the recommender system, potential manipulation by the platform, echo chamber effects, or lack of personalization for less active users due to the dominance of active users during collaborative learning. Therefore, there is an urgent need to develop a new paradigm to protect user interests and alleviate these issues. Recently, some researchers have introduced LLM agents to simulate user behaviors, these approaches primarily aim to optimize platform-side performance, leaving core issues in recommender systems unresolved. To address these limitations, we propose a new user-agent-platform paradigm, where agent serves as the protective shield between user and recommender system that enables indirect exposure. To this end, we first construct four recommendation datasets, denoted as $\\dataset$, along with user instructions for each record.","sentences":["Traditional recommender systems usually take the user-platform paradigm, where users are directly exposed under the control of the platform's recommendation algorithms.","However, the defect of recommendation algorithms may put users in very vulnerable positions under this paradigm.","First, many sophisticated models are often designed with commercial objectives in mind, focusing on the platform's benefits, which may hinder their ability to protect and capture users' true interests.","Second, these models are typically optimized using data from all users, which may overlook individual user's preferences.","Due to these shortcomings, users may experience several disadvantages under the traditional user-platform direct exposure paradigm, such as lack of control over the recommender system, potential manipulation by the platform, echo chamber effects, or lack of personalization for less active users due to the dominance of active users during collaborative learning.","Therefore, there is an urgent need to develop a new paradigm to protect user interests and alleviate these issues.","Recently, some researchers have introduced LLM agents to simulate user behaviors, these approaches primarily aim to optimize platform-side performance, leaving core issues in recommender systems unresolved.","To address these limitations, we propose a new user-agent-platform paradigm, where agent serves as the protective shield between user and recommender system that enables indirect exposure.","To this end, we first construct four recommendation datasets, denoted as $\\dataset$, along with user instructions for each record."],"url":"http://arxiv.org/abs/2502.14662v1"}
{"created":"2025-02-20 15:54:37","title":"MAGO-SP: Detection and Correction of Water-Fat Swaps in Magnitude-Only VIBE MRI","abstract":"Volume Interpolated Breath-Hold Examination (VIBE) MRI generates images suitable for water and fat signal composition estimation. While the two-point VIBE provides water-fat-separated images, the six-point VIBE allows estimation of the effective transversal relaxation rate R2* and the proton density fat fraction (PDFF), which are imaging markers for health and disease. Ambiguity during signal reconstruction can lead to water-fat swaps. This shortcoming challenges the application of VIBE-MRI for automated PDFF analyses of large-scale clinical data and of population studies. This study develops an automated pipeline to detect and correct water-fat swaps in non-contrast-enhanced VIBE images. Our three-step pipeline begins with training a segmentation network to classify volumes as \"fat-like\" or \"water-like,\" using synthetic water-fat swaps generated by merging fat and water volumes with Perlin noise. Next, a denoising diffusion image-to-image network predicts water volumes as signal priors for correction. Finally, we integrate this prior into a physics-constrained model to recover accurate water and fat signals. Our approach achieves a < 1% error rate in water-fat swap detection for a 6-point VIBE. Notably, swaps disproportionately affect individuals in the Underweight and Class 3 Obesity BMI categories. Our correction algorithm ensures accurate solution selection in chemical phase MRIs, enabling reliable PDFF estimation. This forms a solid technical foundation for automated large-scale population imaging analysis.","sentences":["Volume Interpolated Breath-Hold Examination (VIBE) MRI generates images suitable for water and fat signal composition estimation.","While the two-point VIBE provides water-fat-separated images, the six-point VIBE allows estimation of the effective transversal relaxation rate R2* and the proton density fat fraction (PDFF), which are imaging markers for health and disease.","Ambiguity during signal reconstruction can lead to water-fat swaps.","This shortcoming challenges the application of VIBE-MRI for automated PDFF analyses of large-scale clinical data and of population studies.","This study develops an automated pipeline to detect and correct water-fat swaps in non-contrast-enhanced VIBE images.","Our three-step pipeline begins with training a segmentation network to classify volumes as \"fat-like\" or \"water-like,\" using synthetic water-fat swaps generated by merging fat and water volumes with Perlin noise.","Next, a denoising diffusion image-to-image network predicts water volumes as signal priors for correction.","Finally, we integrate this prior into a physics-constrained model to recover accurate water and fat signals.","Our approach achieves a < 1% error rate in water-fat swap detection for a 6-point VIBE.","Notably, swaps disproportionately affect individuals in the Underweight and Class 3 Obesity BMI categories.","Our correction algorithm ensures accurate solution selection in chemical phase MRIs, enabling reliable PDFF estimation.","This forms a solid technical foundation for automated large-scale population imaging analysis."],"url":"http://arxiv.org/abs/2502.14659v1"}
{"created":"2025-02-20 15:22:00","title":"Condorcet Winners and Anscombes Paradox Under Weighted Binary Voting","abstract":"We consider voting on multiple independent binary issues. In addition, a weighting vector for each voter defines how important they consider each issue. The most natural way to aggregate the votes into a single unified proposal is issue-wise majority (IWM): taking a majority opinion for each issue. However, in a scenario known as Ostrogorski's Paradox, an IWM proposal may not be a Condorcet winner, or it may even fail to garner majority support in a special case known as Anscombe's Paradox.   We show that it is co-NP-hard to determine whether there exists a Condorcet-winning proposal even without weights. In contrast, we prove that the single-switch condition provides an Ostrogorski-free voting domain under identical weighting vectors. We show that verifying the condition can be achieved in linear time and no-instances admit short, efficiently computable proofs in the form of forbidden substructures. On the way, we give the simplest linear-time test for the voter/candidate-extremal-interval condition in approval voting and the simplest and most efficient algorithm for recognizing single-crossing preferences in ordinal voting.   We then tackle Anscombe's Paradox. Under identical weight vectors, we can guarantee a majority-supported proposal agreeing with IWM on strictly more than half of the overall weight, while with two distinct weight vectors, such proposals can get arbitrarily far from IWM. The severity of such examples is controlled by the maximum average topic weight $\\tilde{w}_{max}$: a simple bound derived from a partition-based approach is tight on a large portion of the range $\\tilde{w}_{max} \\in (0,1)$. Finally, we extend Wagner's rule to the weighted setting: an average majority across topics of at least $\\frac{3}{4}$'s precludes Anscombe's paradox from occurring.","sentences":["We consider voting on multiple independent binary issues.","In addition, a weighting vector for each voter defines how important they consider each issue.","The most natural way to aggregate the votes into a single unified proposal is issue-wise majority (IWM): taking a majority opinion for each issue.","However, in a scenario known as Ostrogorski's Paradox, an IWM proposal may not be a Condorcet winner, or it may even fail to garner majority support in a special case known as Anscombe's Paradox.   ","We show that it is co-NP-hard to determine whether there exists a Condorcet-winning proposal even without weights.","In contrast, we prove that the single-switch condition provides an Ostrogorski-free voting domain under identical weighting vectors.","We show that verifying the condition can be achieved in linear time and no-instances admit short, efficiently computable proofs in the form of forbidden substructures.","On the way, we give the simplest linear-time test for the voter/candidate-extremal-interval condition in approval voting and the simplest and most efficient algorithm for recognizing single-crossing preferences in ordinal voting.   ","We then tackle Anscombe's Paradox.","Under identical weight vectors, we can guarantee a majority-supported proposal agreeing with IWM on strictly more than half of the overall weight, while with two distinct weight vectors, such proposals can get arbitrarily far from IWM.","The severity of such examples is controlled by the maximum average topic weight $\\tilde{w}_{max}$: a simple bound derived from a partition-based approach is tight on a large portion of the range $\\tilde{w}_{max} \\in (0,1)$.","Finally, we extend Wagner's rule to the weighted setting: an average majority across topics of at least $\\frac{3}{4}$'s precludes Anscombe's paradox from occurring."],"url":"http://arxiv.org/abs/2502.14639v1"}
{"created":"2025-02-20 15:10:05","title":"Augmenting Coaching with GenAI: Insights into Use, Effectiveness, and Future Potential","abstract":"The integration of generative AI (GenAI) tools, particularly large language models (LLMs), is transforming professional coaching workflows. This study explores how coaches use GenAI, the perceived benefits and limitations of these tools, and broader attitudes toward AI-assisted coaching. A survey of 205 coaching professionals reveals widespread adoption of GenAI for research, content creation, and administrative support, while its role in relational and interpretative coaching remains limited. Findings indicate that AI literacy and perceived AI impact strongly predict GenAI adoption, with positive attitudes fostering greater use. Ethical considerations, particularly transparency and data privacy, are a key concern, with frequent AI users demonstrating greater ethical awareness. Regression analyses show that while perceived effectiveness drives GenAI adoption, concerns about AI replacing human coaches do not significantly influence usage. Coaches express interest in future AI capabilities that enhance personalization, real-time feedback, and administrative automation while maintaining human oversight. The study highlights that GenAI functions best as an augmentation tool rather than a replacement, emphasizing the need for AI literacy training, ethical guidelines, and human-centered AI integration. These findings contribute to the ongoing discourse on human-AI collaboration, advocating for responsible and effective AI adoption in professional coaching.","sentences":["The integration of generative AI (GenAI) tools, particularly large language models (LLMs), is transforming professional coaching workflows.","This study explores how coaches use GenAI, the perceived benefits and limitations of these tools, and broader attitudes toward AI-assisted coaching.","A survey of 205 coaching professionals reveals widespread adoption of GenAI for research, content creation, and administrative support, while its role in relational and interpretative coaching remains limited.","Findings indicate that AI literacy and perceived AI impact strongly predict GenAI adoption, with positive attitudes fostering greater use.","Ethical considerations, particularly transparency and data privacy, are a key concern, with frequent AI users demonstrating greater ethical awareness.","Regression analyses show that while perceived effectiveness drives GenAI adoption, concerns about AI replacing human coaches do not significantly influence usage.","Coaches express interest in future AI capabilities that enhance personalization, real-time feedback, and administrative automation while maintaining human oversight.","The study highlights that GenAI functions best as an augmentation tool rather than a replacement, emphasizing the need for AI literacy training, ethical guidelines, and human-centered AI integration.","These findings contribute to the ongoing discourse on human-AI collaboration, advocating for responsible and effective AI adoption in professional coaching."],"url":"http://arxiv.org/abs/2502.14632v1"}
{"created":"2025-02-20 15:10:00","title":"Synergistic Fusion of Multi-Source Knowledge via Evidence Theory for High-Entropy Alloy Discovery","abstract":"Discovering novel high-entropy alloys (HEAs) with desirable properties is challenging due to the vast compositional space and complex phase formation mechanisms. Efficient exploration of this space requires a strategic approach that integrates heterogeneous knowledge sources. Here, we propose a framework that systematically combines knowledge extracted from computational material datasets with domain knowledge distilled from scientific literature using large language models (LLMs). A central feature of this approach is the explicit consideration of element substitutability, identifying chemically similar elements that can be interchanged to potentially stabilize desired HEAs. Dempster-Shafer theory, a mathematical framework for reasoning under uncertainty, is employed to model and combine substitutabilities based on aggregated evidence from multiple sources. The framework predicts the phase stability of candidate HEA compositions and is systematically evaluated on both quaternary alloy systems, demonstrating superior performance compared to baseline machine learning models and methods reliant on single-source evidence in cross-validation experiments. By leveraging multi-source knowledge, the framework retains robust predictive power even when key elements are absent from the training data, underscoring its potential for knowledge transfer and extrapolation. Furthermore, the enhanced interpretability of the methodology offers insights into the fundamental factors governing HEA formation. Overall, this work provides a promising strategy for accelerating HEA discovery by integrating computational and textual knowledge sources, enabling efficient exploration of vast compositional spaces with improved generalization and interpretability.","sentences":["Discovering novel high-entropy alloys (HEAs) with desirable properties is challenging due to the vast compositional space and complex phase formation mechanisms.","Efficient exploration of this space requires a strategic approach that integrates heterogeneous knowledge sources.","Here, we propose a framework that systematically combines knowledge extracted from computational material datasets with domain knowledge distilled from scientific literature using large language models (LLMs).","A central feature of this approach is the explicit consideration of element substitutability, identifying chemically similar elements that can be interchanged to potentially stabilize desired HEAs.","Dempster-Shafer theory, a mathematical framework for reasoning under uncertainty, is employed to model and combine substitutabilities based on aggregated evidence from multiple sources.","The framework predicts the phase stability of candidate HEA compositions and is systematically evaluated on both quaternary alloy systems, demonstrating superior performance compared to baseline machine learning models and methods reliant on single-source evidence in cross-validation experiments.","By leveraging multi-source knowledge, the framework retains robust predictive power even when key elements are absent from the training data, underscoring its potential for knowledge transfer and extrapolation.","Furthermore, the enhanced interpretability of the methodology offers insights into the fundamental factors governing HEA formation.","Overall, this work provides a promising strategy for accelerating HEA discovery by integrating computational and textual knowledge sources, enabling efficient exploration of vast compositional spaces with improved generalization and interpretability."],"url":"http://arxiv.org/abs/2502.14631v1"}
{"created":"2025-02-20 15:06:15","title":"ATRI: Mitigating Multilingual Audio Text Retrieval Inconsistencies by Reducing Data Distribution Errors","abstract":"Multilingual audio-text retrieval (ML-ATR) is a challenging task that aims to retrieve audio clips or multilingual texts from databases. However, existing ML-ATR schemes suffer from inconsistencies for instance similarity matching across languages. We theoretically analyze the inconsistency in terms of both multilingual modal alignment direction error and weight error, and propose the theoretical weight error upper bound for quantifying the inconsistency. Based on the analysis of the weight error upper bound, we find that the inconsistency problem stems from the data distribution error caused by random sampling of languages. We propose a consistent ML-ATR scheme using 1-to-k contrastive learning and audio-English co-anchor contrastive learning, aiming to mitigate the negative impact of data distribution error on recall and consistency in ML-ATR. Experimental results on the translated AudioCaps and Clotho datasets show that our scheme achieves state-of-the-art performance on recall and consistency metrics for eight mainstream languages, including English. Our code will be available at https://github.com/ATRI-ACL/ATRI-ACL.","sentences":["Multilingual audio-text retrieval (ML-ATR) is a challenging task that aims to retrieve audio clips or multilingual texts from databases.","However, existing ML-ATR schemes suffer from inconsistencies for instance similarity matching across languages.","We theoretically analyze the inconsistency in terms of both multilingual modal alignment direction error and weight error, and propose the theoretical weight error upper bound for quantifying the inconsistency.","Based on the analysis of the weight error upper bound, we find that the inconsistency problem stems from the data distribution error caused by random sampling of languages.","We propose a consistent ML-ATR scheme using 1-to-k contrastive learning and audio-English co-anchor contrastive learning, aiming to mitigate the negative impact of data distribution error on recall and consistency in ML-ATR.","Experimental results on the translated AudioCaps and Clotho datasets show that our scheme achieves state-of-the-art performance on recall and consistency metrics for eight mainstream languages, including English.","Our code will be available at https://github.com/ATRI-ACL/ATRI-ACL."],"url":"http://arxiv.org/abs/2502.14627v1"}
{"created":"2025-02-20 15:05:00","title":"Multi-Record Web Page Information Extraction From News Websites","abstract":"In this paper, we focused on the problem of extracting information from web pages containing many records, a task of growing importance in the era of massive web data. Recently, the development of neural network methods has improved the quality of information extraction from web pages. Nevertheless, most of the research and datasets are aimed at studying detailed pages. This has left multi-record \"list pages\" relatively understudied, despite their widespread presence and practical significance.   To address this gap, we created a large-scale, open-access dataset specifically designed for list pages. This is the first dataset for this task in the Russian language. Our dataset contains 13,120 web pages with news lists, significantly exceeding existing datasets in both scale and complexity. Our dataset contains attributes of various types, including optional and multi-valued, providing a realistic representation of real-world list pages. These features make our dataset a valuable resource for studying information extraction from pages containing many records.   Furthermore, we proposed our own multi-stage information extraction methods. In this work, we explore and demonstrate several strategies for applying MarkupLM to the specific challenges of multi-record web pages. Our experiments validate the advantages of our methods.   By releasing our dataset to the public, we aim to advance the field of information extraction from multi-record pages.","sentences":["In this paper, we focused on the problem of extracting information from web pages containing many records, a task of growing importance in the era of massive web data.","Recently, the development of neural network methods has improved the quality of information extraction from web pages.","Nevertheless, most of the research and datasets are aimed at studying detailed pages.","This has left multi-record \"list pages\" relatively understudied, despite their widespread presence and practical significance.   ","To address this gap, we created a large-scale, open-access dataset specifically designed for list pages.","This is the first dataset for this task in the Russian language.","Our dataset contains 13,120 web pages with news lists, significantly exceeding existing datasets in both scale and complexity.","Our dataset contains attributes of various types, including optional and multi-valued, providing a realistic representation of real-world list pages.","These features make our dataset a valuable resource for studying information extraction from pages containing many records.   ","Furthermore, we proposed our own multi-stage information extraction methods.","In this work, we explore and demonstrate several strategies for applying MarkupLM to the specific challenges of multi-record web pages.","Our experiments validate the advantages of our methods.   ","By releasing our dataset to the public, we aim to advance the field of information extraction from multi-record pages."],"url":"http://arxiv.org/abs/2502.14625v1"}
{"created":"2025-02-20 15:03:50","title":"Online Envy Minimization and Multicolor Discrepancy: Equivalences and Separations","abstract":"We consider the fundamental problem of allocating $T$ indivisible items that arrive over time to $n$ agents with additive preferences, with the goal of minimizing envy. This problem is tightly connected to online multicolor discrepancy: vectors $v_1, \\dots, v_T \\in \\mathbb{R}^d$ with $\\| v_i \\|_2 \\leq 1$ arrive over time and must be, immediately and irrevocably, assigned to one of $n$ colors to minimize $\\max_{i,j \\in [n]} \\| \\sum_{v \\in S_i} v - \\sum_{v \\in S_j} v \\|_{\\infty}$ at each step, where $S_\\ell$ is the set of vectors that are assigned color $\\ell$. The special case of $n = 2$ is called online vector balancing. Any bound for multicolor discrepancy implies the same bound for envy minimization. Against an adaptive adversary, both problems have the same optimal bound, $\\Theta(\\sqrt{T})$, but whether this holds for weaker adversaries is unknown.   Against an oblivious adversary, Alweiss et al. give a $O(\\log T)$ bound, with high probability, for multicolor discrepancy. Kulkarni et al. improve this to $O(\\sqrt{\\log T})$ for vector balancing and give a matching lower bound. Whether a $O(\\sqrt{\\log T})$ bound holds for multicolor discrepancy remains open. These results imply the best-known upper bounds for envy minimization (for an oblivious adversary) for $n$ and two agents, respectively; whether better bounds exist is open.   In this paper, we resolve all aforementioned open problems. We prove that online envy minimization and multicolor discrepancy are equivalent against an oblivious adversary: we give a $O(\\sqrt{\\log T})$ upper bound for multicolor discrepancy, and a $\\Omega(\\sqrt{\\log T})$ lower bound for envy minimization. For a weaker, i.i.d. adversary, we prove a separation: For online vector balancing, we give a $\\Omega\\left(\\sqrt{\\frac{\\log T}{\\log \\log T}}\\right)$ lower bound, while for envy minimization, we give an algorithm that guarantees a constant upper bound.","sentences":["We consider the fundamental problem of allocating $T$ indivisible items that arrive over time to $n$ agents with additive preferences, with the goal of minimizing envy.","This problem is tightly connected to online multicolor discrepancy: vectors $v_1, \\dots, v_T \\in \\mathbb{R}^d$ with $\\| v_i","\\|_2 \\leq 1$ arrive over time and must be, immediately and irrevocably, assigned to one of $n$ colors to minimize $\\max_{i,j \\in [n]} \\| \\sum_{v \\in S_i} v - \\sum_{v \\in S_j} v \\|_{\\infty}$ at each step, where $S_\\ell$ is the set of vectors that are assigned color $\\ell$. The special case of $n = 2$ is called online vector balancing.","Any bound for multicolor discrepancy implies the same bound for envy minimization.","Against an adaptive adversary, both problems have the same optimal bound, $\\Theta(\\sqrt{T})$, but whether this holds for weaker adversaries is unknown.   ","Against an oblivious adversary, Alweiss et al. give a $O(\\log T)$ bound, with high probability, for multicolor discrepancy.","Kulkarni et al. improve this to $O(\\sqrt{\\log T})$ for vector balancing and give a matching lower bound.","Whether a $O(\\sqrt{\\log T})$ bound holds for multicolor discrepancy remains open.","These results imply the best-known upper bounds for envy minimization (for an oblivious adversary) for $n$ and two agents, respectively; whether better bounds exist is open.   ","In this paper, we resolve all aforementioned open problems.","We prove that online envy minimization and multicolor discrepancy are equivalent against an oblivious adversary: we give a $O(\\sqrt{\\log T})$ upper bound for multicolor discrepancy, and a $\\Omega(\\sqrt{\\log T})$ lower bound for envy minimization.","For a weaker, i.i.d. adversary, we prove a separation: For online vector balancing, we give a $\\Omega\\left(\\sqrt{\\frac{\\log T}{\\log","\\log T}}\\right)$ lower bound, while for envy minimization, we give an algorithm that guarantees a constant upper bound."],"url":"http://arxiv.org/abs/2502.14624v1"}
{"created":"2025-02-20 14:57:08","title":"Serving Models, Fast and Slow:Optimizing Heterogeneous LLM Inferencing Workloads at Scale","abstract":"Large Language Model (LLM) inference workloads handled by global cloud providers can include both latency-sensitive and insensitive tasks, creating a diverse range of Service Level Agreement (SLA) requirements. Managing these mixed workloads is challenging due to the complexity of the inference stack, which includes multiple LLMs, hardware configurations, and geographic distributions. Current optimization strategies often silo these tasks to ensure that SLAs are met for latency-sensitive tasks, but this leads to significant under-utilization of expensive GPU resources despite the availability of spot and on-demand Virtual Machine (VM) provisioning. We propose SAGESERVE, a comprehensive LLM serving framework that employs adaptive control knobs at varying time scales, ensuring SLA compliance while maximizing the utilization of valuable GPU resources. Short-term optimizations include efficient request routing to data center regions, while long-term strategies involve scaling GPU VMs out/in and redeploying models to existing VMs to align with traffic patterns. These strategies are formulated as an optimization problem for resource allocation and solved using Integer Linear Programming (ILP). We perform empirical and simulation studies based on production workload traces with over 8M requests using four open-source models deployed across three regions. SAGESERVE achieves up to 25% savings in GPU-hours while maintaining tail latency and satisfying all SLOs, and it reduces the scaling overhead compared to baselines by up to 80%, confirming the effectiveness of our proposal. In terms of dollar cost, this can save cloud providers up to $2M over the course of a month.","sentences":["Large Language Model (LLM) inference workloads handled by global cloud providers can include both latency-sensitive and insensitive tasks, creating a diverse range of Service Level Agreement (SLA) requirements.","Managing these mixed workloads is challenging due to the complexity of the inference stack, which includes multiple LLMs, hardware configurations, and geographic distributions.","Current optimization strategies often silo these tasks to ensure that SLAs are met for latency-sensitive tasks, but this leads to significant under-utilization of expensive GPU resources despite the availability of spot and on-demand Virtual Machine (VM) provisioning.","We propose SAGESERVE, a comprehensive LLM serving framework that employs adaptive control knobs at varying time scales, ensuring SLA compliance while maximizing the utilization of valuable GPU resources.","Short-term optimizations include efficient request routing to data center regions, while long-term strategies involve scaling GPU VMs out/in and redeploying models to existing VMs to align with traffic patterns.","These strategies are formulated as an optimization problem for resource allocation and solved using Integer Linear Programming (ILP).","We perform empirical and simulation studies based on production workload traces with over 8M requests using four open-source models deployed across three regions.","SAGESERVE achieves up to 25% savings in GPU-hours while maintaining tail latency and satisfying all SLOs, and it reduces the scaling overhead compared to baselines by up to 80%, confirming the effectiveness of our proposal.","In terms of dollar cost, this can save cloud providers up to $2M over the course of a month."],"url":"http://arxiv.org/abs/2502.14617v1"}
{"created":"2025-02-20 14:51:41","title":"Enumerating minimal dominating sets and variants in chordal bipartite graphs","abstract":"Enumerating minimal dominating sets with polynomial delay in bipartite graphs is a long-standing open problem. To date, even the subcase of chordal bipartite graphs is open, with the best known algorithm due to Golovach, Heggernes, Kant\\'e, Kratsch, Saether, and Villanger running in incremental-polynomial time. We improve on this result by providing a polynomial delay and space algorithm enumerating minimal dominating sets in chordal bipartite graphs. Additionally, we show that the total and connected variants admit polynomial and incremental-polynomial delay algorithms, respectively, within the same class. This provides an alternative proof of a result by Golovach et al. for total dominating sets, and answers an open question for the connected variant. Finally, we give evidence that the techniques used in this paper cannot be generalized to bipartite graphs for (total) minimal dominating sets, unless P = NP, and show that enumerating minimal connected dominating sets in bipartite graphs is harder than enumerating minimal transversals in general hypergraphs.","sentences":["Enumerating minimal dominating sets with polynomial delay in bipartite graphs is a long-standing open problem.","To date, even the subcase of chordal bipartite graphs is open, with the best known algorithm due to Golovach, Heggernes, Kant\\'e, Kratsch, Saether, and Villanger running in incremental-polynomial time.","We improve on this result by providing a polynomial delay and space algorithm enumerating minimal dominating sets in chordal bipartite graphs.","Additionally, we show that the total and connected variants admit polynomial and incremental-polynomial delay algorithms, respectively, within the same class.","This provides an alternative proof of a result by Golovach et al. for total dominating sets, and answers an open question for the connected variant.","Finally, we give evidence that the techniques used in this paper cannot be generalized to bipartite graphs for (total) minimal dominating sets, unless P = NP, and show that enumerating minimal connected dominating sets in bipartite graphs is harder than enumerating minimal transversals in general hypergraphs."],"url":"http://arxiv.org/abs/2502.14611v1"}
{"created":"2025-02-20 14:37:53","title":"Noisy Test-Time Adaptation in Vision-Language Models","abstract":"Test-time adaptation (TTA) aims to address distribution shifts between source and target data by relying solely on target data during testing. In open-world scenarios, models often encounter noisy samples, i.e., samples outside the in-distribution (ID) label space. Leveraging the zero-shot capability of pre-trained vision-language models (VLMs), this paper introduces Zero-Shot Noisy TTA (ZS-NTTA), focusing on adapting the model to target data with noisy samples during test-time in a zero-shot manner. We find existing TTA methods underperform under ZS-NTTA, often lagging behind even the frozen model. We conduct comprehensive experiments to analyze this phenomenon, revealing that the negative impact of unfiltered noisy data outweighs the benefits of clean data during model updating. Also, adapting a classifier for ID classification and noise detection hampers both sub-tasks. Built on this, we propose a framework that decouples the classifier and detector, focusing on developing an individual detector while keeping the classifier frozen. Technically, we introduce the Adaptive Noise Detector (AdaND), which utilizes the frozen model's outputs as pseudo-labels to train a noise detector. To handle clean data streams, we further inject Gaussian noise during adaptation, preventing the detector from misclassifying clean samples as noisy. Beyond the ZS-NTTA, AdaND can also improve the zero-shot out-of-distribution (ZS-OOD) detection ability of VLMs. Experiments show that AdaND outperforms in both ZS-NTTA and ZS-OOD detection. On ImageNet, AdaND achieves a notable improvement of $8.32\\%$ in harmonic mean accuracy ($\\text{Acc}_\\text{H}$) for ZS-NTTA and $9.40\\%$ in FPR95 for ZS-OOD detection, compared to SOTA methods. Importantly, AdaND is computationally efficient and comparable to the model-frozen method. The code is publicly available at: https://github.com/tmlr-group/ZS-NTTA.","sentences":["Test-time adaptation (TTA) aims to address distribution shifts between source and target data by relying solely on target data during testing.","In open-world scenarios, models often encounter noisy samples, i.e., samples outside the in-distribution (ID) label space.","Leveraging the zero-shot capability of pre-trained vision-language models (VLMs), this paper introduces Zero-Shot Noisy TTA (ZS-NTTA), focusing on adapting the model to target data with noisy samples during test-time in a zero-shot manner.","We find existing TTA methods underperform under ZS-NTTA, often lagging behind even the frozen model.","We conduct comprehensive experiments to analyze this phenomenon, revealing that the negative impact of unfiltered noisy data outweighs the benefits of clean data during model updating.","Also, adapting a classifier for ID classification and noise detection hampers both sub-tasks.","Built on this, we propose a framework that decouples the classifier and detector, focusing on developing an individual detector while keeping the classifier frozen.","Technically, we introduce the Adaptive Noise Detector (AdaND), which utilizes the frozen model's outputs as pseudo-labels to train a noise detector.","To handle clean data streams, we further inject Gaussian noise during adaptation, preventing the detector from misclassifying clean samples as noisy.","Beyond the ZS-NTTA, AdaND can also improve the zero-shot out-of-distribution (ZS-OOD) detection ability of VLMs.","Experiments show that AdaND outperforms in both ZS-NTTA and ZS-OOD detection.","On ImageNet, AdaND achieves a notable improvement of $8.32\\%$ in harmonic mean accuracy ($\\text{Acc}_\\text{H}$) for ZS-NTTA and $9.40\\%$ in FPR95 for ZS-OOD detection, compared to SOTA methods.","Importantly, AdaND is computationally efficient and comparable to the model-frozen method.","The code is publicly available at: https://github.com/tmlr-group/ZS-NTTA."],"url":"http://arxiv.org/abs/2502.14604v1"}
{"created":"2025-02-20 14:30:18","title":"Multi-Class Imbalanced Learning with Support Vector Machines via Differential Evolution","abstract":"Support vector machine (SVM) is a powerful machine learning algorithm to handle classification tasks. However, the classical SVM is developed for binary problems with the assumption of balanced datasets. Obviously, the multi-class imbalanced classification problems are more complex. In this paper, we propose an improved SVM via Differential Evolution (i-SVM-DE) method to deal with it. An improved SVM (i-SVM) model is proposed to handle the data imbalance by combining cost sensitive technique and separation margin modification in the constraints, which formalize a parameter optimization problem. By using one-versus-one (OVO) scheme, a multi-class problem is decomposed into a number of binary subproblems. A large optimization problem is formalized through concatenating the parameters in the binary subproblems. To find the optimal model effectively and learn the support vectors for each class simultaneously, an improved differential evolution (DE) algorithm is applied to solve this large optimization problem. Instead of the validation set, we propose the fitness functions to evaluate the learned model and obtain the optimal parameters in the search process of DE. A series of experiments are carried out to verify the benefits of our proposed method. The results indicate that i-SVM-DE is statistically superior by comparing with the other baseline methods.","sentences":["Support vector machine (SVM) is a powerful machine learning algorithm to handle classification tasks.","However, the classical SVM is developed for binary problems with the assumption of balanced datasets.","Obviously, the multi-class imbalanced classification problems are more complex.","In this paper, we propose an improved SVM via Differential Evolution (i-SVM-DE) method to deal with it.","An improved SVM (i-SVM) model is proposed to handle the data imbalance by combining cost sensitive technique and separation margin modification in the constraints, which formalize a parameter optimization problem.","By using one-versus-one (OVO) scheme, a multi-class problem is decomposed into a number of binary subproblems.","A large optimization problem is formalized through concatenating the parameters in the binary subproblems.","To find the optimal model effectively and learn the support vectors for each class simultaneously, an improved differential evolution (DE) algorithm is applied to solve this large optimization problem.","Instead of the validation set, we propose the fitness functions to evaluate the learned model and obtain the optimal parameters in the search process of DE.","A series of experiments are carried out to verify the benefits of our proposed method.","The results indicate that i-SVM-DE is statistically superior by comparing with the other baseline methods."],"url":"http://arxiv.org/abs/2502.14597v1"}
{"created":"2025-02-20 14:16:15","title":"Moshi Moshi? A Model Selection Hijacking Adversarial Attack","abstract":"Model selection is a fundamental task in Machine Learning~(ML), focusing on selecting the most suitable model from a pool of candidates by evaluating their performance on specific metrics. This process ensures optimal performance, computational efficiency, and adaptability to diverse tasks and environments. Despite its critical role, its security from the perspective of adversarial ML remains unexplored. This risk is heightened in the Machine-Learning-as-a-Service model, where users delegate the training phase and the model selection process to third-party providers, supplying data and training strategies. Therefore, attacks on model selection could harm both the user and the provider, undermining model performance and driving up operational costs.   In this work, we present MOSHI (MOdel Selection HIjacking adversarial attack), the first adversarial attack specifically targeting model selection. Our novel approach manipulates model selection data to favor the adversary, even without prior knowledge of the system. Utilizing a framework based on Variational Auto Encoders, we provide evidence that an attacker can induce inefficiencies in ML deployment. We test our attack on diverse computer vision and speech recognition benchmark tasks and different settings, obtaining an average attack success rate of 75.42%. In particular, our attack causes an average 88.30% decrease in generalization capabilities, an 83.33% increase in latency, and an increase of up to 105.85% in energy consumption. These results highlight the significant vulnerabilities in model selection processes and their potential impact on real-world applications.","sentences":["Model selection is a fundamental task in Machine Learning~(ML), focusing on selecting the most suitable model from a pool of candidates by evaluating their performance on specific metrics.","This process ensures optimal performance, computational efficiency, and adaptability to diverse tasks and environments.","Despite its critical role, its security from the perspective of adversarial ML remains unexplored.","This risk is heightened in the Machine-Learning-as-a-Service model, where users delegate the training phase and the model selection process to third-party providers, supplying data and training strategies.","Therefore, attacks on model selection could harm both the user and the provider, undermining model performance and driving up operational costs.   ","In this work, we present MOSHI (MOdel Selection HIjacking adversarial attack), the first adversarial attack specifically targeting model selection.","Our novel approach manipulates model selection data to favor the adversary, even without prior knowledge of the system.","Utilizing a framework based on Variational Auto Encoders, we provide evidence that an attacker can induce inefficiencies in ML deployment.","We test our attack on diverse computer vision and speech recognition benchmark tasks and different settings, obtaining an average attack success rate of 75.42%.","In particular, our attack causes an average 88.30% decrease in generalization capabilities, an 83.33% increase in latency, and an increase of up to 105.85% in energy consumption.","These results highlight the significant vulnerabilities in model selection processes and their potential impact on real-world applications."],"url":"http://arxiv.org/abs/2502.14586v1"}
{"created":"2025-02-20 14:13:24","title":"A Theory for Conditional Generative Modeling on Multiple Data Sources","abstract":"The success of large generative models has driven a paradigm shift, leveraging massive multi-source data to enhance model capabilities. However, the interaction among these sources remains theoretically underexplored. This paper takes the first step toward a rigorous analysis of multi-source training in conditional generative modeling, where each condition represents a distinct data source. Specifically, we establish a general distribution estimation error bound in average total variation distance for conditional maximum likelihood estimation based on the bracketing number. Our result shows that when source distributions share certain similarities and the model is expressive enough, multi-source training guarantees a sharper bound than single-source training. We further instantiate the general theory on conditional Gaussian estimation and deep generative models including autoregressive and flexible energy-based models, by characterizing their bracketing numbers. The results highlight that the number of sources and similarity among source distributions improve the advantage of multi-source training. Simulations and real-world experiments validate our theory. Code is available at: \\url{https://github.com/ML-GSAI/Multi-Source-GM}.","sentences":["The success of large generative models has driven a paradigm shift, leveraging massive multi-source data to enhance model capabilities.","However, the interaction among these sources remains theoretically underexplored.","This paper takes the first step toward a rigorous analysis of multi-source training in conditional generative modeling, where each condition represents a distinct data source.","Specifically, we establish a general distribution estimation error bound in average total variation distance for conditional maximum likelihood estimation based on the bracketing number.","Our result shows that when source distributions share certain similarities and the model is expressive enough, multi-source training guarantees a sharper bound than single-source training.","We further instantiate the general theory on conditional Gaussian estimation and deep generative models including autoregressive and flexible energy-based models, by characterizing their bracketing numbers.","The results highlight that the number of sources and similarity among source distributions improve the advantage of multi-source training.","Simulations and real-world experiments validate our theory.","Code is available at: \\url{https://github.com/ML-GSAI/Multi-Source-GM}."],"url":"http://arxiv.org/abs/2502.14583v1"}
{"created":"2025-02-20 13:59:40","title":"Self-supervised Monocular Depth Estimation Robust to Reflective Surface Leveraged by Triplet Mining","abstract":"Self-supervised monocular depth estimation (SSMDE) aims to predict the dense depth map of a monocular image, by learning depth from RGB image sequences, eliminating the need for ground-truth depth labels. Although this approach simplifies data acquisition compared to supervised methods, it struggles with reflective surfaces, as they violate the assumptions of Lambertian reflectance, leading to inaccurate training on such surfaces. To tackle this problem, we propose a novel training strategy for an SSMDE by leveraging triplet mining to pinpoint reflective regions at the pixel level, guided by the camera geometry between different viewpoints. The proposed reflection-aware triplet mining loss specifically penalizes the inappropriate photometric error minimization on the localized reflective regions while preserving depth accuracy in non-reflective areas. We also incorporate a reflection-aware knowledge distillation method that enables a student model to selectively learn the pixel-level knowledge from reflective and non-reflective regions. This results in robust depth estimation across areas. Evaluation results on multiple datasets demonstrate that our method effectively enhances depth quality on reflective surfaces and outperforms state-of-the-art SSMDE baselines.","sentences":["Self-supervised monocular depth estimation (SSMDE) aims to predict the dense depth map of a monocular image, by learning depth from RGB image sequences, eliminating the need for ground-truth depth labels.","Although this approach simplifies data acquisition compared to supervised methods, it struggles with reflective surfaces, as they violate the assumptions of Lambertian reflectance, leading to inaccurate training on such surfaces.","To tackle this problem, we propose a novel training strategy for an SSMDE by leveraging triplet mining to pinpoint reflective regions at the pixel level, guided by the camera geometry between different viewpoints.","The proposed reflection-aware triplet mining loss specifically penalizes the inappropriate photometric error minimization on the localized reflective regions while preserving depth accuracy in non-reflective areas.","We also incorporate a reflection-aware knowledge distillation method that enables a student model to selectively learn the pixel-level knowledge from reflective and non-reflective regions.","This results in robust depth estimation across areas.","Evaluation results on multiple datasets demonstrate that our method effectively enhances depth quality on reflective surfaces and outperforms state-of-the-art SSMDE baselines."],"url":"http://arxiv.org/abs/2502.14573v1"}
{"created":"2025-02-20 13:56:21","title":"Factor Graph-based Interpretable Neural Networks","abstract":"Comprehensible neural network explanations are foundations for a better understanding of decisions, especially when the input data are infused with malicious perturbations. Existing solutions generally mitigate the impact of perturbations through adversarial training, yet they fail to generate comprehensible explanations under unknown perturbations. To address this challenge, we propose AGAIN, a fActor GrAph-based Interpretable neural Network, which is capable of generating comprehensible explanations under unknown perturbations. Instead of retraining like previous solutions, the proposed AGAIN directly integrates logical rules by which logical errors in explanations are identified and rectified during inference. Specifically, we construct the factor graph to express logical rules between explanations and categories. By treating logical rules as exogenous knowledge, AGAIN can identify incomprehensible explanations that violate real-world logic. Furthermore, we propose an interactive intervention switch strategy rectifying explanations based on the logical guidance from the factor graph without learning perturbations, which overcomes the inherent limitation of adversarial training-based methods in defending only against known perturbations. Additionally, we theoretically demonstrate the effectiveness of employing factor graph by proving that the comprehensibility of explanations is strongly correlated with factor graph. Extensive experiments are conducted on three datasets and experimental results illustrate the superior performance of AGAIN compared to state-of-the-art baselines.","sentences":["Comprehensible neural network explanations are foundations for a better understanding of decisions, especially when the input data are infused with malicious perturbations.","Existing solutions generally mitigate the impact of perturbations through adversarial training, yet they fail to generate comprehensible explanations under unknown perturbations.","To address this challenge, we propose AGAIN, a fActor GrAph-based Interpretable neural Network, which is capable of generating comprehensible explanations under unknown perturbations.","Instead of retraining like previous solutions, the proposed AGAIN directly integrates logical rules by which logical errors in explanations are identified and rectified during inference.","Specifically, we construct the factor graph to express logical rules between explanations and categories.","By treating logical rules as exogenous knowledge, AGAIN can identify incomprehensible explanations that violate real-world logic.","Furthermore, we propose an interactive intervention switch strategy rectifying explanations based on the logical guidance from the factor graph without learning perturbations, which overcomes the inherent limitation of adversarial training-based methods in defending only against known perturbations.","Additionally, we theoretically demonstrate the effectiveness of employing factor graph by proving that the comprehensibility of explanations is strongly correlated with factor graph.","Extensive experiments are conducted on three datasets and experimental results illustrate the superior performance of AGAIN compared to state-of-the-art baselines."],"url":"http://arxiv.org/abs/2502.14572v1"}
{"created":"2025-02-20 13:55:53","title":"Predicting Filter Medium Performances in Chamber Filter Presses with Digital Twins Using Neural Network Technologies","abstract":"Efficient solid-liquid separation is crucial in industries like mining, but traditional chamber filter presses depend heavily on manual monitoring, leading to inefficiencies, downtime, and resource wastage. This paper introduces a machine learning-powered digital twin framework to improve operational flexibility and predictive control. A key challenge addressed is the degradation of the filter medium due to repeated cycles and clogging, which reduces filtration efficiency. To solve this, a neural network-based predictive model was developed to forecast operational parameters, such as pressure and flow rates, under various conditions. This predictive capability allows for optimized filtration cycles, reduced downtime, and improved process efficiency. Additionally, the model predicts the filter mediums lifespan, aiding in maintenance planning and resource sustainability. The digital twin framework enables seamless data exchange between filter press sensors and the predictive model, ensuring continuous updates to the training data and enhancing accuracy over time. Two neural network architectures, feedforward and recurrent, were evaluated. The recurrent neural network outperformed the feedforward model, demonstrating superior generalization. It achieved a relative $L^2$-norm error of $5\\%$ for pressure and $9.3\\%$ for flow rate prediction on partially known data. For completely unknown data, the relative errors were $18.4\\%$ and $15.4\\%$, respectively. Qualitative analysis showed strong alignment between predicted and measured data, with deviations within a confidence band of $8.2\\%$ for pressure and $4.8\\%$ for flow rate predictions. This work contributes an accurate predictive model, a new approach to predicting filter medium cycle impacts, and a real-time interface for model updates, ensuring adaptability to changing operational conditions.","sentences":["Efficient solid-liquid separation is crucial in industries like mining, but traditional chamber filter presses depend heavily on manual monitoring, leading to inefficiencies, downtime, and resource wastage.","This paper introduces a machine learning-powered digital twin framework to improve operational flexibility and predictive control.","A key challenge addressed is the degradation of the filter medium due to repeated cycles and clogging, which reduces filtration efficiency.","To solve this, a neural network-based predictive model was developed to forecast operational parameters, such as pressure and flow rates, under various conditions.","This predictive capability allows for optimized filtration cycles, reduced downtime, and improved process efficiency.","Additionally, the model predicts the filter mediums lifespan, aiding in maintenance planning and resource sustainability.","The digital twin framework enables seamless data exchange between filter press sensors and the predictive model, ensuring continuous updates to the training data and enhancing accuracy over time.","Two neural network architectures, feedforward and recurrent, were evaluated.","The recurrent neural network outperformed the feedforward model, demonstrating superior generalization.","It achieved a relative $L^2$-norm error of $5\\%$ for pressure and $9.3\\%$ for flow rate prediction on partially known data.","For completely unknown data, the relative errors were $18.4\\%$ and $15.4\\%$, respectively.","Qualitative analysis showed strong alignment between predicted and measured data, with deviations within a confidence band of $8.2\\%$ for pressure and $4.8\\%$ for flow rate predictions.","This work contributes an accurate predictive model, a new approach to predicting filter medium cycle impacts, and a real-time interface for model updates, ensuring adaptability to changing operational conditions."],"url":"http://arxiv.org/abs/2502.14571v1"}
{"created":"2025-02-20 13:47:51","title":"Plan-over-Graph: Towards Parallelable LLM Agent Schedule","abstract":"Large Language Models (LLMs) have demonstrated exceptional abilities in reasoning for task planning. However, challenges remain under-explored for parallel schedules. This paper introduces a novel paradigm, plan-over-graph, in which the model first decomposes a real-life textual task into executable subtasks and constructs an abstract task graph. The model then understands this task graph as input and generates a plan for parallel execution. To enhance the planning capability of complex, scalable graphs, we design an automated and controllable pipeline to generate synthetic graphs and propose a two-stage training scheme. Experimental results show that our plan-over-graph method significantly improves task performance on both API-based LLMs and trainable open-sourced LLMs. By normalizing complex tasks as graphs, our method naturally supports parallel execution, demonstrating global efficiency. The code and data are available at https://github.com/zsq259/Plan-over-Graph.","sentences":["Large Language Models (LLMs) have demonstrated exceptional abilities in reasoning for task planning.","However, challenges remain under-explored for parallel schedules.","This paper introduces a novel paradigm, plan-over-graph, in which the model first decomposes a real-life textual task into executable subtasks and constructs an abstract task graph.","The model then understands this task graph as input and generates a plan for parallel execution.","To enhance the planning capability of complex, scalable graphs, we design an automated and controllable pipeline to generate synthetic graphs and propose a two-stage training scheme.","Experimental results show that our plan-over-graph method significantly improves task performance on both API-based LLMs and trainable open-sourced LLMs.","By normalizing complex tasks as graphs, our method naturally supports parallel execution, demonstrating global efficiency.","The code and data are available at https://github.com/zsq259/Plan-over-Graph."],"url":"http://arxiv.org/abs/2502.14563v1"}
{"created":"2025-02-20 13:45:42","title":"Can LLMs Predict Citation Intent? An Experimental Analysis of In-context Learning and Fine-tuning on Open LLMs","abstract":"This work investigates the ability of open Large Language Models (LLMs) to predict citation intent through in-context learning and fine-tuning. Unlike traditional approaches that rely on pre-trained models like SciBERT, which require extensive domain-specific pretraining and specialized architectures, we demonstrate that general-purpose LLMs can be adapted to this task with minimal task-specific data. We evaluate twelve model variations across five prominent open LLM families using zero, one, few, and many-shot prompting to assess performance across scenarios. Our experimental study identifies the top-performing model through extensive experimentation of in-context learning-related parameters, which we fine-tune to further enhance task performance. The results highlight the strengths and limitations of LLMs in recognizing citation intents, providing valuable insights for model selection and prompt engineering. Additionally, we make our end-to-end evaluation framework and models openly available for future use.","sentences":["This work investigates the ability of open Large Language Models (LLMs) to predict citation intent through in-context learning and fine-tuning.","Unlike traditional approaches that rely on pre-trained models like SciBERT, which require extensive domain-specific pretraining and specialized architectures, we demonstrate that general-purpose LLMs can be adapted to this task with minimal task-specific data.","We evaluate twelve model variations across five prominent open LLM families using zero, one, few, and many-shot prompting to assess performance across scenarios.","Our experimental study identifies the top-performing model through extensive experimentation of in-context learning-related parameters, which we fine-tune to further enhance task performance.","The results highlight the strengths and limitations of LLMs in recognizing citation intents, providing valuable insights for model selection and prompt engineering.","Additionally, we make our end-to-end evaluation framework and models openly available for future use."],"url":"http://arxiv.org/abs/2502.14561v1"}
{"created":"2025-02-20 13:45:17","title":"Less is More: Improving LLM Alignment via Preference Data Selection","abstract":"Direct Preference Optimization (DPO) has emerged as a promising approach for aligning large language models with human preferences. While prior work mainly extends DPO from the aspect of the objective function, we instead improve DPO from the largely overlooked but critical aspect of data selection. Specifically, we address the issue of parameter shrinkage caused by noisy data by proposing a novel margin-maximization principle for dataset curation in DPO training. To accurately estimate margins for data selection, we propose a dual-margin guided approach that considers both external reward margins and implicit DPO reward margins. Extensive experiments demonstrate that our method reduces computational cost dramatically while improving performance. Remarkably, by using just 10\\% of the Ultrafeedback dataset, our approach achieves 3\\% to 8\\% improvements across various Llama and Mistral series models on the AlpacaEval 2.0 benchmark. Furthermore, our approach seamlessly extends to iterative DPO, yielding a roughly 3\\% improvement with 25\\% online data, while further reducing training time. These results highlight the potential of data selection strategies for advancing preference optimization.","sentences":["Direct Preference Optimization (DPO) has emerged as a promising approach for aligning large language models with human preferences.","While prior work mainly extends DPO from the aspect of the objective function, we instead improve DPO from the largely overlooked but critical aspect of data selection.","Specifically, we address the issue of parameter shrinkage caused by noisy data by proposing a novel margin-maximization principle for dataset curation in DPO training.","To accurately estimate margins for data selection, we propose a dual-margin guided approach that considers both external reward margins and implicit DPO reward margins.","Extensive experiments demonstrate that our method reduces computational cost dramatically while improving performance.","Remarkably, by using just 10\\% of the Ultrafeedback dataset, our approach achieves 3\\% to 8\\% improvements across various Llama and Mistral series models on the AlpacaEval 2.0 benchmark.","Furthermore, our approach seamlessly extends to iterative DPO, yielding a roughly 3\\% improvement with 25\\% online data, while further reducing training time.","These results highlight the potential of data selection strategies for advancing preference optimization."],"url":"http://arxiv.org/abs/2502.14560v1"}
{"created":"2025-02-20 13:38:36","title":"FUIA: Model Inversion Attack against Federated Unlearning","abstract":"With the introduction of regulations related to the ``right to be forgotten\", federated learning (FL) is facing new privacy compliance challenges. To address these challenges, researchers have proposed federated unlearning (FU). However, existing FU research has primarily focused on improving the efficiency of unlearning, with less attention paid to the potential privacy vulnerabilities inherent in these methods. To address this gap, we draw inspiration from gradient inversion attacks in FL and propose the federated unlearning inversion attack (FUIA). The FUIA is specifically designed for the three types of FU (sample unlearning, client unlearning, and class unlearning), aiming to provide a comprehensive analysis of the privacy leakage risks associated with FU. In FUIA, the server acts as an honest-but-curious attacker, recording and exploiting the model differences before and after unlearning to expose the features and labels of forgotten data. FUIA significantly leaks the privacy of forgotten data and can target all types of FU. This attack contradicts the goal of FU to eliminate specific data influence, instead exploiting its vulnerabilities to recover forgotten data and expose its privacy flaws. Extensive experimental results show that FUIA can effectively reveal the private information of forgotten data. To mitigate this privacy leakage, we also explore two potential defense methods, although these come at the cost of reduced unlearning effectiveness and the usability of the unlearned model.","sentences":["With the introduction of regulations related to the ``right to be forgotten\", federated learning (FL) is facing new privacy compliance challenges.","To address these challenges, researchers have proposed federated unlearning (FU).","However, existing FU research has primarily focused on improving the efficiency of unlearning, with less attention paid to the potential privacy vulnerabilities inherent in these methods.","To address this gap, we draw inspiration from gradient inversion attacks in FL and propose the federated unlearning inversion attack (FUIA).","The FUIA is specifically designed for the three types of FU (sample unlearning, client unlearning, and class unlearning), aiming to provide a comprehensive analysis of the privacy leakage risks associated with FU.","In FUIA, the server acts as an honest-but-curious attacker, recording and exploiting the model differences before and after unlearning to expose the features and labels of forgotten data.","FUIA significantly leaks the privacy of forgotten data and can target all types of FU.","This attack contradicts the goal of FU to eliminate specific data influence, instead exploiting its vulnerabilities to recover forgotten data and expose its privacy flaws.","Extensive experimental results show that FUIA can effectively reveal the private information of forgotten data.","To mitigate this privacy leakage, we also explore two potential defense methods, although these come at the cost of reduced unlearning effectiveness and the usability of the unlearned model."],"url":"http://arxiv.org/abs/2502.14558v1"}
{"created":"2025-02-20 13:31:50","title":"Multiscale Byte Language Models -- A Hierarchical Architecture for Causal Million-Length Sequence Modeling","abstract":"Bytes form the basis of the digital world and thus are a promising building block for multimodal foundation models. Recently, Byte Language Models (BLMs) have emerged to overcome tokenization, yet the excessive length of bytestreams requires new architectural paradigms. Therefore, we present the Multiscale Byte Language Model (MBLM), a model-agnostic hierarchical decoder stack that allows training with context windows of $5$M bytes on single GPU in full model precision. We thoroughly examine MBLM's performance with Transformer and Mamba blocks on both unimodal and multimodal tasks. Our experiments demonstrate that hybrid architectures are efficient in handling extremely long byte sequences during training while achieving near-linear generational efficiency. To the best of our knowledge, we present the first evaluation of BLMs on visual Q\\&A tasks and find that, despite serializing images and the absence of an encoder, a MBLM with pure next token prediction can match custom CNN-LSTM architectures with designated classification heads. We show that MBLMs exhibit strong adaptability in integrating diverse data representations, including pixel and image filestream bytes, underlining their potential toward omnimodal foundation models. Source code is publicly available at: https://github.com/ai4sd/multiscale-byte-lm","sentences":["Bytes form the basis of the digital world and thus are a promising building block for multimodal foundation models.","Recently, Byte Language Models (BLMs) have emerged to overcome tokenization, yet the excessive length of bytestreams requires new architectural paradigms.","Therefore, we present the Multiscale Byte Language Model (MBLM), a model-agnostic hierarchical decoder stack that allows training with context windows of $5$M bytes on single GPU in full model precision.","We thoroughly examine MBLM's performance with Transformer and Mamba blocks on both unimodal and multimodal tasks.","Our experiments demonstrate that hybrid architectures are efficient in handling extremely long byte sequences during training while achieving near-linear generational efficiency.","To the best of our knowledge, we present the first evaluation of BLMs on visual Q\\&A tasks and find that, despite serializing images and the absence of an encoder, a MBLM with pure next token prediction can match custom CNN-LSTM architectures with designated classification heads.","We show that MBLMs exhibit strong adaptability in integrating diverse data representations, including pixel and image filestream bytes, underlining their potential toward omnimodal foundation models.","Source code is publicly available at: https://github.com/ai4sd/multiscale-byte-lm"],"url":"http://arxiv.org/abs/2502.14553v1"}
{"created":"2025-02-20 13:21:47","title":"Position: Graph Learning Will Lose Relevance Due To Poor Benchmarks","abstract":"While machine learning on graphs has demonstrated promise in drug design and molecular property prediction, significant benchmarking challenges hinder its further progress and relevance. Current benchmarking practices often lack focus on transformative, real-world applications, favoring narrow domains like two-dimensional molecular graphs over broader, impactful areas such as combinatorial optimization, relational databases, or chip design. Additionally, many benchmark datasets poorly represent the underlying data, leading to inadequate abstractions and misaligned use cases. Fragmented evaluations and an excessive focus on accuracy further exacerbate these issues, incentivizing overfitting rather than fostering generalizable insights. These limitations have prevented the development of truly useful graph foundation models. This position paper calls for a paradigm shift toward more meaningful benchmarks, rigorous evaluation protocols, and stronger collaboration with domain experts to drive impactful and reliable advances in graph learning research, unlocking the potential of graph learning.","sentences":["While machine learning on graphs has demonstrated promise in drug design and molecular property prediction, significant benchmarking challenges hinder its further progress and relevance.","Current benchmarking practices often lack focus on transformative, real-world applications, favoring narrow domains like two-dimensional molecular graphs over broader, impactful areas such as combinatorial optimization, relational databases, or chip design.","Additionally, many benchmark datasets poorly represent the underlying data, leading to inadequate abstractions and misaligned use cases.","Fragmented evaluations and an excessive focus on accuracy further exacerbate these issues, incentivizing overfitting rather than fostering generalizable insights.","These limitations have prevented the development of truly useful graph foundation models.","This position paper calls for a paradigm shift toward more meaningful benchmarks, rigorous evaluation protocols, and stronger collaboration with domain experts to drive impactful and reliable advances in graph learning research, unlocking the potential of graph learning."],"url":"http://arxiv.org/abs/2502.14546v1"}
{"created":"2025-02-20 13:21:18","title":"An Entropic Metric for Measuring Calibration of Machine Learning Models","abstract":"Understanding the confidence with which a machine learning model classifies an input datum is an important, and perhaps under-investigated, concept. In this paper, we propose a new calibration metric, the Entropic Calibration Difference (ECD). Based on existing research in the field of state estimation, specifically target tracking (TT), we show how ECD may be applied to binary classification machine learning models. We describe the relative importance of under- and over-confidence and how they are not conflated in the TT literature. Indeed, our metric distinguishes under- from over-confidence. We consider this important given that algorithms that are under-confident are likely to be 'safer' than algorithms that are over-confident, albeit at the expense of also being over-cautious and so statistically inefficient. We demonstrate how this new metric performs on real and simulated data and compare with other metrics for machine learning model probability calibration, including the Expected Calibration Error (ECE) and its signed counterpart, the Expected Signed Calibration Error (ESCE).","sentences":["Understanding the confidence with which a machine learning model classifies an input datum is an important, and perhaps under-investigated, concept.","In this paper, we propose a new calibration metric, the Entropic Calibration Difference (ECD).","Based on existing research in the field of state estimation, specifically target tracking (TT), we show how ECD may be applied to binary classification machine learning models.","We describe the relative importance of under- and over-confidence and how they are not conflated in the TT literature.","Indeed, our metric distinguishes under- from over-confidence.","We consider this important given that algorithms that are under-confident are likely to be 'safer' than algorithms that are over-confident, albeit at the expense of also being over-cautious and so statistically inefficient.","We demonstrate how this new metric performs on real and simulated data and compare with other metrics for machine learning model probability calibration, including the Expected Calibration Error (ECE) and its signed counterpart, the Expected Signed Calibration Error (ESCE)."],"url":"http://arxiv.org/abs/2502.14545v1"}
{"created":"2025-02-20 13:20:19","title":"LLM-based User Profile Management for Recommender System","abstract":"The rapid advancement of Large Language Models (LLMs) has opened new opportunities in recommender systems by enabling zero-shot recommendation without conventional training. Despite their potential, most existing works rely solely on users' purchase histories, leaving significant room for improvement by incorporating user-generated textual data, such as reviews and product descriptions. Addressing this gap, we propose PURE, a novel LLM-based recommendation framework that builds and maintains evolving user profiles by systematically extracting and summarizing key information from user reviews. PURE consists of three core components: a Review Extractor for identifying user preferences and key product features, a Profile Updater for refining and updating user profiles, and a Recommender for generating personalized recommendations using the most current profile. To evaluate PURE, we introduce a continuous sequential recommendation task that reflects real-world scenarios by adding reviews over time and updating predictions incrementally. Our experimental results on Amazon datasets demonstrate that PURE outperforms existing LLM-based methods, effectively leveraging long-term user information while managing token limitations.","sentences":["The rapid advancement of Large Language Models (LLMs) has opened new opportunities in recommender systems by enabling zero-shot recommendation without conventional training.","Despite their potential, most existing works rely solely on users' purchase histories, leaving significant room for improvement by incorporating user-generated textual data, such as reviews and product descriptions.","Addressing this gap, we propose PURE, a novel LLM-based recommendation framework that builds and maintains evolving user profiles by systematically extracting and summarizing key information from user reviews.","PURE consists of three core components: a Review Extractor for identifying user preferences and key product features, a Profile Updater for refining and updating user profiles, and a Recommender for generating personalized recommendations using the most current profile.","To evaluate PURE, we introduce a continuous sequential recommendation task that reflects real-world scenarios by adding reviews over time and updating predictions incrementally.","Our experimental results on Amazon datasets demonstrate that PURE outperforms existing LLM-based methods, effectively leveraging long-term user information while managing token limitations."],"url":"http://arxiv.org/abs/2502.14541v1"}
{"created":"2025-02-20 13:06:18","title":"OptiRefine: Densest subgraphs and maximum cuts with $k$ refinements","abstract":"Data-analysis tasks often involve an iterative process, which requires refining previous solutions. For instance, when analyzing dynamic social networks, we may be interested in monitoring the evolution of a community that was identified at an earlier snapshot. This task requires finding a community in the current snapshot of data that is ``close'' to the earlier-discovered community of interest. However, classic optimization algorithms, which typically find solutions from scratch, potentially return communities that are very dissimilar to the initial one. To mitigate these issues, we introduce the \\emph{OptiRefine framework}. The framework optimizes initial solutions by making a small number of \\emph{refinements}, thereby ensuring that the new solution remains close to the initial solution and simultaneously achieving a near-optimal solution for the optimization problem. We apply the OptiRefine framework to two classic graph-optimization problems: \\emph{densest subgraph} and \\emph{maximum cut}. For the \\emph{densest-subgraph problem}, we optimize a given subgraph's density by adding or removing $k$~nodes. We show that this novel problem is a generalization of $k$-densest subgraph, and provide constant-factor approximation algorithms for $k=\\Omega(n)$~refinements. We also study a version of \\emph{maximum cut} in which the goal is to improve a given cut. We provide connections to maximum cut with cardinality constraints and provide an optimal approximation algorithm in most parameter regimes under the Unique Games Conjecture for $k=\\Omega(n)$~refinements. We evaluate our theoretical methods and scalable heuristics on synthetic and real-world data and show that they are highly effective in practice.","sentences":["Data-analysis tasks often involve an iterative process, which requires refining previous solutions.","For instance, when analyzing dynamic social networks, we may be interested in monitoring the evolution of a community that was identified at an earlier snapshot.","This task requires finding a community in the current snapshot of data that is ``close'' to the earlier-discovered community of interest.","However, classic optimization algorithms, which typically find solutions from scratch, potentially return communities that are very dissimilar to the initial one.","To mitigate these issues, we introduce the \\emph{OptiRefine framework}.","The framework optimizes initial solutions by making a small number of \\emph{refinements}, thereby ensuring that the new solution remains close to the initial solution and simultaneously achieving a near-optimal solution for the optimization problem.","We apply the OptiRefine framework to two classic graph-optimization problems:","\\emph{densest subgraph} and \\emph{maximum cut}.","For the \\emph{densest-subgraph problem}, we optimize a given subgraph's density by adding or removing $k$~nodes.","We show that this novel problem is a generalization of $k$-densest subgraph, and provide constant-factor approximation algorithms for $k=\\Omega(n)$~refinements.","We also study a version of \\emph{maximum cut} in which the goal is to improve a given cut.","We provide connections to maximum cut with cardinality constraints and provide an optimal approximation algorithm in most parameter regimes under the Unique Games Conjecture for $k=\\Omega(n)$~refinements.","We evaluate our theoretical methods and scalable heuristics on synthetic and real-world data and show that they are highly effective in practice."],"url":"http://arxiv.org/abs/2502.14532v1"}
{"created":"2025-02-20 13:01:07","title":"Inter-turbine Modelling of Wind-Farm Power using Multi-task Learning","abstract":"Because of the global need to increase power production from renewable energy resources, developments in the online monitoring of the associated infrastructure is of interest to reduce operation and maintenance costs. However, challenges exist for data-driven approaches to this problem, such as incomplete or limited histories of labelled damage-state data, operational and environmental variability, or the desire for the quantification of uncertainty to support risk management.   This work first introduces a probabilistic regression model for predicting wind-turbine power, which adjusts for wake effects learnt from data. Spatial correlations in the learned model parameters for different tasks (turbines) are then leveraged in a hierarchical Bayesian model (an approach to multi-task learning) to develop a \"metamodel\", which can be used to make power-predictions which adjust for turbine location - including on previously unobserved turbines not included in the training data. The results show that the metamodel is able to outperform a series of benchmark models, and demonstrates a novel strategy for making efficient use of data for inference in populations of structures, in particular where correlations exist in the variable(s) of interest (such as those from wind-turbine wake-effects).","sentences":["Because of the global need to increase power production from renewable energy resources, developments in the online monitoring of the associated infrastructure is of interest to reduce operation and maintenance costs.","However, challenges exist for data-driven approaches to this problem, such as incomplete or limited histories of labelled damage-state data, operational and environmental variability, or the desire for the quantification of uncertainty to support risk management.   ","This work first introduces a probabilistic regression model for predicting wind-turbine power, which adjusts for wake effects learnt from data.","Spatial correlations in the learned model parameters for different tasks (turbines) are then leveraged in a hierarchical Bayesian model (an approach to multi-task learning) to develop a \"metamodel\", which can be used to make power-predictions which adjust for turbine location - including on previously unobserved turbines not included in the training data.","The results show that the metamodel is able to outperform a series of benchmark models, and demonstrates a novel strategy for making efficient use of data for inference in populations of structures, in particular where correlations exist in the variable(s) of interest (such as those from wind-turbine wake-effects)."],"url":"http://arxiv.org/abs/2502.14527v1"}
{"created":"2025-02-20 13:00:31","title":"Small Graph Is All You Need: DeepStateGNN for Scalable Traffic Forecasting","abstract":"We propose a novel Graph Neural Network (GNN) model, named DeepStateGNN, for analyzing traffic data, demonstrating its efficacy in two critical tasks: forecasting and reconstruction. Unlike typical GNN methods that treat each traffic sensor as an individual graph node, DeepStateGNN clusters sensors into higher-level graph nodes, dubbed Deep State Nodes, based on various similarity criteria, resulting in a fixed number of nodes in a Deep State graph. The term \"Deep State\" nodes is a play on words, referencing hidden networks of power that, like these nodes, secretly govern traffic independently of visible sensors. These Deep State Nodes are defined by several similarity factors, including spatial proximity (e.g., sensors located nearby in the road network), functional similarity (e.g., sensors on similar types of freeways), and behavioral similarity under specific conditions (e.g., traffic behavior during rain). This clustering approach allows for dynamic and adaptive node grouping, as sensors can belong to multiple clusters and clusters may evolve over time. Our experimental results show that DeepStateGNN offers superior scalability and faster training, while also delivering more accurate results than competitors. It effectively handles large-scale sensor networks, outperforming other methods in both traffic forecasting and reconstruction accuracy.","sentences":["We propose a novel Graph Neural Network (GNN) model, named DeepStateGNN, for analyzing traffic data, demonstrating its efficacy in two critical tasks: forecasting and reconstruction.","Unlike typical GNN methods that treat each traffic sensor as an individual graph node, DeepStateGNN clusters sensors into higher-level graph nodes, dubbed Deep State Nodes, based on various similarity criteria, resulting in a fixed number of nodes in a Deep State graph.","The term \"Deep State\" nodes is a play on words, referencing hidden networks of power that, like these nodes, secretly govern traffic independently of visible sensors.","These Deep State Nodes are defined by several similarity factors, including spatial proximity (e.g., sensors located nearby in the road network), functional similarity (e.g., sensors on similar types of freeways), and behavioral similarity under specific conditions (e.g., traffic behavior during rain).","This clustering approach allows for dynamic and adaptive node grouping, as sensors can belong to multiple clusters and clusters may evolve over time.","Our experimental results show that DeepStateGNN offers superior scalability and faster training, while also delivering more accurate results than competitors.","It effectively handles large-scale sensor networks, outperforming other methods in both traffic forecasting and reconstruction accuracy."],"url":"http://arxiv.org/abs/2502.14525v1"}
{"created":"2025-02-20 12:56:16","title":"Generative adversarial networks vs large language models: a comparative study on synthetic tabular data generation","abstract":"We propose a new framework for zero-shot generation of synthetic tabular data. Using the large language model (LLM) GPT-4o and plain-language prompting, we demonstrate the ability to generate high-fidelity tabular data without task-specific fine-tuning or access to real-world data (RWD) for pre-training. To benchmark GPT-4o, we compared the fidelity and privacy of LLM-generated synthetic data against data generated with the conditional tabular generative adversarial network (CTGAN), across three open-access datasets: Iris, Fish Measurements, and Real Estate Valuation. Despite the zero-shot approach, GPT-4o outperformed CTGAN in preserving means, 95% confidence intervals, bivariate correlations, and data privacy of RWD, even at amplified sample sizes. Notably, correlations between parameters were consistently preserved with appropriate direction and strength. However, refinement is necessary to better retain distributional characteristics. These findings highlight the potential of LLMs in tabular data synthesis, offering an accessible alternative to generative adversarial networks and variational autoencoders.","sentences":["We propose a new framework for zero-shot generation of synthetic tabular data.","Using the large language model (LLM) GPT-4o and plain-language prompting, we demonstrate the ability to generate high-fidelity tabular data without task-specific fine-tuning or access to real-world data (RWD) for pre-training.","To benchmark GPT-4o, we compared the fidelity and privacy of LLM-generated synthetic data against data generated with the conditional tabular generative adversarial network (CTGAN), across three open-access datasets: Iris, Fish Measurements, and Real Estate Valuation.","Despite the zero-shot approach, GPT-4o outperformed CTGAN in preserving means, 95% confidence intervals, bivariate correlations, and data privacy of RWD, even at amplified sample sizes.","Notably, correlations between parameters were consistently preserved with appropriate direction and strength.","However, refinement is necessary to better retain distributional characteristics.","These findings highlight the potential of LLMs in tabular data synthesis, offering an accessible alternative to generative adversarial networks and variational autoencoders."],"url":"http://arxiv.org/abs/2502.14523v1"}
{"created":"2025-02-20 12:54:56","title":"Investigating the Generalizability of ECG Noise Detection Across Diverse Data Sources and Noise Types","abstract":"Electrocardiograms (ECGs) are essential for monitoring cardiac health, allowing clinicians to analyze heart rate variability (HRV), detect abnormal rhythms, and diagnose cardiovascular diseases. However, ECG signals, especially those from wearable devices, are often affected by noise artifacts caused by motion, muscle activity, or device-related interference. These artifacts distort R-peaks and the characteristic QRS complex, making HRV analysis unreliable and increasing the risk of misdiagnosis.   Despite this, the few existing studies on ECG noise detection have primarily focused on a single dataset, limiting the understanding of how well noise detection models generalize across different datasets. In this paper, we investigate the generalizability of noise detection in ECG using a novel HRV-based approach through cross-dataset experiments on four datasets. Our results show that machine learning achieves an average accuracy of over 90\\% and an AUPRC of more than 0.9. These findings suggest that regardless of the ECG data source or the type of noise, the proposed method maintains high accuracy even on unseen datasets, demonstrating the feasibility of generalizability.","sentences":["Electrocardiograms (ECGs) are essential for monitoring cardiac health, allowing clinicians to analyze heart rate variability (HRV), detect abnormal rhythms, and diagnose cardiovascular diseases.","However, ECG signals, especially those from wearable devices, are often affected by noise artifacts caused by motion, muscle activity, or device-related interference.","These artifacts distort R-peaks and the characteristic QRS complex, making HRV analysis unreliable and increasing the risk of misdiagnosis.   ","Despite this, the few existing studies on ECG noise detection have primarily focused on a single dataset, limiting the understanding of how well noise detection models generalize across different datasets.","In this paper, we investigate the generalizability of noise detection in ECG using a novel HRV-based approach through cross-dataset experiments on four datasets.","Our results show that machine learning achieves an average accuracy of over 90\\% and an AUPRC of more than 0.9.","These findings suggest that regardless of the ECG data source or the type of noise, the proposed method maintains high accuracy even on unseen datasets, demonstrating the feasibility of generalizability."],"url":"http://arxiv.org/abs/2502.14522v1"}
{"created":"2025-02-20 12:35:25","title":"MultiSlav: Using Cross-Lingual Knowledge Transfer to Combat the Curse of Multilinguality","abstract":"Does multilingual Neural Machine Translation (NMT) lead to The Curse of the Multlinguality or provides the Cross-lingual Knowledge Transfer within a language family? In this study, we explore multiple approaches for extending the available data-regime in NMT and we prove cross-lingual benefits even in 0-shot translation regime for low-resource languages. With this paper, we provide state-of-the-art open-source NMT models for translating between selected Slavic languages. We released our models on the HuggingFace Hub (https://hf.co/collections/allegro/multislav-6793d6b6419e5963e759a683) under the CC BY 4.0 license. Slavic language family comprises morphologically rich Central and Eastern European languages. Although counting hundreds of millions of native speakers, Slavic Neural Machine Translation is under-studied in our opinion. Recently, most NMT research focuses either on: high-resource languages like English, Spanish, and German - in WMT23 General Translation Task 7 out of 8 task directions are from or to English; massively multilingual models covering multiple language groups; or evaluation techniques.","sentences":["Does multilingual Neural Machine Translation (NMT) lead to The Curse of the Multlinguality or provides the Cross-lingual Knowledge Transfer within a language family?","In this study, we explore multiple approaches for extending the available data-regime in NMT and we prove cross-lingual benefits even in 0-shot translation regime for low-resource languages.","With this paper, we provide state-of-the-art open-source NMT models for translating between selected Slavic languages.","We released our models on the HuggingFace Hub (https://hf.co/collections/allegro/multislav-6793d6b6419e5963e759a683) under the CC BY 4.0 license.","Slavic language family comprises morphologically rich Central and Eastern European languages.","Although counting hundreds of millions of native speakers, Slavic Neural Machine Translation is under-studied in our opinion.","Recently, most NMT research focuses either on: high-resource languages like English, Spanish, and German - in WMT23 General Translation Task 7 out of 8 task directions are from or to English; massively multilingual models covering multiple language groups; or evaluation techniques."],"url":"http://arxiv.org/abs/2502.14509v1"}
{"created":"2025-02-20 12:34:46","title":"Can LLMs Simulate L2-English Dialogue? An Information-Theoretic Analysis of L1-Dependent Biases","abstract":"This study evaluates Large Language Models' (LLMs) ability to simulate non-native-like English use observed in human second language (L2) learners interfered with by their native first language (L1). In dialogue-based interviews, we prompt LLMs to mimic L2 English learners with specific L1s (e.g., Japanese, Thai, Urdu) across seven languages, comparing their outputs to real L2 learner data. Our analysis examines L1-driven linguistic biases, such as reference word usage and avoidance behaviors, using information-theoretic and distributional density measures. Results show that modern LLMs (e.g., Qwen2.5, LLAMA3.3, DeepseekV3, GPT-4o) replicate L1-dependent patterns observed in human L2 data, with distinct influences from various languages (e.g., Japanese, Korean, and Mandarin significantly affect tense agreement, and Urdu influences noun-verb collocations). Our results reveal the potential of LLMs for L2 dialogue generation and evaluation for future educational applications.","sentences":["This study evaluates Large Language Models' (LLMs) ability to simulate non-native-like English use observed in human second language (L2) learners interfered with by their native first language (L1).","In dialogue-based interviews, we prompt LLMs to mimic L2 English learners with specific L1s (e.g., Japanese, Thai, Urdu) across seven languages, comparing their outputs to real L2 learner data.","Our analysis examines L1-driven linguistic biases, such as reference word usage and avoidance behaviors, using information-theoretic and distributional density measures.","Results show that modern LLMs (e.g., Qwen2.5, LLAMA3.3, DeepseekV3, GPT-4o) replicate L1-dependent patterns observed in human L2 data, with distinct influences from various languages (e.g., Japanese, Korean, and Mandarin significantly affect tense agreement, and Urdu influences noun-verb collocations).","Our results reveal the potential of LLMs for L2 dialogue generation and evaluation for future educational applications."],"url":"http://arxiv.org/abs/2502.14507v1"}
{"created":"2025-02-20 12:31:03","title":"How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?","abstract":"The performance of Large Language Models (LLMs) on many tasks is greatly limited by the knowledge learned during pre-training and stored in the model's parameters. Low-rank adaptation (LoRA) is a popular and efficient training technique for updating or domain-specific adaptation of LLMs. In this study, we investigate how new facts can be incorporated into the LLM using LoRA without compromising the previously learned knowledge. We fine-tuned Llama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our experiments have shown that the best results are obtained when the training data contains a mixture of known and new facts. However, this approach is still potentially harmful because the model's performance on external question-answering benchmarks declines after such fine-tuning. When the training data is biased towards certain entities, the model tends to regress to few overrepresented answers. In addition, we found that the model becomes more confident and refuses to provide an answer in only few cases. These findings highlight the potential pitfalls of LoRA-based LLM updates and underscore the importance of training data composition and tuning parameters to balance new knowledge integration and general model capabilities.","sentences":["The performance of Large Language Models (LLMs) on many tasks is greatly limited by the knowledge learned during pre-training and stored in the model's parameters.","Low-rank adaptation (LoRA) is a popular and efficient training technique for updating or domain-specific adaptation of LLMs.","In this study, we investigate how new facts can be incorporated into the LLM using LoRA without compromising the previously learned knowledge.","We fine-tuned Llama-3.1-8B-instruct using LoRA with varying amounts of new knowledge.","Our experiments have shown that the best results are obtained when the training data contains a mixture of known and new facts.","However, this approach is still potentially harmful because the model's performance on external question-answering benchmarks declines after such fine-tuning.","When the training data is biased towards certain entities, the model tends to regress to few overrepresented answers.","In addition, we found that the model becomes more confident and refuses to provide an answer in only few cases.","These findings highlight the potential pitfalls of LoRA-based LLM updates and underscore the importance of training data composition and tuning parameters to balance new knowledge integration and general model capabilities."],"url":"http://arxiv.org/abs/2502.14502v1"}
{"created":"2025-02-20 12:28:23","title":"MLGym: A New Framework and Benchmark for Advancing AI Research Agents","abstract":"We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.","sentences":["We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks.","This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents.","MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory.","Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task.","We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro.","Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks.","We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements.","We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents."],"url":"http://arxiv.org/abs/2502.14499v1"}
{"created":"2025-02-20 12:26:56","title":"Stories that (are) Move(d by) Markets: A Causal Exploration of Market Shocks and Semantic Shifts across Different Partisan Groups","abstract":"Macroeconomic fluctuations and the narratives that shape them form a mutually reinforcing cycle: public discourse can spur behavioural changes leading to economic shifts, which then result in changes in the stories that propagate. We show that shifts in semantic embedding space can be causally linked to financial market shocks -- deviations from the expected market behaviour. Furthermore, we show how partisanship can influence the predictive power of text for market fluctuations and shape reactions to those same shocks. We also provide some evidence that text-based signals are particularly salient during unexpected events such as COVID-19, highlighting the value of language data as an exogenous variable in economic forecasting. Our findings underscore the bidirectional relationship between news outlets and market shocks, offering a novel empirical approach to studying their effect on each other.","sentences":["Macroeconomic fluctuations and the narratives that shape them form a mutually reinforcing cycle: public discourse can spur behavioural changes leading to economic shifts, which then result in changes in the stories that propagate.","We show that shifts in semantic embedding space can be causally linked to financial market shocks -- deviations from the expected market behaviour.","Furthermore, we show how partisanship can influence the predictive power of text for market fluctuations and shape reactions to those same shocks.","We also provide some evidence that text-based signals are particularly salient during unexpected events such as COVID-19, highlighting the value of language data as an exogenous variable in economic forecasting.","Our findings underscore the bidirectional relationship between news outlets and market shocks, offering a novel empirical approach to studying their effect on each other."],"url":"http://arxiv.org/abs/2502.14497v1"}
{"created":"2025-02-20 12:26:15","title":"Enhancing Language Multi-Agent Learning with Multi-Agent Credit Re-Assignment for Interactive Environment Generalization","abstract":"LLM-based agents have made significant advancements in interactive environments, such as mobile operations and web browsing, and other domains beyond computer using. Current multi-agent systems universally excel in performance, compared to single agents, but struggle with generalization across environments due to predefined roles and inadequate strategies for generalizing language agents. The challenge of achieving both strong performance and good generalization has hindered the progress of multi-agent systems for interactive environments. To address these issues, we propose CollabUIAgents, a multi-agent reinforcement learning framework with a novel multi-agent credit re-assignment (CR) strategy, assigning process rewards with LLMs rather than environment-specific rewards and learning with synthesized preference data, in order to foster generalizable, collaborative behaviors among the role-free agents' policies. Empirical results show that our framework improves both performance and cross-environment generalizability of multi-agent systems. Moreover, our 7B-parameter system achieves results on par with or exceed strong closed-source models, and the LLM that guides the CR. We also provide insights in using granular CR rewards effectively for environment generalization, and accommodating trained LLMs in multi-agent systems.","sentences":["LLM-based agents have made significant advancements in interactive environments, such as mobile operations and web browsing, and other domains beyond computer using.","Current multi-agent systems universally excel in performance, compared to single agents, but struggle with generalization across environments due to predefined roles and inadequate strategies for generalizing language agents.","The challenge of achieving both strong performance and good generalization has hindered the progress of multi-agent systems for interactive environments.","To address these issues, we propose CollabUIAgents, a multi-agent reinforcement learning framework with a novel multi-agent credit re-assignment (CR) strategy, assigning process rewards with LLMs rather than environment-specific rewards and learning with synthesized preference data, in order to foster generalizable, collaborative behaviors among the role-free agents' policies.","Empirical results show that our framework improves both performance and cross-environment generalizability of multi-agent systems.","Moreover, our 7B-parameter system achieves results on par with or exceed strong closed-source models, and the LLM that guides the CR.","We also provide insights in using granular CR rewards effectively for environment generalization, and accommodating trained LLMs in multi-agent systems."],"url":"http://arxiv.org/abs/2502.14496v1"}
{"created":"2025-02-20 12:25:30","title":"Nearshore Underwater Target Detection Meets UAV-borne Hyperspectral Remote Sensing: A Novel Hybrid-level Contrastive Learning Framework and Benchmark Dataset","abstract":"UAV-borne hyperspectral remote sensing has emerged as a promising approach for underwater target detection (UTD). However, its effectiveness is hindered by spectral distortions in nearshore environments, which compromise the accuracy of traditional hyperspectral UTD (HUTD) methods that rely on bathymetric model. These distortions lead to significant uncertainty in target and background spectra, challenging the detection process. To address this, we propose the Hyperspectral Underwater Contrastive Learning Network (HUCLNet), a novel framework that integrates contrastive learning with a self-paced learning paradigm for robust HUTD in nearshore regions. HUCLNet extracts discriminative features from distorted hyperspectral data through contrastive learning, while the self-paced learning strategy selectively prioritizes the most informative samples. Additionally, a reliability-guided clustering strategy enhances the robustness of learned representations.To evaluate the method effectiveness, we conduct a novel nearshore HUTD benchmark dataset, ATR2-HUTD, covering three diverse scenarios with varying water types and turbidity, and target types. Extensive experiments demonstrate that HUCLNet significantly outperforms state-of-the-art methods. The dataset and code will be publicly available at: https://github.com/qjh1996/HUTD","sentences":["UAV-borne hyperspectral remote sensing has emerged as a promising approach for underwater target detection (UTD).","However, its effectiveness is hindered by spectral distortions in nearshore environments, which compromise the accuracy of traditional hyperspectral UTD (HUTD) methods that rely on bathymetric model.","These distortions lead to significant uncertainty in target and background spectra, challenging the detection process.","To address this, we propose the Hyperspectral Underwater Contrastive Learning Network (HUCLNet), a novel framework that integrates contrastive learning with a self-paced learning paradigm for robust HUTD in nearshore regions.","HUCLNet extracts discriminative features from distorted hyperspectral data through contrastive learning, while the self-paced learning strategy selectively prioritizes the most informative samples.","Additionally, a reliability-guided clustering strategy enhances the robustness of learned representations.","To evaluate the method effectiveness, we conduct a novel nearshore HUTD benchmark dataset, ATR2-HUTD, covering three diverse scenarios with varying water types and turbidity, and target types.","Extensive experiments demonstrate that HUCLNet significantly outperforms state-of-the-art methods.","The dataset and code will be publicly available at: https://github.com/qjh1996/HUTD"],"url":"http://arxiv.org/abs/2502.14495v1"}
{"created":"2025-02-20 12:19:30","title":"CrossFuse: Learning Infrared and Visible Image Fusion by Cross-Sensor Top-K Vision Alignment and Beyond","abstract":"Infrared and visible image fusion (IVIF) is increasingly applied in critical fields such as video surveillance and autonomous driving systems. Significant progress has been made in deep learning-based fusion methods. However, these models frequently encounter out-of-distribution (OOD) scenes in real-world applications, which severely impact their performance and reliability. Therefore, addressing the challenge of OOD data is crucial for the safe deployment of these models in open-world environments. Unlike existing research, our focus is on the challenges posed by OOD data in real-world applications and on enhancing the robustness and generalization of models. In this paper, we propose an infrared-visible fusion framework based on Multi-View Augmentation. For external data augmentation, Top-k Selective Vision Alignment is employed to mitigate distribution shifts between datasets by performing RGB-wise transformations on visible images. This strategy effectively introduces augmented samples, enhancing the adaptability of the model to complex real-world scenarios. Additionally, for internal data augmentation, self-supervised learning is established using Weak-Aggressive Augmentation. This enables the model to learn more robust and general feature representations during the fusion process, thereby improving robustness and generalization. Extensive experiments demonstrate that the proposed method exhibits superior performance and robustness across various conditions and environments. Our approach significantly enhances the reliability and stability of IVIF tasks in practical applications.","sentences":["Infrared and visible image fusion (IVIF) is increasingly applied in critical fields such as video surveillance and autonomous driving systems.","Significant progress has been made in deep learning-based fusion methods.","However, these models frequently encounter out-of-distribution (OOD) scenes in real-world applications, which severely impact their performance and reliability.","Therefore, addressing the challenge of OOD data is crucial for the safe deployment of these models in open-world environments.","Unlike existing research, our focus is on the challenges posed by OOD data in real-world applications and on enhancing the robustness and generalization of models.","In this paper, we propose an infrared-visible fusion framework based on Multi-View Augmentation.","For external data augmentation, Top-k Selective Vision Alignment is employed to mitigate distribution shifts between datasets by performing RGB-wise transformations on visible images.","This strategy effectively introduces augmented samples, enhancing the adaptability of the model to complex real-world scenarios.","Additionally, for internal data augmentation, self-supervised learning is established using Weak-Aggressive Augmentation.","This enables the model to learn more robust and general feature representations during the fusion process, thereby improving robustness and generalization.","Extensive experiments demonstrate that the proposed method exhibits superior performance and robustness across various conditions and environments.","Our approach significantly enhances the reliability and stability of IVIF tasks in practical applications."],"url":"http://arxiv.org/abs/2502.14493v1"}
{"created":"2025-02-20 12:14:54","title":"Statistical Scenario Modelling and Lookalike Distributions for Multi-Variate AI Risk","abstract":"Evaluating AI safety requires statistically rigorous methods and risk metrics for understanding how the use of AI affects aggregated risk. However, much AI safety literature focuses upon risks arising from AI models in isolation, lacking consideration of how modular use of AI affects risk distribution of workflow components or overall risk metrics. There is also a lack of statistical grounding enabling sensitisation of risk models in the presence of absence of AI to estimate causal contributions of AI. This is in part due to the dearth of AI impact data upon which to fit distributions. In this work, we address these gaps in two ways. First, we demonstrate how scenario modelling (grounded in established statistical techniques such as Markov chains, copulas and Monte Carlo simulation) can be used to model AI risk holistically. Second, we show how lookalike distributions from phenomena analogous to AI can be used to estimate AI impacts in the absence of directly observable data. We demonstrate the utility of our methods for benchmarking cumulative AI risk via risk analysis of a logistic scenario simulations.","sentences":["Evaluating AI safety requires statistically rigorous methods and risk metrics for understanding how the use of AI affects aggregated risk.","However, much AI safety literature focuses upon risks arising from AI models in isolation, lacking consideration of how modular use of AI affects risk distribution of workflow components or overall risk metrics.","There is also a lack of statistical grounding enabling sensitisation of risk models in the presence of absence of AI to estimate causal contributions of AI.","This is in part due to the dearth of AI impact data upon which to fit distributions.","In this work, we address these gaps in two ways.","First, we demonstrate how scenario modelling (grounded in established statistical techniques such as Markov chains, copulas and Monte Carlo simulation) can be used to model AI risk holistically.","Second, we show how lookalike distributions from phenomena analogous to AI can be used to estimate AI impacts in the absence of directly observable data.","We demonstrate the utility of our methods for benchmarking cumulative AI risk via risk analysis of a logistic scenario simulations."],"url":"http://arxiv.org/abs/2502.14491v1"}
{"created":"2025-02-20 12:09:34","title":"U-index: A Universal Indexing Framework for Matching Long Patterns","abstract":"Text indexing is a fundamental and well-studied problem. Classic solutions either replace the original text with a compressed representation, e.g., the FM-index and its variants, or keep it uncompressed but attach some redundancy - an index - to accelerate matching. The former solutions thus retain excellent compressed space, but areslow in practice. The latter approaches, like the suffix array, instead sacrifice space for speed.   We show that efficient text indexing can be achieved using just a small extra space on top of the original text, provided that the query patterns are sufficiently long. More specifically, we develop a new indexing paradigm in which a sketch of a query pattern is first matched against a sketch of the text. Once candidate matches are retrieved, they are verified using the original text. This paradigm is thus universal in the sense that it allows us to use any solution to index the sketched text, like a suffix array, FM-index, or r-index.   We explore both the theory and the practice of this universal framework. With an extensive experimental analysis, we show that, surprisingly, universal indexes can be constructed much faster than their unsketched counterparts and take a fraction of the space, as a direct consequence of (i) having a lower bound on the length of patterns and (ii) working in sketch space. Furthermore, these data structures have the potential of retaining or even improving query time, because matching against the sketched text is faster and verifying candidates can be theoretically done in constant time per occurrence (or, in practice, by short and cache-friendly scans of the text). Finally, we discuss some important applications of this novel indexing paradigm to computational biology. We hypothesize that such indexes will be particularly effective when the queries are sufficiently long, and so demonstrate applications in long-read mapping.","sentences":["Text indexing is a fundamental and well-studied problem.","Classic solutions either replace the original text with a compressed representation, e.g., the FM-index and its variants, or keep it uncompressed but attach some redundancy - an index - to accelerate matching.","The former solutions thus retain excellent compressed space, but areslow in practice.","The latter approaches, like the suffix array, instead sacrifice space for speed.   ","We show that efficient text indexing can be achieved using just a small extra space on top of the original text, provided that the query patterns are sufficiently long.","More specifically, we develop a new indexing paradigm in which a sketch of a query pattern is first matched against a sketch of the text.","Once candidate matches are retrieved, they are verified using the original text.","This paradigm is thus universal in the sense that it allows us to use any solution to index the sketched text, like a suffix array, FM-index, or r-index.   ","We explore both the theory and the practice of this universal framework.","With an extensive experimental analysis, we show that, surprisingly, universal indexes can be constructed much faster than their unsketched counterparts and take a fraction of the space, as a direct consequence of (i) having a lower bound on the length of patterns and (ii) working in sketch space.","Furthermore, these data structures have the potential of retaining or even improving query time, because matching against the sketched text is faster and verifying candidates can be theoretically done in constant time per occurrence (or, in practice, by short and cache-friendly scans of the text).","Finally, we discuss some important applications of this novel indexing paradigm to computational biology.","We hypothesize that such indexes will be particularly effective when the queries are sufficiently long, and so demonstrate applications in long-read mapping."],"url":"http://arxiv.org/abs/2502.14488v1"}
{"created":"2025-02-20 11:52:39","title":"Inductive Synthesis of Inductive Heap Predicates -- Extended Version","abstract":"We present an approach to automatically synthesise recursive predicates in Separation Logic (SL) from concrete data structure instances using Inductive Logic Programming (ILP) techniques. The main challenges to make such synthesis effective are (1) making it work without negative examples that are required in ILP but are difficult to construct for heap-based structures in an automated fashion, and (2) to be capable of summarising not just the shape of a heap (e.g., it is a linked list), but also the properties of the data it stores (e.g., it is a sorted linked list). We tackle these challenges with a new predicate learning algorithm. The key contributions of our work are (a) the formulation of ILP-based learning only using positive examples and (b) an algorithm that synthesises property-rich SL predicates from concrete memory graphs based on the positive-only learning.   We show that our framework can efficiently and correctly synthesise SL predicates for structures that were beyond the reach of the state-of-the-art tools, including those featuring non-trivial payload constraints (e.g., binary search trees) and nested recursion (e.g., n-ary trees). We further extend the usability of our approach by a memory graph generator that produces positive heap examples from programs. Finally, we show how our approach facilitates deductive verification and synthesis of correct-by-construction code.","sentences":["We present an approach to automatically synthesise recursive predicates in Separation Logic (SL) from concrete data structure instances using Inductive Logic Programming (ILP) techniques.","The main challenges to make such synthesis effective are (1) making it work without negative examples that are required in ILP but are difficult to construct for heap-based structures in an automated fashion, and (2) to be capable of summarising not just the shape of a heap (e.g., it is a linked list), but also the properties of the data it stores (e.g., it is a sorted linked list).","We tackle these challenges with a new predicate learning algorithm.","The key contributions of our work are (a) the formulation of ILP-based learning only using positive examples and (b) an algorithm that synthesises property-rich SL predicates from concrete memory graphs based on the positive-only learning.   ","We show that our framework can efficiently and correctly synthesise SL predicates for structures that were beyond the reach of the state-of-the-art tools, including those featuring non-trivial payload constraints (e.g., binary search trees) and nested recursion (e.g., n-ary trees).","We further extend the usability of our approach by a memory graph generator that produces positive heap examples from programs.","Finally, we show how our approach facilitates deductive verification and synthesis of correct-by-construction code."],"url":"http://arxiv.org/abs/2502.14478v1"}
{"created":"2025-02-20 11:52:26","title":"Argument-Based Comparative Question Answering Evaluation Benchmark","abstract":"In this paper, we aim to solve the problems standing in the way of automatic comparative question answering. To this end, we propose an evaluation framework to assess the quality of comparative question answering summaries. We formulate 15 criteria for assessing comparative answers created using manual annotation and annotation from 6 large language models and two comparative question asnwering datasets. We perform our tests using several LLMs and manual annotation under different settings and demonstrate the constituency of both evaluations. Our results demonstrate that the Llama-3 70B Instruct model demonstrates the best results for summary evaluation, while GPT-4 is the best for answering comparative questions. All used data, code, and evaluation results are publicly available\\footnote{\\url{https://anonymous.4open.science/r/cqa-evaluation-benchmark-4561/README.md}}.","sentences":["In this paper, we aim to solve the problems standing in the way of automatic comparative question answering.","To this end, we propose an evaluation framework to assess the quality of comparative question answering summaries.","We formulate 15 criteria for assessing comparative answers created using manual annotation and annotation from 6 large language models and two comparative question asnwering datasets.","We perform our tests using several LLMs and manual annotation under different settings and demonstrate the constituency of both evaluations.","Our results demonstrate that the Llama-3 70B Instruct model demonstrates the best results for summary evaluation, while GPT-4 is the best for answering comparative questions.","All used data, code, and evaluation results are publicly available\\footnote{\\url{https://anonymous.4open.science/r/cqa-evaluation-benchmark-4561/README.md}}."],"url":"http://arxiv.org/abs/2502.14476v1"}
{"created":"2025-02-20 11:49:50","title":"Integrating Extra Modality Helps Segmentor Find Camouflaged Objects Well","abstract":"Camouflaged Object Segmentation (COS) remains a challenging problem due to the subtle visual differences between camouflaged objects and backgrounds. Owing to the exceedingly limited visual cues available from visible spectrum, previous RGB single-modality approaches often struggle to achieve satisfactory results, prompting the exploration of multimodal data to enhance detection accuracy. In this work, we present UniCOS, a novel framework that effectively leverages diverse data modalities to improve segmentation performance. UniCOS comprises two key components: a multimodal segmentor, UniSEG, and a cross-modal knowledge learning module, UniLearner. UniSEG employs a state space fusion mechanism to integrate cross-modal features within a unified state space, enhancing contextual understanding and improving robustness to integration of heterogeneous data. Additionally, it includes a fusion-feedback mechanism that facilitate feature extraction. UniLearner exploits multimodal data unrelated to the COS task to improve the segmentation ability of the COS models by generating pseudo-modal content and cross-modal semantic associations. Extensive experiments demonstrate that UniSEG outperforms existing Multimodal COS (MCOS) segmentors, regardless of whether real or pseudo-multimodal COS data is available. Moreover, in scenarios where multimodal COS data is unavailable but multimodal non-COS data is accessible, UniLearner effectively exploits these data to enhance segmentation performance. Our code will be made publicly available on \\href{https://github.com/cnyvfang/UniCOS}{GitHub}.","sentences":["Camouflaged Object Segmentation (COS) remains a challenging problem due to the subtle visual differences between camouflaged objects and backgrounds.","Owing to the exceedingly limited visual cues available from visible spectrum, previous RGB single-modality approaches often struggle to achieve satisfactory results, prompting the exploration of multimodal data to enhance detection accuracy.","In this work, we present UniCOS, a novel framework that effectively leverages diverse data modalities to improve segmentation performance.","UniCOS comprises two key components: a multimodal segmentor, UniSEG, and a cross-modal knowledge learning module, UniLearner.","UniSEG employs a state space fusion mechanism to integrate cross-modal features within a unified state space, enhancing contextual understanding and improving robustness to integration of heterogeneous data.","Additionally, it includes a fusion-feedback mechanism that facilitate feature extraction.","UniLearner exploits multimodal data unrelated to the COS task to improve the segmentation ability of the COS models by generating pseudo-modal content and cross-modal semantic associations.","Extensive experiments demonstrate that UniSEG outperforms existing Multimodal COS (MCOS) segmentors, regardless of whether real or pseudo-multimodal COS data is available.","Moreover, in scenarios where multimodal COS data is unavailable but multimodal non-COS data is accessible, UniLearner effectively exploits these data to enhance segmentation performance.","Our code will be made publicly available on \\href{https://github.com/cnyvfang/UniCOS}{GitHub}."],"url":"http://arxiv.org/abs/2502.14471v1"}
{"created":"2025-02-20 11:46:51","title":"Enhancing Smart Environments with Context-Aware Chatbots using Large Language Models","abstract":"This work presents a novel architecture for context-aware interactions within smart environments, leveraging Large Language Models (LLMs) to enhance user experiences. Our system integrates user location data obtained through UWB tags and sensor-equipped smart homes with real-time human activity recognition (HAR) to provide a comprehensive understanding of user context. This contextual information is then fed to an LLM-powered chatbot, enabling it to generate personalised interactions and recommendations based on the user's current activity and environment. This approach moves beyond traditional static chatbot interactions by dynamically adapting to the user's real-time situation. A case study conducted from a real-world dataset demonstrates the feasibility and effectiveness of our proposed architecture, showcasing its potential to create more intuitive and helpful interactions within smart homes. The results highlight the significant benefits of integrating LLM with real-time activity and location data to deliver personalised and contextually relevant user experiences.","sentences":["This work presents a novel architecture for context-aware interactions within smart environments, leveraging Large Language Models (LLMs) to enhance user experiences.","Our system integrates user location data obtained through UWB tags and sensor-equipped smart homes with real-time human activity recognition (HAR) to provide a comprehensive understanding of user context.","This contextual information is then fed to an LLM-powered chatbot, enabling it to generate personalised interactions and recommendations based on the user's current activity and environment.","This approach moves beyond traditional static chatbot interactions by dynamically adapting to the user's real-time situation.","A case study conducted from a real-world dataset demonstrates the feasibility and effectiveness of our proposed architecture, showcasing its potential to create more intuitive and helpful interactions within smart homes.","The results highlight the significant benefits of integrating LLM with real-time activity and location data to deliver personalised and contextually relevant user experiences."],"url":"http://arxiv.org/abs/2502.14469v1"}
{"created":"2025-02-20 11:36:08","title":"PQBFL: A Post-Quantum Blockchain-based Protocol for Federated Learning","abstract":"One of the goals of Federated Learning (FL) is to collaboratively train a global model using local models from remote participants. However, the FL process is susceptible to various security challenges, including interception and tampering models, information leakage through shared gradients, and privacy breaches that expose participant identities or data, particularly in sensitive domains such as medical environments. Furthermore, the advent of quantum computing poses a critical threat to existing cryptographic protocols through the Shor and Grover algorithms, causing security concerns in the communication of FL systems. To address these challenges, we propose a Post-Quantum Blockchain-based protocol for Federated Learning (PQBFL) that utilizes post-quantum cryptographic (PQC) algorithms and blockchain to enhance model security and participant identity privacy in FL systems. It employs a hybrid communication strategy that combines off-chain and on-chain channels to optimize cost efficiency, improve security, and preserve participant privacy while ensuring accountability for reputation-based authentication in FL systems. The PQBFL specifically addresses the security requirement for the iterative nature of FL, which is a less notable point in the literature. Hence, it leverages ratcheting mechanisms to provide forward secrecy and post-compromise security during all the rounds of the learning process. In conclusion, PQBFL provides a secure and resilient solution for federated learning that is well-suited to the quantum computing era.","sentences":["One of the goals of Federated Learning (FL) is to collaboratively train a global model using local models from remote participants.","However, the FL process is susceptible to various security challenges, including interception and tampering models, information leakage through shared gradients, and privacy breaches that expose participant identities or data, particularly in sensitive domains such as medical environments.","Furthermore, the advent of quantum computing poses a critical threat to existing cryptographic protocols through the Shor and Grover algorithms, causing security concerns in the communication of FL systems.","To address these challenges, we propose a Post-Quantum Blockchain-based protocol for Federated Learning (PQBFL) that utilizes post-quantum cryptographic (PQC) algorithms and blockchain to enhance model security and participant identity privacy in FL systems.","It employs a hybrid communication strategy that combines off-chain and on-chain channels to optimize cost efficiency, improve security, and preserve participant privacy while ensuring accountability for reputation-based authentication in FL systems.","The PQBFL specifically addresses the security requirement for the iterative nature of FL, which is a less notable point in the literature.","Hence, it leverages ratcheting mechanisms to provide forward secrecy and post-compromise security during all the rounds of the learning process.","In conclusion, PQBFL provides a secure and resilient solution for federated learning that is well-suited to the quantum computing era."],"url":"http://arxiv.org/abs/2502.14464v1"}
{"created":"2025-02-20 11:18:39","title":"Llamba: Scaling Distilled Recurrent Models for Efficient Language Processing","abstract":"We introduce Llamba, a family of efficient recurrent language models distilled from Llama-3.x into the Mamba architecture. The series includes Llamba-1B, Llamba-3B, and Llamba-8B, which achieve higher inference throughput and handle significantly larger batch sizes than Transformer-based models while maintaining comparable benchmark performance. Furthermore, Llamba demonstrates the effectiveness of cross-architecture distillation using MOHAWK (Bick et al., 2024), achieving these results with less than 0.1% of the training data typically used for models of similar size. To take full advantage of their efficiency, we provide an optimized implementation of Llamba for resource-constrained devices such as smartphones and edge platforms, offering a practical and memory-efficient alternative to Transformers. Overall, Llamba improves the tradeoff between speed, memory efficiency, and performance, making high-quality language models more accessible.","sentences":["We introduce Llamba, a family of efficient recurrent language models distilled from Llama-3.x into the Mamba architecture.","The series includes Llamba-1B, Llamba-3B, and Llamba-8B, which achieve higher inference throughput and handle significantly larger batch sizes than Transformer-based models while maintaining comparable benchmark performance.","Furthermore, Llamba demonstrates the effectiveness of cross-architecture distillation using MOHAWK (Bick et al., 2024), achieving these results with less than 0.1% of the training data typically used for models of similar size.","To take full advantage of their efficiency, we provide an optimized implementation of Llamba for resource-constrained devices such as smartphones and edge platforms, offering a practical and memory-efficient alternative to Transformers.","Overall, Llamba improves the tradeoff between speed, memory efficiency, and performance, making high-quality language models more accessible."],"url":"http://arxiv.org/abs/2502.14458v1"}
{"created":"2025-02-20 11:18:35","title":"Watch Less, Feel More: Sim-to-Real RL for Generalizable Articulated Object Manipulation via Motion Adaptation and Impedance Control","abstract":"Articulated object manipulation poses a unique challenge compared to rigid object manipulation as the object itself represents a dynamic environment. In this work, we present a novel RL-based pipeline equipped with variable impedance control and motion adaptation leveraging observation history for generalizable articulated object manipulation, focusing on smooth and dexterous motion during zero-shot sim-to-real transfer. To mitigate the sim-to-real gap, our pipeline diminishes reliance on vision by not leveraging the vision data feature (RGBD/pointcloud) directly as policy input but rather extracting useful low-dimensional data first via off-the-shelf modules. Additionally, we experience less sim-to-real gap by inferring object motion and its intrinsic properties via observation history as well as utilizing impedance control both in the simulation and in the real world. Furthermore, we develop a well-designed training setting with great randomization and a specialized reward system (task-aware and motion-aware) that enables multi-staged, end-to-end manipulation without heuristic motion planning. To the best of our knowledge, our policy is the first to report 84\\% success rate in the real world via extensive experiments with various unseen objects.","sentences":["Articulated object manipulation poses a unique challenge compared to rigid object manipulation as the object itself represents a dynamic environment.","In this work, we present a novel RL-based pipeline equipped with variable impedance control and motion adaptation leveraging observation history for generalizable articulated object manipulation, focusing on smooth and dexterous motion during zero-shot sim-to-real transfer.","To mitigate the sim-to-real gap, our pipeline diminishes reliance on vision by not leveraging the vision data feature (RGBD/pointcloud) directly as policy input but rather extracting useful low-dimensional data first via off-the-shelf modules.","Additionally, we experience less sim-to-real gap by inferring object motion and its intrinsic properties via observation history as well as utilizing impedance control both in the simulation and in the real world.","Furthermore, we develop a well-designed training setting with great randomization and a specialized reward system (task-aware and motion-aware) that enables multi-staged, end-to-end manipulation without heuristic motion planning.","To the best of our knowledge, our policy is the first to report 84\\% success rate in the real world via extensive experiments with various unseen objects."],"url":"http://arxiv.org/abs/2502.14457v1"}
{"created":"2025-02-20 10:55:39","title":"LEIT-motifs: Scalable Motif Mining in Multidimensional Time Series","abstract":"Time series play a fundamental role in many domains, capturing a plethora of information about the underlying data-generating processes. When a process generates multiple synchronized signals we are faced with multidimensional time series. In this context a fundamental problem is that of motif mining, where we seek patterns repeating twice with minor variations, spanning some of the dimensions. State of the art exact solutions for this problem run in time quadratic in the length of the input time series.   We provide a scalable method to find the top-k motifs in multidimensional time series with probabilistic guarantees on the quality of the results. Our algorithm runs in time subquadratic in the length of the input, and returns the exact solution with probability at least $1-\\delta$, where $\\delta$ is a user-defined parameter. The algorithm is designed to be adaptive to the input distribution, self-tuning its parameters while respecting user-defined limits on the memory to use.   Our theoretical analysis is complemented by an extensive experimental evaluation, showing that our algorithm is orders of magnitude faster than the state of the art.","sentences":["Time series play a fundamental role in many domains, capturing a plethora of information about the underlying data-generating processes.","When a process generates multiple synchronized signals we are faced with multidimensional time series.","In this context a fundamental problem is that of motif mining, where we seek patterns repeating twice with minor variations, spanning some of the dimensions.","State of the art exact solutions for this problem run in time quadratic in the length of the input time series.   ","We provide a scalable method to find the top-k motifs in multidimensional time series with probabilistic guarantees on the quality of the results.","Our algorithm runs in time subquadratic in the length of the input, and returns the exact solution with probability at least $1-\\delta$, where $\\delta$ is a user-defined parameter.","The algorithm is designed to be adaptive to the input distribution, self-tuning its parameters while respecting user-defined limits on the memory to use.   ","Our theoretical analysis is complemented by an extensive experimental evaluation, showing that our algorithm is orders of magnitude faster than the state of the art."],"url":"http://arxiv.org/abs/2502.14446v1"}
{"created":"2025-02-20 10:35:01","title":"Daily Land Surface Temperature Reconstruction in Landsat Cross-Track Areas Using Deep Ensemble Learning With Uncertainty Quantification","abstract":"Many real-world applications rely on land surface temperature (LST) data at high spatiotemporal resolution. In complex urban areas, LST exhibits significant variations, fluctuating dramatically within and across city blocks. Landsat provides high spatial resolution data at 100 meters but is limited by long revisit time, with cloud cover further disrupting data collection. Here, we propose DELAG, a deep ensemble learning method that integrates annual temperature cycles and Gaussian processes, to reconstruct Landsat LST in complex urban areas. Leveraging the cross-track characteristics and dual-satellite operation of Landsat since 2021, we further enhance data availability to 4 scenes every 16 days. We select New York City, London and Hong Kong from three different continents as study areas. Experiments show that DELAG successfully reconstructed LST in the three cities under clear-sky (RMSE = 0.73-0.96 K) and heavily-cloudy (RMSE = 0.84-1.62 K) situations, superior to existing methods. Additionally, DELAG can quantify uncertainty that enhances LST reconstruction reliability. We further tested the reconstructed LST to estimate near-surface air temperature, achieving results (RMSE = 1.48-2.11 K) comparable to those derived from clear-sky LST (RMSE = 1.63-2.02 K). The results demonstrate the successful reconstruction through DELAG and highlight the broader applications of LST reconstruction for estimating accurate air temperature. Our study thus provides a novel and practical method for Landsat LST reconstruction, particularly suited for complex urban areas within Landsat cross-track areas, taking one step toward addressing complex climate events at high spatiotemporal resolution.","sentences":["Many real-world applications rely on land surface temperature (LST) data at high spatiotemporal resolution.","In complex urban areas, LST exhibits significant variations, fluctuating dramatically within and across city blocks.","Landsat provides high spatial resolution data at 100 meters but is limited by long revisit time, with cloud cover further disrupting data collection.","Here, we propose DELAG, a deep ensemble learning method that integrates annual temperature cycles and Gaussian processes, to reconstruct Landsat LST in complex urban areas.","Leveraging the cross-track characteristics and dual-satellite operation of Landsat since 2021, we further enhance data availability to 4 scenes every 16 days.","We select New York City, London and Hong Kong from three different continents as study areas.","Experiments show that DELAG successfully reconstructed LST in the three cities under clear-sky (RMSE = 0.73-0.96 K) and heavily-cloudy (RMSE = 0.84-1.62 K) situations, superior to existing methods.","Additionally, DELAG can quantify uncertainty that enhances LST reconstruction reliability.","We further tested the reconstructed LST to estimate near-surface air temperature, achieving results (RMSE = 1.48-2.11 K) comparable to those derived from clear-sky LST (RMSE = 1.63-2.02 K).","The results demonstrate the successful reconstruction through DELAG and highlight the broader applications of LST reconstruction for estimating accurate air temperature.","Our study thus provides a novel and practical method for Landsat LST reconstruction, particularly suited for complex urban areas within Landsat cross-track areas, taking one step toward addressing complex climate events at high spatiotemporal resolution."],"url":"http://arxiv.org/abs/2502.14433v1"}
{"created":"2025-02-20 10:33:21","title":"Port-Hamiltonian Neural Networks with Output Error Noise Models","abstract":"Hamiltonian neural networks (HNNs) represent a promising class of physics-informed deep learning methods that utilize Hamiltonian theory as foundational knowledge within neural networks. However, their direct application to engineering systems is often challenged by practical issues, including the presence of external inputs, dissipation, and noisy measurements. This paper introduces a novel framework that enhances the capabilities of HNNs to address these real-life factors. We integrate port-Hamiltonian theory into the neural network structure, allowing for the inclusion of external inputs and dissipation, while mitigating the impact of measurement noise through an output-error (OE) model structure. The resulting output error port-Hamiltonian neural networks (OE-pHNNs) can be adapted to tackle modeling complex engineering systems with noisy measurements. Furthermore, we propose the identification of OE-pHNNs based on the subspace encoder approach (SUBNET), which efficiently approximates the complete simulation loss using subsections of the data and uses an encoder function to predict initial states. By integrating SUBNET with OE-pHNNs, we achieve consistent models of complex engineering systems under noisy measurements. In addition, we perform a consistency analysis to ensure the reliability of the proposed data-driven model learning method. We demonstrate the effectiveness of our approach on system identification benchmarks, showing its potential as a powerful tool for modeling dynamic systems in real-world applications.","sentences":["Hamiltonian neural networks (HNNs) represent a promising class of physics-informed deep learning methods that utilize Hamiltonian theory as foundational knowledge within neural networks.","However, their direct application to engineering systems is often challenged by practical issues, including the presence of external inputs, dissipation, and noisy measurements.","This paper introduces a novel framework that enhances the capabilities of HNNs to address these real-life factors.","We integrate port-Hamiltonian theory into the neural network structure, allowing for the inclusion of external inputs and dissipation, while mitigating the impact of measurement noise through an output-error (OE) model structure.","The resulting output error port-Hamiltonian neural networks (OE-pHNNs) can be adapted to tackle modeling complex engineering systems with noisy measurements.","Furthermore, we propose the identification of OE-pHNNs based on the subspace encoder approach (SUBNET), which efficiently approximates the complete simulation loss using subsections of the data and uses an encoder function to predict initial states.","By integrating SUBNET with OE-pHNNs, we achieve consistent models of complex engineering systems under noisy measurements.","In addition, we perform a consistency analysis to ensure the reliability of the proposed data-driven model learning method.","We demonstrate the effectiveness of our approach on system identification benchmarks, showing its potential as a powerful tool for modeling dynamic systems in real-world applications."],"url":"http://arxiv.org/abs/2502.14432v1"}
{"created":"2025-02-20 10:27:19","title":"Cardiac Evidence Backtracking for Eating Behavior Monitoring using Collocative Electrocardiogram Imagining","abstract":"Eating monitoring has remained an open challenge in medical research for years due to the lack of non-invasive sensors for continuous monitoring and the reliable methods for automatic behavior detection. In this paper, we present a pilot study using the wearable 24-hour ECG for sensing and tailoring the sophisticated deep learning for ad-hoc and interpretable detection. This is accomplished using a collocative learning framework in which 1) we construct collocative tensors as pseudo-images from 1D ECG signals to improve the feasibility of 2D image-based deep models; 2) we formulate the cardiac logic of analyzing the ECG data in a comparative way as periodic attention regulators so as to guide the deep inference to collect evidence in a human comprehensible manner; and 3) we improve the interpretability of the framework by enabling the backtracking of evidence with a set of methods designed for Class Activation Mapping (CAM) decoding and decision tree/forest generation. The effectiveness of the proposed framework has been validated on the largest ECG dataset of eating behavior with superior performance over conventional models, and its capacity of cardiac evidence mining has also been verified through the consistency of the evidence it backtracked and that of the previous medical studies.","sentences":["Eating monitoring has remained an open challenge in medical research for years due to the lack of non-invasive sensors for continuous monitoring and the reliable methods for automatic behavior detection.","In this paper, we present a pilot study using the wearable 24-hour ECG for sensing and tailoring the sophisticated deep learning for ad-hoc and interpretable detection.","This is accomplished using a collocative learning framework in which 1) we construct collocative tensors as pseudo-images from 1D ECG signals to improve the feasibility of 2D image-based deep models; 2) we formulate the cardiac logic of analyzing the ECG data in a comparative way as periodic attention regulators so as to guide the deep inference to collect evidence in a human comprehensible manner; and 3) we improve the interpretability of the framework by enabling the backtracking of evidence with a set of methods designed for Class Activation Mapping (CAM) decoding and decision tree/forest generation.","The effectiveness of the proposed framework has been validated on the largest ECG dataset of eating behavior with superior performance over conventional models, and its capacity of cardiac evidence mining has also been verified through the consistency of the evidence it backtracked and that of the previous medical studies."],"url":"http://arxiv.org/abs/2502.14430v1"}
{"created":"2025-02-20 10:25:13","title":"Token-Level Density-Based Uncertainty Quantification Methods for Eliciting Truthfulness of Large Language Models","abstract":"Uncertainty quantification (UQ) is a prominent approach for eliciting truthful answers from large language models (LLMs). To date, information-based and consistency-based UQ have been the dominant UQ methods for text generation via LLMs. Density-based methods, despite being very effective for UQ in text classification with encoder-based models, have not been very successful with generative LLMs. In this work, we adapt Mahalanobis Distance (MD) - a well-established UQ technique in classification tasks - for text generation and introduce a new supervised UQ method. Our method extracts token embeddings from multiple layers of LLMs, computes MD scores for each token, and uses linear regression trained on these features to provide robust uncertainty scores. Through extensive experiments on eleven datasets, we demonstrate that our approach substantially improves over existing UQ methods, providing accurate and computationally efficient uncertainty scores for both sequence-level selective generation and claim-level fact-checking tasks. Our method also exhibits strong generalization to out-of-domain data, making it suitable for a wide range of LLM-based applications.","sentences":["Uncertainty quantification (UQ) is a prominent approach for eliciting truthful answers from large language models (LLMs).","To date, information-based and consistency-based UQ have been the dominant UQ methods for text generation via LLMs.","Density-based methods, despite being very effective for UQ in text classification with encoder-based models, have not been very successful with generative LLMs.","In this work, we adapt Mahalanobis Distance (MD) - a well-established UQ technique in classification tasks - for text generation and introduce a new supervised UQ method.","Our method extracts token embeddings from multiple layers of LLMs, computes MD scores for each token, and uses linear regression trained on these features to provide robust uncertainty scores.","Through extensive experiments on eleven datasets, we demonstrate that our approach substantially improves over existing UQ methods, providing accurate and computationally efficient uncertainty scores for both sequence-level selective generation and claim-level fact-checking tasks.","Our method also exhibits strong generalization to out-of-domain data, making it suitable for a wide range of LLM-based applications."],"url":"http://arxiv.org/abs/2502.14427v1"}
{"created":"2025-02-20 10:23:27","title":"A Survey on Data Contamination for Large Language Models","abstract":"Recent advancements in Large Language Models (LLMs) have demonstrated significant progress in various areas, such as text generation and code synthesis. However, the reliability of performance evaluation has come under scrutiny due to data contamination-the unintended overlap between training and test datasets. This overlap has the potential to artificially inflate model performance, as LLMs are typically trained on extensive datasets scraped from publicly available sources. These datasets often inadvertently overlap with the benchmarks used for evaluation, leading to an overestimation of the models' true generalization capabilities. In this paper, we first examine the definition and impacts of data contamination. Secondly, we review methods for contamination-free evaluation, focusing on three strategies: data updating-based methods, data rewriting-based methods, and prevention-based methods. Specifically, we highlight dynamic benchmarks and LLM-driven evaluation methods. Finally, we categorize contamination detecting methods based on model information dependency: white-Box, gray-Box, and black-Box detection approaches. Our survey highlights the requirements for more rigorous evaluation protocols and proposes future directions for addressing data contamination challenges.","sentences":["Recent advancements in Large Language Models (LLMs) have demonstrated significant progress in various areas, such as text generation and code synthesis.","However, the reliability of performance evaluation has come under scrutiny due to data contamination-the unintended overlap between training and test datasets.","This overlap has the potential to artificially inflate model performance, as LLMs are typically trained on extensive datasets scraped from publicly available sources.","These datasets often inadvertently overlap with the benchmarks used for evaluation, leading to an overestimation of the models' true generalization capabilities.","In this paper, we first examine the definition and impacts of data contamination.","Secondly, we review methods for contamination-free evaluation, focusing on three strategies: data updating-based methods, data rewriting-based methods, and prevention-based methods.","Specifically, we highlight dynamic benchmarks and LLM-driven evaluation methods.","Finally, we categorize contamination detecting methods based on model information dependency: white-Box, gray-Box, and black-Box detection approaches.","Our survey highlights the requirements for more rigorous evaluation protocols and proposes future directions for addressing data contamination challenges."],"url":"http://arxiv.org/abs/2502.14425v1"}
{"created":"2025-02-20 10:16:18","title":"ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model","abstract":"Humans possess a unified cognitive ability to perceive, comprehend, and interact with the physical world. Why can't large language models replicate this holistic understanding? Through a systematic analysis of existing training paradigms in vision-language-action models (VLA), we identify two key challenges: spurious forgetting, where robot training overwrites crucial visual-text alignments, and task interference, where competing control and understanding tasks degrade performance when trained jointly. To overcome these limitations, we propose ChatVLA, a novel framework featuring Phased Alignment Training, which incrementally integrates multimodal data after initial control mastery, and a Mixture-of-Experts architecture to minimize task interference. ChatVLA demonstrates competitive performance on visual question-answering datasets and significantly surpasses state-of-the-art vision-language-action (VLA) methods on multimodal understanding benchmarks. Notably, it achieves a six times higher performance on MMMU and scores 47.2% on MMStar with a more parameter-efficient design than ECoT. Furthermore, ChatVLA demonstrates superior performance on 25 real-world robot manipulation tasks compared to existing VLA methods like OpenVLA. Our findings highlight the potential of our unified framework for achieving both robust multimodal understanding and effective robot control.","sentences":["Humans possess a unified cognitive ability to perceive, comprehend, and interact with the physical world.","Why can't large language models replicate this holistic understanding?","Through a systematic analysis of existing training paradigms in vision-language-action models (VLA), we identify two key challenges: spurious forgetting, where robot training overwrites crucial visual-text alignments, and task interference, where competing control and understanding tasks degrade performance when trained jointly.","To overcome these limitations, we propose ChatVLA, a novel framework featuring Phased Alignment Training, which incrementally integrates multimodal data after initial control mastery, and a Mixture-of-Experts architecture to minimize task interference.","ChatVLA demonstrates competitive performance on visual question-answering datasets and significantly surpasses state-of-the-art vision-language-action (VLA) methods on multimodal understanding benchmarks.","Notably, it achieves a six times higher performance on MMMU and scores 47.2% on MMStar with a more parameter-efficient design than ECoT. Furthermore, ChatVLA demonstrates superior performance on 25 real-world robot manipulation tasks compared to existing VLA methods like OpenVLA.","Our findings highlight the potential of our unified framework for achieving both robust multimodal understanding and effective robot control."],"url":"http://arxiv.org/abs/2502.14420v1"}
{"created":"2025-02-20 09:59:28","title":"Evaluating Precise Geolocation Inference Capabilities of Vision Language Models","abstract":"The prevalence of Vision-Language Models (VLMs) raises important questions about privacy in an era where visual information is increasingly available. While foundation VLMs demonstrate broad knowledge and learned capabilities, we specifically investigate their ability to infer geographic location from previously unseen image data. This paper introduces a benchmark dataset collected from Google Street View that represents its global distribution of coverage. Foundation models are evaluated on single-image geolocation inference, with many achieving median distance errors of <300 km. We further evaluate VLM \"agents\" with access to supplemental tools, observing up to a 30.6% decrease in distance error. Our findings establish that modern foundation VLMs can act as powerful image geolocation tools, without being specifically trained for this task. When coupled with increasing accessibility of these models, our findings have greater implications for online privacy. We discuss these risks, as well as future work in this area.","sentences":["The prevalence of Vision-Language Models (VLMs) raises important questions about privacy in an era where visual information is increasingly available.","While foundation VLMs demonstrate broad knowledge and learned capabilities, we specifically investigate their ability to infer geographic location from previously unseen image data.","This paper introduces a benchmark dataset collected from Google Street View that represents its global distribution of coverage.","Foundation models are evaluated on single-image geolocation inference, with many achieving median distance errors of <300 km.","We further evaluate VLM \"agents\" with access to supplemental tools, observing up to a 30.6% decrease in distance error.","Our findings establish that modern foundation VLMs can act as powerful image geolocation tools, without being specifically trained for this task.","When coupled with increasing accessibility of these models, our findings have greater implications for online privacy.","We discuss these risks, as well as future work in this area."],"url":"http://arxiv.org/abs/2502.14412v1"}
{"created":"2025-02-20 09:57:42","title":"Unstructured Evidence Attribution for Long Context Query Focused Summarization","abstract":"Large language models (LLMs) are capable of generating coherent summaries from very long contexts given a user query. Extracting and properly citing evidence spans could help improve the transparency and reliability of these summaries. At the same time, LLMs suffer from positional biases in terms of which information they understand and attend to, which could affect evidence citation. Whereas previous work has focused on evidence citation with predefined levels of granularity (e.g. sentence, paragraph, document, etc.), we propose the task of long-context query focused summarization with unstructured evidence citation. We show how existing systems struggle to generate and properly cite unstructured evidence from their context, and that evidence tends to be \"lost-in-the-middle\". To help mitigate this, we create the Summaries with Unstructured Evidence Text dataset (SUnsET), a synthetic dataset generated using a novel domain-agnostic pipeline which can be used as supervision to adapt LLMs to this task. We demonstrate across 5 LLMs of different sizes and 4 datasets with varying document types and lengths that LLMs adapted with SUnsET data generate more relevant and factually consistent evidence than their base models, extract evidence from more diverse locations in their context, and can generate more relevant and consistent summaries.","sentences":["Large language models (LLMs) are capable of generating coherent summaries from very long contexts given a user query.","Extracting and properly citing evidence spans could help improve the transparency and reliability of these summaries.","At the same time, LLMs suffer from positional biases in terms of which information they understand and attend to, which could affect evidence citation.","Whereas previous work has focused on evidence citation with predefined levels of granularity (e.g. sentence, paragraph, document, etc.), we propose the task of long-context query focused summarization with unstructured evidence citation.","We show how existing systems struggle to generate and properly cite unstructured evidence from their context, and that evidence tends to be \"lost-in-the-middle\".","To help mitigate this, we create the Summaries with Unstructured Evidence Text dataset (SUnsET), a synthetic dataset generated using a novel domain-agnostic pipeline which can be used as supervision to adapt LLMs to this task.","We demonstrate across 5 LLMs of different sizes and 4 datasets with varying document types and lengths that LLMs adapted with SUnsET data generate more relevant and factually consistent evidence than their base models, extract evidence from more diverse locations in their context, and can generate more relevant and consistent summaries."],"url":"http://arxiv.org/abs/2502.14409v1"}
{"created":"2025-02-20 09:38:38","title":"Bibliometric Analysis of Scientific Production on the COVID-19 Effect in Information Sciences","abstract":"This paper analyzes the scientific production on the COVID-19 effect in the area of Information Sciences from a bibliometric perspective. The objectives focused on: 1) determining the most productive authors, countries, institutions and journals; 2) identifying the sources that constitute the core of scientific production; 3) examining the manuscripts with the greatest impact; and 4) visualizing the thematic and conceptual structure of the scientific domain analyzed. Bibliometric indicators and factor analysis techniques were used for data analysis. A total of 1,175 publications indexed in the Web of Science (WoS) core collection from 2020 to 2022 were retrieved. The results showed that the most relevant countries were the United States, United Kingdom, China and Spain. The core of the scientific production was formed by the publications: Journal of the American Medical Informatics Association, Information Professional, Scientometrics and Journal of Health Communication. The papers with the greatest impact were concentrated in those dedicated to the analysis of the role of telemedicine in medical care. The conceptual structure showed the main research fronts, such as the role of telehealth, academic libraries and digital literacy in the fight against the pandemic, the role of social networks in the health crisis, as well as the problem of misinformation and fake news","sentences":["This paper analyzes the scientific production on the COVID-19 effect in the area of Information Sciences from a bibliometric perspective.","The objectives focused on: 1) determining the most productive authors, countries, institutions and journals; 2) identifying the sources that constitute the core of scientific production; 3) examining the manuscripts with the greatest impact; and 4) visualizing the thematic and conceptual structure of the scientific domain analyzed.","Bibliometric indicators and factor analysis techniques were used for data analysis.","A total of 1,175 publications indexed in the Web of Science (WoS) core collection from 2020 to 2022 were retrieved.","The results showed that the most relevant countries were the United States, United Kingdom, China and Spain.","The core of the scientific production was formed by the publications: Journal of the American Medical Informatics Association, Information Professional, Scientometrics and Journal of Health Communication.","The papers with the greatest impact were concentrated in those dedicated to the analysis of the role of telemedicine in medical care.","The conceptual structure showed the main research fronts, such as the role of telehealth, academic libraries and digital literacy in the fight against the pandemic, the role of social networks in the health crisis, as well as the problem of misinformation and fake news"],"url":"http://arxiv.org/abs/2502.14402v1"}
{"created":"2025-02-20 09:35:38","title":"PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data","abstract":"We introduce PhotoDoodle, a novel image editing framework designed to facilitate photo doodling by enabling artists to overlay decorative elements onto photographs. Photo doodling is challenging because the inserted elements must appear seamlessly integrated with the background, requiring realistic blending, perspective alignment, and contextual coherence. Additionally, the background must be preserved without distortion, and the artist's unique style must be captured efficiently from limited training data. These requirements are not addressed by previous methods that primarily focus on global style transfer or regional inpainting. The proposed method, PhotoDoodle, employs a two-stage training strategy. Initially, we train a general-purpose image editing model, OmniEditor, using large-scale data. Subsequently, we fine-tune this model with EditLoRA using a small, artist-curated dataset of before-and-after image pairs to capture distinct editing styles and techniques. To enhance consistency in the generated results, we introduce a positional encoding reuse mechanism. Additionally, we release a PhotoDoodle dataset featuring six high-quality styles. Extensive experiments demonstrate the advanced performance and robustness of our method in customized image editing, opening new possibilities for artistic creation.","sentences":["We introduce PhotoDoodle, a novel image editing framework designed to facilitate photo doodling by enabling artists to overlay decorative elements onto photographs.","Photo doodling is challenging because the inserted elements must appear seamlessly integrated with the background, requiring realistic blending, perspective alignment, and contextual coherence.","Additionally, the background must be preserved without distortion, and the artist's unique style must be captured efficiently from limited training data.","These requirements are not addressed by previous methods that primarily focus on global style transfer or regional inpainting.","The proposed method, PhotoDoodle, employs a two-stage training strategy.","Initially, we train a general-purpose image editing model, OmniEditor, using large-scale data.","Subsequently, we fine-tune this model with EditLoRA using a small, artist-curated dataset of before-and-after image pairs to capture distinct editing styles and techniques.","To enhance consistency in the generated results, we introduce a positional encoding reuse mechanism.","Additionally, we release a PhotoDoodle dataset featuring six high-quality styles.","Extensive experiments demonstrate the advanced performance and robustness of our method in customized image editing, opening new possibilities for artistic creation."],"url":"http://arxiv.org/abs/2502.14397v1"}
{"created":"2025-02-20 09:12:16","title":"Achieving adaptivity and optimality for multi-armed bandits using Exponential-Kullback Leiblier Maillard Sampling","abstract":"We study the problem of Multi-Armed Bandits (MAB) with reward distributions belonging to a One-Parameter Exponential Distribution (OPED) family. In the literature, several criteria have been proposed to evaluate the performance of such algorithms, including Asymptotic Optimality (A.O.), Minimax Optimality (M.O.), Sub-UCB, and variance-adaptive worst-case regret bound. Thompson Sampling (TS)-based and Upper Confidence Bound (UCB)-based algorithms have been employed to achieve some of these criteria. However, none of these algorithms simultaneously satisfy all the aforementioned criteria.   In this paper, we design an algorithm, Exponential Kullback-Leibler Maillard Sampling (abbrev. \\expklms), that can achieve multiple optimality criteria simultaneously, including A.O., M.O. with a logarithmic factor, Sub-UCB, and variance-adaptive worst-case regret bound.","sentences":["We study the problem of Multi-Armed Bandits (MAB) with reward distributions belonging to a One-Parameter Exponential Distribution (OPED) family.","In the literature, several criteria have been proposed to evaluate the performance of such algorithms, including Asymptotic Optimality (A.O.), Minimax Optimality (M.O.), Sub-UCB, and variance-adaptive worst-case regret bound.","Thompson Sampling (TS)-based and Upper Confidence","Bound","(UCB)-based algorithms have been employed to achieve some of these criteria.","However, none of these algorithms simultaneously satisfy all the aforementioned criteria.   ","In this paper, we design an algorithm, Exponential Kullback-Leibler Maillard Sampling (abbrev.","\\expklms), that can achieve multiple optimality criteria simultaneously, including A.O., M.O. with a logarithmic factor, Sub-UCB, and variance-adaptive worst-case regret bound."],"url":"http://arxiv.org/abs/2502.14379v1"}
{"created":"2025-02-20 09:06:44","title":"A Similarity Paradigm Through Textual Regularization Without Forgetting","abstract":"Prompt learning has emerged as a promising method for adapting pre-trained visual-language models (VLMs) to a range of downstream tasks. While optimizing the context can be effective for improving performance on specific tasks, it can often lead to poor generalization performance on unseen classes or datasets sampled from different distributions. It may be attributed to the fact that textual prompts tend to overfit downstream data distributions, leading to the forgetting of generalized knowledge derived from hand-crafted prompts. In this paper, we propose a novel method called Similarity Paradigm with Textual Regularization (SPTR) for prompt learning without forgetting. SPTR is a two-pronged design based on hand-crafted prompts that is an inseparable framework. 1) To avoid forgetting general textual knowledge, we introduce the optimal transport as a textual regularization to finely ensure approximation with hand-crafted features and tuning textual features. 2) In order to continuously unleash the general ability of multiple hand-crafted prompts, we propose a similarity paradigm for natural alignment score and adversarial alignment score to improve model robustness for generalization. Both modules share a common objective in addressing generalization issues, aiming to maximize the generalization capability derived from multiple hand-crafted prompts. Four representative tasks (i.e., non-generalization few-shot learning, base-to-novel generalization, cross-dataset generalization, domain generalization) across 11 datasets demonstrate that SPTR outperforms existing prompt learning methods.","sentences":["Prompt learning has emerged as a promising method for adapting pre-trained visual-language models (VLMs) to a range of downstream tasks.","While optimizing the context can be effective for improving performance on specific tasks, it can often lead to poor generalization performance on unseen classes or datasets sampled from different distributions.","It may be attributed to the fact that textual prompts tend to overfit downstream data distributions, leading to the forgetting of generalized knowledge derived from hand-crafted prompts.","In this paper, we propose a novel method called Similarity Paradigm with Textual Regularization (SPTR) for prompt learning without forgetting.","SPTR is a two-pronged design based on hand-crafted prompts that is an inseparable framework.","1) To avoid forgetting general textual knowledge, we introduce the optimal transport as a textual regularization to finely ensure approximation with hand-crafted features and tuning textual features.","2) In order to continuously unleash the general ability of multiple hand-crafted prompts, we propose a similarity paradigm for natural alignment score and adversarial alignment score to improve model robustness for generalization.","Both modules share a common objective in addressing generalization issues, aiming to maximize the generalization capability derived from multiple hand-crafted prompts.","Four representative tasks (i.e., non-generalization few-shot learning, base-to-novel generalization, cross-dataset generalization, domain generalization) across 11 datasets demonstrate that SPTR outperforms existing prompt learning methods."],"url":"http://arxiv.org/abs/2502.14376v1"}
{"created":"2025-02-20 09:05:55","title":"VFL-RPS: Relevant Participant Selection in Vertical Federated Learning","abstract":"Federated Learning (FL) allows collaboration between different parties, while ensuring that the data across these parties is not shared. However, not every collaboration is helpful in terms of the resulting model performance. Therefore, it is an important challenge to select the correct participants in a collaboration. As it currently stands, most of the efforts in participant selection in the literature have focused on Horizontal Federated Learning (HFL), which assumes that all features are the same across all participants, disregarding the possibility of different features across participants which is captured in Vertical Federated Learning (VFL). To close this gap in the literature, we propose a novel method VFL-RPS for participant selection in VFL, as a pre-training step. We have tested our method on several data sets performing both regression and classification tasks, showing that our method leads to comparable results as using all data by only selecting a few participants. In addition, we show that our method outperforms existing methods for participant selection in VFL.","sentences":["Federated Learning (FL) allows collaboration between different parties, while ensuring that the data across these parties is not shared.","However, not every collaboration is helpful in terms of the resulting model performance.","Therefore, it is an important challenge to select the correct participants in a collaboration.","As it currently stands, most of the efforts in participant selection in the literature have focused on Horizontal Federated Learning (HFL), which assumes that all features are the same across all participants, disregarding the possibility of different features across participants which is captured in Vertical Federated Learning (VFL).","To close this gap in the literature, we propose a novel method VFL-RPS for participant selection in VFL, as a pre-training step.","We have tested our method on several data sets performing both regression and classification tasks, showing that our method leads to comparable results as using all data by only selecting a few participants.","In addition, we show that our method outperforms existing methods for participant selection in VFL."],"url":"http://arxiv.org/abs/2502.14375v1"}
{"created":"2025-02-20 09:05:35","title":"CrossVTON: Mimicking the Logic Reasoning on Cross-category Virtual Try-on guided by Tri-zone Priors","abstract":"Despite remarkable progress in image-based virtual try-on systems, generating realistic and robust fitting images for cross-category virtual try-on remains a challenging task. The primary difficulty arises from the absence of human-like reasoning, which involves addressing size mismatches between garments and models while recognizing and leveraging the distinct functionalities of various regions within the model images. To address this issue, we draw inspiration from human cognitive processes and disentangle the complex reasoning required for cross-category try-on into a structured framework. This framework systematically decomposes the model image into three distinct regions: try-on, reconstruction, and imagination zones. Each zone plays a specific role in accommodating the garment and facilitating realistic synthesis. To endow the model with robust reasoning capabilities for cross-category scenarios, we propose an iterative data constructor. This constructor encompasses diverse scenarios, including intra-category try-on, any-to-dress transformations (replacing any garment category with a dress), and dress-to-any transformations (replacing a dress with another garment category). Utilizing the generated dataset, we introduce a tri-zone priors generator that intelligently predicts the try-on, reconstruction, and imagination zones by analyzing how the input garment is expected to align with the model image. Guided by these tri-zone priors, our proposed method, CrossVTON, achieves state-of-the-art performance, surpassing existing baselines in both qualitative and quantitative evaluations. Notably, it demonstrates superior capability in handling cross-category virtual try-on, meeting the complex demands of real-world applications.","sentences":["Despite remarkable progress in image-based virtual try-on systems, generating realistic and robust fitting images for cross-category virtual try-on remains a challenging task.","The primary difficulty arises from the absence of human-like reasoning, which involves addressing size mismatches between garments and models while recognizing and leveraging the distinct functionalities of various regions within the model images.","To address this issue, we draw inspiration from human cognitive processes and disentangle the complex reasoning required for cross-category try-on into a structured framework.","This framework systematically decomposes the model image into three distinct regions: try-on, reconstruction, and imagination zones.","Each zone plays a specific role in accommodating the garment and facilitating realistic synthesis.","To endow the model with robust reasoning capabilities for cross-category scenarios, we propose an iterative data constructor.","This constructor encompasses diverse scenarios, including intra-category try-on, any-to-dress transformations (replacing any garment category with a dress), and dress-to-any transformations (replacing a dress with another garment category).","Utilizing the generated dataset, we introduce a tri-zone priors generator that intelligently predicts the try-on, reconstruction, and imagination zones by analyzing how the input garment is expected to align with the model image.","Guided by these tri-zone priors, our proposed method, CrossVTON, achieves state-of-the-art performance, surpassing existing baselines in both qualitative and quantitative evaluations.","Notably, it demonstrates superior capability in handling cross-category virtual try-on, meeting the complex demands of real-world applications."],"url":"http://arxiv.org/abs/2502.14373v1"}
{"created":"2025-02-20 08:57:45","title":"PPO-MI: Efficient Black-Box Model Inversion via Proximal Policy Optimization","abstract":"Model inversion attacks pose a significant privacy risk by attempting to reconstruct private training data from trained models. Most of the existing methods either depend on gradient estimation or require white-box access to model parameters, which limits their applicability in practical scenarios. In this paper, we propose PPO-MI, a novel reinforcement learning-based framework for black-box model inversion attacks. Our approach formulates the inversion task as a Markov Decision Process, where an agent navigates the latent space of a generative model to reconstruct private training samples using only model predictions. By employing Proximal Policy Optimization (PPO) with a momentum-based state transition mechanism, along with a reward function balancing prediction accuracy and exploration, PPO-MI ensures efficient latent space exploration and high query efficiency. We conduct extensive experiments illustrates that PPO-MI outperforms the existing methods while require less attack knowledge, and it is robust across various model architectures and datasets. These results underline its effectiveness and generalizability in practical black-box scenarios, raising important considerations for the privacy vulnerabilities of deployed machine learning models.","sentences":["Model inversion attacks pose a significant privacy risk by attempting to reconstruct private training data from trained models.","Most of the existing methods either depend on gradient estimation or require white-box access to model parameters, which limits their applicability in practical scenarios.","In this paper, we propose PPO-MI, a novel reinforcement learning-based framework for black-box model inversion attacks.","Our approach formulates the inversion task as a Markov Decision Process, where an agent navigates the latent space of a generative model to reconstruct private training samples using only model predictions.","By employing Proximal Policy Optimization (PPO) with a momentum-based state transition mechanism, along with a reward function balancing prediction accuracy and exploration, PPO-MI ensures efficient latent space exploration and high query efficiency.","We conduct extensive experiments illustrates that PPO-MI outperforms the existing methods while require less attack knowledge, and it is robust across various model architectures and datasets.","These results underline its effectiveness and generalizability in practical black-box scenarios, raising important considerations for the privacy vulnerabilities of deployed machine learning models."],"url":"http://arxiv.org/abs/2502.14370v1"}
{"created":"2025-02-20 08:40:09","title":"Retrieval-Augmented Process Reward Model for Generalizable Mathematical Reasoning","abstract":"While large language models (LLMs) have significantly advanced mathematical reasoning, Process Reward Models (PRMs) have been developed to evaluate the logical validity of reasoning steps. However, PRMs still struggle with out-of-distribution (OOD) challenges. This paper identifies key OOD issues, including step OOD, caused by differences in reasoning patterns across model types and sizes, and question OOD, which arises from dataset shifts between training data and real-world problems. To address these issues, we introduce Retrieval-Augmented Process Reward Model (RetrievalPRM), a novel framework designed to tackle these OOD issues. By utilizing a two-stage retrieval-enhanced mechanism, RetrievalPRM retrieves semantically similar questions and steps as a warmup, enhancing PRM's ability to evaluate target steps and improving generalization and reasoning consistency across different models and problem types. Our extensive experiments demonstrate that RetrievalPRM outperforms existing baselines across multiple real-world datasets. Our open-source contributions include a retrieval-enhanced dataset, a tuning framework for PRM training, and the RetrievalPRM model, establishing a new standard for PRM performance.","sentences":["While large language models (LLMs) have significantly advanced mathematical reasoning, Process Reward Models (PRMs) have been developed to evaluate the logical validity of reasoning steps.","However, PRMs still struggle with out-of-distribution (OOD) challenges.","This paper identifies key OOD issues, including step OOD, caused by differences in reasoning patterns across model types and sizes, and question OOD, which arises from dataset shifts between training data and real-world problems.","To address these issues, we introduce Retrieval-Augmented Process Reward Model (RetrievalPRM), a novel framework designed to tackle these OOD issues.","By utilizing a two-stage retrieval-enhanced mechanism, RetrievalPRM retrieves semantically similar questions and steps as a warmup, enhancing PRM's ability to evaluate target steps and improving generalization and reasoning consistency across different models and problem types.","Our extensive experiments demonstrate that RetrievalPRM outperforms existing baselines across multiple real-world datasets.","Our open-source contributions include a retrieval-enhanced dataset, a tuning framework for PRM training, and the RetrievalPRM model, establishing a new standard for PRM performance."],"url":"http://arxiv.org/abs/2502.14361v1"}
{"created":"2025-02-20 08:28:01","title":"Triply Laplacian Scale Mixture Modeling for Seismic Data Noise Suppression","abstract":"Sparsity-based tensor recovery methods have shown great potential in suppressing seismic data noise. These methods exploit tensor sparsity measures capturing the low-dimensional structures inherent in seismic data tensors to remove noise by applying sparsity constraints through soft-thresholding or hard-thresholding operators. However, in these methods, considering that real seismic data are non-stationary and affected by noise, the variances of tensor coefficients are unknown and may be difficult to accurately estimate from the degraded seismic data, leading to undesirable noise suppression performance. In this paper, we propose a novel triply Laplacian scale mixture (TLSM) approach for seismic data noise suppression, which significantly improves the estimation accuracy of both the sparse tensor coefficients and hidden scalar parameters. To make the optimization problem manageable, an alternating direction method of multipliers (ADMM) algorithm is employed to solve the proposed TLSM-based seismic data noise suppression problem. Extensive experimental results on synthetic and field seismic data demonstrate that the proposed TLSM algorithm outperforms many state-of-the-art seismic data noise suppression methods in both quantitative and qualitative evaluations while providing exceptional computational efficiency.","sentences":["Sparsity-based tensor recovery methods have shown great potential in suppressing seismic data noise.","These methods exploit tensor sparsity measures capturing the low-dimensional structures inherent in seismic data tensors to remove noise by applying sparsity constraints through soft-thresholding or hard-thresholding operators.","However, in these methods, considering that real seismic data are non-stationary and affected by noise, the variances of tensor coefficients are unknown and may be difficult to accurately estimate from the degraded seismic data, leading to undesirable noise suppression performance.","In this paper, we propose a novel triply Laplacian scale mixture (TLSM) approach for seismic data noise suppression, which significantly improves the estimation accuracy of both the sparse tensor coefficients and hidden scalar parameters.","To make the optimization problem manageable, an alternating direction method of multipliers (ADMM) algorithm is employed to solve the proposed TLSM-based seismic data noise suppression problem.","Extensive experimental results on synthetic and field seismic data demonstrate that the proposed TLSM algorithm outperforms many state-of-the-art seismic data noise suppression methods in both quantitative and qualitative evaluations while providing exceptional computational efficiency."],"url":"http://arxiv.org/abs/2502.14355v1"}
{"created":"2025-02-20 08:27:00","title":"Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts in Multi-Objective Alignment","abstract":"Multi-Objective Alignment (MOA) aims to align LLMs' responses with multiple human preference objectives, with Direct Preference Optimization (DPO) emerging as a prominent approach. However, we find that DPO-based MOA approaches suffer from widespread preference conflicts in the data, where different objectives favor different responses. This results in conflicting optimization directions, hindering the optimization on the Pareto Front. To address this, we propose to construct Pareto-optimal responses to resolve preference conflicts. To efficiently obtain and utilize such responses, we propose a self-improving DPO framework that enables LLMs to self-generate and select Pareto-optimal responses for self-supervised preference alignment. Extensive experiments on two datasets demonstrate the superior Pareto Front achieved by our framework compared to various baselines. Code is available at \\url{https://github.com/zyttt-coder/SIPO}.","sentences":["Multi-Objective Alignment (MOA) aims to align LLMs' responses with multiple human preference objectives, with Direct Preference Optimization (DPO) emerging as a prominent approach.","However, we find that DPO-based MOA approaches suffer from widespread preference conflicts in the data, where different objectives favor different responses.","This results in conflicting optimization directions, hindering the optimization on the Pareto Front.","To address this, we propose to construct Pareto-optimal responses to resolve preference conflicts.","To efficiently obtain and utilize such responses, we propose a self-improving DPO framework that enables LLMs to self-generate and select Pareto-optimal responses for self-supervised preference alignment.","Extensive experiments on two datasets demonstrate the superior Pareto Front achieved by our framework compared to various baselines.","Code is available at \\url{https://github.com/zyttt-coder/SIPO}."],"url":"http://arxiv.org/abs/2502.14354v1"}
{"created":"2025-02-20 08:17:56","title":"SR-LLM: Rethinking the Structured Representation in Large Language Model","abstract":"Structured representations, exemplified by Abstract Meaning Representation (AMR), have long been pivotal in computational linguistics. However, their role remains ambiguous in the Large Language Models (LLMs) era. Initial attempts to integrate structured representation into LLMs via a zero-shot setting yielded inferior performance. We hypothesize that such a decline stems from the structure information being passed into LLMs in a code format unfamiliar to LLMs' training corpora. Consequently, we propose SR-LLM, an innovative framework with two settings to explore a superior way of integrating structured representation with LLMs from training-free and training-dependent perspectives. The former integrates structural information through natural language descriptions in LLM prompts, whereas its counterpart augments the model's inference capability through fine-tuning on linguistically described structured representations. Performance improvements were observed in widely downstream datasets, with particularly notable gains of 3.17% and 12.38% in PAWS. To the best of our knowledge, this work represents the pioneering demonstration that leveraging structural representations can substantially enhance LLMs' inference capability. We hope that our work sheds light and encourages future research to enhance the reasoning and interoperability of LLMs by structure data.","sentences":["Structured representations, exemplified by Abstract Meaning Representation (AMR), have long been pivotal in computational linguistics.","However, their role remains ambiguous in the Large Language Models (LLMs) era.","Initial attempts to integrate structured representation into LLMs via a zero-shot setting yielded inferior performance.","We hypothesize that such a decline stems from the structure information being passed into LLMs in a code format unfamiliar to LLMs' training corpora.","Consequently, we propose SR-LLM, an innovative framework with two settings to explore a superior way of integrating structured representation with LLMs from training-free and training-dependent perspectives.","The former integrates structural information through natural language descriptions in LLM prompts, whereas its counterpart augments the model's inference capability through fine-tuning on linguistically described structured representations.","Performance improvements were observed in widely downstream datasets, with particularly notable gains of 3.17% and 12.38% in PAWS.","To the best of our knowledge, this work represents the pioneering demonstration that leveraging structural representations can substantially enhance LLMs' inference capability.","We hope that our work sheds light and encourages future research to enhance the reasoning and interoperability of LLMs by structure data."],"url":"http://arxiv.org/abs/2502.14352v1"}
{"created":"2025-02-20 08:17:13","title":"SegAnyPET: Universal Promptable Segmentation from Positron Emission Tomography Images","abstract":"Positron Emission Tomography (PET) imaging plays a crucial role in modern medical diagnostics by revealing the metabolic processes within a patient's body, which is essential for quantification of therapy response and monitoring treatment progress. However, the segmentation of PET images presents unique challenges due to their lower contrast and less distinct boundaries compared to other structural medical modalities. Recent developments in segmentation foundation models have shown superior versatility across diverse natural image segmentation tasks. Despite the efforts of medical adaptations, these works primarily focus on structural medical images with detailed physiological structural information and exhibit poor generalization ability when adapted to molecular PET imaging. In this paper, we collect and construct PETS-5k, the largest PET segmentation dataset to date, comprising 5,731 three-dimensional whole-body PET images and encompassing over 1.3M 2D images. Based on the established dataset, we develop SegAnyPET, a modality-specific 3D foundation model for universal promptable segmentation from PET images. To issue the challenge of discrepant annotation quality of PET images, we adopt a cross prompting confident learning (CPCL) strategy with an uncertainty-guided self-rectification process to robustly learn segmentation from high-quality labeled data and low-quality noisy labeled data. Experimental results demonstrate that SegAnyPET can correctly segment seen and unseen targets using only one or a few prompt points, outperforming state-of-the-art foundation models and task-specific fully supervised models with higher accuracy and strong generalization ability for universal segmentation. As the first foundation model for PET images, we believe that SegAnyPET will advance the applications to various downstream tasks for molecular imaging.","sentences":["Positron Emission Tomography (PET) imaging plays a crucial role in modern medical diagnostics by revealing the metabolic processes within a patient's body, which is essential for quantification of therapy response and monitoring treatment progress.","However, the segmentation of PET images presents unique challenges due to their lower contrast and less distinct boundaries compared to other structural medical modalities.","Recent developments in segmentation foundation models have shown superior versatility across diverse natural image segmentation tasks.","Despite the efforts of medical adaptations, these works primarily focus on structural medical images with detailed physiological structural information and exhibit poor generalization ability when adapted to molecular PET imaging.","In this paper, we collect and construct PETS-5k, the largest PET segmentation dataset to date, comprising 5,731 three-dimensional whole-body PET images and encompassing over 1.3M 2D images.","Based on the established dataset, we develop SegAnyPET, a modality-specific 3D foundation model for universal promptable segmentation from PET images.","To issue the challenge of discrepant annotation quality of PET images, we adopt a cross prompting confident learning (CPCL) strategy with an uncertainty-guided self-rectification process to robustly learn segmentation from high-quality labeled data and low-quality noisy labeled data.","Experimental results demonstrate that SegAnyPET can correctly segment seen and unseen targets using only one or a few prompt points, outperforming state-of-the-art foundation models and task-specific fully supervised models with higher accuracy and strong generalization ability for universal segmentation.","As the first foundation model for PET images, we believe that SegAnyPET will advance the applications to various downstream tasks for molecular imaging."],"url":"http://arxiv.org/abs/2502.14351v1"}
{"created":"2025-02-20 07:53:11","title":"Earlier Tokens Contribute More: Learning Direct Preference Optimization From Temporal Decay Perspective","abstract":"Direct Preference Optimization (DPO) has gained attention as an efficient alternative to reinforcement learning from human feedback (RLHF) for aligning large language models (LLMs) with human preferences. Despite its advantages, DPO suffers from a length bias, generating responses longer than those from the reference model. Existing solutions like SimPO and SamPO address this issue but uniformly treat the contribution of rewards across sequences, overlooking temporal dynamics. To this end, we propose an enhanced preference optimization method that incorporates a temporal decay factor controlled by a gamma parameter. This dynamic weighting mechanism adjusts the influence of each reward based on its position in the sequence, prioritizing earlier tokens that are more critical for alignment. By adaptively focusing on more relevant feedback, our approach mitigates overfitting to less pertinent data and remains responsive to evolving human preferences. Experimental results on several benchmarks show that our approach consistently outperforms vanilla DPO by 5.9-8.8 points on AlpacaEval 2 and 3.3-9.7 points on Arena-Hard across different model architectures and sizes. Furthermore, additional experiments on mathematical and reasoning benchmarks (MMLU, GSM8K, and MATH) confirm that our method enhances performance without compromising general capabilities. Our codebase would be available at \\url{https://github.com/LotuSrc/D2PO}.","sentences":["Direct Preference Optimization (DPO) has gained attention as an efficient alternative to reinforcement learning from human feedback (RLHF) for aligning large language models (LLMs) with human preferences.","Despite its advantages, DPO suffers from a length bias, generating responses longer than those from the reference model.","Existing solutions like SimPO and SamPO address this issue but uniformly treat the contribution of rewards across sequences, overlooking temporal dynamics.","To this end, we propose an enhanced preference optimization method that incorporates a temporal decay factor controlled by a gamma parameter.","This dynamic weighting mechanism adjusts the influence of each reward based on its position in the sequence, prioritizing earlier tokens that are more critical for alignment.","By adaptively focusing on more relevant feedback, our approach mitigates overfitting to less pertinent data and remains responsive to evolving human preferences.","Experimental results on several benchmarks show that our approach consistently outperforms vanilla DPO by 5.9-8.8 points on AlpacaEval 2 and 3.3-9.7 points on Arena-Hard across different model architectures and sizes.","Furthermore, additional experiments on mathematical and reasoning benchmarks (MMLU, GSM8K, and MATH) confirm that our method enhances performance without compromising general capabilities.","Our codebase would be available at \\url{https://github.com/LotuSrc/D2PO}."],"url":"http://arxiv.org/abs/2502.14340v1"}
{"created":"2025-02-20 07:47:03","title":"English Please: Evaluating Machine Translation for Multilingual Bug Reports","abstract":"Accurate translation of bug reports is critical for efficient collaboration in global software development. In this study, we conduct the first comprehensive evaluation of machine translation (MT) performance on bug reports, analyzing the capabilities of DeepL, AWS Translate, and ChatGPT using data from the Visual Studio Code GitHub repository, specifically focusing on reports labeled with the english-please tag. To thoroughly assess the accuracy and effectiveness of each system, we employ multiple machine translation metrics, including BLEU, BERTScore, COMET, METEOR, and ROUGE. Our findings indicate that DeepL consistently outperforms the other systems across most automatic metrics, demonstrating strong lexical and semantic alignment. AWS Translate performs competitively, particularly in METEOR, while ChatGPT lags in key metrics. This study underscores the importance of domain adaptation for translating technical texts and offers guidance for integrating automated translation into bug-triaging workflows. Moreover, our results establish a foundation for future research to refine machine translation solutions for specialized engineering contexts. The code and dataset for this paper are available at GitHub: https://github.com/av9ash/gitbugs/tree/main/multilingual.","sentences":["Accurate translation of bug reports is critical for efficient collaboration in global software development.","In this study, we conduct the first comprehensive evaluation of machine translation (MT) performance on bug reports, analyzing the capabilities of DeepL, AWS Translate, and ChatGPT using data from the Visual Studio Code GitHub repository, specifically focusing on reports labeled with the english-please tag.","To thoroughly assess the accuracy and effectiveness of each system, we employ multiple machine translation metrics, including BLEU, BERTScore, COMET, METEOR, and ROUGE.","Our findings indicate that DeepL consistently outperforms the other systems across most automatic metrics, demonstrating strong lexical and semantic alignment.","AWS Translate performs competitively, particularly in METEOR, while ChatGPT lags in key metrics.","This study underscores the importance of domain adaptation for translating technical texts and offers guidance for integrating automated translation into bug-triaging workflows.","Moreover, our results establish a foundation for future research to refine machine translation solutions for specialized engineering contexts.","The code and dataset for this paper are available at GitHub: https://github.com/av9ash/gitbugs/tree/main/multilingual."],"url":"http://arxiv.org/abs/2502.14338v1"}
{"created":"2025-02-20 07:44:04","title":"Information Types in Product Reviews","abstract":"Information in text is communicated in a way that supports a goal for its reader. Product reviews, for example, contain opinions, tips, product descriptions, and many other types of information that provide both direct insights, as well as unexpected signals for downstream applications. We devise a typology of 24 communicative goals in sentences from the product review domain, and employ a zero-shot multi-label classifier that facilitates large-scale analyses of review data. In our experiments, we find that the combination of classes in the typology forecasts helpfulness and sentiment of reviews, while supplying explanations for these decisions. In addition, our typology enables analysis of review intent, effectiveness and rhetorical structure. Characterizing the types of information in reviews unlocks many opportunities for more effective consumption of this genre.","sentences":["Information in text is communicated in a way that supports a goal for its reader.","Product reviews, for example, contain opinions, tips, product descriptions, and many other types of information that provide both direct insights, as well as unexpected signals for downstream applications.","We devise a typology of 24 communicative goals in sentences from the product review domain, and employ a zero-shot multi-label classifier that facilitates large-scale analyses of review data.","In our experiments, we find that the combination of classes in the typology forecasts helpfulness and sentiment of reviews, while supplying explanations for these decisions.","In addition, our typology enables analysis of review intent, effectiveness and rhetorical structure.","Characterizing the types of information in reviews unlocks many opportunities for more effective consumption of this genre."],"url":"http://arxiv.org/abs/2502.14335v1"}
{"created":"2025-02-20 06:25:30","title":"Generalization Certificates for Adversarially Robust Bayesian Linear Regression","abstract":"Adversarial robustness of machine learning models is critical to ensuring reliable performance under data perturbations. Recent progress has been on point estimators, and this paper considers distributional predictors. First, using the link between exponential families and Bregman divergences, we formulate an adversarial Bregman divergence loss as an adversarial negative log-likelihood. Using the geometric properties of Bregman divergences, we compute the adversarial perturbation for such models in closed-form. Second, under such losses, we introduce \\emph{adversarially robust posteriors}, by exploiting the optimization-centric view of generalized Bayesian inference. Third, we derive the \\emph{first} rigorous generalization certificates in the context of an adversarial extension of Bayesian linear regression by leveraging the PAC-Bayesian framework. Finally, experiments on real and synthetic datasets demonstrate the superior robustness of the derived adversarially robust posterior over Bayes posterior, and also validate our theoretical guarantees.","sentences":["Adversarial robustness of machine learning models is critical to ensuring reliable performance under data perturbations.","Recent progress has been on point estimators, and this paper considers distributional predictors.","First, using the link between exponential families and Bregman divergences, we formulate an adversarial Bregman divergence loss as an adversarial negative log-likelihood.","Using the geometric properties of Bregman divergences, we compute the adversarial perturbation for such models in closed-form.","Second, under such losses, we introduce \\emph{adversarially robust posteriors}, by exploiting the optimization-centric view of generalized Bayesian inference.","Third, we derive the \\emph{first} rigorous generalization certificates in the context of an adversarial extension of Bayesian linear regression by leveraging the PAC-Bayesian framework.","Finally, experiments on real and synthetic datasets demonstrate the superior robustness of the derived adversarially robust posterior over Bayes posterior, and also validate our theoretical guarantees."],"url":"http://arxiv.org/abs/2502.14298v1"}
{"created":"2025-02-20 06:14:07","title":"Graph Anomaly Detection via Adaptive Test-time Representation Learning across Out-of-Distribution Domains","abstract":"Graph Anomaly Detection (GAD) has demonstrated great effectiveness in identifying unusual patterns within graph-structured data. However, while labeled anomalies are often scarce in emerging applications, existing supervised GAD approaches are either ineffective or not applicable when moved across graph domains due to distribution shifts and heterogeneous feature spaces. To address these challenges, we present AdaGraph-T3, a novel test-time training framework for cross-domain GAD. AdaGraph-T3 combines supervised and self-supervised learning during training while adapting to a new domain during test time using only self-supervised learning by leveraging a homophily-based affinity score that captures domain-invariant properties of anomalies. Our framework introduces four key innovations to cross-domain GAD: an effective self-supervision scheme, an attention-based mechanism that dynamically learns edge importance weights during message passing, domain-specific encoders for handling heterogeneous features, and class-aware regularization to address imbalance. Experiments across multiple cross-domain settings demonstrate that AdaGraph-T3 significantly outperforms existing approaches, achieving average improvements of over 6.6% in AUROC and 7.9% in AUPRC compared to the best competing model.","sentences":["Graph Anomaly Detection (GAD) has demonstrated great effectiveness in identifying unusual patterns within graph-structured data.","However, while labeled anomalies are often scarce in emerging applications, existing supervised GAD approaches are either ineffective or not applicable when moved across graph domains due to distribution shifts and heterogeneous feature spaces.","To address these challenges, we present AdaGraph-T3, a novel test-time training framework for cross-domain GAD.","AdaGraph-T3 combines supervised and self-supervised learning during training while adapting to a new domain during test time using only self-supervised learning by leveraging a homophily-based affinity score that captures domain-invariant properties of anomalies.","Our framework introduces four key innovations to cross-domain GAD: an effective self-supervision scheme, an attention-based mechanism that dynamically learns edge importance weights during message passing, domain-specific encoders for handling heterogeneous features, and class-aware regularization to address imbalance.","Experiments across multiple cross-domain settings demonstrate that AdaGraph-T3 significantly outperforms existing approaches, achieving average improvements of over 6.6% in AUROC and 7.9% in AUPRC compared to the best competing model."],"url":"http://arxiv.org/abs/2502.14293v1"}
{"created":"2025-02-20 06:07:04","title":"A Note on Efficient Privacy-Preserving Similarity Search for Encrypted Vectors","abstract":"Traditional approaches to vector similarity search over encrypted data rely on fully homomorphic encryption (FHE) to enable computation without decryption. However, the substantial computational overhead of FHE makes it impractical for large-scale real-time applications. This work explores a more efficient alternative: using additively homomorphic encryption (AHE) for privacy-preserving similarity search. We consider scenarios where either the query vector or the database vectors remain encrypted, a setting that frequently arises in applications such as confidential recommender systems and secure federated learning. While AHE only supports addition and scalar multiplication, we show that it is sufficient to compute inner product similarity--one of the most widely used similarity measures in vector retrieval. Compared to FHE-based solutions, our approach significantly reduces computational overhead by avoiding ciphertext-ciphertext multiplications and bootstrapping, while still preserving correctness and privacy. We present an efficient algorithm for encrypted similarity search under AHE and analyze its error growth and security implications. Our method provides a scalable and practical solution for privacy-preserving vector search in real-world machine learning applications.","sentences":["Traditional approaches to vector similarity search over encrypted data rely on fully homomorphic encryption (FHE) to enable computation without decryption.","However, the substantial computational overhead of FHE makes it impractical for large-scale real-time applications.","This work explores a more efficient alternative: using additively homomorphic encryption (AHE) for privacy-preserving similarity search.","We consider scenarios where either the query vector or the database vectors remain encrypted, a setting that frequently arises in applications such as confidential recommender systems and secure federated learning.","While AHE only supports addition and scalar multiplication, we show that it is sufficient to compute inner product similarity--one of the most widely used similarity measures in vector retrieval.","Compared to FHE-based solutions, our approach significantly reduces computational overhead by avoiding ciphertext-ciphertext multiplications and bootstrapping, while still preserving correctness and privacy.","We present an efficient algorithm for encrypted similarity search under AHE and analyze its error growth and security implications.","Our method provides a scalable and practical solution for privacy-preserving vector search in real-world machine learning applications."],"url":"http://arxiv.org/abs/2502.14291v1"}
{"created":"2025-02-20 05:41:52","title":"Correcting Noisy Multilabel Predictions: Modeling Label Noise through Latent Space Shifts","abstract":"Noise in data appears to be inevitable in most real-world machine learning applications and would cause severe overfitting problems. Not only can data features contain noise, but labels are also prone to be noisy due to human input. In this paper, rather than noisy label learning in multiclass classifications, we instead focus on the less explored area of noisy label learning for multilabel classifications. Specifically, we investigate the post-correction of predictions generated from classifiers learned with noisy labels. The reasons are two-fold. Firstly, this approach can directly work with the trained models to save computational resources. Secondly, it could be applied on top of other noisy label correction techniques to achieve further improvements. To handle this problem, we appeal to deep generative approaches that are possible for uncertainty estimation. Our model posits that label noise arises from a stochastic shift in the latent variable, providing a more robust and beneficial means for noisy learning. We develop both unsupervised and semi-supervised learning methods for our model. The extensive empirical study presents solid evidence to that our approach is able to consistently improve the independent models and performs better than a number of existing methods across various noisy label settings. Moreover, a comprehensive empirical analysis of the proposed method is carried out to validate its robustness, including sensitivity analysis and an ablation study, among other elements.","sentences":["Noise in data appears to be inevitable in most real-world machine learning applications and would cause severe overfitting problems.","Not only can data features contain noise, but labels are also prone to be noisy due to human input.","In this paper, rather than noisy label learning in multiclass classifications, we instead focus on the less explored area of noisy label learning for multilabel classifications.","Specifically, we investigate the post-correction of predictions generated from classifiers learned with noisy labels.","The reasons are two-fold.","Firstly, this approach can directly work with the trained models to save computational resources.","Secondly, it could be applied on top of other noisy label correction techniques to achieve further improvements.","To handle this problem, we appeal to deep generative approaches that are possible for uncertainty estimation.","Our model posits that label noise arises from a stochastic shift in the latent variable, providing a more robust and beneficial means for noisy learning.","We develop both unsupervised and semi-supervised learning methods for our model.","The extensive empirical study presents solid evidence to that our approach is able to consistently improve the independent models and performs better than a number of existing methods across various noisy label settings.","Moreover, a comprehensive empirical analysis of the proposed method is carried out to validate its robustness, including sensitivity analysis and an ablation study, among other elements."],"url":"http://arxiv.org/abs/2502.14281v1"}
{"created":"2025-02-20 05:28:44","title":"STeCa: Step-level Trajectory Calibration for LLM Agent Learning","abstract":"Large language model (LLM)-based agents have shown promise in tackling complex tasks by interacting dynamically with the environment. Existing work primarily focuses on behavior cloning from expert demonstrations and preference learning through exploratory trajectory sampling. However, these methods often struggle in long-horizon tasks, where suboptimal actions accumulate step by step, causing agents to deviate from correct task trajectories. To address this, we highlight the importance of timely calibration and the need to automatically construct calibration trajectories for training agents. We propose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM agent learning. Specifically, STeCa identifies suboptimal actions through a step-level reward comparison during exploration. It constructs calibrated trajectories using LLM-driven reflection, enabling agents to learn from improved decision-making processes. These calibrated trajectories, together with successful trajectory data, are utilized for reinforced training. Extensive experiments demonstrate that STeCa significantly outperforms existing methods. Further analysis highlights that step-level calibration enables agents to complete tasks with greater robustness. Our code and data are available at https://github.com/WangHanLinHenry/STeCa.","sentences":["Large language model (LLM)-based agents have shown promise in tackling complex tasks by interacting dynamically with the environment.","Existing work primarily focuses on behavior cloning from expert demonstrations and preference learning through exploratory trajectory sampling.","However, these methods often struggle in long-horizon tasks, where suboptimal actions accumulate step by step, causing agents to deviate from correct task trajectories.","To address this, we highlight the importance of timely calibration and the need to automatically construct calibration trajectories for training agents.","We propose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM agent learning.","Specifically, STeCa identifies suboptimal actions through a step-level reward comparison during exploration.","It constructs calibrated trajectories using LLM-driven reflection, enabling agents to learn from improved decision-making processes.","These calibrated trajectories, together with successful trajectory data, are utilized for reinforced training.","Extensive experiments demonstrate that STeCa significantly outperforms existing methods.","Further analysis highlights that step-level calibration enables agents to complete tasks with greater robustness.","Our code and data are available at https://github.com/WangHanLinHenry/STeCa."],"url":"http://arxiv.org/abs/2502.14276v1"}
{"created":"2025-02-20 05:17:39","title":"Predicting Fetal Birthweight from High Dimensional Data using Advanced Machine Learning","abstract":"Birth weight serves as a fundamental indicator of neonatal health, closely linked to both early medical interventions and long-term developmental risks. Traditional predictive models, often constrained by limited feature selection and incomplete datasets, struggle to achieve overlooking complex maternal and fetal interactions in diverse clinical settings. This research explores machine learning to address these limitations, utilizing a structured methodology that integrates advanced imputation strategies, supervised feature selection techniques, and predictive modeling. Given the constraints of the dataset, the research strengthens the role of data preprocessing in improving the model performance. Among the various methodologies explored, tree-based feature selection methods demonstrated superior capability in identifying the most relevant predictors, while ensemble-based regression models proved highly effective in capturing non-linear relationships and complex maternal-fetal interactions within the data. Beyond model performance, the study highlights the clinical significance of key physiological determinants, offering insights into maternal and fetal health factors that influence birth weight, offering insights that extend over statistical modeling. By bridging computational intelligence with perinatal research, this work underscores the transformative role of machine learning in enhancing predictive accuracy, refining risk assessment and informing data-driven decision-making in maternal and neonatal care. Keywords: Birth weight prediction, maternal-fetal health, MICE, BART, Gradient Boosting, neonatal outcomes, Clinipredictive.","sentences":["Birth weight serves as a fundamental indicator of neonatal health, closely linked to both early medical interventions and long-term developmental risks.","Traditional predictive models, often constrained by limited feature selection and incomplete datasets, struggle to achieve overlooking complex maternal and fetal interactions in diverse clinical settings.","This research explores machine learning to address these limitations, utilizing a structured methodology that integrates advanced imputation strategies, supervised feature selection techniques, and predictive modeling.","Given the constraints of the dataset, the research strengthens the role of data preprocessing in improving the model performance.","Among the various methodologies explored, tree-based feature selection methods demonstrated superior capability in identifying the most relevant predictors, while ensemble-based regression models proved highly effective in capturing non-linear relationships and complex maternal-fetal interactions within the data.","Beyond model performance, the study highlights the clinical significance of key physiological determinants, offering insights into maternal and fetal health factors that influence birth weight, offering insights that extend over statistical modeling.","By bridging computational intelligence with perinatal research, this work underscores the transformative role of machine learning in enhancing predictive accuracy, refining risk assessment and informing data-driven decision-making in maternal and neonatal care.","Keywords: Birth weight prediction, maternal-fetal health, MICE, BART, Gradient Boosting, neonatal outcomes, Clinipredictive."],"url":"http://arxiv.org/abs/2502.14270v1"}
{"created":"2025-02-20 04:55:45","title":"LabTOP: A Unified Model for Lab Test Outcome Prediction on Electronic Health Records","abstract":"Lab tests are fundamental for diagnosing diseases and monitoring patient conditions. However, frequent testing can be burdensome for patients, and test results may not always be immediately available. To address these challenges, we propose LabTOP, a unified model that predicts lab test outcomes by leveraging a language modeling approach on EHR data. Unlike conventional methods that estimate only a subset of lab tests or classify discrete value ranges, LabTOP performs continuous numerical predictions for a diverse range of lab items. We evaluate LabTOP on three publicly available EHR datasets and demonstrate that it outperforms existing methods, including traditional machine learning models and state-of-the-art large language models. We also conduct extensive ablation studies to confirm the effectiveness of our design choices. We believe that LabTOP will serve as an accurate and generalizable framework for lab test outcome prediction, with potential applications in clinical decision support and early detection of critical conditions.","sentences":["Lab tests are fundamental for diagnosing diseases and monitoring patient conditions.","However, frequent testing can be burdensome for patients, and test results may not always be immediately available.","To address these challenges, we propose LabTOP, a unified model that predicts lab test outcomes by leveraging a language modeling approach on EHR data.","Unlike conventional methods that estimate only a subset of lab tests or classify discrete value ranges, LabTOP performs continuous numerical predictions for a diverse range of lab items.","We evaluate LabTOP on three publicly available EHR datasets and demonstrate that it outperforms existing methods, including traditional machine learning models and state-of-the-art large language models.","We also conduct extensive ablation studies to confirm the effectiveness of our design choices.","We believe that LabTOP will serve as an accurate and generalizable framework for lab test outcome prediction, with potential applications in clinical decision support and early detection of critical conditions."],"url":"http://arxiv.org/abs/2502.14259v1"}
{"created":"2025-02-20 03:42:42","title":"SleepGMUformer: A gated multimodal temporal neural network for sleep staging","abstract":"Sleep staging is a key method for assessing sleep quality and diagnosing sleep disorders. However, current deep learning methods face challenges: 1) postfusion techniques ignore the varying contributions of different modalities; 2) unprocessed sleep data can interfere with frequency-domain information. To tackle these issues, this paper proposes a gated multimodal temporal neural network for multidomain sleep data, including heart rate, motion, steps, EEG (Fpz-Cz, Pz-Oz), and EOG from WristHR-Motion-Sleep and SleepEDF-78. The model integrates: 1) a pre-processing module for feature alignment, missing value handling, and EEG de-trending; 2) a feature extraction module for complex sleep features in the time dimension; and 3) a dynamic fusion module for real-time modality weighting.Experiments show classification accuracies of 85.03% on SleepEDF-78 and 94.54% on WristHR-Motion-Sleep datasets. The model handles heterogeneous datasets and outperforms state-of-the-art models by 1.00%-4.00%.","sentences":["Sleep staging is a key method for assessing sleep quality and diagnosing sleep disorders.","However, current deep learning methods face challenges: 1) postfusion techniques ignore the varying contributions of different modalities; 2) unprocessed sleep data can interfere with frequency-domain information.","To tackle these issues, this paper proposes a gated multimodal temporal neural network for multidomain sleep data, including heart rate, motion, steps, EEG (Fpz-Cz, Pz-Oz), and EOG from WristHR-Motion-Sleep and SleepEDF-78.","The model integrates: 1) a pre-processing module for feature alignment, missing value handling, and EEG de-trending; 2) a feature extraction module for complex sleep features in the time dimension; and 3) a dynamic fusion module for real-time modality weighting.","Experiments show classification accuracies of 85.03% on SleepEDF-78 and 94.54% on WristHR-Motion-Sleep datasets.","The model handles heterogeneous datasets and outperforms state-of-the-art models by 1.00%-4.00%."],"url":"http://arxiv.org/abs/2502.14227v1"}
{"created":"2025-02-20 03:37:46","title":"Enhancing Pavement Sensor Data Acquisition for AI-Driven Transportation Research","abstract":"Effective strategies for sensor data management are essential for advancing transportation research, especially in the current data-driven era, due to the advent of novel applications in artificial intelligence. This paper presents comprehensive guidelines for managing transportation sensor data, encompassing both archived static data and real-time data streams. The real-time system architecture integrates various applications with data acquisition systems (DAQ). By deploying the in-house designed, open-source Avena software platform alongside the NATS messaging system as a secure communication broker, reliable data exchange is ensured. While robust databases like TimescaleDB facilitate organized storage, visualization platforms like Grafana provide real-time monitoring capabilities.   In contrast, static data standards address the challenges in handling unstructured, voluminous datasets. The standards advocate for a combination of cost-effective bulk cloud storage for unprocessed sensor data and relational databases for recording summarized analyses. They highlight the role of cloud data transfer tools like FME for efficient migration of sensor data from local storages onto the cloud. Further, integration of robust visualization tools into the framework helps in deriving patterns and trends from these complex datasets.   The proposals were applied to INDOT's real-world case studies involving the I-65 and I-69 Greenfield districts. For real-time data collection, Campbell Scientific DAQ systems were used, enabling continuous generation and monitoring of sensor metrics. In the case of the archived I-69 database, summary data was compiled in Oracle, while the unprocessed data was stored in SharePoint. The results underline the effectiveness of the proposed guidelines and motivate their adoption in research projects.","sentences":["Effective strategies for sensor data management are essential for advancing transportation research, especially in the current data-driven era, due to the advent of novel applications in artificial intelligence.","This paper presents comprehensive guidelines for managing transportation sensor data, encompassing both archived static data and real-time data streams.","The real-time system architecture integrates various applications with data acquisition systems (DAQ).","By deploying the in-house designed, open-source Avena software platform alongside the NATS messaging system as a secure communication broker, reliable data exchange is ensured.","While robust databases like TimescaleDB facilitate organized storage, visualization platforms like Grafana provide real-time monitoring capabilities.   ","In contrast, static data standards address the challenges in handling unstructured, voluminous datasets.","The standards advocate for a combination of cost-effective bulk cloud storage for unprocessed sensor data and relational databases for recording summarized analyses.","They highlight the role of cloud data transfer tools like FME for efficient migration of sensor data from local storages onto the cloud.","Further, integration of robust visualization tools into the framework helps in deriving patterns and trends from these complex datasets.   ","The proposals were applied to INDOT's real-world case studies involving the I-65 and I-69 Greenfield districts.","For real-time data collection, Campbell Scientific DAQ systems were used, enabling continuous generation and monitoring of sensor metrics.","In the case of the archived I-69 database, summary data was compiled in Oracle, while the unprocessed data was stored in SharePoint.","The results underline the effectiveness of the proposed guidelines and motivate their adoption in research projects."],"url":"http://arxiv.org/abs/2502.14222v1"}
{"created":"2025-02-20 03:36:12","title":"H3DE-Net: Efficient and Accurate 3D Landmark Detection in Medical Imaging","abstract":"3D landmark detection is a critical task in medical image analysis, and accurately detecting anatomical landmarks is essential for subsequent medical imaging tasks. However, mainstream deep learning methods in this field struggle to simultaneously capture fine-grained local features and model global spatial relationships, while maintaining a balance between accuracy and computational efficiency. Local feature extraction requires capturing fine-grained anatomical details, while global modeling requires understanding the spatial relationships within complex anatomical structures. The high-dimensional nature of 3D volume further exacerbates these challenges, as landmarks are sparsely distributed, leading to significant computational costs. Therefore, achieving efficient and precise 3D landmark detection remains a pressing challenge in medical image analysis.   In this work, We propose a \\textbf{H}ybrid \\textbf{3}D \\textbf{DE}tection \\textbf{Net}(H3DE-Net), a novel framework that combines CNNs for local feature extraction with a lightweight attention mechanism designed to efficiently capture global dependencies in 3D volumetric data. This mechanism employs a hierarchical routing strategy to reduce computational cost while maintaining global context modeling. To our knowledge, H3DE-Net is the first 3D landmark detection model that integrates such a lightweight attention mechanism with CNNs. Additionally, integrating multi-scale feature fusion further enhances detection accuracy and robustness. Experimental results on a public CT dataset demonstrate that H3DE-Net achieves state-of-the-art(SOTA) performance, significantly improving accuracy and robustness, particularly in scenarios with missing landmarks or complex anatomical variations. We aready open-source our project, including code, data and model weights.","sentences":["3D landmark detection is a critical task in medical image analysis, and accurately detecting anatomical landmarks is essential for subsequent medical imaging tasks.","However, mainstream deep learning methods in this field struggle to simultaneously capture fine-grained local features and model global spatial relationships, while maintaining a balance between accuracy and computational efficiency.","Local feature extraction requires capturing fine-grained anatomical details, while global modeling requires understanding the spatial relationships within complex anatomical structures.","The high-dimensional nature of 3D volume further exacerbates these challenges, as landmarks are sparsely distributed, leading to significant computational costs.","Therefore, achieving efficient and precise 3D landmark detection remains a pressing challenge in medical image analysis.   ","In this work, We propose a \\textbf{H}ybrid \\textbf{3}D \\textbf{DE}tection \\textbf{Net}(H3DE-Net), a novel framework that combines CNNs for local feature extraction with a lightweight attention mechanism designed to efficiently capture global dependencies in 3D volumetric data.","This mechanism employs a hierarchical routing strategy to reduce computational cost while maintaining global context modeling.","To our knowledge, H3DE-Net is the first 3D landmark detection model that integrates such a lightweight attention mechanism with CNNs.","Additionally, integrating multi-scale feature fusion further enhances detection accuracy and robustness.","Experimental results on a public CT dataset demonstrate that H3DE-Net achieves state-of-the-art(SOTA) performance, significantly improving accuracy and robustness, particularly in scenarios with missing landmarks or complex anatomical variations.","We aready open-source our project, including code, data and model weights."],"url":"http://arxiv.org/abs/2502.14221v1"}
{"created":"2025-02-20 03:27:00","title":"NDPage: Efficient Address Translation for Near-Data Processing Architectures via Tailored Page Table","abstract":"Near-Data Processing (NDP) has been a promising architectural paradigm to address the memory wall problem for data-intensive applications. Practical implementation of NDP architectures calls for system support for better programmability, where having virtual memory (VM) is critical. Modern computing systems incorporate a 4-level page table design to support address translation in VM. However, simply adopting an existing 4-level page table in NDP systems causes significant address translation overhead because (1) NDP applications generate a lot of address translations, and (2) the limited L1 cache in NDP systems cannot cover the accesses to page table entries (PTEs). We extensively analyze the 4-level page table design in the NDP scenario and observe that (1) the memory access to page table entries is highly irregular, thus cannot benefit from the L1 cache, and (2) the last two levels of page tables are nearly fully occupied. Based on our observations, we propose NDPage, an efficient page table design tailored for NDP systems. The key mechanisms of NDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates the memory accesses of PTEs but also prevents the pollution of PTEs in the cache system, and (2) a flattened page table design that merges the last two levels of page tables, allowing the page table to enjoy the flexibility of a 4KB page while reducing the number of PTE accesses. We evaluate NDPage using a variety of data-intensive workloads. Our evaluation shows that in a single-core NDP system, NDPage improves the end-to-end performance over the state-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core NDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively.","sentences":["Near-Data Processing (NDP) has been a promising architectural paradigm to address the memory wall problem for data-intensive applications.","Practical implementation of NDP architectures calls for system support for better programmability, where having virtual memory (VM) is critical.","Modern computing systems incorporate a 4-level page table design to support address translation in VM.","However, simply adopting an existing 4-level page table in NDP systems causes significant address translation overhead because (1) NDP applications generate a lot of address translations, and (2) the limited L1 cache in NDP systems cannot cover the accesses to page table entries (PTEs).","We extensively analyze the 4-level page table design in the NDP scenario and observe that (1) the memory access to page table entries is highly irregular, thus cannot benefit from the L1 cache, and (2) the last two levels of page tables are nearly fully occupied.","Based on our observations, we propose NDPage, an efficient page table design tailored for NDP systems.","The key mechanisms of NDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates the memory accesses of PTEs but also prevents the pollution of PTEs in the cache system, and (2) a flattened page table design that merges the last two levels of page tables, allowing the page table to enjoy the flexibility of a 4KB page while reducing the number of PTE accesses.","We evaluate NDPage using a variety of data-intensive workloads.","Our evaluation shows that in a single-core NDP system, NDPage improves the end-to-end performance over the state-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core NDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively."],"url":"http://arxiv.org/abs/2502.14220v1"}
{"created":"2025-02-20 03:07:56","title":"Towards Secure Program Partitioning for Smart Contracts with LLM's In-Context Learning","abstract":"Smart contracts are highly susceptible to manipulation attacks due to the leakage of sensitive information. Addressing manipulation vulnerabilities is particularly challenging because they stem from inherent data confidentiality issues rather than straightforward implementation bugs. To tackle this by preventing sensitive information leakage, we present PartitionGPT, the first LLM-driven approach that combines static analysis with the in-context learning capabilities of large language models (LLMs) to partition smart contracts into privileged and normal codebases, guided by a few annotated sensitive data variables. We evaluated PartitionGPT on 18 annotated smart contracts containing 99 sensitive functions. The results demonstrate that PartitionGPT successfully generates compilable, and verified partitions for 78% of the sensitive functions while reducing approximately 30% code compared to function-level partitioning approach. Furthermore, we evaluated PartitionGPT on nine real-world manipulation attacks that lead to a total loss of 25 million dollars, PartitionGPT effectively prevents eight cases, highlighting its potential for broad applicability and the necessity for secure program partitioning during smart contract development to diminish manipulation vulnerabilities.","sentences":["Smart contracts are highly susceptible to manipulation attacks due to the leakage of sensitive information.","Addressing manipulation vulnerabilities is particularly challenging because they stem from inherent data confidentiality issues rather than straightforward implementation bugs.","To tackle this by preventing sensitive information leakage, we present PartitionGPT, the first LLM-driven approach that combines static analysis with the in-context learning capabilities of large language models (LLMs) to partition smart contracts into privileged and normal codebases, guided by a few annotated sensitive data variables.","We evaluated PartitionGPT on 18 annotated smart contracts containing 99 sensitive functions.","The results demonstrate that PartitionGPT successfully generates compilable, and verified partitions for 78% of the sensitive functions while reducing approximately 30% code compared to function-level partitioning approach.","Furthermore, we evaluated PartitionGPT on nine real-world manipulation attacks that lead to a total loss of 25 million dollars, PartitionGPT effectively prevents eight cases, highlighting its potential for broad applicability and the necessity for secure program partitioning during smart contract development to diminish manipulation vulnerabilities."],"url":"http://arxiv.org/abs/2502.14215v1"}
{"created":"2025-02-20 02:58:45","title":"Asymmetric Co-Training for Source-Free Few-Shot Domain Adaptation","abstract":"Source-free unsupervised domain adaptation (SFUDA) has gained significant attention as an alternative to traditional unsupervised domain adaptation (UDA), which relies on the constant availability of labeled source data. However, SFUDA approaches come with inherent limitations that are frequently overlooked. These challenges include performance degradation when the unlabeled target data fails to meet critical assumptions, such as having a closed-set label distribution identical to that of the source domain, or when sufficient unlabeled target data is unavailable-a common situation in real-world applications. To address these issues, we propose an asymmetric co-training (ACT) method specifically designed for the SFFSDA scenario. SFFSDA presents a more practical alternative to SFUDA, as gathering a few labeled target instances is more feasible than acquiring large volumes of unlabeled target data in many real-world contexts. Our ACT method begins by employing a weak-strong augmentation to enhance data diversity. Then we use a two-step optimization process to train the target model. In the first step, we optimize the label smoothing cross-entropy loss, the entropy of the class-conditional distribution, and the reverse-entropy loss to bolster the model's discriminative ability while mitigating overfitting. The second step focuses on reducing redundancy in the output space by minimizing classifier determinacy disparity. Extensive experiments across four benchmarks demonstrate the superiority of our ACT approach, which outperforms state-of-the-art SFUDA methods and transfer learning techniques. Our findings suggest that adapting a source pre-trained model using only a small amount of labeled target data offers a practical and dependable solution. The code is available at https://github.com/gengxuli/ACT.","sentences":["Source-free unsupervised domain adaptation (SFUDA) has gained significant attention as an alternative to traditional unsupervised domain adaptation (UDA), which relies on the constant availability of labeled source data.","However, SFUDA approaches come with inherent limitations that are frequently overlooked.","These challenges include performance degradation when the unlabeled target data fails to meet critical assumptions, such as having a closed-set label distribution identical to that of the source domain, or when sufficient unlabeled target data is unavailable-a common situation in real-world applications.","To address these issues, we propose an asymmetric co-training (ACT) method specifically designed for the SFFSDA scenario.","SFFSDA presents a more practical alternative to SFUDA, as gathering a few labeled target instances is more feasible than acquiring large volumes of unlabeled target data in many real-world contexts.","Our ACT method begins by employing a weak-strong augmentation to enhance data diversity.","Then we use a two-step optimization process to train the target model.","In the first step, we optimize the label smoothing cross-entropy loss, the entropy of the class-conditional distribution, and the reverse-entropy loss to bolster the model's discriminative ability while mitigating overfitting.","The second step focuses on reducing redundancy in the output space by minimizing classifier determinacy disparity.","Extensive experiments across four benchmarks demonstrate the superiority of our ACT approach, which outperforms state-of-the-art SFUDA methods and transfer learning techniques.","Our findings suggest that adapting a source pre-trained model using only a small amount of labeled target data offers a practical and dependable solution.","The code is available at https://github.com/gengxuli/ACT."],"url":"http://arxiv.org/abs/2502.14214v1"}
{"created":"2025-02-20 02:47:09","title":"Less is More: On the Importance of Data Quality for Unit Test Generation","abstract":"Unit testing is crucial for software development and maintenance. Effective unit testing ensures and improves software quality, but writing unit tests is time-consuming and labor-intensive. Recent studies have proposed deep learning (DL) techniques or large language models (LLMs) to automate unit test generation. These models are usually trained or fine-tuned on large-scale datasets. Despite growing awareness of the importance of data quality, there has been limited research on the quality of datasets used for test generation. To bridge this gap, we systematically examine the impact of noise on the performance of learning-based test generation models. We first apply the open card sorting method to analyze the most popular and largest test generation dataset, Methods2Test, to categorize eight distinct types of noise. Further, we conduct detailed interviews with 17 domain experts to validate and assess the importance, reasonableness, and correctness of the noise taxonomy. Then, we propose CleanTest, an automated noise-cleaning framework designed to improve the quality of test generation datasets. CleanTest comprises three filters: a rule-based syntax filter, a rule-based relevance filter, and a model-based coverage filter. To evaluate its effectiveness, we apply CleanTest on two widely-used test generation datasets, i.e., Methods2Test and Atlas. Our findings indicate that 43.52% and 29.65% of datasets contain noise, highlighting its prevalence. Finally, we conduct comparative experiments using four LLMs (i.e., CodeBERT, AthenaTest, StarCoder, and CodeLlama7B) to assess the impact of noise on test generation performance. The results show that filtering noise positively influences the test generation ability of the models.","sentences":["Unit testing is crucial for software development and maintenance.","Effective unit testing ensures and improves software quality, but writing unit tests is time-consuming and labor-intensive.","Recent studies have proposed deep learning (DL) techniques or large language models (LLMs) to automate unit test generation.","These models are usually trained or fine-tuned on large-scale datasets.","Despite growing awareness of the importance of data quality, there has been limited research on the quality of datasets used for test generation.","To bridge this gap, we systematically examine the impact of noise on the performance of learning-based test generation models.","We first apply the open card sorting method to analyze the most popular and largest test generation dataset, Methods2Test, to categorize eight distinct types of noise.","Further, we conduct detailed interviews with 17 domain experts to validate and assess the importance, reasonableness, and correctness of the noise taxonomy.","Then, we propose CleanTest, an automated noise-cleaning framework designed to improve the quality of test generation datasets.","CleanTest comprises three filters: a rule-based syntax filter, a rule-based relevance filter, and a model-based coverage filter.","To evaluate its effectiveness, we apply CleanTest on two widely-used test generation datasets, i.e., Methods2Test and Atlas.","Our findings indicate that 43.52% and 29.65% of datasets contain noise, highlighting its prevalence.","Finally, we conduct comparative experiments using four LLMs (i.e., CodeBERT, AthenaTest, StarCoder, and CodeLlama7B) to assess the impact of noise on test generation performance.","The results show that filtering noise positively influences the test generation ability of the models."],"url":"http://arxiv.org/abs/2502.14212v1"}
{"created":"2025-02-20 02:35:17","title":"Accurate Forgetting for Heterogeneous Federated Continual Learning","abstract":"Recent years have witnessed a burgeoning interest in federated learning (FL). However, the contexts in which clients engage in sequential learning remain under-explored. Bridging FL and continual learning (CL) gives rise to a challenging practical problem: federated continual learning (FCL). Existing research in FCL primarily focuses on mitigating the catastrophic forgetting issue of continual learning while collaborating with other clients. We argue that the forgetting phenomena are not invariably detrimental. In this paper, we consider a more practical and challenging FCL setting characterized by potentially unrelated or even antagonistic data/tasks across different clients. In the FL scenario, statistical heterogeneity and data noise among clients may exhibit spurious correlations which result in biased feature learning. While existing CL strategies focus on a complete utilization of previous knowledge, we found that forgetting biased information is beneficial in our study. Therefore, we propose a new concept accurate forgetting (AF) and develop a novel generative-replay method~\\method~which selectively utilizes previous knowledge in federated networks. We employ a probabilistic framework based on a normalizing flow model to quantify the credibility of previous knowledge. Comprehensive experiments affirm the superiority of our method over baselines.","sentences":["Recent years have witnessed a burgeoning interest in federated learning (FL).","However, the contexts in which clients engage in sequential learning remain under-explored.","Bridging FL and continual learning (CL) gives rise to a challenging practical problem: federated continual learning (FCL).","Existing research in FCL primarily focuses on mitigating the catastrophic forgetting issue of continual learning while collaborating with other clients.","We argue that the forgetting phenomena are not invariably detrimental.","In this paper, we consider a more practical and challenging FCL setting characterized by potentially unrelated or even antagonistic data/tasks across different clients.","In the FL scenario, statistical heterogeneity and data noise among clients may exhibit spurious correlations which result in biased feature learning.","While existing CL strategies focus on a complete utilization of previous knowledge, we found that forgetting biased information is beneficial in our study.","Therefore, we propose a new concept accurate forgetting (AF) and develop a novel generative-replay method~\\method~which selectively utilizes previous knowledge in federated networks.","We employ a probabilistic framework based on a normalizing flow model to quantify the credibility of previous knowledge.","Comprehensive experiments affirm the superiority of our method over baselines."],"url":"http://arxiv.org/abs/2502.14205v1"}
{"created":"2025-02-20 02:23:09","title":"On-the-fly Preference Alignment via Principle-Guided Decoding","abstract":"With the rapidly expanding landscape of large language models, aligning model generations with human values and preferences is becoming increasingly important. Popular alignment methods, such as Reinforcement Learning from Human Feedback, have shown significant success in guiding models with greater control. However, these methods require considerable computational resources, which is inefficient, and substantial collection of training data to accommodate the diverse and pluralistic nature of human preferences, which is impractical. These limitations significantly constrain the scope and efficacy of both task-specific and general preference alignment methods. In this work, we introduce On-the-fly Preference Alignment via Principle-Guided Decoding (OPAD) to directly align model outputs with human preferences during inference, eliminating the need for fine-tuning. Our approach involves first curating a surrogate solution to an otherwise infeasible optimization problem and then designing a principle-guided reward function based on this surrogate. The final aligned policy is derived by maximizing this customized reward, which exploits the discrepancy between the constrained policy and its unconstrained counterpart. OPAD directly modifies the model's predictions during inference, ensuring principle adherence without incurring the computational overhead of retraining or fine-tuning. Experiments show that OPAD achieves competitive or superior performance in both general and personalized alignment tasks, demonstrating its efficiency and effectiveness compared to state-of-the-art baselines.","sentences":["With the rapidly expanding landscape of large language models, aligning model generations with human values and preferences is becoming increasingly important.","Popular alignment methods, such as Reinforcement Learning from Human Feedback, have shown significant success in guiding models with greater control.","However, these methods require considerable computational resources, which is inefficient, and substantial collection of training data to accommodate the diverse and pluralistic nature of human preferences, which is impractical.","These limitations significantly constrain the scope and efficacy of both task-specific and general preference alignment methods.","In this work, we introduce On-the-fly Preference Alignment via Principle-Guided Decoding (OPAD) to directly align model outputs with human preferences during inference, eliminating the need for fine-tuning.","Our approach involves first curating a surrogate solution to an otherwise infeasible optimization problem and then designing a principle-guided reward function based on this surrogate.","The final aligned policy is derived by maximizing this customized reward, which exploits the discrepancy between the constrained policy and its unconstrained counterpart.","OPAD directly modifies the model's predictions during inference, ensuring principle adherence without incurring the computational overhead of retraining or fine-tuning.","Experiments show that OPAD achieves competitive or superior performance in both general and personalized alignment tasks, demonstrating its efficiency and effectiveness compared to state-of-the-art baselines."],"url":"http://arxiv.org/abs/2502.14204v1"}
{"created":"2025-02-20 01:46:12","title":"QUAD-LLM-MLTC: Large Language Models Ensemble Learning for Healthcare Text Multi-Label Classification","abstract":"The escalating volume of collected healthcare textual data presents a unique challenge for automated Multi-Label Text Classification (MLTC), which is primarily due to the scarcity of annotated texts for training and their nuanced nature. Traditional machine learning models often fail to fully capture the array of expressed topics. However, Large Language Models (LLMs) have demonstrated remarkable effectiveness across numerous Natural Language Processing (NLP) tasks in various domains, which show impressive computational efficiency and suitability for unsupervised learning through prompt engineering. Consequently, these LLMs promise an effective MLTC of medical narratives. However, when dealing with various labels, different prompts can be relevant depending on the topic. To address these challenges, the proposed approach, QUAD-LLM-MLTC, leverages the strengths of four LLMs: GPT-4o, BERT, PEGASUS, and BART. QUAD-LLM-MLTC operates in a sequential pipeline in which BERT extracts key tokens, PEGASUS augments textual data, GPT-4o classifies, and BART provides topics' assignment probabilities, which results in four classifications, all in a 0-shot setting. The outputs are then combined using ensemble learning and processed through a meta-classifier to produce the final MLTC result. The approach is evaluated using three samples of annotated texts, which contrast it with traditional and single-model methods. The results show significant improvements across the majority of the topics in the classification's F1 score and consistency (F1 and Micro-F1 scores of 78.17% and 80.16% with standard deviations of 0.025 and 0.011, respectively). This research advances MLTC using LLMs and provides an efficient and scalable solution to rapidly categorize healthcare-related text data without further training.","sentences":["The escalating volume of collected healthcare textual data presents a unique challenge for automated Multi-Label Text Classification (MLTC), which is primarily due to the scarcity of annotated texts for training and their nuanced nature.","Traditional machine learning models often fail to fully capture the array of expressed topics.","However, Large Language Models (LLMs) have demonstrated remarkable effectiveness across numerous Natural Language Processing (NLP) tasks in various domains, which show impressive computational efficiency and suitability for unsupervised learning through prompt engineering.","Consequently, these LLMs promise an effective MLTC of medical narratives.","However, when dealing with various labels, different prompts can be relevant depending on the topic.","To address these challenges, the proposed approach, QUAD-LLM-MLTC, leverages the strengths of four LLMs: GPT-4o, BERT, PEGASUS, and BART.","QUAD-LLM-MLTC operates in a sequential pipeline in which BERT extracts key tokens, PEGASUS augments textual data, GPT-4o classifies, and BART provides topics' assignment probabilities, which results in four classifications, all in a 0-shot setting.","The outputs are then combined using ensemble learning and processed through a meta-classifier to produce the final MLTC result.","The approach is evaluated using three samples of annotated texts, which contrast it with traditional and single-model methods.","The results show significant improvements across the majority of the topics in the classification's F1 score and consistency (F1 and Micro-F1 scores of 78.17% and 80.16% with standard deviations of 0.025 and 0.011, respectively).","This research advances MLTC using LLMs and provides an efficient and scalable solution to rapidly categorize healthcare-related text data without further training."],"url":"http://arxiv.org/abs/2502.14189v1"}
{"created":"2025-02-20 01:37:58","title":"REFLEX Dataset: A Multimodal Dataset of Human Reactions to Robot Failures and Explanations","abstract":"This work presents REFLEX: Robotic Explanations to FaiLures and Human EXpressions, a comprehensive multimodal dataset capturing human reactions to robot failures and subsequent explanations in collaborative settings. It aims to facilitate research into human-robot interaction dynamics, addressing the need to study reactions to both initial failures and explanations, as well as the evolution of these reactions in long-term interactions. By providing rich, annotated data on human responses to different types of failures, explanation levels, and explanation varying strategies, the dataset contributes to the development of more robust, adaptive, and satisfying robotic systems capable of maintaining positive relationships with human collaborators, even during challenges like repeated failures.","sentences":["This work presents REFLEX:","Robotic Explanations to FaiLures and Human EXpressions, a comprehensive multimodal dataset capturing human reactions to robot failures and subsequent explanations in collaborative settings.","It aims to facilitate research into human-robot interaction dynamics, addressing the need to study reactions to both initial failures and explanations, as well as the evolution of these reactions in long-term interactions.","By providing rich, annotated data on human responses to different types of failures, explanation levels, and explanation varying strategies, the dataset contributes to the development of more robust, adaptive, and satisfying robotic systems capable of maintaining positive relationships with human collaborators, even during challenges like repeated failures."],"url":"http://arxiv.org/abs/2502.14185v1"}
{"created":"2025-02-20 01:26:05","title":"Bayesian SegNet for Semantic Segmentation with Improved Interpretation of Microstructural Evolution During Irradiation of Materials","abstract":"Understanding the relationship between the evolution of microstructures of irradiated LiAlO2 pellets and tritium diffusion, retention and release could improve predictions of tritium-producing burnable absorber rod performance. Given expert-labeled segmented images of irradiated and unirradiated pellets, we trained Deep Convolutional Neural Networks to segment images into defect, grain, and boundary classes. Qualitative microstructural information was calculated from these segmented images to facilitate the comparison of unirradiated and irradiated pellets. We tested modifications to improve the sensitivity of the model, including incorporating meta-data into the model and utilizing uncertainty quantification. The predicted segmentation was similar to the expert-labeled segmentation for most methods of microstructural qualification, including pixel proportion, defect area, and defect density. Overall, the high performance metrics for the best models for both irradiated and unirradiated images shows that utilizing neural network models is a viable alternative to expert-labeled images.","sentences":["Understanding the relationship between the evolution of microstructures of irradiated LiAlO2 pellets and tritium diffusion, retention and release could improve predictions of tritium-producing burnable absorber rod performance.","Given expert-labeled segmented images of irradiated and unirradiated pellets, we trained Deep Convolutional Neural Networks to segment images into defect, grain, and boundary classes.","Qualitative microstructural information was calculated from these segmented images to facilitate the comparison of unirradiated and irradiated pellets.","We tested modifications to improve the sensitivity of the model, including incorporating meta-data into the model and utilizing uncertainty quantification.","The predicted segmentation was similar to the expert-labeled segmentation for most methods of microstructural qualification, including pixel proportion, defect area, and defect density.","Overall, the high performance metrics for the best models for both irradiated and unirradiated images shows that utilizing neural network models is a viable alternative to expert-labeled images."],"url":"http://arxiv.org/abs/2502.14184v1"}
{"created":"2025-02-20 01:26:00","title":"Type 1 Diabetes Management using GLIMMER: Glucose Level Indicator Model with Modified Error Rate","abstract":"Managing Type 1 Diabetes (T1D) demands constant vigilance as individuals strive to regulate their blood glucose levels to avert the dangers of dysglycemia (hyperglycemia or hypoglycemia). Despite the advent of sophisticated technologies such as automated insulin delivery (AID) systems, achieving optimal glycemic control remains a formidable task. AID systems integrate continuous subcutaneous insulin infusion (CSII) and continuous glucose monitors (CGM) data, offering promise in reducing variability and increasing glucose time-in-range. However, these systems often fail to prevent dysglycemia, partly due to limitations in prediction algorithms that lack the precision to avert abnormal glucose events. This gap highlights the need for proactive behavioral adjustments. We address this need with GLIMMER, Glucose Level Indicator Model with Modified Error Rate, a machine learning approach for forecasting blood glucose levels. GLIMMER categorizes glucose values into normal and abnormal ranges and devises a novel custom loss function to prioritize accuracy in dysglycemic events where patient safety is critical. To evaluate the potential of GLIMMER for T1D management, we both use a publicly available dataset and collect new data involving 25 patients with T1D. In predicting next-hour glucose values, GLIMMER achieved a root mean square error (RMSE) of 23.97 (+/-3.77) and a mean absolute error (MAE) of 15.83 (+/-2.09) mg/dL. These results reflect a 23% improvement in RMSE and a 31% improvement in MAE compared to the best-reported error rates.","sentences":["Managing Type 1 Diabetes (T1D) demands constant vigilance as individuals strive to regulate their blood glucose levels to avert the dangers of dysglycemia (hyperglycemia or hypoglycemia).","Despite the advent of sophisticated technologies such as automated insulin delivery (AID) systems, achieving optimal glycemic control remains a formidable task.","AID systems integrate continuous subcutaneous insulin infusion (CSII) and continuous glucose monitors (CGM) data, offering promise in reducing variability and increasing glucose time-in-range.","However, these systems often fail to prevent dysglycemia, partly due to limitations in prediction algorithms that lack the precision to avert abnormal glucose events.","This gap highlights the need for proactive behavioral adjustments.","We address this need with GLIMMER, Glucose Level Indicator Model with Modified Error Rate, a machine learning approach for forecasting blood glucose levels.","GLIMMER categorizes glucose values into normal and abnormal ranges and devises a novel custom loss function to prioritize accuracy in dysglycemic events where patient safety is critical.","To evaluate the potential of GLIMMER for T1D management, we both use a publicly available dataset and collect new data involving 25 patients with T1D. In predicting next-hour glucose values, GLIMMER achieved a root mean square error (RMSE) of 23.97 (+/-3.77) and a mean absolute error (MAE) of 15.83 (+/-2.09)","mg/dL. These results reflect a 23% improvement in RMSE and a 31% improvement in MAE compared to the best-reported error rates."],"url":"http://arxiv.org/abs/2502.14183v1"}
{"created":"2025-02-20 01:19:51","title":"Multi-Faceted Studies on Data Poisoning can Advance LLM Development","abstract":"The lifecycle of large language models (LLMs) is far more complex than that of traditional machine learning models, involving multiple training stages, diverse data sources, and varied inference methods. While prior research on data poisoning attacks has primarily focused on the safety vulnerabilities of LLMs, these attacks face significant challenges in practice. Secure data collection, rigorous data cleaning, and the multistage nature of LLM training make it difficult to inject poisoned data or reliably influence LLM behavior as intended. Given these challenges, this position paper proposes rethinking the role of data poisoning and argue that multi-faceted studies on data poisoning can advance LLM development. From a threat perspective, practical strategies for data poisoning attacks can help evaluate and address real safety risks to LLMs. From a trustworthiness perspective, data poisoning can be leveraged to build more robust LLMs by uncovering and mitigating hidden biases, harmful outputs, and hallucinations. Moreover, from a mechanism perspective, data poisoning can provide valuable insights into LLMs, particularly the interplay between data and model behavior, driving a deeper understanding of their underlying mechanisms.","sentences":["The lifecycle of large language models (LLMs) is far more complex than that of traditional machine learning models, involving multiple training stages, diverse data sources, and varied inference methods.","While prior research on data poisoning attacks has primarily focused on the safety vulnerabilities of LLMs, these attacks face significant challenges in practice.","Secure data collection, rigorous data cleaning, and the multistage nature of LLM training make it difficult to inject poisoned data or reliably influence LLM behavior as intended.","Given these challenges, this position paper proposes rethinking the role of data poisoning and argue that multi-faceted studies on data poisoning can advance LLM development.","From a threat perspective, practical strategies for data poisoning attacks can help evaluate and address real safety risks to LLMs.","From a trustworthiness perspective, data poisoning can be leveraged to build more robust LLMs by uncovering and mitigating hidden biases, harmful outputs, and hallucinations.","Moreover, from a mechanism perspective, data poisoning can provide valuable insights into LLMs, particularly the interplay between data and model behavior, driving a deeper understanding of their underlying mechanisms."],"url":"http://arxiv.org/abs/2502.14182v1"}
{"created":"2025-02-20 01:18:24","title":"On the logical skills of large language models: evaluations using arbitrarily complex first-order logic problems","abstract":"We present a method of generating first-order logic statements whose complexity can be controlled along multiple dimensions. We use this method to automatically create several datasets consisting of questions asking for the truth or falsity of first-order logic statements in Zermelo-Fraenkel set theory. While the resolution of these questions does not require any knowledge beyond basic notation of first-order logic and set theory, it does require a degree of planning and logical reasoning, which can be controlled up to arbitrarily high difficulty by the complexity of the generated statements. Furthermore, we do extensive evaluations of the performance of various large language models, including recent models such as DeepSeek-R1 and OpenAI's o3-mini, on these datasets. All of the datasets along with the code used for generating them, as well as all data from the evaluations is publicly available at https://github.com/bkuckuck/logical-skills-of-llms.","sentences":["We present a method of generating first-order logic statements whose complexity can be controlled along multiple dimensions.","We use this method to automatically create several datasets consisting of questions asking for the truth or falsity of first-order logic statements in Zermelo-Fraenkel set theory.","While the resolution of these questions does not require any knowledge beyond basic notation of first-order logic and set theory, it does require a degree of planning and logical reasoning, which can be controlled up to arbitrarily high difficulty by the complexity of the generated statements.","Furthermore, we do extensive evaluations of the performance of various large language models, including recent models such as DeepSeek-R1 and OpenAI's o3-mini, on these datasets.","All of the datasets along with the code used for generating them, as well as all data from the evaluations is publicly available at https://github.com/bkuckuck/logical-skills-of-llms."],"url":"http://arxiv.org/abs/2502.14180v1"}
