{"created":"2025-02-13 18:59:46","title":"MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency","abstract":"Answering questions with Chain-of-Thought (CoT) has significantly enhanced the reasoning capabilities of Large Language Models (LLMs), yet its impact on Large Multimodal Models (LMMs) still lacks a systematic assessment and in-depth investigation. In this paper, we introduce MME-CoT, a specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes. As the first comprehensive study in this area, we propose a thorough evaluation suite incorporating three novel metrics that assess the reasoning quality, robustness, and efficiency at a fine-grained level. Leveraging curated high-quality data and a unique evaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs, uncovering several key insights: 1) Models with reflection mechanism demonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and demonstrating the highest quality results; 2) CoT prompting often degrades LMM performance on perception-heavy tasks, suggesting a potentially harmful overthinking behavior; and 3) Although the CoT quality is high, LMMs with reflection exhibit significant inefficiency in both normal response and self-correction phases. We hope MME-CoT serves as a foundation for advancing multimodal reasoning in LMMs. Project Page: https://mmecot.github.io/","sentences":["Answering questions with Chain-of-Thought (CoT) has significantly enhanced the reasoning capabilities of Large Language Models (LLMs), yet its impact on Large Multimodal Models (LMMs) still lacks a systematic assessment and in-depth investigation.","In this paper, we introduce MME-CoT, a specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes.","As the first comprehensive study in this area, we propose a thorough evaluation suite incorporating three novel metrics that assess the reasoning quality, robustness, and efficiency at a fine-grained level.","Leveraging curated high-quality data and a unique evaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs, uncovering several key insights: 1) Models with reflection mechanism demonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and demonstrating the highest quality results; 2) CoT prompting often degrades LMM performance on perception-heavy tasks, suggesting a potentially harmful overthinking behavior; and 3)","Although the CoT quality is high, LMMs with reflection exhibit significant inefficiency in both normal response and self-correction phases.","We hope MME-CoT serves as a foundation for advancing multimodal reasoning in LMMs.","Project Page: https://mmecot.github.io/"],"url":"http://arxiv.org/abs/2502.09621v1"}
{"created":"2025-02-13 18:59:44","title":"Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights","abstract":"With the increasing numbers of publicly available models, there are probably pretrained, online models for most tasks users require. However, current model search methods are rudimentary, essentially a text-based search in the documentation, thus users cannot find the relevant models. This paper presents ProbeLog, a method for retrieving classification models that can recognize a target concept, such as \"Dog\", without access to model metadata or training data. Differently from previous probing methods, ProbeLog computes a descriptor for each output dimension (logit) of each model, by observing its responses on a fixed set of inputs (probes). Our method supports both logit-based retrieval (\"find more logits like this\") and zero-shot, text-based retrieval (\"find all logits corresponding to dogs\"). As probing-based representations require multiple costly feedforward passes through the model, we develop a method, based on collaborative filtering, that reduces the cost of encoding repositories by 3x. We demonstrate that ProbeLog achieves high retrieval accuracy, both in real-world and fine-grained search tasks and is scalable to full-size repositories.","sentences":["With the increasing numbers of publicly available models, there are probably pretrained, online models for most tasks users require.","However, current model search methods are rudimentary, essentially a text-based search in the documentation, thus users cannot find the relevant models.","This paper presents ProbeLog, a method for retrieving classification models that can recognize a target concept, such as \"Dog\", without access to model metadata or training data.","Differently from previous probing methods, ProbeLog computes a descriptor for each output dimension (logit) of each model, by observing its responses on a fixed set of inputs (probes).","Our method supports both logit-based retrieval (\"find more logits like this\") and zero-shot, text-based retrieval (\"find all logits corresponding to dogs\").","As probing-based representations require multiple costly feedforward passes through the model, we develop a method, based on collaborative filtering, that reduces the cost of encoding repositories by 3x.","We demonstrate that ProbeLog achieves high retrieval accuracy, both in real-world and fine-grained search tasks and is scalable to full-size repositories."],"url":"http://arxiv.org/abs/2502.09619v1"}
{"created":"2025-02-13 18:59:19","title":"LIFe-GoM: Generalizable Human Rendering with Learned Iterative Feedback Over Multi-Resolution Gaussians-on-Mesh","abstract":"Generalizable rendering of an animatable human avatar from sparse inputs relies on data priors and inductive biases extracted from training on large data to avoid scene-specific optimization and to enable fast reconstruction. This raises two main challenges: First, unlike iterative gradient-based adjustment in scene-specific optimization, generalizable methods must reconstruct the human shape representation in a single pass at inference time. Second, rendering is preferably computationally efficient yet of high resolution. To address both challenges we augment the recently proposed dual shape representation, which combines the benefits of a mesh and Gaussian points, in two ways. To improve reconstruction, we propose an iterative feedback update framework, which successively improves the canonical human shape representation during reconstruction. To achieve computationally efficient yet high-resolution rendering, we study a coupled-multi-resolution Gaussians-on-Mesh representation. We evaluate the proposed approach on the challenging THuman2.0, XHuman and AIST++ data. Our approach reconstructs an animatable representation from sparse inputs in less than 1s, renders views with 95.1FPS at $1024 \\times 1024$, and achieves PSNR/LPIPS*/FID of 24.65/110.82/51.27 on THuman2.0, outperforming the state-of-the-art in rendering quality.","sentences":["Generalizable rendering of an animatable human avatar from sparse inputs relies on data priors and inductive biases extracted from training on large data to avoid scene-specific optimization and to enable fast reconstruction.","This raises two main challenges: First, unlike iterative gradient-based adjustment in scene-specific optimization, generalizable methods must reconstruct the human shape representation in a single pass at inference time.","Second, rendering is preferably computationally efficient yet of high resolution.","To address both challenges we augment the recently proposed dual shape representation, which combines the benefits of a mesh and Gaussian points, in two ways.","To improve reconstruction, we propose an iterative feedback update framework, which successively improves the canonical human shape representation during reconstruction.","To achieve computationally efficient yet high-resolution rendering, we study a coupled-multi-resolution Gaussians-on-Mesh representation.","We evaluate the proposed approach on the challenging THuman2.0, XHuman and AIST++ data.","Our approach reconstructs an animatable representation from sparse inputs in less than 1s, renders views with 95.1FPS at $1024 \\times 1024$, and achieves PSNR/LPIPS*/FID of 24.65/110.82/51.27 on THuman2.0, outperforming the state-of-the-art in rendering quality."],"url":"http://arxiv.org/abs/2502.09617v1"}
{"created":"2025-02-13 18:59:15","title":"Variational Rectified Flow Matching","abstract":"We study Variational Rectified Flow Matching, a framework that enhances classic rectified flow matching by modeling multi-modal velocity vector-fields. At inference time, classic rectified flow matching 'moves' samples from a source distribution to the target distribution by solving an ordinary differential equation via integration along a velocity vector-field. At training time, the velocity vector-field is learnt by linearly interpolating between coupled samples one drawn from the source and one drawn from the target distribution randomly. This leads to ''ground-truth'' velocity vector-fields that point in different directions at the same location, i.e., the velocity vector-fields are multi-modal/ambiguous. However, since training uses a standard mean-squared-error loss, the learnt velocity vector-field averages ''ground-truth'' directions and isn't multi-modal. In contrast, variational rectified flow matching learns and samples from multi-modal flow directions. We show on synthetic data, MNIST, CIFAR-10, and ImageNet that variational rectified flow matching leads to compelling results.","sentences":["We study Variational Rectified Flow Matching, a framework that enhances classic rectified flow matching by modeling multi-modal velocity vector-fields.","At inference time, classic rectified flow matching 'moves' samples from a source distribution to the target distribution by solving an ordinary differential equation via integration along a velocity vector-field.","At training time, the velocity vector-field is learnt by linearly interpolating between coupled samples one drawn from the source and one drawn from the target distribution randomly.","This leads to ''ground-truth'' velocity vector-fields that point in different directions at the same location, i.e., the velocity vector-fields are multi-modal/ambiguous.","However, since training uses a standard mean-squared-error loss, the learnt velocity vector-field averages ''ground-truth'' directions and isn't multi-modal.","In contrast, variational rectified flow matching learns and samples from multi-modal flow directions.","We show on synthetic data, MNIST, CIFAR-10, and ImageNet that variational rectified flow matching leads to compelling results."],"url":"http://arxiv.org/abs/2502.09616v1"}
{"created":"2025-02-13 18:59:13","title":"DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References","abstract":"We address the challenge of developing a generalizable neural tracking controller for dexterous manipulation from human references. This controller aims to manage a dexterous robot hand to manipulate diverse objects for various purposes defined by kinematic human-object interactions. Developing such a controller is complicated by the intricate contact dynamics of dexterous manipulation and the need for adaptivity, generalizability, and robustness. Current reinforcement learning and trajectory optimization methods often fall short due to their dependence on task-specific rewards or precise system models. We introduce an approach that curates large-scale successful robot tracking demonstrations, comprising pairs of human references and robot actions, to train a neural controller. Utilizing a data flywheel, we iteratively enhance the controller's performance, as well as the number and quality of successful tracking demonstrations. We exploit available tracking demonstrations and carefully integrate reinforcement learning and imitation learning to boost the controller's performance in dynamic environments. At the same time, to obtain high-quality tracking demonstrations, we individually optimize per-trajectory tracking by leveraging the learned tracking controller in a homotopy optimization method. The homotopy optimization, mimicking chain-of-thought, aids in solving challenging trajectory tracking problems to increase demonstration diversity. We showcase our success by training a generalizable neural controller and evaluating it in both simulation and real world. Our method achieves over a 10% improvement in success rates compared to leading baselines. The project website with animated results is available at https://meowuu7.github.io/DexTrack/.","sentences":["We address the challenge of developing a generalizable neural tracking controller for dexterous manipulation from human references.","This controller aims to manage a dexterous robot hand to manipulate diverse objects for various purposes defined by kinematic human-object interactions.","Developing such a controller is complicated by the intricate contact dynamics of dexterous manipulation and the need for adaptivity, generalizability, and robustness.","Current reinforcement learning and trajectory optimization methods often fall short due to their dependence on task-specific rewards or precise system models.","We introduce an approach that curates large-scale successful robot tracking demonstrations, comprising pairs of human references and robot actions, to train a neural controller.","Utilizing a data flywheel, we iteratively enhance the controller's performance, as well as the number and quality of successful tracking demonstrations.","We exploit available tracking demonstrations and carefully integrate reinforcement learning and imitation learning to boost the controller's performance in dynamic environments.","At the same time, to obtain high-quality tracking demonstrations, we individually optimize per-trajectory tracking by leveraging the learned tracking controller in a homotopy optimization method.","The homotopy optimization, mimicking chain-of-thought, aids in solving challenging trajectory tracking problems to increase demonstration diversity.","We showcase our success by training a generalizable neural controller and evaluating it in both simulation and real world.","Our method achieves over a 10% improvement in success rates compared to leading baselines.","The project website with animated results is available at https://meowuu7.github.io/DexTrack/."],"url":"http://arxiv.org/abs/2502.09614v1"}
{"created":"2025-02-13 18:59:13","title":"RigAnything: Template-Free Autoregressive Rigging for Diverse 3D Assets","abstract":"We present RigAnything, a novel autoregressive transformer-based model, which makes 3D assets rig-ready by probabilistically generating joints, skeleton topologies, and assigning skinning weights in a template-free manner. Unlike most existing auto-rigging methods, which rely on predefined skeleton template and are limited to specific categories like humanoid, RigAnything approaches the rigging problem in an autoregressive manner, iteratively predicting the next joint based on the global input shape and the previous prediction. While autoregressive models are typically used to generate sequential data, RigAnything extends their application to effectively learn and represent skeletons, which are inherently tree structures. To achieve this, we organize the joints in a breadth-first search (BFS) order, enabling the skeleton to be defined as a sequence of 3D locations and the parent index. Furthermore, our model improves the accuracy of position prediction by leveraging diffusion modeling, ensuring precise and consistent placement of joints within the hierarchy. This formulation allows the autoregressive model to efficiently capture both spatial and hierarchical relationships within the skeleton. Trained end-to-end on both RigNet and Objaverse datasets, RigAnything demonstrates state-of-the-art performance across diverse object types, including humanoids, quadrupeds, marine creatures, insects, and many more, surpassing prior methods in quality, robustness, generalizability, and efficiency. Please check our website for more details: https://www.liuisabella.com/RigAnything.","sentences":["We present RigAnything, a novel autoregressive transformer-based model, which makes 3D assets rig-ready by probabilistically generating joints, skeleton topologies, and assigning skinning weights in a template-free manner.","Unlike most existing auto-rigging methods, which rely on predefined skeleton template and are limited to specific categories like humanoid, RigAnything approaches the rigging problem in an autoregressive manner, iteratively predicting the next joint based on the global input shape and the previous prediction.","While autoregressive models are typically used to generate sequential data, RigAnything extends their application to effectively learn and represent skeletons, which are inherently tree structures.","To achieve this, we organize the joints in a breadth-first search (BFS) order, enabling the skeleton to be defined as a sequence of 3D locations and the parent index.","Furthermore, our model improves the accuracy of position prediction by leveraging diffusion modeling, ensuring precise and consistent placement of joints within the hierarchy.","This formulation allows the autoregressive model to efficiently capture both spatial and hierarchical relationships within the skeleton.","Trained end-to-end on both RigNet and Objaverse datasets, RigAnything demonstrates state-of-the-art performance across diverse object types, including humanoids, quadrupeds, marine creatures, insects, and many more, surpassing prior methods in quality, robustness, generalizability, and efficiency.","Please check our website for more details: https://www.liuisabella.com/RigAnything."],"url":"http://arxiv.org/abs/2502.09615v1"}
{"created":"2025-02-13 18:58:15","title":"Designing a Conditional Prior Distribution for Flow-Based Generative Models","abstract":"Flow-based generative models have recently shown impressive performance for conditional generation tasks, such as text-to-image generation. However, current methods transform a general unimodal noise distribution to a specific mode of the target data distribution. As such, every point in the initial source distribution can be mapped to every point in the target distribution, resulting in long average paths. To this end, in this work, we tap into a non-utilized property of conditional flow-based models: the ability to design a non-trivial prior distribution. Given an input condition, such as a text prompt, we first map it to a point lying in data space, representing an ``average\" data point with the minimal average distance to all data points of the same conditional mode (e.g., class). We then utilize the flow matching formulation to map samples from a parametric distribution centered around this point to the conditional target distribution. Experimentally, our method significantly improves training times and generation efficiency (FID, KID and CLIP alignment scores) compared to baselines, producing high quality samples using fewer sampling steps.","sentences":["Flow-based generative models have recently shown impressive performance for conditional generation tasks, such as text-to-image generation.","However, current methods transform a general unimodal noise distribution to a specific mode of the target data distribution.","As such, every point in the initial source distribution can be mapped to every point in the target distribution, resulting in long average paths.","To this end, in this work, we tap into a non-utilized property of conditional flow-based models: the ability to design a non-trivial prior distribution.","Given an input condition, such as a text prompt, we first map it to a point lying in data space, representing an ``average\" data point with the minimal average distance to all data points of the same conditional mode (e.g., class).","We then utilize the flow matching formulation to map samples from a parametric distribution centered around this point to the conditional target distribution.","Experimentally, our method significantly improves training times and generation efficiency (FID, KID and CLIP alignment scores) compared to baselines, producing high quality samples using fewer sampling steps."],"url":"http://arxiv.org/abs/2502.09611v1"}
{"created":"2025-02-13 18:52:14","title":"GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for Remote Sensing Image Analysis","abstract":"The continuous operation of Earth-orbiting satellites generates vast and ever-growing archives of Remote Sensing (RS) images. Natural language presents an intuitive interface for accessing, querying, and interpreting the data from such archives. However, existing Vision-Language Models (VLMs) are predominantly trained on web-scraped, noisy image-text data, exhibiting limited exposure to the specialized domain of RS. This deficiency results in poor performance on RS-specific tasks, as commonly used datasets often lack detailed, scientifically accurate textual descriptions and instead emphasize solely on attributes like date and location. To bridge this critical gap, we introduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and multi-modal RS image analysis. GAIA comprises of 205,150 meticulously curated RS image-text pairs, representing a diverse range of RS modalities associated to different spatial resolutions. Unlike existing vision-language datasets in RS, GAIA specifically focuses on capturing a diverse range of RS applications, providing unique information about environmental changes, natural disasters, and various other dynamic phenomena. The dataset provides a spatially and temporally balanced distribution, spanning across the globe, covering the last 25 years with a balanced temporal distribution of observations. GAIA's construction involved a two-stage process: (1) targeted web-scraping of images and accompanying text from reputable RS-related sources, and (2) generation of five high-quality, scientifically grounded synthetic captions for each image using carefully crafted prompts that leverage the advanced vision-language capabilities of GPT-4o. Our extensive experiments, including fine-tuning of CLIP and BLIP2 models, demonstrate that GAIA significantly improves performance on RS image classification, cross-modal retrieval and image captioning tasks.","sentences":["The continuous operation of Earth-orbiting satellites generates vast and ever-growing archives of Remote Sensing (RS) images.","Natural language presents an intuitive interface for accessing, querying, and interpreting the data from such archives.","However, existing Vision-Language Models (VLMs) are predominantly trained on web-scraped, noisy image-text data, exhibiting limited exposure to the specialized domain of RS.","This deficiency results in poor performance on RS-specific tasks, as commonly used datasets often lack detailed, scientifically accurate textual descriptions and instead emphasize solely on attributes like date and location.","To bridge this critical gap, we introduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and multi-modal RS image analysis.","GAIA comprises of 205,150 meticulously curated RS image-text pairs, representing a diverse range of RS modalities associated to different spatial resolutions.","Unlike existing vision-language datasets in RS, GAIA specifically focuses on capturing a diverse range of RS applications, providing unique information about environmental changes, natural disasters, and various other dynamic phenomena.","The dataset provides a spatially and temporally balanced distribution, spanning across the globe, covering the last 25 years with a balanced temporal distribution of observations.","GAIA's construction involved a two-stage process: (1) targeted web-scraping of images and accompanying text from reputable RS-related sources, and (2) generation of five high-quality, scientifically grounded synthetic captions for each image using carefully crafted prompts that leverage the advanced vision-language capabilities of GPT-4o.","Our extensive experiments, including fine-tuning of CLIP and BLIP2 models, demonstrate that GAIA significantly improves performance on RS image classification, cross-modal retrieval and image captioning tasks."],"url":"http://arxiv.org/abs/2502.09598v1"}
{"created":"2025-02-13 18:52:03","title":"Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs","abstract":"Large Language Models (LLMs) are increasingly used as chatbots, yet their ability to personalize responses to user preferences remains limited. We introduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize and adhere to user preferences in a long-context conversational setting. PrefEval comprises 3,000 manually curated user preference and query pairs spanning 20 topics. PrefEval contains user personalization or preference information in both explicit and implicit forms, and evaluates LLM performance using a generation and a classification task. With PrefEval, we evaluated the aforementioned preference following capabilities of 10 open-source and proprietary LLMs in multi-session conversations with varying context lengths up to 100k tokens. We benchmark with various prompting, iterative feedback, and retrieval-augmented generation methods. Our benchmarking effort reveals that state-of-the-art LLMs face significant challenges in proactively following users' preferences during conversations. In particular, in zero-shot settings, preference following accuracy falls below 10% at merely 10 turns (~3k tokens) across most evaluated models. Even with advanced prompting and retrieval methods, preference following still deteriorates in long-context conversations. Furthermore, we show that fine-tuning on PrefEval significantly improves performance. We believe PrefEval serves as a valuable resource for measuring, understanding, and enhancing LLMs' preference following abilities, paving the way for personalized conversational agents. Our code and dataset are available at https://prefeval.github.io/.","sentences":["Large Language Models (LLMs) are increasingly used as chatbots, yet their ability to personalize responses to user preferences remains limited.","We introduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize and adhere to user preferences in a long-context conversational setting.","PrefEval comprises 3,000 manually curated user preference and query pairs spanning 20 topics.","PrefEval contains user personalization or preference information in both explicit and implicit forms, and evaluates LLM performance using a generation and a classification task.","With PrefEval, we evaluated the aforementioned preference following capabilities of 10 open-source and proprietary LLMs in multi-session conversations with varying context lengths up to 100k tokens.","We benchmark with various prompting, iterative feedback, and retrieval-augmented generation methods.","Our benchmarking effort reveals that state-of-the-art LLMs face significant challenges in proactively following users' preferences during conversations.","In particular, in zero-shot settings, preference following accuracy falls below 10% at merely 10 turns (~3k tokens) across most evaluated models.","Even with advanced prompting and retrieval methods, preference following still deteriorates in long-context conversations.","Furthermore, we show that fine-tuning on PrefEval significantly improves performance.","We believe PrefEval serves as a valuable resource for measuring, understanding, and enhancing LLMs' preference following abilities, paving the way for personalized conversational agents.","Our code and dataset are available at https://prefeval.github.io/."],"url":"http://arxiv.org/abs/2502.09597v1"}
{"created":"2025-02-13 18:51:12","title":"KIMAs: A Configurable Knowledge Integrated Multi-Agent System","abstract":"Knowledge-intensive conversations supported by large language models (LLMs) have become one of the most popular and helpful applications that can assist people in different aspects. Many current knowledge-intensive applications are centered on retrieval-augmented generation (RAG) techniques. While many open-source RAG frameworks facilitate the development of RAG-based applications, they often fall short in handling practical scenarios complicated by heterogeneous data in topics and formats, conversational context management, and the requirement of low-latency response times. This technical report presents a configurable knowledge integrated multi-agent system, KIMAs, to address these challenges. KIMAs features a flexible and configurable system for integrating diverse knowledge sources with 1) context management and query rewrite mechanisms to improve retrieval accuracy and multi-turn conversational coherency, 2) efficient knowledge routing and retrieval, 3) simple but effective filter and reference generation mechanisms, and 4) optimized parallelizable multi-agent pipeline execution. Our work provides a scalable framework for advancing the deployment of LLMs in real-world settings. To show how KIMAs can help developers build knowledge-intensive applications with different scales and emphases, we demonstrate how we configure the system to three applications already running in practice with reliable performance.","sentences":["Knowledge-intensive conversations supported by large language models (LLMs) have become one of the most popular and helpful applications that can assist people in different aspects.","Many current knowledge-intensive applications are centered on retrieval-augmented generation (RAG) techniques.","While many open-source RAG frameworks facilitate the development of RAG-based applications, they often fall short in handling practical scenarios complicated by heterogeneous data in topics and formats, conversational context management, and the requirement of low-latency response times.","This technical report presents a configurable knowledge integrated multi-agent system, KIMAs, to address these challenges.","KIMAs features a flexible and configurable system for integrating diverse knowledge sources with 1) context management and query rewrite mechanisms to improve retrieval accuracy and multi-turn conversational coherency, 2) efficient knowledge routing and retrieval, 3) simple but effective filter and reference generation mechanisms, and 4) optimized parallelizable multi-agent pipeline execution.","Our work provides a scalable framework for advancing the deployment of LLMs in real-world settings.","To show how KIMAs can help developers build knowledge-intensive applications with different scales and emphases, we demonstrate how we configure the system to three applications already running in practice with reliable performance."],"url":"http://arxiv.org/abs/2502.09596v1"}
{"created":"2025-02-13 18:48:04","title":"Censor Dependent Variational Inference","abstract":"This paper provides a comprehensive analysis of variational inference in latent variable models for survival analysis, emphasizing the distinctive challenges associated with applying variational methods to survival data. We identify a critical weakness in the existing methodology, demonstrating how a poorly designed variational distribution may hinder the objective of survival analysis tasks--modeling time-to-event distributions. We prove that the optimal variational distribution, which perfectly bounds the log-likelihood, may depend on the censoring mechanism. To address this issue, we propose censor-dependent variational inference (CDVI), tailored for latent variable models in survival analysis. More practically, we introduce CD-CVAE, a V-structure Variational Autoencoder (VAE) designed for the scalable implementation of CDVI. Further discussion extends some existing theories and training techniques to survival analysis. Extensive experiments validate our analysis and demonstrate significant improvements in the estimation of individual survival distributions.","sentences":["This paper provides a comprehensive analysis of variational inference in latent variable models for survival analysis, emphasizing the distinctive challenges associated with applying variational methods to survival data.","We identify a critical weakness in the existing methodology, demonstrating how a poorly designed variational distribution may hinder the objective of survival analysis tasks--modeling time-to-event distributions.","We prove that the optimal variational distribution, which perfectly bounds the log-likelihood, may depend on the censoring mechanism.","To address this issue, we propose censor-dependent variational inference (CDVI), tailored for latent variable models in survival analysis.","More practically, we introduce CD-CVAE, a V-structure Variational Autoencoder (VAE) designed for the scalable implementation of CDVI.","Further discussion extends some existing theories and training techniques to survival analysis.","Extensive experiments validate our analysis and demonstrate significant improvements in the estimation of individual survival distributions."],"url":"http://arxiv.org/abs/2502.09591v1"}
{"created":"2025-02-13 18:42:20","title":"Differentially Private Compression and the Sensitivity of LZ77","abstract":"We initiate the study of differentially private data-compression schemes motivated by the insecurity of the popular \"Compress-Then-Encrypt\" framework. Data compression is a useful tool which exploits redundancy in data to reduce storage/bandwidth when files are stored or transmitted. However, if the contents of a file are confidential then the length of a compressed file might leak confidential information about the content of the file itself. Encrypting a compressed file does not eliminate this leakage as data encryption schemes are only designed to hide the content of confidential message instead of the length of the message. In our proposed Differentially Private Compress-Then-Encrypt framework, we add a random positive amount of padding to the compressed file to ensure that any leakage satisfies the rigorous privacy guarantee of $(\\epsilon,\\delta)$-differential privacy. The amount of padding that needs to be added depends on the sensitivity of the compression scheme to small changes in the input, i.e., to what degree can changing a single character of the input message impact the length of the compressed file. While some popular compression schemes are highly sensitive to small changes in the input, we argue that effective data compression schemes do not necessarily have high sensitivity. Our primary technical contribution is analyzing the fine-grained sensitivity of the LZ77 compression scheme (IEEE Trans. Inf. Theory 1977) which is one of the most common compression schemes used in practice. We show that the global sensitivity of the LZ77 compression scheme has the upper bound $\\mathcal{O}(W^{2/3}\\log n)$ where $W\\leq n$ denotes the size of the sliding window. When $W=n$, we show the lower bound $\\Omega(n^{2/3}\\log^{1/3}n)$ for the global sensitivity of the LZ77 compression scheme which is tight up to a sublogarithmic factor.","sentences":["We initiate the study of differentially private data-compression schemes motivated by the insecurity of the popular \"Compress-Then-Encrypt\" framework.","Data compression is a useful tool which exploits redundancy in data to reduce storage/bandwidth when files are stored or transmitted.","However, if the contents of a file are confidential then the length of a compressed file might leak confidential information about the content of the file itself.","Encrypting a compressed file does not eliminate this leakage as data encryption schemes are only designed to hide the content of confidential message instead of the length of the message.","In our proposed Differentially Private Compress-Then-Encrypt framework, we add a random positive amount of padding to the compressed file to ensure that any leakage satisfies the rigorous privacy guarantee of $(\\epsilon,\\delta)$-differential privacy.","The amount of padding that needs to be added depends on the sensitivity of the compression scheme to small changes in the input, i.e., to what degree can changing a single character of the input message impact the length of the compressed file.","While some popular compression schemes are highly sensitive to small changes in the input, we argue that effective data compression schemes do not necessarily have high sensitivity.","Our primary technical contribution is analyzing the fine-grained sensitivity of the LZ77 compression scheme (IEEE Trans.","Inf.","Theory 1977) which is one of the most common compression schemes used in practice.","We show that the global sensitivity of the LZ77 compression scheme has the upper bound $\\mathcal{O}(W^{2/3}\\log n)$ where $W\\leq n$ denotes the size of the sliding window.","When $W=n$, we show the lower bound $\\Omega(n^{2/3}\\log^{1/3}n)$ for the global sensitivity of the LZ77 compression scheme which is tight up to a sublogarithmic factor."],"url":"http://arxiv.org/abs/2502.09584v1"}
{"created":"2025-02-13 18:21:15","title":"Zero-shot generation of synthetic neurosurgical data with large language models","abstract":"Clinical data is fundamental to advance neurosurgical research, but access is often constrained by data availability, small sample sizes, privacy regulations, and resource-intensive preprocessing and de-identification procedures. Synthetic data offers a potential solution to challenges associated with accessing and using real-world data (RWD). This study aims to evaluate the capability of zero-shot generation of synthetic neurosurgical data with a large language model (LLM), GPT-4o, by benchmarking with the conditional tabular generative adversarial network (CTGAN). Synthetic datasets were compared to real-world neurosurgical data to assess fidelity (means, proportions, distributions, and bivariate correlations), utility (ML classifier performance on RWD), and privacy (duplication of records from RWD). The GPT-4o-generated datasets matched or exceeded CTGAN performance, despite no fine-tuning or access to RWD for pre-training. Datasets demonstrated high univariate and bivariate fidelity to RWD without directly exposing any real patient records, even at amplified sample size. Training an ML classifier on GPT-4o-generated data and testing on RWD for a binary prediction task showed an F1 score (0.706) with comparable performance to training on the CTGAN data (0.705) for predicting postoperative functional status deterioration. GPT-4o demonstrated a promising ability to generate high-fidelity synthetic neurosurgical data. These findings also indicate that data synthesized with GPT-4o can effectively augment clinical data with small sample sizes, and train ML models for prediction of neurosurgical outcomes. Further investigation is necessary to improve the preservation of distributional characteristics and boost classifier performance.","sentences":["Clinical data is fundamental to advance neurosurgical research, but access is often constrained by data availability, small sample sizes, privacy regulations, and resource-intensive preprocessing and de-identification procedures.","Synthetic data offers a potential solution to challenges associated with accessing and using real-world data (RWD).","This study aims to evaluate the capability of zero-shot generation of synthetic neurosurgical data with a large language model (LLM), GPT-4o, by benchmarking with the conditional tabular generative adversarial network (CTGAN).","Synthetic datasets were compared to real-world neurosurgical data to assess fidelity (means, proportions, distributions, and bivariate correlations), utility (ML classifier performance on RWD), and privacy (duplication of records from RWD).","The GPT-4o-generated datasets matched or exceeded CTGAN performance, despite no fine-tuning or access to RWD for pre-training.","Datasets demonstrated high univariate and bivariate fidelity to RWD without directly exposing any real patient records, even at amplified sample size.","Training an ML classifier on GPT-4o-generated data and testing on RWD for a binary prediction task showed an F1 score (0.706) with comparable performance to training on the CTGAN data (0.705) for predicting postoperative functional status deterioration.","GPT-4o demonstrated a promising ability to generate high-fidelity synthetic neurosurgical data.","These findings also indicate that data synthesized with GPT-4o can effectively augment clinical data with small sample sizes, and train ML models for prediction of neurosurgical outcomes.","Further investigation is necessary to improve the preservation of distributional characteristics and boost classifier performance."],"url":"http://arxiv.org/abs/2502.09566v1"}
{"created":"2025-02-13 18:17:03","title":"Diffusing DeBias: a Recipe for Turning a Bug into a Feature","abstract":"Deep learning model effectiveness in classification tasks is often challenged by the quality and quantity of training data which, whenever containing strong spurious correlations between specific attributes and target labels, can result in unrecoverable biases in model predictions. Tackling these biases is crucial in improving model generalization and trust, especially in real-world scenarios. This paper presents Diffusing DeBias (DDB), a novel approach acting as a plug-in for common methods in model debiasing while exploiting the inherent bias-learning tendency of diffusion models. Our approach leverages conditional diffusion models to generate synthetic bias-aligned images, used to train a bias amplifier model, to be further employed as an auxiliary method in different unsupervised debiasing approaches. Our proposed method, which also tackles the common issue of training set memorization typical of this type of tech- niques, beats current state-of-the-art in multiple benchmark datasets by significant margins, demonstrating its potential as a versatile and effective tool for tackling dataset bias in deep learning applications.","sentences":["Deep learning model effectiveness in classification tasks is often challenged by the quality and quantity of training data which, whenever containing strong spurious correlations between specific attributes and target labels, can result in unrecoverable biases in model predictions.","Tackling these biases is crucial in improving model generalization and trust, especially in real-world scenarios.","This paper presents Diffusing DeBias (DDB), a novel approach acting as a plug-in for common methods in model debiasing while exploiting the inherent bias-learning tendency of diffusion models.","Our approach leverages conditional diffusion models to generate synthetic bias-aligned images, used to train a bias amplifier model, to be further employed as an auxiliary method in different unsupervised debiasing approaches.","Our proposed method, which also tackles the common issue of training set memorization typical of this type of tech- niques, beats current state-of-the-art in multiple benchmark datasets by significant margins, demonstrating its potential as a versatile and effective tool for tackling dataset bias in deep learning applications."],"url":"http://arxiv.org/abs/2502.09564v1"}
{"created":"2025-02-13 17:57:05","title":"Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated Large-Scale Data Analytics","abstract":"Despite the high computational throughput of GPUs, limited memory capacity and bandwidth-limited CPU-GPU communication via PCIe links remain significant bottlenecks for accelerating large-scale data analytics workloads. This paper introduces Vortex, a GPU-accelerated framework designed for data analytics workloads that exceed GPU memory capacity. A key aspect of our framework is an optimized IO primitive that leverages all available PCIe links in multi-GPU systems for the IO demand of a single target GPU. It routes data through other GPUs to such target GPU that handles IO-intensive analytics tasks. This approach is advantageous when other GPUs are occupied with compute-bound workloads, such as popular AI applications that typically underutilize IO resources. We also introduce a novel programming model that separates GPU kernel development from IO scheduling, reducing programmer burden and enabling GPU code reuse. Additionally, we present the design of certain important query operators and discuss a late materialization technique based on GPU's zero-copy memory access. Without caching any data in GPU memory, Vortex improves the performance of the state-of-the-art GPU baseline, Proteus, by 5.7$\\times$ on average and enhances price performance by 2.5$\\times$ compared to a CPU-based DuckDB baseline.","sentences":["Despite the high computational throughput of GPUs, limited memory capacity and bandwidth-limited CPU-GPU communication via PCIe links remain significant bottlenecks for accelerating large-scale data analytics workloads.","This paper introduces Vortex, a GPU-accelerated framework designed for data analytics workloads that exceed GPU memory capacity.","A key aspect of our framework is an optimized IO primitive that leverages all available PCIe links in multi-GPU systems for the IO demand of a single target GPU.","It routes data through other GPUs to such target GPU that handles IO-intensive analytics tasks.","This approach is advantageous when other GPUs are occupied with compute-bound workloads, such as popular AI applications that typically underutilize IO resources.","We also introduce a novel programming model that separates GPU kernel development from IO scheduling, reducing programmer burden and enabling GPU code reuse.","Additionally, we present the design of certain important query operators and discuss a late materialization technique based on GPU's zero-copy memory access.","Without caching any data in GPU memory, Vortex improves the performance of the state-of-the-art GPU baseline, Proteus, by 5.7$\\times$ on average and enhances price performance by 2.5$\\times$ compared to a CPU-based DuckDB baseline."],"url":"http://arxiv.org/abs/2502.09541v1"}
{"created":"2025-02-13 17:50:58","title":"Entropy Collapse in Mobile Sensors: The Hidden Risks of Sensor-Based Security","abstract":"Mobile sensor data has been proposed for security-critical applications such as device pairing, proximity detection, and continuous authentication. However, the foundational assumption that these signals provide sufficient entropy remains under-explored. In this work, we systematically analyse the entropy of smartphone sensor data across four diverse datasets spanning multiple application contexts. Our findings reveal pervasive biases, with single-sensor mean min-entropy values ranging from 3.408-3.508 bits (S.D.=1.018-1.574), while conventional Shannon entropy is several multiples higher. We further demonstrate that correlations between sensor modalities reduce the worst-case entropy of using multiple sensors by up to approx. 75% compared to average-case Shannon entropy. This brings joint min-entropy well below 10 bits in many cases and, in the best case, yielding only approx. 24 bits of min-entropy when combining 20 sensor modalities. These results call into question the widely held assumption that adding more sensors inherently yields higher security. We ultimately caution against relying on raw sensor data as a primary source of randomness.","sentences":["Mobile sensor data has been proposed for security-critical applications such as device pairing, proximity detection, and continuous authentication.","However, the foundational assumption that these signals provide sufficient entropy remains under-explored.","In this work, we systematically analyse the entropy of smartphone sensor data across four diverse datasets spanning multiple application contexts.","Our findings reveal pervasive biases, with single-sensor mean min-entropy values ranging from 3.408-3.508 bits (S.D.=1.018-1.574), while conventional Shannon entropy is several multiples higher.","We further demonstrate that correlations between sensor modalities reduce the worst-case entropy of using multiple sensors by up to approx.","75% compared to average-case Shannon entropy.","This brings joint min-entropy well below 10 bits in many cases and, in the best case, yielding only approx.","24 bits of min-entropy when combining 20 sensor modalities.","These results call into question the widely held assumption that adding more sensors inherently yields higher security.","We ultimately caution against relying on raw sensor data as a primary source of randomness."],"url":"http://arxiv.org/abs/2502.09535v1"}
{"created":"2025-02-13 17:50:27","title":"Fast Tensor Completion via Approximate Richardson Iteration","abstract":"We study tensor completion (TC) through the lens of low-rank tensor decomposition (TD). Many TD algorithms use fast alternating minimization methods, which solve highly structured linear regression problems at each step (e.g., for CP, Tucker, and tensor-train decompositions). However, such algebraic structure is lost in TC regression problems, making direct extensions unclear. To address this, we propose a lifting approach that approximately solves TC regression problems using structured TD regression algorithms as blackbox subroutines, enabling sublinear-time methods. We theoretically analyze the convergence rate of our approximate Richardson iteration based algorithm, and we demonstrate on real-world tensors that its running time can be 100x faster than direct methods for CP completion.","sentences":["We study tensor completion (TC) through the lens of low-rank tensor decomposition (TD).","Many TD algorithms use fast alternating minimization methods, which solve highly structured linear regression problems at each step (e.g., for CP, Tucker, and tensor-train decompositions).","However, such algebraic structure is lost in TC regression problems, making direct extensions unclear.","To address this, we propose a lifting approach that approximately solves TC regression problems using structured TD regression algorithms as blackbox subroutines, enabling sublinear-time methods.","We theoretically analyze the convergence rate of our approximate Richardson iteration based algorithm, and we demonstrate on real-world tensors that its running time can be 100x faster than direct methods for CP completion."],"url":"http://arxiv.org/abs/2502.09534v1"}
{"created":"2025-02-13 17:37:42","title":"Robust Learning of Multi-index Models via Iterative Subspace Approximation","abstract":"We study the task of learning Multi-Index Models (MIMs) with label noise under the Gaussian distribution. A $K$-MIM is any function $f$ that only depends on a $K$-dimensional subspace. We focus on well-behaved MIMs with finite ranges that satisfy certain regularity properties. Our main contribution is a general robust learner that is qualitatively optimal in the Statistical Query (SQ) model. Our algorithm iteratively constructs better approximations to the defining subspace by computing low-degree moments conditional on the projection to the subspace computed thus far, and adding directions with relatively large empirical moments. This procedure efficiently finds a subspace $V$ so that $f(\\mathbf{x})$ is close to a function of the projection of $\\mathbf{x}$ onto $V$. Conversely, for functions for which these conditional moments do not help, we prove an SQ lower bound suggesting that no efficient learner exists.   As applications, we provide faster robust learners for the following concept classes:   * {\\bf Multiclass Linear Classifiers} We give a constant-factor approximate agnostic learner with sample complexity $N = O(d) 2^{\\mathrm{poly}(K/\\epsilon)}$ and computational complexity $\\mathrm{poly}(N ,d)$. This is the first constant-factor agnostic learner for this class whose complexity is a fixed-degree polynomial in $d$.   * {\\bf Intersections of Halfspaces} We give an approximate agnostic learner for this class achieving 0-1 error $K \\tilde{O}(\\mathrm{OPT}) + \\epsilon$ with sample complexity $N=O(d^2) 2^{\\mathrm{poly}(K/\\epsilon)}$ and computational complexity $\\mathrm{poly}(N ,d)$. This is the first agnostic learner for this class with near-linear error dependence and complexity a fixed-degree polynomial in $d$.   Furthermore, we show that in the presence of random classification noise, the complexity of our algorithm scales polynomially with $1/\\epsilon$.","sentences":["We study the task of learning Multi-Index Models (MIMs) with label noise under the Gaussian distribution.","A $K$-MIM is any function $f$ that only depends on a $K$-dimensional subspace.","We focus on well-behaved MIMs with finite ranges that satisfy certain regularity properties.","Our main contribution is a general robust learner that is qualitatively optimal in the Statistical Query (SQ) model.","Our algorithm iteratively constructs better approximations to the defining subspace by computing low-degree moments conditional on the projection to the subspace computed thus far, and adding directions with relatively large empirical moments.","This procedure efficiently finds a subspace $V$ so that $f(\\mathbf{x})$ is close to a function of the projection of $\\mathbf{x}$ onto $V$. Conversely, for functions for which these conditional moments do not help, we prove an SQ lower bound suggesting that no efficient learner exists.   ","As applications, we provide faster robust learners for the following concept classes:   * {\\bf Multiclass Linear Classifiers} We give a constant-factor approximate agnostic learner with sample complexity $N = O(d) 2^{\\mathrm{poly}(K/\\epsilon)}$ and computational complexity $\\mathrm{poly}(N ,d)$.","This is the first constant-factor agnostic learner for this class whose complexity is a fixed-degree polynomial in $d$.   * {\\bf Intersections of Halfspaces} We give an approximate agnostic learner for this class achieving 0-1 error $K \\tilde{O}(\\mathrm{OPT})","+ \\epsilon$ with sample complexity $N=O(d^2) 2^{\\mathrm{poly}(K/\\epsilon)}$ and computational complexity $\\mathrm{poly}(N ,d)$.","This is the first agnostic learner for this class with near-linear error dependence and complexity a fixed-degree polynomial in $d$.   ","Furthermore, we show that in the presence of random classification noise, the complexity of our algorithm scales polynomially with $1/\\epsilon$."],"url":"http://arxiv.org/abs/2502.09525v1"}
{"created":"2025-02-13 17:36:12","title":"Forward-backward Contention Resolution Schemes for Fair Rationing","abstract":"We use contention resolution schemes (CRS) to derive algorithms for the fair rationing of a single resource when agents have stochastic demands. We aim to provide ex-ante guarantees on the level of service provided to each agent, who may measure service in different ways (Type-I, II, or III), calling for CRS under different feasibility constraints (rank-1 matroid or knapsack). We are particularly interested in two-order CRS where the agents are equally likely to arrive in a known forward order or its reverse, which is motivated by online rationing at food banks.   In particular, we derive a two-order CRS for rank-1 matroids with guarantee $1/(1+e^{-1/2})\\approx 0.622$, which we prove is tight. This improves upon the $1/2$ guarantee that is best-possible under a single order (Alaei, SIAM J. Comput. 2014), while achieving separation with the $1-1/e\\approx 0.632$ guarantee that is possible for random-order CRS (Lee and Singla, ESA 2018). Because CRS guarantees imply prophet inequalities, this also beats the two-order prophet inequality with ratio $(\\sqrt{5}-1)/2\\approx 0.618$ from (Arsenis, SODA 2021), which was tight for single-threshold policies. Rank-1 matroids suffice to provide guarantees under Type-II or III service, but Type-I service requires knapsack. Accordingly, we derive a two-order CRS for knapsack with guarantee $1/3$, improving upon the $1/(3+e^{-2})\\approx 0.319$ guarantee that is best-possible under a single order (Jiang et al., SODA 2022). To our knowledge, $1/3$ provides the best-known guarantee for knapsack CRS even in the offline setting. Finally, we provide an upper bound of $1/(2+e^{-1})\\approx 0.422$ for two-order knapsack CRS, strictly smaller than the upper bound of $(1-e^{-2})/2\\approx0.432$ for random-order knapsack CRS.","sentences":["We use contention resolution schemes (CRS) to derive algorithms for the fair rationing of a single resource when agents have stochastic demands.","We aim to provide ex-ante guarantees on the level of service provided to each agent, who may measure service in different ways (Type-I, II, or III), calling for CRS under different feasibility constraints (rank-1 matroid or knapsack).","We are particularly interested in two-order CRS where the agents are equally likely to arrive in a known forward order or its reverse, which is motivated by online rationing at food banks.   ","In particular, we derive a two-order CRS for rank-1 matroids with guarantee $1/(1+e^{-1/2})\\approx 0.622$, which we prove is tight.","This improves upon the $1/2$ guarantee that is best-possible under a single order (Alaei, SIAM J. Comput. 2014), while achieving separation with the $1-1/e\\approx 0.632$ guarantee that is possible for random-order CRS (Lee and Singla, ESA 2018).","Because CRS guarantees imply prophet inequalities, this also beats the two-order prophet inequality with ratio $(\\sqrt{5}-1)/2\\approx 0.618$ from (Arsenis, SODA 2021), which was tight for single-threshold policies.","Rank-1 matroids suffice to provide guarantees under Type-II or III service, but Type-I service requires knapsack.","Accordingly, we derive a two-order CRS for knapsack with guarantee $1/3$, improving upon the $1/(3+e^{-2})\\approx 0.319$ guarantee that is best-possible under a single order (Jiang et al., SODA 2022).","To our knowledge, $1/3$ provides the best-known guarantee for knapsack CRS even in the offline setting.","Finally, we provide an upper bound of $1/(2+e^{-1})\\approx 0.422$ for two-order knapsack CRS, strictly smaller than the upper bound of $(1-e^{-2})/2\\approx0.432$ for random-order knapsack CRS."],"url":"http://arxiv.org/abs/2502.09521v1"}
{"created":"2025-02-13 17:22:50","title":"Diffusion Models for Molecules: A Survey of Methods and Tasks","abstract":"Generative tasks about molecules, including but not limited to molecule generation, are crucial for drug discovery and material design, and have consistently attracted significant attention. In recent years, diffusion models have emerged as an impressive class of deep generative models, sparking extensive research and leading to numerous studies on their application to molecular generative tasks. Despite the proliferation of related work, there remains a notable lack of up-to-date and systematic surveys in this area. Particularly, due to the diversity of diffusion model formulations, molecular data modalities, and generative task types, the research landscape is challenging to navigate, hindering understanding and limiting the area's growth. To address this, this paper conducts a comprehensive survey of diffusion model-based molecular generative methods. We systematically review the research from the perspectives of methodological formulations, data modalities, and task types, offering a novel taxonomy. This survey aims to facilitate understanding and further flourishing development in this area. The relevant papers are summarized at: https://github.com/AzureLeon1/awesome-molecular-diffusion-models.","sentences":["Generative tasks about molecules, including but not limited to molecule generation, are crucial for drug discovery and material design, and have consistently attracted significant attention.","In recent years, diffusion models have emerged as an impressive class of deep generative models, sparking extensive research and leading to numerous studies on their application to molecular generative tasks.","Despite the proliferation of related work, there remains a notable lack of up-to-date and systematic surveys in this area.","Particularly, due to the diversity of diffusion model formulations, molecular data modalities, and generative task types, the research landscape is challenging to navigate, hindering understanding and limiting the area's growth.","To address this, this paper conducts a comprehensive survey of diffusion model-based molecular generative methods.","We systematically review the research from the perspectives of methodological formulations, data modalities, and task types, offering a novel taxonomy.","This survey aims to facilitate understanding and further flourishing development in this area.","The relevant papers are summarized at: https://github.com/AzureLeon1/awesome-molecular-diffusion-models."],"url":"http://arxiv.org/abs/2502.09511v1"}
{"created":"2025-02-13 17:21:37","title":"When and How Does CLIP Enable Domain and Compositional Generalization?","abstract":"The remarkable generalization performance of contrastive vision-language models like CLIP is often attributed to the diversity of their training distributions. However, key questions remain unanswered: Can CLIP generalize to an entirely unseen domain when trained on a diverse mixture of domains (domain generalization)? Can it generalize to unseen classes within partially seen domains (compositional generalization)? What factors affect such generalization? To answer these questions, we trained CLIP models on systematically constructed training distributions with controlled domain diversity and object class exposure. Our experiments show that domain diversity is essential for both domain and compositional generalization, yet compositional generalization can be surprisingly weaker than domain generalization when the training distribution contains a suboptimal subset of the test domain. Through data-centric and mechanistic analyses, we find that successful generalization requires learning of shared representations already in intermediate layers and shared circuitry.","sentences":["The remarkable generalization performance of contrastive vision-language models like CLIP is often attributed to the diversity of their training distributions.","However, key questions remain unanswered: Can CLIP generalize to an entirely unseen domain when trained on a diverse mixture of domains (domain generalization)?","Can it generalize to unseen classes within partially seen domains (compositional generalization)?","What factors affect such generalization?","To answer these questions, we trained CLIP models on systematically constructed training distributions with controlled domain diversity and object class exposure.","Our experiments show that domain diversity is essential for both domain and compositional generalization, yet compositional generalization can be surprisingly weaker than domain generalization when the training distribution contains a suboptimal subset of the test domain.","Through data-centric and mechanistic analyses, we find that successful generalization requires learning of shared representations already in intermediate layers and shared circuitry."],"url":"http://arxiv.org/abs/2502.09507v1"}
{"created":"2025-02-13 17:13:46","title":"Prior-Constrained Association Learning for Fine-Grained Generalized Category Discovery","abstract":"This paper addresses generalized category discovery (GCD), the task of clustering unlabeled data from potentially known or unknown categories with the help of labeled instances from each known category. Compared to traditional semi-supervised learning, GCD is more challenging because unlabeled data could be from novel categories not appearing in labeled data. Current state-of-the-art methods typically learn a parametric classifier assisted by self-distillation. While being effective, these methods do not make use of cross-instance similarity to discover class-specific semantics which are essential for representation learning and category discovery. In this paper, we revisit the association-based paradigm and propose a Prior-constrained Association Learning method to capture and learn the semantic relations within data. In particular, the labeled data from known categories provides a unique prior for the association of unlabeled data. Unlike previous methods that only adopts the prior as a pre or post-clustering refinement, we fully incorporate the prior into the association process, and let it constrain the association towards a reliable grouping outcome. The estimated semantic groups are utilized through non-parametric prototypical contrast to enhance the representation learning. A further combination of both parametric and non-parametric classification complements each other and leads to a model that outperforms existing methods by a significant margin. On multiple GCD benchmarks, we perform extensive experiments and validate the effectiveness of our proposed method.","sentences":["This paper addresses generalized category discovery (GCD), the task of clustering unlabeled data from potentially known or unknown categories with the help of labeled instances from each known category.","Compared to traditional semi-supervised learning, GCD is more challenging because unlabeled data could be from novel categories not appearing in labeled data.","Current state-of-the-art methods typically learn a parametric classifier assisted by self-distillation.","While being effective, these methods do not make use of cross-instance similarity to discover class-specific semantics which are essential for representation learning and category discovery.","In this paper, we revisit the association-based paradigm and propose a Prior-constrained Association Learning method to capture and learn the semantic relations within data.","In particular, the labeled data from known categories provides a unique prior for the association of unlabeled data.","Unlike previous methods that only adopts the prior as a pre or post-clustering refinement, we fully incorporate the prior into the association process, and let it constrain the association towards a reliable grouping outcome.","The estimated semantic groups are utilized through non-parametric prototypical contrast to enhance the representation learning.","A further combination of both parametric and non-parametric classification complements each other and leads to a model that outperforms existing methods by a significant margin.","On multiple GCD benchmarks, we perform extensive experiments and validate the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2502.09501v1"}
{"created":"2025-02-13 17:10:43","title":"Eidetic Learning: an Efficient and Provable Solution to Catastrophic Forgetting","abstract":"Catastrophic forgetting -- the phenomenon of a neural network learning a task t1 and losing the ability to perform it after being trained on some other task t2 -- is a long-standing problem for neural networks [McCloskey and Cohen, 1989]. We present a method, Eidetic Learning, that provably solves catastrophic forgetting. A network trained with Eidetic Learning -- here, an EideticNet -- requires no rehearsal or replay. We consider successive discrete tasks and show how at inference time an EideticNet automatically routes new instances without auxiliary task information. An EideticNet bears a family resemblance to the sparsely-gated Mixture-of-Experts layer Shazeer et al. [2016] in that network capacity is partitioned across tasks and the network itself performs data-conditional routing. An EideticNet is easy to implement and train, is efficient, and has time and space complexity linear in the number of parameters. The guarantee of our method holds for normalization layers of modern neural networks during both pre-training and fine-tuning. We show with a variety of network architectures and sets of tasks that EideticNets are immune to forgetting. While the practical benefits of EideticNets are substantial, we believe they can be benefit practitioners and theorists alike. The code for training EideticNets is available at \\href{https://github.com/amazon-science/eideticnet-training}{this https URL}.","sentences":["Catastrophic forgetting -- the phenomenon of a neural network learning a task t1 and losing the ability to perform it after being trained on some other task t2 -- is a long-standing problem for neural networks [McCloskey and Cohen, 1989].","We present a method, Eidetic Learning, that provably solves catastrophic forgetting.","A network trained with Eidetic Learning -- here, an EideticNet -- requires no rehearsal or replay.","We consider successive discrete tasks and show how at inference time an EideticNet automatically routes new instances without auxiliary task information.","An EideticNet bears a family resemblance to the sparsely-gated Mixture-of-Experts layer Shazeer et al.","[2016] in that network capacity is partitioned across tasks and the network itself performs data-conditional routing.","An EideticNet is easy to implement and train, is efficient, and has time and space complexity linear in the number of parameters.","The guarantee of our method holds for normalization layers of modern neural networks during both pre-training and fine-tuning.","We show with a variety of network architectures and sets of tasks that EideticNets are immune to forgetting.","While the practical benefits of EideticNets are substantial, we believe they can be benefit practitioners and theorists alike.","The code for training EideticNets is available at \\href{https://github.com/amazon-science/eideticnet-training}{this https URL}."],"url":"http://arxiv.org/abs/2502.09500v1"}
{"created":"2025-02-13 16:57:07","title":"Inverse Design with Dynamic Mode Decomposition","abstract":"We introduce a computationally efficient method for the automation of inverse design in science and engineering. Based on simple least-square regression, the underlying dynamic mode decomposition algorithm can be used to construct a low-rank subspace spanning multiple experiments in parameter space. The proposed inverse design dynamic mode composition (ID-DMD) algorithm leverages the computed low-dimensional subspace to enable fast digital design and optimization on laptop-level computing, including the potential to prescribe the dynamics themselves. Moreover, the method is robust to noise, physically interpretable, and can provide uncertainty quantification metrics. The architecture can also efficiently scale to large-scale design problems using randomized algorithms in the ID-DMD. The simplicity of the method and its implementation are highly attractive in practice, and the ID-DMD has been demonstrated to be an order of magnitude more accurate than competing methods while simultaneously being 3-5 orders faster on challenging engineering design problems ranging from structural vibrations to fluid dynamics. Due to its speed, robustness, interpretability, and ease-of-use, ID-DMD in comparison with other leading machine learning methods represents a significant advancement in data-driven methods for inverse design and optimization, promising a paradigm shift in how to approach inverse design in practice.","sentences":["We introduce a computationally efficient method for the automation of inverse design in science and engineering.","Based on simple least-square regression, the underlying dynamic mode decomposition algorithm can be used to construct a low-rank subspace spanning multiple experiments in parameter space.","The proposed inverse design dynamic mode composition (ID-DMD) algorithm leverages the computed low-dimensional subspace to enable fast digital design and optimization on laptop-level computing, including the potential to prescribe the dynamics themselves.","Moreover, the method is robust to noise, physically interpretable, and can provide uncertainty quantification metrics.","The architecture can also efficiently scale to large-scale design problems using randomized algorithms in the ID-DMD.","The simplicity of the method and its implementation are highly attractive in practice, and the ID-DMD has been demonstrated to be an order of magnitude more accurate than competing methods while simultaneously being 3-5 orders faster on challenging engineering design problems ranging from structural vibrations to fluid dynamics.","Due to its speed, robustness, interpretability, and ease-of-use, ID-DMD in comparison with other leading machine learning methods represents a significant advancement in data-driven methods for inverse design and optimization, promising a paradigm shift in how to approach inverse design in practice."],"url":"http://arxiv.org/abs/2502.09490v1"}
{"created":"2025-02-13 16:45:39","title":"Standardisation of Convex Ultrasound Data Through Geometric Analysis and Augmentation","abstract":"The application of ultrasound in healthcare has seen increased diversity and importance. Unlike other medical imaging modalities, ultrasound research and development has historically lagged, particularly in the case of applications with data-driven algorithms. A significant issue with ultrasound is the extreme variability of the images, due to the number of different machines available and the possible combination of parameter settings. One outcome of this is the lack of standardised and benchmarking ultrasound datasets. The method proposed in this article is an approach to alleviating this issue of disorganisation. For this purpose, the issue of ultrasound data sparsity is examined and a novel perspective, approach, and solution is proposed; involving the extraction of the underlying ultrasound plane within the image and representing it using annulus sector geometry. An application of this methodology is proposed, which is the extraction of scan lines and the linearisation of convex planes. Validation of the robustness of the proposed method is performed on both private and public data. The impact of deformation and the invertibility of augmentation using the estimated annulus sector parameters is also studied. Keywords: Ultrasound, Annulus Sector, Augmentation, Linearisation.","sentences":["The application of ultrasound in healthcare has seen increased diversity and importance.","Unlike other medical imaging modalities, ultrasound research and development has historically lagged, particularly in the case of applications with data-driven algorithms.","A significant issue with ultrasound is the extreme variability of the images, due to the number of different machines available and the possible combination of parameter settings.","One outcome of this is the lack of standardised and benchmarking ultrasound datasets.","The method proposed in this article is an approach to alleviating this issue of disorganisation.","For this purpose, the issue of ultrasound data sparsity is examined and a novel perspective, approach, and solution is proposed; involving the extraction of the underlying ultrasound plane within the image and representing it using annulus sector geometry.","An application of this methodology is proposed, which is the extraction of scan lines and the linearisation of convex planes.","Validation of the robustness of the proposed method is performed on both private and public data.","The impact of deformation and the invertibility of augmentation using the estimated annulus sector parameters is also studied.","Keywords: Ultrasound, Annulus Sector, Augmentation, Linearisation."],"url":"http://arxiv.org/abs/2502.09482v1"}
{"created":"2025-02-13 16:27:23","title":"Metamorphic Testing for Pose Estimation Systems","abstract":"Pose estimation systems are used in a variety of fields, from sports analytics to livestock care. Given their potential impact, it is paramount to systematically test their behaviour and potential for failure. This is a complex task due to the oracle problem and the high cost of manual labelling necessary to build ground truth keypoints. This problem is exacerbated by the fact that different applications require systems to focus on different subjects (e.g., human versus animal) or landmarks (e.g., only extremities versus whole body and face), which makes labelled test data rarely reusable. To combat these problems we propose MET-POSE, a metamorphic testing framework for pose estimation systems that bypasses the need for manual annotation while assessing the performance of these systems under different circumstances. MET-POSE thus allows users of pose estimation systems to assess the systems in conditions that more closely relate to their application without having to label an ad-hoc test dataset or rely only on available datasets, which may not be adapted to their application domain. While we define MET-POSE in general terms, we also present a non-exhaustive list of metamorphic rules that represent common challenges in computer vision applications, as well as a specific way to evaluate these rules. We then experimentally show the effectiveness of MET-POSE by applying it to Mediapipe Holistic, a state of the art human pose estimation system, with the FLIC and PHOENIX datasets. With these experiments, we outline numerous ways in which the outputs of MET-POSE can uncover faults in pose estimation systems at a similar or higher rate than classic testing using hand labelled data, and show that users can tailor the rule set they use to the faults and level of accuracy relevant to their application.","sentences":["Pose estimation systems are used in a variety of fields, from sports analytics to livestock care.","Given their potential impact, it is paramount to systematically test their behaviour and potential for failure.","This is a complex task due to the oracle problem and the high cost of manual labelling necessary to build ground truth keypoints.","This problem is exacerbated by the fact that different applications require systems to focus on different subjects (e.g., human versus animal) or landmarks (e.g., only extremities versus whole body and face), which makes labelled test data rarely reusable.","To combat these problems we propose MET-POSE, a metamorphic testing framework for pose estimation systems that bypasses the need for manual annotation while assessing the performance of these systems under different circumstances.","MET-POSE thus allows users of pose estimation systems to assess the systems in conditions that more closely relate to their application without having to label an ad-hoc test dataset or rely only on available datasets, which may not be adapted to their application domain.","While we define MET-POSE in general terms, we also present a non-exhaustive list of metamorphic rules that represent common challenges in computer vision applications, as well as a specific way to evaluate these rules.","We then experimentally show the effectiveness of MET-POSE by applying it to Mediapipe Holistic, a state of the art human pose estimation system, with the FLIC and PHOENIX datasets.","With these experiments, we outline numerous ways in which the outputs of MET-POSE can uncover faults in pose estimation systems at a similar or higher rate than classic testing using hand labelled data, and show that users can tailor the rule set they use to the faults and level of accuracy relevant to their application."],"url":"http://arxiv.org/abs/2502.09460v1"}
{"created":"2025-02-13 16:25:16","title":"The Multilingual Mind : A Survey of Multilingual Reasoning in Language Models","abstract":"While reasoning and multilingual capabilities in Language Models (LMs) have achieved remarkable progress in recent years, their integration into a unified paradigm, multilingual reasoning, is at a nascent stage. Multilingual reasoning requires language models to handle logical reasoning across languages while addressing misalignment, biases, and challenges in low-resource settings. This survey provides the first in-depth review of multilingual reasoning in LMs. In this survey, we provide a systematic overview of existing methods that leverage LMs for multilingual reasoning, specifically outlining the challenges, motivations, and foundational aspects of applying language models to reason across diverse languages. We provide an overview of the standard data resources used for training multilingual reasoning in LMs and the evaluation benchmarks employed to assess their multilingual capabilities. Next, we analyze various state-of-the-art methods and their performance on these benchmarks. Finally, we explore future research opportunities to improve multilingual reasoning in LMs, focusing on enhancing their ability to handle diverse languages and complex reasoning tasks.","sentences":["While reasoning and multilingual capabilities in Language Models (LMs) have achieved remarkable progress in recent years, their integration into a unified paradigm, multilingual reasoning, is at a nascent stage.","Multilingual reasoning requires language models to handle logical reasoning across languages while addressing misalignment, biases, and challenges in low-resource settings.","This survey provides the first in-depth review of multilingual reasoning in LMs.","In this survey, we provide a systematic overview of existing methods that leverage LMs for multilingual reasoning, specifically outlining the challenges, motivations, and foundational aspects of applying language models to reason across diverse languages.","We provide an overview of the standard data resources used for training multilingual reasoning in LMs and the evaluation benchmarks employed to assess their multilingual capabilities.","Next, we analyze various state-of-the-art methods and their performance on these benchmarks.","Finally, we explore future research opportunities to improve multilingual reasoning in LMs, focusing on enhancing their ability to handle diverse languages and complex reasoning tasks."],"url":"http://arxiv.org/abs/2502.09457v1"}
{"created":"2025-02-13 16:17:57","title":"Spiking Neural Networks for Temporal Processing: Status Quo and Future Prospects","abstract":"Temporal processing is fundamental for both biological and artificial intelligence systems, as it enables the comprehension of dynamic environments and facilitates timely responses. Spiking Neural Networks (SNNs) excel in handling such data with high efficiency, owing to their rich neuronal dynamics and sparse activity patterns. Given the recent surge in the development of SNNs, there is an urgent need for a comprehensive evaluation of their temporal processing capabilities. In this paper, we first conduct an in-depth assessment of commonly used neuromorphic benchmarks, revealing critical limitations in their ability to evaluate the temporal processing capabilities of SNNs. To bridge this gap, we further introduce a benchmark suite consisting of three temporal processing tasks characterized by rich temporal dynamics across multiple timescales. Utilizing this benchmark suite, we perform a thorough evaluation of recently introduced SNN approaches to elucidate the current status of SNNs in temporal processing. Our findings indicate significant advancements in recently developed spiking neuron models and neural architectures regarding their temporal processing capabilities, while also highlighting a performance gap in handling long-range dependencies when compared to state-of-the-art non-spiking models. Finally, we discuss the key challenges and outline potential avenues for future research.","sentences":["Temporal processing is fundamental for both biological and artificial intelligence systems, as it enables the comprehension of dynamic environments and facilitates timely responses.","Spiking Neural Networks (SNNs) excel in handling such data with high efficiency, owing to their rich neuronal dynamics and sparse activity patterns.","Given the recent surge in the development of SNNs, there is an urgent need for a comprehensive evaluation of their temporal processing capabilities.","In this paper, we first conduct an in-depth assessment of commonly used neuromorphic benchmarks, revealing critical limitations in their ability to evaluate the temporal processing capabilities of SNNs.","To bridge this gap, we further introduce a benchmark suite consisting of three temporal processing tasks characterized by rich temporal dynamics across multiple timescales.","Utilizing this benchmark suite, we perform a thorough evaluation of recently introduced SNN approaches to elucidate the current status of SNNs in temporal processing.","Our findings indicate significant advancements in recently developed spiking neuron models and neural architectures regarding their temporal processing capabilities, while also highlighting a performance gap in handling long-range dependencies when compared to state-of-the-art non-spiking models.","Finally, we discuss the key challenges and outline potential avenues for future research."],"url":"http://arxiv.org/abs/2502.09449v1"}
{"created":"2025-02-13 16:16:54","title":"Pixel-Level Reasoning Segmentation via Multi-turn Conversations","abstract":"Existing visual perception systems focus on region-level segmentation in single-turn dialogues, relying on complex and explicit query instructions. Such systems cannot reason at the pixel level and comprehend dynamic user intent that changes over interaction. Our work tackles this issue by introducing a novel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on multi-turn conversations, tracking evolving user intent via multi-turn interactions for fine-grained segmentation. To establish a benchmark for this novel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on Multi-Turn Conversations (PRIST), comprising 24k utterances from 8.3k multi-turn conversational scenarios with segmentation targets. Building on PRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning Segmentation framework, integrates pixel-level segmentation with robust multi-turn conversation understanding, generating pixel-grounded explanations aligned with user intent. The PRIST dataset and MIRSA framework fill the gap in pixel-level reasoning segmentation. Experimental results on the PRIST dataset demonstrate that our method outperforms current segmentation-specific baselines in terms of segmentation and LLM-based reasoning metrics. The code and data are available at: https://github.com/ccccai239/PixelRIST.","sentences":["Existing visual perception systems focus on region-level segmentation in single-turn dialogues, relying on complex and explicit query instructions.","Such systems cannot reason at the pixel level and comprehend dynamic user intent that changes over interaction.","Our work tackles this issue by introducing a novel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on multi-turn conversations, tracking evolving user intent via multi-turn interactions for fine-grained segmentation.","To establish a benchmark for this novel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on Multi-Turn Conversations (PRIST), comprising 24k utterances from 8.3k multi-turn conversational scenarios with segmentation targets.","Building on PRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning Segmentation framework, integrates pixel-level segmentation with robust multi-turn conversation understanding, generating pixel-grounded explanations aligned with user intent.","The PRIST dataset and MIRSA framework fill the gap in pixel-level reasoning segmentation.","Experimental results on the PRIST dataset demonstrate that our method outperforms current segmentation-specific baselines in terms of segmentation and LLM-based reasoning metrics.","The code and data are available at: https://github.com/ccccai239/PixelRIST."],"url":"http://arxiv.org/abs/2502.09447v1"}
{"created":"2025-02-13 16:12:17","title":"Relational Conformal Prediction for Correlated Time Series","abstract":"We address the problem of uncertainty quantification in time series forecasting by exploiting observations at correlated sequences. Relational deep learning methods leveraging graph representations are among the most effective tools for obtaining point estimates from spatiotemporal data and correlated time series. However, the problem of exploiting relational structures to estimate the uncertainty of such predictions has been largely overlooked in the same context. To this end, we propose a novel distribution-free approach based on the conformal prediction framework and quantile regression. Despite the recent applications of conformal prediction to sequential data, existing methods operate independently on each target time series and do not account for relationships among them when constructing the prediction interval. We fill this void by introducing a novel conformal prediction method based on graph deep learning operators. Our method, named Conformal Relational Prediction (CoRel), does not require the relational structure (graph) to be known as a prior and can be applied on top of any pre-trained time series predictor. Additionally, CoRel includes an adaptive component to handle non-exchangeable data and changes in the input time series. Our approach provides accurate coverage and archives state-of-the-art uncertainty quantification in relevant benchmarks.","sentences":["We address the problem of uncertainty quantification in time series forecasting by exploiting observations at correlated sequences.","Relational deep learning methods leveraging graph representations are among the most effective tools for obtaining point estimates from spatiotemporal data and correlated time series.","However, the problem of exploiting relational structures to estimate the uncertainty of such predictions has been largely overlooked in the same context.","To this end, we propose a novel distribution-free approach based on the conformal prediction framework and quantile regression.","Despite the recent applications of conformal prediction to sequential data, existing methods operate independently on each target time series and do not account for relationships among them when constructing the prediction interval.","We fill this void by introducing a novel conformal prediction method based on graph deep learning operators.","Our method, named Conformal Relational Prediction (CoRel), does not require the relational structure (graph) to be known as a prior and can be applied on top of any pre-trained time series predictor.","Additionally, CoRel includes an adaptive component to handle non-exchangeable data and changes in the input time series.","Our approach provides accurate coverage and archives state-of-the-art uncertainty quantification in relevant benchmarks."],"url":"http://arxiv.org/abs/2502.09443v1"}
{"created":"2025-02-13 16:06:52","title":"Deterministic Independent Sets in the Semi-Streaming Model","abstract":"We consider the independent set problem in the semi-streaming model. For any input graph $G=(V, E)$ with $n$ vertices, an independent set is a set of vertices with no edges between any two elements. In the semi-streaming model, $G$ is presented as a stream of edges and any algorithm must use $\\tilde O(n)$ bits of memory to output a large independent set at the end of the stream.   Prior work has designed various semi-streaming algorithms for finding independent sets. Due to the hardness of finding maximum and maximal independent sets in the semi-streaming model, the focus has primarily been on finding independent sets in terms of certain parameters, such as the maximum degree $\\Delta$. In particular, there is a simple randomized algorithm that obtains independent sets of size $\\frac n{\\Delta+1}$ in expectation, which can also be achieved with high probability using more complicated algorithms. For deterministic algorithms, the best bounds are significantly weaker. In fact, the best we currently know is a straightforward algorithm that finds an $\\tilde\\Omega\\left(\\frac n{\\Delta^2}\\right)$ size independent set.   We show that this straightforward algorithm is nearly optimal by proving that any deterministic semi-streaming algorithm can only output an $\\tilde O\\left(\\frac n{\\Delta^2}\\right)$ size independent set. Our result proves a strong separation between the power of deterministic and randomized semi-streaming algorithms for the independent set problem.","sentences":["We consider the independent set problem in the semi-streaming model.","For any input graph $G=(V, E)$ with $n$ vertices, an independent set is a set of vertices with no edges between any two elements.","In the semi-streaming model, $G$ is presented as a stream of edges and any algorithm must use $\\tilde O(n)$ bits of memory to output a large independent set at the end of the stream.   ","Prior work has designed various semi-streaming algorithms for finding independent sets.","Due to the hardness of finding maximum and maximal independent sets in the semi-streaming model, the focus has primarily been on finding independent sets in terms of certain parameters, such as the maximum degree $\\Delta$.","In particular, there is a simple randomized algorithm that obtains independent sets of size $\\frac n{\\Delta+1}$ in expectation, which can also be achieved with high probability using more complicated algorithms.","For deterministic algorithms, the best bounds are significantly weaker.","In fact, the best we currently know is a straightforward algorithm that finds an $\\tilde\\Omega\\left(\\frac n{\\Delta^2}\\right)$ size independent set.   ","We show that this straightforward algorithm is nearly optimal by proving that any deterministic semi-streaming algorithm can only output an $\\tilde O\\left(\\frac n{\\Delta^2}\\right)$ size independent set.","Our result proves a strong separation between the power of deterministic and randomized semi-streaming algorithms for the independent set problem."],"url":"http://arxiv.org/abs/2502.09440v1"}
{"created":"2025-02-13 15:56:44","title":"Redistribute Ensemble Training for Mitigating Memorization in Diffusion Models","abstract":"Diffusion models, known for their tremendous ability to generate high-quality samples, have recently raised concerns due to their data memorization behavior, which poses privacy risks. Recent methods for memory mitigation have primarily addressed the issue within the context of the text modality in cross-modal generation tasks, restricting their applicability to specific conditions. In this paper, we propose a novel method for diffusion models from the perspective of visual modality, which is more generic and fundamental for mitigating memorization. Directly exposing visual data to the model increases memorization risk, so we design a framework where models learn through proxy model parameters instead. Specially, the training dataset is divided into multiple shards, with each shard training a proxy model, then aggregated to form the final model. Additionally, practical analysis of training losses illustrates that the losses for easily memorable images tend to be obviously lower. Thus, we skip the samples with abnormally low loss values from the current mini-batch to avoid memorizing. However, balancing the need to skip memorization-prone samples while maintaining sufficient training data for high-quality image generation presents a key challenge. Thus, we propose IET-AGC+, which redistributes highly memorizable samples between shards, to mitigate these samples from over-skipping. Furthermore, we dynamically augment samples based on their loss values to further reduce memorization. Extensive experiments and analysis on four datasets show that our method successfully reduces memory capacity while maintaining performance. Moreover, we fine-tune the pre-trained diffusion models, e.g., Stable Diffusion, and decrease the memorization score by 46.7\\%, demonstrating the effectiveness of our method. Code is available in: https://github.com/liuxiao-guan/IET_AGC.","sentences":["Diffusion models, known for their tremendous ability to generate high-quality samples, have recently raised concerns due to their data memorization behavior, which poses privacy risks.","Recent methods for memory mitigation have primarily addressed the issue within the context of the text modality in cross-modal generation tasks, restricting their applicability to specific conditions.","In this paper, we propose a novel method for diffusion models from the perspective of visual modality, which is more generic and fundamental for mitigating memorization.","Directly exposing visual data to the model increases memorization risk, so we design a framework where models learn through proxy model parameters instead.","Specially, the training dataset is divided into multiple shards, with each shard training a proxy model, then aggregated to form the final model.","Additionally, practical analysis of training losses illustrates that the losses for easily memorable images tend to be obviously lower.","Thus, we skip the samples with abnormally low loss values from the current mini-batch to avoid memorizing.","However, balancing the need to skip memorization-prone samples while maintaining sufficient training data for high-quality image generation presents a key challenge.","Thus, we propose IET-AGC+, which redistributes highly memorizable samples between shards, to mitigate these samples from over-skipping.","Furthermore, we dynamically augment samples based on their loss values to further reduce memorization.","Extensive experiments and analysis on four datasets show that our method successfully reduces memory capacity while maintaining performance.","Moreover, we fine-tune the pre-trained diffusion models, e.g., Stable Diffusion, and decrease the memorization score by 46.7\\%, demonstrating the effectiveness of our method.","Code is available in: https://github.com/liuxiao-guan/IET_AGC."],"url":"http://arxiv.org/abs/2502.09434v1"}
{"created":"2025-02-13 15:53:34","title":"On Usage of Non-Volatile Memory as Primary Storage for Database Management Systems","abstract":"This paper explores the implications of employing non-volatile memory (NVM) as primary storage for a data base management system (DBMS). We investigate the modifications necessary to be applied on top of a traditional relational DBMS to take advantage of NVM features. As a case study, we modify the storage engine (SE) of PostgreSQL enabling efficient use of NVM hardware. We detail the necessary changes and challenges such modifications entail and evaluate them using a comprehensive emulation platform. Results indicate that our modified SE reduces query execution time by up to 45% and 13% when compared to disk and NVM storage, with average reductions of 19% and 4%, respectively. Detailed analysis of these results shows that while our modified SE is able to access data more efficiently, data is not close to the processing units when needed for processing, incurring long latency misses that hinder the performance. To solve this, we develop a general purpose library that employs helper threads to prefetch data from NVM hardware via a simple API. Our library further improves query execution time for our modified SE when compared to disk and NVM storage by up to 54% and 17%, with average reductions of 23% and 8%, respectively.","sentences":["This paper explores the implications of employing non-volatile memory (NVM) as primary storage for a data base management system (DBMS).","We investigate the modifications necessary to be applied on top of a traditional relational DBMS to take advantage of NVM features.","As a case study, we modify the storage engine (SE) of PostgreSQL enabling efficient use of NVM hardware.","We detail the necessary changes and challenges such modifications entail and evaluate them using a comprehensive emulation platform.","Results indicate that our modified SE reduces query execution time by up to 45% and 13% when compared to disk and NVM storage, with average reductions of 19% and 4%, respectively.","Detailed analysis of these results shows that while our modified SE is able to access data more efficiently, data is not close to the processing units when needed for processing, incurring long latency misses that hinder the performance.","To solve this, we develop a general purpose library that employs helper threads to prefetch data from NVM hardware via a simple API.","Our library further improves query execution time for our modified SE when compared to disk and NVM storage by up to 54% and 17%, with average reductions of 23% and 8%, respectively."],"url":"http://arxiv.org/abs/2502.09431v1"}
{"created":"2025-02-13 15:42:44","title":"On multi-token prediction for efficient LLM inference","abstract":"We systematically investigate multi-token prediction (MTP) capabilities within LLMs pre-trained for next-token prediction (NTP). We first show that such models inherently possess MTP capabilities via numerical marginalization over intermediate token probabilities, though performance is data-dependent and improves with model scale. Furthermore, we explore the challenges of integrating MTP heads into frozen LLMs and find that their hidden layers are strongly specialized for NTP, making adaptation non-trivial. Finally, we show that while joint training of MTP heads with the backbone improves performance, it cannot fully overcome this barrier, prompting further research in this direction. Our findings provide a deeper understanding of MTP applied to pretrained LLMs, informing strategies for accelerating inference through parallel token prediction.","sentences":["We systematically investigate multi-token prediction (MTP) capabilities within LLMs pre-trained for next-token prediction (NTP).","We first show that such models inherently possess MTP capabilities via numerical marginalization over intermediate token probabilities, though performance is data-dependent and improves with model scale.","Furthermore, we explore the challenges of integrating MTP heads into frozen LLMs and find that their hidden layers are strongly specialized for NTP, making adaptation non-trivial.","Finally, we show that while joint training of MTP heads with the backbone improves performance, it cannot fully overcome this barrier, prompting further research in this direction.","Our findings provide a deeper understanding of MTP applied to pretrained LLMs, informing strategies for accelerating inference through parallel token prediction."],"url":"http://arxiv.org/abs/2502.09419v1"}
{"created":"2025-02-13 15:36:31","title":"A LP-rounding based algorithm for soft capacitated facility location problem with submodular penalties","abstract":"The soft capacitated facility location problem (SCFLP) is a classic combinatorial optimization problem, with its variants widely applied in the fields of operations research and computer science. In the SCFLP, given a set $\\mathcal{F}$ of facilities and a set $\\mathcal{D}$ of clients, each facility has a capacity and an open cost, allowing to open multiple times, and each client has a demand.   This problem is to find a subset of facilities in $\\mathcal{F}$ and connect each client to the facilities opened, such that the total cost including open cost and connection cost is minimied. SCFLP is a NP-hard problem, which has led to a focus on approximation algorithms. Based on this, we consider a variant, that is, soft capacitated facility location problem with submodular penalties (SCFLPSP), which allows some clients not to be served by accepting the penalty cost. And we consider the integer splittable case of demand, that is, the demand of each client is served by multiple facilities with the integer service amount by each facility. Based on LP-rounding, we propose a $(\\lambda R+4)$-approximation algorithm, where $R=\\frac{\\max_{i \\in \\mathcal{F} }f_i}{\\min_{i \\in \\mathcal{F} }f_i},\\lambda=\\frac{R+\\sqrt{R^2+8R}}{2R}$. In particular, when the open cost is uniform, the approximation ratio is 6.","sentences":["The soft capacitated facility location problem (SCFLP) is a classic combinatorial optimization problem, with its variants widely applied in the fields of operations research and computer science.","In the SCFLP, given a set $\\mathcal{F}$ of facilities and a set $\\mathcal{D}$ of clients, each facility has a capacity and an open cost, allowing to open multiple times, and each client has a demand.   ","This problem is to find a subset of facilities in $\\mathcal{F}$ and connect each client to the facilities opened, such that the total cost including open cost and connection cost is minimied.","SCFLP is a NP-hard problem, which has led to a focus on approximation algorithms.","Based on this, we consider a variant, that is, soft capacitated facility location problem with submodular penalties (SCFLPSP), which allows some clients not to be served by accepting the penalty cost.","And we consider the integer splittable case of demand, that is, the demand of each client is served by multiple facilities with the integer service amount by each facility.","Based on LP-rounding, we propose a $(\\lambda R+4)$-approximation algorithm, where $R=\\frac{\\max_{i \\in \\mathcal{F} }f_i}{\\min_{i \\in \\mathcal{F} }f_i},\\lambda=\\frac{R+\\sqrt{R^2+8R}}{2R}$. In particular, when the open cost is uniform, the approximation ratio is 6."],"url":"http://arxiv.org/abs/2502.09412v1"}
{"created":"2025-02-13 15:16:53","title":"A hierarchical approach for assessing the vulnerability of tree-based classification models to membership inference attack","abstract":"Machine learning models can inadvertently expose confidential properties of their training data, making them vulnerable to membership inference attacks (MIA). While numerous evaluation methods exist, many require computationally expensive processes, such as training multiple shadow models. This article presents two new complementary approaches for efficiently identifying vulnerable tree-based models: an ante-hoc analysis of hyperparameter choices and a post-hoc examination of trained model structure. While these new methods cannot certify whether a model is safe from MIA, they provide practitioners with a means to significantly reduce the number of models that need to undergo expensive MIA assessment through a hierarchical filtering approach.   More specifically, it is shown that the rank order of disclosure risk for different hyperparameter combinations remains consistent across datasets, enabling the development of simple, human-interpretable rules for identifying relatively high-risk models before training. While this ante-hoc analysis cannot determine absolute safety since this also depends on the specific dataset, it allows the elimination of unnecessarily risky configurations during hyperparameter tuning. Additionally, computationally inexpensive structural metrics serve as indicators of MIA vulnerability, providing a second filtering stage to identify risky models after training but before conducting expensive attacks. Empirical results show that hyperparameter-based risk prediction rules can achieve high accuracy in predicting the most at risk combinations of hyperparameters across different tree-based model types, while requiring no model training. Moreover, target model accuracy is not seen to correlate with privacy risk, suggesting opportunities to optimise model configurations for both performance and privacy.","sentences":["Machine learning models can inadvertently expose confidential properties of their training data, making them vulnerable to membership inference attacks (MIA).","While numerous evaluation methods exist, many require computationally expensive processes, such as training multiple shadow models.","This article presents two new complementary approaches for efficiently identifying vulnerable tree-based models: an ante-hoc analysis of hyperparameter choices and a post-hoc examination of trained model structure.","While these new methods cannot certify whether a model is safe from MIA, they provide practitioners with a means to significantly reduce the number of models that need to undergo expensive MIA assessment through a hierarchical filtering approach.   ","More specifically, it is shown that the rank order of disclosure risk for different hyperparameter combinations remains consistent across datasets, enabling the development of simple, human-interpretable rules for identifying relatively high-risk models before training.","While this ante-hoc analysis cannot determine absolute safety since this also depends on the specific dataset, it allows the elimination of unnecessarily risky configurations during hyperparameter tuning.","Additionally, computationally inexpensive structural metrics serve as indicators of MIA vulnerability, providing a second filtering stage to identify risky models after training but before conducting expensive attacks.","Empirical results show that hyperparameter-based risk prediction rules can achieve high accuracy in predicting the most at risk combinations of hyperparameters across different tree-based model types, while requiring no model training.","Moreover, target model accuracy is not seen to correlate with privacy risk, suggesting opportunities to optimise model configurations for both performance and privacy."],"url":"http://arxiv.org/abs/2502.09396v1"}
{"created":"2025-02-13 15:16:52","title":"Robot Pouring: Identifying Causes of Spillage and Selecting Alternative Action Parameters Using Probabilistic Actual Causation","abstract":"In everyday life, we perform tasks (e.g., cooking or cleaning) that involve a large variety of objects and goals. When confronted with an unexpected or unwanted outcome, we take corrective actions and try again until achieving the desired result. The reasoning performed to identify a cause of the observed outcome and to select an appropriate corrective action is a crucial aspect of human reasoning for successful task execution. Central to this reasoning is the assumption that a factor is responsible for producing the observed outcome. In this paper, we investigate the use of probabilistic actual causation to determine whether a factor is the cause of an observed undesired outcome. Furthermore, we show how the actual causation probabilities can be used to find alternative actions to change the outcome. We apply the probabilistic actual causation analysis to a robot pouring task. When spillage occurs, the analysis indicates whether a task parameter is the cause and how it should be changed to avoid spillage. The analysis requires a causal graph of the task and the corresponding conditional probability distributions. To fulfill these requirements, we perform a complete causal modeling procedure (i.e., task analysis, definition of variables, determination of the causal graph structure, and estimation of conditional probability distributions) using data from a realistic simulation of the robot pouring task, covering a large combinatorial space of task parameters. Based on the results, we discuss the implications of the variables' representation and how the alternative actions suggested by the actual causation analysis would compare to the alternative solutions proposed by a human observer. The practical use of the analysis of probabilistic actual causation to select alternative action parameters is demonstrated.","sentences":["In everyday life, we perform tasks (e.g., cooking or cleaning) that involve a large variety of objects and goals.","When confronted with an unexpected or unwanted outcome, we take corrective actions and try again until achieving the desired result.","The reasoning performed to identify a cause of the observed outcome and to select an appropriate corrective action is a crucial aspect of human reasoning for successful task execution.","Central to this reasoning is the assumption that a factor is responsible for producing the observed outcome.","In this paper, we investigate the use of probabilistic actual causation to determine whether a factor is the cause of an observed undesired outcome.","Furthermore, we show how the actual causation probabilities can be used to find alternative actions to change the outcome.","We apply the probabilistic actual causation analysis to a robot pouring task.","When spillage occurs, the analysis indicates whether a task parameter is the cause and how it should be changed to avoid spillage.","The analysis requires a causal graph of the task and the corresponding conditional probability distributions.","To fulfill these requirements, we perform a complete causal modeling procedure (i.e., task analysis, definition of variables, determination of the causal graph structure, and estimation of conditional probability distributions) using data from a realistic simulation of the robot pouring task, covering a large combinatorial space of task parameters.","Based on the results, we discuss the implications of the variables' representation and how the alternative actions suggested by the actual causation analysis would compare to the alternative solutions proposed by a human observer.","The practical use of the analysis of probabilistic actual causation to select alternative action parameters is demonstrated."],"url":"http://arxiv.org/abs/2502.09395v1"}
{"created":"2025-02-13 15:06:42","title":"S$^2$-Diffusion: Generalizing from Instance-level to Category-level Skills in Robot Manipulation","abstract":"Recent advances in skill learning has propelled robot manipulation to new heights by enabling it to learn complex manipulation tasks from a practical number of demonstrations. However, these skills are often limited to the particular action, object, and environment \\textit{instances} that are shown in the training data, and have trouble transferring to other instances of the same category. In this work we present an open-vocabulary Spatial-Semantic Diffusion policy (S$^2$-Diffusion) which enables generalization from instance-level training data to category-level, enabling skills to be transferable between instances of the same category. We show that functional aspects of skills can be captured via a promptable semantic module combined with a spatial representation. We further propose leveraging depth estimation networks to allow the use of only a single RGB camera. Our approach is evaluated and compared on a diverse number of robot manipulation tasks, both in simulation and in the real world. Our results show that S$^2$-Diffusion is invariant to changes in category-irrelevant factors as well as enables satisfying performance on other instances within the same category, even if it was not trained on that specific instance. Full videos of all real-world experiments are available in the supplementary material.","sentences":["Recent advances in skill learning has propelled robot manipulation to new heights by enabling it to learn complex manipulation tasks from a practical number of demonstrations.","However, these skills are often limited to the particular action, object, and environment \\textit{instances} that are shown in the training data, and have trouble transferring to other instances of the same category.","In this work we present an open-vocabulary Spatial-Semantic Diffusion policy (S$^2$-Diffusion) which enables generalization from instance-level training data to category-level, enabling skills to be transferable between instances of the same category.","We show that functional aspects of skills can be captured via a promptable semantic module combined with a spatial representation.","We further propose leveraging depth estimation networks to allow the use of only a single RGB camera.","Our approach is evaluated and compared on a diverse number of robot manipulation tasks, both in simulation and in the real world.","Our results show that S$^2$-Diffusion is invariant to changes in category-irrelevant factors as well as enables satisfying performance on other instances within the same category, even if it was not trained on that specific instance.","Full videos of all real-world experiments are available in the supplementary material."],"url":"http://arxiv.org/abs/2502.09389v1"}
{"created":"2025-02-13 15:02:38","title":"Code Style Sheets: CSS for Code","abstract":"Program text is rendered using impoverished typographic styles. Beyond choice of fonts and syntax-highlighting colors, code editors and related tools utilize very few text decorations. These limited styles are, furthermore, applied in monolithic fashion, regardless of the programs and tasks at hand.   We present the notion of code style sheets for styling the textual representation of programs. Motivated by analogy to cascading style sheets (CSS) for styling HTML documents, code style sheets provide mechanisms for defining rules to select and style abstract syntax trees (ASTs). Technically, code style sheets generalize notions from CSS over untyped HTML trees to a programming-language setting with algebraic data types (e.g. ASTs). Practically, code style sheets allow ASTs to be styled granularly, based on semantic information -- such as the structure of abstract syntax, static type information, and corresponding run-time values -- as well as design choices on the part of authors and readers of a program. In this paper, we design and implement a code style sheets system for a subset of Haskell -- the rich syntactic and semantic structure of Haskell provide a fertile first setting in which to explore the notion of code style sheets. We illustrate several use cases involving code presentation and visualization tasks.","sentences":["Program text is rendered using impoverished typographic styles.","Beyond choice of fonts and syntax-highlighting colors, code editors and related tools utilize very few text decorations.","These limited styles are, furthermore, applied in monolithic fashion, regardless of the programs and tasks at hand.   ","We present the notion of code style sheets for styling the textual representation of programs.","Motivated by analogy to cascading style sheets (CSS) for styling HTML documents, code style sheets provide mechanisms for defining rules to select and style abstract syntax trees (ASTs).","Technically, code style sheets generalize notions from CSS over untyped HTML trees to a programming-language setting with algebraic data types (e.g. ASTs).","Practically, code style sheets allow ASTs to be styled granularly, based on semantic information -- such as the structure of abstract syntax, static type information, and corresponding run-time values -- as well as design choices on the part of authors and readers of a program.","In this paper, we design and implement a code style sheets system for a subset of Haskell -- the rich syntactic and semantic structure of Haskell provide a fertile first setting in which to explore the notion of code style sheets.","We illustrate several use cases involving code presentation and visualization tasks."],"url":"http://arxiv.org/abs/2502.09386v1"}
{"created":"2025-02-13 15:01:18","title":"APT-LLM: Embedding-Based Anomaly Detection of Cyber Advanced Persistent Threats Using Large Language Models","abstract":"Advanced Persistent Threats (APTs) pose a major cybersecurity challenge due to their stealth and ability to mimic normal system behavior, making detection particularly difficult in highly imbalanced datasets. Traditional anomaly detection methods struggle to effectively differentiate APT-related activities from benign processes, limiting their applicability in real-world scenarios. This paper introduces APT-LLM, a novel embedding-based anomaly detection framework that integrates large language models (LLMs) -- BERT, ALBERT, DistilBERT, and RoBERTa -- with autoencoder architectures to detect APTs. Unlike prior approaches, which rely on manually engineered features or conventional anomaly detection models, APT-LLM leverages LLMs to encode process-action provenance traces into semantically rich embeddings, capturing nuanced behavioral patterns. These embeddings are analyzed using three autoencoder architectures -- Baseline Autoencoder (AE), Variational Autoencoder (VAE), and Denoising Autoencoder (DAE) -- to model normal process behavior and identify anomalies. The best-performing model is selected for comparison against traditional methods. The framework is evaluated on real-world, highly imbalanced provenance trace datasets from the DARPA Transparent Computing program, where APT-like attacks constitute as little as 0.004\\% of the data across multiple operating systems (Android, Linux, BSD, and Windows) and attack scenarios. Results demonstrate that APT-LLM significantly improves detection performance under extreme imbalance conditions, outperforming existing anomaly detection methods and highlighting the effectiveness of LLM-based feature extraction in cybersecurity.","sentences":["Advanced Persistent Threats (APTs) pose a major cybersecurity challenge due to their stealth and ability to mimic normal system behavior, making detection particularly difficult in highly imbalanced datasets.","Traditional anomaly detection methods struggle to effectively differentiate APT-related activities from benign processes, limiting their applicability in real-world scenarios.","This paper introduces APT-LLM, a novel embedding-based anomaly detection framework that integrates large language models (LLMs) -- BERT, ALBERT, DistilBERT, and RoBERTa -- with autoencoder architectures to detect APTs.","Unlike prior approaches, which rely on manually engineered features or conventional anomaly detection models, APT-LLM leverages LLMs to encode process-action provenance traces into semantically rich embeddings, capturing nuanced behavioral patterns.","These embeddings are analyzed using three autoencoder architectures -- Baseline Autoencoder (AE), Variational Autoencoder (VAE), and Denoising Autoencoder (DAE) -- to model normal process behavior and identify anomalies.","The best-performing model is selected for comparison against traditional methods.","The framework is evaluated on real-world, highly imbalanced provenance trace datasets from the DARPA Transparent Computing program, where APT-like attacks constitute as little as 0.004\\% of the data across multiple operating systems (Android, Linux, BSD, and Windows) and attack scenarios.","Results demonstrate that APT-LLM significantly improves detection performance under extreme imbalance conditions, outperforming existing anomaly detection methods and highlighting the effectiveness of LLM-based feature extraction in cybersecurity."],"url":"http://arxiv.org/abs/2502.09385v1"}
{"created":"2025-02-13 14:46:04","title":"A Deep Inverse-Mapping Model for a Flapping Robotic Wing","abstract":"In systems control, the dynamics of a system are governed by modulating its inputs to achieve a desired outcome. For example, to control the thrust of a quad-copter propeller the controller modulates its rotation rate, relying on a straightforward mapping between the input rotation rate and the resulting thrust. This mapping can be inverted to determine the rotation rate needed to generate a desired thrust. However, in complex systems, such as flapping-wing robots where intricate fluid motions are involved, mapping inputs (wing kinematics) to outcomes (aerodynamic forces) is nontrivial and inverting this mapping for real-time control is computationally impractical. Here, we report a machine-learning solution for the inverse mapping of a flapping-wing system based on data from an experimental system we have developed. Our model learns the input wing motion required to generate a desired aerodynamic force outcome. We used a sequence-to-sequence model tailored for time-series data and augmented it with a novel adaptive-spectrum layer that implements representation learning in the frequency domain. To train our model, we developed a flapping wing system that simultaneously measures the wing's aerodynamic force and its 3D motion using high-speed cameras. We demonstrate the performance of our system on an additional open-source dataset of a flapping wing in a different flow regime. Results show superior performance compared with more complex state-of-the-art transformer-based models, with 11% improvement on the test datasets median loss. Moreover, our model shows superior inference time, making it practical for onboard robotic control. Our open-source data and framework may improve modeling and real-time control of systems governed by complex dynamics, from biomimetic robots to biomedical devices.","sentences":["In systems control, the dynamics of a system are governed by modulating its inputs to achieve a desired outcome.","For example, to control the thrust of a quad-copter propeller the controller modulates its rotation rate, relying on a straightforward mapping between the input rotation rate and the resulting thrust.","This mapping can be inverted to determine the rotation rate needed to generate a desired thrust.","However, in complex systems, such as flapping-wing robots where intricate fluid motions are involved, mapping inputs (wing kinematics) to outcomes (aerodynamic forces) is nontrivial and inverting this mapping for real-time control is computationally impractical.","Here, we report a machine-learning solution for the inverse mapping of a flapping-wing system based on data from an experimental system we have developed.","Our model learns the input wing motion required to generate a desired aerodynamic force outcome.","We used a sequence-to-sequence model tailored for time-series data and augmented it with a novel adaptive-spectrum layer that implements representation learning in the frequency domain.","To train our model, we developed a flapping wing system that simultaneously measures the wing's aerodynamic force and its 3D motion using high-speed cameras.","We demonstrate the performance of our system on an additional open-source dataset of a flapping wing in a different flow regime.","Results show superior performance compared with more complex state-of-the-art transformer-based models, with 11% improvement on the test datasets median loss.","Moreover, our model shows superior inference time, making it practical for onboard robotic control.","Our open-source data and framework may improve modeling and real-time control of systems governed by complex dynamics, from biomimetic robots to biomedical devices."],"url":"http://arxiv.org/abs/2502.09378v1"}
{"created":"2025-02-13 14:44:15","title":"FARM: Frequency-Aware Model for Cross-Domain Live-Streaming Recommendation","abstract":"Live-streaming services have attracted widespread popularity due to their real-time interactivity and entertainment value. Users can engage with live-streaming authors by participating in live chats, posting likes, or sending virtual gifts to convey their preferences and support. However, the live-streaming services faces serious data-sparsity problem, which can be attributed to the following two points: (1) User's valuable behaviors are usually sparse, e.g., like, comment and gift, which are easily overlooked by the model, making it difficult to describe user's personalized preference. (2) The main exposure content on our platform is short-video, which is 9 times higher than the exposed live-streaming, leading to the inability of live-streaming content to fully model user preference. To this end, we propose a Frequency-Aware Model for Cross-Domain Live-Streaming Recommendation, termed as FARM. Specifically, we first present the intra-domain frequency aware module to enable our model to perceive user's sparse yet valuable behaviors, i.e., high-frequency information, supported by the Discrete Fourier Transform (DFT). To transfer user preference across the short-video and live-streaming domains, we propose a novel preference align before fuse strategy, which consists of two parts: the cross-domain preference align module to align user preference in both domains with contrastive learning, and the cross-domain preference fuse module to further fuse user preference in both domains using a serious of tailor-designed attention mechanisms. Extensive offline experiments and online A/B testing on Kuaishou live-streaming services demonstrate the effectiveness and superiority of FARM. Our FARM has been deployed in online live-streaming services and currently serves hundreds of millions of users on Kuaishou.","sentences":["Live-streaming services have attracted widespread popularity due to their real-time interactivity and entertainment value.","Users can engage with live-streaming authors by participating in live chats, posting likes, or sending virtual gifts to convey their preferences and support.","However, the live-streaming services faces serious data-sparsity problem, which can be attributed to the following two points: (1) User's valuable behaviors are usually sparse, e.g., like, comment and gift, which are easily overlooked by the model, making it difficult to describe user's personalized preference.","(2) The main exposure content on our platform is short-video, which is 9 times higher than the exposed live-streaming, leading to the inability of live-streaming content to fully model user preference.","To this end, we propose a Frequency-Aware Model for Cross-Domain Live-Streaming Recommendation, termed as FARM.","Specifically, we first present the intra-domain frequency aware module to enable our model to perceive user's sparse yet valuable behaviors, i.e., high-frequency information, supported by the Discrete Fourier Transform (DFT).","To transfer user preference across the short-video and live-streaming domains, we propose a novel preference align before fuse strategy, which consists of two parts: the cross-domain preference align module to align user preference in both domains with contrastive learning, and the cross-domain preference fuse module to further fuse user preference in both domains using a serious of tailor-designed attention mechanisms.","Extensive offline experiments and online A/B testing on Kuaishou live-streaming services demonstrate the effectiveness and superiority of FARM.","Our FARM has been deployed in online live-streaming services and currently serves hundreds of millions of users on Kuaishou."],"url":"http://arxiv.org/abs/2502.09375v1"}
{"created":"2025-02-13 14:33:02","title":"Simple Path Structural Encoding for Graph Transformers","abstract":"Graph transformers extend global self-attention to graph-structured data, achieving notable success in graph learning. Recently, random walk structural encoding (RWSE) has been found to further enhance their predictive power by encoding both structural and positional information into the edge representation. However, RWSE cannot always distinguish between edges that belong to different local graph patterns, which reduces its ability to capture the full structural complexity of graphs. This work introduces Simple Path Structural Encoding (SPSE), a novel method that utilizes simple path counts for edge encoding. We show theoretically and experimentally that SPSE overcomes the limitations of RWSE, providing a richer representation of graph structures, particularly for capturing local cyclic patterns. To make SPSE computationally tractable, we propose an efficient approximate algorithm for simple path counting. SPSE demonstrates significant performance improvements over RWSE on various benchmarks, including molecular and long-range graph datasets, achieving statistically significant gains in discriminative tasks. These results pose SPSE as a powerful edge encoding alternative for enhancing the expressivity of graph transformers.","sentences":["Graph transformers extend global self-attention to graph-structured data, achieving notable success in graph learning.","Recently, random walk structural encoding (RWSE) has been found to further enhance their predictive power by encoding both structural and positional information into the edge representation.","However, RWSE cannot always distinguish between edges that belong to different local graph patterns, which reduces its ability to capture the full structural complexity of graphs.","This work introduces Simple Path Structural Encoding (SPSE), a novel method that utilizes simple path counts for edge encoding.","We show theoretically and experimentally that SPSE overcomes the limitations of RWSE, providing a richer representation of graph structures, particularly for capturing local cyclic patterns.","To make SPSE computationally tractable, we propose an efficient approximate algorithm for simple path counting.","SPSE demonstrates significant performance improvements over RWSE on various benchmarks, including molecular and long-range graph datasets, achieving statistically significant gains in discriminative tasks.","These results pose SPSE as a powerful edge encoding alternative for enhancing the expressivity of graph transformers."],"url":"http://arxiv.org/abs/2502.09365v1"}
{"created":"2025-02-13 14:31:49","title":"The Accuracy Cost of Weakness: A Theoretical Analysis of Fixed-Segment Weak Labeling for Events in Time","abstract":"Accurate labels are critical for deriving robust machine learning models. Labels are used to train supervised learning models and to evaluate most machine learning paradigms. In this paper, we model the accuracy and cost of a common weak labeling process where annotators assign presence or absence labels to fixed-length data segments for a given event class. The annotator labels a segment as \"present\" if it sufficiently covers an event from that class, e.g., a birdsong sound event in audio data. We analyze how the segment length affects the label accuracy and the required number of annotations, and compare this fixed-length labeling approach with an oracle method that uses the true event activations to construct the segments. Furthermore, we quantify the gap between these methods and verify that in most realistic scenarios the oracle method is better than the fixed-length labeling method in both accuracy and cost. Our findings provide a theoretical justification for adaptive weak labeling strategies that mimic the oracle process, and a foundation for optimizing weak labeling processes in sequence labeling tasks.","sentences":["Accurate labels are critical for deriving robust machine learning models.","Labels are used to train supervised learning models and to evaluate most machine learning paradigms.","In this paper, we model the accuracy and cost of a common weak labeling process where annotators assign presence or absence labels to fixed-length data segments for a given event class.","The annotator labels a segment as \"present\" if it sufficiently covers an event from that class, e.g., a birdsong sound event in audio data.","We analyze how the segment length affects the label accuracy and the required number of annotations, and compare this fixed-length labeling approach with an oracle method that uses the true event activations to construct the segments.","Furthermore, we quantify the gap between these methods and verify that in most realistic scenarios the oracle method is better than the fixed-length labeling method in both accuracy and cost.","Our findings provide a theoretical justification for adaptive weak labeling strategies that mimic the oracle process, and a foundation for optimizing weak labeling processes in sequence labeling tasks."],"url":"http://arxiv.org/abs/2502.09363v1"}
{"created":"2025-02-13 14:21:03","title":"Galileo: Learning Global and Local Features in Pretrained Remote Sensing Models","abstract":"From crop mapping to flood detection, machine learning in remote sensing has a wide range of societally beneficial applications. The commonalities between remote sensing data in these applications present an opportunity for pretrained machine learning models tailored to remote sensing to reduce the labeled data and effort required to solve individual tasks. However, such models must be: (i) flexible enough to ingest input data of varying sensor modalities and shapes (i.e., of varying spatial and temporal dimensions), and (ii) able to model Earth surface phenomena of varying scales and types. To solve this gap, we present Galileo, a family of pretrained remote sensing models designed to flexibly process multimodal remote sensing data. We also introduce a novel and highly effective self-supervised learning approach to learn both large- and small-scale features, a challenge not addressed by previous models. Our Galileo models obtain state-of-the-art results across diverse remote sensing tasks.","sentences":["From crop mapping to flood detection, machine learning in remote sensing has a wide range of societally beneficial applications.","The commonalities between remote sensing data in these applications present an opportunity for pretrained machine learning models tailored to remote sensing to reduce the labeled data and effort required to solve individual tasks.","However, such models must be: (i) flexible enough to ingest input data of varying sensor modalities and shapes (i.e., of varying spatial and temporal dimensions), and (ii) able to model Earth surface phenomena of varying scales and types.","To solve this gap, we present Galileo, a family of pretrained remote sensing models designed to flexibly process multimodal remote sensing data.","We also introduce a novel and highly effective self-supervised learning approach to learn both large- and small-scale features, a challenge not addressed by previous models.","Our Galileo models obtain state-of-the-art results across diverse remote sensing tasks."],"url":"http://arxiv.org/abs/2502.09356v1"}
{"created":"2025-02-13 14:11:33","title":"Machine learning for modelling unstructured grid data in computational physics: a review","abstract":"Unstructured grid data are essential for modelling complex geometries and dynamics in computational physics. Yet, their inherent irregularity presents significant challenges for conventional machine learning (ML) techniques. This paper provides a comprehensive review of advanced ML methodologies designed to handle unstructured grid data in high-dimensional dynamical systems. Key approaches discussed include graph neural networks, transformer models with spatial attention mechanisms, interpolation-integrated ML methods, and meshless techniques such as physics-informed neural networks. These methodologies have proven effective across diverse fields, including fluid dynamics and environmental simulations. This review is intended as a guidebook for computational scientists seeking to apply ML approaches to unstructured grid data in their domains, as well as for ML researchers looking to address challenges in computational physics. It places special focus on how ML methods can overcome the inherent limitations of traditional numerical techniques and, conversely, how insights from computational physics can inform ML development. To support benchmarking, this review also provides a summary of open-access datasets of unstructured grid data in computational physics. Finally, emerging directions such as generative models with unstructured data, reinforcement learning for mesh generation, and hybrid physics-data-driven paradigms are discussed to inspire future advancements in this evolving field.","sentences":["Unstructured grid data are essential for modelling complex geometries and dynamics in computational physics.","Yet, their inherent irregularity presents significant challenges for conventional machine learning (ML) techniques.","This paper provides a comprehensive review of advanced ML methodologies designed to handle unstructured grid data in high-dimensional dynamical systems.","Key approaches discussed include graph neural networks, transformer models with spatial attention mechanisms, interpolation-integrated ML methods, and meshless techniques such as physics-informed neural networks.","These methodologies have proven effective across diverse fields, including fluid dynamics and environmental simulations.","This review is intended as a guidebook for computational scientists seeking to apply ML approaches to unstructured grid data in their domains, as well as for ML researchers looking to address challenges in computational physics.","It places special focus on how ML methods can overcome the inherent limitations of traditional numerical techniques and, conversely, how insights from computational physics can inform ML development.","To support benchmarking, this review also provides a summary of open-access datasets of unstructured grid data in computational physics.","Finally, emerging directions such as generative models with unstructured data, reinforcement learning for mesh generation, and hybrid physics-data-driven paradigms are discussed to inspire future advancements in this evolving field."],"url":"http://arxiv.org/abs/2502.09346v1"}
{"created":"2025-02-13 14:01:15","title":"Neural Spatiotemporal Point Processes: Trends and Challenges","abstract":"Spatiotemporal point processes (STPPs) are probabilistic models for events occurring in continuous space and time. Real-world event data often exhibit intricate dependencies and heterogeneous dynamics. By incorporating modern deep learning techniques, STPPs can model these complexities more effectively than traditional approaches. Consequently, the fusion of neural methods with STPPs has become an active and rapidly evolving research area. In this review, we categorize existing approaches, unify key design choices, and explain the challenges of working with this data modality. We further highlight emerging trends and diverse application domains. Finally, we identify open challenges and gaps in the literature.","sentences":["Spatiotemporal point processes (STPPs) are probabilistic models for events occurring in continuous space and time.","Real-world event data often exhibit intricate dependencies and heterogeneous dynamics.","By incorporating modern deep learning techniques, STPPs can model these complexities more effectively than traditional approaches.","Consequently, the fusion of neural methods with STPPs has become an active and rapidly evolving research area.","In this review, we categorize existing approaches, unify key design choices, and explain the challenges of working with this data modality.","We further highlight emerging trends and diverse application domains.","Finally, we identify open challenges and gaps in the literature."],"url":"http://arxiv.org/abs/2502.09341v1"}
{"created":"2025-02-13 13:54:58","title":"Graph Diffusion Network for Drug-Gene Prediction","abstract":"Predicting drug-gene associations is crucial for drug development and disease treatment. While graph neural networks (GNN) have shown effectiveness in this task, they face challenges with data sparsity and efficient contrastive learning implementation. We introduce a graph diffusion network for drug-gene prediction (GDNDGP), a framework that addresses these limitations through two key innovations. First, it employs meta-path-based homogeneous graph learning to capture drug-drug and gene-gene relationships, ensuring similar entities share embedding spaces. Second, it incorporates a parallel diffusion network that generates hard negative samples during training, eliminating the need for exhaustive negative sample retrieval. Our model achieves superior performance on the DGIdb 4.0 dataset and demonstrates strong generalization capability on tripartite drug-gene-disease networks. Results show significant improvements over existing methods in drug-gene prediction tasks, particularly in handling complex heterogeneous relationships. The source code is publicly available at https://github.com/csjywu1/GDNDGP.","sentences":["Predicting drug-gene associations is crucial for drug development and disease treatment.","While graph neural networks (GNN) have shown effectiveness in this task, they face challenges with data sparsity and efficient contrastive learning implementation.","We introduce a graph diffusion network for drug-gene prediction (GDNDGP), a framework that addresses these limitations through two key innovations.","First, it employs meta-path-based homogeneous graph learning to capture drug-drug and gene-gene relationships, ensuring similar entities share embedding spaces.","Second, it incorporates a parallel diffusion network that generates hard negative samples during training, eliminating the need for exhaustive negative sample retrieval.","Our model achieves superior performance on the DGIdb 4.0 dataset and demonstrates strong generalization capability on tripartite drug-gene-disease networks.","Results show significant improvements over existing methods in drug-gene prediction tasks, particularly in handling complex heterogeneous relationships.","The source code is publicly available at https://github.com/csjywu1/GDNDGP."],"url":"http://arxiv.org/abs/2502.09335v1"}
{"created":"2025-02-13 13:49:30","title":"Beyond English: The Impact of Prompt Translation Strategies across Languages and Tasks in Multilingual LLMs","abstract":"Despite advances in the multilingual capabilities of Large Language Models (LLMs) across diverse tasks, English remains the dominant language for LLM research and development. So, when working with a different language, this has led to the widespread practice of pre-translation, i.e., translating the task prompt into English before inference. Selective pre-translation, a more surgical approach, focuses on translating specific prompt components. However, its current use is sporagic and lacks a systematic research foundation. Consequently, the optimal pre-translation strategy for various multilingual settings and tasks remains unclear. In this work, we aim to uncover the optimal setup for pre-translation by systematically assessing its use. Specifically, we view the prompt as a modular entity, composed of four functional parts: instruction, context, examples, and output, either of which could be translated or not. We evaluate pre-translation strategies across 35 languages covering both low and high-resource languages, on various tasks including Question Answering (QA), Natural Language Inference (NLI), Named Entity Recognition (NER), and Abstractive Summarization. Our experiments show the impact of factors as similarity to English, translation quality and the size of pre-trained data, on the model performance with pre-translation. We suggest practical guidelines for choosing optimal strategies in various multilingual settings.","sentences":["Despite advances in the multilingual capabilities of Large Language Models (LLMs) across diverse tasks, English remains the dominant language for LLM research and development.","So, when working with a different language, this has led to the widespread practice of pre-translation, i.e., translating the task prompt into English before inference.","Selective pre-translation, a more surgical approach, focuses on translating specific prompt components.","However, its current use is sporagic and lacks a systematic research foundation.","Consequently, the optimal pre-translation strategy for various multilingual settings and tasks remains unclear.","In this work, we aim to uncover the optimal setup for pre-translation by systematically assessing its use.","Specifically, we view the prompt as a modular entity, composed of four functional parts: instruction, context, examples, and output, either of which could be translated or not.","We evaluate pre-translation strategies across 35 languages covering both low and high-resource languages, on various tasks including Question Answering (QA), Natural Language Inference (NLI), Named Entity Recognition (NER), and Abstractive Summarization.","Our experiments show the impact of factors as similarity to English, translation quality and the size of pre-trained data, on the model performance with pre-translation.","We suggest practical guidelines for choosing optimal strategies in various multilingual settings."],"url":"http://arxiv.org/abs/2502.09331v1"}
{"created":"2025-02-13 13:40:52","title":"Copilot Arena: A Platform for Code LLM Evaluation in the Wild","abstract":"Evaluating in-the-wild coding capabilities of large language models (LLMs) is a challenging endeavor with no clear solution. We introduce Copilot Arena, a platform to collect user preferences for code generation through native integration into a developer's working environment. Copilot Arena comprises a novel interface for comparing pairs of model outputs, a sampling strategy optimized to reduce latency, and a prompting scheme to enable code completion functionality. Copilot Arena has served over 4.5 million suggestions from 10 models and collected over 11k pairwise judgements. Our results highlight the importance of model evaluations in integrated settings. We find that model rankings from Copilot Arena differ from those of existing evaluations, which we attribute to the more realistic distribution of data and tasks contained in Copilot Arena. We also identify novel insights into human preferences on code such as an observed consistency in user preference across programming languages yet significant variation in preference due to task category. We open-source Copilot Arena and release data to enable human-centric evaluations and improve understanding of coding assistants.","sentences":["Evaluating in-the-wild coding capabilities of large language models (LLMs) is a challenging endeavor with no clear solution.","We introduce Copilot Arena, a platform to collect user preferences for code generation through native integration into a developer's working environment.","Copilot Arena comprises a novel interface for comparing pairs of model outputs, a sampling strategy optimized to reduce latency, and a prompting scheme to enable code completion functionality.","Copilot Arena has served over 4.5 million suggestions from 10 models and collected over 11k pairwise judgements.","Our results highlight the importance of model evaluations in integrated settings.","We find that model rankings from Copilot Arena differ from those of existing evaluations, which we attribute to the more realistic distribution of data and tasks contained in Copilot Arena.","We also identify novel insights into human preferences on code such as an observed consistency in user preference across programming languages yet significant variation in preference due to task category.","We open-source Copilot Arena and release data to enable human-centric evaluations and improve understanding of coding assistants."],"url":"http://arxiv.org/abs/2502.09328v1"}
{"created":"2025-02-13 13:38:17","title":"A Benchmark for Crime Surveillance Video Analysis with Large Models","abstract":"Anomaly analysis in surveillance videos is a crucial topic in computer vision. In recent years, multimodal large language models (MLLMs) have outperformed task-specific models in various domains. Although MLLMs are particularly versatile, their abilities to understand anomalous concepts and details are insufficiently studied because of the outdated benchmarks of this field not providing MLLM-style QAs and efficient algorithms to assess the model's open-ended text responses. To fill this gap, we propose a benchmark for crime surveillance video analysis with large models denoted as UCVL, including 1,829 videos and reorganized annotations from the UCF-Crime and UCF-Crime Annotation datasets. We design six types of questions and generate diverse QA pairs. Then we develop detailed instructions and use OpenAI's GPT-4o for accurate assessment. We benchmark eight prevailing MLLMs ranging from 0.5B to 40B parameters, and the results demonstrate the reliability of this bench. Moreover, we finetune LLaVA-OneVision on UCVL's training set. The improvement validates our data's high quality for video anomaly analysis.","sentences":["Anomaly analysis in surveillance videos is a crucial topic in computer vision.","In recent years, multimodal large language models (MLLMs) have outperformed task-specific models in various domains.","Although MLLMs are particularly versatile, their abilities to understand anomalous concepts and details are insufficiently studied because of the outdated benchmarks of this field not providing MLLM-style QAs and efficient algorithms to assess the model's open-ended text responses.","To fill this gap, we propose a benchmark for crime surveillance video analysis with large models denoted as UCVL, including 1,829 videos and reorganized annotations from the UCF-Crime and UCF-Crime Annotation datasets.","We design six types of questions and generate diverse QA pairs.","Then we develop detailed instructions and use OpenAI's GPT-4o for accurate assessment.","We benchmark eight prevailing MLLMs ranging from 0.5B to 40B parameters, and the results demonstrate the reliability of this bench.","Moreover, we finetune LLaVA-OneVision on UCVL's training set.","The improvement validates our data's high quality for video anomaly analysis."],"url":"http://arxiv.org/abs/2502.09325v1"}
{"created":"2025-02-13 13:33:45","title":"Bridging Jensen Gap for Max-Min Group Fairness Optimization in Recommendation","abstract":"Group max-min fairness (MMF) is commonly used in fairness-aware recommender systems (RS) as an optimization objective, as it aims to protect marginalized item groups and ensures a fair competition platform. However, our theoretical analysis indicates that integrating MMF constraint violates the assumption of sample independence during optimization, causing the loss function to deviate from linear additivity. Such nonlinearity property introduces the Jensen gap between the model's convergence point and the optimal point if mini-batch sampling is applied. Both theoretical and empirical studies show that as the mini-batch size decreases and the group size increases, the Jensen gap will widen accordingly. Some methods using heuristic re-weighting or debiasing strategies have the potential to bridge the Jensen gap. However, they either lack theoretical guarantees or suffer from heavy computational costs. To overcome these limitations, we first theoretically demonstrate that the MMF-constrained objective can be essentially reformulated as a group-weighted optimization objective. Then we present an efficient and effective algorithm named FairDual, which utilizes a dual optimization technique to minimize the Jensen gap. Our theoretical analysis demonstrates that FairDual can achieve a sub-linear convergence rate to the globally optimal solution and the Jensen gap can be well bounded under a mini-batch sampling strategy with random shuffle. Extensive experiments conducted using six large-scale RS backbone models on three publicly available datasets demonstrate that FairDual outperforms all baselines in terms of both accuracy and fairness. Our data and codes are shared at https://github.com/XuChen0427/FairDual.","sentences":["Group max-min fairness (MMF) is commonly used in fairness-aware recommender systems (RS) as an optimization objective, as it aims to protect marginalized item groups and ensures a fair competition platform.","However, our theoretical analysis indicates that integrating MMF constraint violates the assumption of sample independence during optimization, causing the loss function to deviate from linear additivity.","Such nonlinearity property introduces the Jensen gap between the model's convergence point and the optimal point if mini-batch sampling is applied.","Both theoretical and empirical studies show that as the mini-batch size decreases and the group size increases, the Jensen gap will widen accordingly.","Some methods using heuristic re-weighting or debiasing strategies have the potential to bridge the Jensen gap.","However, they either lack theoretical guarantees or suffer from heavy computational costs.","To overcome these limitations, we first theoretically demonstrate that the MMF-constrained objective can be essentially reformulated as a group-weighted optimization objective.","Then we present an efficient and effective algorithm named FairDual, which utilizes a dual optimization technique to minimize the Jensen gap.","Our theoretical analysis demonstrates that FairDual can achieve a sub-linear convergence rate to the globally optimal solution and the Jensen gap can be well bounded under a mini-batch sampling strategy with random shuffle.","Extensive experiments conducted using six large-scale RS backbone models on three publicly available datasets demonstrate that FairDual outperforms all baselines in terms of both accuracy and fairness.","Our data and codes are shared at https://github.com/XuChen0427/FairDual."],"url":"http://arxiv.org/abs/2502.09319v1"}
{"created":"2025-02-13 13:25:13","title":"Mitigating the Impact of Prominent Position Shift in Drone-based RGBT Object Detection","abstract":"Drone-based RGBT object detection plays a crucial role in many around-the-clock applications. However, real-world drone-viewed RGBT data suffers from the prominent position shift problem, i.e., the position of a tiny object differs greatly in different modalities. For instance, a slight deviation of a tiny object in the thermal modality will induce it to drift from the main body of itself in the RGB modality. Considering RGBT data are usually labeled on one modality (reference), this will cause the unlabeled modality (sensed) to lack accurate supervision signals and prevent the detector from learning a good representation. Moreover, the mismatch of the corresponding feature point between the modalities will make the fused features confusing for the detection head. In this paper, we propose to cast the cross-modality box shift issue as the label noise problem and address it on the fly via a novel Mean Teacher-based Cross-modality Box Correction head ensemble (CBC). In this way, the network can learn more informative representations for both modalities. Furthermore, to alleviate the feature map mismatch problem in RGBT fusion, we devise a Shifted Window-Based Cascaded Alignment (SWCA) module. SWCA mines long-range dependencies between the spatially unaligned features inside shifted windows and cascaded aligns the sensed features with the reference ones. Extensive experiments on two drone-based RGBT object detection datasets demonstrate that the correction results are both visually and quantitatively favorable, thereby improving the detection performance. In particular, our CBC module boosts the precision of the sensed modality ground truth by 25.52 aSim points. Overall, the proposed detector achieves an mAP_50 of 43.55 points on RGBTDronePerson and surpasses a state-of-the-art method by 8.6 mAP50 on a shift subset of DroneVehicle dataset. The code and data will be made publicly available.","sentences":["Drone-based RGBT object detection plays a crucial role in many around-the-clock applications.","However, real-world drone-viewed RGBT data suffers from the prominent position shift problem, i.e., the position of a tiny object differs greatly in different modalities.","For instance, a slight deviation of a tiny object in the thermal modality will induce it to drift from the main body of itself in the RGB modality.","Considering RGBT data are usually labeled on one modality (reference), this will cause the unlabeled modality (sensed) to lack accurate supervision signals and prevent the detector from learning a good representation.","Moreover, the mismatch of the corresponding feature point between the modalities will make the fused features confusing for the detection head.","In this paper, we propose to cast the cross-modality box shift issue as the label noise problem and address it on the fly via a novel Mean Teacher-based Cross-modality Box Correction head ensemble (CBC).","In this way, the network can learn more informative representations for both modalities.","Furthermore, to alleviate the feature map mismatch problem in RGBT fusion, we devise a Shifted Window-Based Cascaded Alignment (SWCA) module.","SWCA mines long-range dependencies between the spatially unaligned features inside shifted windows and cascaded aligns the sensed features with the reference ones.","Extensive experiments on two drone-based RGBT object detection datasets demonstrate that the correction results are both visually and quantitatively favorable, thereby improving the detection performance.","In particular, our CBC module boosts the precision of the sensed modality ground truth by 25.52 aSim points.","Overall, the proposed detector achieves an mAP_50 of 43.55 points on RGBTDronePerson and surpasses a state-of-the-art method by 8.6 mAP50 on a shift subset of DroneVehicle dataset.","The code and data will be made publicly available."],"url":"http://arxiv.org/abs/2502.09311v1"}
{"created":"2025-02-13 13:17:31","title":"Predicting Drive Test Results in Mobile Networks Using Optimization Techniques","abstract":"Mobile network operators constantly optimize their networks to ensure superior service quality and coverage. This optimization is crucial for maintaining an optimal user experience and requires extensive data collection and analysis. One of the primary methods for gathering this data is through drive tests, where technical teams use specialized equipment to collect signal information across various regions. However, drive tests are both costly and time-consuming, and they face challenges such as traffic conditions, environmental factors, and limited access to certain areas. These constraints make it difficult to replicate drive tests under similar conditions. In this study, we propose a method that enables operators to predict received signal strength at specific locations using data from other drive test points. By reducing the need for widespread drive tests, this approach allows operators to save time and resources while still obtaining the necessary data to optimize their networks and mitigate the challenges associated with traditional drive tests.","sentences":["Mobile network operators constantly optimize their networks to ensure superior service quality and coverage.","This optimization is crucial for maintaining an optimal user experience and requires extensive data collection and analysis.","One of the primary methods for gathering this data is through drive tests, where technical teams use specialized equipment to collect signal information across various regions.","However, drive tests are both costly and time-consuming, and they face challenges such as traffic conditions, environmental factors, and limited access to certain areas.","These constraints make it difficult to replicate drive tests under similar conditions.","In this study, we propose a method that enables operators to predict received signal strength at specific locations using data from other drive test points.","By reducing the need for widespread drive tests, this approach allows operators to save time and resources while still obtaining the necessary data to optimize their networks and mitigate the challenges associated with traditional drive tests."],"url":"http://arxiv.org/abs/2502.09305v1"}
{"created":"2025-02-13 13:13:44","title":"Moving Matter: Efficient Reconfiguration of Tile Arrangements by a Single Active Robot","abstract":"We consider the problem of reconfiguring a two-dimensional connected grid arrangement of passive building blocks from a start configuration to a goal configuration, using a single active robot that can move on the tiles, remove individual tiles from a given location and physically move them to a new position by walking on the remaining configuration. The objective is to determine a reconfiguration schedule that minimizes the overall makespan, while ensuring that the tile configuration remains connected. We provide both negative and positive results. (1) We present a generalized version of the problem, parameterized by weighted costs for moving with or without tiles, and show that this is NP-complete. (2) We give a polynomial-time constant-factor approximation algorithm for the case of disjoint start and target bounding boxes. In addition, our approach yields optimal carry distance for 2-scaled instances.","sentences":["We consider the problem of reconfiguring a two-dimensional connected grid arrangement of passive building blocks from a start configuration to a goal configuration, using a single active robot that can move on the tiles, remove individual tiles from a given location and physically move them to a new position by walking on the remaining configuration.","The objective is to determine a reconfiguration schedule that minimizes the overall makespan, while ensuring that the tile configuration remains connected.","We provide both negative and positive results.","(1) We present a generalized version of the problem, parameterized by weighted costs for moving with or without tiles, and show that this is NP-complete.","(2) We give a polynomial-time constant-factor approximation algorithm for the case of disjoint start and target bounding boxes.","In addition, our approach yields optimal carry distance for 2-scaled instances."],"url":"http://arxiv.org/abs/2502.09299v1"}
{"created":"2025-02-13 13:11:54","title":"When do neural networks learn world models?","abstract":"Humans develop world models that capture the underlying generation process of data. Whether neural networks can learn similar world models remains an open problem. In this work, we provide the first theoretical results for this problem, showing that in a multi-task setting, models with a low-degree bias provably recover latent data-generating variables under mild assumptions -- even if proxy tasks involve complex, non-linear functions of the latents. However, such recovery is also sensitive to model architecture. Our analysis leverages Boolean models of task solutions via the Fourier-Walsh transform and introduces new techniques for analyzing invertible Boolean transforms, which may be of independent interest. We illustrate the algorithmic implications of our results and connect them to related research areas, including self-supervised learning, out-of-distribution generalization, and the linear representation hypothesis in large language models.","sentences":["Humans develop world models that capture the underlying generation process of data.","Whether neural networks can learn similar world models remains an open problem.","In this work, we provide the first theoretical results for this problem, showing that in a multi-task setting, models with a low-degree bias provably recover latent data-generating variables under mild assumptions -- even if proxy tasks involve complex, non-linear functions of the latents.","However, such recovery is also sensitive to model architecture.","Our analysis leverages Boolean models of task solutions via the Fourier-Walsh transform and introduces new techniques for analyzing invertible Boolean transforms, which may be of independent interest.","We illustrate the algorithmic implications of our results and connect them to related research areas, including self-supervised learning, out-of-distribution generalization, and the linear representation hypothesis in large language models."],"url":"http://arxiv.org/abs/2502.09297v1"}
{"created":"2025-02-13 13:09:55","title":"A Physics-Informed Deep Learning Model for MRI Brain Motion Correction","abstract":"Background: MRI is crucial for brain imaging but is highly susceptible to motion artifacts due to long acquisition times. This study introduces PI-MoCoNet, a physics-informed motion correction network that integrates spatial and k-space information to remove motion artifacts without explicit motion parameter estimation, enhancing image fidelity and diagnostic reliability. Materials and Methods: PI-MoCoNet consists of a motion detection network (U-net with spatial averaging) to identify corrupted k-space lines and a motion correction network (U-net with Swin Transformer blocks) to reconstruct motion-free images. The correction is guided by three loss functions: reconstruction (L1), perceptual (LPIPS), and data consistency (Ldc). Motion artifacts were simulated via rigid phase encoding perturbations and evaluated on IXI and MR-ART datasets against Pix2Pix, CycleGAN, and U-net using PSNR, SSIM, and NMSE. Results: PI-MoCoNet significantly improved image quality. On IXI, for minor artifacts, PSNR increased from 34.15 dB to 45.95 dB, SSIM from 0.87 to 1.00, and NMSE reduced from 0.55% to 0.04%. For moderate artifacts, PSNR improved from 30.23 dB to 42.16 dB, SSIM from 0.80 to 0.99, and NMSE from 1.32% to 0.09%. For heavy artifacts, PSNR rose from 27.99 dB to 36.01 dB, SSIM from 0.75 to 0.97, and NMSE decreased from 2.21% to 0.36%. On MR-ART, PI-MoCoNet achieved PSNR gains of ~10 dB and SSIM improvements of up to 0.20, with NMSE reductions of ~6%. Ablation studies confirmed the importance of data consistency and perceptual losses, yielding a 1 dB PSNR gain and 0.17% NMSE reduction. Conclusions: PI-MoCoNet effectively mitigates motion artifacts in brain MRI, outperforming existing methods. Its ability to integrate spatial and k-space information makes it a promising tool for clinical use in motion-prone settings. Code: https://github.com/mosaf/PI-MoCoNet.git.","sentences":["Background: MRI is crucial for brain imaging but is highly susceptible to motion artifacts due to long acquisition times.","This study introduces PI-MoCoNet, a physics-informed motion correction network that integrates spatial and k-space information to remove motion artifacts without explicit motion parameter estimation, enhancing image fidelity and diagnostic reliability.","Materials and Methods: PI-MoCoNet consists of a motion detection network (U-net with spatial averaging) to identify corrupted k-space lines and a motion correction network (U-net with Swin Transformer blocks) to reconstruct motion-free images.","The correction is guided by three loss functions: reconstruction (L1), perceptual (LPIPS), and data consistency (Ldc).","Motion artifacts were simulated via rigid phase encoding perturbations and evaluated on IXI and MR-ART datasets against Pix2Pix, CycleGAN, and U-net using PSNR, SSIM, and NMSE.","Results: PI-MoCoNet significantly improved image quality.","On IXI, for minor artifacts, PSNR increased from 34.15 dB to 45.95 dB, SSIM from 0.87 to 1.00, and NMSE reduced from 0.55% to 0.04%.","For moderate artifacts, PSNR improved from 30.23 dB to 42.16 dB, SSIM from 0.80 to 0.99, and NMSE from 1.32% to 0.09%.","For heavy artifacts, PSNR rose from 27.99 dB to 36.01 dB, SSIM from 0.75 to 0.97, and NMSE decreased from 2.21% to 0.36%.","On MR-ART, PI-MoCoNet achieved PSNR gains of ~10 dB and SSIM improvements of up to 0.20, with NMSE reductions of ~6%.","Ablation studies confirmed the importance of data consistency and perceptual losses, yielding a 1 dB PSNR gain and 0.17% NMSE reduction.","Conclusions: PI-MoCoNet effectively mitigates motion artifacts in brain MRI, outperforming existing methods.","Its ability to integrate spatial and k-space information makes it a promising tool for clinical use in motion-prone settings.","Code: https://github.com/mosaf/PI-MoCoNet.git."],"url":"http://arxiv.org/abs/2502.09296v1"}
{"created":"2025-02-13 13:08:42","title":"Indeterminacy in Affective Computing: Considering Meaning and Context in Data Collection Practices","abstract":"Automatic Affect Prediction (AAP) uses computational analysis of input data such as text, speech, images, and physiological signals to predict various affective phenomena (e.g., emotions or moods). These models are typically constructed using supervised machine-learning algorithms, which rely heavily on labeled training datasets. In this position paper, we posit that all AAP training data are derived from human Affective Interpretation Processes, resulting in a form of Affective Meaning. Research on human affect indicates a form of complexity that is fundamental to such meaning: it can possess what we refer to here broadly as Qualities of Indeterminacy (QIs) - encompassing Subjectivity (meaning depends on who is interpreting), Uncertainty (lack of confidence regarding meanings' correctness), Ambiguity (meaning contains mutually exclusive concepts) and Vagueness (meaning is situated at different levels in a nested hierarchy). Failing to appropriately consider QIs leads to results incapable of meaningful and reliable predictions. Based on this premise, we argue that a crucial step in adequately addressing indeterminacy in AAP is the development of data collection practices for modeling corpora that involve the systematic consideration of 1) a relevant set of QIs and 2) context for the associated interpretation processes. To this end, we are 1) outlining a conceptual model of AIPs and the QIs associated with the meaning these produce and a conceptual structure of relevant context, supporting understanding of its role. Finally, we use our framework for 2) discussing examples of context-sensitivity-related challenges for addressing QIs in data collection setups. We believe our efforts can stimulate a structured discussion of both the role of aspects of indeterminacy and context in research on AAP, informing the development of better practices for data collection and analysis.","sentences":["Automatic Affect Prediction (AAP) uses computational analysis of input data such as text, speech, images, and physiological signals to predict various affective phenomena (e.g., emotions or moods).","These models are typically constructed using supervised machine-learning algorithms, which rely heavily on labeled training datasets.","In this position paper, we posit that all AAP training data are derived from human Affective Interpretation Processes, resulting in a form of Affective Meaning.","Research on human affect indicates a form of complexity that is fundamental to such meaning: it can possess what we refer to here broadly as Qualities of Indeterminacy (QIs) - encompassing Subjectivity (meaning depends on who is interpreting), Uncertainty (lack of confidence regarding meanings' correctness), Ambiguity (meaning contains mutually exclusive concepts) and Vagueness (meaning is situated at different levels in a nested hierarchy).","Failing to appropriately consider QIs leads to results incapable of meaningful and reliable predictions.","Based on this premise, we argue that a crucial step in adequately addressing indeterminacy in AAP is the development of data collection practices for modeling corpora that involve the systematic consideration of 1) a relevant set of QIs and 2) context for the associated interpretation processes.","To this end, we are 1) outlining a conceptual model of AIPs and the QIs associated with the meaning these produce and a conceptual structure of relevant context, supporting understanding of its role.","Finally, we use our framework for 2) discussing examples of context-sensitivity-related challenges for addressing QIs in data collection setups.","We believe our efforts can stimulate a structured discussion of both the role of aspects of indeterminacy and context in research on AAP, informing the development of better practices for data collection and analysis."],"url":"http://arxiv.org/abs/2502.09294v1"}
{"created":"2025-02-13 13:00:33","title":"EmoAssist: Emotional Assistant for Visual Impairment Community","abstract":"The rapid advancement of large multi-modality models (LMMs) has significantly propelled the integration of artificial intelligence into practical applications. Visual Question Answering (VQA) systems, which can process multi-modal data including vision, text, and audio, hold great potential for assisting the Visual Impairment (VI) community in navigating complex and dynamic real-world environments. However, existing VI assistive LMMs overlook the emotional needs of VI individuals, and current benchmarks lack emotional evaluation of these LMMs. To address these gaps, this paper introduces the EmoAssist Benchmark, a comprehensive benchmark designed to evaluate the assistive performance of LMMs for the VI community. To the best of our knowledge, this is the first benchmark that incorporates emotional intelligence as a key consideration. Furthermore, we propose the EmoAssist Model, an Emotion-Assistive LMM specifically designed for the VI community. The EmoAssist Model utilizes Direct Preference Optimization (DPO) to align outputs with human emotional preferences. Experiment results demonstrate that the EmoAssist Model significantly enhances the recognition of implicit emotions and intentions of VI users, delivers empathetic responses, and provides actionable guidance. Specifically, it shows respective improvements of 147.8% and 89.7% in the Empathy and Suggestion metrics on the EmoAssist Benchmark, compared to the pre-tuning LMM, and even outperforms state-of-the-art LLMs such as GPT-4o.","sentences":["The rapid advancement of large multi-modality models (LMMs) has significantly propelled the integration of artificial intelligence into practical applications.","Visual Question Answering (VQA) systems, which can process multi-modal data including vision, text, and audio, hold great potential for assisting the Visual Impairment (VI) community in navigating complex and dynamic real-world environments.","However, existing VI assistive LMMs overlook the emotional needs of VI individuals, and current benchmarks lack emotional evaluation of these LMMs.","To address these gaps, this paper introduces the EmoAssist Benchmark, a comprehensive benchmark designed to evaluate the assistive performance of LMMs for the VI community.","To the best of our knowledge, this is the first benchmark that incorporates emotional intelligence as a key consideration.","Furthermore, we propose the EmoAssist Model, an Emotion-Assistive LMM specifically designed for the VI community.","The EmoAssist Model utilizes Direct Preference Optimization (DPO) to align outputs with human emotional preferences.","Experiment results demonstrate that the EmoAssist Model significantly enhances the recognition of implicit emotions and intentions of VI users, delivers empathetic responses, and provides actionable guidance.","Specifically, it shows respective improvements of 147.8% and 89.7% in the Empathy and Suggestion metrics on the EmoAssist Benchmark, compared to the pre-tuning LMM, and even outperforms state-of-the-art LLMs such as GPT-4o."],"url":"http://arxiv.org/abs/2502.09285v1"}
{"created":"2025-02-13 12:57:15","title":"SparQLe: Speech Queries to Text Translation Through LLMs","abstract":"With the growing influence of Large Language Models (LLMs), there is increasing interest in integrating speech representations with them to enable more seamless multi-modal processing and speech understanding. This study introduces a novel approach that leverages self-supervised speech representations in combination with instruction-tuned LLMs for speech-to-text translation. The proposed approach leverages a modality adapter to align extracted speech features with instruction-tuned LLMs using English-language data. Our experiments demonstrate that this method effectively preserves the semantic content of the input speech and serves as an effective bridge between self-supervised speech models and instruction-tuned LLMs, offering a promising solution for various speech understanding applications.","sentences":["With the growing influence of Large Language Models (LLMs), there is increasing interest in integrating speech representations with them to enable more seamless multi-modal processing and speech understanding.","This study introduces a novel approach that leverages self-supervised speech representations in combination with instruction-tuned LLMs for speech-to-text translation.","The proposed approach leverages a modality adapter to align extracted speech features with instruction-tuned LLMs using English-language data.","Our experiments demonstrate that this method effectively preserves the semantic content of the input speech and serves as an effective bridge between self-supervised speech models and instruction-tuned LLMs, offering a promising solution for various speech understanding applications."],"url":"http://arxiv.org/abs/2502.09284v1"}
{"created":"2025-02-13 12:39:26","title":"FLARES: Fast and Accurate LiDAR Multi-Range Semantic Segmentation","abstract":"3D scene understanding is a critical yet challenging task in autonomous driving, primarily due to the irregularity and sparsity of LiDAR data, as well as the computational demands of processing large-scale point clouds. Recent methods leverage the range-view representation to improve processing efficiency. To mitigate the performance drop caused by information loss inherent to the \"many-to-one\" problem, where multiple nearby 3D points are mapped to the same 2D grids and only the closest is retained, prior works tend to choose a higher azimuth resolution for range-view projection. However, this can bring the drawback of reducing the proportion of pixels that carry information and heavier computation within the network. We argue that it is not the optimal solution and show that, in contrast, decreasing the resolution is more advantageous in both efficiency and accuracy. In this work, we present a comprehensive re-design of the workflow for range-view-based LiDAR semantic segmentation. Our approach addresses data representation, augmentation, and post-processing methods for improvements. Through extensive experiments on two public datasets, we demonstrate that our pipeline significantly enhances the performance of various network architectures over their baselines, paving the way for more effective LiDAR-based perception in autonomous systems.","sentences":["3D scene understanding is a critical yet challenging task in autonomous driving, primarily due to the irregularity and sparsity of LiDAR data, as well as the computational demands of processing large-scale point clouds.","Recent methods leverage the range-view representation to improve processing efficiency.","To mitigate the performance drop caused by information loss inherent to the \"many-to-one\" problem, where multiple nearby 3D points are mapped to the same 2D grids and only the closest is retained, prior works tend to choose a higher azimuth resolution for range-view projection.","However, this can bring the drawback of reducing the proportion of pixels that carry information and heavier computation within the network.","We argue that it is not the optimal solution and show that, in contrast, decreasing the resolution is more advantageous in both efficiency and accuracy.","In this work, we present a comprehensive re-design of the workflow for range-view-based LiDAR semantic segmentation.","Our approach addresses data representation, augmentation, and post-processing methods for improvements.","Through extensive experiments on two public datasets, we demonstrate that our pipeline significantly enhances the performance of various network architectures over their baselines, paving the way for more effective LiDAR-based perception in autonomous systems."],"url":"http://arxiv.org/abs/2502.09274v1"}
{"created":"2025-02-13 12:33:39","title":"LiSA: Leveraging Link Recommender to Attack Graph Neural Networks via Subgraph Injection","abstract":"Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in modeling data with graph structures, yet recent research reveals their susceptibility to adversarial attacks. Traditional attack methodologies, which rely on manipulating the original graph or adding links to artificially created nodes, often prove impractical in real-world settings. This paper introduces a novel adversarial scenario involving the injection of an isolated subgraph to deceive both the link recommender and the node classifier within a GNN system. Specifically, the link recommender is mislead to propose links between targeted victim nodes and the subgraph, encouraging users to unintentionally establish connections and that would degrade the node classification accuracy, thereby facilitating a successful attack. To address this, we present the LiSA framework, which employs a dual surrogate model and bi-level optimization to simultaneously meet two adversarial objectives. Extensive experiments on real-world datasets demonstrate the effectiveness of our method.","sentences":["Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in modeling data with graph structures, yet recent research reveals their susceptibility to adversarial attacks.","Traditional attack methodologies, which rely on manipulating the original graph or adding links to artificially created nodes, often prove impractical in real-world settings.","This paper introduces a novel adversarial scenario involving the injection of an isolated subgraph to deceive both the link recommender and the node classifier within a GNN system.","Specifically, the link recommender is mislead to propose links between targeted victim nodes and the subgraph, encouraging users to unintentionally establish connections and that would degrade the node classification accuracy, thereby facilitating a successful attack.","To address this, we present the LiSA framework, which employs a dual surrogate model and bi-level optimization to simultaneously meet two adversarial objectives.","Extensive experiments on real-world datasets demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2502.09271v1"}
{"created":"2025-02-13 12:13:25","title":"Bandit Multiclass List Classification","abstract":"We study the problem of multiclass list classification with (semi-)bandit feedback, where input examples are mapped into subsets of size $m$ of a collection of $K$ possible labels, and the feedback consists of the predicted labels which lie in the set of true labels of the given example. Our main result is for the $(\\varepsilon,\\delta)$-PAC variant of the problem for which we design an algorithm that returns an $\\varepsilon$-optimal hypothesis with high probability using a sample complexity of $O \\big( (\\mathrm{poly}(K/m) + sm / \\varepsilon^2) \\log (|H|/\\delta) \\big)$ where $H$ is the underlying (finite) hypothesis class and $s$ is an upper bound on the number of true labels for a given example. This bound improves upon known bounds for combinatorial semi-bandits whenever $s \\ll K$. Moreover, in the regime where $s = O(1)$ the leading terms in our bound match the corresponding full-information rates, implying that bandit feedback essentially comes at no cost. Our PAC learning algorithm is also computationally efficient given access to an ERM oracle for $H$. Additionally, we consider the regret minimization setting where data can be generated adversarially, and establish a regret bound of $\\widetilde O(|H| + \\sqrt{smT \\log |H|})$. Our results generalize and extend those of Erez et al. (2024) who consider the simpler single-label setting corresponding to $s=m=1$, and in fact hold for the more general contextual combinatorial semi-bandit problem with $s$-sparse rewards.","sentences":["We study the problem of multiclass list classification with (semi-)bandit feedback, where input examples are mapped into subsets of size $m$ of a collection of $K$ possible labels, and the feedback consists of the predicted labels which lie in the set of true labels of the given example.","Our main result is for the $(\\varepsilon,\\delta)$-PAC variant of the problem for which we design an algorithm that returns an $\\varepsilon$-optimal hypothesis with high probability using a sample complexity of $O \\big( (\\mathrm{poly}(K/m)","+ sm / \\varepsilon^2) \\log (|H|/\\delta) \\big)$ where $H$ is the underlying (finite) hypothesis class and $s$ is an upper bound on the number of true labels for a given example.","This bound improves upon known bounds for combinatorial semi-bandits whenever $s \\ll K$.","Moreover, in the regime where $s = O(1)$ the leading terms in our bound match the corresponding full-information rates, implying that bandit feedback essentially comes at no cost.","Our PAC learning algorithm is also computationally efficient given access to an ERM oracle for $H$. Additionally, we consider the regret minimization setting where data can be generated adversarially, and establish a regret bound of $\\widetilde O(|H| + \\sqrt{smT \\log |H|})$. Our results generalize and extend those of Erez et al. (2024) who consider the simpler single-label setting corresponding to $s=m=1$, and in fact hold for the more general contextual combinatorial semi-bandit problem with $s$-sparse rewards."],"url":"http://arxiv.org/abs/2502.09257v1"}
{"created":"2025-02-13 12:10:05","title":"AnomalyGFM: Graph Foundation Model for Zero/Few-shot Anomaly Detection","abstract":"Graph anomaly detection (GAD) aims to identify abnormal nodes that differ from the majority of the nodes in a graph, which has been attracting significant attention in recent years. Existing generalist graph models have achieved remarkable success in different graph tasks but struggle to generalize to the GAD task. This limitation arises from their difficulty in learning generalized knowledge for capturing the inherently infrequent, irregular and heterogeneous abnormality patterns in graphs from different domains. To address this challenge, we propose AnomalyGFM, a GAD-oriented graph foundation model that supports zero-shot inference and few-shot prompt tuning for GAD in diverse graph datasets. One key insight is that graph-agnostic representations for normal and abnormal classes are required to support effective zero/few-shot GAD across different graphs. Motivated by this, AnomalyGFM is pre-trained to align data-independent, learnable normal and abnormal class prototypes with node representation residuals (i.e., representation deviation of a node from its neighbors). The residual features essentially project the node information into a unified feature space where we can effectively measure the abnormality of nodes from different graphs in a consistent way. This provides a driving force for the learning of graph-agnostic, discriminative prototypes for the normal and abnormal classes, which can be used to enable zero-shot GAD on new graphs, including very large-scale graphs. If there are few-shot labeled normal nodes available in the new graphs, AnomalyGFM can further support prompt tuning to leverage these nodes for better adaptation. Comprehensive experiments on 11 widely-used GAD datasets with real anomalies, demonstrate that AnomalyGFM significantly outperforms state-of-the-art competing methods under both zero- and few-shot GAD settings.","sentences":["Graph anomaly detection (GAD) aims to identify abnormal nodes that differ from the majority of the nodes in a graph, which has been attracting significant attention in recent years.","Existing generalist graph models have achieved remarkable success in different graph tasks but struggle to generalize to the GAD task.","This limitation arises from their difficulty in learning generalized knowledge for capturing the inherently infrequent, irregular and heterogeneous abnormality patterns in graphs from different domains.","To address this challenge, we propose AnomalyGFM, a GAD-oriented graph foundation model that supports zero-shot inference and few-shot prompt tuning for GAD in diverse graph datasets.","One key insight is that graph-agnostic representations for normal and abnormal classes are required to support effective zero/few-shot GAD across different graphs.","Motivated by this, AnomalyGFM is pre-trained to align data-independent, learnable normal and abnormal class prototypes with node representation residuals (i.e., representation deviation of a node from its neighbors).","The residual features essentially project the node information into a unified feature space where we can effectively measure the abnormality of nodes from different graphs in a consistent way.","This provides a driving force for the learning of graph-agnostic, discriminative prototypes for the normal and abnormal classes, which can be used to enable zero-shot GAD on new graphs, including very large-scale graphs.","If there are few-shot labeled normal nodes available in the new graphs, AnomalyGFM can further support prompt tuning to leverage these nodes for better adaptation.","Comprehensive experiments on 11 widely-used GAD datasets with real anomalies, demonstrate that AnomalyGFM significantly outperforms state-of-the-art competing methods under both zero- and few-shot GAD settings."],"url":"http://arxiv.org/abs/2502.09254v1"}
{"created":"2025-02-13 12:09:17","title":"On the Importance of Embedding Norms in Self-Supervised Learning","abstract":"Self-supervised learning (SSL) allows training data representations without a supervised signal and has become an important paradigm in machine learning. Most SSL methods employ the cosine similarity between embedding vectors and hence effectively embed data on a hypersphere. While this seemingly implies that embedding norms cannot play any role in SSL, a few recent works have suggested that embedding norms have properties related to network convergence and confidence. In this paper, we resolve this apparent contradiction and systematically establish the embedding norm's role in SSL training. Using theoretical analysis, simulations, and experiments, we show that embedding norms (i) govern SSL convergence rates and (ii) encode network confidence, with smaller norms corresponding to unexpected samples. Additionally, we show that manipulating embedding norms can have large effects on convergence speed. Our findings demonstrate that SSL embedding norms are integral to understanding and optimizing network behavior.","sentences":["Self-supervised learning (SSL) allows training data representations without a supervised signal and has become an important paradigm in machine learning.","Most SSL methods employ the cosine similarity between embedding vectors and hence effectively embed data on a hypersphere.","While this seemingly implies that embedding norms cannot play any role in SSL, a few recent works have suggested that embedding norms have properties related to network convergence and confidence.","In this paper, we resolve this apparent contradiction and systematically establish the embedding norm's role in SSL training.","Using theoretical analysis, simulations, and experiments, we show that embedding norms (i) govern SSL convergence rates and (ii) encode network confidence, with smaller norms corresponding to unexpected samples.","Additionally, we show that manipulating embedding norms can have large effects on convergence speed.","Our findings demonstrate that SSL embedding norms are integral to understanding and optimizing network behavior."],"url":"http://arxiv.org/abs/2502.09252v1"}
{"created":"2025-02-13 12:00:11","title":"Memristor-Based Meta-Learning for Fast mmWave Beam Prediction in Non-Stationary Environments","abstract":"Traditional machine learning techniques have achieved great success in improving data-rate performance and reducing latency in millimeter wave (mmWave) communications. However, these methods still face two key challenges: (i) their reliance on large-scale paired data for model training and tuning which limits performance gains and makes beam predictions outdated, especially in multi-user mmWave systems with large antenna arrays, and (ii) meta-learning (ML)-based beamforming solutions are prone to overfitting when trained on a limited number of tasks. To address these issues, we propose a memristorbased meta-learning (M-ML) framework for predicting mmWave beam in real time. The M-ML framework generates optimal initialization parameters during the training phase, providing a strong starting point for adapting to unknown environments during the testing phase. By leveraging memory to store key data, M-ML ensures the predicted beamforming vectors are wellsuited to episodically dynamic channel distributions, even when testing and training environments do not align. Simulation results show that our approach delivers high prediction accuracy in new environments, without relying on large datasets. Moreover, MML enhances the model's generalization ability and adaptability.","sentences":["Traditional machine learning techniques have achieved great success in improving data-rate performance and reducing latency in millimeter wave (mmWave) communications.","However, these methods still face two key challenges: (i) their reliance on large-scale paired data for model training and tuning which limits performance gains and makes beam predictions outdated, especially in multi-user mmWave systems with large antenna arrays, and (ii) meta-learning (ML)-based beamforming solutions are prone to overfitting when trained on a limited number of tasks.","To address these issues, we propose a memristorbased meta-learning (M-ML) framework for predicting mmWave beam in real time.","The M-ML framework generates optimal initialization parameters during the training phase, providing a strong starting point for adapting to unknown environments during the testing phase.","By leveraging memory to store key data, M-ML ensures the predicted beamforming vectors are wellsuited to episodically dynamic channel distributions, even when testing and training environments do not align.","Simulation results show that our approach delivers high prediction accuracy in new environments, without relying on large datasets.","Moreover, MML enhances the model's generalization ability and adaptability."],"url":"http://arxiv.org/abs/2502.09244v1"}
{"created":"2025-02-13 11:57:51","title":"From large language models to multimodal AI: A scoping review on the potential of generative AI in medicine","abstract":"Generative artificial intelligence (AI) models, such as diffusion models and OpenAI's ChatGPT, are transforming medicine by enhancing diagnostic accuracy and automating clinical workflows. The field has advanced rapidly, evolving from text-only large language models for tasks such as clinical documentation and decision support to multimodal AI systems capable of integrating diverse data modalities, including imaging, text, and structured data, within a single model. The diverse landscape of these technologies, along with rising interest, highlights the need for a comprehensive review of their applications and potential. This scoping review explores the evolution of multimodal AI, highlighting its methods, applications, datasets, and evaluation in clinical settings. Adhering to PRISMA-ScR guidelines, we systematically queried PubMed, IEEE Xplore, and Web of Science, prioritizing recent studies published up to the end of 2024. After rigorous screening, 144 papers were included, revealing key trends and challenges in this dynamic field. Our findings underscore a shift from unimodal to multimodal approaches, driving innovations in diagnostic support, medical report generation, drug discovery, and conversational AI. However, critical challenges remain, including the integration of heterogeneous data types, improving model interpretability, addressing ethical concerns, and validating AI systems in real-world clinical settings. This review summarizes the current state of the art, identifies critical gaps, and provides insights to guide the development of scalable, trustworthy, and clinically impactful multimodal AI solutions in healthcare.","sentences":["Generative artificial intelligence (AI) models, such as diffusion models and OpenAI's ChatGPT, are transforming medicine by enhancing diagnostic accuracy and automating clinical workflows.","The field has advanced rapidly, evolving from text-only large language models for tasks such as clinical documentation and decision support to multimodal AI systems capable of integrating diverse data modalities, including imaging, text, and structured data, within a single model.","The diverse landscape of these technologies, along with rising interest, highlights the need for a comprehensive review of their applications and potential.","This scoping review explores the evolution of multimodal AI, highlighting its methods, applications, datasets, and evaluation in clinical settings.","Adhering to PRISMA-ScR guidelines, we systematically queried PubMed, IEEE Xplore, and Web of Science, prioritizing recent studies published up to the end of 2024.","After rigorous screening, 144 papers were included, revealing key trends and challenges in this dynamic field.","Our findings underscore a shift from unimodal to multimodal approaches, driving innovations in diagnostic support, medical report generation, drug discovery, and conversational AI.","However, critical challenges remain, including the integration of heterogeneous data types, improving model interpretability, addressing ethical concerns, and validating AI systems in real-world clinical settings.","This review summarizes the current state of the art, identifies critical gaps, and provides insights to guide the development of scalable, trustworthy, and clinically impactful multimodal AI solutions in healthcare."],"url":"http://arxiv.org/abs/2502.09242v1"}
{"created":"2025-02-13 11:53:25","title":"Commonsense Reasoning-Aided Autonomous Vehicle Systems","abstract":"Autonomous Vehicle (AV) systems have been developed with a strong reliance on machine learning techniques. While machine learning approaches, such as deep learning, are extremely effective at tasks that involve observation and classification, they struggle when it comes to performing higher level reasoning about situations on the road. This research involves incorporating commonsense reasoning models that use image data to improve AV systems. This will allow AV systems to perform more accurate reasoning while also making them more adjustable, explainable, and ethical. This paper will discuss the findings so far and motivate its direction going forward.","sentences":["Autonomous Vehicle (AV) systems have been developed with a strong reliance on machine learning techniques.","While machine learning approaches, such as deep learning, are extremely effective at tasks that involve observation and classification, they struggle when it comes to performing higher level reasoning about situations on the road.","This research involves incorporating commonsense reasoning models that use image data to improve AV systems.","This will allow AV systems to perform more accurate reasoning while also making them more adjustable, explainable, and ethical.","This paper will discuss the findings so far and motivate its direction going forward."],"url":"http://arxiv.org/abs/2502.09233v1"}
{"created":"2025-02-13 11:52:40","title":"Relating Answer Set Programming and Many-sorted Logics for Formal Verification","abstract":"Answer Set Programming (ASP) is an important logic programming paradigm within the field of Knowledge Representation and Reasoning. As a concise, human-readable, declarative language, ASP is an excellent tool for developing trustworthy (especially, artificially intelligent) software systems. However, formally verifying ASP programs offers some unique challenges, such as   1. a lack of modularity (the meanings of rules are difficult to define in isolation from the enclosing program),   2. the ground-and-solve semantics (the meanings of rules are dependent on the input data with which the program is grounded), and   3. limitations of existing tools.   My research agenda has been focused on addressing these three issues with the intention of making ASP verification an accessible, routine task that is regularly performed alongside program development. In this vein, I have investigated alternative semantics for ASP based on translations into the logic of here-and-there and many-sorted first-order logic. These semantics promote a modular understanding of logic programs, bypass grounding, and enable us to use automated theorem provers to automatically verify properties of programs.","sentences":["Answer Set Programming (ASP) is an important logic programming paradigm within the field of Knowledge Representation and Reasoning.","As a concise, human-readable, declarative language, ASP is an excellent tool for developing trustworthy (especially, artificially intelligent) software systems.","However, formally verifying ASP programs offers some unique challenges, such as   1.","a lack of modularity (the meanings of rules are difficult to define in isolation from the enclosing program),   2.","the ground-and-solve semantics (the meanings of rules are dependent on the input data with which the program is grounded), and   3. limitations of existing tools.   ","My research agenda has been focused on addressing these three issues with the intention of making ASP verification an accessible, routine task that is regularly performed alongside program development.","In this vein, I have investigated alternative semantics for ASP based on translations into the logic of here-and-there and many-sorted first-order logic.","These semantics promote a modular understanding of logic programs, bypass grounding, and enable us to use automated theorem provers to automatically verify properties of programs."],"url":"http://arxiv.org/abs/2502.09230v1"}
{"created":"2025-02-13 11:52:09","title":"Bridging Logic Programming and Deep Learning for Explainability through ILASP","abstract":"My research explores integrating deep learning and logic programming to set the basis for a new generation of AI systems. By combining neural networks with Inductive Logic Programming (ILP), the goal is to construct systems that make accurate predictions and generate comprehensible rules to validate these predictions. Deep learning models process and analyze complex data, while ILP techniques derive logical rules to prove the network's conclusions. Explainable AI methods, like eXplainable Answer Set Programming (XASP), elucidate the reasoning behind these rules and decisions. The focus is on applying ILP frameworks, specifically ILASP and FastLAS, to enhance explainability in various domains. My test cases span weather prediction, the legal field, and image recognition. In weather forecasting, the system will predict events and provides explanations using FastLAS, with plans to integrate recurrent neural networks in the future. In the legal domain, the research focuses on interpreting vague decisions and assisting legal professionals by encoding Italian legal articles and learning reasoning patterns from Court of Cassation decisions using ILASP. For biological laboratories, we will collaborate with a research group to automate spermatozoa morphology classification for Bull Breeding Soundness Evaluation using YOLO networks and ILP to explain classification outcomes. This hybrid approach aims to bridge the gap between the high performance of deep learning models and the transparency of symbolic reasoning, advancing AI by providing interpretable and trustworthy applications.","sentences":["My research explores integrating deep learning and logic programming to set the basis for a new generation of AI systems.","By combining neural networks with Inductive Logic Programming (ILP), the goal is to construct systems that make accurate predictions and generate comprehensible rules to validate these predictions.","Deep learning models process and analyze complex data, while ILP techniques derive logical rules to prove the network's conclusions.","Explainable AI methods, like eXplainable Answer Set Programming (XASP), elucidate the reasoning behind these rules and decisions.","The focus is on applying ILP frameworks, specifically ILASP and FastLAS, to enhance explainability in various domains.","My test cases span weather prediction, the legal field, and image recognition.","In weather forecasting, the system will predict events and provides explanations using FastLAS, with plans to integrate recurrent neural networks in the future.","In the legal domain, the research focuses on interpreting vague decisions and assisting legal professionals by encoding Italian legal articles and learning reasoning patterns from Court of Cassation decisions using ILASP.","For biological laboratories, we will collaborate with a research group to automate spermatozoa morphology classification for Bull Breeding Soundness Evaluation using YOLO networks and ILP to explain classification outcomes.","This hybrid approach aims to bridge the gap between the high performance of deep learning models and the transparency of symbolic reasoning, advancing AI by providing interpretable and trustworthy applications."],"url":"http://arxiv.org/abs/2502.09227v1"}
{"created":"2025-02-13 11:50:04","title":"Abduction of Domain Relationships from Data for VQA","abstract":"In this paper, we study the problem of visual question answering (VQA) where the image and query are represented by ASP programs that lack domain data. We provide an approach that is orthogonal and complementary to existing knowledge augmentation techniques where we abduce domain relationships of image constructs from past examples. After framing the abduction problem, we provide a baseline approach, and an implementation that significantly improves the accuracy of query answering yet requires few examples.","sentences":["In this paper, we study the problem of visual question answering (VQA) where the image and query are represented by ASP programs that lack domain data.","We provide an approach that is orthogonal and complementary to existing knowledge augmentation techniques where we abduce domain relationships of image constructs from past examples.","After framing the abduction problem, we provide a baseline approach, and an implementation that significantly improves the accuracy of query answering yet requires few examples."],"url":"http://arxiv.org/abs/2502.09219v1"}
{"created":"2025-02-13 11:49:48","title":"Data2Concept2Text: An Explainable Multilingual Framework for Data Analysis Narration","abstract":"This paper presents a complete explainable system that interprets a set of data, abstracts the underlying features and describes them in a natural language of choice. The system relies on two crucial stages: (i) identifying emerging properties from data and transforming them into abstract concepts, and (ii) converting these concepts into natural language. Despite the impressive natural language generation capabilities demonstrated by Large Language Models, their statistical nature and the intricacy of their internal mechanism still force us to employ these techniques as black boxes, forgoing trustworthiness. Developing an explainable pipeline for data interpretation would allow facilitating its use in safety-critical environments like processing medical information and allowing non-experts and visually impaired people to access narrated information. To this end, we believe that the fields of knowledge representation and automated reasoning research could present a valid alternative. Expanding on prior research that tackled the first stage (i), we focus on the second stage, named Concept2Text. Being explainable, data translation is easily modeled through logic-based rules, once again emphasizing the role of declarative programming in achieving AI explainability. This paper explores a Prolog/CLP-based rewriting system to interpret concepts-articulated in terms of classes and relations, plus common knowledge-derived from a generic ontology, generating natural language text. Its main features include hierarchical tree rewritings, modular multilingual generation, support for equivalent variants across semantic, grammar, and lexical levels, and a transparent rule-based system. We outline the architecture and demonstrate its flexibility through some examples capable of generating numerous diverse and equivalent rewritings based on the input concept.","sentences":["This paper presents a complete explainable system that interprets a set of data, abstracts the underlying features and describes them in a natural language of choice.","The system relies on two crucial stages: (i) identifying emerging properties from data and transforming them into abstract concepts, and (ii) converting these concepts into natural language.","Despite the impressive natural language generation capabilities demonstrated by Large Language Models, their statistical nature and the intricacy of their internal mechanism still force us to employ these techniques as black boxes, forgoing trustworthiness.","Developing an explainable pipeline for data interpretation would allow facilitating its use in safety-critical environments like processing medical information and allowing non-experts and visually impaired people to access narrated information.","To this end, we believe that the fields of knowledge representation and automated reasoning research could present a valid alternative.","Expanding on prior research that tackled the first stage (i), we focus on the second stage, named Concept2Text.","Being explainable, data translation is easily modeled through logic-based rules, once again emphasizing the role of declarative programming in achieving AI explainability.","This paper explores a Prolog/CLP-based rewriting system to interpret concepts-articulated in terms of classes and relations, plus common knowledge-derived from a generic ontology, generating natural language text.","Its main features include hierarchical tree rewritings, modular multilingual generation, support for equivalent variants across semantic, grammar, and lexical levels, and a transparent rule-based system.","We outline the architecture and demonstrate its flexibility through some examples capable of generating numerous diverse and equivalent rewritings based on the input concept."],"url":"http://arxiv.org/abs/2502.09218v1"}
{"created":"2025-02-13 11:48:46","title":"Neuro-Symbolic Contrastive Learning for Cross-domain Inference","abstract":"Pre-trained language models (PLMs) have made significant advances in natural language inference (NLI) tasks, however their sensitivity to textual perturbations and dependence on large datasets indicate an over-reliance on shallow heuristics. In contrast, inductive logic programming (ILP) excels at inferring logical relationships across diverse, sparse and limited datasets, but its discrete nature requires the inputs to be precisely specified, which limits their application. This paper proposes a bridge between the two approaches: neuro-symbolic contrastive learning. This allows for smooth and differentiable optimisation that improves logical accuracy across an otherwise discrete, noisy, and sparse topological space of logical functions. We show that abstract logical relationships can be effectively embedded within a neuro-symbolic paradigm, by representing data as logic programs and sets of logic rules. The embedding space captures highly varied textual information with similar semantic logical relations, but can also separate similar textual relations that have dissimilar logical relations. Experimental results demonstrate that our approach significantly improves the inference capabilities of the models in terms of generalisation and reasoning.","sentences":["Pre-trained language models (PLMs) have made significant advances in natural language inference (NLI) tasks, however their sensitivity to textual perturbations and dependence on large datasets indicate an over-reliance on shallow heuristics.","In contrast, inductive logic programming (ILP) excels at inferring logical relationships across diverse, sparse and limited datasets, but its discrete nature requires the inputs to be precisely specified, which limits their application.","This paper proposes a bridge between the two approaches: neuro-symbolic contrastive learning.","This allows for smooth and differentiable optimisation that improves logical accuracy across an otherwise discrete, noisy, and sparse topological space of logical functions.","We show that abstract logical relationships can be effectively embedded within a neuro-symbolic paradigm, by representing data as logic programs and sets of logic rules.","The embedding space captures highly varied textual information with similar semantic logical relations, but can also separate similar textual relations that have dissimilar logical relations.","Experimental results demonstrate that our approach significantly improves the inference capabilities of the models in terms of generalisation and reasoning."],"url":"http://arxiv.org/abs/2502.09213v1"}
{"created":"2025-02-13 11:47:44","title":"On LLM-generated Logic Programs and their Inference Execution Methods","abstract":"Large Language Models (LLMs) trained on petabytes of data are highly compressed repositories of a significant proportion of the knowledge accumulated and distilled so far. In this paper we study techniques to elicit this knowledge in the form of several classes of logic programs, including propositional Horn clauses, Dual Horn clauses, relational triplets and Definite Clause Grammars. Exposing this knowledge as logic programs enables sound reasoning methods that can verify alignment of LLM outputs to their intended uses and extend their inference capabilities. We study new execution methods for the generated programs, including soft-unification of abducible facts against LLM-generated content stored in a vector database as well as GPU-based acceleration of minimal model computation that supports inference with large LLM-generated programs.","sentences":["Large Language Models (LLMs) trained on petabytes of data are highly compressed repositories of a significant proportion of the knowledge accumulated and distilled so far.","In this paper we study techniques to elicit this knowledge in the form of several classes of logic programs, including propositional Horn clauses, Dual Horn clauses, relational triplets and Definite Clause Grammars.","Exposing this knowledge as logic programs enables sound reasoning methods that can verify alignment of LLM outputs to their intended uses and extend their inference capabilities.","We study new execution methods for the generated programs, including soft-unification of abducible facts against LLM-generated content stored in a vector database as well as GPU-based acceleration of minimal model computation that supports inference with large LLM-generated programs."],"url":"http://arxiv.org/abs/2502.09209v1"}
{"created":"2025-02-13 11:43:43","title":"Revisiting Euclidean Alignment for Transfer Learning in EEG-Based Brain-Computer Interfaces","abstract":"Due to the non-stationarity and large individual differences of EEG signals, EEG-based brain-computer interfaces (BCIs) usually need subject-specific calibration to tailor the decoding algorithm for each new subject, which is time-consuming and user-unfriendly, hindering their real-world applications. Transfer learning (TL) has been extensively used to expedite the calibration, by making use of EEG data from other subjects/sessions. An important consideration in TL for EEG-based BCIs is to reduce the data distribution discrepancies among different subjects/session, to avoid negative transfer. Euclidean alignment (EA) was proposed in 2020 to address this challenge. Numerous experiments from 10 different BCI paradigms demonstrated its effectiveness and efficiency. This paper revisits the EA, explaining its procedure and correct usage, introducing its applications and extensions, and pointing out potential new research directions. It should be very helpful to BCI researchers, especially those who are working on EEG signal decoding.","sentences":["Due to the non-stationarity and large individual differences of EEG signals, EEG-based brain-computer interfaces (BCIs) usually need subject-specific calibration to tailor the decoding algorithm for each new subject, which is time-consuming and user-unfriendly, hindering their real-world applications.","Transfer learning (TL) has been extensively used to expedite the calibration, by making use of EEG data from other subjects/sessions.","An important consideration in TL for EEG-based BCIs is to reduce the data distribution discrepancies among different subjects/session, to avoid negative transfer.","Euclidean alignment (EA) was proposed in 2020 to address this challenge.","Numerous experiments from 10 different BCI paradigms demonstrated its effectiveness and efficiency.","This paper revisits the EA, explaining its procedure and correct usage, introducing its applications and extensions, and pointing out potential new research directions.","It should be very helpful to BCI researchers, especially those who are working on EEG signal decoding."],"url":"http://arxiv.org/abs/2502.09203v1"}
{"created":"2025-02-13 11:39:58","title":"Commitment Schemes from OWFs with Applications to qOT","abstract":"Commitment schemes are essential to many cryptographic protocols and schemes with applications that include privacy-preserving computation on data, privacy-preserving authentication, and, in particular, oblivious transfer protocols. For quantum oblivious transfer (qOT) protocols, unconditionally binding commitment schemes that do not rely on hardness assumptions from structured mathematical problems are required. These additional constraints severely limit the choice of commitment schemes to random oracle-based constructions or Naor's bit commitment scheme. As these protocols commit to individual bits, the use of such commitment schemes comes at a high bandwidth and computational cost.   In this work, we investigate improvements to the efficiency of commitment schemes used in qOT protocols and propose an extension of Naor's commitment scheme requiring the existence of one-way functions (OWF) to reduce communication complexity for 2-bit strings. Additionally, we provide an interactive string commitment scheme with preprocessing to enable a fast and efficient computation of commitments.","sentences":["Commitment schemes are essential to many cryptographic protocols and schemes with applications that include privacy-preserving computation on data, privacy-preserving authentication, and, in particular, oblivious transfer protocols.","For quantum oblivious transfer (qOT) protocols, unconditionally binding commitment schemes that do not rely on hardness assumptions from structured mathematical problems are required.","These additional constraints severely limit the choice of commitment schemes to random oracle-based constructions or Naor's bit commitment scheme.","As these protocols commit to individual bits, the use of such commitment schemes comes at a high bandwidth and computational cost.   ","In this work, we investigate improvements to the efficiency of commitment schemes used in qOT protocols and propose an extension of Naor's commitment scheme requiring the existence of one-way functions (OWF) to reduce communication complexity for 2-bit strings.","Additionally, we provide an interactive string commitment scheme with preprocessing to enable a fast and efficient computation of commitments."],"url":"http://arxiv.org/abs/2502.09201v1"}
{"created":"2025-02-13 11:33:29","title":"XAInomaly: Explainable and Interpretable Deep Contractive Autoencoder for O-RAN Traffic Anomaly Detection","abstract":"Generative Artificial Intelligence (AI) techniques have become integral part in advancing next generation wireless communication systems by enabling sophisticated data modeling and feature extraction for enhanced network performance. In the realm of open radio access networks (O-RAN), characterized by their disaggregated architecture and heterogeneous components from multiple vendors, the deployment of generative models offers significant advantages for network management such as traffic analysis, traffic forecasting and anomaly detection. However, the complex and dynamic nature of O-RAN introduces challenges that necessitate not only accurate detection mechanisms but also reduced complexity, scalability, and most importantly interpretability to facilitate effective network management. In this study, we introduce the XAInomaly framework, an explainable and interpretable Semi-supervised (SS) Deep Contractive Autoencoder (DeepCAE) design for anomaly detection in O-RAN. Our approach leverages the generative modeling capabilities of our SS-DeepCAE model to learn compressed, robust representations of normal network behavior, which captures essential features, enabling the identification of deviations indicative of anomalies. To address the black-box nature of deep learning models, we propose reactive Explainable AI (XAI) technique called fastshap-C.","sentences":["Generative Artificial Intelligence (AI) techniques have become integral part in advancing next generation wireless communication systems by enabling sophisticated data modeling and feature extraction for enhanced network performance.","In the realm of open radio access networks (O-RAN), characterized by their disaggregated architecture and heterogeneous components from multiple vendors, the deployment of generative models offers significant advantages for network management such as traffic analysis, traffic forecasting and anomaly detection.","However, the complex and dynamic nature of O-RAN introduces challenges that necessitate not only accurate detection mechanisms but also reduced complexity, scalability, and most importantly interpretability to facilitate effective network management.","In this study, we introduce the XAInomaly framework, an explainable and interpretable Semi-supervised (SS)","Deep Contractive Autoencoder (DeepCAE) design for anomaly detection in O-RAN.","Our approach leverages the generative modeling capabilities of our SS-DeepCAE model to learn compressed, robust representations of normal network behavior, which captures essential features, enabling the identification of deviations indicative of anomalies.","To address the black-box nature of deep learning models, we propose reactive Explainable AI (XAI) technique called fastshap-C."],"url":"http://arxiv.org/abs/2502.09194v1"}
{"created":"2025-02-13 11:33:17","title":"Generalizability through Explainability: Countering Overfitting with Counterfactual Examples","abstract":"Overfitting is a well-known issue in machine learning that occurs when a model struggles to generalize its predictions to new, unseen data beyond the scope of its training set. Traditional techniques to mitigate overfitting include early stopping, data augmentation, and regularization. In this work, we demonstrate that the degree of overfitting of a trained model is correlated with the ability to generate counterfactual examples. The higher the overfitting, the easier it will be to find a valid counterfactual example for a randomly chosen input data point. Therefore, we introduce CF-Reg, a novel regularization term in the training loss that controls overfitting by ensuring enough margin between each instance and its corresponding counterfactual. Experiments conducted across multiple datasets and models show that our counterfactual regularizer generally outperforms existing regularization techniques.","sentences":["Overfitting is a well-known issue in machine learning that occurs when a model struggles to generalize its predictions to new, unseen data beyond the scope of its training set.","Traditional techniques to mitigate overfitting include early stopping, data augmentation, and regularization.","In this work, we demonstrate that the degree of overfitting of a trained model is correlated with the ability to generate counterfactual examples.","The higher the overfitting, the easier it will be to find a valid counterfactual example for a randomly chosen input data point.","Therefore, we introduce CF-Reg, a novel regularization term in the training loss that controls overfitting by ensuring enough margin between each instance and its corresponding counterfactual.","Experiments conducted across multiple datasets and models show that our counterfactual regularizer generally outperforms existing regularization techniques."],"url":"http://arxiv.org/abs/2502.09193v1"}
{"created":"2025-02-13 11:27:19","title":"Data Structures for Finite Downsets of Natural Vectors: Theory and Practice","abstract":"Manipulating downward-closed sets of vectors forms the basis of so-called antichain-based algorithms in verification. In that context, the dimension of the vectors is intimately tied to the size of the input structure to be verified. In this work, we formally analyze the complexity of classical list-based algorithms to manipulate antichains as well as that of Zampuni\\'eris's sharing trees and traditional and novel kdtree-based antichain algorithms. In contrast to the existing literature, and to better address the needs of formal verification, our analysis of \\kdtree algorithms does not assume that the dimension of the vectors is fixed. Our theoretical results show that kdtrees are asymptotically better than both list- and sharing-tree-based algorithms, as an antichain data structure, when the antichains become exponentially larger than the dimension of the vectors. We evaluate this on applications in the synthesis of reactive systems from linear-temporal logic and parity-objective specifications, and establish empirically that current benchmarks for these computational tasks do not lead to a favorable situation for current implementations of kdtrees.","sentences":["Manipulating downward-closed sets of vectors forms the basis of so-called antichain-based algorithms in verification.","In that context, the dimension of the vectors is intimately tied to the size of the input structure to be verified.","In this work, we formally analyze the complexity of classical list-based algorithms to manipulate antichains as well as that of Zampuni\\'eris's sharing trees and traditional and novel kdtree-based antichain algorithms.","In contrast to the existing literature, and to better address the needs of formal verification, our analysis of \\kdtree algorithms does not assume that the dimension of the vectors is fixed.","Our theoretical results show that kdtrees are asymptotically better than both list- and sharing-tree-based algorithms, as an antichain data structure, when the antichains become exponentially larger than the dimension of the vectors.","We evaluate this on applications in the synthesis of reactive systems from linear-temporal logic and parity-objective specifications, and establish empirically that current benchmarks for these computational tasks do not lead to a favorable situation for current implementations of kdtrees."],"url":"http://arxiv.org/abs/2502.09189v1"}
{"created":"2025-02-13 11:22:19","title":"Matina: A Large-Scale 73B Token Persian Text Corpus","abstract":"Text corpora are essential for training models used in tasks like summarization, translation, and large language models (LLMs). While various efforts have been made to collect monolingual and multilingual datasets in many languages, Persian has often been underrepresented due to limited resources for data collection and preprocessing. Existing Persian datasets are typically small and lack content diversity, consisting mainly of weblogs and news articles. This shortage of high-quality, varied data has slowed the development of NLP models and open-source LLMs for Persian. Since model performance depends heavily on the quality of training data, we address this gap by introducing the Matina corpus, a new Persian dataset of 72.9B tokens, carefully preprocessed and deduplicated to ensure high data quality. We further assess its effectiveness by training and evaluating transformer-based models on key NLP tasks. Both the dataset and preprocessing codes are publicly available, enabling researchers to build on and improve this resource for future Persian NLP advancements.","sentences":["Text corpora are essential for training models used in tasks like summarization, translation, and large language models (LLMs).","While various efforts have been made to collect monolingual and multilingual datasets in many languages, Persian has often been underrepresented due to limited resources for data collection and preprocessing.","Existing Persian datasets are typically small and lack content diversity, consisting mainly of weblogs and news articles.","This shortage of high-quality, varied data has slowed the development of NLP models and open-source LLMs for Persian.","Since model performance depends heavily on the quality of training data, we address this gap by introducing the Matina corpus, a new Persian dataset of 72.9B tokens, carefully preprocessed and deduplicated to ensure high data quality.","We further assess its effectiveness by training and evaluating transformer-based models on key NLP tasks.","Both the dataset and preprocessing codes are publicly available, enabling researchers to build on and improve this resource for future Persian NLP advancements."],"url":"http://arxiv.org/abs/2502.09188v1"}
{"created":"2025-02-13 11:17:53","title":"RefineCoder: Iterative Improving of Large Language Models via Adaptive Critique Refinement for Code Generation","abstract":"Code generation has attracted increasing attention with the rise of Large Language Models (LLMs). Many studies have developed powerful code LLMs by synthesizing code-related instruction data and applying supervised fine-tuning. However, these methods are limited by teacher model distillation and ignore the potential of iterative refinement by self-generated code. In this paper, we propose Adaptive Critique Refinement (ACR), which enables the model to refine itself by self-generated code and external critique, rather than directly imitating the code responses of the teacher model. Concretely, ACR includes a composite scoring system with LLM-as-a-Judge to evaluate the quality of code responses and a selective critique strategy with LLM-as-a-Critic to critique self-generated low-quality code responses. We develop the RefineCoder series by iteratively applying ACR, achieving continuous performance improvement on multiple code generation benchmarks. Compared to the baselines of the same size, our proposed RefineCoder series can achieve comparable or even superior performance using less data.","sentences":["Code generation has attracted increasing attention with the rise of Large Language Models (LLMs).","Many studies have developed powerful code LLMs by synthesizing code-related instruction data and applying supervised fine-tuning.","However, these methods are limited by teacher model distillation and ignore the potential of iterative refinement by self-generated code.","In this paper, we propose Adaptive Critique Refinement (ACR), which enables the model to refine itself by self-generated code and external critique, rather than directly imitating the code responses of the teacher model.","Concretely, ACR includes a composite scoring system with LLM-as-a-Judge to evaluate the quality of code responses and a selective critique strategy with LLM-as-a-Critic to critique self-generated low-quality code responses.","We develop the RefineCoder series by iteratively applying ACR, achieving continuous performance improvement on multiple code generation benchmarks.","Compared to the baselines of the same size, our proposed RefineCoder series can achieve comparable or even superior performance using less data."],"url":"http://arxiv.org/abs/2502.09183v1"}
{"created":"2025-02-13 11:15:37","title":"A Machine Learning Approach to Sensor Substitution for Non-Prehensile Manipulation","abstract":"Mobile manipulators are increasingly deployed in complex environments, requiring diverse sensors to perceive and interact with their surroundings. However, equipping every robot with every possible sensor is often impractical due to cost and physical constraints. A critical challenge arises when robots with differing sensor capabilities need to collaborate or perform similar tasks. For example, consider a scenario where a mobile manipulator equipped with high-resolution tactile skin is skilled at non-prehensile manipulation tasks like pushing. If this robot needs to be replaced or augmented by a robot lacking such tactile sensing, the learned manipulation policies become inapplicable. This paper addresses the problem of sensor substitution in non-prehensile manipulation. We propose a novel machine learning-based framework that enables a robot with a limited sensor set (e.g., LiDAR or RGB-D camera) to effectively perform tasks previously reliant on a richer sensor suite (e.g., tactile skin). Our approach learns a mapping between the available sensor data and the information provided by the substituted sensor, effectively synthesizing the missing sensory input. Specifically, we demonstrate the efficacy of our framework by training a model to substitute tactile skin data for the task of non-prehensile pushing using a mobile manipulator. We show that a manipulator equipped only with LiDAR or RGB-D can, after training, achieve comparable and sometimes even better pushing performance to a mobile base utilizing direct tactile feedback.","sentences":["Mobile manipulators are increasingly deployed in complex environments, requiring diverse sensors to perceive and interact with their surroundings.","However, equipping every robot with every possible sensor is often impractical due to cost and physical constraints.","A critical challenge arises when robots with differing sensor capabilities need to collaborate or perform similar tasks.","For example, consider a scenario where a mobile manipulator equipped with high-resolution tactile skin is skilled at non-prehensile manipulation tasks like pushing.","If this robot needs to be replaced or augmented by a robot lacking such tactile sensing, the learned manipulation policies become inapplicable.","This paper addresses the problem of sensor substitution in non-prehensile manipulation.","We propose a novel machine learning-based framework that enables a robot with a limited sensor set (e.g., LiDAR or RGB-D camera) to effectively perform tasks previously reliant on a richer sensor suite (e.g., tactile skin).","Our approach learns a mapping between the available sensor data and the information provided by the substituted sensor, effectively synthesizing the missing sensory input.","Specifically, we demonstrate the efficacy of our framework by training a model to substitute tactile skin data for the task of non-prehensile pushing using a mobile manipulator.","We show that a manipulator equipped only with LiDAR or RGB-D can, after training, achieve comparable and sometimes even better pushing performance to a mobile base utilizing direct tactile feedback."],"url":"http://arxiv.org/abs/2502.09180v1"}
{"created":"2025-02-13 10:57:25","title":"Two-Stage Representation Learning for Analyzing Movement Behavior Dynamics in People Living with Dementia","abstract":"In remote healthcare monitoring, time series representation learning reveals critical patient behavior patterns from high-frequency data. This study analyzes home activity data from individuals living with dementia by proposing a two-stage, self-supervised learning approach tailored to uncover low-rank structures. The first stage converts time-series activities into text sequences encoded by a pre-trained language model, providing a rich, high-dimensional latent state space using a PageRank-based method. This PageRank vector captures latent state transitions, effectively compressing complex behaviour data into a succinct form that enhances interpretability. This low-rank representation not only enhances model interpretability but also facilitates clustering and transition analysis, revealing key behavioral patterns correlated with clinicalmetrics such as MMSE and ADAS-COG scores. Our findings demonstrate the framework's potential in supporting cognitive status prediction, personalized care interventions, and large-scale health monitoring.","sentences":["In remote healthcare monitoring, time series representation learning reveals critical patient behavior patterns from high-frequency data.","This study analyzes home activity data from individuals living with dementia by proposing a two-stage, self-supervised learning approach tailored to uncover low-rank structures.","The first stage converts time-series activities into text sequences encoded by a pre-trained language model, providing a rich, high-dimensional latent state space using a PageRank-based method.","This PageRank vector captures latent state transitions, effectively compressing complex behaviour data into a succinct form that enhances interpretability.","This low-rank representation not only enhances model interpretability but also facilitates clustering and transition analysis, revealing key behavioral patterns correlated with clinicalmetrics such as MMSE and ADAS-COG scores.","Our findings demonstrate the framework's potential in supporting cognitive status prediction, personalized care interventions, and large-scale health monitoring."],"url":"http://arxiv.org/abs/2502.09173v1"}
{"created":"2025-02-13 10:56:58","title":"LOB-Bench: Benchmarking Generative AI for Finance - an Application to Limit Order Book Data","abstract":"While financial data presents one of the most challenging and interesting sequence modelling tasks due to high noise, heavy tails, and strategic interactions, progress in this area has been hindered by the lack of consensus on quantitative evaluation paradigms. To address this, we present LOB-Bench, a benchmark, implemented in python, designed to evaluate the quality and realism of generative message-by-order data for limit order books (LOB) in the LOBSTER format. Our framework measures distributional differences in conditional and unconditional statistics between generated and real LOB data, supporting flexible multivariate statistical evaluation. The benchmark also includes features commonly used LOB statistics such as spread, order book volumes, order imbalance, and message inter-arrival times, along with scores from a trained discriminator network. Lastly, LOB-Bench contains \"market impact metrics\", i.e. the cross-correlations and price response functions for specific events in the data. We benchmark generative autoregressive state-space models, a (C)GAN, as well as a parametric LOB model and find that the autoregressive GenAI approach beats traditional model classes.","sentences":["While financial data presents one of the most challenging and interesting sequence modelling tasks due to high noise, heavy tails, and strategic interactions, progress in this area has been hindered by the lack of consensus on quantitative evaluation paradigms.","To address this, we present LOB-Bench, a benchmark, implemented in python, designed to evaluate the quality and realism of generative message-by-order data for limit order books (LOB) in the LOBSTER format.","Our framework measures distributional differences in conditional and unconditional statistics between generated and real LOB data, supporting flexible multivariate statistical evaluation.","The benchmark also includes features commonly used LOB statistics such as spread, order book volumes, order imbalance, and message inter-arrival times, along with scores from a trained discriminator network.","Lastly, LOB-Bench contains \"market impact metrics\", i.e. the cross-correlations and price response functions for specific events in the data.","We benchmark generative autoregressive state-space models, a (C)GAN, as well as a parametric LOB model and find that the autoregressive GenAI approach beats traditional model classes."],"url":"http://arxiv.org/abs/2502.09172v1"}
{"created":"2025-02-13 10:36:17","title":"Use of Air Quality Sensor Network Data for Real-time Pollution-Aware POI Suggestion","abstract":"This demo paper presents AirSense-R, a privacy-preserving mobile application that provides real-time, pollution-aware recommendations for points of interest (POIs) in urban environments. By combining real-time air quality monitoring data with user preferences, the proposed system aims to help users make health-conscious decisions about the locations they visit. The application utilizes collaborative filtering for personalized suggestions, and federated learning for privacy protection, and integrates air pollutant readings from AirSENCE sensor networks in cities such as Bari, Italy, and Cork, Ireland. Additionally, the AirSENCE prediction engine can be employed to detect anomaly readings and interpolate for air quality readings in areas with sparse sensor coverage. This system offers a promising, health-oriented POI recommendation solution that adapts dynamically to current urban air quality conditions while safeguarding user privacy. The code of AirTOWN and a demonstration video is made available at the following repo: https://github.com/AirtownApp/Airtown-Application.git.","sentences":["This demo paper presents AirSense-R, a privacy-preserving mobile application that provides real-time, pollution-aware recommendations for points of interest (POIs) in urban environments.","By combining real-time air quality monitoring data with user preferences, the proposed system aims to help users make health-conscious decisions about the locations they visit.","The application utilizes collaborative filtering for personalized suggestions, and federated learning for privacy protection, and integrates air pollutant readings from AirSENCE sensor networks in cities such as Bari, Italy, and Cork, Ireland.","Additionally, the AirSENCE prediction engine can be employed to detect anomaly readings and interpolate for air quality readings in areas with sparse sensor coverage.","This system offers a promising, health-oriented POI recommendation solution that adapts dynamically to current urban air quality conditions while safeguarding user privacy.","The code of AirTOWN and a demonstration video is made available at the following repo: https://github.com/AirtownApp/Airtown-Application.git."],"url":"http://arxiv.org/abs/2502.09155v1"}
{"created":"2025-02-13 10:27:30","title":"Regularization can make diffusion models more efficient","abstract":"Diffusion models are one of the key architectures of generative AI. Their main drawback, however, is the computational costs. This study indicates that the concept of sparsity, well known especially in statistics, can provide a pathway to more efficient diffusion pipelines. Our mathematical guarantees prove that sparsity can reduce the input dimension's influence on the computational complexity to that of a much smaller intrinsic dimension of the data. Our empirical findings confirm that inducing sparsity can indeed lead to better samples at a lower cost.","sentences":["Diffusion models are one of the key architectures of generative AI.","Their main drawback, however, is the computational costs.","This study indicates that the concept of sparsity, well known especially in statistics, can provide a pathway to more efficient diffusion pipelines.","Our mathematical guarantees prove that sparsity can reduce the input dimension's influence on the computational complexity to that of a much smaller intrinsic dimension of the data.","Our empirical findings confirm that inducing sparsity can indeed lead to better samples at a lower cost."],"url":"http://arxiv.org/abs/2502.09151v1"}
{"created":"2025-02-13 10:25:52","title":"Shortcut Learning Susceptibility in Vision Classifiers","abstract":"Shortcut learning, where machine learning models exploit spurious correlations in data instead of capturing meaningful features, poses a significant challenge to building robust and generalizable models. This phenomenon is prevalent across various machine learning applications, including vision, natural language processing, and speech recognition, where models may find unintended cues that minimize training loss but fail to capture the underlying structure of the data. Vision classifiers such as Convolutional Neural Networks (CNNs), Multi-Layer Perceptrons (MLPs), and Vision Transformers (ViTs) leverage distinct architectural principles to process spatial and structural information, making them differently susceptible to shortcut learning. In this study, we systematically evaluate these architectures by introducing deliberate shortcuts into the dataset that are positionally correlated with class labels, creating a controlled setup to assess whether models rely on these artificial cues or learn actual distinguishing features. We perform both quantitative evaluation by training on the shortcut-modified dataset and testing them on two different test sets -- one containing the same shortcuts and another without them -- to determine the extent of reliance on shortcuts. Additionally, qualitative evaluation is performed by using network inversion-based reconstruction techniques to analyze what the models internalize in their weights, aiming to reconstruct the training data as perceived by the classifiers. We evaluate shortcut learning behavior across multiple benchmark datasets, including MNIST, Fashion-MNIST, SVHN, and CIFAR-10, to compare the susceptibility of different vision classifier architectures to shortcut reliance and assess their varying degrees of sensitivity to spurious correlations.","sentences":["Shortcut learning, where machine learning models exploit spurious correlations in data instead of capturing meaningful features, poses a significant challenge to building robust and generalizable models.","This phenomenon is prevalent across various machine learning applications, including vision, natural language processing, and speech recognition, where models may find unintended cues that minimize training loss but fail to capture the underlying structure of the data.","Vision classifiers such as Convolutional Neural Networks (CNNs), Multi-Layer Perceptrons (MLPs), and Vision Transformers (ViTs) leverage distinct architectural principles to process spatial and structural information, making them differently susceptible to shortcut learning.","In this study, we systematically evaluate these architectures by introducing deliberate shortcuts into the dataset that are positionally correlated with class labels, creating a controlled setup to assess whether models rely on these artificial cues or learn actual distinguishing features.","We perform both quantitative evaluation by training on the shortcut-modified dataset and testing them on two different test sets -- one containing the same shortcuts and another without them -- to determine the extent of reliance on shortcuts.","Additionally, qualitative evaluation is performed by using network inversion-based reconstruction techniques to analyze what the models internalize in their weights, aiming to reconstruct the training data as perceived by the classifiers.","We evaluate shortcut learning behavior across multiple benchmark datasets, including MNIST, Fashion-MNIST, SVHN, and CIFAR-10, to compare the susceptibility of different vision classifier architectures to shortcut reliance and assess their varying degrees of sensitivity to spurious correlations."],"url":"http://arxiv.org/abs/2502.09150v1"}
{"created":"2025-02-13 10:23:45","title":"Multimodal HIE Lesion Segmentation in Neonates: A Comparative Study of Loss Functions","abstract":"Segmentation of Hypoxic-Ischemic Encephalopathy (HIE) lesions in neonatal MRI is a crucial but challenging task due to diffuse multifocal lesions with varying volumes and the limited availability of annotated HIE lesion datasets. Using the BONBID-HIE dataset, we implemented a 3D U-Net with optimized preprocessing, augmentation, and training strategies to overcome data constraints. The goal of this study is to identify the optimal loss function specifically for the HIE lesion segmentation task. To this end, we evaluated various loss functions, including Dice, Dice-Focal, Tversky, Hausdorff Distance (HausdorffDT) Loss, and two proposed compound losses -- Dice-Focal-HausdorffDT and Tversky-HausdorffDT -- to enhance segmentation performance. The results show that different loss functions predict distinct segmentation masks, with compound losses outperforming standalone losses. Tversky-HausdorffDT Loss achieves the highest Dice and Normalized Surface Dice scores, while Dice-Focal-HausdorffDT Loss minimizes Mean Surface Distance. This work underscores the significance of task-specific loss function optimization, demonstrating that combining region-based and boundary-aware losses leads to more accurate HIE lesion segmentation, even with limited training data.","sentences":["Segmentation of Hypoxic-Ischemic Encephalopathy (HIE) lesions in neonatal MRI is a crucial but challenging task due to diffuse multifocal lesions with varying volumes and the limited availability of annotated HIE lesion datasets.","Using the BONBID-HIE dataset, we implemented a 3D U-Net with optimized preprocessing, augmentation, and training strategies to overcome data constraints.","The goal of this study is to identify the optimal loss function specifically for the HIE lesion segmentation task.","To this end, we evaluated various loss functions, including Dice, Dice-Focal, Tversky, Hausdorff Distance (HausdorffDT) Loss, and two proposed compound losses -- Dice-Focal-HausdorffDT and Tversky-HausdorffDT -- to enhance segmentation performance.","The results show that different loss functions predict distinct segmentation masks, with compound losses outperforming standalone losses.","Tversky-HausdorffDT Loss achieves the highest Dice and Normalized Surface Dice scores, while Dice-Focal-HausdorffDT Loss minimizes Mean Surface Distance.","This work underscores the significance of task-specific loss function optimization, demonstrating that combining region-based and boundary-aware losses leads to more accurate HIE lesion segmentation, even with limited training data."],"url":"http://arxiv.org/abs/2502.09148v1"}
{"created":"2025-02-13 10:18:44","title":"Feature-based Graph Attention Networks Improve Online Continual Learning","abstract":"Online continual learning for image classification is crucial for models to adapt to new data while retaining knowledge of previously learned tasks. This capability is essential to address real-world challenges involving dynamic environments and evolving data distributions. Traditional approaches predominantly employ Convolutional Neural Networks, which are limited to processing images as grids and primarily capture local patterns rather than relational information. Although the emergence of transformer architectures has improved the ability to capture relationships, these models often require significantly larger resources. In this paper, we present a novel online continual learning framework based on Graph Attention Networks (GATs), which effectively capture contextual relationships and dynamically update the task-specific representation via learned attention weights. Our approach utilizes a pre-trained feature extractor to convert images into graphs using hierarchical feature maps, representing information at varying levels of granularity. These graphs are then processed by a GAT and incorporate an enhanced global pooling strategy to improve classification performance for continual learning. In addition, we propose the rehearsal memory duplication technique that improves the representation of the previous tasks while maintaining the memory budget. Comprehensive evaluations on benchmark datasets, including SVHN, CIFAR10, CIFAR100, and MiniImageNet, demonstrate the superiority of our method compared to the state-of-the-art methods.","sentences":["Online continual learning for image classification is crucial for models to adapt to new data while retaining knowledge of previously learned tasks.","This capability is essential to address real-world challenges involving dynamic environments and evolving data distributions.","Traditional approaches predominantly employ Convolutional Neural Networks, which are limited to processing images as grids and primarily capture local patterns rather than relational information.","Although the emergence of transformer architectures has improved the ability to capture relationships, these models often require significantly larger resources.","In this paper, we present a novel online continual learning framework based on Graph Attention Networks (GATs), which effectively capture contextual relationships and dynamically update the task-specific representation via learned attention weights.","Our approach utilizes a pre-trained feature extractor to convert images into graphs using hierarchical feature maps, representing information at varying levels of granularity.","These graphs are then processed by a GAT and incorporate an enhanced global pooling strategy to improve classification performance for continual learning.","In addition, we propose the rehearsal memory duplication technique that improves the representation of the previous tasks while maintaining the memory budget.","Comprehensive evaluations on benchmark datasets, including SVHN, CIFAR10, CIFAR100, and MiniImageNet, demonstrate the superiority of our method compared to the state-of-the-art methods."],"url":"http://arxiv.org/abs/2502.09143v1"}
{"created":"2025-02-13 10:15:16","title":"Replay-free Online Continual Learning with Self-Supervised MultiPatches","abstract":"Online Continual Learning (OCL) methods train a model on a non-stationary data stream where only a few examples are available at a time, often leveraging replay strategies. However, usage of replay is sometimes forbidden, especially in applications with strict privacy regulations. Therefore, we propose Continual MultiPatches (CMP), an effective plug-in for existing OCL self-supervised learning strategies that avoids the use of replay samples. CMP generates multiple patches from a single example and projects them into a shared feature space, where patches coming from the same example are pushed together without collapsing into a single point. CMP surpasses replay and other SSL-based strategies on OCL streams, challenging the role of replay as a go-to solution for self-supervised OCL.","sentences":["Online Continual Learning (OCL) methods train a model on a non-stationary data stream where only a few examples are available at a time, often leveraging replay strategies.","However, usage of replay is sometimes forbidden, especially in applications with strict privacy regulations.","Therefore, we propose Continual MultiPatches (CMP), an effective plug-in for existing OCL self-supervised learning strategies that avoids the use of replay samples.","CMP generates multiple patches from a single example and projects them into a shared feature space, where patches coming from the same example are pushed together without collapsing into a single point.","CMP surpasses replay and other SSL-based strategies on OCL streams, challenging the role of replay as a go-to solution for self-supervised OCL."],"url":"http://arxiv.org/abs/2502.09140v1"}
{"created":"2025-02-13 10:14:19","title":"Zebrafix: Mitigating Memory-Centric Side-Channel Leakage via Interleaving","abstract":"Constant-time code has become the de-facto standard for secure cryptographic implementations. However, some memory-based leakage classes such as ciphertext side-channels, silent stores, and data memory-dependent prefetching remain unaddressed. In the context of ciphertext side-channel mitigations, the practicality of interleaving data with counter values remains to be explored. To close this gap, we define design choices and requirements to leverage interleaving for a generic ciphertext side-channel mitigation. Based on these results, we implement Zebrafix, a compiler-based tool to ensure freshness of memory stores. We evaluate Zebrafix and find that interleaving can perform much better than other ciphertext side-channel mitigations, at the cost of a high practical complexity. We further observe that ciphertext side-channels, silent stores and data memory-dependent prefetching belong to a broader attack category: memory-centric side-channels. Under this unified view, we discuss to what extent ciphertext side-channel mitigations can be adapted to prevent all three memory-centric side-channel attacks via interleaving.","sentences":["Constant-time code has become the de-facto standard for secure cryptographic implementations.","However, some memory-based leakage classes such as ciphertext side-channels, silent stores, and data memory-dependent prefetching remain unaddressed.","In the context of ciphertext side-channel mitigations, the practicality of interleaving data with counter values remains to be explored.","To close this gap, we define design choices and requirements to leverage interleaving for a generic ciphertext side-channel mitigation.","Based on these results, we implement Zebrafix, a compiler-based tool to ensure freshness of memory stores.","We evaluate Zebrafix and find that interleaving can perform much better than other ciphertext side-channel mitigations, at the cost of a high practical complexity.","We further observe that ciphertext side-channels, silent stores and data memory-dependent prefetching belong to a broader attack category: memory-centric side-channels.","Under this unified view, we discuss to what extent ciphertext side-channel mitigations can be adapted to prevent all three memory-centric side-channel attacks via interleaving."],"url":"http://arxiv.org/abs/2502.09139v1"}
{"created":"2025-02-13 10:07:35","title":"Finite-Time Analysis of Discrete-Time Stochastic Interpolants","abstract":"The stochastic interpolant framework offers a powerful approach for constructing generative models based on ordinary differential equations (ODEs) or stochastic differential equations (SDEs) to transform arbitrary data distributions. However, prior analyses of this framework have primarily focused on the continuous-time setting, assuming a perfect solution of the underlying equations. In this work, we present the first discrete-time analysis of the stochastic interpolant framework, where we introduce an innovative discrete-time sampler and derive a finite-time upper bound on its distribution estimation error. Our result provides a novel quantification of how different factors, including the distance between source and target distributions and estimation accuracy, affect the convergence rate and also offers a new principled way to design efficient schedules for convergence acceleration. Finally, numerical experiments are conducted on the discrete-time sampler to corroborate our theoretical findings.","sentences":["The stochastic interpolant framework offers a powerful approach for constructing generative models based on ordinary differential equations (ODEs) or stochastic differential equations (SDEs) to transform arbitrary data distributions.","However, prior analyses of this framework have primarily focused on the continuous-time setting, assuming a perfect solution of the underlying equations.","In this work, we present the first discrete-time analysis of the stochastic interpolant framework, where we introduce an innovative discrete-time sampler and derive a finite-time upper bound on its distribution estimation error.","Our result provides a novel quantification of how different factors, including the distance between source and target distributions and estimation accuracy, affect the convergence rate and also offers a new principled way to design efficient schedules for convergence acceleration.","Finally, numerical experiments are conducted on the discrete-time sampler to corroborate our theoretical findings."],"url":"http://arxiv.org/abs/2502.09130v1"}
{"created":"2025-02-13 10:03:29","title":"Automatic Pruning via Structured Lasso with Class-wise Information","abstract":"Most pruning methods concentrate on unimportant filters of neural networks. However, they face the loss of statistical information due to a lack of consideration for class-wise data. In this paper, from the perspective of leveraging precise class-wise information for model pruning, we utilize structured lasso with guidance from Information Bottleneck theory. Our approach ensures that statistical information is retained during the pruning process. With these techniques, we introduce two innovative adaptive network pruning schemes: sparse graph-structured lasso pruning with Information Bottleneck (\\textbf{sGLP-IB}) and sparse tree-guided lasso pruning with Information Bottleneck (\\textbf{sTLP-IB}). The key aspect is pruning model filters using sGLP-IB and sTLP-IB to better capture class-wise relatedness. Compared to multiple state-of-the-art methods, our approaches demonstrate superior performance across three datasets and six model architectures in extensive experiments. For instance, using the VGG16 model on the CIFAR-10 dataset, we achieve a parameter reduction of 85%, a decrease in FLOPs by 61%, and maintain an accuracy of 94.10% (0.14% higher than the original model); we reduce the parameters by 55% with the accuracy at 76.12% using the ResNet architecture on ImageNet (only drops 0.03%). In summary, we successfully reduce model size and computational resource usage while maintaining accuracy. Our codes are at https://anonymous.4open.science/r/IJCAI-8104.","sentences":["Most pruning methods concentrate on unimportant filters of neural networks.","However, they face the loss of statistical information due to a lack of consideration for class-wise data.","In this paper, from the perspective of leveraging precise class-wise information for model pruning, we utilize structured lasso with guidance from Information Bottleneck theory.","Our approach ensures that statistical information is retained during the pruning process.","With these techniques, we introduce two innovative adaptive network pruning schemes: sparse graph-structured lasso pruning with Information Bottleneck (\\textbf{sGLP-IB}) and sparse tree-guided lasso pruning with Information Bottleneck (\\textbf{sTLP-IB}).","The key aspect is pruning model filters using sGLP-IB and sTLP-IB to better capture class-wise relatedness.","Compared to multiple state-of-the-art methods, our approaches demonstrate superior performance across three datasets and six model architectures in extensive experiments.","For instance, using the VGG16 model on the CIFAR-10 dataset, we achieve a parameter reduction of 85%, a decrease in FLOPs by 61%, and maintain an accuracy of 94.10% (0.14% higher than the original model); we reduce the parameters by 55% with the accuracy at 76.12% using the ResNet architecture on ImageNet (only drops 0.03%).","In summary, we successfully reduce model size and computational resource usage while maintaining accuracy.","Our codes are at https://anonymous.4open.science/r/IJCAI-8104."],"url":"http://arxiv.org/abs/2502.09125v1"}
{"created":"2025-02-13 09:29:04","title":"Scaling Law for Stochastic Gradient Descent in Quadratically Parameterized Linear Regression","abstract":"In machine learning, the scaling law describes how the model performance improves with the model and data size scaling up. From a learning theory perspective, this class of results establishes upper and lower generalization bounds for a specific learning algorithm. Here, the exact algorithm running using a specific model parameterization often offers a crucial implicit regularization effect, leading to good generalization. To characterize the scaling law, previous theoretical studies mainly focus on linear models, whereas, feature learning, a notable process that contributes to the remarkable empirical success of neural networks, is regretfully vacant. This paper studies the scaling law over a linear regression with the model being quadratically parameterized. We consider infinitely dimensional data and slope ground truth, both signals exhibiting certain power-law decay rates. We study convergence rates for Stochastic Gradient Descent and demonstrate the learning rates for variables will automatically adapt to the ground truth. As a result, in the canonical linear regression, we provide explicit separations for generalization curves between SGD with and without feature learning, and the information-theoretical lower bound that is agnostic to parametrization method and the algorithm. Our analysis for decaying ground truth provides a new characterization for the learning dynamic of the model.","sentences":["In machine learning, the scaling law describes how the model performance improves with the model and data size scaling up.","From a learning theory perspective, this class of results establishes upper and lower generalization bounds for a specific learning algorithm.","Here, the exact algorithm running using a specific model parameterization often offers a crucial implicit regularization effect, leading to good generalization.","To characterize the scaling law, previous theoretical studies mainly focus on linear models, whereas, feature learning, a notable process that contributes to the remarkable empirical success of neural networks, is regretfully vacant.","This paper studies the scaling law over a linear regression with the model being quadratically parameterized.","We consider infinitely dimensional data and slope ground truth, both signals exhibiting certain power-law decay rates.","We study convergence rates for Stochastic Gradient Descent and demonstrate the learning rates for variables will automatically adapt to the ground truth.","As a result, in the canonical linear regression, we provide explicit separations for generalization curves between SGD with and without feature learning, and the information-theoretical lower bound that is agnostic to parametrization method and the algorithm.","Our analysis for decaying ground truth provides a new characterization for the learning dynamic of the model."],"url":"http://arxiv.org/abs/2502.09106v1"}
{"created":"2025-02-13 09:27:10","title":"Incremental Approximate Maximum Flow via Residual Graph Sparsification","abstract":"We give an algorithm that, with high probability, maintains a $(1-\\epsilon)$-approximate $s$-$t$ maximum flow in undirected, uncapacitated $n$-vertex graphs undergoing $m$ edge insertions in $\\tilde{O}(m+ n F^*/\\epsilon)$ total update time, where $F^{*}$ is the maximum flow on the final graph. This is the first algorithm to achieve polylogarithmic amortized update time for dense graphs ($m = \\Omega(n^2)$), and more generally, for graphs where $F^*= \\tilde{O}(m/n)$.   At the heart of our incremental algorithm is the residual graph sparsification technique of Karger and Levine [SICOMP '15], originally designed for computing exact maximum flows in the static setting. Our main contributions are (i) showing how to maintain such sparsifiers for approximate maximum flows in the incremental setting and (ii) generalizing the cut sparsification framework of Fung et al. [SICOMP '19] from undirected graphs to balanced directed graphs.","sentences":["We give an algorithm that, with high probability, maintains a $(1-\\epsilon)$-approximate $s$-$t$ maximum flow in undirected, uncapacitated $n$-vertex graphs undergoing $m$ edge insertions in $\\tilde{O}(m+ n F^*/\\epsilon)$ total update time, where $F^{*}$ is the maximum flow on the final graph.","This is the first algorithm to achieve polylogarithmic amortized update time for dense graphs ($m = \\Omega(n^2)$), and more generally, for graphs where $F^*= \\tilde{O}(m/n)$.   At the heart of our incremental algorithm is the residual graph sparsification technique of Karger and Levine","[SICOMP '15], originally designed for computing exact maximum flows in the static setting.","Our main contributions are (i) showing how to maintain such sparsifiers for approximate maximum flows in the incremental setting and (ii) generalizing the cut sparsification framework of Fung et al.","[SICOMP '19] from undirected graphs to balanced directed graphs."],"url":"http://arxiv.org/abs/2502.09105v1"}
{"created":"2025-02-13 09:26:44","title":"One-shot Federated Learning Methods: A Practical Guide","abstract":"One-shot Federated Learning (OFL) is a distributed machine learning paradigm that constrains client-server communication to a single round, addressing privacy and communication overhead issues associated with multiple rounds of data exchange in traditional Federated Learning (FL). OFL demonstrates the practical potential for integration with future approaches that require collaborative training models, such as large language models (LLMs). However, current OFL methods face two major challenges: data heterogeneity and model heterogeneity, which result in subpar performance compared to conventional FL methods. Worse still, despite numerous studies addressing these limitations, a comprehensive summary is still lacking. To address these gaps, this paper presents a systematic analysis of the challenges faced by OFL and thoroughly reviews the current methods. We also offer an innovative categorization method and analyze the trade-offs of various techniques. Additionally, we discuss the most promising future directions and the technologies that should be integrated into the OFL field. This work aims to provide guidance and insights for future research.","sentences":["One-shot Federated Learning (OFL) is a distributed machine learning paradigm that constrains client-server communication to a single round, addressing privacy and communication overhead issues associated with multiple rounds of data exchange in traditional Federated Learning (FL).","OFL demonstrates the practical potential for integration with future approaches that require collaborative training models, such as large language models (LLMs).","However, current OFL methods face two major challenges: data heterogeneity and model heterogeneity, which result in subpar performance compared to conventional FL methods.","Worse still, despite numerous studies addressing these limitations, a comprehensive summary is still lacking.","To address these gaps, this paper presents a systematic analysis of the challenges faced by OFL and thoroughly reviews the current methods.","We also offer an innovative categorization method and analyze the trade-offs of various techniques.","Additionally, we discuss the most promising future directions and the technologies that should be integrated into the OFL field.","This work aims to provide guidance and insights for future research."],"url":"http://arxiv.org/abs/2502.09104v1"}
{"created":"2025-02-13 09:19:14","title":"Logical Reasoning in Large Language Models: A Survey","abstract":"With the emergence of advanced reasoning models like OpenAI o3 and DeepSeek-R1, large language models (LLMs) have demonstrated remarkable reasoning capabilities. However, their ability to perform rigorous logical reasoning remains an open question. This survey synthesizes recent advancements in logical reasoning within LLMs, a critical area of AI research. It outlines the scope of logical reasoning in LLMs, its theoretical foundations, and the benchmarks used to evaluate reasoning proficiency. We analyze existing capabilities across different reasoning paradigms - deductive, inductive, abductive, and analogical - and assess strategies to enhance reasoning performance, including data-centric tuning, reinforcement learning, decoding strategies, and neuro-symbolic approaches. The review concludes with future directions, emphasizing the need for further exploration to strengthen logical reasoning in AI systems.","sentences":["With the emergence of advanced reasoning models like OpenAI o3 and DeepSeek-R1, large language models (LLMs) have demonstrated remarkable reasoning capabilities.","However, their ability to perform rigorous logical reasoning remains an open question.","This survey synthesizes recent advancements in logical reasoning within LLMs, a critical area of AI research.","It outlines the scope of logical reasoning in LLMs, its theoretical foundations, and the benchmarks used to evaluate reasoning proficiency.","We analyze existing capabilities across different reasoning paradigms - deductive, inductive, abductive, and analogical - and assess strategies to enhance reasoning performance, including data-centric tuning, reinforcement learning, decoding strategies, and neuro-symbolic approaches.","The review concludes with future directions, emphasizing the need for further exploration to strengthen logical reasoning in AI systems."],"url":"http://arxiv.org/abs/2502.09100v1"}
{"created":"2025-02-13 09:04:28","title":"From Visuals to Vocabulary: Establishing Equivalence Between Image and Text Token Through Autoregressive Pre-training in MLLMs","abstract":"While MLLMs perform well on perceptual tasks, they lack precise multimodal alignment, limiting performance. To address this challenge, we propose Vision Dynamic Embedding-Guided Pretraining (VDEP), a hybrid autoregressive training paradigm for MLLMs. Utilizing dynamic embeddings from the MLP following the visual encoder, this approach supervises image hidden states and integrates image tokens into autoregressive training. Existing MLLMs primarily focused on recovering information from textual inputs, often neglecting the effective processing of image data. In contrast, the key improvement of this work is the reinterpretation of multimodal alignment as a process of recovering information from input data, with particular emphasis on reconstructing detailed visual features.The proposed method seamlessly integrates into standard models without architectural changes. Experiments on 13 benchmarks show VDEP outperforms baselines, surpassing existing methods.","sentences":["While MLLMs perform well on perceptual tasks, they lack precise multimodal alignment, limiting performance.","To address this challenge, we propose Vision Dynamic Embedding-Guided Pretraining (VDEP), a hybrid autoregressive training paradigm for MLLMs.","Utilizing dynamic embeddings from the MLP following the visual encoder, this approach supervises image hidden states and integrates image tokens into autoregressive training.","Existing MLLMs primarily focused on recovering information from textual inputs, often neglecting the effective processing of image data.","In contrast, the key improvement of this work is the reinterpretation of multimodal alignment as a process of recovering information from input data, with particular emphasis on reconstructing detailed visual features.","The proposed method seamlessly integrates into standard models without architectural changes.","Experiments on 13 benchmarks show VDEP outperforms baselines, surpassing existing methods."],"url":"http://arxiv.org/abs/2502.09093v1"}
{"created":"2025-02-13 09:01:34","title":"Semantic Ads Retrieval at Walmart eCommerce with Language Models Progressively Trained on Multiple Knowledge Domains","abstract":"Sponsored search in e-commerce poses several unique and complex challenges. These challenges stem from factors such as the asymmetric language structure between search queries and product names, the inherent ambiguity in user search intent, and the vast volume of sparse and imbalanced search corpus data. The role of the retrieval component within a sponsored search system is pivotal, serving as the initial step that directly affects the subsequent ranking and bidding systems. In this paper, we present an end-to-end solution tailored to optimize the ads retrieval system on Walmart.com. Our approach is to pretrain the BERT-like classification model with product category information, enhancing the model's understanding of Walmart product semantics. Second, we design a two-tower Siamese Network structure for embedding structures to augment training efficiency. Third, we introduce a Human-in-the-loop Progressive Fusion Training method to ensure robust model performance. Our results demonstrate the effectiveness of this pipeline. It enhances the search relevance metric by up to 16% compared to a baseline DSSM-based model. Moreover, our large-scale online A/B testing demonstrates that our approach surpasses the ad revenue of the existing production model.","sentences":["Sponsored search in e-commerce poses several unique and complex challenges.","These challenges stem from factors such as the asymmetric language structure between search queries and product names, the inherent ambiguity in user search intent, and the vast volume of sparse and imbalanced search corpus data.","The role of the retrieval component within a sponsored search system is pivotal, serving as the initial step that directly affects the subsequent ranking and bidding systems.","In this paper, we present an end-to-end solution tailored to optimize the ads retrieval system on Walmart.com.","Our approach is to pretrain the BERT-like classification model with product category information, enhancing the model's understanding of Walmart product semantics.","Second, we design a two-tower Siamese Network structure for embedding structures to augment training efficiency.","Third, we introduce a Human-in-the-loop Progressive Fusion Training method to ensure robust model performance.","Our results demonstrate the effectiveness of this pipeline.","It enhances the search relevance metric by up to 16% compared to a baseline DSSM-based model.","Moreover, our large-scale online A/B testing demonstrates that our approach surpasses the ad revenue of the existing production model."],"url":"http://arxiv.org/abs/2502.09089v1"}
{"created":"2025-02-13 09:00:32","title":"A Hybrid Model for Few-Shot Text Classification Using Transfer and Meta-Learning","abstract":"With the continuous development of natural language processing (NLP) technology, text classification tasks have been widely used in multiple application fields. However, obtaining labeled data is often expensive and difficult, especially in few-shot learning scenarios. To solve this problem, this paper proposes a few-shot text classification model based on transfer learning and meta-learning. The model uses the knowledge of the pre-trained model for transfer and optimizes the model's rapid adaptability in few-sample tasks through a meta-learning mechanism. Through a series of comparative experiments and ablation experiments, we verified the effectiveness of the proposed method. The experimental results show that under the conditions of few samples and medium samples, the model based on transfer learning and meta-learning significantly outperforms traditional machine learning and deep learning methods. In addition, ablation experiments further analyzed the contribution of each component to the model performance and confirmed the key role of transfer learning and meta-learning in improving model accuracy. Finally, this paper discusses future research directions and looks forward to the potential of this method in practical applications.","sentences":["With the continuous development of natural language processing (NLP) technology, text classification tasks have been widely used in multiple application fields.","However, obtaining labeled data is often expensive and difficult, especially in few-shot learning scenarios.","To solve this problem, this paper proposes a few-shot text classification model based on transfer learning and meta-learning.","The model uses the knowledge of the pre-trained model for transfer and optimizes the model's rapid adaptability in few-sample tasks through a meta-learning mechanism.","Through a series of comparative experiments and ablation experiments, we verified the effectiveness of the proposed method.","The experimental results show that under the conditions of few samples and medium samples, the model based on transfer learning and meta-learning significantly outperforms traditional machine learning and deep learning methods.","In addition, ablation experiments further analyzed the contribution of each component to the model performance and confirmed the key role of transfer learning and meta-learning in improving model accuracy.","Finally, this paper discusses future research directions and looks forward to the potential of this method in practical applications."],"url":"http://arxiv.org/abs/2502.09086v1"}
{"created":"2025-02-13 08:59:04","title":"Application of Tabular Transformer Architectures for Operating System Fingerprinting","abstract":"Operating System (OS) fingerprinting is essential for network management and cybersecurity, enabling accurate device identification based on network traffic analysis. Traditional rule-based tools such as Nmap and p0f face challenges in dynamic environments due to frequent OS updates and obfuscation techniques. While Machine Learning (ML) approaches have been explored, Deep Learning (DL) models, particularly Transformer architectures, remain unexploited in this domain. This study investigates the application of Tabular Transformer architectures-specifically TabTransformer and FT-Transformer-for OS fingerprinting, leveraging structured network data from three publicly available datasets. Our experiments demonstrate that FT-Transformer generally outperforms traditional ML models, previous approaches and TabTransformer across multiple classification levels (OS family, major, and minor versions). The results establish a strong foundation for DL-based OS fingerprinting, improving accuracy and adaptability in complex network environments. Furthermore, we ensure the reproducibility of our research by providing an open-source implementation.","sentences":["Operating System (OS) fingerprinting is essential for network management and cybersecurity, enabling accurate device identification based on network traffic analysis.","Traditional rule-based tools such as Nmap and p0f face challenges in dynamic environments due to frequent OS updates and obfuscation techniques.","While Machine Learning (ML) approaches have been explored, Deep Learning (DL) models, particularly Transformer architectures, remain unexploited in this domain.","This study investigates the application of Tabular Transformer architectures-specifically TabTransformer and FT-Transformer-for OS fingerprinting, leveraging structured network data from three publicly available datasets.","Our experiments demonstrate that FT-Transformer generally outperforms traditional ML models, previous approaches and TabTransformer across multiple classification levels (OS family, major, and minor versions).","The results establish a strong foundation for DL-based OS fingerprinting, improving accuracy and adaptability in complex network environments.","Furthermore, we ensure the reproducibility of our research by providing an open-source implementation."],"url":"http://arxiv.org/abs/2502.09084v1"}
{"created":"2025-02-13 08:55:24","title":"CoSER: Coordinating LLM-Based Persona Simulation of Established Roles","abstract":"Role-playing language agents (RPLAs) have emerged as promising applications of large language models (LLMs). However, simulating established characters presents a challenging task for RPLAs, due to the lack of authentic character datasets and nuanced evaluation methods using such data. In this paper, we present CoSER, a collection of a high-quality dataset, open models, and an evaluation protocol towards effective RPLAs of established characters. The CoSER dataset covers 17,966 characters from 771 renowned books. It provides authentic dialogues with real-world intricacies, as well as diverse data types such as conversation setups, character experiences and internal thoughts. Drawing from acting methodology, we introduce given-circumstance acting for training and evaluating role-playing LLMs, where LLMs sequentially portray multiple characters in book scenes. Using our dataset, we develop CoSER 8B and CoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models. Extensive experiments demonstrate the value of the CoSER dataset for RPLA training, evaluation and retrieval. Moreover, CoSER 70B exhibits state-of-the-art performance surpassing or matching GPT-4o on our evaluation and three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on the InCharacter and LifeChoice benchmarks respectively.","sentences":["Role-playing language agents (RPLAs) have emerged as promising applications of large language models (LLMs).","However, simulating established characters presents a challenging task for RPLAs, due to the lack of authentic character datasets and nuanced evaluation methods using such data.","In this paper, we present CoSER, a collection of a high-quality dataset, open models, and an evaluation protocol towards effective RPLAs of established characters.","The CoSER dataset covers 17,966 characters from 771 renowned books.","It provides authentic dialogues with real-world intricacies, as well as diverse data types such as conversation setups, character experiences and internal thoughts.","Drawing from acting methodology, we introduce given-circumstance acting for training and evaluating role-playing LLMs, where LLMs sequentially portray multiple characters in book scenes.","Using our dataset, we develop CoSER 8B and CoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models.","Extensive experiments demonstrate the value of the CoSER dataset for RPLA training, evaluation and retrieval.","Moreover, CoSER 70B exhibits state-of-the-art performance surpassing or matching GPT-4o on our evaluation and three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on the InCharacter and LifeChoice benchmarks respectively."],"url":"http://arxiv.org/abs/2502.09082v1"}
{"created":"2025-02-13 08:32:24","title":"FlowAR: une plateforme uniformis\u00e9e pour la reconnaissance des activit\u00e9s humaines \u00e0 partir de capteurs binaires","abstract":"This demo showcases a platform for developing human activity recognition (AR) systems, focusing on daily activities using sensor data, like binary sensors. With a data-driven approach, this platform, named FlowAR, features a three-step pipeline (flow): data cleaning, segmentation, and personalized classification. Its modularity allows flexibility to test methods, datasets, and ensure rigorous evaluations. A concrete use case demonstrates its effectiveness.","sentences":["This demo showcases a platform for developing human activity recognition (AR) systems, focusing on daily activities using sensor data, like binary sensors.","With a data-driven approach, this platform, named FlowAR, features a three-step pipeline (flow): data cleaning, segmentation, and personalized classification.","Its modularity allows flexibility to test methods, datasets, and ensure rigorous evaluations.","A concrete use case demonstrates its effectiveness."],"url":"http://arxiv.org/abs/2502.09067v1"}
{"created":"2025-02-13 08:23:33","title":"Anchor Sponsor Firms in Open Source Software Ecosystems","abstract":"Firms are intensifying their involvement with open source software (OSS), going beyond contributing to individual projects and releasing their own core technologies as OSS. These technologies, from web frameworks to programming languages, are the foundations of large and growing ecosystems. Yet we know little about how these anchor sponsors shape the behavior of OSS contributors. We examine Mozilla Corporation's role as incubator and anchor sponsor in the Rust programming language ecosystem, leveraging data on nearly 30,000 developers and 40,000 OSS projects from 2015 to 2022. When Mozilla abruptly exited Rust in August 2020, event-study models estimate a negative impact on ecosystem activity: a 9\\% immediate drop in weekly commits and a 0.6 percentage point decline in trend. We observe an asymmetry in the shock's effects: former Mozilla developers and close collaborators continued contributing relatively quickly, whereas more distant developers showed reduced or ceased activity even six months later. An agent-based model of an OSS ecosystem with an anchor sponsor replicates these patterns. We also find a marked slowdown in new developers and projects entering Rust post-shock. Our results suggest that Mozilla served as a critical signal of Rust's quality and stability. Once withdrawn, newcomers and less-embedded developers were the most discouraged, raising concerns about long-term ecosystem sustainability.","sentences":["Firms are intensifying their involvement with open source software (OSS), going beyond contributing to individual projects and releasing their own core technologies as OSS.","These technologies, from web frameworks to programming languages, are the foundations of large and growing ecosystems.","Yet we know little about how these anchor sponsors shape the behavior of OSS contributors.","We examine Mozilla Corporation's role as incubator and anchor sponsor in the Rust programming language ecosystem, leveraging data on nearly 30,000 developers and 40,000 OSS projects from 2015 to 2022.","When Mozilla abruptly exited Rust in August 2020, event-study models estimate a negative impact on ecosystem activity: a 9\\% immediate drop in weekly commits and a 0.6 percentage point decline in trend.","We observe an asymmetry in the shock's effects: former Mozilla developers and close collaborators continued contributing relatively quickly, whereas more distant developers showed reduced or ceased activity even six months later.","An agent-based model of an OSS ecosystem with an anchor sponsor replicates these patterns.","We also find a marked slowdown in new developers and projects entering Rust post-shock.","Our results suggest that Mozilla served as a critical signal of Rust's quality and stability.","Once withdrawn, newcomers and less-embedded developers were the most discouraged, raising concerns about long-term ecosystem sustainability."],"url":"http://arxiv.org/abs/2502.09060v1"}
{"created":"2025-02-13 08:19:45","title":"Unleashing the Power of Large Language Model for Denoising Recommendation","abstract":"Recommender systems are crucial for personalizing user experiences but often depend on implicit feedback data, which can be noisy and misleading. Existing denoising studies involve incorporating auxiliary information or learning strategies from interaction data. However, they struggle with the inherent limitations of external knowledge and interaction data, as well as the non-universality of certain predefined assumptions, hindering accurate noise identification. Recently, large language models (LLMs) have gained attention for their extensive world knowledge and reasoning abilities, yet their potential in enhancing denoising in recommendations remains underexplored. In this paper, we introduce LLaRD, a framework leveraging LLMs to improve denoising in recommender systems, thereby boosting overall recommendation performance. Specifically, LLaRD generates denoising-related knowledge by first enriching semantic insights from observational data via LLMs and inferring user-item preference knowledge. It then employs a novel Chain-of-Thought (CoT) technique over user-item interaction graphs to reveal relation knowledge for denoising. Finally, it applies the Information Bottleneck (IB) principle to align LLM-generated denoising knowledge with recommendation targets, filtering out noise and irrelevant LLM knowledge. Empirical results demonstrate LLaRD's effectiveness in enhancing denoising and recommendation accuracy.","sentences":["Recommender systems are crucial for personalizing user experiences but often depend on implicit feedback data, which can be noisy and misleading.","Existing denoising studies involve incorporating auxiliary information or learning strategies from interaction data.","However, they struggle with the inherent limitations of external knowledge and interaction data, as well as the non-universality of certain predefined assumptions, hindering accurate noise identification.","Recently, large language models (LLMs) have gained attention for their extensive world knowledge and reasoning abilities, yet their potential in enhancing denoising in recommendations remains underexplored.","In this paper, we introduce LLaRD, a framework leveraging LLMs to improve denoising in recommender systems, thereby boosting overall recommendation performance.","Specifically, LLaRD generates denoising-related knowledge by first enriching semantic insights from observational data via LLMs and inferring user-item preference knowledge.","It then employs a novel Chain-of-Thought (CoT) technique over user-item interaction graphs to reveal relation knowledge for denoising.","Finally, it applies the Information Bottleneck (IB) principle to align LLM-generated denoising knowledge with recommendation targets, filtering out noise and irrelevant LLM knowledge.","Empirical results demonstrate LLaRD's effectiveness in enhancing denoising and recommendation accuracy."],"url":"http://arxiv.org/abs/2502.09058v1"}
{"created":"2025-02-13 08:10:45","title":"An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging","abstract":"This paper investigates data selection and model merging methodologies aimed at incorporating advanced reasoning capabilities such as those of DeepSeek R1 into language-specific large language models (LLMs), with a particular focus on the Thai LLM. Our goal is to enhance the reasoning capabilities of language-specific LLMs while maintaining their target language abilities. DeepSeek R1 excels in reasoning but primarily benefits high-resource languages such as English and Chinese. However, low-resource languages remain underserved due to the dominance of English-centric training data and model optimizations, which limit performance in these languages. This limitation results in unreliable code-switching and diminished effectiveness on tasks in low-resource languages. Meanwhile, local and regional LLM initiatives have attempted to bridge this gap by developing language-specific LLMs that focus on improving local linguistic fidelity. We demonstrate that, with only publicly available datasets and a computational budget of $120, it is possible to enhance the reasoning capabilities of language-specific LLMs to match the level of DeepSeek R1, without compromising their performance on target language tasks.","sentences":["This paper investigates data selection and model merging methodologies aimed at incorporating advanced reasoning capabilities such as those of DeepSeek R1 into language-specific large language models (LLMs), with a particular focus on the Thai LLM.","Our goal is to enhance the reasoning capabilities of language-specific LLMs while maintaining their target language abilities.","DeepSeek R1 excels in reasoning but primarily benefits high-resource languages such as English and Chinese.","However, low-resource languages remain underserved due to the dominance of English-centric training data and model optimizations, which limit performance in these languages.","This limitation results in unreliable code-switching and diminished effectiveness on tasks in low-resource languages.","Meanwhile, local and regional LLM initiatives have attempted to bridge this gap by developing language-specific LLMs that focus on improving local linguistic fidelity.","We demonstrate that, with only publicly available datasets and a computational budget of $120, it is possible to enhance the reasoning capabilities of language-specific LLMs to match the level of DeepSeek R1, without compromising their performance on target language tasks."],"url":"http://arxiv.org/abs/2502.09056v1"}
{"created":"2025-02-13 08:05:44","title":"AIDE: Agentically Improve Visual Language Model with Domain Experts","abstract":"The enhancement of Visual Language Models (VLMs) has traditionally relied on knowledge distillation from larger, more capable models. This dependence creates a fundamental bottleneck for improving state-of-the-art systems, particularly when no superior models exist. We introduce AIDE (Agentic Improvement through Domain Experts), a novel framework that enables VLMs to autonomously enhance their capabilities by leveraging specialized domain expert models. AIDE operates through a four-stage process: (1) identifying instances for refinement, (2) engaging domain experts for targeted analysis, (3) synthesizing expert outputs with existing data, and (4) integrating enhanced instances into the training pipeline. Experiments on multiple benchmarks, including MMMU, MME, MMBench, etc., demonstrate AIDE's ability to achieve notable performance gains without relying on larger VLMs nor human supervision. Our framework provides a scalable, resource-efficient approach to continuous VLM improvement, addressing critical limitations in current methodologies, particularly valuable when larger models are unavailable to access.","sentences":["The enhancement of Visual Language Models (VLMs) has traditionally relied on knowledge distillation from larger, more capable models.","This dependence creates a fundamental bottleneck for improving state-of-the-art systems, particularly when no superior models exist.","We introduce AIDE (Agentic Improvement through Domain Experts), a novel framework that enables VLMs to autonomously enhance their capabilities by leveraging specialized domain expert models.","AIDE operates through a four-stage process: (1) identifying instances for refinement, (2) engaging domain experts for targeted analysis, (3) synthesizing expert outputs with existing data, and (4) integrating enhanced instances into the training pipeline.","Experiments on multiple benchmarks, including MMMU, MME, MMBench, etc., demonstrate AIDE's ability to achieve notable performance gains without relying on larger VLMs nor human supervision.","Our framework provides a scalable, resource-efficient approach to continuous VLM improvement, addressing critical limitations in current methodologies, particularly valuable when larger models are unavailable to access."],"url":"http://arxiv.org/abs/2502.09051v1"}
{"created":"2025-02-13 08:02:16","title":"The Social Construction of Visualizations: Practitioner Challenges and Experiences of Visualizing Race and Gender Demographic Data","abstract":"Data visualizations are increasingly seen as socially constructed, with several recent studies positing that perceptions and interpretations of visualization artifacts are shaped through complex sets of interactions between members of a community. However, most of these works have focused on audiences and researchers, and little is known about if and how practitioners account for the socially constructed framing of data visualization. In this paper, we study and analyze how visualization practitioners understand the influence of their beliefs, values, and biases in their design processes and the challenges they experience. In 17 semi-structured interviews with designers working with race and gender demographic data, we find that a complex mix of factors interact to inform how practitioners approach their design process, including their personal experiences, values, and their understandings of power, neutrality, and politics. Based on our findings, we suggest a series of implications for research, design, and education in this space.","sentences":["Data visualizations are increasingly seen as socially constructed, with several recent studies positing that perceptions and interpretations of visualization artifacts are shaped through complex sets of interactions between members of a community.","However, most of these works have focused on audiences and researchers, and little is known about if and how practitioners account for the socially constructed framing of data visualization.","In this paper, we study and analyze how visualization practitioners understand the influence of their beliefs, values, and biases in their design processes and the challenges they experience.","In 17 semi-structured interviews with designers working with race and gender demographic data, we find that a complex mix of factors interact to inform how practitioners approach their design process, including their personal experiences, values, and their understandings of power, neutrality, and politics.","Based on our findings, we suggest a series of implications for research, design, and education in this space."],"url":"http://arxiv.org/abs/2502.09048v1"}
{"created":"2025-02-13 08:01:29","title":"Evolution of Data-driven Single- and Multi-Hazard Susceptibility Mapping and Emergence of Deep Learning Methods","abstract":"Data-driven susceptibility mapping of natural hazards has harnessed the advances in classification methods used on heterogeneous sources represented as raster images. Susceptibility mapping is an important step towards risk assessment for any natural hazard. Increasingly, multiple hazards co-occur spatially, temporally, or both, which calls for an in-depth study on multi-hazard susceptibility mapping. In recent years, single-hazard susceptibility mapping algorithms have become well-established and have been extended to multi-hazard susceptibility mapping. Deep learning is also emerging as a promising method for single-hazard susceptibility mapping. Here, we discuss the evolution of methods for a single hazard, their extensions to multi-hazard maps as a late fusion of decisions, and the use of deep learning methods in susceptibility mapping. We finally propose a vision for adapting data fusion strategies in multimodal deep learning to multi-hazard susceptibility mapping. From the background study of susceptibility methods, we demonstrate that deep learning models are promising, untapped methods for multi-hazard susceptibility mapping. Data fusion strategies provide a larger space of deep learning models applicable to multi-hazard susceptibility mapping.","sentences":["Data-driven susceptibility mapping of natural hazards has harnessed the advances in classification methods used on heterogeneous sources represented as raster images.","Susceptibility mapping is an important step towards risk assessment for any natural hazard.","Increasingly, multiple hazards co-occur spatially, temporally, or both, which calls for an in-depth study on multi-hazard susceptibility mapping.","In recent years, single-hazard susceptibility mapping algorithms have become well-established and have been extended to multi-hazard susceptibility mapping.","Deep learning is also emerging as a promising method for single-hazard susceptibility mapping.","Here, we discuss the evolution of methods for a single hazard, their extensions to multi-hazard maps as a late fusion of decisions, and the use of deep learning methods in susceptibility mapping.","We finally propose a vision for adapting data fusion strategies in multimodal deep learning to multi-hazard susceptibility mapping.","From the background study of susceptibility methods, we demonstrate that deep learning models are promising, untapped methods for multi-hazard susceptibility mapping.","Data fusion strategies provide a larger space of deep learning models applicable to multi-hazard susceptibility mapping."],"url":"http://arxiv.org/abs/2502.09045v1"}
{"created":"2025-02-13 07:57:27","title":"The Datafication of Care in Public Homelessness Services","abstract":"Homelessness systems in North America adopt coordinated data-driven approaches to efficiently match support services to clients based on their assessed needs and available resources. AI tools are increasingly being implemented to allocate resources, reduce costs and predict risks in this space. In this study, we conducted an ethnographic case study on the City of Toronto's homelessness system's data practices across different critical points. We show how the City's data practices offer standardized processes for client care but frontline workers also engage in heuristic decision-making in their work to navigate uncertainties, client resistance to sharing information, and resource constraints. From these findings, we show the temporality of client data which constrain the validity of predictive AI models. Additionally, we highlight how the City adopts an iterative and holistic client assessment approach which contrasts to commonly used risk assessment tools in homelessness, providing future directions to design holistic decision-making tools for homelessness.","sentences":["Homelessness systems in North America adopt coordinated data-driven approaches to efficiently match support services to clients based on their assessed needs and available resources.","AI tools are increasingly being implemented to allocate resources, reduce costs and predict risks in this space.","In this study, we conducted an ethnographic case study on the City of Toronto's homelessness system's data practices across different critical points.","We show how the City's data practices offer standardized processes for client care but frontline workers also engage in heuristic decision-making in their work to navigate uncertainties, client resistance to sharing information, and resource constraints.","From these findings, we show the temporality of client data which constrain the validity of predictive AI models.","Additionally, we highlight how the City adopts an iterative and holistic client assessment approach which contrasts to commonly used risk assessment tools in homelessness, providing future directions to design holistic decision-making tools for homelessness."],"url":"http://arxiv.org/abs/2502.09043v1"}
{"created":"2025-02-13 07:55:54","title":"Typhoon T1: An Open Thai Reasoning Model","abstract":"This paper introduces Typhoon T1, an open effort to develop an open Thai reasoning model. A reasoning model is a relatively new type of generative model built on top of large language models (LLMs). A reasoning model generates a long chain of thought before arriving at a final answer, an approach found to improve performance on complex tasks. However, details on developing such a model are limited, especially for reasoning models that can generate traces in a low-resource language. Typhoon T1 presents an open effort that dives into the details of developing a reasoning model in a more cost-effective way by leveraging supervised fine-tuning using open datasets, instead of reinforcement learning. This paper shares the details about synthetic data generation and training, as well as our dataset and model weights. Additionally, we provide insights gained from developing a reasoning model that generalizes across domains and is capable of generating reasoning traces in a low-resource language, using Thai as an example. We hope this open effort provides a foundation for further research in this field.","sentences":["This paper introduces Typhoon T1, an open effort to develop an open Thai reasoning model.","A reasoning model is a relatively new type of generative model built on top of large language models (LLMs).","A reasoning model generates a long chain of thought before arriving at a final answer, an approach found to improve performance on complex tasks.","However, details on developing such a model are limited, especially for reasoning models that can generate traces in a low-resource language.","Typhoon T1 presents an open effort that dives into the details of developing a reasoning model in a more cost-effective way by leveraging supervised fine-tuning using open datasets, instead of reinforcement learning.","This paper shares the details about synthetic data generation and training, as well as our dataset and model weights.","Additionally, we provide insights gained from developing a reasoning model that generalizes across domains and is capable of generating reasoning traces in a low-resource language, using Thai as an example.","We hope this open effort provides a foundation for further research in this field."],"url":"http://arxiv.org/abs/2502.09042v1"}
{"created":"2025-02-13 07:48:36","title":"AoI-Sensitive Data Forwarding with Distributed Beamforming in UAV-Assisted IoT","abstract":"This paper proposes a UAV-assisted forwarding system based on distributed beamforming to enhance age of information (AoI) in Internet of Things (IoT). Specifically, UAVs collect and relay data between sensor nodes (SNs) and the remote base station (BS). However, flight delays increase the AoI and degrade the network performance. To mitigate this, we adopt distributed beamforming to extend the communication range, reduce the flight frequency and ensure the continuous data relay and efficient energy utilization. Then, we formulate an optimization problem to minimize AoI and UAV energy consumption, by jointly optimizing the UAV trajectories and communication schedules. The problem is non-convex and with high dynamic, and thus we propose a deep reinforcement learning (DRL)-based algorithm to solve the problem, thereby enhancing the stability and accelerate convergence speed. Simulation results show that the proposed algorithm effectively addresses the problem and outperforms other benchmark algorithms.","sentences":["This paper proposes a UAV-assisted forwarding system based on distributed beamforming to enhance age of information (AoI) in Internet of Things (IoT).","Specifically, UAVs collect and relay data between sensor nodes (SNs) and the remote base station (BS).","However, flight delays increase the AoI and degrade the network performance.","To mitigate this, we adopt distributed beamforming to extend the communication range, reduce the flight frequency and ensure the continuous data relay and efficient energy utilization.","Then, we formulate an optimization problem to minimize AoI and UAV energy consumption, by jointly optimizing the UAV trajectories and communication schedules.","The problem is non-convex and with high dynamic, and thus we propose a deep reinforcement learning (DRL)-based algorithm to solve the problem, thereby enhancing the stability and accelerate convergence speed.","Simulation results show that the proposed algorithm effectively addresses the problem and outperforms other benchmark algorithms."],"url":"http://arxiv.org/abs/2502.09038v1"}
{"created":"2025-02-13 07:42:37","title":"Implementation of a Fuzzy Relational Database. Case Study: Chilean Cardboard Industry in the Maule Region","abstract":"The international database community refers to the manipulation of data with inaccuracy and uncertainty using the term fuzzy, which has been translated into Spanish as \"borroso\" and into French as \"flou\". Semantically, this term conveys two main ideas: first, the natural concept of ambiguity or vagueness in human reasoning, and second, its connection to fuzzy set theory, fuzzy logic, and possibility theory, as developed by Zadeh between 1965 and 1977. This article explores two key aspects: the attributes of the fuzzy data model GEFRED (GENeralized model for Fuzzy RElational Database) and their implementation in a Relational Database (RDB). The modeling of these attributes was conducted in a Chilian cardboard manufacturing company located in the Maule Region, where the described phenomena involve imprecise and uncertain attributes and values. Specifically, our focus is on the knowledge related to the manufacturing process of coated cardboard, particularly the quality control process for finished products in the company's Conversion Department. The quality of these products, categorized as either stacks or rolls, is characterized using both classical and fuzzy attributes. Classical attributes are typically measured with physical instruments, whereas fuzzy attributes are assessed through human senses, primarily sight and touch, as perceived by the operators.","sentences":["The international database community refers to the manipulation of data with inaccuracy and uncertainty using the term fuzzy, which has been translated into Spanish as \"borroso\" and into French as \"flou\".","Semantically, this term conveys two main ideas: first, the natural concept of ambiguity or vagueness in human reasoning, and second, its connection to fuzzy set theory, fuzzy logic, and possibility theory, as developed by Zadeh between 1965 and 1977.","This article explores two key aspects: the attributes of the fuzzy data model GEFRED (GENeralized model for Fuzzy RElational Database) and their implementation in a Relational Database (RDB).","The modeling of these attributes was conducted in a Chilian cardboard manufacturing company located in the Maule Region, where the described phenomena involve imprecise and uncertain attributes and values.","Specifically, our focus is on the knowledge related to the manufacturing process of coated cardboard, particularly the quality control process for finished products in the company's Conversion Department.","The quality of these products, categorized as either stacks or rolls, is characterized using both classical and fuzzy attributes.","Classical attributes are typically measured with physical instruments, whereas fuzzy attributes are assessed through human senses, primarily sight and touch, as perceived by the operators."],"url":"http://arxiv.org/abs/2502.09035v1"}
{"created":"2025-02-13 07:31:03","title":"Billet Number Recognition Based on Test-Time Adaptation","abstract":"During the steel billet production process, it is essential to recognize machine-printed or manually written billet numbers on moving billets in real-time. To address the issue of low recognition accuracy for existing scene text recognition methods, caused by factors such as image distortions and distribution differences between training and test data, we propose a billet number recognition method that integrates test-time adaptation with prior knowledge. First, we introduce a test-time adaptation method into a model that uses the DB network for text detection and the SVTR network for text recognition. By minimizing the model's entropy during the testing phase, the model can adapt to the distribution of test data without the need for supervised fine-tuning. Second, we leverage the billet number encoding rules as prior knowledge to assess the validity of each recognition result. Invalid results, which do not comply with the encoding rules, are replaced. Finally, we introduce a validation mechanism into the CTC algorithm using prior knowledge to address its limitations in recognizing damaged characters. Experimental results on real datasets, including both machine-printed billet numbers and handwritten billet numbers, show significant improvements in evaluation metrics, validating the effectiveness of the proposed method.","sentences":["During the steel billet production process, it is essential to recognize machine-printed or manually written billet numbers on moving billets in real-time.","To address the issue of low recognition accuracy for existing scene text recognition methods, caused by factors such as image distortions and distribution differences between training and test data, we propose a billet number recognition method that integrates test-time adaptation with prior knowledge.","First, we introduce a test-time adaptation method into a model that uses the DB network for text detection and the SVTR network for text recognition.","By minimizing the model's entropy during the testing phase, the model can adapt to the distribution of test data without the need for supervised fine-tuning.","Second, we leverage the billet number encoding rules as prior knowledge to assess the validity of each recognition result.","Invalid results, which do not comply with the encoding rules, are replaced.","Finally, we introduce a validation mechanism into the CTC algorithm using prior knowledge to address its limitations in recognizing damaged characters.","Experimental results on real datasets, including both machine-printed billet numbers and handwritten billet numbers, show significant improvements in evaluation metrics, validating the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2502.09026v1"}
{"created":"2025-02-13 07:18:57","title":"From Occupations to Tasks: A New Perspective on Automatability Prediction Using BERT","abstract":"As automation technologies continue to advance at an unprecedented rate, concerns about job displacement and the future of work have become increasingly prevalent. While existing research has primarily focused on the potential impact of automation at the occupation level, there has been a lack of investigation into the automatability of individual tasks. This paper addresses this gap by proposing a BERT-based classifier to predict the automatability of tasks in the forthcoming decade at a granular level leveraging the context and semantics information of tasks. We leverage three public datasets: O*NET Task Statements, ESCO Skills, and Australian Labour Market Insights Tasks, and perform expert annotation. Our BERT-based classifier, fine-tuned on our task statement data, demonstrates superior performance over traditional machine learning models, neural network architectures, and other transformer models. Our findings also indicate that approximately 25.1% of occupations within the O*NET database are at substantial risk of automation, with a diverse spectrum of automation vulnerability across sectors. This research provides a robust tool for assessing the future impact of automation on the labor market, offering valuable insights for policymakers, workers, and industry leaders in the face of rapid technological advancement.","sentences":["As automation technologies continue to advance at an unprecedented rate, concerns about job displacement and the future of work have become increasingly prevalent.","While existing research has primarily focused on the potential impact of automation at the occupation level, there has been a lack of investigation into the automatability of individual tasks.","This paper addresses this gap by proposing a BERT-based classifier to predict the automatability of tasks in the forthcoming decade at a granular level leveraging the context and semantics information of tasks.","We leverage three public datasets: O*NET","Task Statements, ESCO Skills, and Australian Labour Market Insights Tasks, and perform expert annotation.","Our BERT-based classifier, fine-tuned on our task statement data, demonstrates superior performance over traditional machine learning models, neural network architectures, and other transformer models.","Our findings also indicate that approximately 25.1% of occupations within the O*NET database are at substantial risk of automation, with a diverse spectrum of automation vulnerability across sectors.","This research provides a robust tool for assessing the future impact of automation on the labor market, offering valuable insights for policymakers, workers, and industry leaders in the face of rapid technological advancement."],"url":"http://arxiv.org/abs/2502.09021v1"}
{"created":"2025-02-13 07:00:27","title":"Data-Driven Discovery of Population Balance Equations for the Particulate Sciences","abstract":"Understanding the behavior of particles in a dispersed phase system via population balances holds fundamental importance in studies of particulate sciences across various fields. Particle behavior, however, is sophisticated as a single particle can undergo internal property changes (e.g., size, cell age, and energy content) through various mechanisms. When confronted with an unknown distributed particulate system, discovering the underlying population balance equation (PBE) entails firstly learning the underlying particulate phenomena followed by the associated phenomenological laws that govern the kinetics and mechanisms of particle transformations in their local conditions. Conventional inverse problem approaches reveal the shape of phenomenological functions for predetermined forms of PBE (e.g., pure breakage/aggregation PBE, etc.). However, these methods can be limited in their ability to uncover the mechanisms which govern uncharacterized particulate systems from data. Leveraging the increasing abundance of data, we devise a data-driven framework based on sparse regression to learn PBEs as linear combinations of an extensive pool of candidate terms. Thus, this approach enables effective and accurate functional identification of PBEs without assuming the structure a priori, hence mitigating any potential loss of details, while minimizing model overfitting and providing a more interpretable representation of particulate systems. We showcase the proficiency of our approach across a wide spectrum of particulate systems, ranging from simple canonical pure breakage and pure aggregation systems to complex systems with multiple particulate processes. Our approach holds the potential to generalize the discovery of PBEs along with their phenomenological laws from data, thus facilitating wider adoption of population balances.","sentences":["Understanding the behavior of particles in a dispersed phase system via population balances holds fundamental importance in studies of particulate sciences across various fields.","Particle behavior, however, is sophisticated as a single particle can undergo internal property changes (e.g., size, cell age, and energy content) through various mechanisms.","When confronted with an unknown distributed particulate system, discovering the underlying population balance equation (PBE) entails firstly learning the underlying particulate phenomena followed by the associated phenomenological laws that govern the kinetics and mechanisms of particle transformations in their local conditions.","Conventional inverse problem approaches reveal the shape of phenomenological functions for predetermined forms of PBE (e.g., pure breakage/aggregation PBE, etc.).","However, these methods can be limited in their ability to uncover the mechanisms which govern uncharacterized particulate systems from data.","Leveraging the increasing abundance of data, we devise a data-driven framework based on sparse regression to learn PBEs as linear combinations of an extensive pool of candidate terms.","Thus, this approach enables effective and accurate functional identification of PBEs without assuming the structure a priori, hence mitigating any potential loss of details, while minimizing model overfitting and providing a more interpretable representation of particulate systems.","We showcase the proficiency of our approach across a wide spectrum of particulate systems, ranging from simple canonical pure breakage and pure aggregation systems to complex systems with multiple particulate processes.","Our approach holds the potential to generalize the discovery of PBEs along with their phenomenological laws from data, thus facilitating wider adoption of population balances."],"url":"http://arxiv.org/abs/2502.09010v1"}
{"created":"2025-02-13 06:53:26","title":"RED: Energy Optimization Framework for eDRAM-based PIM with Reconfigurable Voltage Swing and Retention-aware Scheduling","abstract":"In the era of artificial intelligence (AI), Transformer demonstrates its performance across various applications. The excessive amount of parameters incurs high latency and energy overhead when processed in the von Neumann architecture. Processing-in-memory (PIM) has shown the potential in accelerating data-intensive applications by reducing data movement. While previous works mainly optimize the computational part of PIM to enhance energy efficiency, the importance of memory design, which consumes the most power in PIM, has been rather neglected. In this work, we present RED, an energy optimization framework for eDRAM-based PIM. We first analyze the PIM operations in eDRAM, obtaining two key observations: 1) memory access energy consumption is predominant in PIM, and 2) read bitline (RBL) voltage swing, sense amplifier power, and retention time are in trade-off relations. Leveraging them, we propose a novel reconfigurable eDRAM and retention-aware scheduling that minimizes the runtime energy consumption of the eDRAM macro. The framework pinpoints the optimal operating point by pre-estimating energy consumption across all possible tiling schemes and memory operations. Then, the reconfigurable eDRAM controls the RBL voltage swing at runtime according to the scheduling, optimizing the memory access power. Moreover, RED employs refresh skipping and sense amplifier power gating to mitigate the energy consumption overhead coming from the trade-off relation. Finally, the RED framework achieves up to 3.05x higher energy efficiency than the prior SRAM-based PIM, reducing the energy consumption of eDRAM macro up to 74.88% with reconfigurable eDRAM and optimization schemes, requiring only 3.5% area and 0.77% energy overhead for scheduling.","sentences":["In the era of artificial intelligence (AI), Transformer demonstrates its performance across various applications.","The excessive amount of parameters incurs high latency and energy overhead when processed in the von Neumann architecture.","Processing-in-memory (PIM) has shown the potential in accelerating data-intensive applications by reducing data movement.","While previous works mainly optimize the computational part of PIM to enhance energy efficiency, the importance of memory design, which consumes the most power in PIM, has been rather neglected.","In this work, we present RED, an energy optimization framework for eDRAM-based PIM.","We first analyze the PIM operations in eDRAM, obtaining two key observations: 1) memory access energy consumption is predominant in PIM, and 2) read bitline (RBL) voltage swing, sense amplifier power, and retention time are in trade-off relations.","Leveraging them, we propose a novel reconfigurable eDRAM and retention-aware scheduling that minimizes the runtime energy consumption of the eDRAM macro.","The framework pinpoints the optimal operating point by pre-estimating energy consumption across all possible tiling schemes and memory operations.","Then, the reconfigurable eDRAM controls the RBL voltage swing at runtime according to the scheduling, optimizing the memory access power.","Moreover, RED employs refresh skipping and sense amplifier power gating to mitigate the energy consumption overhead coming from the trade-off relation.","Finally, the RED framework achieves up to 3.05x higher energy efficiency than the prior SRAM-based PIM, reducing the energy consumption of eDRAM macro up to 74.88% with reconfigurable eDRAM and optimization schemes, requiring only 3.5% area and 0.77% energy overhead for scheduling."],"url":"http://arxiv.org/abs/2502.09007v1"}
{"created":"2025-02-13 06:43:46","title":"End-to-End triplet loss based fine-tuning for network embedding in effective PII detection","abstract":"There are many approaches in mobile data ecosystem that inspect network traffic generated by applications running on user's device to detect personal data exfiltration from the user's device. State-of-the-art methods rely on features extracted from HTTP requests and in this context, machine learning involves training classifiers on these features and making predictions using labelled packet traces. However, most of these methods include external feature selection before model training. Deep learning, on the other hand, typically does not require such techniques, as it can autonomously learn and identify patterns in the data without external feature extraction or selection algorithms. In this article, we propose a novel deep learning based end-to-end learning framework for prediction of exposure of personally identifiable information (PII) in mobile packets. The framework employs a pre-trained large language model (LLM) and an autoencoder to generate embedding of network packets and then uses a triplet-loss based fine-tuning method to train the model, increasing detection effectiveness using two real-world datasets. We compare our proposed detection framework with other state-of-the-art works in detecting PII leaks from user's device.","sentences":["There are many approaches in mobile data ecosystem that inspect network traffic generated by applications running on user's device to detect personal data exfiltration from the user's device.","State-of-the-art methods rely on features extracted from HTTP requests and in this context, machine learning involves training classifiers on these features and making predictions using labelled packet traces.","However, most of these methods include external feature selection before model training.","Deep learning, on the other hand, typically does not require such techniques, as it can autonomously learn and identify patterns in the data without external feature extraction or selection algorithms.","In this article, we propose a novel deep learning based end-to-end learning framework for prediction of exposure of personally identifiable information (PII) in mobile packets.","The framework employs a pre-trained large language model (LLM) and an autoencoder to generate embedding of network packets and then uses a triplet-loss based fine-tuning method to train the model, increasing detection effectiveness using two real-world datasets.","We compare our proposed detection framework with other state-of-the-art works in detecting PII leaks from user's device."],"url":"http://arxiv.org/abs/2502.09002v1"}
{"created":"2025-02-13 06:33:16","title":"Privacy-Preserving Hybrid Ensemble Model for Network Anomaly Detection: Balancing Security and Data Protection","abstract":"Privacy-preserving network anomaly detection has become an essential area of research due to growing concerns over the protection of sensitive data. Traditional anomaly de- tection models often prioritize accuracy while neglecting the critical aspect of privacy. In this work, we propose a hybrid ensemble model that incorporates privacy-preserving techniques to address both detection accuracy and data protection. Our model combines the strengths of several machine learning algo- rithms, including K-Nearest Neighbors (KNN), Support Vector Machines (SVM), XGBoost, and Artificial Neural Networks (ANN), to create a robust system capable of identifying network anomalies while ensuring privacy. The proposed approach in- tegrates advanced preprocessing techniques that enhance data quality and address the challenges of small sample sizes and imbalanced datasets. By embedding privacy measures into the model design, our solution offers a significant advancement over existing methods, ensuring both enhanced detection performance and strong privacy safeguards.","sentences":["Privacy-preserving network anomaly detection has become an essential area of research due to growing concerns over the protection of sensitive data.","Traditional anomaly de- tection models often prioritize accuracy while neglecting the critical aspect of privacy.","In this work, we propose a hybrid ensemble model that incorporates privacy-preserving techniques to address both detection accuracy and data protection.","Our model combines the strengths of several machine learning algo- rithms, including K-Nearest Neighbors (KNN), Support Vector Machines (SVM), XGBoost, and Artificial Neural Networks (ANN), to create a robust system capable of identifying network anomalies while ensuring privacy.","The proposed approach in- tegrates advanced preprocessing techniques that enhance data quality and address the challenges of small sample sizes and imbalanced datasets.","By embedding privacy measures into the model design, our solution offers a significant advancement over existing methods, ensuring both enhanced detection performance and strong privacy safeguards."],"url":"http://arxiv.org/abs/2502.09001v1"}
{"created":"2025-02-13 06:14:59","title":"PixLift: Accelerating Web Browsing via AI Upscaling","abstract":"Accessing the internet in regions with expensive data plans and limited connectivity poses significant challenges, restricting information access and economic growth. Images, as a major contributor to webpage sizes, exacerbate this issue, despite advances in compression formats like WebP and AVIF. The continued growth of complex and curated web content, coupled with suboptimal optimization practices in many regions, has prevented meaningful reductions in web page sizes. This paper introduces PixLift, a novel solution to reduce webpage sizes by downscaling their images during transmission and leveraging AI models on user devices to upscale them. By trading computational resources for bandwidth, PixLift enables more affordable and inclusive web access. We address key challenges, including the feasibility of scaled image requests on popular websites, the implementation of PixLift as a browser extension, and its impact on user experience. Through the analysis of 71.4k webpages, evaluations of three mainstream upscaling models, and a user study, we demonstrate PixLift's ability to significantly reduce data usage without compromising image quality, fostering a more equitable internet.","sentences":["Accessing the internet in regions with expensive data plans and limited connectivity poses significant challenges, restricting information access and economic growth.","Images, as a major contributor to webpage sizes, exacerbate this issue, despite advances in compression formats like WebP and AVIF.","The continued growth of complex and curated web content, coupled with suboptimal optimization practices in many regions, has prevented meaningful reductions in web page sizes.","This paper introduces PixLift, a novel solution to reduce webpage sizes by downscaling their images during transmission and leveraging AI models on user devices to upscale them.","By trading computational resources for bandwidth, PixLift enables more affordable and inclusive web access.","We address key challenges, including the feasibility of scaled image requests on popular websites, the implementation of PixLift as a browser extension, and its impact on user experience.","Through the analysis of 71.4k webpages, evaluations of three mainstream upscaling models, and a user study, we demonstrate PixLift's ability to significantly reduce data usage without compromising image quality, fostering a more equitable internet."],"url":"http://arxiv.org/abs/2502.08995v1"}
{"created":"2025-02-13 06:01:09","title":"RLSA-PFL: Robust Lightweight Secure Aggregation with Model Inconsistency Detection in Privacy-Preserving Federated Learning","abstract":"Federated Learning (FL) allows users to collaboratively train a global machine learning model by sharing local model only, without exposing their private data to a central server. This distributed learning is particularly appealing in scenarios where data privacy is crucial, and it has garnered substantial attention from both industry and academia. However, studies have revealed privacy vulnerabilities in FL, where adversaries can potentially infer sensitive information from the shared model parameters. In this paper, we present an efficient masking-based secure aggregation scheme utilizing lightweight cryptographic primitives to mitigate privacy risks. Our scheme offers several advantages over existing methods. First, it requires only a single setup phase for the entire FL training session, significantly reducing communication overhead. Second, it minimizes user-side overhead by eliminating the need for user-to-user interactions, utilizing an intermediate server layer and a lightweight key negotiation method. Third, the scheme is highly resilient to user dropouts, and the users can join at any FL round. Fourth, it can detect and defend against malicious server activities, including recently discovered model inconsistency attacks. Finally, our scheme ensures security in both semi-honest and malicious settings. We provide security analysis to formally prove the robustness of our approach. Furthermore, we implemented an end-to-end prototype of our scheme. We conducted comprehensive experiments and comparisons, which show that it outperforms existing solutions in terms of communication and computation overhead, functionality, and security.","sentences":["Federated Learning (FL) allows users to collaboratively train a global machine learning model by sharing local model only, without exposing their private data to a central server.","This distributed learning is particularly appealing in scenarios where data privacy is crucial, and it has garnered substantial attention from both industry and academia.","However, studies have revealed privacy vulnerabilities in FL, where adversaries can potentially infer sensitive information from the shared model parameters.","In this paper, we present an efficient masking-based secure aggregation scheme utilizing lightweight cryptographic primitives to mitigate privacy risks.","Our scheme offers several advantages over existing methods.","First, it requires only a single setup phase for the entire FL training session, significantly reducing communication overhead.","Second, it minimizes user-side overhead by eliminating the need for user-to-user interactions, utilizing an intermediate server layer and a lightweight key negotiation method.","Third, the scheme is highly resilient to user dropouts, and the users can join at any FL round.","Fourth, it can detect and defend against malicious server activities, including recently discovered model inconsistency attacks.","Finally, our scheme ensures security in both semi-honest and malicious settings.","We provide security analysis to formally prove the robustness of our approach.","Furthermore, we implemented an end-to-end prototype of our scheme.","We conducted comprehensive experiments and comparisons, which show that it outperforms existing solutions in terms of communication and computation overhead, functionality, and security."],"url":"http://arxiv.org/abs/2502.08989v1"}
{"created":"2025-02-13 05:51:41","title":"Latents of latents to delineate pixels: hybrid Matryoshka autoencoder-to-U-Net pairing for segmenting large medical images in GPU-poor and low-data regimes","abstract":"Medical images are often high-resolution and lose important detail if downsampled, making pixel-level methods such as semantic segmentation much less efficient if performed on a low-dimensional image. We propose a low-rank Matryoshka projection and a hybrid segmenting architecture that preserves important information while retaining sufficient pixel geometry for pixel-level tasks. We design the Matryoshka Autoencoder (MatAE-U-Net) which combines the hierarchical encoding of the Matryoshka Autoencoder with the spatial reconstruction capabilities of a U-Net decoder, leveraging multi-scale feature extraction and skip connections to enhance accuracy and generalisation. We apply it to the problem of segmenting the left ventricle (LV) in echocardiographic images using the Stanford EchoNet-D dataset, including 1,000 standardised video-mask pairs of cardiac ultrasound videos resized to 112x112 pixels. The MatAE-UNet model achieves a Mean IoU of 77.68\\%, Mean Pixel Accuracy of 97.46\\%, and Dice Coefficient of 86.91\\%, outperforming the baseline U-Net, which attains a Mean IoU of 74.70\\%, Mean Pixel Accuracy of 97.31\\%, and Dice Coefficient of 85.20\\%. The results highlight the potential of using the U-Net in the recursive Matroshka latent space for imaging problems with low-contrast such as echocardiographic analysis.","sentences":["Medical images are often high-resolution and lose important detail if downsampled, making pixel-level methods such as semantic segmentation much less efficient if performed on a low-dimensional image.","We propose a low-rank Matryoshka projection and a hybrid segmenting architecture that preserves important information while retaining sufficient pixel geometry for pixel-level tasks.","We design the Matryoshka Autoencoder (MatAE-U-Net) which combines the hierarchical encoding of the Matryoshka Autoencoder with the spatial reconstruction capabilities of a U-Net decoder, leveraging multi-scale feature extraction and skip connections to enhance accuracy and generalisation.","We apply it to the problem of segmenting the left ventricle (LV) in echocardiographic images using the Stanford EchoNet-D dataset, including 1,000 standardised video-mask pairs of cardiac ultrasound videos resized to 112x112 pixels.","The MatAE-UNet model achieves a Mean IoU of 77.68\\%, Mean Pixel Accuracy of 97.46\\%, and Dice Coefficient of 86.91\\%, outperforming the baseline U-Net, which attains a Mean IoU of 74.70\\%, Mean Pixel Accuracy of 97.31\\%, and Dice Coefficient of 85.20\\%.","The results highlight the potential of using the U-Net in the recursive Matroshka latent space for imaging problems with low-contrast such as echocardiographic analysis."],"url":"http://arxiv.org/abs/2502.08988v1"}
{"created":"2025-02-13 05:50:13","title":"Neural Force Field: Learning Generalized Physical Representation from a Few Examples","abstract":"Physical reasoning is a remarkable human ability that enables rapid learning and generalization from limited experience. Current AI models, despite extensive training, still struggle to achieve similar generalization, especially in Out-of-distribution (OOD) settings. This limitation stems from their inability to abstract core physical principles from observations. A key challenge is developing representations that can efficiently learn and generalize physical dynamics from minimal data. Here we present Neural Force Field (NFF) a modeling framework built on Neural Ordinary Differential Equation (NODE) that learns interpretable force field representations which can be efficiently integrated through an Ordinary Differential Equation ( ODE) solver to predict object trajectories. Unlike existing approaches that rely on high-dimensional latent spaces, NFF captures fundamental physical concepts such as gravity, support, and collision in an interpretable manner. Experiments on two challenging physical reasoning tasks demonstrate that NFF, trained with only a few examples, achieves strong generalization to unseen scenarios. This physics-grounded representation enables efficient forward-backward planning and rapid adaptation through interactive refinement. Our work suggests that incorporating physics-inspired representations into learning systems can help bridge the gap between artificial and human physical reasoning capabilities.","sentences":["Physical reasoning is a remarkable human ability that enables rapid learning and generalization from limited experience.","Current AI models, despite extensive training, still struggle to achieve similar generalization, especially in Out-of-distribution (OOD) settings.","This limitation stems from their inability to abstract core physical principles from observations.","A key challenge is developing representations that can efficiently learn and generalize physical dynamics from minimal data.","Here we present Neural Force Field (NFF) a modeling framework built on Neural Ordinary Differential Equation (NODE) that learns interpretable force field representations which can be efficiently integrated through an Ordinary Differential Equation ( ODE) solver to predict object trajectories.","Unlike existing approaches that rely on high-dimensional latent spaces, NFF captures fundamental physical concepts such as gravity, support, and collision in an interpretable manner.","Experiments on two challenging physical reasoning tasks demonstrate that NFF, trained with only a few examples, achieves strong generalization to unseen scenarios.","This physics-grounded representation enables efficient forward-backward planning and rapid adaptation through interactive refinement.","Our work suggests that incorporating physics-inspired representations into learning systems can help bridge the gap between artificial and human physical reasoning capabilities."],"url":"http://arxiv.org/abs/2502.08987v1"}
{"created":"2025-02-13 05:47:57","title":"Few is More: Task-Efficient Skill-Discovery for Multi-Task Offline Multi-Agent Reinforcement Learning","abstract":"As a data-driven approach, offline MARL learns superior policies solely from offline datasets, ideal for domains rich in historical data but with high interaction costs and risks. However, most existing methods are task-specific, requiring retraining for new tasks, leading to redundancy and inefficiency. To address this issue, in this paper, we propose a task-efficient multi-task offline MARL algorithm, Skill-Discovery Conservative Q-Learning (SD-CQL). Unlike existing offline skill-discovery methods, SD-CQL discovers skills by reconstructing the next observation. It then evaluates fixed and variable actions separately and employs behavior-regularized conservative Q-learning to execute the optimal action for each skill. This approach eliminates the need for local-global alignment and enables strong multi-task generalization from limited small-scale source tasks. Substantial experiments on StarCraftII demonstrates the superior generalization performance and task-efficiency of SD-CQL. It achieves the best performance on $\\textbf{10}$ out of $14$ task sets, with up to $\\textbf{65%}$ improvement on individual task sets, and is within $4\\%$ of the best baseline on the remaining four.","sentences":["As a data-driven approach, offline MARL learns superior policies solely from offline datasets, ideal for domains rich in historical data but with high interaction costs and risks.","However, most existing methods are task-specific, requiring retraining for new tasks, leading to redundancy and inefficiency.","To address this issue, in this paper, we propose a task-efficient multi-task offline MARL algorithm, Skill-Discovery Conservative Q-Learning (SD-CQL).","Unlike existing offline skill-discovery methods, SD-CQL discovers skills by reconstructing the next observation.","It then evaluates fixed and variable actions separately and employs behavior-regularized conservative Q-learning to execute the optimal action for each skill.","This approach eliminates the need for local-global alignment and enables strong multi-task generalization from limited small-scale source tasks.","Substantial experiments on StarCraftII demonstrates the superior generalization performance and task-efficiency of SD-CQL.","It achieves the best performance on $\\textbf{10}$ out of $14$ task sets, with up to $\\textbf{65%}$ improvement on individual task sets, and is within $4\\%$ of the best baseline on the remaining four."],"url":"http://arxiv.org/abs/2502.08985v1"}
{"created":"2025-02-13 05:28:29","title":"What exactly has TabPFN learned to do?","abstract":"TabPFN [Hollmann et al., 2023], a Transformer model pretrained to perform in-context learning on fresh tabular classification problems, was presented at the last ICLR conference. To better understand its behavior, we treat it as a black-box function approximator generator and observe its generated function approximations on a varied selection of training datasets. Exploring its learned inductive biases in this manner, we observe behavior that is at turns either brilliant or baffling. We conclude this post with thoughts on how these results might inform the development, evaluation, and application of prior-data fitted networks (PFNs) in the future.","sentences":["TabPFN [Hollmann et al., 2023], a Transformer model pretrained to perform in-context learning on fresh tabular classification problems, was presented at the last ICLR conference.","To better understand its behavior, we treat it as a black-box function approximator generator and observe its generated function approximations on a varied selection of training datasets.","Exploring its learned inductive biases in this manner, we observe behavior that is at turns either brilliant or baffling.","We conclude this post with thoughts on how these results might inform the development, evaluation, and application of prior-data fitted networks (PFNs) in the future."],"url":"http://arxiv.org/abs/2502.08978v1"}
{"created":"2025-02-13 05:13:15","title":"Quantum Approaches for Dysphonia Assessment in Small Speech Datasets","abstract":"Dysphonia, a prevalent medical condition, leads to voice loss, hoarseness, or speech interruptions. To assess it, researchers have been investigating various machine learning techniques alongside traditional medical assessments. Convolutional Neural Networks (CNNs) have gained popularity for their success in audio classification and speech recognition. However, the limited availability of speech data, poses a challenge for CNNs. This study evaluates the performance of CNNs against a novel hybrid quantum-classical approach, Quanvolutional Neural Networks (QNNs), which are well-suited for small datasets. The audio data was preprocessed into Mel spectrograms, comprising 243 training samples and 61 testing samples in total, and used in ten experiments. Four models were developed (two QNNs and two CNNs) with the second models incorporating additional layers to boost performance. The results revealed that QNN models consistently outperformed CNN models in accuracy and stability across most experiments.","sentences":["Dysphonia, a prevalent medical condition, leads to voice loss, hoarseness, or speech interruptions.","To assess it, researchers have been investigating various machine learning techniques alongside traditional medical assessments.","Convolutional Neural Networks (CNNs) have gained popularity for their success in audio classification and speech recognition.","However, the limited availability of speech data, poses a challenge for CNNs.","This study evaluates the performance of CNNs against a novel hybrid quantum-classical approach, Quanvolutional Neural Networks (QNNs), which are well-suited for small datasets.","The audio data was preprocessed into Mel spectrograms, comprising 243 training samples and 61 testing samples in total, and used in ten experiments.","Four models were developed (two QNNs and two CNNs) with the second models incorporating additional layers to boost performance.","The results revealed that QNN models consistently outperformed CNN models in accuracy and stability across most experiments."],"url":"http://arxiv.org/abs/2502.08968v1"}
{"created":"2025-02-13 05:06:22","title":"RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage","abstract":"Tool-Based Agent Systems (TBAS) allow Language Models (LMs) to use external tools for tasks beyond their standalone capabilities, such as searching websites, booking flights, or making financial transactions. However, these tools greatly increase the risks of prompt injection attacks, where malicious content hijacks the LM agent to leak confidential data or trigger harmful actions. Existing defenses (OpenAI GPTs) require user confirmation before every tool call, placing onerous burdens on users. We introduce Robust TBAS (RTBAS), which automatically detects and executes tool calls that preserve integrity and confidentiality, requiring user confirmation only when these safeguards cannot be ensured. RTBAS adapts Information Flow Control to the unique challenges presented by TBAS. We present two novel dependency screeners, using LM-as-a-judge and attention-based saliency, to overcome these challenges. Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy leaks.","sentences":["Tool-Based Agent Systems (TBAS) allow Language Models (LMs) to use external tools for tasks beyond their standalone capabilities, such as searching websites, booking flights, or making financial transactions.","However, these tools greatly increase the risks of prompt injection attacks, where malicious content hijacks the LM agent to leak confidential data or trigger harmful actions.","Existing defenses (OpenAI GPTs) require user confirmation before every tool call, placing onerous burdens on users.","We introduce Robust TBAS (RTBAS), which automatically detects and executes tool calls that preserve integrity and confidentiality, requiring user confirmation only when these safeguards cannot be ensured.","RTBAS adapts Information Flow Control to the unique challenges presented by TBAS.","We present two novel dependency screeners, using LM-as-a-judge and attention-based saliency, to overcome these challenges.","Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy leaks."],"url":"http://arxiv.org/abs/2502.08966v1"}
{"created":"2025-02-13 04:59:01","title":"Modeling Time-evolving Causality over Data Streams","abstract":"Given an extensive, semi-infinite collection of multivariate coevolving data sequences (e.g., sensor/web activity streams) whose observations influence each other, how can we discover the time-changing cause-and-effect relationships in co-evolving data streams? How efficiently can we reveal dynamical patterns that allow us to forecast future values? In this paper, we present a novel streaming method, ModePlait, which is designed for modeling such causal relationships (i.e., time-evolving causality) in multivariate co-evolving data streams and forecasting their future values. The solution relies on characteristics of the causal relationships that evolve over time in accordance with the dynamic changes of exogenous variables. ModePlait has the following properties: (a) Effective: it discovers the time-evolving causality in multivariate co-evolving data streams by detecting the transitions of distinct dynamical patterns adaptively. (b) Accurate: it enables both the discovery of time-evolving causality and the forecasting of future values in a streaming fashion. (c) Scalable: our algorithm does not depend on data stream length and thus is applicable to very large sequences. Extensive experiments on both synthetic and real-world datasets demonstrate that our proposed model outperforms state-of-the-art methods in terms of discovering the time-evolving causality as well as forecasting.","sentences":["Given an extensive, semi-infinite collection of multivariate coevolving data sequences (e.g., sensor/web activity streams) whose observations influence each other, how can we discover the time-changing cause-and-effect relationships in co-evolving data streams?","How efficiently can we reveal dynamical patterns that allow us to forecast future values?","In this paper, we present a novel streaming method, ModePlait, which is designed for modeling such causal relationships (i.e., time-evolving causality) in multivariate co-evolving data streams and forecasting their future values.","The solution relies on characteristics of the causal relationships that evolve over time in accordance with the dynamic changes of exogenous variables.","ModePlait has the following properties: (a) Effective: it discovers the time-evolving causality in multivariate co-evolving data streams by detecting the transitions of distinct dynamical patterns adaptively.","(b) Accurate: it enables both the discovery of time-evolving causality and the forecasting of future values in a streaming fashion.","(c) Scalable: our algorithm does not depend on data stream length and thus is applicable to very large sequences.","Extensive experiments on both synthetic and real-world datasets demonstrate that our proposed model outperforms state-of-the-art methods in terms of discovering the time-evolving causality as well as forecasting."],"url":"http://arxiv.org/abs/2502.08963v1"}
{"created":"2025-02-13 04:53:17","title":"A Comprehensive Survey on Imbalanced Data Learning","abstract":"With the expansion of data availability, machine learning (ML) has achieved remarkable breakthroughs in both academia and industry. However, imbalanced data distributions are prevalent in various types of raw data and severely hinder the performance of ML by biasing the decision-making processes. To deepen the understanding of imbalanced data and facilitate the related research and applications, this survey systematically analyzing various real-world data formats and concludes existing researches for different data formats into four distinct categories: data re-balancing, feature representation, training strategy, and ensemble learning. This structured analysis help researchers comprehensively understand the pervasive nature of imbalance across diverse data format, thereby paving a clearer path toward achieving specific research goals. we provide an overview of relevant open-source libraries, spotlight current challenges, and offer novel insights aimed at fostering future advancements in this critical area of study.","sentences":["With the expansion of data availability, machine learning (ML) has achieved remarkable breakthroughs in both academia and industry.","However, imbalanced data distributions are prevalent in various types of raw data and severely hinder the performance of ML by biasing the decision-making processes.","To deepen the understanding of imbalanced data and facilitate the related research and applications, this survey systematically analyzing various real-world data formats and concludes existing researches for different data formats into four distinct categories: data re-balancing, feature representation, training strategy, and ensemble learning.","This structured analysis help researchers comprehensively understand the pervasive nature of imbalance across diverse data format, thereby paving a clearer path toward achieving specific research goals.","we provide an overview of relevant open-source libraries, spotlight current challenges, and offer novel insights aimed at fostering future advancements in this critical area of study."],"url":"http://arxiv.org/abs/2502.08960v1"}
{"created":"2025-02-13 04:49:14","title":"Training Trajectory Predictors Without Ground-Truth Data","abstract":"This paper presents a framework capable of accurately and smoothly estimating position, heading, and velocity. Using this high-quality input, we propose a system based on Trajectron++, able to consistently generate precise trajectory predictions. Unlike conventional models that require ground-truth data for training, our approach eliminates this dependency. Our analysis demonstrates that poor quality input leads to noisy and unreliable predictions, which can be detrimental to navigation modules. We evaluate both input data quality and model output to illustrate the impact of input noise. Furthermore, we show that our estimation system enables effective training of trajectory prediction models even with limited data, producing robust predictions across different environments. Accurate estimations are crucial for deploying trajectory prediction models in real-world scenarios, and our system ensures meaningful and reliable results across various application contexts.","sentences":["This paper presents a framework capable of accurately and smoothly estimating position, heading, and velocity.","Using this high-quality input, we propose a system based on Trajectron++, able to consistently generate precise trajectory predictions.","Unlike conventional models that require ground-truth data for training, our approach eliminates this dependency.","Our analysis demonstrates that poor quality input leads to noisy and unreliable predictions, which can be detrimental to navigation modules.","We evaluate both input data quality and model output to illustrate the impact of input noise.","Furthermore, we show that our estimation system enables effective training of trajectory prediction models even with limited data, producing robust predictions across different environments.","Accurate estimations are crucial for deploying trajectory prediction models in real-world scenarios, and our system ensures meaningful and reliable results across various application contexts."],"url":"http://arxiv.org/abs/2502.08957v1"}
