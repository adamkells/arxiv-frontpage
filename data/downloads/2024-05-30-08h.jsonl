{"created":"2024-05-29 17:59:58","title":"X-VILA: Cross-Modality Alignment for Large Language Model","abstract":"We introduce X-VILA, an omni-modality model designed to extend the capabilities of large language models (LLMs) by incorporating image, video, and audio modalities. By aligning modality-specific encoders with LLM inputs and diffusion decoders with LLM outputs, X-VILA achieves cross-modality understanding, reasoning, and generation. To facilitate this cross-modality alignment, we curate an effective interleaved any-to-any modality instruction-following dataset. Furthermore, we identify a significant problem with the current cross-modality alignment method, which results in visual information loss. To address the issue, we propose a visual alignment mechanism with a visual embedding highway module. We then introduce a resource-efficient recipe for training X-VILA, that exhibits proficiency in any-to-any modality conversation, surpassing previous approaches by large margins. X-VILA also showcases emergent properties across modalities even in the absence of similar training data. The project will be made open-source.","sentences":["We introduce X-VILA, an omni-modality model designed to extend the capabilities of large language models (LLMs) by incorporating image, video, and audio modalities.","By aligning modality-specific encoders with LLM inputs and diffusion decoders with LLM outputs, X-VILA achieves cross-modality understanding, reasoning, and generation.","To facilitate this cross-modality alignment, we curate an effective interleaved any-to-any modality instruction-following dataset.","Furthermore, we identify a significant problem with the current cross-modality alignment method, which results in visual information loss.","To address the issue, we propose a visual alignment mechanism with a visual embedding highway module.","We then introduce a resource-efficient recipe for training X-VILA, that exhibits proficiency in any-to-any modality conversation, surpassing previous approaches by large margins.","X-VILA also showcases emergent properties across modalities even in the absence of similar training data.","The project will be made open-source."],"url":"http://arxiv.org/abs/2405.19335v1"}
{"created":"2024-05-29 17:58:09","title":"NPGA: Neural Parametric Gaussian Avatars","abstract":"The creation of high-fidelity, digital versions of human heads is an important stepping stone in the process of further integrating virtual components into our everyday lives. Constructing such avatars is a challenging research problem, due to a high demand for photo-realism and real-time rendering performance. In this work, we propose Neural Parametric Gaussian Avatars (NPGA), a data-driven approach to create high-fidelity, controllable avatars from multi-view video recordings. We build our method around 3D Gaussian Splatting for its highly efficient rendering and to inherit the topological flexibility of point clouds. In contrast to previous work, we condition our avatars' dynamics on the rich expression space of neural parametric head models (NPHM), instead of mesh-based 3DMMs. To this end, we distill the backward deformation field of our underlying NPHM into forward deformations which are compatible with rasterization-based rendering. All remaining fine-scale, expression-dependent details are learned from the multi-view videos. To increase the representational capacity of our avatars, we augment the canonical Gaussian point cloud using per-primitive latent features which govern its dynamic behavior. To regularize this increased dynamic expressivity, we propose Laplacian terms on the latent features and predicted dynamics. We evaluate our method on the public NeRSemble dataset, demonstrating that NPGA significantly outperforms the previous state-of-the-art avatars on the self-reenactment task by 2.6 PSNR. Furthermore, we demonstrate accurate animation capabilities from real-world monocular videos.","sentences":["The creation of high-fidelity, digital versions of human heads is an important stepping stone in the process of further integrating virtual components into our everyday lives.","Constructing such avatars is a challenging research problem, due to a high demand for photo-realism and real-time rendering performance.","In this work, we propose Neural Parametric Gaussian Avatars (NPGA), a data-driven approach to create high-fidelity, controllable avatars from multi-view video recordings.","We build our method around 3D Gaussian Splatting for its highly efficient rendering and to inherit the topological flexibility of point clouds.","In contrast to previous work, we condition our avatars' dynamics on the rich expression space of neural parametric head models (NPHM), instead of mesh-based 3DMMs.","To this end, we distill the backward deformation field of our underlying NPHM into forward deformations which are compatible with rasterization-based rendering.","All remaining fine-scale, expression-dependent details are learned from the multi-view videos.","To increase the representational capacity of our avatars, we augment the canonical Gaussian point cloud using per-primitive latent features which govern its dynamic behavior.","To regularize this increased dynamic expressivity, we propose Laplacian terms on the latent features and predicted dynamics.","We evaluate our method on the public NeRSemble dataset, demonstrating that NPGA significantly outperforms the previous state-of-the-art avatars on the self-reenactment task by 2.6 PSNR.","Furthermore, we demonstrate accurate animation capabilities from real-world monocular videos."],"url":"http://arxiv.org/abs/2405.19331v1"}
{"created":"2024-05-29 17:57:16","title":"MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series","abstract":"Large Language Models (LLMs) have made great strides in recent years to achieve unprecedented performance across different tasks. However, due to commercial interest, the most competitive models like GPT, Gemini, and Claude have been gated behind proprietary interfaces without disclosing the training details. Recently, many institutions have open-sourced several strong LLMs like LLaMA-3, comparable to existing closed-source LLMs. However, only the model's weights are provided with most details (e.g., intermediate checkpoints, pre-training corpus, and training code, etc.) being undisclosed. To improve the transparency of LLMs, the research community has formed to open-source truly open LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training corpus and training code) are being provided. These models have greatly advanced the scientific study of these large models including their strengths, weaknesses, biases and risks. However, we observe that the existing truly open LLMs on reasoning, knowledge, and coding tasks are still inferior to existing state-of-the-art LLMs with similar model sizes. To this end, we open-source MAP-Neo, a highly capable and transparent bilingual language model with 7B parameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the first fully open-sourced bilingual LLM with comparable performance compared to existing state-of-the-art LLMs. Moreover, we open-source all details to reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning pipeline, checkpoints, and well-optimized training/evaluation framework are provided. Finally, we hope our MAP-Neo will enhance and strengthen the open research community and inspire more innovations and creativities to facilitate the further improvements of LLMs.","sentences":["Large Language Models (LLMs) have made great strides in recent years to achieve unprecedented performance across different tasks.","However, due to commercial interest, the most competitive models like GPT, Gemini, and Claude have been gated behind proprietary interfaces without disclosing the training details.","Recently, many institutions have open-sourced several strong LLMs like LLaMA-3, comparable to existing closed-source LLMs.","However, only the model's weights are provided with most details (e.g., intermediate checkpoints, pre-training corpus, and training code, etc.)","being undisclosed.","To improve the transparency of LLMs, the research community has formed to open-source truly open LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training corpus and training code) are being provided.","These models have greatly advanced the scientific study of these large models including their strengths, weaknesses, biases and risks.","However, we observe that the existing truly open LLMs on reasoning, knowledge, and coding tasks are still inferior to existing state-of-the-art LLMs with similar model sizes.","To this end, we open-source MAP-Neo, a highly capable and transparent bilingual language model with 7B parameters trained from scratch on 4.5T high-quality tokens.","Our MAP-Neo is the first fully open-sourced bilingual LLM with comparable performance compared to existing state-of-the-art LLMs.","Moreover, we open-source all details to reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning pipeline, checkpoints, and well-optimized training/evaluation framework are provided.","Finally, we hope our MAP-Neo will enhance and strengthen the open research community and inspire more innovations and creativities to facilitate the further improvements of LLMs."],"url":"http://arxiv.org/abs/2405.19327v1"}
{"created":"2024-05-29 17:56:07","title":"Reasoning3D -- Grounding and Reasoning in 3D: Fine-Grained Zero-Shot Open-Vocabulary 3D Reasoning Part Segmentation via Large Vision-Language Models","abstract":"In this paper, we introduce a new task: Zero-Shot 3D Reasoning Segmentation for parts searching and localization for objects, which is a new paradigm to 3D segmentation that transcends limitations for previous category-specific 3D semantic segmentation, 3D instance segmentation, and open-vocabulary 3D segmentation. We design a simple baseline method, Reasoning3D, with the capability to understand and execute complex commands for (fine-grained) segmenting specific parts for 3D meshes with contextual awareness and reasoned answers for interactive segmentation. Specifically, Reasoning3D leverages an off-the-shelf pre-trained 2D segmentation network, powered by Large Language Models (LLMs), to interpret user input queries in a zero-shot manner. Previous research have shown that extensive pre-training endows foundation models with prior world knowledge, enabling them to comprehend complex commands, a capability we can harness to \"segment anything\" in 3D with limited 3D datasets (source efficient). Experimentation reveals that our approach is generalizable and can effectively localize and highlight parts of 3D objects (in 3D mesh) based on implicit textual queries, including these articulated 3d objects and real-world scanned data. Our method can also generate natural language explanations corresponding to these 3D models and the decomposition. Moreover, our training-free approach allows rapid deployment and serves as a viable universal baseline for future research of part-level 3d (semantic) object understanding in various fields including robotics, object manipulation, part assembly, autonomous driving applications, augment reality and virtual reality (AR/VR), and medical applications. The code, the model weight, the deployment guide, and the evaluation protocol are: http://tianrun-chen.github.io/Reason3D/","sentences":["In this paper, we introduce a new task: Zero-Shot 3D Reasoning Segmentation for parts searching and localization for objects, which is a new paradigm to 3D segmentation that transcends limitations for previous category-specific 3D semantic segmentation, 3D instance segmentation, and open-vocabulary 3D segmentation.","We design a simple baseline method, Reasoning3D, with the capability to understand and execute complex commands for (fine-grained) segmenting specific parts for 3D meshes with contextual awareness and reasoned answers for interactive segmentation.","Specifically, Reasoning3D leverages an off-the-shelf pre-trained 2D segmentation network, powered by Large Language Models (LLMs), to interpret user input queries in a zero-shot manner.","Previous research have shown that extensive pre-training endows foundation models with prior world knowledge, enabling them to comprehend complex commands, a capability we can harness to \"segment anything\" in 3D with limited 3D datasets (source efficient).","Experimentation reveals that our approach is generalizable and can effectively localize and highlight parts of 3D objects (in 3D mesh) based on implicit textual queries, including these articulated 3d objects and real-world scanned data.","Our method can also generate natural language explanations corresponding to these 3D models and the decomposition.","Moreover, our training-free approach allows rapid deployment and serves as a viable universal baseline for future research of part-level 3d (semantic) object understanding in various fields including robotics, object manipulation, part assembly, autonomous driving applications, augment reality and virtual reality (AR/VR), and medical applications.","The code, the model weight, the deployment guide, and the evaluation protocol are: http://tianrun-chen.github.io/Reason3D/"],"url":"http://arxiv.org/abs/2405.19326v1"}
{"created":"2024-05-29 17:55:03","title":"Nearest Neighbor Speculative Decoding for LLM Generation and Attribution","abstract":"Large language models (LLMs) often hallucinate and lack the ability to provide attribution for their generations. Semi-parametric LMs, such as kNN-LM, approach these limitations by refining the output of an LM for a given prompt using its nearest neighbor matches in a non-parametric data store. However, these models often exhibit slow inference speeds and produce non-fluent texts. In this paper, we introduce Nearest Neighbor Speculative Decoding (NEST), a novel semi-parametric language modeling approach that is capable of incorporating real-world text spans of arbitrary length into the LM generations and providing attribution to their sources. NEST performs token-level retrieval at each inference step to compute a semi-parametric mixture distribution and identify promising span continuations in a corpus. It then uses an approximate speculative decoding procedure that accepts a prefix of the retrieved span or generates a new token. NEST significantly enhances the generation quality and attribution rate of the base LM across a variety of knowledge-intensive tasks, surpassing the conventional kNN-LM method and performing competitively with in-context retrieval augmentation. In addition, NEST substantially improves the generation speed, achieving a 1.8x speedup in inference time when applied to Llama-2-Chat 70B.","sentences":["Large language models (LLMs) often hallucinate and lack the ability to provide attribution for their generations.","Semi-parametric LMs, such as kNN-LM, approach these limitations by refining the output of an LM for a given prompt using its nearest neighbor matches in a non-parametric data store.","However, these models often exhibit slow inference speeds and produce non-fluent texts.","In this paper, we introduce Nearest Neighbor Speculative Decoding (NEST), a novel semi-parametric language modeling approach that is capable of incorporating real-world text spans of arbitrary length into the LM generations and providing attribution to their sources.","NEST performs token-level retrieval at each inference step to compute a semi-parametric mixture distribution and identify promising span continuations in a corpus.","It then uses an approximate speculative decoding procedure that accepts a prefix of the retrieved span or generates a new token.","NEST significantly enhances the generation quality and attribution rate of the base LM across a variety of knowledge-intensive tasks, surpassing the conventional kNN-LM method and performing competitively with in-context retrieval augmentation.","In addition, NEST substantially improves the generation speed, achieving a 1.8x speedup in inference time when applied to Llama-2-Chat 70B."],"url":"http://arxiv.org/abs/2405.19325v1"}
{"created":"2024-05-29 17:54:22","title":"Are Large Language Models Chameleons?","abstract":"Do large language models (LLMs) have their own worldviews and personality tendencies? Simulations in which an LLM was asked to answer subjective questions were conducted more than 1 million times. Comparison of the responses from different LLMs with real data from the European Social Survey (ESS) suggests that the effect of prompts on bias and variability is fundamental, highlighting major cultural, age, and gender biases. Methods for measuring the difference between LLMs and survey data are discussed, such as calculating weighted means and a new proposed measure inspired by Jaccard similarity. We conclude that it is important to analyze the robustness and variability of prompts before using LLMs to model individual decisions or collective behavior, as their imitation abilities are approximate at best.","sentences":["Do large language models (LLMs) have their own worldviews and personality tendencies?","Simulations in which an LLM was asked to answer subjective questions were conducted more than 1 million times.","Comparison of the responses from different LLMs with real data from the European Social Survey (ESS) suggests that the effect of prompts on bias and variability is fundamental, highlighting major cultural, age, and gender biases.","Methods for measuring the difference between LLMs and survey data are discussed, such as calculating weighted means and a new proposed measure inspired by Jaccard similarity.","We conclude that it is important to analyze the robustness and variability of prompts before using LLMs to model individual decisions or collective behavior, as their imitation abilities are approximate at best."],"url":"http://arxiv.org/abs/2405.19323v1"}
{"created":"2024-05-29 17:51:42","title":"Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF","abstract":"Reinforcement learning from human feedback (RLHF) has demonstrated great promise in aligning large language models (LLMs) with human preference. Depending on the availability of preference data, both online and offline RLHF are active areas of investigation. A key bottleneck is understanding how to incorporate uncertainty estimation in the reward function learned from the preference data for RLHF, regardless of how the preference data is collected. While the principles of optimism or pessimism under uncertainty are well-established in standard reinforcement learning (RL), a practically-implementable and theoretically-grounded form amenable to large language models is not yet available, as standard techniques for constructing confidence intervals become intractable under arbitrary policy parameterizations.   In this paper, we introduce a unified approach to online and offline RLHF -- value-incentivized preference optimization (VPO) -- which regularizes the maximum-likelihood estimate of the reward function with the corresponding value function, modulated by a $\\textit{sign}$ to indicate whether the optimism or pessimism is chosen. VPO also directly optimizes the policy with implicit reward modeling, and therefore shares a simpler RLHF pipeline similar to direct preference optimization. Theoretical guarantees of VPO are provided for both online and offline settings, matching the rates of their standard RL counterparts. Moreover, experiments on text summarization and dialog verify the practicality and effectiveness of VPO.","sentences":["Reinforcement learning from human feedback (RLHF) has demonstrated great promise in aligning large language models (LLMs) with human preference.","Depending on the availability of preference data, both online and offline RLHF are active areas of investigation.","A key bottleneck is understanding how to incorporate uncertainty estimation in the reward function learned from the preference data for RLHF, regardless of how the preference data is collected.","While the principles of optimism or pessimism under uncertainty are well-established in standard reinforcement learning (RL), a practically-implementable and theoretically-grounded form amenable to large language models is not yet available, as standard techniques for constructing confidence intervals become intractable under arbitrary policy parameterizations.   ","In this paper, we introduce a unified approach to online and offline RLHF -- value-incentivized preference optimization (VPO) -- which regularizes the maximum-likelihood estimate of the reward function with the corresponding value function, modulated by a $\\textit{sign}$ to indicate whether the optimism or pessimism is chosen.","VPO also directly optimizes the policy with implicit reward modeling, and therefore shares a simpler RLHF pipeline similar to direct preference optimization.","Theoretical guarantees of VPO are provided for both online and offline settings, matching the rates of their standard RL counterparts.","Moreover, experiments on text summarization and dialog verify the practicality and effectiveness of VPO."],"url":"http://arxiv.org/abs/2405.19320v1"}
{"created":"2024-05-29 17:39:48","title":"Robust Preference Optimization through Reward Model Distillation","abstract":"Language model (LM) post-training (or alignment) involves maximizing a reward function that is derived from preference annotations. Direct Preference Optimization (DPO) is a popular offline alignment method that trains a policy directly on preference data without the need to train a reward model or apply reinforcement learning. However, typical preference datasets have only a single, or at most a few, annotation per preference pair, which causes DPO to overconfidently assign rewards that trend towards infinite magnitude. This frequently leads to degenerate policies, sometimes causing even the probabilities of the preferred generations to go to zero. In this work, we analyze this phenomenon and propose distillation to get a better proxy for the true preference distribution over generation pairs: we train the LM to produce probabilities that match the distribution induced by a reward model trained on the preference data. Moreover, to account for uncertainty in the reward model we are distilling from, we optimize against a family of reward models that, as a whole, is likely to include at least one reasonable proxy for the preference distribution. Our results show that distilling from such a family of reward models leads to improved robustness to distribution shift in preference annotations, while preserving the simple supervised nature of DPO.","sentences":["Language model (LM) post-training (or alignment) involves maximizing a reward function that is derived from preference annotations.","Direct Preference Optimization (DPO) is a popular offline alignment method that trains a policy directly on preference data without the need to train a reward model or apply reinforcement learning.","However, typical preference datasets have only a single, or at most a few, annotation per preference pair, which causes DPO to overconfidently assign rewards that trend towards infinite magnitude.","This frequently leads to degenerate policies, sometimes causing even the probabilities of the preferred generations to go to zero.","In this work, we analyze this phenomenon and propose distillation to get a better proxy for the true preference distribution over generation pairs: we train the LM to produce probabilities that match the distribution induced by a reward model trained on the preference data.","Moreover, to account for uncertainty in the reward model we are distilling from, we optimize against a family of reward models that, as a whole, is likely to include at least one reasonable proxy for the preference distribution.","Our results show that distilling from such a family of reward models leads to improved robustness to distribution shift in preference annotations, while preserving the simple supervised nature of DPO."],"url":"http://arxiv.org/abs/2405.19316v1"}
{"created":"2024-05-29 17:37:14","title":"Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice","abstract":"The observed similarities in the behavior of humans and Large Language Models (LLMs) have prompted researchers to consider the potential of using LLMs as models of human cognition. However, several significant challenges must be addressed before LLMs can be legitimately regarded as cognitive models. For instance, LLMs are trained on far more data than humans typically encounter, and may have been directly trained on human data in specific cognitive tasks or aligned with human preferences. Consequently, the origins of these behavioral similarities are not well understood. In this paper, we propose a novel way to enhance the utility of LLMs as cognitive models. This approach involves (i) leveraging computationally equivalent tasks that both an LLM and a rational agent need to master for solving a cognitive problem and (ii) examining the specific task distributions required for an LLM to exhibit human-like behaviors. We apply this approach to decision-making -- specifically risky and intertemporal choice -- where the key computationally equivalent task is the arithmetic of expected value calculations. We show that an LLM pretrained on an ecologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts human behavior better than many traditional cognitive models. Pretraining LLMs on ecologically valid arithmetic datasets is sufficient to produce a strong correspondence between these models and human decision-making. Our results also suggest that LLMs used as cognitive models should be carefully investigated via ablation studies of the pretraining data.","sentences":["The observed similarities in the behavior of humans and Large Language Models (LLMs) have prompted researchers to consider the potential of using LLMs as models of human cognition.","However, several significant challenges must be addressed before LLMs can be legitimately regarded as cognitive models.","For instance, LLMs are trained on far more data than humans typically encounter, and may have been directly trained on human data in specific cognitive tasks or aligned with human preferences.","Consequently, the origins of these behavioral similarities are not well understood.","In this paper, we propose a novel way to enhance the utility of LLMs as cognitive models.","This approach involves (i) leveraging computationally equivalent tasks that both an LLM and a rational agent need to master for solving a cognitive problem and (ii) examining the specific task distributions required for an LLM to exhibit human-like behaviors.","We apply this approach to decision-making -- specifically risky and intertemporal choice -- where the key computationally equivalent task is the arithmetic of expected value calculations.","We show that an LLM pretrained on an ecologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts human behavior better than many traditional cognitive models.","Pretraining LLMs on ecologically valid arithmetic datasets is sufficient to produce a strong correspondence between these models and human decision-making.","Our results also suggest that LLMs used as cognitive models should be carefully investigated via ablation studies of the pretraining data."],"url":"http://arxiv.org/abs/2405.19313v1"}
{"created":"2024-05-29 17:31:25","title":"Data Efficient Behavior Cloning for Fine Manipulation via Continuity-based Corrective Labels","abstract":"We consider imitation learning with access only to expert demonstrations, whose real-world application is often limited by covariate shift due to compounding errors during execution. We investigate the effectiveness of the Continuity-based Corrective Labels for Imitation Learning (CCIL) framework in mitigating this issue for real-world fine manipulation tasks. CCIL generates corrective labels by learning a locally continuous dynamics model from demonstrations to guide the agent back toward expert states. Through extensive experiments on peg insertion and fine grasping, we provide the first empirical validation that CCIL can significantly improve imitation learning performance despite discontinuities present in contact-rich manipulation. We find that: (1) real-world manipulation exhibits sufficient local smoothness to apply CCIL, (2) generated corrective labels are most beneficial in low-data regimes, and (3) label filtering based on estimated dynamics model error enables performance gains. To effectively apply CCIL to robotic domains, we offer a practical instantiation of the framework and insights into design choices and hyperparameter selection. Our work demonstrates CCIL's practicality for alleviating compounding errors in imitation learning on physical robots.","sentences":["We consider imitation learning with access only to expert demonstrations, whose real-world application is often limited by covariate shift due to compounding errors during execution.","We investigate the effectiveness of the Continuity-based Corrective Labels for Imitation Learning (CCIL) framework in mitigating this issue for real-world fine manipulation tasks.","CCIL generates corrective labels by learning a locally continuous dynamics model from demonstrations to guide the agent back toward expert states.","Through extensive experiments on peg insertion and fine grasping, we provide the first empirical validation that CCIL can significantly improve imitation learning performance despite discontinuities present in contact-rich manipulation.","We find that: (1) real-world manipulation exhibits sufficient local smoothness to apply CCIL, (2) generated corrective labels are most beneficial in low-data regimes, and (3) label filtering based on estimated dynamics model error enables performance gains.","To effectively apply CCIL to robotic domains, we offer a practical instantiation of the framework and insights into design choices and hyperparameter selection.","Our work demonstrates CCIL's practicality for alleviating compounding errors in imitation learning on physical robots."],"url":"http://arxiv.org/abs/2405.19307v1"}
{"created":"2024-05-29 17:27:30","title":"Safe and Efficient Estimation for Robotics through the Optimal Use of Resources","abstract":"In order to operate in and interact with the physical world, robots need to have estimates of the current and future state of the environment. We thus equip robots with sensors and build models and algorithms that, given some measurements, produce estimates of the current or future states. Environments can be unpredictable and sensors are not perfect. Therefore, it is important to both use all information available, and to do so optimally: making sure that we get the best possible answer from the amount of information we have. However, in prevalent research, uncommon sensors, such as sound or radio-frequency signals, are commonly ignored for state estimation; and the most popular solvers employed to produce state estimates are only of local nature, meaning they may produce suboptimal estimates for the typically non-convex estimation problems. My research aims to use resources more optimally, by building on 1) multi-modality: using ubiquitous RF transceivers and microphones to support state estimation, 2) building certifiably optimal solvers and 3) learning and improving adequate models from data.","sentences":["In order to operate in and interact with the physical world, robots need to have estimates of the current and future state of the environment.","We thus equip robots with sensors and build models and algorithms that, given some measurements, produce estimates of the current or future states.","Environments can be unpredictable and sensors are not perfect.","Therefore, it is important to both use all information available, and to do so optimally: making sure that we get the best possible answer from the amount of information we have.","However, in prevalent research, uncommon sensors, such as sound or radio-frequency signals, are commonly ignored for state estimation; and the most popular solvers employed to produce state estimates are only of local nature, meaning they may produce suboptimal estimates for the typically non-convex estimation problems.","My research aims to use resources more optimally, by building on 1) multi-modality: using ubiquitous RF transceivers and microphones to support state estimation, 2) building certifiably optimal solvers and 3) learning and improving adequate models from data."],"url":"http://arxiv.org/abs/2405.19301v1"}
{"created":"2024-05-29 17:26:52","title":"Expert-Guided Extinction of Toxic Tokens for Debiased Generation","abstract":"Large language models (LLMs) can elicit social bias during generations, especially when inference with toxic prompts. Controlling the sensitive attributes in generation encounters challenges in data distribution, generalizability, and efficiency. Specifically, fine-tuning and retrieval demand extensive unbiased corpus, while direct prompting requires meticulously curated instructions for correcting the output in multiple rounds of thoughts but poses challenges on memory and inference latency. In this work, we propose the Expert-Guided Extinction of Toxic Tokens for Debiased Generation (EXPOSED) to eliminate the undesired harmful outputs for LLMs without the aforementioned requirements. EXPOSED constructs a debiasing expert based on the abundant toxic corpus to expose and elicit the potentially dangerous tokens. It then processes the output to the LLMs and constructs a fair distribution by suppressing and attenuating the toxic tokens. EXPOSED is evaluated on fairness benchmarks over three LLM families. Extensive experiments demonstrate that compared with other baselines, the proposed EXPOSED significantly reduces the potential social bias while balancing fairness and generation performance.","sentences":["Large language models (LLMs) can elicit social bias during generations, especially when inference with toxic prompts.","Controlling the sensitive attributes in generation encounters challenges in data distribution, generalizability, and efficiency.","Specifically, fine-tuning and retrieval demand extensive unbiased corpus, while direct prompting requires meticulously curated instructions for correcting the output in multiple rounds of thoughts but poses challenges on memory and inference latency.","In this work, we propose the Expert-Guided Extinction of Toxic Tokens for Debiased Generation (EXPOSED) to eliminate the undesired harmful outputs for LLMs without the aforementioned requirements.","EXPOSED constructs a debiasing expert based on the abundant toxic corpus to expose and elicit the potentially dangerous tokens.","It then processes the output to the LLMs and constructs a fair distribution by suppressing and attenuating the toxic tokens.","EXPOSED is evaluated on fairness benchmarks over three LLM families.","Extensive experiments demonstrate that compared with other baselines, the proposed EXPOSED significantly reduces the potential social bias while balancing fairness and generation performance."],"url":"http://arxiv.org/abs/2405.19299v1"}
{"created":"2024-05-29 17:21:25","title":"Act Natural! Projecting Autonomous System Trajectories Into Naturalistic Behavior Sets","abstract":"Autonomous agents operating around human actors must consider how their behaviors might affect those humans, even when not directly interacting with them. To this end, it is often beneficial to be predictable and appear naturalistic. Existing methods to address this problem use human actor intent modeling or imitation learning techniques, but these approaches rarely capture all possible motivations for human behavior or require significant amounts of data. In contrast, we propose a technique for modeling naturalistic behavior as a set of convex hulls computed over a relatively small dataset of human behavior. Given this set, we design an optimization-based filter which projects arbitrary trajectories into it to make them more naturalistic for autonomous agents to execute while also satisfying dynamics constraints. We demonstrate our methods on real-world human driving data from the inD intersection dataset (Bock et al., 2020).","sentences":["Autonomous agents operating around human actors must consider how their behaviors might affect those humans, even when not directly interacting with them.","To this end, it is often beneficial to be predictable and appear naturalistic.","Existing methods to address this problem use human actor intent modeling or imitation learning techniques, but these approaches rarely capture all possible motivations for human behavior or require significant amounts of data.","In contrast, we propose a technique for modeling naturalistic behavior as a set of convex hulls computed over a relatively small dataset of human behavior.","Given this set, we design an optimization-based filter which projects arbitrary trajectories into it to make them more naturalistic for autonomous agents to execute while also satisfying dynamics constraints.","We demonstrate our methods on real-world human driving data from the inD intersection dataset (Bock et al., 2020)."],"url":"http://arxiv.org/abs/2405.19292v1"}
{"created":"2024-05-29 17:19:04","title":"Integrating Multi-scale Contextualized Information for Byte-based Neural Machine Translation","abstract":"Subword tokenization is a common method for vocabulary building in Neural Machine Translation (NMT) models. However, increasingly complex tasks have revealed its disadvantages. First, a vocabulary cannot be modified once it is learned, making it hard to adapt to new words. Second, in multilingual translation, the imbalance in data volumes across different languages spreads to the vocabulary, exacerbating translations involving low-resource languages. While byte-based tokenization addresses these issues, byte-based models struggle with the low information density inherent in UTF-8 byte sequences. Previous works enhance token semantics through local contextualization but fail to select an appropriate contextualizing scope based on the input. Consequently, we propose the Multi-Scale Contextualization (MSC) method, which learns contextualized information of varying scales across different hidden state dimensions. It then leverages the attention module to dynamically integrate the multi-scale contextualized information. Experiments show that MSC significantly outperforms subword-based and other byte-based methods in both multilingual and out-of-domain scenarios. Code can be found in https://github.com/ictnlp/Multiscale-Contextualization.","sentences":["Subword tokenization is a common method for vocabulary building in Neural Machine Translation (NMT) models.","However, increasingly complex tasks have revealed its disadvantages.","First, a vocabulary cannot be modified once it is learned, making it hard to adapt to new words.","Second, in multilingual translation, the imbalance in data volumes across different languages spreads to the vocabulary, exacerbating translations involving low-resource languages.","While byte-based tokenization addresses these issues, byte-based models struggle with the low information density inherent in UTF-8 byte sequences.","Previous works enhance token semantics through local contextualization but fail to select an appropriate contextualizing scope based on the input.","Consequently, we propose the Multi-Scale Contextualization (MSC) method, which learns contextualized information of varying scales across different hidden state dimensions.","It then leverages the attention module to dynamically integrate the multi-scale contextualized information.","Experiments show that MSC significantly outperforms subword-based and other byte-based methods in both multilingual and out-of-domain scenarios.","Code can be found in https://github.com/ictnlp/Multiscale-Contextualization."],"url":"http://arxiv.org/abs/2405.19290v1"}
{"created":"2024-05-29 17:14:55","title":"Programmable Motion Generation for Open-Set Motion Control Tasks","abstract":"Character animation in real-world scenarios necessitates a variety of constraints, such as trajectories, key-frames, interactions, etc. Existing methodologies typically treat single or a finite set of these constraint(s) as separate control tasks. They are often specialized, and the tasks they address are rarely extendable or customizable. We categorize these as solutions to the close-set motion control problem. In response to the complexity of practical motion control, we propose and attempt to solve the open-set motion control problem. This problem is characterized by an open and fully customizable set of motion control tasks. To address this, we introduce a new paradigm, programmable motion generation. In this paradigm, any given motion control task is broken down into a combination of atomic constraints. These constraints are then programmed into an error function that quantifies the degree to which a motion sequence adheres to them. We utilize a pre-trained motion generation model and optimize its latent code to minimize the error function of the generated motion. Consequently, the generated motion not only inherits the prior of the generative model but also satisfies the required constraints. Experiments show that we can generate high-quality motions when addressing a wide range of unseen tasks. These tasks encompass motion control by motion dynamics, geometric constraints, physical laws, interactions with scenes, objects or the character own body parts, etc. All of these are achieved in a unified approach, without the need for ad-hoc paired training data collection or specialized network designs. During the programming of novel tasks, we observed the emergence of new skills beyond those of the prior model. With the assistance of large language models, we also achieved automatic programming. We hope that this work will pave the way for the motion control of general AI agents.","sentences":["Character animation in real-world scenarios necessitates a variety of constraints, such as trajectories, key-frames, interactions, etc.","Existing methodologies typically treat single or a finite set of these constraint(s) as separate control tasks.","They are often specialized, and the tasks they address are rarely extendable or customizable.","We categorize these as solutions to the close-set motion control problem.","In response to the complexity of practical motion control, we propose and attempt to solve the open-set motion control problem.","This problem is characterized by an open and fully customizable set of motion control tasks.","To address this, we introduce a new paradigm, programmable motion generation.","In this paradigm, any given motion control task is broken down into a combination of atomic constraints.","These constraints are then programmed into an error function that quantifies the degree to which a motion sequence adheres to them.","We utilize a pre-trained motion generation model and optimize its latent code to minimize the error function of the generated motion.","Consequently, the generated motion not only inherits the prior of the generative model but also satisfies the required constraints.","Experiments show that we can generate high-quality motions when addressing a wide range of unseen tasks.","These tasks encompass motion control by motion dynamics, geometric constraints, physical laws, interactions with scenes, objects or the character own body parts, etc.","All of these are achieved in a unified approach, without the need for ad-hoc paired training data collection or specialized network designs.","During the programming of novel tasks, we observed the emergence of new skills beyond those of the prior model.","With the assistance of large language models, we also achieved automatic programming.","We hope that this work will pave the way for the motion control of general AI agents."],"url":"http://arxiv.org/abs/2405.19283v1"}
{"created":"2024-05-29 17:07:33","title":"Deep Latent Variable Modeling of Physiological Signals","abstract":"A deep latent variable model is a powerful method for capturing complex distributions. These models assume that underlying structures, but unobserved, are present within the data. In this dissertation, we explore high-dimensional problems related to physiological monitoring using latent variable models. First, we present a novel deep state-space model to generate electrical waveforms of the heart using optically obtained signals as inputs. This can bring about clinical diagnoses of heart disease via simple assessment through wearable devices. Second, we present a brain signal modeling scheme that combines the strengths of probabilistic graphical models and deep adversarial learning. The structured representations can provide interpretability and encode inductive biases to reduce the data complexity of neural oscillations. The efficacy of the learned representations is further studied in epilepsy seizure detection formulated as an unsupervised learning problem. Third, we propose a framework for the joint modeling of physiological measures and behavior. Existing methods to combine multiple sources of brain data provided are limited. Direct analysis of the relationship between different types of physiological measures usually does not involve behavioral data. Our method can identify the unique and shared contributions of brain regions to behavior and can be used to discover new functions of brain regions. The success of these innovative computational methods would allow the translation of biomarker findings across species and provide insight into neurocognitive analysis in numerous biological studies and clinical diagnoses, as well as emerging consumer applications.","sentences":["A deep latent variable model is a powerful method for capturing complex distributions.","These models assume that underlying structures, but unobserved, are present within the data.","In this dissertation, we explore high-dimensional problems related to physiological monitoring using latent variable models.","First, we present a novel deep state-space model to generate electrical waveforms of the heart using optically obtained signals as inputs.","This can bring about clinical diagnoses of heart disease via simple assessment through wearable devices.","Second, we present a brain signal modeling scheme that combines the strengths of probabilistic graphical models and deep adversarial learning.","The structured representations can provide interpretability and encode inductive biases to reduce the data complexity of neural oscillations.","The efficacy of the learned representations is further studied in epilepsy seizure detection formulated as an unsupervised learning problem.","Third, we propose a framework for the joint modeling of physiological measures and behavior.","Existing methods to combine multiple sources of brain data provided are limited.","Direct analysis of the relationship between different types of physiological measures usually does not involve behavioral data.","Our method can identify the unique and shared contributions of brain regions to behavior and can be used to discover new functions of brain regions.","The success of these innovative computational methods would allow the translation of biomarker findings across species and provide insight into neurocognitive analysis in numerous biological studies and clinical diagnoses, as well as emerging consumer applications."],"url":"http://arxiv.org/abs/2405.19277v1"}
{"created":"2024-05-29 17:03:31","title":"Mitigating Disparate Impact of Differential Privacy in Federated Learning through Robust Clustering","abstract":"Federated Learning (FL) is a decentralized machine learning (ML) approach that keeps data localized and often incorporates Differential Privacy (DP) to enhance privacy guarantees. Similar to previous work on DP in ML, we observed that differentially private federated learning (DPFL) introduces performance disparities, particularly affecting minority groups. Recent work has attempted to address performance fairness in vanilla FL through clustering, but this method remains sensitive and prone to errors, which are further exacerbated by the DP noise in DPFL. To fill this gap, in this paper, we propose a novel clustered DPFL algorithm designed to effectively identify clients' clusters in highly heterogeneous settings while maintaining high accuracy with DP guarantees. To this end, we propose to cluster clients based on both their model updates and training loss values. Our proposed approach also addresses the server's uncertainties in clustering clients' model updates by employing larger batch sizes along with Gaussian Mixture Model (GMM) to alleviate the impact of noise and potential clustering errors, especially in privacy-sensitive scenarios. We provide theoretical analysis of the effectiveness of our proposed approach. We also extensively evaluate our approach across diverse data distributions and privacy budgets and show its effectiveness in mitigating the disparate impact of DP in FL settings with a small computational cost.","sentences":["Federated Learning (FL) is a decentralized machine learning (ML) approach that keeps data localized and often incorporates Differential Privacy (DP) to enhance privacy guarantees.","Similar to previous work on DP in ML, we observed that differentially private federated learning (DPFL) introduces performance disparities, particularly affecting minority groups.","Recent work has attempted to address performance fairness in vanilla FL through clustering, but this method remains sensitive and prone to errors, which are further exacerbated by the DP noise in DPFL.","To fill this gap, in this paper, we propose a novel clustered DPFL algorithm designed to effectively identify clients' clusters in highly heterogeneous settings while maintaining high accuracy with DP guarantees.","To this end, we propose to cluster clients based on both their model updates and training loss values.","Our proposed approach also addresses the server's uncertainties in clustering clients' model updates by employing larger batch sizes along with Gaussian Mixture Model (GMM) to alleviate the impact of noise and potential clustering errors, especially in privacy-sensitive scenarios.","We provide theoretical analysis of the effectiveness of our proposed approach.","We also extensively evaluate our approach across diverse data distributions and privacy budgets and show its effectiveness in mitigating the disparate impact of DP in FL settings with a small computational cost."],"url":"http://arxiv.org/abs/2405.19272v1"}
{"created":"2024-05-29 16:59:38","title":"PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications","abstract":"Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce. Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures. To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline. In the continuous pre-training phase, we introduce a hybrid instruction pre-training mechanism to mitigate the internal-injected knowledge inconsistency of LLMs for medical domain adaptation. Immediately, the full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the general medical knowledge schema into the models. After that, we devise a direct following preference optimization to enhance the generation of pediatrician-like humanistic responses. In the parameter-efficient secondary SFT phase, a mixture of universal-specific experts strategy is presented to resolve the competency conflict between medical generalist and pediatric expertise mastery. Extensive results based on the metrics, GPT-4, and doctor evaluations on distinct doctor downstream tasks show that PediatricsGPT consistently outperforms previous Chinese medical LLMs. Our model and dataset will be open-source for community development.","sentences":["Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce.","Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures.","To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnostic demands.","Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline.","In the continuous pre-training phase, we introduce a hybrid instruction pre-training mechanism to mitigate the internal-injected knowledge inconsistency of LLMs for medical domain adaptation.","Immediately, the full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the general medical knowledge schema into the models.","After that, we devise a direct following preference optimization to enhance the generation of pediatrician-like humanistic responses.","In the parameter-efficient secondary SFT phase, a mixture of universal-specific experts strategy is presented to resolve the competency conflict between medical generalist and pediatric expertise mastery.","Extensive results based on the metrics, GPT-4, and doctor evaluations on distinct doctor downstream tasks show that PediatricsGPT consistently outperforms previous Chinese medical LLMs.","Our model and dataset will be open-source for community development."],"url":"http://arxiv.org/abs/2405.19266v1"}
{"created":"2024-05-29 16:57:33","title":"AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight Tuning on Multi-source Data","abstract":"Open-source Large Language Models (LLMs) and their specialized variants, particularly Code LLMs, have recently delivered impressive performance. However, previous Code LLMs are typically fine-tuned on single-source data with limited quality and diversity, which may insufficiently elicit the potential of pre-trained Code LLMs. In this paper, we present AlchemistCoder, a series of Code LLMs with enhanced code generation and generalization capabilities fine-tuned on multi-source data. To achieve this, we pioneer to unveil inherent conflicts among the various styles and qualities in multi-source code corpora and introduce data-specific prompts with hindsight relabeling, termed AlchemistPrompts, to harmonize different data sources and instruction-response pairs. Additionally, we propose incorporating the data construction process into the fine-tuning data as code comprehension tasks, including instruction evolution, data filtering, and code review. Extensive experiments demonstrate that AlchemistCoder holds a clear lead among all models of the same size (6.7B/7B) and rivals or even surpasses larger models (15B/33B/70B), showcasing the efficacy of our method in refining instruction-following capabilities and advancing the boundaries of code intelligence.","sentences":["Open-source Large Language Models (LLMs) and their specialized variants, particularly Code LLMs, have recently delivered impressive performance.","However, previous Code LLMs are typically fine-tuned on single-source data with limited quality and diversity, which may insufficiently elicit the potential of pre-trained Code LLMs.","In this paper, we present AlchemistCoder, a series of Code LLMs with enhanced code generation and generalization capabilities fine-tuned on multi-source data.","To achieve this, we pioneer to unveil inherent conflicts among the various styles and qualities in multi-source code corpora and introduce data-specific prompts with hindsight relabeling, termed AlchemistPrompts, to harmonize different data sources and instruction-response pairs.","Additionally, we propose incorporating the data construction process into the fine-tuning data as code comprehension tasks, including instruction evolution, data filtering, and code review.","Extensive experiments demonstrate that AlchemistCoder holds a clear lead among all models of the same size (6.7B/7B) and rivals or even surpasses larger models (15B/33B/70B), showcasing the efficacy of our method in refining instruction-following capabilities and advancing the boundaries of code intelligence."],"url":"http://arxiv.org/abs/2405.19265v1"}
{"created":"2024-05-29 16:47:38","title":"A Privacy-Preserving Graph Encryption Scheme Based on Oblivious RAM","abstract":"Graph encryption schemes play a crucial role in facilitating secure queries on encrypted graphs hosted on untrusted servers. With applications spanning navigation systems, network topology, and social networks, the need to safeguard sensitive data becomes paramount. Existing graph encryption methods, however, exhibit vulnerabilities by inadvertently revealing aspects of the graph structure and query patterns, posing threats to security and privacy. In response, we propose a novel graph encryption scheme designed to mitigate access pattern and query pattern leakage through the integration of oblivious RAM and trusted execution environment techniques, exemplified by a Trusted Execution Environment (TEE). Our solution establishes two key security objectives: (1) ensuring that adversaries, when presented with an encrypted graph, remain oblivious to any information regarding the underlying graph, and (2) achieving query indistinguishability by concealing access patterns. Additionally, we conducted experimentation to evaluate the efficiency of the proposed schemes when dealing with real-world location navigation services.","sentences":["Graph encryption schemes play a crucial role in facilitating secure queries on encrypted graphs hosted on untrusted servers.","With applications spanning navigation systems, network topology, and social networks, the need to safeguard sensitive data becomes paramount.","Existing graph encryption methods, however, exhibit vulnerabilities by inadvertently revealing aspects of the graph structure and query patterns, posing threats to security and privacy.","In response, we propose a novel graph encryption scheme designed to mitigate access pattern and query pattern leakage through the integration of oblivious RAM and trusted execution environment techniques, exemplified by a Trusted Execution Environment (TEE).","Our solution establishes two key security objectives: (1) ensuring that adversaries, when presented with an encrypted graph, remain oblivious to any information regarding the underlying graph, and (2) achieving query indistinguishability by concealing access patterns.","Additionally, we conducted experimentation to evaluate the efficiency of the proposed schemes when dealing with real-world location navigation services."],"url":"http://arxiv.org/abs/2405.19259v1"}
{"created":"2024-05-29 16:44:09","title":"Hybrid-Parallel: Achieving High Performance and Energy Efficient Distributed Inference on Robots","abstract":"The rapid advancements in machine learning techniques have led to significant achievements in various real-world robotic tasks. These tasks heavily rely on fast and energy-efficient inference of deep neural network (DNN) models when deployed on robots. To enhance inference performance, distributed inference has emerged as a promising approach, parallelizing inference across multiple powerful GPU devices in modern data centers using techniques such as data parallelism, tensor parallelism, and pipeline parallelism. However, when deployed on real-world robots, existing parallel methods fail to provide low inference latency and meet the energy requirements due to the limited bandwidth of robotic IoT. We present Hybrid-Parallel, a high-performance distributed inference system optimized for robotic IoT. Hybrid-Parallel employs a fine-grained approach to parallelize inference at the granularity of local operators within DNN layers (i.e., operators that can be computed independently with the partial input, such as the convolution kernel in the convolution layer). By doing so, Hybrid-Parallel enables different operators of different layers to be computed and transmitted concurrently, and overlap the computation and transmission phases within the same inference task. The evaluation demonstrate that Hybrid-Parallel reduces inference time by 14.9% ~41.1% and energy consumption per inference by up to 35.3% compared to the state-of-the-art baselines.","sentences":["The rapid advancements in machine learning techniques have led to significant achievements in various real-world robotic tasks.","These tasks heavily rely on fast and energy-efficient inference of deep neural network (DNN) models when deployed on robots.","To enhance inference performance, distributed inference has emerged as a promising approach, parallelizing inference across multiple powerful GPU devices in modern data centers using techniques such as data parallelism, tensor parallelism, and pipeline parallelism.","However, when deployed on real-world robots, existing parallel methods fail to provide low inference latency and meet the energy requirements due to the limited bandwidth of robotic IoT. We present Hybrid-Parallel, a high-performance distributed inference system optimized for robotic IoT. Hybrid-Parallel employs a fine-grained approach to parallelize inference at the granularity of local operators within DNN layers (i.e., operators that can be computed independently with the partial input, such as the convolution kernel in the convolution layer).","By doing so, Hybrid-Parallel enables different operators of different layers to be computed and transmitted concurrently, and overlap the computation and transmission phases within the same inference task.","The evaluation demonstrate that Hybrid-Parallel reduces inference time by 14.9% ~41.1% and energy consumption per inference by up to 35.3% compared to the state-of-the-art baselines."],"url":"http://arxiv.org/abs/2405.19257v1"}
{"created":"2024-05-29 16:41:42","title":"Weak Generative Sampler to Efficiently Sample Invariant Distribution of Stochastic Differential Equation","abstract":"Sampling invariant distributions from an Ito diffusion process presents a significant challenge in stochastic simulation. Traditional numerical solvers for stochastic differential equations require both a fine step size and a lengthy simulation period, resulting in both biased and correlated samples. Current deep learning-based method solves the stationary Fokker--Planck equation to determine the invariant probability density function in form of deep neural networks, but they generally do not directly address the problem of sampling from the computed density function. In this work, we introduce a framework that employs a weak generative sampler (WGS) to directly generate independent and identically distributed (iid) samples induced by a transformation map derived from the stationary Fokker--Planck equation. Our proposed loss function is based on the weak form of the Fokker--Planck equation, integrating normalizing flows to characterize the invariant distribution and facilitate sample generation from the base distribution. Our randomized test function circumvents the need for mini-max optimization in the traditional weak formulation. Distinct from conventional generative models, our method neither necessitates the computationally intensive calculation of the Jacobian determinant nor the invertibility of the transformation map. A crucial component of our framework is the adaptively chosen family of test functions in the form of Gaussian kernel functions with centres selected from the generated data samples. Experimental results on several benchmark examples demonstrate the effectiveness of our method, which offers both low computational costs and excellent capability in exploring multiple metastable states.","sentences":["Sampling invariant distributions from an Ito diffusion process presents a significant challenge in stochastic simulation.","Traditional numerical solvers for stochastic differential equations require both a fine step size and a lengthy simulation period, resulting in both biased and correlated samples.","Current deep learning-based method solves the stationary Fokker--Planck equation to determine the invariant probability density function in form of deep neural networks, but they generally do not directly address the problem of sampling from the computed density function.","In this work, we introduce a framework that employs a weak generative sampler (WGS) to directly generate independent and identically distributed (iid) samples induced by a transformation map derived from the stationary Fokker--Planck equation.","Our proposed loss function is based on the weak form of the Fokker--Planck equation, integrating normalizing flows to characterize the invariant distribution and facilitate sample generation from the base distribution.","Our randomized test function circumvents the need for mini-max optimization in the traditional weak formulation.","Distinct from conventional generative models, our method neither necessitates the computationally intensive calculation of the Jacobian determinant nor the invertibility of the transformation map.","A crucial component of our framework is the adaptively chosen family of test functions in the form of Gaussian kernel functions with centres selected from the generated data samples.","Experimental results on several benchmark examples demonstrate the effectiveness of our method, which offers both low computational costs and excellent capability in exploring multiple metastable states."],"url":"http://arxiv.org/abs/2405.19256v1"}
{"created":"2024-05-29 16:40:31","title":"Towards Next-Generation Urban Decision Support Systems through AI-Powered Generation of Scientific Ontology using Large Language Models -- A Case in Optimizing Intermodal Freight Transportation","abstract":"The incorporation of Artificial Intelligence (AI) models into various optimization systems is on the rise. Yet, addressing complex urban and environmental management problems normally requires in-depth domain science and informatics expertise. This expertise is essential for deriving data and simulation-driven for informed decision support. In this context, we investigate the potential of leveraging the pre-trained Large Language Models (LLMs). By adopting ChatGPT API as the reasoning core, we outline an integrated workflow that encompasses natural language processing, methontology-based prompt tuning, and transformers. This workflow automates the creation of scenario-based ontology using existing research articles and technical manuals of urban datasets and simulations. The outcomes of our methodology are knowledge graphs in widely adopted ontology languages (e.g., OWL, RDF, SPARQL). These facilitate the development of urban decision support systems by enhancing the data and metadata modeling, the integration of complex datasets, the coupling of multi-domain simulation models, and the formulation of decision-making metrics and workflow. The feasibility of our methodology is evaluated through a comparative analysis that juxtaposes our AI-generated ontology with the well-known Pizza Ontology employed in tutorials for popular ontology software (e.g., prot\\'eg\\'e). We close with a real-world case study of optimizing the complex urban system of multi-modal freight transportation by generating anthologies of various domain data and simulations to support informed decision-making.","sentences":["The incorporation of Artificial Intelligence (AI) models into various optimization systems is on the rise.","Yet, addressing complex urban and environmental management problems normally requires in-depth domain science and informatics expertise.","This expertise is essential for deriving data and simulation-driven for informed decision support.","In this context, we investigate the potential of leveraging the pre-trained Large Language Models (LLMs).","By adopting ChatGPT API as the reasoning core, we outline an integrated workflow that encompasses natural language processing, methontology-based prompt tuning, and transformers.","This workflow automates the creation of scenario-based ontology using existing research articles and technical manuals of urban datasets and simulations.","The outcomes of our methodology are knowledge graphs in widely adopted ontology languages (e.g., OWL, RDF, SPARQL).","These facilitate the development of urban decision support systems by enhancing the data and metadata modeling, the integration of complex datasets, the coupling of multi-domain simulation models, and the formulation of decision-making metrics and workflow.","The feasibility of our methodology is evaluated through a comparative analysis that juxtaposes our AI-generated ontology with the well-known Pizza Ontology employed in tutorials for popular ontology software (e.g., prot\\'eg\\'e).","We close with a real-world case study of optimizing the complex urban system of multi-modal freight transportation by generating anthologies of various domain data and simulations to support informed decision-making."],"url":"http://arxiv.org/abs/2405.19255v1"}
{"created":"2024-05-29 16:33:50","title":"Kotlin ML Pack: Technical Report","abstract":"In this technical report, we present three novel datasets of Kotlin code: KStack, KStack-clean, and KExercises. We also describe the results of fine-tuning CodeLlama and DeepSeek models on this data. Additionally, we present a version of the HumanEval benchmark rewritten by human experts into Kotlin - both the solutions and the tests. Our results demonstrate that small, high-quality datasets (KStack-clean and KExercises) can significantly improve model performance on code generation tasks, achieving up to a 16-point increase in pass rate on the HumanEval benchmark. Lastly, we discuss potential future work in the field of improving language modeling for Kotlin, including the use of static analysis tools in the learning process and the introduction of more intricate and realistic benchmarks.","sentences":["In this technical report, we present three novel datasets of Kotlin code: KStack, KStack-clean, and KExercises.","We also describe the results of fine-tuning CodeLlama and DeepSeek models on this data.","Additionally, we present a version of the HumanEval benchmark rewritten by human experts into Kotlin - both the solutions and the tests.","Our results demonstrate that small, high-quality datasets (KStack-clean and KExercises) can significantly improve model performance on code generation tasks, achieving up to a 16-point increase in pass rate on the HumanEval benchmark.","Lastly, we discuss potential future work in the field of improving language modeling for Kotlin, including the use of static analysis tools in the learning process and the introduction of more intricate and realistic benchmarks."],"url":"http://arxiv.org/abs/2405.19250v1"}
{"created":"2024-05-29 16:24:38","title":"Challenge-Device-Synthesis: A multi-disciplinary approach for the development of social innovation competences for students of Artificial Intelligence","abstract":"The advent of Artificial Intelligence is expected to imply profound changes in the short-term. It is therefore imperative for Academia, and particularly for the Computer Science scope, to develop cross-disciplinary tools that bond AI developments to their social dimension. To this aim, we introduce the Challenge-Device-Synthesis methodology (CDS), in which a specific challenge is presented to the students of AI, who are required to develop a device as a solution for the challenge. The device becomes the object of study for the different dimensions of social transformation, and the conclusions addressed by the students during the discussion around the device are presented in a synthesis piece in the shape of a 10-page scientific paper. The latter is evaluated taking into account both the depth of analysis and the level to which it genuinely reflects the social transformations associated with the proposed AI-based device. We provide data obtained during the pilot for the implementation phase of CDS within the subject of Social Innovation, a 6-ECTS subject from the 6th semester of the Degree of Artificial Intelligence, UAB-Barcelona. We provide details on temporalisation, task distribution, methodological tools used and assessment delivery procedure, as well as qualitative analysis of the results obtained.","sentences":["The advent of Artificial Intelligence is expected to imply profound changes in the short-term.","It is therefore imperative for Academia, and particularly for the Computer Science scope, to develop cross-disciplinary tools that bond AI developments to their social dimension.","To this aim, we introduce the Challenge-Device-Synthesis methodology (CDS), in which a specific challenge is presented to the students of AI, who are required to develop a device as a solution for the challenge.","The device becomes the object of study for the different dimensions of social transformation, and the conclusions addressed by the students during the discussion around the device are presented in a synthesis piece in the shape of a 10-page scientific paper.","The latter is evaluated taking into account both the depth of analysis and the level to which it genuinely reflects the social transformations associated with the proposed AI-based device.","We provide data obtained during the pilot for the implementation phase of CDS within the subject of Social Innovation, a 6-ECTS subject from the 6th semester of the Degree of Artificial Intelligence, UAB-Barcelona.","We provide details on temporalisation, task distribution, methodological tools used and assessment delivery procedure, as well as qualitative analysis of the results obtained."],"url":"http://arxiv.org/abs/2405.19243v1"}
{"created":"2024-05-29 16:19:37","title":"ConceptPrune: Concept Editing in Diffusion Models via Skilled Neuron Pruning","abstract":"While large-scale text-to-image diffusion models have demonstrated impressive image-generation capabilities, there are significant concerns about their potential misuse for generating unsafe content, violating copyright, and perpetuating societal biases. Recently, the text-to-image generation community has begun addressing these concerns by editing or unlearning undesired concepts from pre-trained models. However, these methods often involve data-intensive and inefficient fine-tuning or utilize various forms of token remapping, rendering them susceptible to adversarial jailbreaks. In this paper, we present a simple and effective training-free approach, ConceptPrune, wherein we first identify critical regions within pre-trained models responsible for generating undesirable concepts, thereby facilitating straightforward concept unlearning via weight pruning. Experiments across a range of concepts including artistic styles, nudity, object erasure, and gender debiasing demonstrate that target concepts can be efficiently erased by pruning a tiny fraction, approximately 0.12% of total weights, enabling multi-concept erasure and robustness against various white-box and black-box adversarial attacks.","sentences":["While large-scale text-to-image diffusion models have demonstrated impressive image-generation capabilities, there are significant concerns about their potential misuse for generating unsafe content, violating copyright, and perpetuating societal biases.","Recently, the text-to-image generation community has begun addressing these concerns by editing or unlearning undesired concepts from pre-trained models.","However, these methods often involve data-intensive and inefficient fine-tuning or utilize various forms of token remapping, rendering them susceptible to adversarial jailbreaks.","In this paper, we present a simple and effective training-free approach, ConceptPrune, wherein we first identify critical regions within pre-trained models responsible for generating undesirable concepts, thereby facilitating straightforward concept unlearning via weight pruning.","Experiments across a range of concepts including artistic styles, nudity, object erasure, and gender debiasing demonstrate that target concepts can be efficiently erased by pruning a tiny fraction, approximately 0.12% of total weights, enabling multi-concept erasure and robustness against various white-box and black-box adversarial attacks."],"url":"http://arxiv.org/abs/2405.19237v1"}
{"created":"2024-05-29 16:13:54","title":"Forward-Backward Knowledge Distillation for Continual Clustering","abstract":"Unsupervised Continual Learning (UCL) is a burgeoning field in machine learning, focusing on enabling neural networks to sequentially learn tasks without explicit label information. Catastrophic Forgetting (CF), where models forget previously learned tasks upon learning new ones, poses a significant challenge in continual learning, especially in UCL, where labeled information of data is not accessible. CF mitigation strategies, such as knowledge distillation and replay buffers, often face memory inefficiency and privacy issues. Although current research in UCL has endeavored to refine data representations and address CF in streaming data contexts, there is a noticeable lack of algorithms specifically designed for unsupervised clustering. To fill this gap, in this paper, we introduce the concept of Unsupervised Continual Clustering (UCC). We propose Forward-Backward Knowledge Distillation for unsupervised Continual Clustering (FBCC) to counteract CF within the context of UCC. FBCC employs a single continual learner (the ``teacher'') with a cluster projector, along with multiple student models, to address the CF issue. The proposed method consists of two phases: Forward Knowledge Distillation, where the teacher learns new clusters while retaining knowledge from previous tasks with guidance from specialized student models, and Backward Knowledge Distillation, where a student model mimics the teacher's behavior to retain task-specific knowledge, aiding the teacher in subsequent tasks. FBCC marks a pioneering approach to UCC, demonstrating enhanced performance and memory efficiency in clustering across various tasks, outperforming the application of clustering algorithms to the latent space of state-of-the-art UCL algorithms.","sentences":["Unsupervised Continual Learning (UCL) is a burgeoning field in machine learning, focusing on enabling neural networks to sequentially learn tasks without explicit label information.","Catastrophic Forgetting (CF), where models forget previously learned tasks upon learning new ones, poses a significant challenge in continual learning, especially in UCL, where labeled information of data is not accessible.","CF mitigation strategies, such as knowledge distillation and replay buffers, often face memory inefficiency and privacy issues.","Although current research in UCL has endeavored to refine data representations and address CF in streaming data contexts, there is a noticeable lack of algorithms specifically designed for unsupervised clustering.","To fill this gap, in this paper, we introduce the concept of Unsupervised Continual Clustering (UCC).","We propose Forward-Backward Knowledge Distillation for unsupervised Continual Clustering (FBCC) to counteract CF within the context of UCC.","FBCC employs a single continual learner (the ``teacher'') with a cluster projector, along with multiple student models, to address the CF issue.","The proposed method consists of two phases: Forward Knowledge Distillation, where the teacher learns new clusters while retaining knowledge from previous tasks with guidance from specialized student models, and Backward Knowledge Distillation, where a student model mimics the teacher's behavior to retain task-specific knowledge, aiding the teacher in subsequent tasks.","FBCC marks a pioneering approach to UCC, demonstrating enhanced performance and memory efficiency in clustering across various tasks, outperforming the application of clustering algorithms to the latent space of state-of-the-art UCL algorithms."],"url":"http://arxiv.org/abs/2405.19234v1"}
{"created":"2024-05-29 16:05:57","title":"Synthetic Potential Outcomes for Mixtures of Treatment Effects","abstract":"Modern data analysis frequently relies on the use of large datasets, often constructed as amalgamations of diverse populations or data-sources. Heterogeneity across these smaller datasets constitutes two major challenges for causal inference: (1) the source of each sample can introduce latent confounding between treatment and effect, and (2) diverse populations may respond differently to the same treatment, giving rise to heterogeneous treatment effects (HTEs). The issues of latent confounding and HTEs have been studied separately but not in conjunction. In particular, previous works only report the conditional average treatment effect (CATE) among similar individuals (with respect to the measured covariates). CATEs cannot resolve mixtures of potential treatment effects driven by latent heterogeneity, which we call mixtures of treatment effects (MTEs). Inspired by method of moment approaches to mixture models, we propose \"synthetic potential outcomes\" (SPOs). Our new approach deconfounds heterogeneity while also guaranteeing the identifiability of MTEs. This technique bypasses full recovery of a mixture, which significantly simplifies its requirements for identifiability. We demonstrate the efficacy of SPOs on synthetic data.","sentences":["Modern data analysis frequently relies on the use of large datasets, often constructed as amalgamations of diverse populations or data-sources.","Heterogeneity across these smaller datasets constitutes two major challenges for causal inference: (1) the source of each sample can introduce latent confounding between treatment and effect, and (2) diverse populations may respond differently to the same treatment, giving rise to heterogeneous treatment effects (HTEs).","The issues of latent confounding and HTEs have been studied separately but not in conjunction.","In particular, previous works only report the conditional average treatment effect (CATE) among similar individuals (with respect to the measured covariates).","CATEs cannot resolve mixtures of potential treatment effects driven by latent heterogeneity, which we call mixtures of treatment effects (MTEs).","Inspired by method of moment approaches to mixture models, we propose \"synthetic potential outcomes\" (SPOs).","Our new approach deconfounds heterogeneity while also guaranteeing the identifiability of MTEs.","This technique bypasses full recovery of a mixture, which significantly simplifies its requirements for identifiability.","We demonstrate the efficacy of SPOs on synthetic data."],"url":"http://arxiv.org/abs/2405.19225v1"}
{"created":"2024-05-29 16:00:46","title":"WRDScore: New Metric for Evaluation of Natural Language Generation Models","abstract":"The problem of natural language generation, and, more specifically, method name prediction, faces significant difficulties when proposed models need to be evaluated on test data. Such a metric would need to consider the versatility with which a single method can be named, with respect to both semantics and syntax. Measuring the direct overlap between the predicted and reference (true) sequences will not be able to capture these subtleties. Other existing embedding based metrics either do not measure precision and recall or impose strict unrealistic assumptions on both sequences. To address these issues, we propose a new metric that, on the one hand, is very simple and lightweight, and, on the other hand, is able to calculate precision and recall without resorting to any assumptions while obtaining good performance with respect to the human judgement.","sentences":["The problem of natural language generation, and, more specifically, method name prediction, faces significant difficulties when proposed models need to be evaluated on test data.","Such a metric would need to consider the versatility with which a single method can be named, with respect to both semantics and syntax.","Measuring the direct overlap between the predicted and reference (true) sequences will not be able to capture these subtleties.","Other existing embedding based metrics either do not measure precision and recall or impose strict unrealistic assumptions on both sequences.","To address these issues, we propose a new metric that, on the one hand, is very simple and lightweight, and, on the other hand, is able to calculate precision and recall without resorting to any assumptions while obtaining good performance with respect to the human judgement."],"url":"http://arxiv.org/abs/2405.19220v1"}
{"created":"2024-05-29 16:00:19","title":"LoByITFL: Low Communication Secure and Private Federated Learning","abstract":"Federated Learning (FL) faces several challenges, such as the privacy of the clients data and security against Byzantine clients. Existing works treating privacy and security jointly make sacrifices on the privacy guarantee. In this work, we introduce LoByITFL, the first communication-efficient Information-Theoretic (IT) private and secure FL scheme that makes no sacrifices on the privacy guarantees while ensuring security against Byzantine adversaries. The key ingredients are a small and representative dataset available to the federator, a careful transformation of the FLTrust algorithm and the use of a trusted third party only in a one-time preprocessing phase before the start of the learning algorithm. We provide theoretical guarantees on privacy and Byzantine-resilience, and provide convergence guarantee and experimental results validating our theoretical findings.","sentences":["Federated Learning (FL) faces several challenges, such as the privacy of the clients data and security against Byzantine clients.","Existing works treating privacy and security jointly make sacrifices on the privacy guarantee.","In this work, we introduce LoByITFL, the first communication-efficient Information-Theoretic (IT) private and secure FL scheme that makes no sacrifices on the privacy guarantees while ensuring security against Byzantine adversaries.","The key ingredients are a small and representative dataset available to the federator, a careful transformation of the FLTrust algorithm and the use of a trusted third party only in a one-time preprocessing phase before the start of the learning algorithm.","We provide theoretical guarantees on privacy and Byzantine-resilience, and provide convergence guarantee and experimental results validating our theoretical findings."],"url":"http://arxiv.org/abs/2405.19217v1"}
{"created":"2024-05-29 15:54:03","title":"Partial Information Decomposition for Data Interpretability and Feature Selection","abstract":"In this paper, we introduce Partial Information Decomposition of Features (PIDF), a new paradigm for simultaneous data interpretability and feature selection. Contrary to traditional methods that assign a single importance value, our approach is based on three metrics per feature: the mutual information shared with the target variable, the feature's contribution to synergistic information, and the amount of this information that is redundant. In particular, we develop a novel procedure based on these three metrics, which reveals not only how features are correlated with the target but also the additional and overlapping information provided by considering them in combination with other features. We extensively evaluate PIDF using both synthetic and real-world data, demonstrating its potential applications and effectiveness, by considering case studies from genetics and neuroscience.","sentences":["In this paper, we introduce Partial Information Decomposition of Features (PIDF), a new paradigm for simultaneous data interpretability and feature selection.","Contrary to traditional methods that assign a single importance value, our approach is based on three metrics per feature: the mutual information shared with the target variable, the feature's contribution to synergistic information, and the amount of this information that is redundant.","In particular, we develop a novel procedure based on these three metrics, which reveals not only how features are correlated with the target but also the additional and overlapping information provided by considering them in combination with other features.","We extensively evaluate PIDF using both synthetic and real-world data, demonstrating its potential applications and effectiveness, by considering case studies from genetics and neuroscience."],"url":"http://arxiv.org/abs/2405.19212v1"}
{"created":"2024-05-29 15:53:23","title":"Gone but Not Forgotten: Improved Benchmarks for Machine Unlearning","abstract":"Machine learning models are vulnerable to adversarial attacks, including attacks that leak information about the model's training data. There has recently been an increase in interest about how to best address privacy concerns, especially in the presence of data-removal requests. Machine unlearning algorithms aim to efficiently update trained models to comply with data deletion requests while maintaining performance and without having to resort to retraining the model from scratch, a costly endeavor. Several algorithms in the machine unlearning literature demonstrate some level of privacy gains, but they are often evaluated only on rudimentary membership inference attacks, which do not represent realistic threats. In this paper we describe and propose alternative evaluation methods for three key shortcomings in the current evaluation of unlearning algorithms. We show the utility of our alternative evaluations via a series of experiments of state-of-the-art unlearning algorithms on different computer vision datasets, presenting a more detailed picture of the state of the field.","sentences":["Machine learning models are vulnerable to adversarial attacks, including attacks that leak information about the model's training data.","There has recently been an increase in interest about how to best address privacy concerns, especially in the presence of data-removal requests.","Machine unlearning algorithms aim to efficiently update trained models to comply with data deletion requests while maintaining performance and without having to resort to retraining the model from scratch, a costly endeavor.","Several algorithms in the machine unlearning literature demonstrate some level of privacy gains, but they are often evaluated only on rudimentary membership inference attacks, which do not represent realistic threats.","In this paper we describe and propose alternative evaluation methods for three key shortcomings in the current evaluation of unlearning algorithms.","We show the utility of our alternative evaluations via a series of experiments of state-of-the-art unlearning algorithms on different computer vision datasets, presenting a more detailed picture of the state of the field."],"url":"http://arxiv.org/abs/2405.19211v1"}
{"created":"2024-05-29 15:51:40","title":"Gradient Guided Hypotheses: A unified solution to enable machine learning models on scarce and noisy data regimes","abstract":"Ensuring high-quality data is paramount for maximizing the performance of machine learning models and business intelligence systems. However, challenges in data quality, including noise in data capture, missing records, limited data production, and confounding variables, significantly constrain the potential performance of these systems. In this study, we propose an architecture-agnostic algorithm, Gradient Guided Hypotheses (GGH), designed to address these challenges. GGH analyses gradients from hypotheses as a proxy of distinct and possibly contradictory patterns in the data. This framework entails an additional step in machine learning training, where gradients can be included or excluded from backpropagation. In this manner, missing and noisy data are addressed through a unified solution that perceives both challenges as facets of the same overarching issue: the propagation of erroneous information. Experimental validation of GGH is conducted using real-world open-source datasets, where records with missing rates of up to 98.5% are simulated. Comparative analysis with state-of-the-art imputation methods demonstrates a substantial improvement in model performance achieved by GGH. Specifically in very high scarcity regimes, GGH was found to be the only viable solution. Additionally, GGH's noise detection capabilities are showcased by introducing simulated noise into the datasets and observing enhanced model performance after filtering out the noisy data. This study presents GGH as a promising solution for improving data quality and model performance in various applications.","sentences":["Ensuring high-quality data is paramount for maximizing the performance of machine learning models and business intelligence systems.","However, challenges in data quality, including noise in data capture, missing records, limited data production, and confounding variables, significantly constrain the potential performance of these systems.","In this study, we propose an architecture-agnostic algorithm, Gradient Guided Hypotheses (GGH), designed to address these challenges.","GGH analyses gradients from hypotheses as a proxy of distinct and possibly contradictory patterns in the data.","This framework entails an additional step in machine learning training, where gradients can be included or excluded from backpropagation.","In this manner, missing and noisy data are addressed through a unified solution that perceives both challenges as facets of the same overarching issue: the propagation of erroneous information.","Experimental validation of GGH is conducted using real-world open-source datasets, where records with missing rates of up to 98.5% are simulated.","Comparative analysis with state-of-the-art imputation methods demonstrates a substantial improvement in model performance achieved by GGH.","Specifically in very high scarcity regimes, GGH was found to be the only viable solution.","Additionally, GGH's noise detection capabilities are showcased by introducing simulated noise into the datasets and observing enhanced model performance after filtering out the noisy data.","This study presents GGH as a promising solution for improving data quality and model performance in various applications."],"url":"http://arxiv.org/abs/2405.19210v1"}
{"created":"2024-05-29 15:42:10","title":"Vulnerable Road User Detection and Safety Enhancement: A Comprehensive Survey","abstract":"Traffic incidents involving vulnerable road users (VRUs) constitute a significant proportion of global road accidents. Advances in traffic communication ecosystems, coupled with sophisticated signal processing and machine learning techniques, have facilitated the utilization of data from diverse sensors. Despite these advancements and the availability of extensive datasets, substantial progress is required to mitigate traffic casualties. This paper provides a comprehensive survey of state-of-the-art technologies and methodologies to enhance the safety of VRUs. The study delves into the communication networks between vehicles and VRUs, emphasizing the integration of advanced sensors and the availability of relevant datasets. It explores preprocessing techniques and data fusion methods to enhance sensor data quality. Furthermore, our study assesses critical simulation environments essential for developing and testing VRU safety systems. Our research also highlights recent advances in VRU detection and classification algorithms, addressing challenges such as variable environmental conditions. Additionally, we cover cutting-edge research in predicting VRU intentions and behaviors, which is crucial for proactive collision avoidance strategies. Through this survey, we aim to provide a comprehensive understanding of the current landscape of VRU safety technologies, identifying areas of progress and areas needing further research and development.","sentences":["Traffic incidents involving vulnerable road users (VRUs) constitute a significant proportion of global road accidents.","Advances in traffic communication ecosystems, coupled with sophisticated signal processing and machine learning techniques, have facilitated the utilization of data from diverse sensors.","Despite these advancements and the availability of extensive datasets, substantial progress is required to mitigate traffic casualties.","This paper provides a comprehensive survey of state-of-the-art technologies and methodologies to enhance the safety of VRUs.","The study delves into the communication networks between vehicles and VRUs, emphasizing the integration of advanced sensors and the availability of relevant datasets.","It explores preprocessing techniques and data fusion methods to enhance sensor data quality.","Furthermore, our study assesses critical simulation environments essential for developing and testing VRU safety systems.","Our research also highlights recent advances in VRU detection and classification algorithms, addressing challenges such as variable environmental conditions.","Additionally, we cover cutting-edge research in predicting VRU intentions and behaviors, which is crucial for proactive collision avoidance strategies.","Through this survey, we aim to provide a comprehensive understanding of the current landscape of VRU safety technologies, identifying areas of progress and areas needing further research and development."],"url":"http://arxiv.org/abs/2405.19202v1"}
{"created":"2024-05-29 15:41:53","title":"Going beyond compositional generalization, DDPMs can produce zero-shot interpolation","abstract":"Denoising Diffusion Probabilistic Models (DDPMs) exhibit remarkable capabilities in image generation, with studies suggesting that they can generalize by composing latent factors learned from the training data. In this work, we go further and study DDPMs trained on strictly separate subsets of the data distribution with large gaps on the support of the latent factors. We show that such a model can effectively generate images in the unexplored, intermediate regions of the distribution. For instance, when trained on clearly smiling and non-smiling faces, we demonstrate a sampling procedure which can generate slightly smiling faces without reference images (zero-shot interpolation). We replicate these findings for other attributes as well as other datasets. $\\href{https://github.com/jdeschena/ddpm-zero-shot-interpolation}{\\text{Our code is available on GitHub.}}$","sentences":["Denoising Diffusion Probabilistic Models (DDPMs) exhibit remarkable capabilities in image generation, with studies suggesting that they can generalize by composing latent factors learned from the training data.","In this work, we go further and study DDPMs trained on strictly separate subsets of the data distribution with large gaps on the support of the latent factors.","We show that such a model can effectively generate images in the unexplored, intermediate regions of the distribution.","For instance, when trained on clearly smiling and non-smiling faces, we demonstrate a sampling procedure which can generate slightly smiling faces without reference images (zero-shot interpolation).","We replicate these findings for other attributes as well as other datasets.","$\\href{https://github.com/jdeschena/ddpm-zero-shot-interpolation}{\\text{Our code is available on GitHub.}}$"],"url":"http://arxiv.org/abs/2405.19201v1"}
{"created":"2024-05-29 15:29:46","title":"Diffusion-based Dynamics Models for Long-Horizon Rollout in Offline Reinforcement Learning","abstract":"With the great success of diffusion models (DMs) in generating realistic synthetic vision data, many researchers have investigated their potential in decision-making and control. Most of these works utilized DMs to sample directly from the trajectory space, where DMs can be viewed as a combination of dynamics models and policies. In this work, we explore how to decouple DMs' ability as dynamics models in fully offline settings, allowing the learning policy to roll out trajectories. As DMs learn the data distribution from the dataset, their intrinsic policy is actually the behavior policy induced from the dataset, which results in a mismatch between the behavior policy and the learning policy. We propose Dynamics Diffusion, short as DyDiff, which can inject information from the learning policy to DMs iteratively. DyDiff ensures long-horizon rollout accuracy while maintaining policy consistency and can be easily deployed on model-free algorithms. We provide theoretical analysis to show the advantage of DMs on long-horizon rollout over models and demonstrate the effectiveness of DyDiff in the context of offline reinforcement learning, where the rollout dataset is provided but no online environment for interaction. Our code is at https://github.com/FineArtz/DyDiff.","sentences":["With the great success of diffusion models (DMs) in generating realistic synthetic vision data, many researchers have investigated their potential in decision-making and control.","Most of these works utilized DMs to sample directly from the trajectory space, where DMs can be viewed as a combination of dynamics models and policies.","In this work, we explore how to decouple DMs' ability as dynamics models in fully offline settings, allowing the learning policy to roll out trajectories.","As DMs learn the data distribution from the dataset, their intrinsic policy is actually the behavior policy induced from the dataset, which results in a mismatch between the behavior policy and the learning policy.","We propose Dynamics Diffusion, short as DyDiff, which can inject information from the learning policy to DMs iteratively.","DyDiff ensures long-horizon rollout accuracy while maintaining policy consistency and can be easily deployed on model-free algorithms.","We provide theoretical analysis to show the advantage of DMs on long-horizon rollout over models and demonstrate the effectiveness of DyDiff in the context of offline reinforcement learning, where the rollout dataset is provided but no online environment for interaction.","Our code is at https://github.com/FineArtz/DyDiff."],"url":"http://arxiv.org/abs/2405.19189v1"}
{"created":"2024-05-29 15:29:21","title":"Personalized Interiors at Scale: Leveraging AI for Efficient and Customizable Design Solutions","abstract":"In this paper, we introduce an innovative application of artificial intelligence in the realm of interior design through the integration of Stable Diffusion and Dreambooth models. This paper explores the potential of these advanced generative models to streamline and democratize the process of room interior generation, offering a significant departure from conventional, labor-intensive techniques. Our approach leverages the capabilities of Stable Diffusion for generating high-quality images and Dreambooth for rapid customization with minimal training data, addressing the need for efficiency and personalization in the design industry. We detail a comprehensive methodology that combines these models, providing a robust framework for the creation of tailored room interiors that reflect individual tastes and functional requirements. We presents an extensive evaluation of our method, supported by experimental results that demonstrate its effectiveness and a series of case studies that illustrate its practical application in interior design projects. Our study contributes to the ongoing discourse on the role of AI in creative fields, highlighting the benefits of leveraging generative models to enhance creativity and reshape the future of interior design.","sentences":["In this paper, we introduce an innovative application of artificial intelligence in the realm of interior design through the integration of Stable Diffusion and Dreambooth models.","This paper explores the potential of these advanced generative models to streamline and democratize the process of room interior generation, offering a significant departure from conventional, labor-intensive techniques.","Our approach leverages the capabilities of Stable Diffusion for generating high-quality images and Dreambooth for rapid customization with minimal training data, addressing the need for efficiency and personalization in the design industry.","We detail a comprehensive methodology that combines these models, providing a robust framework for the creation of tailored room interiors that reflect individual tastes and functional requirements.","We presents an extensive evaluation of our method, supported by experimental results that demonstrate its effectiveness and a series of case studies that illustrate its practical application in interior design projects.","Our study contributes to the ongoing discourse on the role of AI in creative fields, highlighting the benefits of leveraging generative models to enhance creativity and reshape the future of interior design."],"url":"http://arxiv.org/abs/2405.19188v1"}
{"created":"2024-05-29 15:28:42","title":"MetaToken: Detecting Hallucination in Image Descriptions by Meta Classification","abstract":"Large Vision Language Models (LVLMs) have shown remarkable capabilities in multimodal tasks like visual question answering or image captioning. However, inconsistencies between the visual information and the generated text, a phenomenon referred to as hallucinations, remain an unsolved problem with regard to the trustworthiness of LVLMs. To address this problem, recent works proposed to incorporate computationally costly Large (Vision) Language Models in order to detect hallucinations on a sentence- or subsentence-level. In this work, we introduce MetaToken, a lightweight binary classifier to detect hallucinations on the token-level at negligible cost. Based on a statistical analysis, we reveal key factors of hallucinations in LVLMs which have been overseen in previous works. MetaToken can be applied to any open-source LVLM without any knowledge about ground truth data providing a reliable detection of hallucinations. We evaluate our method on four state-of-the-art LVLMs demonstrating the effectiveness of our approach.","sentences":["Large Vision Language Models (LVLMs) have shown remarkable capabilities in multimodal tasks like visual question answering or image captioning.","However, inconsistencies between the visual information and the generated text, a phenomenon referred to as hallucinations, remain an unsolved problem with regard to the trustworthiness of LVLMs.","To address this problem, recent works proposed to incorporate computationally costly Large (Vision) Language Models in order to detect hallucinations on a sentence- or subsentence-level.","In this work, we introduce MetaToken, a lightweight binary classifier to detect hallucinations on the token-level at negligible cost.","Based on a statistical analysis, we reveal key factors of hallucinations in LVLMs which have been overseen in previous works.","MetaToken can be applied to any open-source LVLM without any knowledge about ground truth data providing a reliable detection of hallucinations.","We evaluate our method on four state-of-the-art LVLMs demonstrating the effectiveness of our approach."],"url":"http://arxiv.org/abs/2405.19186v1"}
{"created":"2024-05-29 15:15:52","title":"Exploring AI-based Anonymization of Industrial Image and Video Data in the Context of Feature Preservation","abstract":"With rising technologies, the protection of privacy-sensitive information is becoming increasingly important. In industry and production facilities, image or video recordings are beneficial for documentation, tracing production errors or coordinating workflows. Individuals in images or videos need to be anonymized. However, the anonymized data should be reusable for further applications. In this work, we apply the Deep Learning-based full-body anonymization framework DeepPrivacy2, which generates artificial identities, to industrial image and video data. We compare its performance with conventional anonymization techniques. Therefore, we consider the quality of identity generation, temporal consistency, and the applicability of pose estimation and action recognition.","sentences":["With rising technologies, the protection of privacy-sensitive information is becoming increasingly important.","In industry and production facilities, image or video recordings are beneficial for documentation, tracing production errors or coordinating workflows.","Individuals in images or videos need to be anonymized.","However, the anonymized data should be reusable for further applications.","In this work, we apply the Deep Learning-based full-body anonymization framework DeepPrivacy2, which generates artificial identities, to industrial image and video data.","We compare its performance with conventional anonymization techniques.","Therefore, we consider the quality of identity generation, temporal consistency, and the applicability of pose estimation and action recognition."],"url":"http://arxiv.org/abs/2405.19173v1"}
{"created":"2024-05-29 15:10:24","title":"Transformers as Neural Operators for Solutions of Differential Equations with Finite Regularity","abstract":"Neural operator learning models have emerged as very effective surrogates in data-driven methods for partial differential equations (PDEs) across different applications from computational science and engineering. Such operator learning models not only predict particular instances of a physical or biological system in real-time but also forecast classes of solutions corresponding to a distribution of initial and boundary conditions or forcing terms. % DeepONet is the first neural operator model and has been tested extensively for a broad class of solutions, including Riemann problems. Transformers have not been used in that capacity, and specifically, they have not been tested for solutions of PDEs with low regularity. %   In this work, we first establish the theoretical groundwork that transformers possess the universal approximation property as operator learning models.   We then apply transformers to forecast solutions of diverse dynamical systems with solutions of finite regularity for a plurality of initial conditions and forcing terms. In particular, we consider three examples: the Izhikevich neuron model, the tempered fractional-order Leaky Integrate-and-Fire (LIF) model, and the one-dimensional Euler equation Riemann problem. For the latter problem, we also compare with variants of DeepONet, and we find that transformers outperform DeepONet in accuracy but they are computationally more expensive.","sentences":["Neural operator learning models have emerged as very effective surrogates in data-driven methods for partial differential equations (PDEs) across different applications from computational science and engineering.","Such operator learning models not only predict particular instances of a physical or biological system in real-time but also forecast classes of solutions corresponding to a distribution of initial and boundary conditions or forcing terms.","% DeepONet is the first neural operator model and has been tested extensively for a broad class of solutions, including Riemann problems.","Transformers have not been used in that capacity, and specifically, they have not been tested for solutions of PDEs with low regularity.","%   In this work, we first establish the theoretical groundwork that transformers possess the universal approximation property as operator learning models.   ","We then apply transformers to forecast solutions of diverse dynamical systems with solutions of finite regularity for a plurality of initial conditions and forcing terms.","In particular, we consider three examples: the Izhikevich neuron model, the tempered fractional-order Leaky Integrate-and-Fire (LIF) model, and the one-dimensional Euler equation Riemann problem.","For the latter problem, we also compare with variants of DeepONet, and we find that transformers outperform DeepONet in accuracy but they are computationally more expensive."],"url":"http://arxiv.org/abs/2405.19166v1"}
{"created":"2024-05-29 15:06:10","title":"Does learning the right latent variables necessarily improve in-context learning?","abstract":"Large autoregressive models like Transformers can solve tasks through in-context learning (ICL) without learning new weights, suggesting avenues for efficiently solving new tasks. For many tasks, e.g., linear regression, the data factorizes: examples are independent given a task latent that generates the data, e.g., linear coefficients. While an optimal predictor leverages this factorization by inferring task latents, it is unclear if Transformers implicitly do so or if they instead exploit heuristics and statistical shortcuts enabled by attention layers. Both scenarios have inspired active ongoing work. In this paper, we systematically investigate the effect of explicitly inferring task latents. We minimally modify the Transformer architecture with a bottleneck designed to prevent shortcuts in favor of more structured solutions, and then compare performance against standard Transformers across various ICL tasks. Contrary to intuition and some recent works, we find little discernible difference between the two; biasing towards task-relevant latent variables does not lead to better out-of-distribution performance, in general. Curiously, we find that while the bottleneck effectively learns to extract latent task variables from context, downstream processing struggles to utilize them for robust prediction. Our study highlights the intrinsic limitations of Transformers in achieving structured ICL solutions that generalize, and shows that while inferring the right latents aids interpretability, it is not sufficient to alleviate this problem.","sentences":["Large autoregressive models like Transformers can solve tasks through in-context learning (ICL) without learning new weights, suggesting avenues for efficiently solving new tasks.","For many tasks, e.g., linear regression, the data factorizes: examples are independent given a task latent that generates the data, e.g., linear coefficients.","While an optimal predictor leverages this factorization by inferring task latents, it is unclear if Transformers implicitly do so or if they instead exploit heuristics and statistical shortcuts enabled by attention layers.","Both scenarios have inspired active ongoing work.","In this paper, we systematically investigate the effect of explicitly inferring task latents.","We minimally modify the Transformer architecture with a bottleneck designed to prevent shortcuts in favor of more structured solutions, and then compare performance against standard Transformers across various ICL tasks.","Contrary to intuition and some recent works, we find little discernible difference between the two; biasing towards task-relevant latent variables does not lead to better out-of-distribution performance, in general.","Curiously, we find that while the bottleneck effectively learns to extract latent task variables from context, downstream processing struggles to utilize them for robust prediction.","Our study highlights the intrinsic limitations of Transformers in achieving structured ICL solutions that generalize, and shows that while inferring the right latents aids interpretability, it is not sufficient to alleviate this problem."],"url":"http://arxiv.org/abs/2405.19162v1"}
{"created":"2024-05-29 15:00:19","title":"Beyond Discrepancy: A Closer Look at the Theory of Distribution Shift","abstract":"Many machine learning models appear to deploy effortlessly under distribution shift, and perform well on a target distribution that is considerably different from the training distribution. Yet, learning theory of distribution shift bounds performance on the target distribution as a function of the discrepancy between the source and target, rarely guaranteeing high target accuracy. Motivated by this gap, this work takes a closer look at the theory of distribution shift for a classifier from a source to a target distribution. Instead of relying on the discrepancy, we adopt an Invariant-Risk-Minimization (IRM)-like assumption connecting the distributions, and characterize conditions under which data from a source distribution is sufficient for accurate classification of the target. When these conditions are not met, we show when only unlabeled data from the target is sufficient, and when labeled target data is needed. In all cases, we provide rigorous theoretical guarantees in the large sample regime.","sentences":["Many machine learning models appear to deploy effortlessly under distribution shift, and perform well on a target distribution that is considerably different from the training distribution.","Yet, learning theory of distribution shift bounds performance on the target distribution as a function of the discrepancy between the source and target, rarely guaranteeing high target accuracy.","Motivated by this gap, this work takes a closer look at the theory of distribution shift for a classifier from a source to a target distribution.","Instead of relying on the discrepancy, we adopt an Invariant-Risk-Minimization (IRM)-like assumption connecting the distributions, and characterize conditions under which data from a source distribution is sufficient for accurate classification of the target.","When these conditions are not met, we show when only unlabeled data from the target is sufficient, and when labeled target data is needed.","In all cases, we provide rigorous theoretical guarantees in the large sample regime."],"url":"http://arxiv.org/abs/2405.19156v1"}
{"created":"2024-05-29 14:41:57","title":"Multi-Source Coflow Scheduling in Collaborative Edge Computing with Multihop Network","abstract":"Collaborative edge computing has become a popular paradigm where edge devices collaborate by sharing resources. Data dissemination is a fundamental problem in CEC to decide what data is transmitted from which device and how. Existing works on data dissemination have not focused on coflow scheduling in CEC, which involves deciding the order of flows within and across coflows at network links. Coflow implies a set of parallel flows with a shared objective. The existing works on coflow scheduling in data centers usually assume a non-blocking switch and do not consider congestion at different links in the multi-hop path in CEC, leading to increased coflow completion time (CCT). Furthermore, existing works do not consider multiple flow sources that cannot be ignored, as data can have duplicate copies at different edge devices. This work formulates the multi-source coflow scheduling problem in CEC, which includes jointly deciding the source and flow ordering for multiple coflows to minimize the sum of CCT. This problem is shown to be NP-hard and challenging as each flow can have multiple dependent conflicts at multiple links. We propose a source and coflow-aware search and adjust (SCASA) heuristic that first provides an initial solution considering the coflow characteristics. SCASA further improves the initial solution using the source search and adjust heuristic by leveraging the knowledge of both coflows and network congestion at links. Evaluation done using simulation experiments shows that SCASA leads to up to 83% reduction in the sum of CCT compared to benchmarks without a joint solution.","sentences":["Collaborative edge computing has become a popular paradigm where edge devices collaborate by sharing resources.","Data dissemination is a fundamental problem in CEC to decide what data is transmitted from which device and how.","Existing works on data dissemination have not focused on coflow scheduling in CEC, which involves deciding the order of flows within and across coflows at network links.","Coflow implies a set of parallel flows with a shared objective.","The existing works on coflow scheduling in data centers usually assume a non-blocking switch and do not consider congestion at different links in the multi-hop path in CEC, leading to increased coflow completion time (CCT).","Furthermore, existing works do not consider multiple flow sources that cannot be ignored, as data can have duplicate copies at different edge devices.","This work formulates the multi-source coflow scheduling problem in CEC, which includes jointly deciding the source and flow ordering for multiple coflows to minimize the sum of CCT.","This problem is shown to be NP-hard and challenging as each flow can have multiple dependent conflicts at multiple links.","We propose a source and coflow-aware search and adjust (SCASA) heuristic that first provides an initial solution considering the coflow characteristics.","SCASA further improves the initial solution using the source search and adjust heuristic by leveraging the knowledge of both coflows and network congestion at links.","Evaluation done using simulation experiments shows that SCASA leads to up to 83% reduction in the sum of CCT compared to benchmarks without a joint solution."],"url":"http://arxiv.org/abs/2405.19136v1"}
{"created":"2024-05-29 14:38:32","title":"Analyzing Chat Protocols of Novice Programmers Solving Introductory Programming Tasks with ChatGPT","abstract":"Large Language Models (LLMs) have taken the world by storm, and students are assumed to use related tools at a great scale. In this research paper we aim to gain an understanding of how introductory programming students chat with LLMs and related tools, e.g., ChatGPT-3.5. To address this goal, computing students at a large German university were motivated to solve programming exercises with the assistance of ChatGPT as part of their weekly introductory course exercises. Then students (n=213) submitted their chat protocols (with 2335 prompts in sum) as data basis for this analysis. The data was analyzed w.r.t. the prompts, frequencies, the chats' progress, contents, and other use pattern, which revealed a great variety of interactions, both potentially supportive and concerning. Learning about students' interactions with ChatGPT will help inform and align teaching practices and instructions for future introductory programming courses in higher education.","sentences":["Large Language Models (LLMs) have taken the world by storm, and students are assumed to use related tools at a great scale.","In this research paper we aim to gain an understanding of how introductory programming students chat with LLMs and related tools, e.g., ChatGPT-3.5.","To address this goal, computing students at a large German university were motivated to solve programming exercises with the assistance of ChatGPT as part of their weekly introductory course exercises.","Then students (n=213) submitted their chat protocols (with 2335 prompts in sum) as data basis for this analysis.","The data was analyzed w.r.t.","the prompts, frequencies, the chats' progress, contents, and other use pattern, which revealed a great variety of interactions, both potentially supportive and concerning.","Learning about students' interactions with ChatGPT will help inform and align teaching practices and instructions for future introductory programming courses in higher education."],"url":"http://arxiv.org/abs/2405.19132v1"}
{"created":"2024-05-29 14:37:48","title":"Learning Interpretable Scheduling Algorithms for Data Processing Clusters","abstract":"Workloads in data processing clusters are often represented in the form of DAG (Directed Acyclic Graph) jobs. Scheduling DAG jobs is challenging. Simple heuristic scheduling algorithms are often adopted in practice in production data centres. There is much room for scheduling performance optimisation for cost saving. Recently, reinforcement learning approaches (like decima) have been attempted to optimise DAG job scheduling and demonstrate clear performance gain in comparison to traditional algorithms. However, reinforcement learning (RL) approaches face their own problems in real-world deployment. In particular, their black-box decision making processes and generalizability in unseen workloads may add a non-trivial burden to the cluster administrators. Moreover, adapting RL models on unseen workloads often requires significant amount of training data, which leaves edge cases run in a sub-optimal mode. To fill the gap, we propose a new method to distill a simple scheduling policy based on observations of the behaviours of a complex deep learning model. The simple model not only provides interpretability of scheduling decisions, but also adaptive to edge cases easily through tuning. We show that our method achieves high fidelity to the decisions made by deep learning models and outperforms these models when additional heuristics are taken into account.","sentences":["Workloads in data processing clusters are often represented in the form of DAG (Directed Acyclic Graph) jobs.","Scheduling DAG jobs is challenging.","Simple heuristic scheduling algorithms are often adopted in practice in production data centres.","There is much room for scheduling performance optimisation for cost saving.","Recently, reinforcement learning approaches (like decima) have been attempted to optimise DAG job scheduling and demonstrate clear performance gain in comparison to traditional algorithms.","However, reinforcement learning (RL) approaches face their own problems in real-world deployment.","In particular, their black-box decision making processes and generalizability in unseen workloads may add a non-trivial burden to the cluster administrators.","Moreover, adapting RL models on unseen workloads often requires significant amount of training data, which leaves edge cases run in a sub-optimal mode.","To fill the gap, we propose a new method to distill a simple scheduling policy based on observations of the behaviours of a complex deep learning model.","The simple model not only provides interpretability of scheduling decisions, but also adaptive to edge cases easily through tuning.","We show that our method achieves high fidelity to the decisions made by deep learning models and outperforms these models when additional heuristics are taken into account."],"url":"http://arxiv.org/abs/2405.19131v1"}
{"created":"2024-05-29 14:31:39","title":"Early Detection of Critical Urban Events using Mobile Phone Network Data","abstract":"Network Signalling Data (NSD) have the potential to provide continuous spatio-temporal information about the presence, mobility, and usage patterns of cell phone services by individuals. Such information is invaluable for monitoring large urban areas and supporting the implementation of decision-making services. When analyzed in real time, NSD can enable the early detection of critical urban events, including fires, large accidents, stampedes, terrorist attacks, and sports and leisure gatherings, especially if these events significantly impact mobile phone network activity in the affected areas. This paper presents empirical evidence that advanced NSD can detect anomalies in mobile traffic service consumption, attributable to critical urban events, with fine spatial and temporal resolutions. We introduce two methodologies for real-time anomaly detection from multivariate time series extracted from large-scale NSD, utilizing a range of algorithms adapted from the state-of-the-art in unsupervised machine learning techniques for anomaly detection. Our research includes a comprehensive quantitative evaluation of these algorithms on a large-scale dataset of NSD service consumption for the Paris region. The evaluation uses an original dataset of documented critical or unusual urban events. This dataset has been built as a ground truth basis for assessing the algorithms performance. The obtained results demonstrate that our framework can detect unusual events almost instantaneously and locate the affected areas with high precision, largely outperforming random classifiers. This efficiency and effectiveness underline the potential of NSD-based anomaly detection in significantly enhancing emergency response strategies and urban planning.","sentences":["Network Signalling Data (NSD) have the potential to provide continuous spatio-temporal information about the presence, mobility, and usage patterns of cell phone services by individuals.","Such information is invaluable for monitoring large urban areas and supporting the implementation of decision-making services.","When analyzed in real time, NSD can enable the early detection of critical urban events, including fires, large accidents, stampedes, terrorist attacks, and sports and leisure gatherings, especially if these events significantly impact mobile phone network activity in the affected areas.","This paper presents empirical evidence that advanced NSD can detect anomalies in mobile traffic service consumption, attributable to critical urban events, with fine spatial and temporal resolutions.","We introduce two methodologies for real-time anomaly detection from multivariate time series extracted from large-scale NSD, utilizing a range of algorithms adapted from the state-of-the-art in unsupervised machine learning techniques for anomaly detection.","Our research includes a comprehensive quantitative evaluation of these algorithms on a large-scale dataset of NSD service consumption for the Paris region.","The evaluation uses an original dataset of documented critical or unusual urban events.","This dataset has been built as a ground truth basis for assessing the algorithms performance.","The obtained results demonstrate that our framework can detect unusual events almost instantaneously and locate the affected areas with high precision, largely outperforming random classifiers.","This efficiency and effectiveness underline the potential of NSD-based anomaly detection in significantly enhancing emergency response strategies and urban planning."],"url":"http://arxiv.org/abs/2405.19125v1"}
{"created":"2024-05-29 14:28:08","title":"Spatio-Spectral Graph Neural Networks","abstract":"Spatial Message Passing Graph Neural Networks (MPGNNs) are widely used for learning on graph-structured data. However, key limitations of l-step MPGNNs are that their \"receptive field\" is typically limited to the l-hop neighborhood of a node and that information exchange between distant nodes is limited by over-squashing. Motivated by these limitations, we propose Spatio-Spectral Graph Neural Networks (S$^2$GNNs) -- a new modeling paradigm for Graph Neural Networks (GNNs) that synergistically combines spatially and spectrally parametrized graph filters. Parameterizing filters partially in the frequency domain enables global yet efficient information propagation. We show that S$^2$GNNs vanquish over-squashing and yield strictly tighter approximation-theoretic error bounds than MPGNNs. Further, rethinking graph convolutions at a fundamental level unlocks new design spaces. For example, S$^2$GNNs allow for free positional encodings that make them strictly more expressive than the 1-Weisfeiler-Lehman (WL) test. Moreover, to obtain general-purpose S$^2$GNNs, we propose spectrally parametrized filters for directed graphs. S$^2$GNNs outperform spatial MPGNNs, graph transformers, and graph rewirings, e.g., on the peptide long-range benchmark tasks, and are competitive with state-of-the-art sequence modeling. On a 40 GB GPU, S$^2$GNNs scale to millions of nodes.","sentences":["Spatial Message Passing Graph Neural Networks (MPGNNs) are widely used for learning on graph-structured data.","However, key limitations of l-step MPGNNs are that their \"receptive field\" is typically limited to the l-hop neighborhood of a node and that information exchange between distant nodes is limited by over-squashing.","Motivated by these limitations, we propose Spatio-Spectral Graph Neural Networks (S$^2$GNNs) -- a new modeling paradigm for Graph Neural Networks (GNNs) that synergistically combines spatially and spectrally parametrized graph filters.","Parameterizing filters partially in the frequency domain enables global yet efficient information propagation.","We show that S$^2$GNNs vanquish over-squashing and yield strictly tighter approximation-theoretic error bounds than MPGNNs.","Further, rethinking graph convolutions at a fundamental level unlocks new design spaces.","For example, S$^2$GNNs allow for free positional encodings that make them strictly more expressive than the 1-Weisfeiler-Lehman (WL) test.","Moreover, to obtain general-purpose S$^2$GNNs, we propose spectrally parametrized filters for directed graphs.","S$^2$GNNs outperform spatial MPGNNs, graph transformers, and graph rewirings, e.g., on the peptide long-range benchmark tasks, and are competitive with state-of-the-art sequence modeling.","On a 40 GB GPU, S$^2$GNNs scale to millions of nodes."],"url":"http://arxiv.org/abs/2405.19121v1"}
{"created":"2024-05-29 14:24:42","title":"ChartFormer: A Large Vision Language Model for Converting Chart Images into Tactile Accessible SVGs","abstract":"Visualizations, such as charts, are crucial for interpreting complex data. However, they are often provided as raster images, which are not compatible with assistive technologies for people with blindness and visual impairments, such as embossed papers or tactile displays. At the same time, creating accessible vector graphics requires a skilled sighted person and is time-intensive. In this work, we leverage advancements in the field of chart analysis to generate tactile charts in an end-to-end manner. Our three key contributions are as follows: (1) introducing the ChartFormer model trained to convert raster chart images into tactile-accessible SVGs, (2) training this model on the Chart2Tactile dataset, a synthetic chart dataset we created following accessibility standards, and (3) evaluating the effectiveness of our SVGs through a pilot user study with an refreshable two-dimensional tactile display. Our work is publicly available at https://github.com/nsothman/ChartFormer .","sentences":["Visualizations, such as charts, are crucial for interpreting complex data.","However, they are often provided as raster images, which are not compatible with assistive technologies for people with blindness and visual impairments, such as embossed papers or tactile displays.","At the same time, creating accessible vector graphics requires a skilled sighted person and is time-intensive.","In this work, we leverage advancements in the field of chart analysis to generate tactile charts in an end-to-end manner.","Our three key contributions are as follows: (1) introducing the ChartFormer model trained to convert raster chart images into tactile-accessible SVGs, (2) training this model on the Chart2Tactile dataset, a synthetic chart dataset we created following accessibility standards, and (3) evaluating the effectiveness of our SVGs through a pilot user study with an refreshable two-dimensional tactile display.","Our work is publicly available at https://github.com/nsothman/ChartFormer ."],"url":"http://arxiv.org/abs/2405.19117v1"}
{"created":"2024-05-29 14:14:05","title":"PathReasoner: Modeling Reasoning Path with Equivalent Extension for Logical Question Answering","abstract":"Logical reasoning task has attracted great interest since it was proposed. Faced with such a task, current competitive models, even large language models (e.g., ChatGPT and PaLM 2), still perform badly. Previous promising LMs struggle in logical consistency modeling and logical structure perception. To this end, we model the logical reasoning task by transforming each logical sample into reasoning paths and propose an architecture \\textbf{PathReasoner}. It addresses the task from the views of both data and model. To expand the diversity of the logical samples, we propose an atom extension strategy supported by equivalent logical formulas, to form new reasoning paths. From the model perspective, we design a stack of transformer-style blocks. In particular, we propose a path-attention module to joint model in-atom and cross-atom relations with the high-order diffusion strategy. Experiments show that PathReasoner achieves competitive performances on two logical reasoning benchmarks and great generalization abilities.","sentences":["Logical reasoning task has attracted great interest since it was proposed.","Faced with such a task, current competitive models, even large language models (e.g., ChatGPT and PaLM 2), still perform badly.","Previous promising LMs struggle in logical consistency modeling and logical structure perception.","To this end, we model the logical reasoning task by transforming each logical sample into reasoning paths and propose an architecture \\textbf{PathReasoner}.","It addresses the task from the views of both data and model.","To expand the diversity of the logical samples, we propose an atom extension strategy supported by equivalent logical formulas, to form new reasoning paths.","From the model perspective, we design a stack of transformer-style blocks.","In particular, we propose a path-attention module to joint model in-atom and cross-atom relations with the high-order diffusion strategy.","Experiments show that PathReasoner achieves competitive performances on two logical reasoning benchmarks and great generalization abilities."],"url":"http://arxiv.org/abs/2405.19109v1"}
{"created":"2024-05-29 14:11:29","title":"Offline Regularised Reinforcement Learning for Large Language Models Alignment","abstract":"The dominant framework for alignment of large language models (LLM), whether through reinforcement learning from human feedback or direct preference optimisation, is to learn from preference data. This involves building datasets where each element is a quadruplet composed of a prompt, two independent responses (completions of the prompt) and a human preference between the two independent responses, yielding a preferred and a dis-preferred response. Such data is typically scarce and expensive to collect. On the other hand, \\emph{single-trajectory} datasets where each element is a triplet composed of a prompt, a response and a human feedback is naturally more abundant. The canonical element of such datasets is for instance an LLM's response to a user's prompt followed by a user's feedback such as a thumbs-up/down. Consequently, in this work, we propose DRO, or \\emph{Direct Reward Optimisation}, as a framework and associated algorithms that do not require pairwise preferences. DRO uses a simple mean-squared objective that can be implemented in various ways. We validate our findings empirically, using T5 encoder-decoder language models, and show DRO's performance over selected baselines such as Kahneman-Tversky Optimization (KTO). Thus, we confirm that DRO is a simple and empirically compelling method for single-trajectory policy optimisation.","sentences":["The dominant framework for alignment of large language models (LLM), whether through reinforcement learning from human feedback or direct preference optimisation, is to learn from preference data.","This involves building datasets where each element is a quadruplet composed of a prompt, two independent responses (completions of the prompt) and a human preference between the two independent responses, yielding a preferred and a dis-preferred response.","Such data is typically scarce and expensive to collect.","On the other hand, \\emph{single-trajectory} datasets where each element is a triplet composed of a prompt, a response and a human feedback is naturally more abundant.","The canonical element of such datasets is for instance an LLM's response to a user's prompt followed by a user's feedback such as a thumbs-up/down.","Consequently, in this work, we propose DRO, or \\emph{Direct Reward Optimisation}, as a framework and associated algorithms that do not require pairwise preferences.","DRO uses a simple mean-squared objective that can be implemented in various ways.","We validate our findings empirically, using T5 encoder-decoder language models, and show DRO's performance over selected baselines such as Kahneman-Tversky Optimization (KTO).","Thus, we confirm that DRO is a simple and empirically compelling method for single-trajectory policy optimisation."],"url":"http://arxiv.org/abs/2405.19107v1"}
{"created":"2024-05-29 14:06:51","title":"Poseidon: Efficient Foundation Models for PDEs","abstract":"We introduce Poseidon, a foundation model for learning the solution operators of PDEs. It is based on a multiscale operator transformer, with time-conditioned layer norms that enable continuous-in-time evaluations. A novel training strategy leveraging the semi-group property of time-dependent PDEs to allow for significant scaling-up of the training data is also proposed. Poseidon is pretrained on a diverse, large scale dataset for the governing equations of fluid dynamics. It is then evaluated on a suite of 15 challenging downstream tasks that include a wide variety of PDE types and operators. We show that Poseidon exhibits excellent performance across the board by outperforming baselines significantly, both in terms of sample efficiency and accuracy. Poseidon also generalizes very well to new physics that is not seen during pretraining. Moreover, Poseidon scales with respect to model and data size, both for pretraining and for downstream tasks. Taken together, our results showcase the surprising ability of Poseidon to learn effective representations from a very small set of PDEs during pretraining in order to generalize well to unseen and unrelated PDEs downstream, demonstrating its potential as an effective, general purpose PDE foundation model. Finally, the Poseidon model as well as underlying pretraining and downstream datasets are open sourced, with code being available at https://github.com/camlab-ethz/poseidon and pretrained models and datasets at https://huggingface.co/camlab-ethz.","sentences":["We introduce Poseidon, a foundation model for learning the solution operators of PDEs.","It is based on a multiscale operator transformer, with time-conditioned layer norms that enable continuous-in-time evaluations.","A novel training strategy leveraging the semi-group property of time-dependent PDEs to allow for significant scaling-up of the training data is also proposed.","Poseidon is pretrained on a diverse, large scale dataset for the governing equations of fluid dynamics.","It is then evaluated on a suite of 15 challenging downstream tasks that include a wide variety of PDE types and operators.","We show that Poseidon exhibits excellent performance across the board by outperforming baselines significantly, both in terms of sample efficiency and accuracy.","Poseidon also generalizes very well to new physics that is not seen during pretraining.","Moreover, Poseidon scales with respect to model and data size, both for pretraining and for downstream tasks.","Taken together, our results showcase the surprising ability of Poseidon to learn effective representations from a very small set of PDEs during pretraining in order to generalize well to unseen and unrelated PDEs downstream, demonstrating its potential as an effective, general purpose PDE foundation model.","Finally, the Poseidon model as well as underlying pretraining and downstream datasets are open sourced, with code being available at https://github.com/camlab-ethz/poseidon and pretrained models and datasets at https://huggingface.co/camlab-ethz."],"url":"http://arxiv.org/abs/2405.19101v1"}
{"created":"2024-05-29 14:06:09","title":"Enhancing Zero-Shot Facial Expression Recognition by LLM Knowledge Transfer","abstract":"Current facial expression recognition (FER) models are often designed in a supervised learning manner thus are constrained by the lack of large-scale facial expression images with high-quality annotations. Consequently, these models often fail to generalize well, performing poorly on unseen images in training. Vision-language-based zero-shot models demonstrate a promising potential for addressing such challenges. However, these models lack task-specific knowledge therefore are not optimized for the nuances of recognizing facial expressions. To bridge this gap, this work proposes a novel method, Exp-CLIP, to enhance zero-shot FER by transferring the task knowledge from large language models (LLMs). Specifically, based on the pre-trained vision-language encoders, we incorporate a projection head designed to map the initial joint vision-language space into a space that captures representations of facial actions. To train this projection head for subsequent zero-shot predictions, we propose to align the projected visual representations with task-specific semantic meanings derived from the LLM encoder, and the text instruction-based strategy is employed to customize the LLM knowledge. Given unlabelled facial data and efficient training of the projection head, Exp-CLIP achieves superior zero-shot results to the CLIP models and several other large vision-language models (LVLMs) on seven in-the-wild FER datasets. The code and pre-trained models are available at \\url{https://github.com/zengqunzhao/Exp-CLIP}.","sentences":["Current facial expression recognition (FER) models are often designed in a supervised learning manner thus are constrained by the lack of large-scale facial expression images with high-quality annotations.","Consequently, these models often fail to generalize well, performing poorly on unseen images in training.","Vision-language-based zero-shot models demonstrate a promising potential for addressing such challenges.","However, these models lack task-specific knowledge therefore are not optimized for the nuances of recognizing facial expressions.","To bridge this gap, this work proposes a novel method, Exp-CLIP, to enhance zero-shot FER by transferring the task knowledge from large language models (LLMs).","Specifically, based on the pre-trained vision-language encoders, we incorporate a projection head designed to map the initial joint vision-language space into a space that captures representations of facial actions.","To train this projection head for subsequent zero-shot predictions, we propose to align the projected visual representations with task-specific semantic meanings derived from the LLM encoder, and the text instruction-based strategy is employed to customize the LLM knowledge.","Given unlabelled facial data and efficient training of the projection head, Exp-CLIP achieves superior zero-shot results to the CLIP models and several other large vision-language models (LVLMs) on seven in-the-wild FER datasets.","The code and pre-trained models are available at \\url{https://github.com/zengqunzhao/Exp-CLIP}."],"url":"http://arxiv.org/abs/2405.19100v1"}
{"created":"2024-05-29 13:55:06","title":"Faithful Chart Summarization with ChaTS-Pi","abstract":"Chart-to-summary generation can help explore data, communicate insights, and help the visually impaired people. Multi-modal generative models have been used to produce fluent summaries, but they can suffer from factual and perceptual errors. In this work we present CHATS-CRITIC, a reference-free chart summarization metric for scoring faithfulness. CHATS-CRITIC is composed of an image-to-text model to recover the table from a chart, and a tabular entailment model applied to score the summary sentence by sentence. We find that CHATS-CRITIC evaluates the summary quality according to human ratings better than reference-based metrics, either learned or n-gram based, and can be further used to fix candidate summaries by removing not supported sentences. We then introduce CHATS-PI, a chart-to-summary pipeline that leverages CHATS-CRITIC during inference to fix and rank sampled candidates from any chart-summarization model. We evaluate CHATS-PI and CHATS-CRITIC using human raters, establishing state-of-the-art results on two popular chart-to-summary datasets.","sentences":["Chart-to-summary generation can help explore data, communicate insights, and help the visually impaired people.","Multi-modal generative models have been used to produce fluent summaries, but they can suffer from factual and perceptual errors.","In this work we present CHATS-CRITIC, a reference-free chart summarization metric for scoring faithfulness.","CHATS-CRITIC is composed of an image-to-text model to recover the table from a chart, and a tabular entailment model applied to score the summary sentence by sentence.","We find that CHATS-CRITIC evaluates the summary quality according to human ratings better than reference-based metrics, either learned or n-gram based, and can be further used to fix candidate summaries by removing not supported sentences.","We then introduce CHATS-PI, a chart-to-summary pipeline that leverages CHATS-CRITIC during inference to fix and rank sampled candidates from any chart-summarization model.","We evaluate CHATS-PI and CHATS-CRITIC using human raters, establishing state-of-the-art results on two popular chart-to-summary datasets."],"url":"http://arxiv.org/abs/2405.19094v1"}
{"created":"2024-05-29 13:54:12","title":"Benchmarking and Improving Detail Image Caption","abstract":"Image captioning has long been regarded as a fundamental task in visual understanding. Recently, however, few large vision-language model (LVLM) research discusses model's image captioning performance because of the outdated short-caption benchmarks and unreliable evaluation metrics. In this work, we propose to benchmark detail image caption task by curating high-quality evaluation datasets annotated by human experts, GPT-4V and Gemini-1.5-Pro. We also design a more reliable caption evaluation metric called CAPTURE (CAPtion evaluation by exTracting and coUpling coRE information). CAPTURE extracts visual elements, e.g., objects, attributes and relations from captions, and then matches these elements through three stages, achieving the highest consistency with expert judgements over other rule-based or model-based caption metrics. The proposed benchmark and metric provide reliable evaluation for LVLM's detailed image captioning ability. Guided by this evaluation, we further explore to unleash LVLM's detail caption capabilities by synthesizing high-quality data through a five-stage data construction pipeline. Our pipeline only uses a given LVLM itself and other open-source tools, without any human or GPT-4V annotation in the loop. Experiments show that the proposed data construction strategy significantly improves model-generated detail caption data quality for LVLMs with leading performance, and the data quality can be further improved in a self-looping paradigm. All code and dataset will be publicly available at https://github.com/foundation-multimodal-models/CAPTURE.","sentences":["Image captioning has long been regarded as a fundamental task in visual understanding.","Recently, however, few large vision-language model (LVLM) research discusses model's image captioning performance because of the outdated short-caption benchmarks and unreliable evaluation metrics.","In this work, we propose to benchmark detail image caption task by curating high-quality evaluation datasets annotated by human experts, GPT-4V and Gemini-1.5-Pro.","We also design a more reliable caption evaluation metric called CAPTURE (CAPtion evaluation by exTracting and coUpling coRE information).","CAPTURE extracts visual elements, e.g., objects, attributes and relations from captions, and then matches these elements through three stages, achieving the highest consistency with expert judgements over other rule-based or model-based caption metrics.","The proposed benchmark and metric provide reliable evaluation for LVLM's detailed image captioning ability.","Guided by this evaluation, we further explore to unleash LVLM's detail caption capabilities by synthesizing high-quality data through a five-stage data construction pipeline.","Our pipeline only uses a given LVLM itself and other open-source tools, without any human or GPT-4V annotation in the loop.","Experiments show that the proposed data construction strategy significantly improves model-generated detail caption data quality for LVLMs with leading performance, and the data quality can be further improved in a self-looping paradigm.","All code and dataset will be publicly available at https://github.com/foundation-multimodal-models/CAPTURE."],"url":"http://arxiv.org/abs/2405.19092v1"}
{"created":"2024-05-29 13:36:36","title":"OMPO: A Unified Framework for RL under Policy and Dynamics Shifts","abstract":"Training reinforcement learning policies using environment interaction data collected from varying policies or dynamics presents a fundamental challenge. Existing works often overlook the distribution discrepancies induced by policy or dynamics shifts, or rely on specialized algorithms with task priors, thus often resulting in suboptimal policy performances and high learning variances. In this paper, we identify a unified strategy for online RL policy learning under diverse settings of policy and dynamics shifts: transition occupancy matching. In light of this, we introduce a surrogate policy learning objective by considering the transition occupancy discrepancies and then cast it into a tractable min-max optimization problem through dual reformulation. Our method, dubbed Occupancy-Matching Policy Optimization (OMPO), features a specialized actor-critic structure equipped with a distribution discriminator and a small-size local buffer. We conduct extensive experiments based on the OpenAI Gym, Meta-World, and Panda Robots environments, encompassing policy shifts under stationary and nonstationary dynamics, as well as domain adaption. The results demonstrate that OMPO outperforms the specialized baselines from different categories in all settings. We also find that OMPO exhibits particularly strong performance when combined with domain randomization, highlighting its potential in RL-based robotics applications","sentences":["Training reinforcement learning policies using environment interaction data collected from varying policies or dynamics presents a fundamental challenge.","Existing works often overlook the distribution discrepancies induced by policy or dynamics shifts, or rely on specialized algorithms with task priors, thus often resulting in suboptimal policy performances and high learning variances.","In this paper, we identify a unified strategy for online RL policy learning under diverse settings of policy and dynamics shifts: transition occupancy matching.","In light of this, we introduce a surrogate policy learning objective by considering the transition occupancy discrepancies and then cast it into a tractable min-max optimization problem through dual reformulation.","Our method, dubbed Occupancy-Matching Policy Optimization (OMPO), features a specialized actor-critic structure equipped with a distribution discriminator and a small-size local buffer.","We conduct extensive experiments based on the OpenAI Gym, Meta-World, and Panda Robots environments, encompassing policy shifts under stationary and nonstationary dynamics, as well as domain adaption.","The results demonstrate that OMPO outperforms the specialized baselines from different categories in all settings.","We also find that OMPO exhibits particularly strong performance when combined with domain randomization, highlighting its potential in RL-based robotics applications"],"url":"http://arxiv.org/abs/2405.19080v1"}
{"created":"2024-05-29 13:34:32","title":"Cephalo: Multi-Modal Vision-Language Models for Bio-Inspired Materials Analysis and Design","abstract":"We present Cephalo, a series of multimodal vision large language models (V-LLMs) designed for materials science applications, integrating visual and linguistic data for enhanced understanding and interaction within human-AI and multi-agent AI frameworks. A key innovation of Cephalo is its advanced dataset generation method, which employs a sophisticated algorithm to accurately detect and separate images and their corresponding textual descriptions from PDF documents, such as scientific papers. The method includes a careful refinement of image-text pairs through integrated vision and language processing, ensuring high-quality, contextually relevant, and well reasoned training data. Cephalo is trained on integrated image and text data extracted from thousands of scientific papers and science-focused Wikipedia pages demonstrates can interpret complex visual scenes, generate precise language descriptions, and answer queries about images effectively. The combination of a vision encoder with an autoregressive transformer supports complex natural language understanding in an integrated model, which can be coupled with other generative methods to create an image-to-text-to-image or image-to-text-to-3D pipeline. To explore the development of larger models from smaller ones, we merge sets of layers that originate from different pre-trained source models. This hybrid approach allows us to leverage the domain-specific expertise and general conversational capabilities to harness the strengths of multiple models. We examine the models in diverse use cases that incorporate biological materials, fracture and engineering analysis, protein biophysics, and bio-inspired design based on insect behavior. Generative applications include bio-inspired designs, including pollen-inspired architected materials, as well as the synthesis of bio-inspired material microstructures from a photograph of a solar eclipse.","sentences":["We present Cephalo, a series of multimodal vision large language models (V-LLMs) designed for materials science applications, integrating visual and linguistic data for enhanced understanding and interaction within human-AI and multi-agent AI frameworks.","A key innovation of Cephalo is its advanced dataset generation method, which employs a sophisticated algorithm to accurately detect and separate images and their corresponding textual descriptions from PDF documents, such as scientific papers.","The method includes a careful refinement of image-text pairs through integrated vision and language processing, ensuring high-quality, contextually relevant, and well reasoned training data.","Cephalo is trained on integrated image and text data extracted from thousands of scientific papers and science-focused Wikipedia pages demonstrates can interpret complex visual scenes, generate precise language descriptions, and answer queries about images effectively.","The combination of a vision encoder with an autoregressive transformer supports complex natural language understanding in an integrated model, which can be coupled with other generative methods to create an image-to-text-to-image or image-to-text-to-3D pipeline.","To explore the development of larger models from smaller ones, we merge sets of layers that originate from different pre-trained source models.","This hybrid approach allows us to leverage the domain-specific expertise and general conversational capabilities to harness the strengths of multiple models.","We examine the models in diverse use cases that incorporate biological materials, fracture and engineering analysis, protein biophysics, and bio-inspired design based on insect behavior.","Generative applications include bio-inspired designs, including pollen-inspired architected materials, as well as the synthesis of bio-inspired material microstructures from a photograph of a solar eclipse."],"url":"http://arxiv.org/abs/2405.19076v1"}
{"created":"2024-05-29 13:31:42","title":"Resurrecting Old Classes with New Data for Exemplar-Free Continual Learning","abstract":"Continual learning methods are known to suffer from catastrophic forgetting, a phenomenon that is particularly hard to counter for methods that do not store exemplars of previous tasks. Therefore, to reduce potential drift in the feature extractor, existing exemplar-free methods are typically evaluated in settings where the first task is significantly larger than subsequent tasks. Their performance drops drastically in more challenging settings starting with a smaller first task. To address this problem of feature drift estimation for exemplar-free methods, we propose to adversarially perturb the current samples such that their embeddings are close to the old class prototypes in the old model embedding space. We then estimate the drift in the embedding space from the old to the new model using the perturbed images and compensate the prototypes accordingly. We exploit the fact that adversarial samples are transferable from the old to the new feature space in a continual learning setting. The generation of these images is simple and computationally cheap. We demonstrate in our experiments that the proposed approach better tracks the movement of prototypes in embedding space and outperforms existing methods on several standard continual learning benchmarks as well as on fine-grained datasets. Code is available at https://github.com/dipamgoswami/ADC.","sentences":["Continual learning methods are known to suffer from catastrophic forgetting, a phenomenon that is particularly hard to counter for methods that do not store exemplars of previous tasks.","Therefore, to reduce potential drift in the feature extractor, existing exemplar-free methods are typically evaluated in settings where the first task is significantly larger than subsequent tasks.","Their performance drops drastically in more challenging settings starting with a smaller first task.","To address this problem of feature drift estimation for exemplar-free methods, we propose to adversarially perturb the current samples such that their embeddings are close to the old class prototypes in the old model embedding space.","We then estimate the drift in the embedding space from the old to the new model using the perturbed images and compensate the prototypes accordingly.","We exploit the fact that adversarial samples are transferable from the old to the new feature space in a continual learning setting.","The generation of these images is simple and computationally cheap.","We demonstrate in our experiments that the proposed approach better tracks the movement of prototypes in embedding space and outperforms existing methods on several standard continual learning benchmarks as well as on fine-grained datasets.","Code is available at https://github.com/dipamgoswami/ADC."],"url":"http://arxiv.org/abs/2405.19074v1"}
{"created":"2024-05-29 13:09:33","title":"SIG: Efficient Self-Interpretable Graph Neural Network for Continuous-time Dynamic Graphs","abstract":"While dynamic graph neural networks have shown promise in various applications, explaining their predictions on continuous-time dynamic graphs (CTDGs) is difficult. This paper investigates a new research task: self-interpretable GNNs for CTDGs. We aim to predict future links within the dynamic graph while simultaneously providing causal explanations for these predictions. There are two key challenges: (1) capturing the underlying structural and temporal information that remains consistent across both independent and identically distributed (IID) and out-of-distribution (OOD) data, and (2) efficiently generating high-quality link prediction results and explanations. To tackle these challenges, we propose a novel causal inference model, namely the Independent and Confounded Causal Model (ICCM). ICCM is then integrated into a deep learning architecture that considers both effectiveness and efficiency. Extensive experiments demonstrate that our proposed model significantly outperforms existing methods across link prediction accuracy, explanation quality, and robustness to shortcut features. Our code and datasets are anonymously released at https://github.com/2024SIG/SIG.","sentences":["While dynamic graph neural networks have shown promise in various applications, explaining their predictions on continuous-time dynamic graphs (CTDGs) is difficult.","This paper investigates a new research task: self-interpretable GNNs for CTDGs.","We aim to predict future links within the dynamic graph while simultaneously providing causal explanations for these predictions.","There are two key challenges: (1) capturing the underlying structural and temporal information that remains consistent across both independent and identically distributed (IID) and out-of-distribution (OOD) data, and (2) efficiently generating high-quality link prediction results and explanations.","To tackle these challenges, we propose a novel causal inference model, namely the Independent and Confounded Causal Model (ICCM).","ICCM is then integrated into a deep learning architecture that considers both effectiveness and efficiency.","Extensive experiments demonstrate that our proposed model significantly outperforms existing methods across link prediction accuracy, explanation quality, and robustness to shortcut features.","Our code and datasets are anonymously released at https://github.com/2024SIG/SIG."],"url":"http://arxiv.org/abs/2405.19062v1"}
{"created":"2024-05-29 13:00:10","title":"Robust Entropy Search for Safe Efficient Bayesian Optimization","abstract":"The practical use of Bayesian Optimization (BO) in engineering applications imposes special requirements: high sampling efficiency on the one hand and finding a robust solution on the other hand. We address the case of adversarial robustness, where all parameters are controllable during the optimization process, but a subset of them is uncontrollable or even adversely perturbed at the time of application. To this end, we develop an efficient information-based acquisition function that we call Robust Entropy Search (RES). We empirically demonstrate its benefits in experiments on synthetic and real-life data. The results showthat RES reliably finds robust optima, outperforming state-of-the-art algorithms.","sentences":["The practical use of Bayesian Optimization (BO) in engineering applications imposes special requirements: high sampling efficiency on the one hand and finding a robust solution on the other hand.","We address the case of adversarial robustness, where all parameters are controllable during the optimization process, but a subset of them is uncontrollable or even adversely perturbed at the time of application.","To this end, we develop an efficient information-based acquisition function that we call Robust Entropy Search (RES).","We empirically demonstrate its benefits in experiments on synthetic and real-life data.","The results showthat RES reliably finds robust optima, outperforming state-of-the-art algorithms."],"url":"http://arxiv.org/abs/2405.19059v1"}
{"created":"2024-05-29 12:56:11","title":"FUSU: A Multi-temporal-source Land Use Change Segmentation Dataset for Fine-grained Urban Semantic Understanding","abstract":"Fine urban change segmentation using multi-temporal remote sensing images is essential for understanding human-environment interactions. Despite advances in remote sensing data for urban monitoring, coarse-grained classification systems and the lack of continuous temporal observations hinder the application of deep learning to urban change analysis. To address this, we introduce FUSU, a multi-source, multi-temporal change segmentation dataset for fine-grained urban semantic understanding. FUSU features the most detailed land use classification system to date, with 17 classes and 30 billion pixels of annotations. It includes bi-temporal high-resolution satellite images with 20-50 cm ground sample distance and monthly optical and radar satellite time series, covering 847 km2 across five urban areas in China. The fine-grained pixel-wise annotations and high spatial-temporal resolution data provide a robust foundation for deep learning models to understand urbanization and land use changes. To fully leverage FUSU, we propose a unified time-series architecture for both change detection and segmentation and benchmark FUSU on various methods for several tasks. Dataset and code will be available at: https://github.com/yuanshuai0914/FUSU.","sentences":["Fine urban change segmentation using multi-temporal remote sensing images is essential for understanding human-environment interactions.","Despite advances in remote sensing data for urban monitoring, coarse-grained classification systems and the lack of continuous temporal observations hinder the application of deep learning to urban change analysis.","To address this, we introduce FUSU, a multi-source, multi-temporal change segmentation dataset for fine-grained urban semantic understanding.","FUSU features the most detailed land use classification system to date, with 17 classes and 30 billion pixels of annotations.","It includes bi-temporal high-resolution satellite images with 20-50 cm ground sample distance and monthly optical and radar satellite time series, covering 847 km2 across five urban areas in China.","The fine-grained pixel-wise annotations and high spatial-temporal resolution data provide a robust foundation for deep learning models to understand urbanization and land use changes.","To fully leverage FUSU, we propose a unified time-series architecture for both change detection and segmentation and benchmark FUSU on various methods for several tasks.","Dataset and code will be available at: https://github.com/yuanshuai0914/FUSU."],"url":"http://arxiv.org/abs/2405.19055v1"}
{"created":"2024-05-29 12:44:41","title":"Statistical Context Detection for Deep Lifelong Reinforcement Learning","abstract":"Context detection involves labeling segments of an online stream of data as belonging to different tasks. Task labels are used in lifelong learning algorithms to perform consolidation or other procedures that prevent catastrophic forgetting. Inferring task labels from online experiences remains a challenging problem. Most approaches assume finite and low-dimension observation spaces or a preliminary training phase during which task labels are learned. Moreover, changes in the transition or reward functions can be detected only in combination with a policy, and therefore are more difficult to detect than changes in the input distribution. This paper presents an approach to learning both policies and labels in an online deep reinforcement learning setting. The key idea is to use distance metrics, obtained via optimal transport methods, i.e., Wasserstein distance, on suitable latent action-reward spaces to measure distances between sets of data points from past and current streams. Such distances can then be used for statistical tests based on an adapted Kolmogorov-Smirnov calculation to assign labels to sequences of experiences. A rollback procedure is introduced to learn multiple policies by ensuring that only the appropriate data is used to train the corresponding policy. The combination of task detection and policy deployment allows for the optimization of lifelong reinforcement learning agents without an oracle that provides task labels. The approach is tested using two benchmarks and the results show promising performance when compared with related context detection algorithms. The results suggest that optimal transport statistical methods provide an explainable and justifiable procedure for online context detection and reward optimization in lifelong reinforcement learning.","sentences":["Context detection involves labeling segments of an online stream of data as belonging to different tasks.","Task labels are used in lifelong learning algorithms to perform consolidation or other procedures that prevent catastrophic forgetting.","Inferring task labels from online experiences remains a challenging problem.","Most approaches assume finite and low-dimension observation spaces or a preliminary training phase during which task labels are learned.","Moreover, changes in the transition or reward functions can be detected only in combination with a policy, and therefore are more difficult to detect than changes in the input distribution.","This paper presents an approach to learning both policies and labels in an online deep reinforcement learning setting.","The key idea is to use distance metrics, obtained via optimal transport methods, i.e., Wasserstein distance, on suitable latent action-reward spaces to measure distances between sets of data points from past and current streams.","Such distances can then be used for statistical tests based on an adapted Kolmogorov-Smirnov calculation to assign labels to sequences of experiences.","A rollback procedure is introduced to learn multiple policies by ensuring that only the appropriate data is used to train the corresponding policy.","The combination of task detection and policy deployment allows for the optimization of lifelong reinforcement learning agents without an oracle that provides task labels.","The approach is tested using two benchmarks and the results show promising performance when compared with related context detection algorithms.","The results suggest that optimal transport statistical methods provide an explainable and justifiable procedure for online context detection and reward optimization in lifelong reinforcement learning."],"url":"http://arxiv.org/abs/2405.19047v1"}
{"created":"2024-05-29 12:43:39","title":"Continual Collaborative Distillation for Recommender System","abstract":"Knowledge distillation (KD) has emerged as a promising technique for addressing the computational challenges associated with deploying large-scale recommender systems. KD transfers the knowledge of a massive teacher system to a compact student model, to reduce the huge computational burdens for inference while retaining high accuracy. The existing KD studies primarily focus on one-time distillation in static environments, leaving a substantial gap in their applicability to real-world scenarios dealing with continuously incoming users, items, and their interactions. In this work, we delve into a systematic approach to operating the teacher-student KD in a non-stationary data stream. Our goal is to enable efficient deployment through a compact student, which preserves the high performance of the massive teacher, while effectively adapting to continuously incoming data. We propose Continual Collaborative Distillation (CCD) framework, where both the teacher and the student continually and collaboratively evolve along the data stream. CCD facilitates the student in effectively adapting to new data, while also enabling the teacher to fully leverage accumulated knowledge. We validate the effectiveness of CCD through extensive quantitative, ablative, and exploratory experiments on two real-world datasets. We expect this research direction to contribute to narrowing the gap between existing KD studies and practical applications, thereby enhancing the applicability of KD in real-world systems.","sentences":["Knowledge distillation (KD) has emerged as a promising technique for addressing the computational challenges associated with deploying large-scale recommender systems.","KD transfers the knowledge of a massive teacher system to a compact student model, to reduce the huge computational burdens for inference while retaining high accuracy.","The existing KD studies primarily focus on one-time distillation in static environments, leaving a substantial gap in their applicability to real-world scenarios dealing with continuously incoming users, items, and their interactions.","In this work, we delve into a systematic approach to operating the teacher-student KD in a non-stationary data stream.","Our goal is to enable efficient deployment through a compact student, which preserves the high performance of the massive teacher, while effectively adapting to continuously incoming data.","We propose Continual Collaborative Distillation (CCD) framework, where both the teacher and the student continually and collaboratively evolve along the data stream.","CCD facilitates the student in effectively adapting to new data, while also enabling the teacher to fully leverage accumulated knowledge.","We validate the effectiveness of CCD through extensive quantitative, ablative, and exploratory experiments on two real-world datasets.","We expect this research direction to contribute to narrowing the gap between existing KD studies and practical applications, thereby enhancing the applicability of KD in real-world systems."],"url":"http://arxiv.org/abs/2405.19046v1"}
{"created":"2024-05-29 12:24:08","title":"On the formalization of the notion of an interactive algorithm","abstract":"An earlier paper gives an account of a quest for a satisfactory formalization of the classical informal notion of an algorithm. In this paper, an attempt is made to generalize the results of that quest to the informal notion of an interactive algorithm. The notion of an interactive proto-algorithm is introduced. Interactive algorithms are expected to be equivalence classes of interactive proto-algorithms under an appropriate equivalence relation. As in the non-interactive case, three equivalence relations are defined. Two of them are deemed to be bounds for an appropriate equivalence relation and the third is likely an appropriate one.","sentences":["An earlier paper gives an account of a quest for a satisfactory formalization of the classical informal notion of an algorithm.","In this paper, an attempt is made to generalize the results of that quest to the informal notion of an interactive algorithm.","The notion of an interactive proto-algorithm is introduced.","Interactive algorithms are expected to be equivalence classes of interactive proto-algorithms under an appropriate equivalence relation.","As in the non-interactive case, three equivalence relations are defined.","Two of them are deemed to be bounds for an appropriate equivalence relation and the third is likely an appropriate one."],"url":"http://arxiv.org/abs/2405.19037v1"}
{"created":"2024-05-29 12:23:29","title":"A Good Foundation is Worth Many Labels: Label-Efficient Panoptic Segmentation","abstract":"A key challenge for the widespread application of learning-based models for robotic perception is to significantly reduce the required amount of annotated training data while achieving accurate predictions. This is essential not only to decrease operating costs but also to speed up deployment time. In this work, we address this challenge for PAnoptic SegmenTation with fEw Labels (PASTEL) by exploiting the groundwork paved by visual foundation models. We leverage descriptive image features from such a model to train two lightweight network heads for semantic segmentation and object boundary detection, using very few annotated training samples. We then merge their predictions via a novel fusion module that yields panoptic maps based on normalized cut. To further enhance the performance, we utilize self-training on unlabeled images selected by a feature-driven similarity scheme. We underline the relevance of our approach by employing PASTEL to important robot perception use cases from autonomous driving and agricultural robotics. In extensive experiments, we demonstrate that PASTEL significantly outperforms previous methods for label-efficient segmentation even when using fewer annotations. The code of our work is publicly available at http://pastel.cs.uni-freiburg.de.","sentences":["A key challenge for the widespread application of learning-based models for robotic perception is to significantly reduce the required amount of annotated training data while achieving accurate predictions.","This is essential not only to decrease operating costs but also to speed up deployment time.","In this work, we address this challenge for PAnoptic SegmenTation with fEw Labels (PASTEL) by exploiting the groundwork paved by visual foundation models.","We leverage descriptive image features from such a model to train two lightweight network heads for semantic segmentation and object boundary detection, using very few annotated training samples.","We then merge their predictions via a novel fusion module that yields panoptic maps based on normalized cut.","To further enhance the performance, we utilize self-training on unlabeled images selected by a feature-driven similarity scheme.","We underline the relevance of our approach by employing PASTEL to important robot perception use cases from autonomous driving and agricultural robotics.","In extensive experiments, we demonstrate that PASTEL significantly outperforms previous methods for label-efficient segmentation even when using fewer annotations.","The code of our work is publicly available at http://pastel.cs.uni-freiburg.de."],"url":"http://arxiv.org/abs/2405.19035v1"}
{"created":"2024-05-29 12:22:59","title":"CiliaGraph: Enabling Expression-enhanced Hyper-Dimensional Computation in Ultra-Lightweight and One-Shot Graph Classification on Edge","abstract":"Graph Neural Networks (GNNs) are computationally demanding and inefficient when applied to graph classification tasks in resource-constrained edge scenarios due to their inherent process, involving multiple rounds of forward and backward propagation. As a lightweight alternative, Hyper-Dimensional Computing (HDC), which leverages high-dimensional vectors for data encoding and processing, offers a more efficient solution by addressing computational bottleneck. However, current HDC methods primarily focus on static graphs and neglect to effectively capture node attributes and structural information, which leads to poor accuracy. In this work, we propose CiliaGraph, an enhanced expressive yet ultra-lightweight HDC model for graph classification. This model introduces a novel node encoding strategy that preserves relative distance isomorphism for accurate node connection representation. In addition, node distances are utilized as edge weights for information aggregation, and the encoded node attributes and structural information are concatenated to obtain a comprehensive graph representation. Furthermore, we explore the relationship between orthogonality and dimensionality to reduce the dimensions, thereby further enhancing computational efficiency. Compared to the SOTA GNNs, extensive experiments show that CiliaGraph reduces memory usage and accelerates training speed by an average of 292 times(up to 2341 times) and 103 times(up to 313 times) respectively while maintaining comparable accuracy.","sentences":["Graph Neural Networks (GNNs) are computationally demanding and inefficient when applied to graph classification tasks in resource-constrained edge scenarios due to their inherent process, involving multiple rounds of forward and backward propagation.","As a lightweight alternative, Hyper-Dimensional Computing (HDC), which leverages high-dimensional vectors for data encoding and processing, offers a more efficient solution by addressing computational bottleneck.","However, current HDC methods primarily focus on static graphs and neglect to effectively capture node attributes and structural information, which leads to poor accuracy.","In this work, we propose CiliaGraph, an enhanced expressive yet ultra-lightweight HDC model for graph classification.","This model introduces a novel node encoding strategy that preserves relative distance isomorphism for accurate node connection representation.","In addition, node distances are utilized as edge weights for information aggregation, and the encoded node attributes and structural information are concatenated to obtain a comprehensive graph representation.","Furthermore, we explore the relationship between orthogonality and dimensionality to reduce the dimensions, thereby further enhancing computational efficiency.","Compared to the SOTA GNNs, extensive experiments show that CiliaGraph reduces memory usage and accelerates training speed by an average of 292 times(up to 2341 times) and 103 times(up to 313 times) respectively while maintaining comparable accuracy."],"url":"http://arxiv.org/abs/2405.19033v1"}
{"created":"2024-05-29 12:18:32","title":"SynerGraph: An Integrated Graph Convolution Network for Multimodal Recommendation","abstract":"This article presents a novel approach to multimodal recommendation systems, focusing on integrating and purifying multimodal data. Our methodology starts by developing a filter to remove noise from various types of data, making the recommendations more reliable. We studied the impact of top-K sparsification on different datasets, finding optimal values that strike a balance between underfitting and overfitting concerns. The study emphasizes the significant role of textual information compared to visual data in providing a deep understanding of items. We conducted sensitivity analyses to understand how different modalities and the use of purifier circle loss affect the efficiency of the model. The findings indicate that systems that incorporate multiple modalities perform better than those relying on just one modality. Our approach highlights the importance of modality purifiers in filtering out irrelevant data, ensuring that user preferences remain relevant. Models without modality purifiers showed reduced performance, emphasizing the need for effective integration of pre-extracted features. The proposed model, which includes an novel self supervised auxiliary task, shows promise in accurately capturing user preferences. The main goal of the fusion technique is to enhance the modeling of user preferences by combining knowledge with item information, utilizing sophisticated language models. Extensive experiments show that our model produces better results than the existing state-of-the-art multimodal recommendation systems.","sentences":["This article presents a novel approach to multimodal recommendation systems, focusing on integrating and purifying multimodal data.","Our methodology starts by developing a filter to remove noise from various types of data, making the recommendations more reliable.","We studied the impact of top-K sparsification on different datasets, finding optimal values that strike a balance between underfitting and overfitting concerns.","The study emphasizes the significant role of textual information compared to visual data in providing a deep understanding of items.","We conducted sensitivity analyses to understand how different modalities and the use of purifier circle loss affect the efficiency of the model.","The findings indicate that systems that incorporate multiple modalities perform better than those relying on just one modality.","Our approach highlights the importance of modality purifiers in filtering out irrelevant data, ensuring that user preferences remain relevant.","Models without modality purifiers showed reduced performance, emphasizing the need for effective integration of pre-extracted features.","The proposed model, which includes an novel self supervised auxiliary task, shows promise in accurately capturing user preferences.","The main goal of the fusion technique is to enhance the modeling of user preferences by combining knowledge with item information, utilizing sophisticated language models.","Extensive experiments show that our model produces better results than the existing state-of-the-art multimodal recommendation systems."],"url":"http://arxiv.org/abs/2405.19031v1"}
{"created":"2024-05-29 12:12:09","title":"DiveR-CT: Diversity-enhanced Red Teaming with Relaxing Constraints","abstract":"Recent advances in large language models (LLMs) have made them indispensable, raising significant concerns over managing their safety. Automated red teaming offers a promising alternative to the labor-intensive and error-prone manual probing for vulnerabilities, providing more consistent and scalable safety evaluations. However, existing approaches often compromise diversity by focusing on maximizing attack success rate. Additionally, methods that decrease the cosine similarity from historical embeddings with semantic diversity rewards lead to novelty stagnation as history grows. To address these issues, we introduce DiveR-CT, which relaxes conventional constraints on the objective and semantic reward, granting greater freedom for the policy to enhance diversity. Our experiments demonstrate DiveR-CT's marked superiority over baselines by 1) generating data that perform better in various diversity metrics across different attack success rate levels, 2) better-enhancing resiliency in blue team models through safety tuning based on collected data, 3) allowing dynamic control of objective weights for reliable and controllable attack success rates, and 4) reducing susceptibility to reward overoptimization. Project details and code can be found at https://andrewzh112.github.io/#diverct.","sentences":["Recent advances in large language models (LLMs) have made them indispensable, raising significant concerns over managing their safety.","Automated red teaming offers a promising alternative to the labor-intensive and error-prone manual probing for vulnerabilities, providing more consistent and scalable safety evaluations.","However, existing approaches often compromise diversity by focusing on maximizing attack success rate.","Additionally, methods that decrease the cosine similarity from historical embeddings with semantic diversity rewards lead to novelty stagnation as history grows.","To address these issues, we introduce DiveR-CT, which relaxes conventional constraints on the objective and semantic reward, granting greater freedom for the policy to enhance diversity.","Our experiments demonstrate DiveR-CT's marked superiority over baselines by 1) generating data that perform better in various diversity metrics across different attack success rate levels, 2) better-enhancing resiliency in blue team models through safety tuning based on collected data, 3) allowing dynamic control of objective weights for reliable and controllable attack success rates, and 4) reducing susceptibility to reward overoptimization.","Project details and code can be found at https://andrewzh112.github.io/#diverct."],"url":"http://arxiv.org/abs/2405.19026v1"}
{"created":"2024-05-29 11:53:07","title":"Trust the Model Where It Trusts Itself -- Model-Based Actor-Critic with Uncertainty-Aware Rollout Adaption","abstract":"Dyna-style model-based reinforcement learning (MBRL) combines model-free agents with predictive transition models through model-based rollouts. This combination raises a critical question: 'When to trust your model?'; i.e., which rollout length results in the model providing useful data? Janner et al. (2019) address this question by gradually increasing rollout lengths throughout the training. While theoretically tempting, uniform model accuracy is a fallacy that collapses at the latest when extrapolating. Instead, we propose asking the question 'Where to trust your model?'. Using inherent model uncertainty to consider local accuracy, we obtain the Model-Based Actor-Critic with Uncertainty-Aware Rollout Adaption (MACURA) algorithm. We propose an easy-to-tune rollout mechanism and demonstrate substantial improvements in data efficiency and performance compared to state-of-the-art deep MBRL methods on the MuJoCo benchmark.","sentences":["Dyna-style model-based reinforcement learning (MBRL) combines model-free agents with predictive transition models through model-based rollouts.","This combination raises a critical question: 'When to trust your model?'; i.e., which rollout length results in the model providing useful data?","Janner et al. (2019) address this question by gradually increasing rollout lengths throughout the training.","While theoretically tempting, uniform model accuracy is a fallacy that collapses at the latest when extrapolating.","Instead, we propose asking the question 'Where to trust your model?'.","Using inherent model uncertainty to consider local accuracy, we obtain the Model-Based Actor-Critic with Uncertainty-Aware Rollout Adaption (MACURA) algorithm.","We propose an easy-to-tune rollout mechanism and demonstrate substantial improvements in data efficiency and performance compared to state-of-the-art deep MBRL methods on the MuJoCo benchmark."],"url":"http://arxiv.org/abs/2405.19014v1"}
{"created":"2024-05-29 11:51:33","title":"Implicit Neural Image Field for Biological Microscopy Image Compression","abstract":"The rapid pace of innovation in biological microscopy imaging has led to large images, putting pressure on data storage and impeding efficient sharing, management, and visualization. This necessitates the development of efficient compression solutions. Traditional CODEC methods struggle to adapt to the diverse bioimaging data and often suffer from sub-optimal compression. In this study, we propose an adaptive compression workflow based on Implicit Neural Representation (INR). This approach permits application-specific compression objectives, capable of compressing images of any shape and arbitrary pixel-wise decompression. We demonstrated on a wide range of microscopy images from real applications that our workflow not only achieved high, controllable compression ratios (e.g., 512x) but also preserved detailed information critical for downstream analysis.","sentences":["The rapid pace of innovation in biological microscopy imaging has led to large images, putting pressure on data storage and impeding efficient sharing, management, and visualization.","This necessitates the development of efficient compression solutions.","Traditional CODEC methods struggle to adapt to the diverse bioimaging data and often suffer from sub-optimal compression.","In this study, we propose an adaptive compression workflow based on Implicit Neural Representation (INR).","This approach permits application-specific compression objectives, capable of compressing images of any shape and arbitrary pixel-wise decompression.","We demonstrated on a wide range of microscopy images from real applications that our workflow not only achieved high, controllable compression ratios (e.g., 512x) but also preserved detailed information critical for downstream analysis."],"url":"http://arxiv.org/abs/2405.19012v1"}
{"created":"2024-05-29 11:48:27","title":"Evaluating the External and Parametric Knowledge Fusion of Large Language Models","abstract":"Integrating external knowledge into large language models (LLMs) presents a promising solution to overcome the limitations imposed by their antiquated and static parametric memory. Prior studies, however, have tended to over-reliance on external knowledge, underestimating the valuable contributions of an LLMs' intrinsic parametric knowledge. The efficacy of LLMs in blending external and parametric knowledge remains largely unexplored, especially in cases where external knowledge is incomplete and necessitates supplementation by their parametric knowledge. We propose to deconstruct knowledge fusion into four distinct scenarios, offering the first thorough investigation of LLM behavior across each. We develop a systematic pipeline for data construction and knowledge infusion to simulate these fusion scenarios, facilitating a series of controlled experiments. Our investigation reveals that enhancing parametric knowledge within LLMs can significantly bolster their capability for knowledge integration. Nonetheless, we identify persistent challenges in memorizing and eliciting parametric knowledge, and determining parametric knowledge boundaries. Our findings aim to steer future explorations on harmonizing external and parametric knowledge within LLMs.","sentences":["Integrating external knowledge into large language models (LLMs) presents a promising solution to overcome the limitations imposed by their antiquated and static parametric memory.","Prior studies, however, have tended to over-reliance on external knowledge, underestimating the valuable contributions of an LLMs' intrinsic parametric knowledge.","The efficacy of LLMs in blending external and parametric knowledge remains largely unexplored, especially in cases where external knowledge is incomplete and necessitates supplementation by their parametric knowledge.","We propose to deconstruct knowledge fusion into four distinct scenarios, offering the first thorough investigation of LLM behavior across each.","We develop a systematic pipeline for data construction and knowledge infusion to simulate these fusion scenarios, facilitating a series of controlled experiments.","Our investigation reveals that enhancing parametric knowledge within LLMs can significantly bolster their capability for knowledge integration.","Nonetheless, we identify persistent challenges in memorizing and eliciting parametric knowledge, and determining parametric knowledge boundaries.","Our findings aim to steer future explorations on harmonizing external and parametric knowledge within LLMs."],"url":"http://arxiv.org/abs/2405.19010v1"}
{"created":"2024-05-29 11:28:06","title":"FedMAP: Unlocking Potential in Personalized Federated Learning through Bi-Level MAP Optimization","abstract":"Federated Learning (FL) enables collaborative training of machine learning models on decentralized data while preserving data privacy. However, data across clients often differs significantly due to class imbalance, feature distribution skew, sample size imbalance, and other phenomena. Leveraging information from these not identically distributed (non-IID) datasets poses substantial challenges. FL methods based on a single global model cannot effectively capture the variations in client data and underperform in non-IID settings. Consequently, Personalized FL (PFL) approaches that adapt to each client's data distribution but leverage other clients' data are essential but currently underexplored. We propose a novel Bayesian PFL framework using bi-level optimization to tackle the data heterogeneity challenges. Our proposed framework utilizes the global model as a prior distribution within a Maximum A Posteriori (MAP) estimation of personalized client models. This approach facilitates PFL by integrating shared knowledge from the prior, thereby enhancing local model performance, generalization ability, and communication efficiency. We extensively evaluated our bi-level optimization approach on real-world and synthetic datasets, demonstrating significant improvements in model accuracy compared to existing methods while reducing communication overhead. This study contributes to PFL by establishing a solid theoretical foundation for the proposed method and offering a robust, ready-to-use framework that effectively addresses the challenges posed by non-IID data in FL.","sentences":["Federated Learning (FL) enables collaborative training of machine learning models on decentralized data while preserving data privacy.","However, data across clients often differs significantly due to class imbalance, feature distribution skew, sample size imbalance, and other phenomena.","Leveraging information from these not identically distributed (non-IID) datasets poses substantial challenges.","FL methods based on a single global model cannot effectively capture the variations in client data and underperform in non-IID settings.","Consequently, Personalized FL (PFL) approaches that adapt to each client's data distribution but leverage other clients' data are essential but currently underexplored.","We propose a novel Bayesian PFL framework using bi-level optimization to tackle the data heterogeneity challenges.","Our proposed framework utilizes the global model as a prior distribution within a Maximum A Posteriori (MAP) estimation of personalized client models.","This approach facilitates PFL by integrating shared knowledge from the prior, thereby enhancing local model performance, generalization ability, and communication efficiency.","We extensively evaluated our bi-level optimization approach on real-world and synthetic datasets, demonstrating significant improvements in model accuracy compared to existing methods while reducing communication overhead.","This study contributes to PFL by establishing a solid theoretical foundation for the proposed method and offering a robust, ready-to-use framework that effectively addresses the challenges posed by non-IID data in FL."],"url":"http://arxiv.org/abs/2405.19000v1"}
{"created":"2024-05-29 11:15:12","title":"ParsEval: Evaluation of Parsing Behavior using Real-world Out-in-the-wild X.509 Certificates","abstract":"X.509 certificates play a crucial role in establishing secure communication over the internet by enabling authentication and data integrity. Equipped with a rich feature set, the X.509 standard is defined by multiple, comprehensive ISO/IEC documents. Due to its internet-wide usage, there are different implementations in multiple programming languages leading to a large and fragmented ecosystem. This work addresses the research question \"Are there user-visible and security-related differences between X.509 certificate parsers?\". Relevant libraries offering APIs for parsing X.509 certificates were investigated and an appropriate test suite was developed. From 34 libraries 6 were chosen for further analysis. The X.509 parsing modules of the chosen libraries were called with 186,576,846 different certificates from a real-world dataset and the observed error codes were investigated. This study reveals an anomaly in wolfSSL's X.509 parsing module and that there are fundamental differences in the ecosystem. While related studies nowadays mostly focus on fuzzing techniques resulting in artificial certificates, this study confirms that available X.509 parsing modules differ largely and yield different results, even for real-world out-in-the-wild certificates.","sentences":["X.509 certificates play a crucial role in establishing secure communication over the internet by enabling authentication and data integrity.","Equipped with a rich feature set, the X.509 standard is defined by multiple, comprehensive ISO/IEC documents.","Due to its internet-wide usage, there are different implementations in multiple programming languages leading to a large and fragmented ecosystem.","This work addresses the research question \"Are there user-visible and security-related differences between X.509 certificate parsers?\".","Relevant libraries offering APIs for parsing X.509 certificates were investigated and an appropriate test suite was developed.","From 34 libraries 6 were chosen for further analysis.","The X.509 parsing modules of the chosen libraries were called with 186,576,846 different certificates from a real-world dataset and the observed error codes were investigated.","This study reveals an anomaly in wolfSSL's X.509 parsing module and that there are fundamental differences in the ecosystem.","While related studies nowadays mostly focus on fuzzing techniques resulting in artificial certificates, this study confirms that available X.509 parsing modules differ largely and yield different results, even for real-world out-in-the-wild certificates."],"url":"http://arxiv.org/abs/2405.18993v1"}
{"created":"2024-05-29 11:11:07","title":"EasyAnimate: A High-Performance Long Video Generation Method based on Transformer Architecture","abstract":"This paper presents EasyAnimate, an advanced method for video generation that leverages the power of transformer architecture for high-performance outcomes. We have expanded the DiT framework originally designed for 2D image synthesis to accommodate the complexities of 3D video generation by incorporating a motion module block. It is used to capture temporal dynamics, thereby ensuring the production of consistent frames and seamless motion transitions. The motion module can be adapted to various DiT baseline methods to generate video with different styles. It can also generate videos with different frame rates and resolutions during both training and inference phases, suitable for both images and videos. Moreover, we introduce slice VAE, a novel approach to condense the temporal axis, facilitating the generation of long duration videos. Currently, EasyAnimate exhibits the proficiency to generate videos with 144 frames. We provide a holistic ecosystem for video production based on DiT, encompassing aspects such as data pre-processing, VAE training, DiT models training (both the baseline model and LoRA model), and end-to-end video inference. Code is available at: https://github.com/aigc-apps/EasyAnimate. We are continuously working to enhance the performance of our method.","sentences":["This paper presents EasyAnimate, an advanced method for video generation that leverages the power of transformer architecture for high-performance outcomes.","We have expanded the DiT framework originally designed for 2D image synthesis to accommodate the complexities of 3D video generation by incorporating a motion module block.","It is used to capture temporal dynamics, thereby ensuring the production of consistent frames and seamless motion transitions.","The motion module can be adapted to various DiT baseline methods to generate video with different styles.","It can also generate videos with different frame rates and resolutions during both training and inference phases, suitable for both images and videos.","Moreover, we introduce slice VAE, a novel approach to condense the temporal axis, facilitating the generation of long duration videos.","Currently, EasyAnimate exhibits the proficiency to generate videos with 144 frames.","We provide a holistic ecosystem for video production based on DiT, encompassing aspects such as data pre-processing, VAE training, DiT models training (both the baseline model and LoRA model), and end-to-end video inference.","Code is available at: https://github.com/aigc-apps/EasyAnimate.","We are continuously working to enhance the performance of our method."],"url":"http://arxiv.org/abs/2405.18991v1"}
{"created":"2024-05-29 10:56:13","title":"Federated Learning under Partially Class-Disjoint Data via Manifold Reshaping","abstract":"Statistical heterogeneity severely limits the performance of federated learning (FL), motivating several explorations e.g., FedProx, MOON and FedDyn, to alleviate this problem. Despite effectiveness, their considered scenario generally requires samples from almost all classes during the local training of each client, although some covariate shifts may exist among clients. In fact, the natural case of partially class-disjoint data (PCDD), where each client contributes a few classes (instead of all classes) of samples, is practical yet underexplored. Specifically, the unique collapse and invasion characteristics of PCDD can induce the biased optimization direction in local training, which prevents the efficiency of federated learning. To address this dilemma, we propose a manifold reshaping approach called FedMR to calibrate the feature space of local training. Our FedMR adds two interplaying losses to the vanilla federated learning: one is intra-class loss to decorrelate feature dimensions for anti-collapse; and the other one is inter-class loss to guarantee the proper margin among categories in the feature expansion. We conduct extensive experiments on a range of datasets to demonstrate that our FedMR achieves much higher accuracy and better communication efficiency. Source code is available at: https://github.com/MediaBrain-SJTU/FedMR.git.","sentences":["Statistical heterogeneity severely limits the performance of federated learning (FL), motivating several explorations e.g., FedProx, MOON and FedDyn, to alleviate this problem.","Despite effectiveness, their considered scenario generally requires samples from almost all classes during the local training of each client, although some covariate shifts may exist among clients.","In fact, the natural case of partially class-disjoint data (PCDD), where each client contributes a few classes (instead of all classes) of samples, is practical yet underexplored.","Specifically, the unique collapse and invasion characteristics of PCDD can induce the biased optimization direction in local training, which prevents the efficiency of federated learning.","To address this dilemma, we propose a manifold reshaping approach called FedMR to calibrate the feature space of local training.","Our FedMR adds two interplaying losses to the vanilla federated learning: one is intra-class loss to decorrelate feature dimensions for anti-collapse; and the other one is inter-class loss to guarantee the proper margin among categories in the feature expansion.","We conduct extensive experiments on a range of datasets to demonstrate that our FedMR achieves much higher accuracy and better communication efficiency.","Source code is available at: https://github.com/MediaBrain-SJTU/FedMR.git."],"url":"http://arxiv.org/abs/2405.18983v1"}
{"created":"2024-05-29 10:45:06","title":"MANO: Exploiting Matrix Norm for Unsupervised Accuracy Estimation Under Distribution Shifts","abstract":"Leveraging the models' outputs, specifically the logits, is a common approach to estimating the test accuracy of a pre-trained neural network on out-of-distribution (OOD) samples without requiring access to the corresponding ground truth labels. Despite their ease of implementation and computational efficiency, current logit-based methods are vulnerable to overconfidence issues, leading to prediction bias, especially under the natural shift. In this work, we first study the relationship between logits and generalization performance from the view of low-density separation assumption. Our findings motivate our proposed method MaNo which (1) applies a data-dependent normalization on the logits to reduce prediction bias, and (2) takes the $L_p$ norm of the matrix of normalized logits as the estimation score. Our theoretical analysis highlights the connection between the provided score and the model's uncertainty. We conduct an extensive empirical study on common unsupervised accuracy estimation benchmarks and demonstrate that MaNo achieves state-of-the-art performance across various architectures in the presence of synthetic, natural, or subpopulation shifts.","sentences":["Leveraging the models' outputs, specifically the logits, is a common approach to estimating the test accuracy of a pre-trained neural network on out-of-distribution (OOD) samples without requiring access to the corresponding ground truth labels.","Despite their ease of implementation and computational efficiency, current logit-based methods are vulnerable to overconfidence issues, leading to prediction bias, especially under the natural shift.","In this work, we first study the relationship between logits and generalization performance from the view of low-density separation assumption.","Our findings motivate our proposed method MaNo which (1) applies a data-dependent normalization on the logits to reduce prediction bias, and (2) takes the $L_p$ norm of the matrix of normalized logits as the estimation score.","Our theoretical analysis highlights the connection between the provided score and the model's uncertainty.","We conduct an extensive empirical study on common unsupervised accuracy estimation benchmarks and demonstrate that MaNo achieves state-of-the-art performance across various architectures in the presence of synthetic, natural, or subpopulation shifts."],"url":"http://arxiv.org/abs/2405.18979v1"}
{"created":"2024-05-29 10:38:25","title":"Hierarchical Classification Auxiliary Network for Time Series Forecasting","abstract":"Deep learning has significantly advanced time series forecasting through its powerful capacity to capture sequence relationships. However, training these models with the Mean Square Error (MSE) loss often results in over-smooth predictions, making it challenging to handle the complexity and learn high-entropy features from time series data with high variability and unpredictability. In this work, we introduce a novel approach by tokenizing time series values to train forecasting models via cross-entropy loss, while considering the continuous nature of time series data. Specifically, we propose Hierarchical Classification Auxiliary Network, HCAN, a general model-agnostic component that can be integrated with any forecasting model. HCAN is based on a Hierarchy-Aware Attention module that integrates multi-granularity high-entropy features at different hierarchy levels. At each level, we assign a class label for timesteps to train an Uncertainty-Aware Classifier. This classifier mitigates the over-confidence in softmax loss via evidence theory. We also implement a Hierarchical Consistency Loss to maintain prediction consistency across hierarchy levels. Extensive experiments integrating HCAN with state-of-the-art forecasting models demonstrate substantial improvements over baselines on several real-world datasets. Code is available at:https://github.com/syrGitHub/HCAN.","sentences":["Deep learning has significantly advanced time series forecasting through its powerful capacity to capture sequence relationships.","However, training these models with the Mean Square Error (MSE) loss often results in over-smooth predictions, making it challenging to handle the complexity and learn high-entropy features from time series data with high variability and unpredictability.","In this work, we introduce a novel approach by tokenizing time series values to train forecasting models via cross-entropy loss, while considering the continuous nature of time series data.","Specifically, we propose Hierarchical Classification Auxiliary Network, HCAN, a general model-agnostic component that can be integrated with any forecasting model.","HCAN is based on a Hierarchy-Aware Attention module that integrates multi-granularity high-entropy features at different hierarchy levels.","At each level, we assign a class label for timesteps to train an Uncertainty-Aware Classifier.","This classifier mitigates the over-confidence in softmax loss via evidence theory.","We also implement a Hierarchical Consistency Loss to maintain prediction consistency across hierarchy levels.","Extensive experiments integrating HCAN with state-of-the-art forecasting models demonstrate substantial improvements over baselines on several real-world datasets.","Code is available at:https://github.com/syrGitHub/HCAN."],"url":"http://arxiv.org/abs/2405.18975v1"}
{"created":"2024-05-29 10:34:44","title":"Federated Learning with Bilateral Curation for Partially Class-Disjoint Data","abstract":"Partially class-disjoint data (PCDD), a common yet under-explored data formation where each client contributes a part of classes (instead of all classes) of samples, severely challenges the performance of federated algorithms. Without full classes, the local objective will contradict the global objective, yielding the angle collapse problem for locally missing classes and the space waste problem for locally existing classes. As far as we know, none of the existing methods can intrinsically mitigate PCDD challenges to achieve holistic improvement in the bilateral views (both global view and local view) of federated learning. To address this dilemma, we are inspired by the strong generalization of simplex Equiangular Tight Frame~(ETF) on the imbalanced data, and propose a novel approach called FedGELA where the classifier is globally fixed as a simplex ETF while locally adapted to the personal distributions. Globally, FedGELA provides fair and equal discrimination for all classes and avoids inaccurate updates of the classifier, while locally it utilizes the space of locally missing classes for locally existing classes. We conduct extensive experiments on a range of datasets to demonstrate that our FedGELA achieves promising performance~(averaged improvement of 3.9% to FedAvg and 1.5% to best baselines) and provide both local and global convergence guarantees. Source code is available at:https://github.com/MediaBrain-SJTU/FedGELA.git.","sentences":["Partially class-disjoint data (PCDD), a common yet under-explored data formation where each client contributes a part of classes (instead of all classes) of samples, severely challenges the performance of federated algorithms.","Without full classes, the local objective will contradict the global objective, yielding the angle collapse problem for locally missing classes and the space waste problem for locally existing classes.","As far as we know, none of the existing methods can intrinsically mitigate PCDD challenges to achieve holistic improvement in the bilateral views (both global view and local view) of federated learning.","To address this dilemma, we are inspired by the strong generalization of simplex Equiangular Tight Frame~(ETF) on the imbalanced data, and propose a novel approach called FedGELA where the classifier is globally fixed as a simplex ETF while locally adapted to the personal distributions.","Globally, FedGELA provides fair and equal discrimination for all classes and avoids inaccurate updates of the classifier, while locally it utilizes the space of locally missing classes for locally existing classes.","We conduct extensive experiments on a range of datasets to demonstrate that our FedGELA achieves promising performance~(averaged improvement of 3.9% to FedAvg and 1.5% to best baselines) and provide both local and global convergence guarantees.","Source code is available at:https://github.com/MediaBrain-SJTU/FedGELA.git."],"url":"http://arxiv.org/abs/2405.18972v1"}
{"created":"2024-05-29 10:26:16","title":"UniIF: Unified Molecule Inverse Folding","abstract":"Molecule inverse folding has been a long-standing challenge in chemistry and biology, with the potential to revolutionize drug discovery and material science. Despite specified models have been proposed for different small- or macro-molecules, few have attempted to unify the learning process, resulting in redundant efforts. Complementary to recent advancements in molecular structure prediction, such as RoseTTAFold All-Atom and AlphaFold3, we propose the unified model UniIF for the inverse folding of all molecules. We do such unification in two levels: 1) Data-Level: We propose a unified block graph data form for all molecules, including the local frame building and geometric feature initialization. 2) Model-Level: We introduce a geometric block attention network, comprising a geometric interaction, interactive attention and virtual long-term dependency modules, to capture the 3D interactions of all molecules. Through comprehensive evaluations across various tasks such as protein design, RNA design, and material design, we demonstrate that our proposed method surpasses state-of-the-art methods on all tasks. UniIF offers a versatile and effective solution for general molecule inverse folding.","sentences":["Molecule inverse folding has been a long-standing challenge in chemistry and biology, with the potential to revolutionize drug discovery and material science.","Despite specified models have been proposed for different small- or macro-molecules, few have attempted to unify the learning process, resulting in redundant efforts.","Complementary to recent advancements in molecular structure prediction, such as RoseTTAFold All-Atom and AlphaFold3, we propose the unified model UniIF for the inverse folding of all molecules.","We do such unification in two levels: 1) Data-Level: We propose a unified block graph data form for all molecules, including the local frame building and geometric feature initialization.","2) Model-Level: We introduce a geometric block attention network, comprising a geometric interaction, interactive attention and virtual long-term dependency modules, to capture the 3D interactions of all molecules.","Through comprehensive evaluations across various tasks such as protein design, RNA design, and material design, we demonstrate that our proposed method surpasses state-of-the-art methods on all tasks.","UniIF offers a versatile and effective solution for general molecule inverse folding."],"url":"http://arxiv.org/abs/2405.18968v1"}
{"created":"2024-05-29 10:19:11","title":"Transcending Fusion: A Multi-Scale Alignment Method for Remote Sensing Image-Text Retrieval","abstract":"Remote Sensing Image-Text Retrieval (RSITR) is pivotal for knowledge services and data mining in the remote sensing (RS) domain. Considering the multi-scale representations in image content and text vocabulary can enable the models to learn richer representations and enhance retrieval. Current multi-scale RSITR approaches typically align multi-scale fused image features with text features, but overlook aligning image-text pairs at distinct scales separately. This oversight restricts their ability to learn joint representations suitable for effective retrieval. We introduce a novel Multi-Scale Alignment (MSA) method to overcome this limitation. Our method comprises three key innovations: (1) Multi-scale Cross-Modal Alignment Transformer (MSCMAT), which computes cross-attention between single-scale image features and localized text features, integrating global textual context to derive a matching score matrix within a mini-batch, (2) a multi-scale cross-modal semantic alignment loss that enforces semantic alignment across scales, and (3) a cross-scale multi-modal semantic consistency loss that uses the matching matrix from the largest scale to guide alignment at smaller scales. We evaluated our method across multiple datasets, demonstrating its efficacy with various visual backbones and establishing its superiority over existing state-of-the-art methods. The GitHub URL for our project is: https://github.com/yr666666/MSA","sentences":["Remote Sensing Image-Text Retrieval (RSITR) is pivotal for knowledge services and data mining in the remote sensing (RS) domain.","Considering the multi-scale representations in image content and text vocabulary can enable the models to learn richer representations and enhance retrieval.","Current multi-scale RSITR approaches typically align multi-scale fused image features with text features, but overlook aligning image-text pairs at distinct scales separately.","This oversight restricts their ability to learn joint representations suitable for effective retrieval.","We introduce a novel Multi-Scale Alignment (MSA) method to overcome this limitation.","Our method comprises three key innovations: (1) Multi-scale Cross-Modal Alignment Transformer (MSCMAT), which computes cross-attention between single-scale image features and localized text features, integrating global textual context to derive a matching score matrix within a mini-batch, (2) a multi-scale cross-modal semantic alignment loss that enforces semantic alignment across scales, and (3) a cross-scale multi-modal semantic consistency loss that uses the matching matrix from the largest scale to guide alignment at smaller scales.","We evaluated our method across multiple datasets, demonstrating its efficacy with various visual backbones and establishing its superiority over existing state-of-the-art methods.","The GitHub URL for our project is: https://github.com/yr666666/MSA"],"url":"http://arxiv.org/abs/2405.18959v1"}
{"created":"2024-05-29 10:11:10","title":"MAGIC: Modular Auto-encoder for Generalisable Model Inversion with Bias Corrections","abstract":"Scientists often model physical processes to understand the natural world and uncover the causation behind observations. Due to unavoidable simplification, discrepancies often arise between model predictions and actual observations, in the form of systematic biases, whose impact varies with model completeness. Classical model inversion methods such as Bayesian inference or regressive neural networks tend either to overlook biases or make assumptions about their nature during data preprocessing, potentially leading to implausible results. Inspired by recent work in inverse graphics, we replace the decoder stage of a standard autoencoder with a physical model followed by a bias-correction layer. This generalisable approach simultaneously inverts the model and corrects its biases in an end-to-end manner without making strong assumptions about the nature of the biases. We demonstrate the effectiveness of our approach using two physical models from disparate domains: a complex radiative transfer model from remote sensing; and a volcanic deformation model from geodesy. Our method matches or surpasses results from classical approaches without requiring biases to be explicitly filtered out, suggesting an effective pathway for understanding the causation of various physical processes.","sentences":["Scientists often model physical processes to understand the natural world and uncover the causation behind observations.","Due to unavoidable simplification, discrepancies often arise between model predictions and actual observations, in the form of systematic biases, whose impact varies with model completeness.","Classical model inversion methods such as Bayesian inference or regressive neural networks tend either to overlook biases or make assumptions about their nature during data preprocessing, potentially leading to implausible results.","Inspired by recent work in inverse graphics, we replace the decoder stage of a standard autoencoder with a physical model followed by a bias-correction layer.","This generalisable approach simultaneously inverts the model and corrects its biases in an end-to-end manner without making strong assumptions about the nature of the biases.","We demonstrate the effectiveness of our approach using two physical models from disparate domains: a complex radiative transfer model from remote sensing; and a volcanic deformation model from geodesy.","Our method matches or surpasses results from classical approaches without requiring biases to be explicitly filtered out, suggesting an effective pathway for understanding the causation of various physical processes."],"url":"http://arxiv.org/abs/2405.18953v1"}
{"created":"2024-05-29 10:03:57","title":"Learning to Recover from Plan Execution Errors during Robot Manipulation: A Neuro-symbolic Approach","abstract":"Automatically detecting and recovering from failures is an important but challenging problem for autonomous robots. Most of the recent work on learning to plan from demonstrations lacks the ability to detect and recover from errors in the absence of an explicit state representation and/or a (sub-) goal check function. We propose an approach (blending learning with symbolic search) for automated error discovery and recovery, without needing annotated data of failures. Central to our approach is a neuro-symbolic state representation, in the form of dense scene graph, structured based on the objects present within the environment. This enables efficient learning of the transition function and a discriminator that not only identifies failures but also localizes them facilitating fast re-planning via computation of heuristic distance function. We also present an anytime version of our algorithm, where instead of recovering to the last correct state, we search for a sub-goal in the original plan minimizing the total distance to the goal given a re-planning budget. Experiments on a physics simulator with a variety of simulated failures show the effectiveness of our approach compared to existing baselines, both in terms of efficiency as well as accuracy of our recovery mechanism.","sentences":["Automatically detecting and recovering from failures is an important but challenging problem for autonomous robots.","Most of the recent work on learning to plan from demonstrations lacks the ability to detect and recover from errors in the absence of an explicit state representation and/or a (sub-) goal check function.","We propose an approach (blending learning with symbolic search) for automated error discovery and recovery, without needing annotated data of failures.","Central to our approach is a neuro-symbolic state representation, in the form of dense scene graph, structured based on the objects present within the environment.","This enables efficient learning of the transition function and a discriminator that not only identifies failures but also localizes them facilitating fast re-planning via computation of heuristic distance function.","We also present an anytime version of our algorithm, where instead of recovering to the last correct state, we search for a sub-goal in the original plan minimizing the total distance to the goal given a re-planning budget.","Experiments on a physics simulator with a variety of simulated failures show the effectiveness of our approach compared to existing baselines, both in terms of efficiency as well as accuracy of our recovery mechanism."],"url":"http://arxiv.org/abs/2405.18948v1"}
{"created":"2024-05-29 09:50:43","title":"Verifiably Robust Conformal Prediction","abstract":"Conformal Prediction (CP) is a popular uncertainty quantification method that provides distribution-free, statistically valid prediction sets, assuming that training and test data are exchangeable. In such a case, CP's prediction sets are guaranteed to cover the (unknown) true test output with a user-specified probability. Nevertheless, this guarantee is violated when the data is subjected to adversarial attacks, which often result in a significant loss of coverage. Recently, several approaches have been put forward to recover CP guarantees in this setting. These approaches leverage variations of randomised smoothing to produce conservative sets which account for the effect of the adversarial perturbations. They are, however, limited in that they only support $\\ell^2$-bounded perturbations and classification tasks. This paper introduces \\emph{VRCP (Verifiably Robust Conformal Prediction)}, a new framework that leverages recent neural network verification methods to recover coverage guarantees under adversarial attacks. Our VRCP method is the first to support perturbations bounded by arbitrary norms including $\\ell^1$, $\\ell^2$, and $\\ell^\\infty$, as well as regression tasks. We evaluate and compare our approach on image classification tasks (CIFAR10, CIFAR100, and TinyImageNet) and regression tasks for deep reinforcement learning environments. In every case, VRCP achieves above nominal coverage and yields significantly more efficient and informative prediction regions than the SotA.","sentences":["Conformal Prediction (CP) is a popular uncertainty quantification method that provides distribution-free, statistically valid prediction sets, assuming that training and test data are exchangeable.","In such a case, CP's prediction sets are guaranteed to cover the (unknown) true test output with a user-specified probability.","Nevertheless, this guarantee is violated when the data is subjected to adversarial attacks, which often result in a significant loss of coverage.","Recently, several approaches have been put forward to recover CP guarantees in this setting.","These approaches leverage variations of randomised smoothing to produce conservative sets which account for the effect of the adversarial perturbations.","They are, however, limited in that they only support $\\ell^2$-bounded perturbations and classification tasks.","This paper introduces \\emph{VRCP (Verifiably Robust Conformal Prediction)}, a new framework that leverages recent neural network verification methods to recover coverage guarantees under adversarial attacks.","Our VRCP method is the first to support perturbations bounded by arbitrary norms including $\\ell^1$, $\\ell^2$, and $\\ell^\\infty$, as well as regression tasks.","We evaluate and compare our approach on image classification tasks (CIFAR10, CIFAR100, and TinyImageNet) and regression tasks for deep reinforcement learning environments.","In every case, VRCP achieves above nominal coverage and yields significantly more efficient and informative prediction regions than the SotA."],"url":"http://arxiv.org/abs/2405.18942v1"}
{"created":"2024-05-29 09:50:39","title":"Content-Agnostic Moderation for Stance-Neutral Recommendation","abstract":"Personalized recommendation systems often drive users towards more extreme content, exacerbating opinion polarization. While (content-aware) moderation has been proposed to mitigate these effects, such approaches risk curtailing the freedom of speech and of information. To address this concern, we propose and explore the feasibility of \\emph{content-agnostic} moderation as an alternative approach for reducing polarization. Content-agnostic moderation does not rely on the actual content being moderated, arguably making it less prone to forms of censorship. We establish theoretically that content-agnostic moderation cannot be guaranteed to work in a fully generic setting. However, we show that it can often be effectively achieved in practice with plausible assumptions. We introduce two novel content-agnostic moderation methods that modify the recommendations from the content recommender to disperse user-item co-clusters without relying on content features.   To evaluate the potential of content-agnostic moderation in controlled experiments, we built a simulation environment to analyze the closed-loop behavior of a system with a given set of users, recommendation system, and moderation approach. Through comprehensive experiments in this environment, we show that our proposed moderation methods significantly enhance stance neutrality and maintain high recommendation quality across various data scenarios. Our results indicate that achieving stance neutrality without direct content information is not only feasible but can also help in developing more balanced and informative recommendation systems without substantially degrading user engagement.","sentences":["Personalized recommendation systems often drive users towards more extreme content, exacerbating opinion polarization.","While (content-aware) moderation has been proposed to mitigate these effects, such approaches risk curtailing the freedom of speech and of information.","To address this concern, we propose and explore the feasibility of \\emph{content-agnostic} moderation as an alternative approach for reducing polarization.","Content-agnostic moderation does not rely on the actual content being moderated, arguably making it less prone to forms of censorship.","We establish theoretically that content-agnostic moderation cannot be guaranteed to work in a fully generic setting.","However, we show that it can often be effectively achieved in practice with plausible assumptions.","We introduce two novel content-agnostic moderation methods that modify the recommendations from the content recommender to disperse user-item co-clusters without relying on content features.   ","To evaluate the potential of content-agnostic moderation in controlled experiments, we built a simulation environment to analyze the closed-loop behavior of a system with a given set of users, recommendation system, and moderation approach.","Through comprehensive experiments in this environment, we show that our proposed moderation methods significantly enhance stance neutrality and maintain high recommendation quality across various data scenarios.","Our results indicate that achieving stance neutrality without direct content information is not only feasible but can also help in developing more balanced and informative recommendation systems without substantially degrading user engagement."],"url":"http://arxiv.org/abs/2405.18941v1"}
{"created":"2024-05-29 09:37:23","title":"LSPI: Heterogeneous Graph Neural Network Classification Aggregation Algorithm Based on Size Neighbor Path Identification","abstract":"Existing heterogeneous graph neural network algorithms (HGNNs) mostly rely on meta-paths to capture the rich semantic information contained in heterogeneous graphs (also known as heterogeneous information networks (HINs)), but most of these HGNNs focus on different ways of feature aggre gation and ignore the properties of the meta-paths themselves. This paper studies meta-paths in three commonly used data sets and finds that there are huge differences in the number of neighbors connected by different meta paths. At the same time, the noise information contained in large neigh bor paths will have an adverse impact on model performance. Therefore, this paper proposes a Heterogeneous Graph Neural Network Classification and Aggregation Algorithm Based on Large and Small Neighbor Path Iden tification(LSPI). LSPI firstly divides the meta-paths into large and small neighbor paths through the path discriminator , and in order to reduce the noise interference problem in large neighbor paths, LSPI selects neighbor nodes with higher similarity from both topology and feature perspectives, and passes small neighbor paths and filtered large neighbor paths through different graph convolution components. Aggregation is performed to obtain feature information under different subgraphs, and then LSPI uses subgraph level attention to fuse the feature information under different subgraphs to generate the final node embedding. Finally this paper verifies the superiority of the method through extensive experiments and also gives suggestions on the number of nodes to be retained in large neighbor paths through exper iments. The complete reproducible code adn data has been published at: https://github.com/liuhua811/LSPIA.","sentences":["Existing heterogeneous graph neural network algorithms (HGNNs) mostly rely on meta-paths to capture the rich semantic information contained in heterogeneous graphs (also known as heterogeneous information networks (HINs)), but most of these HGNNs focus on different ways of feature aggre gation and ignore the properties of the meta-paths themselves.","This paper studies meta-paths in three commonly used data sets and finds that there are huge differences in the number of neighbors connected by different meta paths.","At the same time, the noise information contained in large neigh bor paths will have an adverse impact on model performance.","Therefore, this paper proposes a Heterogeneous Graph Neural Network Classification and Aggregation Algorithm Based on Large and Small Neighbor Path Iden tification(LSPI).","LSPI firstly divides the meta-paths into large and small neighbor paths through the path discriminator , and in order to reduce the noise interference problem in large neighbor paths, LSPI selects neighbor nodes with higher similarity from both topology and feature perspectives, and passes small neighbor paths and filtered large neighbor paths through different graph convolution components.","Aggregation is performed to obtain feature information under different subgraphs, and then LSPI uses subgraph level attention to fuse the feature information under different subgraphs to generate the final node embedding.","Finally this paper verifies the superiority of the method through extensive experiments and also gives suggestions on the number of nodes to be retained in large neighbor paths through exper iments.","The complete reproducible code adn data has been published at: https://github.com/liuhua811/LSPIA."],"url":"http://arxiv.org/abs/2405.18933v1"}
{"created":"2024-05-29 09:29:39","title":"Federated Continual Learning Goes Online: Leveraging Uncertainty for Modality-Agnostic Class-Incremental Learning","abstract":"Given the ability to model more realistic and dynamic problems, Federated Continual Learning (FCL) has been increasingly investigated recently. A well-known problem encountered in this setting is the so-called catastrophic forgetting, for which the learning model is inclined to focus on more recent tasks while forgetting the previously learned knowledge. The majority of the current approaches in FCL propose generative-based solutions to solve said problem. However, this setting requires multiple training epochs over the data, implying an offline setting where datasets are stored locally and remain unchanged over time. Furthermore, the proposed solutions are tailored for vision tasks solely. To overcome these limitations, we propose a new modality-agnostic approach to deal with the online scenario where new data arrive in streams of mini-batches that can only be processed once. To solve catastrophic forgetting, we propose an uncertainty-aware memory-based approach. In particular, we suggest using an estimator based on the Bregman Information (BI) to compute the model's variance at the sample level. Through measures of predictive uncertainty, we retrieve samples with specific characteristics, and - by retraining the model on such samples - we demonstrate the potential of this approach to reduce the forgetting effect in realistic settings.","sentences":["Given the ability to model more realistic and dynamic problems, Federated Continual Learning (FCL) has been increasingly investigated recently.","A well-known problem encountered in this setting is the so-called catastrophic forgetting, for which the learning model is inclined to focus on more recent tasks while forgetting the previously learned knowledge.","The majority of the current approaches in FCL propose generative-based solutions to solve said problem.","However, this setting requires multiple training epochs over the data, implying an offline setting where datasets are stored locally and remain unchanged over time.","Furthermore, the proposed solutions are tailored for vision tasks solely.","To overcome these limitations, we propose a new modality-agnostic approach to deal with the online scenario where new data arrive in streams of mini-batches that can only be processed once.","To solve catastrophic forgetting, we propose an uncertainty-aware memory-based approach.","In particular, we suggest using an estimator based on the Bregman Information (BI) to compute the model's variance at the sample level.","Through measures of predictive uncertainty, we retrieve samples with specific characteristics, and - by retraining the model on such samples - we demonstrate the potential of this approach to reduce the forgetting effect in realistic settings."],"url":"http://arxiv.org/abs/2405.18925v1"}
{"created":"2024-05-29 09:25:49","title":"Understanding and Addressing the Under-Translation Problem from the Perspective of Decoding Objective","abstract":"Neural Machine Translation (NMT) has made remarkable progress over the past years. However, under-translation and over-translation remain two challenging problems in state-of-the-art NMT systems. In this work, we conduct an in-depth analysis on the underlying cause of under-translation in NMT, providing an explanation from the perspective of decoding objective. To optimize the beam search objective, the model tends to overlook words it is less confident about, leading to the under-translation phenomenon. Correspondingly, the model's confidence in predicting the End Of Sentence (EOS) diminishes when under-translation occurs, serving as a mild penalty for under-translated candidates. Building upon this analysis, we propose employing the confidence of predicting EOS as a detector for under-translation, and strengthening the confidence-based penalty to penalize candidates with a high risk of under-translation. Experiments on both synthetic and real-world data show that our method can accurately detect and rectify under-translated outputs, with minor impact on other correct translations.","sentences":["Neural Machine Translation (NMT) has made remarkable progress over the past years.","However, under-translation and over-translation remain two challenging problems in state-of-the-art NMT systems.","In this work, we conduct an in-depth analysis on the underlying cause of under-translation in NMT, providing an explanation from the perspective of decoding objective.","To optimize the beam search objective, the model tends to overlook words it is less confident about, leading to the under-translation phenomenon.","Correspondingly, the model's confidence in predicting the End Of Sentence (EOS) diminishes when under-translation occurs, serving as a mild penalty for under-translated candidates.","Building upon this analysis, we propose employing the confidence of predicting EOS as a detector for under-translation, and strengthening the confidence-based penalty to penalize candidates with a high risk of under-translation.","Experiments on both synthetic and real-world data show that our method can accurately detect and rectify under-translated outputs, with minor impact on other correct translations."],"url":"http://arxiv.org/abs/2405.18922v1"}
{"created":"2024-05-29 09:22:25","title":"Exploiting Inter-Satellite Links for In-Flight Connectivity Scheme in Space-Air-Ground Integrated Networks","abstract":"Space-air-ground integrated networks (SAGIN) are pivotal for achieving uninterrupted in-flight connectivity (IFC). Most existing studies, however, merely treat satellites as transparent forwarding nodes, and overlook their caching capabilities in enhancing the IFC data rate. In this paper, we consider an IFC-oriented SAGIN, where the satellites collaboratively deliver the content to airborne passengers to facilitate airborne communication. Considering the cached files instantaneously accessible via satellites, this work pioneers the integration of multiple inter-satellite links (ISLs) into the IFC framework, thereby innovating the content delivery process. To minimize the average delay of content delivery, we formulate an optimization problem and propose an exact penalty-based method to derive the satellite association scheme. Our proposed framework has a low complexity and thus paves the way for high-speed Internet connectivity to aviation passengers. Finally, simulation results are presented to demonstrate the effectiveness of our proposed IFC framework for SAGIN.","sentences":["Space-air-ground integrated networks (SAGIN) are pivotal for achieving uninterrupted in-flight connectivity (IFC).","Most existing studies, however, merely treat satellites as transparent forwarding nodes, and overlook their caching capabilities in enhancing the IFC data rate.","In this paper, we consider an IFC-oriented SAGIN, where the satellites collaboratively deliver the content to airborne passengers to facilitate airborne communication.","Considering the cached files instantaneously accessible via satellites, this work pioneers the integration of multiple inter-satellite links (ISLs) into the IFC framework, thereby innovating the content delivery process.","To minimize the average delay of content delivery, we formulate an optimization problem and propose an exact penalty-based method to derive the satellite association scheme.","Our proposed framework has a low complexity and thus paves the way for high-speed Internet connectivity to aviation passengers.","Finally, simulation results are presented to demonstrate the effectiveness of our proposed IFC framework for SAGIN."],"url":"http://arxiv.org/abs/2405.18919v1"}
{"created":"2024-05-29 09:19:50","title":"Causal Action Influence Aware Counterfactual Data Augmentation","abstract":"Offline data are both valuable and practical resources for teaching robots complex behaviors. Ideally, learning agents should not be constrained by the scarcity of available demonstrations, but rather generalize beyond the training distribution. However, the complexity of real-world scenarios typically requires huge amounts of data to prevent neural network policies from picking up on spurious correlations and learning non-causal relationships. We propose CAIAC, a data augmentation method that can create feasible synthetic transitions from a fixed dataset without having access to online environment interactions. By utilizing principled methods for quantifying causal influence, we are able to perform counterfactual reasoning by swapping $\\it{action}$-unaffected parts of the state-space between independent trajectories in the dataset. We empirically show that this leads to a substantial increase in robustness of offline learning algorithms against distributional shift.","sentences":["Offline data are both valuable and practical resources for teaching robots complex behaviors.","Ideally, learning agents should not be constrained by the scarcity of available demonstrations, but rather generalize beyond the training distribution.","However, the complexity of real-world scenarios typically requires huge amounts of data to prevent neural network policies from picking up on spurious correlations and learning non-causal relationships.","We propose CAIAC, a data augmentation method that can create feasible synthetic transitions from a fixed dataset without having access to online environment interactions.","By utilizing principled methods for quantifying causal influence, we are able to perform counterfactual reasoning by swapping $\\it{action}$-unaffected parts of the state-space between independent trajectories in the dataset.","We empirically show that this leads to a substantial increase in robustness of offline learning algorithms against distributional shift."],"url":"http://arxiv.org/abs/2405.18917v1"}
{"created":"2024-05-29 09:16:03","title":"Leveraging Time-Series Foundation Models in Smart Agriculture for Soil Moisture Forecasting","abstract":"The recent surge in foundation models for natural language processing and computer vision has fueled innovation across various domains. Inspired by this progress, we explore the potential of foundation models for time-series forecasting in smart agriculture, a field often plagued by limited data availability. Specifically, this work presents a novel application of $\\texttt{TimeGPT}$, a state-of-the-art (SOTA) time-series foundation model, to predict soil water potential ($\\psi_\\mathrm{soil}$), a key indicator of field water status that is typically used for irrigation advice. Traditionally, this task relies on a wide array of input variables. We explore $\\psi_\\mathrm{soil}$'s ability to forecast $\\psi_\\mathrm{soil}$ in: ($i$) a zero-shot setting, ($ii$) a fine-tuned setting relying solely on historic $\\psi_\\mathrm{soil}$ measurements, and ($iii$) a fine-tuned setting where we also add exogenous variables to the model. We compare $\\texttt{TimeGPT}$'s performance to established SOTA baseline models for forecasting $\\psi_\\mathrm{soil}$. Our results demonstrate that $\\texttt{TimeGPT}$ achieves competitive forecasting accuracy using only historical $\\psi_\\mathrm{soil}$ data, highlighting its remarkable potential for agricultural applications. This research paves the way for foundation time-series models for sustainable development in agriculture by enabling forecasting tasks that were traditionally reliant on extensive data collection and domain expertise.","sentences":["The recent surge in foundation models for natural language processing and computer vision has fueled innovation across various domains.","Inspired by this progress, we explore the potential of foundation models for time-series forecasting in smart agriculture, a field often plagued by limited data availability.","Specifically, this work presents a novel application of $\\texttt{TimeGPT}$, a state-of-the-art (SOTA) time-series foundation model, to predict soil water potential ($\\psi_\\mathrm{soil}$), a key indicator of field water status that is typically used for irrigation advice.","Traditionally, this task relies on a wide array of input variables.","We explore $\\psi_\\mathrm{soil}$'s ability to forecast $\\psi_\\mathrm{soil}$ in: ($i$) a zero-shot setting, ($ii$) a fine-tuned setting relying solely on historic $\\psi_\\mathrm{soil}$ measurements, and ($iii$) a fine-tuned setting where we also add exogenous variables to the model.","We compare $\\texttt{TimeGPT}$'s performance to established SOTA baseline models for forecasting $\\psi_\\mathrm{soil}$. Our results demonstrate that $\\texttt{TimeGPT}$ achieves competitive forecasting accuracy using only historical $\\psi_\\mathrm{soil}$ data, highlighting its remarkable potential for agricultural applications.","This research paves the way for foundation time-series models for sustainable development in agriculture by enabling forecasting tasks that were traditionally reliant on extensive data collection and domain expertise."],"url":"http://arxiv.org/abs/2405.18913v1"}
{"created":"2024-05-29 09:13:30","title":"Exploring Human-in-the-Loop Test-Time Adaptation by Synergizing Active Learning and Model Selection","abstract":"Existing test-time adaptation (TTA) approaches often adapt models with the unlabeled testing data stream. A recent attempt relaxed the assumption by introducing limited human annotation, referred to as Human-In-the-Loop Test-Time Adaptation (HILTTA) in this study. The focus of existing HILTTA lies on selecting the most informative samples to label, a.k.a. active learning. In this work, we are motivated by a pitfall of TTA, i.e. sensitive to hyper-parameters, and propose to approach HILTTA by synergizing active learning and model selection. Specifically, we first select samples for human annotation (active learning) and then use the labeled data to select optimal hyper-parameters (model selection). A sample selection strategy is tailored for choosing samples by considering the balance between active learning and model selection purposes. We demonstrate on 4 TTA datasets that the proposed HILTTA approach is compatible with off-the-shelf TTA methods which outperform the state-of-the-art HILTTA methods and stream-based active learning methods. Importantly, our proposed method can always prevent choosing the worst hyper-parameters on all off-the-shelf TTA methods. The source code will be released upon publication.","sentences":["Existing test-time adaptation (TTA) approaches often adapt models with the unlabeled testing data stream.","A recent attempt relaxed the assumption by introducing limited human annotation, referred to as Human-In-the-Loop Test-Time Adaptation (HILTTA) in this study.","The focus of existing HILTTA lies on selecting the most informative samples to label, a.k.a. active learning.","In this work, we are motivated by a pitfall of TTA, i.e. sensitive to hyper-parameters, and propose to approach HILTTA by synergizing active learning and model selection.","Specifically, we first select samples for human annotation (active learning) and then use the labeled data to select optimal hyper-parameters (model selection).","A sample selection strategy is tailored for choosing samples by considering the balance between active learning and model selection purposes.","We demonstrate on 4 TTA datasets that the proposed HILTTA approach is compatible with off-the-shelf TTA methods which outperform the state-of-the-art HILTTA methods and stream-based active learning methods.","Importantly, our proposed method can always prevent choosing the worst hyper-parameters on all off-the-shelf TTA methods.","The source code will be released upon publication."],"url":"http://arxiv.org/abs/2405.18911v1"}
{"created":"2024-05-29 09:11:51","title":"Predicting Parking Availability in Singapore with Cross-Domain Data: A New Dataset and A Data-Driven Approach","abstract":"The increasing number of vehicles highlights the need for efficient parking space management. Predicting real-time Parking Availability (PA) can help mitigate traffic congestion and the corresponding social problems, which is a pressing issue in densely populated cities like Singapore. In this study, we aim to collectively predict future PA across Singapore with complex factors from various domains. The contributions in this paper are listed as follows: (1) A New Dataset: We introduce the \\texttt{SINPA} dataset, containing a year's worth of PA data from 1,687 parking lots in Singapore, enriched with various spatial and temporal factors. (2) A Data-Driven Approach: We present DeepPA, a novel deep-learning framework, to collectively and efficiently predict future PA across thousands of parking lots. (3) Extensive Experiments and Deployment: DeepPA demonstrates a 9.2% reduction in prediction error for up to 3-hour forecasts compared to existing advanced models. Furthermore, we implement DeepPA in a practical web-based platform to provide real-time PA predictions to aid drivers and inform urban planning for the governors in Singapore. We release the dataset and source code at https://github.com/yoshall/SINPA.","sentences":["The increasing number of vehicles highlights the need for efficient parking space management.","Predicting real-time Parking Availability (PA) can help mitigate traffic congestion and the corresponding social problems, which is a pressing issue in densely populated cities like Singapore.","In this study, we aim to collectively predict future PA across Singapore with complex factors from various domains.","The contributions in this paper are listed as follows: (1) A New Dataset: We introduce the \\texttt{SINPA} dataset, containing a year's worth of PA data from 1,687 parking lots in Singapore, enriched with various spatial and temporal factors.","(2) A Data-Driven Approach: We present DeepPA, a novel deep-learning framework, to collectively and efficiently predict future PA across thousands of parking lots.","(3) Extensive Experiments and Deployment: DeepPA demonstrates a 9.2% reduction in prediction error for up to 3-hour forecasts compared to existing advanced models.","Furthermore, we implement DeepPA in a practical web-based platform to provide real-time PA predictions to aid drivers and inform urban planning for the governors in Singapore.","We release the dataset and source code at https://github.com/yoshall/SINPA."],"url":"http://arxiv.org/abs/2405.18910v1"}
{"created":"2024-05-29 08:52:48","title":"EVM Analysis of Distributed Massive MIMO with 1-Bit Radio-Over-Fiber Fronthaul","abstract":"We analyze the uplink performance of a distributed massive multiple-input multiple-output (MIMO) architecture in which the remotely located access points (APs) are connected to a central processing unit via a fiber-optical fronthaul carrying a dithered and 1-bit quantized version of the received radio-frequency (RF) signal. The innovative feature of the proposed architecture is that no down-conversion is performed at the APs. This eliminates the need to equip the APs with local oscillators, which may be difficult to synchronize. Under the assumption that a constraint is imposed on the amount of data that can be exchanged across the fiber-optical fronthaul, we investigate the tradeoff between spatial oversampling, defined in terms of the total number of APs, and temporal oversampling, defined in terms of the oversampling factor selected at the central processing unit, to facilitate the recovery of the transmitted signal from 1-bit samples of the RF received signal. Using the so-called error-vector magnitude (EVM) as performance metric, we shed light on the optimal design of the dither signal, and quantify, for a given number of APs, the minimum fronthaul rate required for our proposed distributed massive MIMO architecture to outperform a standard co-located massive MIMO architecture in terms of EVM.","sentences":["We analyze the uplink performance of a distributed massive multiple-input multiple-output (MIMO) architecture in which the remotely located access points (APs) are connected to a central processing unit via a fiber-optical fronthaul carrying a dithered and 1-bit quantized version of the received radio-frequency (RF) signal.","The innovative feature of the proposed architecture is that no down-conversion is performed at the APs.","This eliminates the need to equip the APs with local oscillators, which may be difficult to synchronize.","Under the assumption that a constraint is imposed on the amount of data that can be exchanged across the fiber-optical fronthaul, we investigate the tradeoff between spatial oversampling, defined in terms of the total number of APs, and temporal oversampling, defined in terms of the oversampling factor selected at the central processing unit, to facilitate the recovery of the transmitted signal from 1-bit samples of the RF received signal.","Using the so-called error-vector magnitude (EVM) as performance metric, we shed light on the optimal design of the dither signal, and quantify, for a given number of APs, the minimum fronthaul rate required for our proposed distributed massive MIMO architecture to outperform a standard co-located massive MIMO architecture in terms of EVM."],"url":"http://arxiv.org/abs/2405.18892v1"}
{"created":"2024-05-29 08:46:21","title":"Locally Estimated Global Perturbations are Better than Local Perturbations for Federated Sharpness-aware Minimization","abstract":"In federated learning (FL), the multi-step update and data heterogeneity among clients often lead to a loss landscape with sharper minima, degenerating the performance of the resulted global model. Prevalent federated approaches incorporate sharpness-aware minimization (SAM) into local training to mitigate this problem. However, the local loss landscapes may not accurately reflect the flatness of global loss landscape in heterogeneous environments; as a result, minimizing local sharpness and calculating perturbations on client data might not align the efficacy of SAM in FL with centralized training. To overcome this challenge, we propose FedLESAM, a novel algorithm that locally estimates the direction of global perturbation on client side as the difference between global models received in the previous active and current rounds. Besides the improved quality, FedLESAM also speed up federated SAM-based approaches since it only performs once backpropagation in each iteration. Theoretically, we prove a slightly tighter bound than its original FedSAM by ensuring consistent perturbation. Empirically, we conduct comprehensive experiments on four federated benchmark datasets under three partition strategies to demonstrate the superior performance and efficiency of FedLESAM.","sentences":["In federated learning (FL), the multi-step update and data heterogeneity among clients often lead to a loss landscape with sharper minima, degenerating the performance of the resulted global model.","Prevalent federated approaches incorporate sharpness-aware minimization (SAM) into local training to mitigate this problem.","However, the local loss landscapes may not accurately reflect the flatness of global loss landscape in heterogeneous environments; as a result, minimizing local sharpness and calculating perturbations on client data might not align the efficacy of SAM in FL with centralized training.","To overcome this challenge, we propose FedLESAM, a novel algorithm that locally estimates the direction of global perturbation on client side as the difference between global models received in the previous active and current rounds.","Besides the improved quality, FedLESAM also speed up federated SAM-based approaches since it only performs once backpropagation in each iteration.","Theoretically, we prove a slightly tighter bound than its original FedSAM by ensuring consistent perturbation.","Empirically, we conduct comprehensive experiments on four federated benchmark datasets under three partition strategies to demonstrate the superior performance and efficiency of FedLESAM."],"url":"http://arxiv.org/abs/2405.18890v1"}
{"created":"2024-05-29 08:46:00","title":"On Perception of Prevalence of Cheating and Usage of Generative AI","abstract":"This report investigates the perceptions of teaching staff on the prevalence of student cheating and the impact of Generative AI on academic integrity. Data was collected via an anonymous survey of teachers at the Department of Information Technology at Uppsala University and analyzed alongside institutional statistics on cheating investigations from 2004 to 2023. The results indicate that while teachers generally do not view cheating as highly prevalent, there is a strong belief that its incidence is increasing, potentially due to the accessibility of Generative AI. Most teachers do not equate AI usage with cheating but acknowledge its widespread use among students. Furthermore, teachers' perceptions align with objective data on cheating trends, highlighting their awareness of the evolving landscape of academic dishonesty.","sentences":["This report investigates the perceptions of teaching staff on the prevalence of student cheating and the impact of Generative AI on academic integrity.","Data was collected via an anonymous survey of teachers at the Department of Information Technology at Uppsala University and analyzed alongside institutional statistics on cheating investigations from 2004 to 2023.","The results indicate that while teachers generally do not view cheating as highly prevalent, there is a strong belief that its incidence is increasing, potentially due to the accessibility of Generative AI.","Most teachers do not equate AI usage with cheating but acknowledge its widespread use among students.","Furthermore, teachers' perceptions align with objective data on cheating trends, highlighting their awareness of the evolving landscape of academic dishonesty."],"url":"http://arxiv.org/abs/2405.18889v1"}
{"created":"2024-05-29 08:42:30","title":"Compressing Large Language Models using Low Rank and Low Precision Decomposition","abstract":"The prohibitive sizes of Large Language Models (LLMs) today make it difficult to deploy them on memory-constrained edge devices. This work introduces $\\rm CALDERA$ -- a new post-training LLM compression algorithm that harnesses the inherent low-rank structure of a weight matrix $\\mathbf{W}$ by approximating it via a low-rank, low-precision decomposition as $\\mathbf{W} \\approx \\mathbf{Q} + \\mathbf{L}\\mathbf{R}$. Here, $\\mathbf{L}$ and $\\mathbf{R}$ are low rank factors, and the entries of $\\mathbf{Q}$, $\\mathbf{L}$ and $\\mathbf{R}$ are quantized. The model is compressed by substituting each layer with its $\\mathbf{Q} + \\mathbf{L}\\mathbf{R}$ decomposition, and the zero-shot performance of the compressed model is evaluated. Additionally, $\\mathbf{L}$ and $\\mathbf{R}$ are readily amenable to low-rank adaptation, consequently enhancing the zero-shot performance. $\\rm CALDERA$ obtains this decomposition by formulating it as an optimization problem $\\min_{\\mathbf{Q},\\mathbf{L},\\mathbf{R}}\\lVert(\\mathbf{Q} + \\mathbf{L}\\mathbf{R} - \\mathbf{W})\\mathbf{X}^\\top\\rVert_{\\rm F}^2$, where $\\mathbf{X}$ is the calibration data, and $\\mathbf{Q}, \\mathbf{L}, \\mathbf{R}$ are constrained to be representable using low-precision formats. Theoretical upper bounds on the approximation error of $\\rm CALDERA$ are established using a rank-constrained regression framework, and the tradeoff between compression ratio and model performance is studied by analyzing the impact of target rank and quantization bit budget. Results illustrate that compressing LlaMa-$2$ $7$B/$70$B and LlaMa-$3$ $8$B models obtained using $\\rm CALDERA$ outperforms existing post-training LLM compression techniques in the regime of less than $2.5$ bits per parameter. The implementation is available at: \\href{https://github.com/pilancilab/caldera}{https://github.com/pilancilab/caldera}.","sentences":["The prohibitive sizes of Large Language Models (LLMs) today make it difficult to deploy them on memory-constrained edge devices.","This work introduces $\\rm CALDERA$ -- a new post-training LLM compression algorithm that harnesses the inherent low-rank structure of a weight matrix $\\mathbf{W}$ by approximating it via a low-rank, low-precision decomposition as $\\mathbf{W} \\approx \\mathbf{Q} + \\mathbf{L}\\mathbf{R}$. Here, $\\mathbf{L}$ and $\\mathbf{R}$ are low rank factors, and the entries of $\\mathbf{Q}$, $\\mathbf{L}$ and $\\mathbf{R}$ are quantized.","The model is compressed by substituting each layer with its $\\mathbf{Q} + \\mathbf{L}\\mathbf{R}$ decomposition, and the zero-shot performance of the compressed model is evaluated.","Additionally, $\\mathbf{L}$ and $\\mathbf{R}$ are readily amenable to low-rank adaptation, consequently enhancing the zero-shot performance.","$\\rm CALDERA$ obtains this decomposition by formulating it as an optimization problem $\\min_{\\mathbf{Q},\\mathbf{L},\\mathbf{R}}\\lVert(\\mathbf{Q} + \\mathbf{L}\\mathbf{R} - \\mathbf{W})\\mathbf{X}^\\top\\rVert_{\\rm F}^2$, where $\\mathbf{X}$ is the calibration data, and $\\mathbf{Q}, \\mathbf{L}, \\mathbf{R}$ are constrained to be representable using low-precision formats.","Theoretical upper bounds on the approximation error of $\\rm CALDERA$ are established using a rank-constrained regression framework, and the tradeoff between compression ratio and model performance is studied by analyzing the impact of target rank and quantization bit budget.","Results illustrate that compressing LlaMa-$2$ $7$B/$70$B and LlaMa-$3$ $8$B models obtained using $\\rm CALDERA$ outperforms existing post-training LLM compression techniques in the regime of less than $2.5$ bits per parameter.","The implementation is available at: \\href{https://github.com/pilancilab/caldera}{https://github.com/pilancilab/caldera}."],"url":"http://arxiv.org/abs/2405.18886v1"}
{"created":"2024-05-29 08:41:08","title":"Learning Mixture-of-Experts for General-Purpose Black-Box Discrete Optimization","abstract":"Real-world applications involve various discrete optimization problems. Designing a specialized optimizer for each of these problems is challenging, typically requiring significant domain knowledge and human efforts. Hence, developing general-purpose optimizers as an off-the-shelf tool for a wide range of problems has been a long-standing research target. This article introduces MEGO, a novel general-purpose neural optimizer trained through a fully data-driven learning-to-optimize (L2O) approach. MEGO consists of a mixture-of-experts trained on experiences from solving training problems and can be viewed as a foundation model for optimization problems with binary decision variables. When presented with a problem to solve, MEGO actively selects relevant expert models to generate high-quality solutions. MEGO can be used as a standalone sample-efficient optimizer or in conjunction with existing search methods as an initial solution generator. The generality of MEGO is validated across six problem classes, including three classic problem classes and three problem classes arising from real-world applications in compilers, network analysis, and 3D reconstruction. Trained solely on classic problem classes, MEGO performs very well on all six problem classes, significantly surpassing widely used general-purpose optimizers in both solution quality and efficiency. In some cases, MEGO even surpasses specialized state-of-the-art optimizers. Additionally, MEGO provides a similarity measure between problems, yielding a new perspective for problem classification. In the pursuit of general-purpose optimizers through L2O, MEGO represents an initial yet significant step forward.","sentences":["Real-world applications involve various discrete optimization problems.","Designing a specialized optimizer for each of these problems is challenging, typically requiring significant domain knowledge and human efforts.","Hence, developing general-purpose optimizers as an off-the-shelf tool for a wide range of problems has been a long-standing research target.","This article introduces MEGO, a novel general-purpose neural optimizer trained through a fully data-driven learning-to-optimize (L2O) approach.","MEGO consists of a mixture-of-experts trained on experiences from solving training problems and can be viewed as a foundation model for optimization problems with binary decision variables.","When presented with a problem to solve, MEGO actively selects relevant expert models to generate high-quality solutions.","MEGO can be used as a standalone sample-efficient optimizer or in conjunction with existing search methods as an initial solution generator.","The generality of MEGO is validated across six problem classes, including three classic problem classes and three problem classes arising from real-world applications in compilers, network analysis, and 3D reconstruction.","Trained solely on classic problem classes, MEGO performs very well on all six problem classes, significantly surpassing widely used general-purpose optimizers in both solution quality and efficiency.","In some cases, MEGO even surpasses specialized state-of-the-art optimizers.","Additionally, MEGO provides a similarity measure between problems, yielding a new perspective for problem classification.","In the pursuit of general-purpose optimizers through L2O, MEGO represents an initial yet significant step forward."],"url":"http://arxiv.org/abs/2405.18884v1"}
{"created":"2024-05-29 08:39:39","title":"Tuning-Free Alignment of Diffusion Models with Direct Noise Optimization","abstract":"In this work, we focus on the alignment problem of diffusion models with a continuous reward function, which represents specific objectives for downstream tasks, such as improving human preference. The central goal of the alignment problem is to adjust the distribution learned by diffusion models such that the generated samples maximize the target reward function. We propose a novel alignment approach, named Direct Noise Optimization (DNO), that optimizes the injected noise during the sampling process of diffusion models. By design, DNO is tuning-free and prompt-agnostic, as the alignment occurs in an online fashion during generation. We rigorously study the theoretical properties of DNO and also propose variants to deal with non-differentiable reward functions. Furthermore, we identify that naive implementation of DNO occasionally suffers from the out-of-distribution reward hacking problem, where optimized samples have high rewards but are no longer in the support of the pretrained distribution. To remedy this issue, we leverage classical high-dimensional statistics theory and propose to augment the DNO loss with certain probability regularization. We conduct extensive experiments on several popular reward functions trained on human feedback data and demonstrate that the proposed DNO approach achieves state-of-the-art reward scores as well as high image quality, all within a reasonable time budget for generation.","sentences":["In this work, we focus on the alignment problem of diffusion models with a continuous reward function, which represents specific objectives for downstream tasks, such as improving human preference.","The central goal of the alignment problem is to adjust the distribution learned by diffusion models such that the generated samples maximize the target reward function.","We propose a novel alignment approach, named Direct Noise Optimization (DNO), that optimizes the injected noise during the sampling process of diffusion models.","By design, DNO is tuning-free and prompt-agnostic, as the alignment occurs in an online fashion during generation.","We rigorously study the theoretical properties of DNO and also propose variants to deal with non-differentiable reward functions.","Furthermore, we identify that naive implementation of DNO occasionally suffers from the out-of-distribution reward hacking problem, where optimized samples have high rewards but are no longer in the support of the pretrained distribution.","To remedy this issue, we leverage classical high-dimensional statistics theory and propose to augment the DNO loss with certain probability regularization.","We conduct extensive experiments on several popular reward functions trained on human feedback data and demonstrate that the proposed DNO approach achieves state-of-the-art reward scores as well as high image quality, all within a reasonable time budget for generation."],"url":"http://arxiv.org/abs/2405.18881v1"}
{"created":"2024-05-29 08:39:31","title":"EventZoom: A Progressive Approach to Event-Based Data Augmentation for Enhanced Neuromorphic Vision","abstract":"Event data captured by Dynamic Vision Sensors (DVS) offers a unique approach to visual processing that differs from traditional video capture, showcasing its efficiency in dynamic and real-time scenarios. Despite advantages such as high temporal resolution and low energy consumption, the application of event data faces challenges due to limited dataset size and diversity. To address this, we developed EventZoom -- a data augmentation strategy specifically designed for event data. EventZoom employs a progressive temporal strategy that intelligently blends time and space to enhance the diversity and complexity of the data while maintaining its authenticity. This method aims to improve the quality of data for model training and enhance the adaptability and robustness of algorithms in handling complex dynamic scenes. We have experimentally validated EventZoom across various supervised learning frameworks, including supervised, semi-supervised, and unsupervised learning. Our results demonstrate that EventZoom consistently outperforms other data augmentation methods, confirming its effectiveness and applicability as a powerful event-based data augmentation tool in diverse learning settings.","sentences":["Event data captured by Dynamic Vision Sensors (DVS) offers a unique approach to visual processing that differs from traditional video capture, showcasing its efficiency in dynamic and real-time scenarios.","Despite advantages such as high temporal resolution and low energy consumption, the application of event data faces challenges due to limited dataset size and diversity.","To address this, we developed EventZoom -- a data augmentation strategy specifically designed for event data.","EventZoom employs a progressive temporal strategy that intelligently blends time and space to enhance the diversity and complexity of the data while maintaining its authenticity.","This method aims to improve the quality of data for model training and enhance the adaptability and robustness of algorithms in handling complex dynamic scenes.","We have experimentally validated EventZoom across various supervised learning frameworks, including supervised, semi-supervised, and unsupervised learning.","Our results demonstrate that EventZoom consistently outperforms other data augmentation methods, confirming its effectiveness and applicability as a powerful event-based data augmentation tool in diverse learning settings."],"url":"http://arxiv.org/abs/2405.18880v1"}
{"created":"2024-05-29 08:37:48","title":"Spatiotemporal Forecasting Meets Efficiency: Causal Graph Process Neural Networks","abstract":"Graph Neural Networks (GNNs) have advanced spatiotemporal forecasting by leveraging relational inductive biases among sensors (or any other measuring scheme) represented as nodes in a graph. However, current methods often rely on Recurrent Neural Networks (RNNs), leading to increased runtimes and memory use. Moreover, these methods typically operate within 1-hop neighborhoods, exacerbating the reduction of the receptive field. Causal Graph Processes (CGPs) offer an alternative, using graph filters instead of MLP layers to reduce parameters and minimize memory consumption. This paper introduces the Causal Graph Process Neural Network (CGProNet), a non-linear model combining CGPs and GNNs for spatiotemporal forecasting. CGProNet employs higher-order graph filters, optimizing the model with fewer parameters, reducing memory usage, and improving runtime efficiency. We present a comprehensive theoretical and experimental stability analysis, highlighting key aspects of CGProNet. Experiments on synthetic and real data demonstrate CGProNet's superior efficiency, minimizing memory and time requirements while maintaining competitive forecasting performance.","sentences":["Graph Neural Networks (GNNs) have advanced spatiotemporal forecasting by leveraging relational inductive biases among sensors (or any other measuring scheme) represented as nodes in a graph.","However, current methods often rely on Recurrent Neural Networks (RNNs), leading to increased runtimes and memory use.","Moreover, these methods typically operate within 1-hop neighborhoods, exacerbating the reduction of the receptive field.","Causal Graph Processes (CGPs) offer an alternative, using graph filters instead of MLP layers to reduce parameters and minimize memory consumption.","This paper introduces the Causal Graph Process Neural Network (CGProNet), a non-linear model combining CGPs and GNNs for spatiotemporal forecasting.","CGProNet employs higher-order graph filters, optimizing the model with fewer parameters, reducing memory usage, and improving runtime efficiency.","We present a comprehensive theoretical and experimental stability analysis, highlighting key aspects of CGProNet.","Experiments on synthetic and real data demonstrate CGProNet's superior efficiency, minimizing memory and time requirements while maintaining competitive forecasting performance."],"url":"http://arxiv.org/abs/2405.18879v1"}
{"created":"2024-05-29 08:36:42","title":"Privacy Preserving Data Imputation via Multi-party Computation for Medical Applications","abstract":"Handling missing data is crucial in machine learning, but many datasets contain gaps due to errors or non-response. Unlike traditional methods such as listwise deletion, which are simple but inadequate, the literature offers more sophisticated and effective methods, thereby improving sample size and accuracy. However, these methods require accessing the whole dataset, which contradicts the privacy regulations when the data is distributed among multiple sources. Especially in the medical and healthcare domain, such access reveals sensitive information about patients. This study addresses privacy-preserving imputation methods for sensitive data using secure multi-party computation, enabling secure computations without revealing any party's sensitive information. In this study, we realized the mean, median, regression, and kNN imputation methods in a privacy-preserving way. We specifically target the medical and healthcare domains considering the significance of protection of the patient data, showcasing our methods on a diabetes dataset. Experiments on the diabetes dataset validated the correctness of our privacy-preserving imputation methods, yielding the largest error around $3 \\times 10^{-3}$, closely matching plaintext methods. We also analyzed the scalability of our methods to varying numbers of samples, showing their applicability to real-world healthcare problems. Our analysis demonstrated that all our methods scale linearly with the number of samples. Except for kNN, the runtime of all our methods indicates that they can be utilized for large datasets.","sentences":["Handling missing data is crucial in machine learning, but many datasets contain gaps due to errors or non-response.","Unlike traditional methods such as listwise deletion, which are simple but inadequate, the literature offers more sophisticated and effective methods, thereby improving sample size and accuracy.","However, these methods require accessing the whole dataset, which contradicts the privacy regulations when the data is distributed among multiple sources.","Especially in the medical and healthcare domain, such access reveals sensitive information about patients.","This study addresses privacy-preserving imputation methods for sensitive data using secure multi-party computation, enabling secure computations without revealing any party's sensitive information.","In this study, we realized the mean, median, regression, and kNN imputation methods in a privacy-preserving way.","We specifically target the medical and healthcare domains considering the significance of protection of the patient data, showcasing our methods on a diabetes dataset.","Experiments on the diabetes dataset validated the correctness of our privacy-preserving imputation methods, yielding the largest error around $3 \\times 10^{-3}$, closely matching plaintext methods.","We also analyzed the scalability of our methods to varying numbers of samples, showing their applicability to real-world healthcare problems.","Our analysis demonstrated that all our methods scale linearly with the number of samples.","Except for kNN, the runtime of all our methods indicates that they can be utilized for large datasets."],"url":"http://arxiv.org/abs/2405.18878v1"}
{"created":"2024-05-29 08:36:09","title":"Continuous Product Graph Neural Networks","abstract":"Processing multidomain data defined on multiple graphs holds significant potential in various practical applications in computer science. However, current methods are mostly limited to discrete graph filtering operations. Tensorial partial differential equations on graphs (TPDEGs) provide a principled framework for modeling structured data across multiple interacting graphs, addressing the limitations of the existing discrete methodologies. In this paper, we introduce Continuous Product Graph Neural Networks (CITRUS) that emerge as a natural solution to the TPDEG. CITRUS leverages the separability of continuous heat kernels from Cartesian graph products to efficiently implement graph spectral decomposition. We conduct thorough theoretical analyses of the stability and over-smoothing properties of CITRUS in response to domain-specific graph perturbations and graph spectra effects on the performance. We evaluate CITRUS on well-known traffic and weather spatiotemporal forecasting datasets, demonstrating superior performance over existing approaches.","sentences":["Processing multidomain data defined on multiple graphs holds significant potential in various practical applications in computer science.","However, current methods are mostly limited to discrete graph filtering operations.","Tensorial partial differential equations on graphs (TPDEGs) provide a principled framework for modeling structured data across multiple interacting graphs, addressing the limitations of the existing discrete methodologies.","In this paper, we introduce Continuous Product Graph Neural Networks (CITRUS) that emerge as a natural solution to the TPDEG.","CITRUS leverages the separability of continuous heat kernels from Cartesian graph products to efficiently implement graph spectral decomposition.","We conduct thorough theoretical analyses of the stability and over-smoothing properties of CITRUS in response to domain-specific graph perturbations and graph spectra effects on the performance.","We evaluate CITRUS on well-known traffic and weather spatiotemporal forecasting datasets, demonstrating superior performance over existing approaches."],"url":"http://arxiv.org/abs/2405.18877v1"}
{"created":"2024-05-29 08:31:34","title":"DFAMiner: Mining minimal separating DFAs from labelled samples","abstract":"We propose DFAMiner, a passive learning tool for learning minimal separating deterministic finite automata (DFA) from a set of labelled samples. Separating automata are an interesting class of automata that occurs generally in regular model checking and has raised interest in foundational questions of parity game solving. We first propose a simple and linear-time algorithm that incrementally constructs a three-valued DFA (3DFA) from a set of labelled samples given in the usual lexicographical order. This 3DFA has accepting and rejecting states as well as don't-care states, so that it can exactly recognise the labelled examples. We then apply our tool to mining a minimal separating DFA for the labelled samples by minimising the constructed automata via a reduction to solving SAT problems. Empirical evaluation shows that our tool outperforms current state-of-the-art tools significantly on standard benchmarks for learning minimal separating DFAs from samples. Progress in the efficient construction of separating DFAs can also lead to finding the lower bound of parity game solving, where we show that DFAMiner can create optimal separating automata for simple languages with up to 7 colours. Future improvements might offer inroads to better data structures.","sentences":["We propose DFAMiner, a passive learning tool for learning minimal separating deterministic finite automata (DFA) from a set of labelled samples.","Separating automata are an interesting class of automata that occurs generally in regular model checking and has raised interest in foundational questions of parity game solving.","We first propose a simple and linear-time algorithm that incrementally constructs a three-valued DFA (3DFA) from a set of labelled samples given in the usual lexicographical order.","This 3DFA has accepting and rejecting states as well as don't-care states, so that it can exactly recognise the labelled examples.","We then apply our tool to mining a minimal separating DFA for the labelled samples by minimising the constructed automata via a reduction to solving SAT problems.","Empirical evaluation shows that our tool outperforms current state-of-the-art tools significantly on standard benchmarks for learning minimal separating DFAs from samples.","Progress in the efficient construction of separating DFAs can also lead to finding the lower bound of parity game solving, where we show that DFAMiner can create optimal separating automata for simple languages with up to 7 colours.","Future improvements might offer inroads to better data structures."],"url":"http://arxiv.org/abs/2405.18871v1"}
{"created":"2024-05-29 08:30:34","title":"Towards Data-Driven Electricity Management: Multi-Region Harmonized Data and Knowledge Graph","abstract":"Due to growing population and technological advances, global electricity consumption, and consequently also CO2 emissions are increasing. The residential sector makes up 25% of global electricity consumption and has great potential to increase efficiency and reduce CO2 footprint without sacrificing comfort. However, a lack of uniform consumption data at the household level spanning multiple regions hinders large-scale studies and robust multi-region model development. This paper introduces a multi-region dataset compiled from publicly available sources and presented in a uniform format. This data enables machine learning tasks such as disaggregation, demand forecasting, appliance ON/OFF classification, etc. Furthermore, we develop an RDF knowledge graph that characterizes the electricity consumption of the households and contextualizes it with household related properties enabling semantic queries and interoperability with other open knowledge bases like Wikidata and DBpedia. This structured data can be utilized to inform various stakeholders towards data-driven policy and business development.","sentences":["Due to growing population and technological advances, global electricity consumption, and consequently also CO2 emissions are increasing.","The residential sector makes up 25% of global electricity consumption and has great potential to increase efficiency and reduce CO2 footprint without sacrificing comfort.","However, a lack of uniform consumption data at the household level spanning multiple regions hinders large-scale studies and robust multi-region model development.","This paper introduces a multi-region dataset compiled from publicly available sources and presented in a uniform format.","This data enables machine learning tasks such as disaggregation, demand forecasting, appliance ON/OFF classification, etc.","Furthermore, we develop an RDF knowledge graph that characterizes the electricity consumption of the households and contextualizes it with household related properties enabling semantic queries and interoperability with other open knowledge bases like Wikidata and DBpedia.","This structured data can be utilized to inform various stakeholders towards data-driven policy and business development."],"url":"http://arxiv.org/abs/2405.18869v1"}
{"created":"2024-05-29 08:28:23","title":"Topological Perspectives on Optimal Multimodal Embedding Spaces","abstract":"Recent strides in multimodal model development have ignited a paradigm shift in the realm of text-to-image generation. Among these advancements, CLIP stands out as a remarkable achievement which is a sophisticated autoencoder adept at encoding both textual and visual information within a unified latent space. This paper delves into a comparative analysis between CLIP and its recent counterpart, CLOOB. To unravel the intricate distinctions within the embedding spaces crafted by these models, we employ topological data analysis. Our approach encompasses a comprehensive examination of the modality gap drivers, the clustering structures existing across both high and low dimensions, and the pivotal role that dimension collapse plays in shaping their respective embedding spaces. Empirical experiments substantiate the implications of our analyses on downstream performance across various contextual scenarios. Through this investigation, we aim to shed light on the nuanced intricacies that underlie the comparative efficacy of CLIP and CLOOB, offering insights into their respective strengths and weaknesses, and providing a foundation for further refinement and advancement in multimodal model research.","sentences":["Recent strides in multimodal model development have ignited a paradigm shift in the realm of text-to-image generation.","Among these advancements, CLIP stands out as a remarkable achievement which is a sophisticated autoencoder adept at encoding both textual and visual information within a unified latent space.","This paper delves into a comparative analysis between CLIP and its recent counterpart, CLOOB.","To unravel the intricate distinctions within the embedding spaces crafted by these models, we employ topological data analysis.","Our approach encompasses a comprehensive examination of the modality gap drivers, the clustering structures existing across both high and low dimensions, and the pivotal role that dimension collapse plays in shaping their respective embedding spaces.","Empirical experiments substantiate the implications of our analyses on downstream performance across various contextual scenarios.","Through this investigation, we aim to shed light on the nuanced intricacies that underlie the comparative efficacy of CLIP and CLOOB, offering insights into their respective strengths and weaknesses, and providing a foundation for further refinement and advancement in multimodal model research."],"url":"http://arxiv.org/abs/2405.18867v1"}
{"created":"2024-05-29 08:27:59","title":"The Structural Complexity Landscape of Finding Balance-Fair Shortest Paths","abstract":"We study the parameterized complexity of finding shortest s-t-paths with an additional fairness requirement. The task is to compute a shortest path in a vertex-colored graph where each color appears (roughly) equally often in the solution. We provide a complete picture of the parameterized complexity landscape of the problem with respect to structural parameters by showing a tetrachotomy including polynomial kernels, fixed-parameter tractability, XP-time algorithms (and W[1]-hardness), and para-NP-hardness.","sentences":["We study the parameterized complexity of finding shortest s-t-paths with an additional fairness requirement.","The task is to compute a shortest path in a vertex-colored graph where each color appears (roughly) equally often in the solution.","We provide a complete picture of the parameterized complexity landscape of the problem with respect to structural parameters by showing a tetrachotomy including polynomial kernels, fixed-parameter tractability, XP-time algorithms (and W[1]-hardness), and para-NP-hardness."],"url":"http://arxiv.org/abs/2405.18866v1"}
{"created":"2024-05-29 08:25:04","title":"Neural Radiance Fields for Novel View Synthesis in Monocular Gastroscopy","abstract":"Enabling the synthesis of arbitrarily novel viewpoint images within a patient's stomach from pre-captured monocular gastroscopic images is a promising topic in stomach diagnosis. Typical methods to achieve this objective integrate traditional 3D reconstruction techniques, including structure-from-motion (SfM) and Poisson surface reconstruction. These methods produce explicit 3D representations, such as point clouds and meshes, thereby enabling the rendering of the images from novel viewpoints. However, the existence of low-texture and non-Lambertian regions within the stomach often results in noisy and incomplete reconstructions of point clouds and meshes, hindering the attainment of high-quality image rendering. In this paper, we apply the emerging technique of neural radiance fields (NeRF) to monocular gastroscopic data for synthesizing photo-realistic images for novel viewpoints. To address the performance degradation due to view sparsity in local regions of monocular gastroscopy, we incorporate geometry priors from a pre-reconstructed point cloud into the training of NeRF, which introduces a novel geometry-based loss to both pre-captured observed views and generated unobserved views. Compared to other recent NeRF methods, our approach showcases high-fidelity image renderings from novel viewpoints within the stomach both qualitatively and quantitatively.","sentences":["Enabling the synthesis of arbitrarily novel viewpoint images within a patient's stomach from pre-captured monocular gastroscopic images is a promising topic in stomach diagnosis.","Typical methods to achieve this objective integrate traditional 3D reconstruction techniques, including structure-from-motion (SfM) and Poisson surface reconstruction.","These methods produce explicit 3D representations, such as point clouds and meshes, thereby enabling the rendering of the images from novel viewpoints.","However, the existence of low-texture and non-Lambertian regions within the stomach often results in noisy and incomplete reconstructions of point clouds and meshes, hindering the attainment of high-quality image rendering.","In this paper, we apply the emerging technique of neural radiance fields (NeRF) to monocular gastroscopic data for synthesizing photo-realistic images for novel viewpoints.","To address the performance degradation due to view sparsity in local regions of monocular gastroscopy, we incorporate geometry priors from a pre-reconstructed point cloud into the training of NeRF, which introduces a novel geometry-based loss to both pre-captured observed views and generated unobserved views.","Compared to other recent NeRF methods, our approach showcases high-fidelity image renderings from novel viewpoints within the stomach both qualitatively and quantitatively."],"url":"http://arxiv.org/abs/2405.18863v1"}
{"created":"2024-05-29 08:15:56","title":"Empowering Embodied Manipulation: A Bimanual-Mobile Robot Manipulation Dataset for Household Tasks","abstract":"As Embodied AI advances, it increasingly enables robots to handle the complexity of household manipulation tasks more effectively. However, the application of robots in these settings remains limited due to the scarcity of bimanual-mobile robot manipulation datasets. Existing datasets either focus solely on simple grasping tasks using single-arm robots without mobility, or collect sensor data limited to a narrow scope of sensory inputs. As a result, these datasets often fail to encapsulate the intricate and dynamic nature of real-world tasks that bimanual-mobile robots are expected to perform. To address these limitations, we introduce BRMData, a Bimanual-mobile Robot Manipulation Dataset designed specifically for household applications. The dataset includes 10 diverse household tasks, ranging from simple single-arm manipulation to more complex dual-arm and mobile manipulations. It is collected using multi-view and depth-sensing data acquisition strategies. Human-robot interactions and multi-object manipulations are integrated into the task designs to closely simulate real-world household applications. Moreover, we present a Manipulation Efficiency Score (MES) metric to evaluate both the precision and efficiency of robot manipulation methods. BRMData aims to drive the development of versatile robot manipulation technologies, specifically focusing on advancing imitation learning methods from human demonstrations. The dataset is now open-sourced and available at https://embodiedrobot.github.io/, enhancing research and development efforts in the field of Embodied Manipulation.","sentences":["As Embodied AI advances, it increasingly enables robots to handle the complexity of household manipulation tasks more effectively.","However, the application of robots in these settings remains limited due to the scarcity of bimanual-mobile robot manipulation datasets.","Existing datasets either focus solely on simple grasping tasks using single-arm robots without mobility, or collect sensor data limited to a narrow scope of sensory inputs.","As a result, these datasets often fail to encapsulate the intricate and dynamic nature of real-world tasks that bimanual-mobile robots are expected to perform.","To address these limitations, we introduce BRMData, a Bimanual-mobile Robot Manipulation Dataset designed specifically for household applications.","The dataset includes 10 diverse household tasks, ranging from simple single-arm manipulation to more complex dual-arm and mobile manipulations.","It is collected using multi-view and depth-sensing data acquisition strategies.","Human-robot interactions and multi-object manipulations are integrated into the task designs to closely simulate real-world household applications.","Moreover, we present a Manipulation Efficiency Score (MES) metric to evaluate both the precision and efficiency of robot manipulation methods.","BRMData aims to drive the development of versatile robot manipulation technologies, specifically focusing on advancing imitation learning methods from human demonstrations.","The dataset is now open-sourced and available at https://embodiedrobot.github.io/, enhancing research and development efforts in the field of Embodied Manipulation."],"url":"http://arxiv.org/abs/2405.18860v1"}
{"created":"2024-05-29 08:03:52","title":"Supervised Contrastive Learning for Snapshot Spectral Imaging Face Anti-Spoofing","abstract":"This study reveals a cutting-edge re-balanced contrastive learning strategy aimed at strengthening face anti-spoofing capabilities within facial recognition systems, with a focus on countering the challenges posed by printed photos, and highly realistic silicone or latex masks. Leveraging the HySpeFAS dataset, which benefits from Snapshot Spectral Imaging technology to provide hyperspectral images, our approach harmonizes class-level contrastive learning with data resampling and an innovative real-face oriented reweighting technique. This method effectively mitigates dataset imbalances and reduces identity-related biases. Notably, our strategy achieved an unprecedented 0.0000\\% Average Classification Error Rate (ACER) on the HySpeFAS dataset, ranking first at the Chalearn Snapshot Spectral Imaging Face Anti-spoofing Challenge on CVPR 2024.","sentences":["This study reveals a cutting-edge re-balanced contrastive learning strategy aimed at strengthening face anti-spoofing capabilities within facial recognition systems, with a focus on countering the challenges posed by printed photos, and highly realistic silicone or latex masks.","Leveraging the HySpeFAS dataset, which benefits from Snapshot Spectral Imaging technology to provide hyperspectral images, our approach harmonizes class-level contrastive learning with data resampling and an innovative real-face oriented reweighting technique.","This method effectively mitigates dataset imbalances and reduces identity-related biases.","Notably, our strategy achieved an unprecedented 0.0000\\% Average Classification Error Rate (ACER) on the HySpeFAS dataset, ranking first at the Chalearn Snapshot Spectral Imaging Face Anti-spoofing Challenge on CVPR 2024."],"url":"http://arxiv.org/abs/2405.18853v1"}
{"created":"2024-05-29 08:03:36","title":"LetsMap: Unsupervised Representation Learning for Semantic BEV Mapping","abstract":"Semantic Bird's Eye View (BEV) maps offer a rich representation with strong occlusion reasoning for various decision making tasks in autonomous driving. However, most BEV mapping approaches employ a fully supervised learning paradigm that relies on large amounts of human-annotated BEV ground truth data. In this work, we address this limitation by proposing the first unsupervised representation learning approach to generate semantic BEV maps from a monocular frontal view (FV) image in a label-efficient manner. Our approach pretrains the network to independently reason about scene geometry and scene semantics using two disjoint neural pathways in an unsupervised manner and then finetunes it for the task of semantic BEV mapping using only a small fraction of labels in the BEV. We achieve label-free pretraining by exploiting spatial and temporal consistency of FV images to learn scene geometry while relying on a novel temporal masked autoencoder formulation to encode the scene representation. Extensive evaluations on the KITTI-360 and nuScenes datasets demonstrate that our approach performs on par with the existing state-of-the-art approaches while using only 1% of BEV labels and no additional labeled data.","sentences":["Semantic Bird's Eye View (BEV) maps offer a rich representation with strong occlusion reasoning for various decision making tasks in autonomous driving.","However, most BEV mapping approaches employ a fully supervised learning paradigm that relies on large amounts of human-annotated BEV ground truth data.","In this work, we address this limitation by proposing the first unsupervised representation learning approach to generate semantic BEV maps from a monocular frontal view (FV) image in a label-efficient manner.","Our approach pretrains the network to independently reason about scene geometry and scene semantics using two disjoint neural pathways in an unsupervised manner and then finetunes it for the task of semantic BEV mapping using only a small fraction of labels in the BEV.","We achieve label-free pretraining by exploiting spatial and temporal consistency of FV images to learn scene geometry while relying on a novel temporal masked autoencoder formulation to encode the scene representation.","Extensive evaluations on the KITTI-360 and nuScenes datasets demonstrate that our approach performs on par with the existing state-of-the-art approaches while using only 1% of BEV labels and no additional labeled data."],"url":"http://arxiv.org/abs/2405.18852v1"}
{"created":"2024-05-29 08:00:15","title":"SFANet: Spatial-Frequency Attention Network for Weather Forecasting","abstract":"Weather forecasting plays a critical role in various sectors, driving decision-making and risk management. However, traditional methods often struggle to capture the complex dynamics of meteorological systems, particularly in the presence of high-resolution data. In this paper, we propose the Spatial-Frequency Attention Network (SFANet), a novel deep learning framework designed to address these challenges and enhance the accuracy of spatiotemporal weather prediction. Drawing inspiration from the limitations of existing methodologies, we present an innovative approach that seamlessly integrates advanced token mixing and attention mechanisms. By leveraging both pooling and spatial mixing strategies, SFANet optimizes the processing of high-dimensional spatiotemporal sequences, preserving inter-component relational information and modeling extensive long-range relationships. To further enhance feature integration, we introduce a novel spatial-frequency attention module, enabling the model to capture intricate cross-modal correlations. Our extensive experimental evaluation on two distinct datasets, the Storm EVent ImageRy (SEVIR) and the Institute for Climate and Application Research (ICAR) - El Ni\\~{n}o Southern Oscillation (ENSO) dataset, demonstrates the remarkable performance of SFANet. Notably, SFANet achieves substantial advancements over state-of-the-art methods, showcasing its proficiency in forecasting precipitation patterns and predicting El Ni\\~{n}o events.","sentences":["Weather forecasting plays a critical role in various sectors, driving decision-making and risk management.","However, traditional methods often struggle to capture the complex dynamics of meteorological systems, particularly in the presence of high-resolution data.","In this paper, we propose the Spatial-Frequency Attention Network (SFANet), a novel deep learning framework designed to address these challenges and enhance the accuracy of spatiotemporal weather prediction.","Drawing inspiration from the limitations of existing methodologies, we present an innovative approach that seamlessly integrates advanced token mixing and attention mechanisms.","By leveraging both pooling and spatial mixing strategies, SFANet optimizes the processing of high-dimensional spatiotemporal sequences, preserving inter-component relational information and modeling extensive long-range relationships.","To further enhance feature integration, we introduce a novel spatial-frequency attention module, enabling the model to capture intricate cross-modal correlations.","Our extensive experimental evaluation on two distinct datasets, the Storm EVent ImageRy (SEVIR) and the Institute for Climate and Application Research (ICAR) - El Ni\\~{n}o Southern Oscillation (ENSO) dataset, demonstrates the remarkable performance of SFANet.","Notably, SFANet achieves substantial advancements over state-of-the-art methods, showcasing its proficiency in forecasting precipitation patterns and predicting El Ni\\~{n}o events."],"url":"http://arxiv.org/abs/2405.18849v1"}
{"created":"2024-05-29 07:59:06","title":"Anomaly Detection by Context Contrasting","abstract":"Anomaly Detection focuses on identifying samples that deviate from the norm. When working with high-dimensional data such as images, a crucial requirement for detecting anomalous patterns is learning lower-dimensional representations that capture normal concepts seen during training. Recent advances in self-supervised learning have shown great promise in this regard. However, many of the most successful self-supervised anomaly detection methods assume prior knowledge about the structure of anomalies and leverage synthetic anomalies during training. Yet, in many real-world applications, we do not know what to expect from unseen data, and we can solely leverage knowledge about normal data. In this work, we propose Con2, which addresses this problem by setting normal training data into distinct contexts while preserving its normal properties, letting us observe the data from different perspectives. Unseen normal data consequently adheres to learned context representations while anomalies fail to do so, letting us detect them without any knowledge about anomalies during training. Our experiments demonstrate that our approach achieves state-of-the-art performance on various benchmarks while exhibiting superior performance in a more realistic healthcare setting, where knowledge about potential anomalies is often scarce.","sentences":["Anomaly Detection focuses on identifying samples that deviate from the norm.","When working with high-dimensional data such as images, a crucial requirement for detecting anomalous patterns is learning lower-dimensional representations that capture normal concepts seen during training.","Recent advances in self-supervised learning have shown great promise in this regard.","However, many of the most successful self-supervised anomaly detection methods assume prior knowledge about the structure of anomalies and leverage synthetic anomalies during training.","Yet, in many real-world applications, we do not know what to expect from unseen data, and we can solely leverage knowledge about normal data.","In this work, we propose Con2, which addresses this problem by setting normal training data into distinct contexts while preserving its normal properties, letting us observe the data from different perspectives.","Unseen normal data consequently adheres to learned context representations while anomalies fail to do so, letting us detect them without any knowledge about anomalies during training.","Our experiments demonstrate that our approach achieves state-of-the-art performance on various benchmarks while exhibiting superior performance in a more realistic healthcare setting, where knowledge about potential anomalies is often scarce."],"url":"http://arxiv.org/abs/2405.18848v1"}
{"created":"2024-05-29 07:56:08","title":"Simulation, Modelling and Classification of Wiki Contributors: Spotting The Good, The Bad, and The Ugly","abstract":"Data crowdsourcing is a data acquisition process where groups of voluntary contributors feed platforms with highly relevant data ranging from news, comments, and media to knowledge and classifications. It typically processes user-generated data streams to provide and refine popular services such as wikis, collaborative maps, e-commerce sites, and social networks. Nevertheless, this modus operandi raises severe concerns regarding ill-intentioned data manipulation in adversarial environments. This paper presents a simulation, modelling, and classification approach to automatically identify human and non-human (bots) as well as benign and malign contributors by using data fabrication to balance classes within experimental data sets, data stream modelling to build and update contributor profiles and, finally, autonomic data stream classification. By employing WikiVoyage - a free worldwide wiki travel guide open to contribution from the general public - as a testbed, our approach proves to significantly boost the confidence and quality of the classifier by using a class-balanced data stream, comprising both real and synthetic data. Our empirical results show that the proposed method distinguishes between benign and malign bots as well as human contributors with a classification accuracy of up to 92 %.","sentences":["Data crowdsourcing is a data acquisition process where groups of voluntary contributors feed platforms with highly relevant data ranging from news, comments, and media to knowledge and classifications.","It typically processes user-generated data streams to provide and refine popular services such as wikis, collaborative maps, e-commerce sites, and social networks.","Nevertheless, this modus operandi raises severe concerns regarding ill-intentioned data manipulation in adversarial environments.","This paper presents a simulation, modelling, and classification approach to automatically identify human and non-human (bots) as well as benign and malign contributors by using data fabrication to balance classes within experimental data sets, data stream modelling to build and update contributor profiles and, finally, autonomic data stream classification.","By employing WikiVoyage - a free worldwide wiki travel guide open to contribution from the general public - as a testbed, our approach proves to significantly boost the confidence and quality of the classifier by using a class-balanced data stream, comprising both real and synthetic data.","Our empirical results show that the proposed method distinguishes between benign and malign bots as well as human contributors with a classification accuracy of up to 92 %."],"url":"http://arxiv.org/abs/2405.18845v1"}
{"created":"2024-05-29 07:50:47","title":"Data-driven Machinery Fault Detection: A Comprehensive Review","abstract":"In this era of advanced manufacturing, it's now more crucial than ever to diagnose machine faults as early as possible to guarantee their safe and efficient operation. With the massive surge in industrial big data and advancement in sensing and computational technologies, data-driven Machinery Fault Diagnosis (MFD) solutions based on machine/deep learning approaches have been used ubiquitously in manufacturing. Timely and accurately identifying faulty machine signals is vital in industrial applications for which many relevant solutions have been proposed and are reviewed in many articles. Despite the availability of numerous solutions and reviews on MFD, existing works often lack several aspects. Most of the available literature has limited applicability in a wide range of manufacturing settings due to their concentration on a particular type of equipment or method of analysis. Additionally, discussions regarding the challenges associated with implementing data-driven approaches, such as dealing with noisy data, selecting appropriate features, and adapting models to accommodate new or unforeseen faults, are often superficial or completely overlooked. Thus, this survey provides a comprehensive review of the articles using different types of machine learning approaches for the detection and diagnosis of various types of machinery faults, highlights their strengths and limitations, provides a review of the methods used for condition-based analyses, comprehensively discusses the available machinery fault datasets, introduces future researchers to the possible challenges they have to encounter while using these approaches for MFD and recommends the probable solutions to mitigate those problems. The future research prospects are also pointed out for a better understanding of the field. We believe this article will help researchers and contribute to the further development of the field.","sentences":["In this era of advanced manufacturing, it's now more crucial than ever to diagnose machine faults as early as possible to guarantee their safe and efficient operation.","With the massive surge in industrial big data and advancement in sensing and computational technologies, data-driven Machinery Fault Diagnosis (MFD) solutions based on machine/deep learning approaches have been used ubiquitously in manufacturing.","Timely and accurately identifying faulty machine signals is vital in industrial applications for which many relevant solutions have been proposed and are reviewed in many articles.","Despite the availability of numerous solutions and reviews on MFD, existing works often lack several aspects.","Most of the available literature has limited applicability in a wide range of manufacturing settings due to their concentration on a particular type of equipment or method of analysis.","Additionally, discussions regarding the challenges associated with implementing data-driven approaches, such as dealing with noisy data, selecting appropriate features, and adapting models to accommodate new or unforeseen faults, are often superficial or completely overlooked.","Thus, this survey provides a comprehensive review of the articles using different types of machine learning approaches for the detection and diagnosis of various types of machinery faults, highlights their strengths and limitations, provides a review of the methods used for condition-based analyses, comprehensively discusses the available machinery fault datasets, introduces future researchers to the possible challenges they have to encounter while using these approaches for MFD and recommends the probable solutions to mitigate those problems.","The future research prospects are also pointed out for a better understanding of the field.","We believe this article will help researchers and contribute to the further development of the field."],"url":"http://arxiv.org/abs/2405.18843v1"}
{"created":"2024-05-29 07:49:15","title":"Descriptive Image Quality Assessment in the Wild","abstract":"With the rapid advancement of Vision Language Models (VLMs), VLM-based Image Quality Assessment (IQA) seeks to describe image quality linguistically to align with human expression and capture the multifaceted nature of IQA tasks. However, current methods are still far from practical usage. First, prior works focus narrowly on specific sub-tasks or settings, which do not align with diverse real-world applications. Second, their performance is sub-optimal due to limitations in dataset coverage, scale, and quality. To overcome these challenges, we introduce Depicted image Quality Assessment in the Wild (DepictQA-Wild). Our method includes a multi-functional IQA task paradigm that encompasses both assessment and comparison tasks, brief and detailed responses, full-reference and non-reference scenarios. We introduce a ground-truth-informed dataset construction approach to enhance data quality, and scale up the dataset to 495K under the brief-detail joint framework. Consequently, we construct a comprehensive, large-scale, and high-quality dataset, named DQ-495K. We also retain image resolution during training to better handle resolution-related quality issues, and estimate a confidence score that is helpful to filter out low-quality responses. Experimental results demonstrate that DepictQA-Wild significantly outperforms traditional score-based methods, prior VLM-based IQA models, and proprietary GPT-4V in distortion identification, instant rating, and reasoning tasks. Our advantages are further confirmed by real-world applications including assessing the web-downloaded images and ranking model-processed images. Datasets and codes will be released in https://depictqa.github.io/depictqa-wild/.","sentences":["With the rapid advancement of Vision Language Models (VLMs), VLM-based Image Quality Assessment (IQA) seeks to describe image quality linguistically to align with human expression and capture the multifaceted nature of IQA tasks.","However, current methods are still far from practical usage.","First, prior works focus narrowly on specific sub-tasks or settings, which do not align with diverse real-world applications.","Second, their performance is sub-optimal due to limitations in dataset coverage, scale, and quality.","To overcome these challenges, we introduce Depicted image Quality Assessment in the Wild (DepictQA-Wild).","Our method includes a multi-functional IQA task paradigm that encompasses both assessment and comparison tasks, brief and detailed responses, full-reference and non-reference scenarios.","We introduce a ground-truth-informed dataset construction approach to enhance data quality, and scale up the dataset to 495K under the brief-detail joint framework.","Consequently, we construct a comprehensive, large-scale, and high-quality dataset, named DQ-495K. We also retain image resolution during training to better handle resolution-related quality issues, and estimate a confidence score that is helpful to filter out low-quality responses.","Experimental results demonstrate that DepictQA-Wild significantly outperforms traditional score-based methods, prior VLM-based IQA models, and proprietary GPT-4V in distortion identification, instant rating, and reasoning tasks.","Our advantages are further confirmed by real-world applications including assessing the web-downloaded images and ranking model-processed images.","Datasets and codes will be released in https://depictqa.github.io/depictqa-wild/."],"url":"http://arxiv.org/abs/2405.18842v1"}
{"created":"2024-05-29 07:23:29","title":"MoNDE: Mixture of Near-Data Experts for Large-Scale Sparse Models","abstract":"Mixture-of-Experts (MoE) large language models (LLM) have memory requirements that often exceed the GPU memory capacity, requiring costly parameter movement from secondary memories to the GPU for expert computation. In this work, we present Mixture of Near-Data Experts (MoNDE), a near-data computing solution that efficiently enables MoE LLM inference. MoNDE reduces the volume of MoE parameter movement by transferring only the $\\textit{hot}$ experts to the GPU, while computing the remaining $\\textit{cold}$ experts inside the host memory device. By replacing the transfers of massive expert parameters with the ones of small activations, MoNDE enables far more communication-efficient MoE inference, thereby resulting in substantial speedups over the existing parameter offloading frameworks for both encoder and decoder operations.","sentences":["Mixture-of-Experts (MoE) large language models (LLM) have memory requirements that often exceed the GPU memory capacity, requiring costly parameter movement from secondary memories to the GPU for expert computation.","In this work, we present Mixture of Near-Data Experts (MoNDE), a near-data computing solution that efficiently enables MoE LLM inference.","MoNDE reduces the volume of MoE parameter movement by transferring only the $\\textit{hot}$ experts to the GPU, while computing the remaining $\\textit{cold}$ experts inside the host memory device.","By replacing the transfers of massive expert parameters with the ones of small activations, MoNDE enables far more communication-efficient MoE inference, thereby resulting in substantial speedups over the existing parameter offloading frameworks for both encoder and decoder operations."],"url":"http://arxiv.org/abs/2405.18832v1"}
{"created":"2024-05-29 07:11:58","title":"Theoretical insights and an experimental comparison of tango trees and multi-splay trees","abstract":"The tango tree is the first proven $O(\\lg \\lg n)$-competitive binary search tree (BST). We present the first ever experimental implementation of tango trees and compare the running time of the tango tree with the multi-splay tree and the splay tree on a variety of families of access sequences. We construct access sequences that are intended to test specific properties of BSTs. The results of the other experiments demonstrate the optimality of the splay tree and multi-splay tree on these accesses, while simultaneously demonstrating the tango trees inability to achieve optimality. We prove that the running time of tango trees on the sequential access is $\\Theta(n \\lg \\lg n)$, which provides insight into why the $\\Theta(\\lg \\lg n)$ slow down exists on many access sequences. Motivated by experimental results, we conduct a deeper analysis of the working set access on multi-splay trees, leading to new insights about multi-splay tree behavior. Finally, all of the experiments also reveal insights about large constants and lower order terms in the multi-splay tree, which make it less practical than the splay tree, even though its proven competitive bound is tighter.","sentences":["The tango tree is the first proven $O(\\lg \\lg n)$-competitive binary search tree (BST).","We present the first ever experimental implementation of tango trees and compare the running time of the tango tree with the multi-splay tree and the splay tree on a variety of families of access sequences.","We construct access sequences that are intended to test specific properties of BSTs.","The results of the other experiments demonstrate the optimality of the splay tree and multi-splay tree on these accesses, while simultaneously demonstrating the tango trees inability to achieve optimality.","We prove that the running time of tango trees on the sequential access is $\\Theta(n \\lg \\lg n)$, which provides insight into why the $\\Theta(\\lg \\lg n)$ slow down exists on many access sequences.","Motivated by experimental results, we conduct a deeper analysis of the working set access on multi-splay trees, leading to new insights about multi-splay tree behavior.","Finally, all of the experiments also reveal insights about large constants and lower order terms in the multi-splay tree, which make it less practical than the splay tree, even though its proven competitive bound is tighter."],"url":"http://arxiv.org/abs/2405.18825v1"}
{"created":"2024-05-29 07:00:28","title":"Diffeomorphic interpolation for efficient persistence-based topological optimization","abstract":"Topological Data Analysis (TDA) provides a pipeline to extract quantitative topological descriptors from structured objects. This enables the definition of topological loss functions, which assert to what extent a given object exhibits some topological properties. These losses can then be used to perform topological optimizationvia gradient descent routines. While theoretically sounded, topological optimization faces an important challenge: gradients tend to be extremely sparse, in the sense that the loss function typically depends on only very few coordinates of the input object, yielding dramatically slow optimization schemes in practice.Focusing on the central case of topological optimization for point clouds, we propose in this work to overcome this limitation using diffeomorphic interpolation, turning sparse gradients into smooth vector fields defined on the whole space, with quantifiable Lipschitz constants. In particular, we show that our approach combines efficiently with subsampling techniques routinely used in TDA, as the diffeomorphism derived from the gradient computed on a subsample can be used to update the coordinates of the full input object, allowing us to perform topological optimization on point clouds at an unprecedented scale. Finally, we also showcase the relevance of our approach for black-box autoencoder (AE) regularization, where we aim at enforcing topological priors on the latent spaces associated to fixed, pre-trained, black-box AE models, and where we show thatlearning a diffeomorphic flow can be done once and then re-applied to new data in linear time (while vanilla topological optimization has to be re-run from scratch). Moreover, reverting the flow allows us to generate data by sampling the topologically-optimized latent space directly, yielding better interpretability of the model.","sentences":["Topological Data Analysis (TDA) provides a pipeline to extract quantitative topological descriptors from structured objects.","This enables the definition of topological loss functions, which assert to what extent a given object exhibits some topological properties.","These losses can then be used to perform topological optimizationvia gradient descent routines.","While theoretically sounded, topological optimization faces an important challenge: gradients tend to be extremely sparse, in the sense that the loss function typically depends on only very few coordinates of the input object, yielding dramatically slow optimization schemes in practice.","Focusing on the central case of topological optimization for point clouds, we propose in this work to overcome this limitation using diffeomorphic interpolation, turning sparse gradients into smooth vector fields defined on the whole space, with quantifiable Lipschitz constants.","In particular, we show that our approach combines efficiently with subsampling techniques routinely used in TDA, as the diffeomorphism derived from the gradient computed on a subsample can be used to update the coordinates of the full input object, allowing us to perform topological optimization on point clouds at an unprecedented scale.","Finally, we also showcase the relevance of our approach for black-box autoencoder (AE) regularization, where we aim at enforcing topological priors on the latent spaces associated to fixed, pre-trained, black-box AE models, and where we show thatlearning a diffeomorphic flow can be done once and then re-applied to new data in linear time (while vanilla topological optimization has to be re-run from scratch).","Moreover, reverting the flow allows us to generate data by sampling the topologically-optimized latent space directly, yielding better interpretability of the model."],"url":"http://arxiv.org/abs/2405.18820v1"}
{"created":"2024-05-29 06:53:18","title":"UniPTS: A Unified Framework for Proficient Post-Training Sparsity","abstract":"Post-training Sparsity (PTS) is a recently emerged avenue that chases efficient network sparsity with limited data in need. Existing PTS methods, however, undergo significant performance degradation compared with traditional methods that retrain the sparse networks via the whole dataset, especially at high sparsity ratios. In this paper, we attempt to reconcile this disparity by transposing three cardinal factors that profoundly alter the performance of conventional sparsity into the context of PTS. Our endeavors particularly comprise (1) A base-decayed sparsity objective that promotes efficient knowledge transferring from dense network to the sparse counterpart. (2) A reducing-regrowing search algorithm designed to ascertain the optimal sparsity distribution while circumventing overfitting to the small calibration set in PTS. (3) The employment of dynamic sparse training predicated on the preceding aspects, aimed at comprehensively optimizing the sparsity structure while ensuring training stability. Our proposed framework, termed UniPTS, is validated to be much superior to existing PTS methods across extensive benchmarks. As an illustration, it amplifies the performance of POT, a recently proposed recipe, from 3.9% to 68.6% when pruning ResNet-50 at 90% sparsity ratio on ImageNet. We release the code of our paper at https://github.com/xjjxmu/UniPTS.","sentences":["Post-training Sparsity (PTS) is a recently emerged avenue that chases efficient network sparsity with limited data in need.","Existing PTS methods, however, undergo significant performance degradation compared with traditional methods that retrain the sparse networks via the whole dataset, especially at high sparsity ratios.","In this paper, we attempt to reconcile this disparity by transposing three cardinal factors that profoundly alter the performance of conventional sparsity into the context of PTS.","Our endeavors particularly comprise (1) A base-decayed sparsity objective that promotes efficient knowledge transferring from dense network to the sparse counterpart.","(2) A reducing-regrowing search algorithm designed to ascertain the optimal sparsity distribution while circumventing overfitting to the small calibration set in PTS.","(3) The employment of dynamic sparse training predicated on the preceding aspects, aimed at comprehensively optimizing the sparsity structure while ensuring training stability.","Our proposed framework, termed UniPTS, is validated to be much superior to existing PTS methods across extensive benchmarks.","As an illustration, it amplifies the performance of POT, a recently proposed recipe, from 3.9% to 68.6% when pruning ResNet-50 at 90% sparsity ratio on ImageNet.","We release the code of our paper at https://github.com/xjjxmu/UniPTS."],"url":"http://arxiv.org/abs/2405.18810v1"}
{"created":"2024-05-29 06:52:03","title":"Multiplicative Weights Update, Area Convexity and Random Coordinate Descent for Densest Subgraph Problems","abstract":"We study the densest subgraph problem and give algorithms via multiplicative weights updated area convexity that converge in $O\\left(\\frac{\\log m}{\\epsilon^{2}}\\right)$ and $O\\left(\\frac{\\log m}{\\epsilon}\\right)$ iterations, respectively, both with nearly-linear time per iteration. Compared with the work by Bahmani et al. (2014), our MWU algorithm uses a very different and much simpler procedure for recovering the dense subgraph from the fractional solution and does not employ a binary search. Compared with the work by Boob et al. (2019), our algorithm via area convexity improves the iteration complexity by a factor $\\Delta$ -- the maximum degree in the graph, and matches the fastest theoretical runtime currently known via flows (Chekuri et al., 2022) in total time. Next, we study the dense subgraph decomposition problem and give the first practical iterative algorithm with linear convergence rate $O\\left(mn\\log\\frac{1}{\\epsilon}\\right)$ via accelerated random coordinate descent. This significantly improves over $O\\left(\\frac{m\\sqrt{mn\\Delta}}{\\epsilon}\\right)$ time of the FISTA-based algorithm by Harb et al. (2022). In the high precision regime $\\epsilon\\ll\\frac{1}{n}$ where we can even recover the exact solution, our algorithm has a total runtime of $O\\left(mn\\log n\\right)$, matching the exact algorithm via parametric flows (Gallo et al., 1989). Empirically, we show that this algorithm is very practical and scales to very large graphs, and its performance is competitive with widely used methods that have significantly weaker theoretical guarantees.","sentences":["We study the densest subgraph problem and give algorithms via multiplicative weights updated area convexity that converge in $O\\left(\\frac{\\log m}{\\epsilon^{2}}\\right)$ and $O\\left(\\frac{\\log m}{\\epsilon}\\right)$ iterations, respectively, both with nearly-linear time per iteration.","Compared with the work by Bahmani et al. (2014), our MWU algorithm uses a very different and much simpler procedure for recovering the dense subgraph from the fractional solution and does not employ a binary search.","Compared with the work by Boob et al. (2019), our algorithm via area convexity improves the iteration complexity by a factor $\\Delta$ -- the maximum degree in the graph, and matches the fastest theoretical runtime currently known via flows (Chekuri et al., 2022) in total time.","Next, we study the dense subgraph decomposition problem and give the first practical iterative algorithm with linear convergence rate $O\\left(mn\\log\\frac{1}{\\epsilon}\\right)$ via accelerated random coordinate descent.","This significantly improves over $O\\left(\\frac{m\\sqrt{mn\\Delta}}{\\epsilon}\\right)$ time of the FISTA-based algorithm by Harb et al. (2022).","In the high precision regime $\\epsilon\\ll\\frac{1}{n}$ where we can even recover the exact solution, our algorithm has a total runtime of $O\\left(mn\\log n\\right)$, matching the exact algorithm via parametric flows (Gallo et al., 1989).","Empirically, we show that this algorithm is very practical and scales to very large graphs, and its performance is competitive with widely used methods that have significantly weaker theoretical guarantees."],"url":"http://arxiv.org/abs/2405.18809v1"}
{"created":"2024-05-29 06:47:34","title":"Tilde: Teleoperation for Dexterous In-Hand Manipulation Learning with a DeltaHand","abstract":"Dexterous robotic manipulation remains a challenging domain due to its strict demands for precision and robustness on both hardware and software. While dexterous robotic hands have demonstrated remarkable capabilities in complex tasks, efficiently learning adaptive control policies for hands still presents a significant hurdle given the high dimensionalities of hands and tasks. To bridge this gap, we propose Tilde, an imitation learning-based in-hand manipulation system on a dexterous DeltaHand. It leverages 1) a low-cost, configurable, simple-to-control, soft dexterous robotic hand, DeltaHand, 2) a user-friendly, precise, real-time teleoperation interface, TeleHand, and 3) an efficient and generalizable imitation learning approach with diffusion policies. Our proposed TeleHand has a kinematic twin design to the DeltaHand that enables precise one-to-one joint control of the DeltaHand during teleoperation. This facilitates efficient high-quality data collection of human demonstrations in the real world. To evaluate the effectiveness of our system, we demonstrate the fully autonomous closed-loop deployment of diffusion policies learned from demonstrations across seven dexterous manipulation tasks with an average 90% success rate.","sentences":["Dexterous robotic manipulation remains a challenging domain due to its strict demands for precision and robustness on both hardware and software.","While dexterous robotic hands have demonstrated remarkable capabilities in complex tasks, efficiently learning adaptive control policies for hands still presents a significant hurdle given the high dimensionalities of hands and tasks.","To bridge this gap, we propose Tilde, an imitation learning-based in-hand manipulation system on a dexterous DeltaHand.","It leverages 1) a low-cost, configurable, simple-to-control, soft dexterous robotic hand, DeltaHand, 2) a user-friendly, precise, real-time teleoperation interface, TeleHand, and 3) an efficient and generalizable imitation learning approach with diffusion policies.","Our proposed TeleHand has a kinematic twin design to the DeltaHand that enables precise one-to-one joint control of the DeltaHand during teleoperation.","This facilitates efficient high-quality data collection of human demonstrations in the real world.","To evaluate the effectiveness of our system, we demonstrate the fully autonomous closed-loop deployment of diffusion policies learned from demonstrations across seven dexterous manipulation tasks with an average 90% success rate."],"url":"http://arxiv.org/abs/2405.18804v1"}
{"created":"2024-05-29 06:46:10","title":"Enhancing Security and Privacy in Federated Learning using Update Digests and Voting-Based Defense","abstract":"Federated Learning (FL) is a promising privacy-preserving machine learning paradigm that allows data owners to collaboratively train models while keeping their data localized. Despite its potential, FL faces challenges related to the trustworthiness of both clients and servers, especially in the presence of curious or malicious adversaries. In this paper, we introduce a novel framework named \\underline{\\textbf{F}}ederated \\underline{\\textbf{L}}earning with \\underline{\\textbf{U}}pdate \\underline{\\textbf{D}}igest (FLUD), which addresses the critical issues of privacy preservation and resistance to Byzantine attacks within distributed learning environments. FLUD utilizes an innovative approach, the $\\mathsf{LinfSample}$ method, allowing clients to compute the $l_{\\infty}$ norm across sliding windows of updates as an update digest. This digest enables the server to calculate a shared distance matrix, significantly reducing the overhead associated with Secure Multi-Party Computation (SMPC) by three orders of magnitude while effectively distinguishing between benign and malicious updates. Additionally, FLUD integrates a privacy-preserving, voting-based defense mechanism that employs optimized SMPC protocols to minimize communication rounds. Our comprehensive experiments demonstrate FLUD's effectiveness in countering Byzantine adversaries while incurring low communication and runtime overhead. FLUD offers a scalable framework for secure and reliable FL in distributed environments, facilitating its application in scenarios requiring robust data management and security.","sentences":["Federated Learning (FL) is a promising privacy-preserving machine learning paradigm that allows data owners to collaboratively train models while keeping their data localized.","Despite its potential, FL faces challenges related to the trustworthiness of both clients and servers, especially in the presence of curious or malicious adversaries.","In this paper, we introduce a novel framework named \\underline{\\textbf{F}}ederated \\underline{\\textbf{L}}earning with \\underline{\\textbf{U}}pdate \\underline{\\textbf{D}}igest (FLUD), which addresses the critical issues of privacy preservation and resistance to Byzantine attacks within distributed learning environments.","FLUD utilizes an innovative approach, the $\\mathsf{LinfSample}$ method, allowing clients to compute the $l_{\\infty}$ norm across sliding windows of updates as an update digest.","This digest enables the server to calculate a shared distance matrix, significantly reducing the overhead associated with Secure Multi-Party Computation (SMPC) by three orders of magnitude while effectively distinguishing between benign and malicious updates.","Additionally, FLUD integrates a privacy-preserving, voting-based defense mechanism that employs optimized SMPC protocols to minimize communication rounds.","Our comprehensive experiments demonstrate FLUD's effectiveness in countering Byzantine adversaries while incurring low communication and runtime overhead.","FLUD offers a scalable framework for secure and reliable FL in distributed environments, facilitating its application in scenarios requiring robust data management and security."],"url":"http://arxiv.org/abs/2405.18802v1"}
{"created":"2024-05-29 06:33:25","title":"User Association and Channel Allocation in 5G Mobile Asymmetric Multi-band Heterogeneous Networks","abstract":"With the proliferation of mobile terminals and the continuous upgrading of services, 4G LTE networks are showing signs of weakness. To enhance the capacity of wireless networks, millimeter waves are introduced to drive the evolution of networks towards multi-band 5G heterogeneous networks. The distinct propagation characteristics of mmWaves and microwaves, as well as the vastly different hardware configurations of heterogeneous base stations, make traditional access strategies no longer effective. Therefore, to narrowing the gap between theory and practice, we investigate the access strategy in multi-band 5G heterogeneous networks, taking into account the characteristics of mobile users, asynchronous switching between uplink and downlink of pico base stations, asymmetric service requirements, and user communication continuity. We formulate the problem as integer nonlinear programming and prove its intractability. Thereby, we decouple it into three subproblems: user association, switch point selection, and subchannel allocation, and design an algorithm based on optimal matching and spectral clustering to solve it efficiently. The simulation results show that the proposed algorithm outperforms the comparison methods in terms of overall data rate, effective data rate, and number of satisfied users.","sentences":["With the proliferation of mobile terminals and the continuous upgrading of services, 4G LTE networks are showing signs of weakness.","To enhance the capacity of wireless networks, millimeter waves are introduced to drive the evolution of networks towards multi-band 5G heterogeneous networks.","The distinct propagation characteristics of mmWaves and microwaves, as well as the vastly different hardware configurations of heterogeneous base stations, make traditional access strategies no longer effective.","Therefore, to narrowing the gap between theory and practice, we investigate the access strategy in multi-band 5G heterogeneous networks, taking into account the characteristics of mobile users, asynchronous switching between uplink and downlink of pico base stations, asymmetric service requirements, and user communication continuity.","We formulate the problem as integer nonlinear programming and prove its intractability.","Thereby, we decouple it into three subproblems: user association, switch point selection, and subchannel allocation, and design an algorithm based on optimal matching and spectral clustering to solve it efficiently.","The simulation results show that the proposed algorithm outperforms the comparison methods in terms of overall data rate, effective data rate, and number of satisfied users."],"url":"http://arxiv.org/abs/2405.18797v1"}
{"created":"2024-05-29 06:09:34","title":"Opinion-Unaware Blind Image Quality Assessment using Multi-Scale Deep Feature Statistics","abstract":"Deep learning-based methods have significantly influenced the blind image quality assessment (BIQA) field, however, these methods often require training using large amounts of human rating data. In contrast, traditional knowledge-based methods are cost-effective for training but face challenges in effectively extracting features aligned with human visual perception. To bridge these gaps, we propose integrating deep features from pre-trained visual models with a statistical analysis model into a Multi-scale Deep Feature Statistics (MDFS) model for achieving opinion-unaware BIQA (OU-BIQA), thereby eliminating the reliance on human rating data and significantly improving training efficiency. Specifically, we extract patch-wise multi-scale features from pre-trained vision models, which are subsequently fitted into a multivariate Gaussian (MVG) model. The final quality score is determined by quantifying the distance between the MVG model derived from the test image and the benchmark MVG model derived from the high-quality image set. A comprehensive series of experiments conducted on various datasets show that our proposed model exhibits superior consistency with human visual perception compared to state-of-the-art BIQA models. Furthermore, it shows improved generalizability across diverse target-specific BIQA tasks. Our code is available at: https://github.com/eezkni/MDFS","sentences":["Deep learning-based methods have significantly influenced the blind image quality assessment (BIQA) field, however, these methods often require training using large amounts of human rating data.","In contrast, traditional knowledge-based methods are cost-effective for training but face challenges in effectively extracting features aligned with human visual perception.","To bridge these gaps, we propose integrating deep features from pre-trained visual models with a statistical analysis model into a Multi-scale Deep Feature Statistics (MDFS) model for achieving opinion-unaware BIQA (OU-BIQA), thereby eliminating the reliance on human rating data and significantly improving training efficiency.","Specifically, we extract patch-wise multi-scale features from pre-trained vision models, which are subsequently fitted into a multivariate Gaussian (MVG) model.","The final quality score is determined by quantifying the distance between the MVG model derived from the test image and the benchmark MVG model derived from the high-quality image set.","A comprehensive series of experiments conducted on various datasets show that our proposed model exhibits superior consistency with human visual perception compared to state-of-the-art BIQA models.","Furthermore, it shows improved generalizability across diverse target-specific BIQA tasks.","Our code is available at: https://github.com/eezkni/MDFS"],"url":"http://arxiv.org/abs/2405.18790v1"}
{"created":"2024-05-29 06:05:47","title":"An overview of some single machine scheduling problems: polynomial algorithms, complexity and approximability","abstract":"Since the publication of the first scheduling paper in 1954, a huge number of works dealing with different types of single machine problems appeared. They addressed many heuristics and enumerative procedures, complexity results or structural properties of certain problems. Regarding surveys, often particular subjects like special objective functions are discussed, or more general scheduling problems were surveyed, where a substantial part is devoted to single machine problems. In this paper we present some results on polynomial algorithms, complexity and approximation issues, where the main focus is on results, which have been published during the last decades in papers, where at least one of the first two authors of this paper was involved. We hope that the reviewed results will stimulate further investigation in related research fields.","sentences":["Since the publication of the first scheduling paper in 1954, a huge number of works dealing with different types of single machine problems appeared.","They addressed many heuristics and enumerative procedures, complexity results or structural properties of certain problems.","Regarding surveys, often particular subjects like special objective functions are discussed, or more general scheduling problems were surveyed, where a substantial part is devoted to single machine problems.","In this paper we present some results on polynomial algorithms, complexity and approximation issues, where the main focus is on results, which have been published during the last decades in papers, where at least one of the first two authors of this paper was involved.","We hope that the reviewed results will stimulate further investigation in related research fields."],"url":"http://arxiv.org/abs/2405.18789v1"}
{"created":"2024-05-29 05:59:52","title":"MOKD: Cross-domain Finetuning for Few-shot Classification via Maximizing Optimized Kernel Dependence","abstract":"In cross-domain few-shot classification, \\emph{nearest centroid classifier} (NCC) aims to learn representations to construct a metric space where few-shot classification can be performed by measuring the similarities between samples and the prototype of each class. An intuition behind NCC is that each sample is pulled closer to the class centroid it belongs to while pushed away from those of other classes. However, in this paper, we find that there exist high similarities between NCC-learned representations of two samples from different classes. In order to address this problem, we propose a bi-level optimization framework, \\emph{maximizing optimized kernel dependence} (MOKD) to learn a set of class-specific representations that match the cluster structures indicated by labeled data of the given task. Specifically, MOKD first optimizes the kernel adopted in \\emph{Hilbert-Schmidt independence criterion} (HSIC) to obtain the optimized kernel HSIC (opt-HSIC) that can capture the dependence more precisely. Then, an optimization problem regarding the opt-HSIC is addressed to simultaneously maximize the dependence between representations and labels and minimize the dependence among all samples. Extensive experiments on Meta-Dataset demonstrate that MOKD can not only achieve better generalization performance on unseen domains in most cases but also learn better data representation clusters. The project repository of MOKD is available at: \\href{https://github.com/tmlr-group/MOKD}{https://github.com/tmlr-group/MOKD}.","sentences":["In cross-domain few-shot classification, \\emph{nearest centroid classifier} (NCC) aims to learn representations to construct a metric space where few-shot classification can be performed by measuring the similarities between samples and the prototype of each class.","An intuition behind NCC is that each sample is pulled closer to the class centroid it belongs to while pushed away from those of other classes.","However, in this paper, we find that there exist high similarities between NCC-learned representations of two samples from different classes.","In order to address this problem, we propose a bi-level optimization framework, \\emph{maximizing optimized kernel dependence} (MOKD) to learn a set of class-specific representations that match the cluster structures indicated by labeled data of the given task.","Specifically, MOKD first optimizes the kernel adopted in \\emph{Hilbert-Schmidt independence criterion} (HSIC) to obtain the optimized kernel HSIC (opt-HSIC) that can capture the dependence more precisely.","Then, an optimization problem regarding the opt-HSIC is addressed to simultaneously maximize the dependence between representations and labels and minimize the dependence among all samples.","Extensive experiments on Meta-Dataset demonstrate that MOKD can not only achieve better generalization performance on unseen domains in most cases but also learn better data representation clusters.","The project repository of MOKD is available at: \\href{https://github.com/tmlr-group/MOKD}{https://github.com/tmlr-group/MOKD}."],"url":"http://arxiv.org/abs/2405.18786v1"}
{"created":"2024-05-29 05:20:02","title":"Leveraging Many-To-Many Relationships for Defending Against Visual-Language Adversarial Attacks","abstract":"Recent studies have revealed that vision-language (VL) models are vulnerable to adversarial attacks for image-text retrieval (ITR). However, existing defense strategies for VL models primarily focus on zero-shot image classification, which do not consider the simultaneous manipulation of image and text, as well as the inherent many-to-many (N:N) nature of ITR, where a single image can be described in numerous ways, and vice versa. To this end, this paper studies defense strategies against adversarial attacks on VL models for ITR for the first time. Particularly, we focus on how to leverage the N:N relationship in ITR to enhance adversarial robustness. We found that, although adversarial training easily overfits to specific one-to-one (1:1) image-text pairs in the train data, diverse augmentation techniques to create one-to-many (1:N) / many-to-one (N:1) image-text pairs can significantly improve adversarial robustness in VL models. Additionally, we show that the alignment of the augmented image-text pairs is crucial for the effectiveness of the defense strategy, and that inappropriate augmentations can even degrade the model's performance. Based on these findings, we propose a novel defense strategy that leverages the N:N relationship in ITR, which effectively generates diverse yet highly-aligned N:N pairs using basic augmentations and generative model-based augmentations. This work provides a novel perspective on defending against adversarial attacks in VL tasks and opens up new research directions for future work.","sentences":["Recent studies have revealed that vision-language (VL) models are vulnerable to adversarial attacks for image-text retrieval (ITR).","However, existing defense strategies for VL models primarily focus on zero-shot image classification, which do not consider the simultaneous manipulation of image and text, as well as the inherent many-to-many (N:N) nature of ITR, where a single image can be described in numerous ways, and vice versa.","To this end, this paper studies defense strategies against adversarial attacks on VL models for ITR for the first time.","Particularly, we focus on how to leverage the N:N relationship in ITR to enhance adversarial robustness.","We found that, although adversarial training easily overfits to specific one-to-one (1:1) image-text pairs in the train data, diverse augmentation techniques to create one-to-many (1:N) / many-to-one (N:1) image-text pairs can significantly improve adversarial robustness in VL models.","Additionally, we show that the alignment of the augmented image-text pairs is crucial for the effectiveness of the defense strategy, and that inappropriate augmentations can even degrade the model's performance.","Based on these findings, we propose a novel defense strategy that leverages the N:N relationship in ITR, which effectively generates diverse yet highly-aligned N:N pairs using basic augmentations and generative model-based augmentations.","This work provides a novel perspective on defending against adversarial attacks in VL tasks and opens up new research directions for future work."],"url":"http://arxiv.org/abs/2405.18770v1"}
{"created":"2024-05-29 05:08:16","title":"Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI","abstract":"The current electroencephalogram (EEG) based deep learning models are typically designed for specific datasets and applications in brain-computer interaction (BCI), limiting the scale of the models and thus diminishing their perceptual capabilities and generalizability. Recently, Large Language Models (LLMs) have achieved unprecedented success in text processing, prompting us to explore the capabilities of Large EEG Models (LEMs). We hope that LEMs can break through the limitations of different task types of EEG datasets, and obtain universal perceptual capabilities of EEG signals through unsupervised pre-training. Then the models can be fine-tuned for different downstream tasks. However, compared to text data, the volume of EEG datasets is generally small and the format varies widely. For example, there can be mismatched numbers of electrodes, unequal length data samples, varied task designs, and low signal-to-noise ratio. To overcome these challenges, we propose a unified foundation model for EEG called Large Brain Model (LaBraM). LaBraM enables cross-dataset learning by segmenting the EEG signals into EEG channel patches. Vector-quantized neural spectrum prediction is used to train a semantically rich neural tokenizer that encodes continuous raw EEG channel patches into compact neural codes. We then pre-train neural Transformers by predicting the original neural codes for the masked EEG channel patches. The LaBraMs were pre-trained on about 2,500 hours of various types of EEG signals from around 20 datasets and validated on multiple different types of downstream tasks. Experiments on abnormal detection, event type classification, emotion recognition, and gait prediction show that our LaBraM outperforms all compared SOTA methods in their respective fields. Our code is available at https://github.com/935963004/LaBraM.","sentences":["The current electroencephalogram (EEG) based deep learning models are typically designed for specific datasets and applications in brain-computer interaction (BCI), limiting the scale of the models and thus diminishing their perceptual capabilities and generalizability.","Recently, Large Language Models (LLMs) have achieved unprecedented success in text processing, prompting us to explore the capabilities of Large EEG Models (LEMs).","We hope that LEMs can break through the limitations of different task types of EEG datasets, and obtain universal perceptual capabilities of EEG signals through unsupervised pre-training.","Then the models can be fine-tuned for different downstream tasks.","However, compared to text data, the volume of EEG datasets is generally small and the format varies widely.","For example, there can be mismatched numbers of electrodes, unequal length data samples, varied task designs, and low signal-to-noise ratio.","To overcome these challenges, we propose a unified foundation model for EEG called Large Brain Model (LaBraM).","LaBraM enables cross-dataset learning by segmenting the EEG signals into EEG channel patches.","Vector-quantized neural spectrum prediction is used to train a semantically rich neural tokenizer that encodes continuous raw EEG channel patches into compact neural codes.","We then pre-train neural Transformers by predicting the original neural codes for the masked EEG channel patches.","The LaBraMs were pre-trained on about 2,500 hours of various types of EEG signals from around 20 datasets and validated on multiple different types of downstream tasks.","Experiments on abnormal detection, event type classification, emotion recognition, and gait prediction show that our LaBraM outperforms all compared SOTA methods in their respective fields.","Our code is available at https://github.com/935963004/LaBraM."],"url":"http://arxiv.org/abs/2405.18765v1"}
{"created":"2024-05-29 05:00:50","title":"FDQN: A Flexible Deep Q-Network Framework for Game Automation","abstract":"In reinforcement learning, it is often difficult to automate high-dimensional, rapid decision-making in dynamic environments, especially when domains require real-time online interaction and adaptive strategies such as web-based games. This work proposes a state-of-the-art Flexible Deep Q-Network (FDQN) framework that can address this challenge with a selfadaptive approach that is processing high-dimensional sensory data in realtime using a CNN and dynamically adapting the model architecture to varying action spaces of different gaming environments and outperforming previous baseline models in various Atari games and the Chrome Dino game as baselines. Using the epsilon-greedy policy, it effectively balances the new learning and exploitation for improved performance, and it has been designed with a modular structure that it can be easily adapted to other HTML-based games without touching the core part of the framework. It is demonstrated that the FDQN framework can successfully solve a well-defined task in a laboratory condition, but more importantly it also discusses potential applications to more challenging real-world cases and serve as the starting point for future further exploration into automated game play and beyond.","sentences":["In reinforcement learning, it is often difficult to automate high-dimensional, rapid decision-making in dynamic environments, especially when domains require real-time online interaction and adaptive strategies such as web-based games.","This work proposes a state-of-the-art Flexible Deep Q-Network (FDQN) framework that can address this challenge with a selfadaptive approach that is processing high-dimensional sensory data in realtime using a CNN and dynamically adapting the model architecture to varying action spaces of different gaming environments and outperforming previous baseline models in various Atari games and the Chrome Dino game as baselines.","Using the epsilon-greedy policy, it effectively balances the new learning and exploitation for improved performance, and it has been designed with a modular structure that it can be easily adapted to other HTML-based games without touching the core part of the framework.","It is demonstrated that the FDQN framework can successfully solve a well-defined task in a laboratory condition, but more importantly it also discusses potential applications to more challenging real-world cases and serve as the starting point for future further exploration into automated game play and beyond."],"url":"http://arxiv.org/abs/2405.18761v1"}
{"created":"2024-05-29 04:53:31","title":"Learning to Continually Learn with the Bayesian Principle","abstract":"In the present era of deep learning, continual learning research is mainly focused on mitigating forgetting when training a neural network with stochastic gradient descent on a non-stationary stream of data. On the other hand, in the more classical literature of statistical machine learning, many models have sequential Bayesian update rules that yield the same learning outcome as the batch training, i.e., they are completely immune to catastrophic forgetting. However, they are often overly simple to model complex real-world data. In this work, we adopt the meta-learning paradigm to combine the strong representational power of neural networks and simple statistical models' robustness to forgetting. In our novel meta-continual learning framework, continual learning takes place only in statistical models via ideal sequential Bayesian update rules, while neural networks are meta-learned to bridge the raw data and the statistical models. Since the neural networks remain fixed during continual learning, they are protected from catastrophic forgetting. This approach not only achieves significantly improved performance but also exhibits excellent scalability. Since our approach is domain-agnostic and model-agnostic, it can be applied to a wide range of problems and easily integrated with existing model architectures.","sentences":["In the present era of deep learning, continual learning research is mainly focused on mitigating forgetting when training a neural network with stochastic gradient descent on a non-stationary stream of data.","On the other hand, in the more classical literature of statistical machine learning, many models have sequential Bayesian update rules that yield the same learning outcome as the batch training, i.e., they are completely immune to catastrophic forgetting.","However, they are often overly simple to model complex real-world data.","In this work, we adopt the meta-learning paradigm to combine the strong representational power of neural networks and simple statistical models' robustness to forgetting.","In our novel meta-continual learning framework, continual learning takes place only in statistical models via ideal sequential Bayesian update rules, while neural networks are meta-learned to bridge the raw data and the statistical models.","Since the neural networks remain fixed during continual learning, they are protected from catastrophic forgetting.","This approach not only achieves significantly improved performance but also exhibits excellent scalability.","Since our approach is domain-agnostic and model-agnostic, it can be applied to a wide range of problems and easily integrated with existing model architectures."],"url":"http://arxiv.org/abs/2405.18758v1"}
{"created":"2024-05-29 04:50:53","title":"Multi-objective Cross-task Learning via Goal-conditioned GPT-based Decision Transformers for Surgical Robot Task Automation","abstract":"Surgical robot task automation has been a promising research topic for improving surgical efficiency and quality. Learning-based methods have been recognized as an interesting paradigm and been increasingly investigated. However, existing approaches encounter difficulties in long-horizon goal-conditioned tasks due to the intricate compositional structure, which requires decision-making for a sequence of sub-steps and understanding of inherent dynamics of goal-reaching tasks. In this paper, we propose a new learning-based framework by leveraging the strong reasoning capability of the GPT-based architecture to automate surgical robotic tasks. The key to our approach is developing a goal-conditioned decision transformer to achieve sequential representations with goal-aware future indicators in order to enhance temporal reasoning. Moreover, considering to exploit a general understanding of dynamics inherent in manipulations, thus making the model's reasoning ability to be task-agnostic, we also design a cross-task pretraining paradigm that uses multiple training objectives associated with data from diverse tasks. We have conducted extensive experiments on 10 tasks using the surgical robot learning simulator SurRoL~\\cite{long2023human}. The results show that our new approach achieves promising performance and task versatility compared to existing methods. The learned trajectories can be deployed on the da Vinci Research Kit (dVRK) for validating its practicality in real surgical robot settings. Our project website is at: https://med-air.github.io/SurRoL.","sentences":["Surgical robot task automation has been a promising research topic for improving surgical efficiency and quality.","Learning-based methods have been recognized as an interesting paradigm and been increasingly investigated.","However, existing approaches encounter difficulties in long-horizon goal-conditioned tasks due to the intricate compositional structure, which requires decision-making for a sequence of sub-steps and understanding of inherent dynamics of goal-reaching tasks.","In this paper, we propose a new learning-based framework by leveraging the strong reasoning capability of the GPT-based architecture to automate surgical robotic tasks.","The key to our approach is developing a goal-conditioned decision transformer to achieve sequential representations with goal-aware future indicators in order to enhance temporal reasoning.","Moreover, considering to exploit a general understanding of dynamics inherent in manipulations, thus making the model's reasoning ability to be task-agnostic, we also design a cross-task pretraining paradigm that uses multiple training objectives associated with data from diverse tasks.","We have conducted extensive experiments on 10 tasks using the surgical robot learning simulator SurRoL~\\cite{long2023human}.","The results show that our new approach achieves promising performance and task versatility compared to existing methods.","The learned trajectories can be deployed on the da Vinci Research Kit (dVRK) for validating its practicality in real surgical robot settings.","Our project website is at: https://med-air.github.io/SurRoL."],"url":"http://arxiv.org/abs/2405.18757v1"}
{"created":"2024-05-29 04:48:11","title":"Provable Contrastive Continual Learning","abstract":"Continual learning requires learning incremental tasks with dynamic data distributions. So far, it has been observed that employing a combination of contrastive loss and distillation loss for training in continual learning yields strong performance. To the best of our knowledge, however, this contrastive continual learning framework lacks convincing theoretical explanations. In this work, we fill this gap by establishing theoretical performance guarantees, which reveal how the performance of the model is bounded by training losses of previous tasks in the contrastive continual learning framework. Our theoretical explanations further support the idea that pre-training can benefit continual learning. Inspired by our theoretical analysis of these guarantees, we propose a novel contrastive continual learning algorithm called CILA, which uses adaptive distillation coefficients for different tasks. These distillation coefficients are easily computed by the ratio between average distillation losses and average contrastive losses from previous tasks. Our method shows great improvement on standard benchmarks and achieves new state-of-the-art performance.","sentences":["Continual learning requires learning incremental tasks with dynamic data distributions.","So far, it has been observed that employing a combination of contrastive loss and distillation loss for training in continual learning yields strong performance.","To the best of our knowledge, however, this contrastive continual learning framework lacks convincing theoretical explanations.","In this work, we fill this gap by establishing theoretical performance guarantees, which reveal how the performance of the model is bounded by training losses of previous tasks in the contrastive continual learning framework.","Our theoretical explanations further support the idea that pre-training can benefit continual learning.","Inspired by our theoretical analysis of these guarantees, we propose a novel contrastive continual learning algorithm called CILA, which uses adaptive distillation coefficients for different tasks.","These distillation coefficients are easily computed by the ratio between average distillation losses and average contrastive losses from previous tasks.","Our method shows great improvement on standard benchmarks and achieves new state-of-the-art performance."],"url":"http://arxiv.org/abs/2405.18756v1"}
