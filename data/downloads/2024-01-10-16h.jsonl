{"created":"2024-01-09 18:58:40","title":"Revisiting Adversarial Training at Scale","abstract":"The machine learning community has witnessed a drastic change in the training pipeline, pivoted by those ''foundation models'' with unprecedented scales. However, the field of adversarial training is lagging behind, predominantly centered around small model sizes like ResNet-50, and tiny and low-resolution datasets like CIFAR-10. To bridge this transformation gap, this paper provides a modern re-examination with adversarial training, investigating its potential benefits when applied at scale. Additionally, we introduce an efficient and effective training strategy to enable adversarial training with giant models and web-scale data at an affordable computing cost. We denote this newly introduced framework as AdvXL.   Empirical results demonstrate that AdvXL establishes new state-of-the-art robust accuracy records under AutoAttack on ImageNet-1K. For example, by training on DataComp-1B dataset, our AdvXL empowers a vanilla ViT-g model to substantially surpass the previous records of $l_{\\infty}$-, $l_{2}$-, and $l_{1}$-robust accuracy by margins of 11.4%, 14.2% and 12.9%, respectively. This achievement posits AdvXL as a pioneering approach, charting a new trajectory for the efficient training of robust visual representations at significantly larger scales. Our code is available at https://github.com/UCSC-VLAA/AdvXL.","sentences":["The machine learning community has witnessed a drastic change in the training pipeline, pivoted by those ''foundation models'' with unprecedented scales.","However, the field of adversarial training is lagging behind, predominantly centered around small model sizes like ResNet-50, and tiny and low-resolution datasets like CIFAR-10.","To bridge this transformation gap, this paper provides a modern re-examination with adversarial training, investigating its potential benefits when applied at scale.","Additionally, we introduce an efficient and effective training strategy to enable adversarial training with giant models and web-scale data at an affordable computing cost.","We denote this newly introduced framework as AdvXL.   ","Empirical results demonstrate that AdvXL establishes new state-of-the-art robust accuracy records under AutoAttack on ImageNet-1K. For example, by training on DataComp-1B dataset, our AdvXL empowers a vanilla ViT-g model to substantially surpass the previous records of $l_{\\infty}$-, $l_{2}$-, and $l_{1}$-robust accuracy by margins of 11.4%, 14.2% and 12.9%, respectively.","This achievement posits AdvXL as a pioneering approach, charting a new trajectory for the efficient training of robust visual representations at significantly larger scales.","Our code is available at https://github.com/UCSC-VLAA/AdvXL."],"url":"http://arxiv.org/abs/2401.04727v1"}
{"created":"2024-01-09 18:46:59","title":"Low-resource finetuning of foundation models beats state-of-the-art in histopathology","abstract":"To handle the large scale of whole slide images in computational pathology, most approaches first tessellate the images into smaller patches, extract features from these patches, and finally aggregate the feature vectors with weakly-supervised learning. The performance of this workflow strongly depends on the quality of the extracted features. Recently, foundation models in computer vision showed that leveraging huge amounts of data through supervised or self-supervised learning improves feature quality and generalizability for a variety of tasks. In this study, we benchmark the most popular vision foundation models as feature extractors for histopathology data. We evaluate the models in two settings: slide-level classification and patch-level classification. We show that foundation models are a strong baseline. Our experiments demonstrate that by finetuning a foundation model on a single GPU for only two hours or three days depending on the dataset, we can match or outperform state-of-the-art feature extractors for computational pathology. These findings imply that even with little resources one can finetune a feature extractor tailored towards a specific downstream task and dataset. This is a considerable shift from the current state, where only few institutions with large amounts of resources and datasets are able to train a feature extractor. We publish all code used for training and evaluation as well as the finetuned models.","sentences":["To handle the large scale of whole slide images in computational pathology, most approaches first tessellate the images into smaller patches, extract features from these patches, and finally aggregate the feature vectors with weakly-supervised learning.","The performance of this workflow strongly depends on the quality of the extracted features.","Recently, foundation models in computer vision showed that leveraging huge amounts of data through supervised or self-supervised learning improves feature quality and generalizability for a variety of tasks.","In this study, we benchmark the most popular vision foundation models as feature extractors for histopathology data.","We evaluate the models in two settings: slide-level classification and patch-level classification.","We show that foundation models are a strong baseline.","Our experiments demonstrate that by finetuning a foundation model on a single GPU for only two hours or three days depending on the dataset, we can match or outperform state-of-the-art feature extractors for computational pathology.","These findings imply that even with little resources one can finetune a feature extractor tailored towards a specific downstream task and dataset.","This is a considerable shift from the current state, where only few institutions with large amounts of resources and datasets are able to train a feature extractor.","We publish all code used for training and evaluation as well as the finetuned models."],"url":"http://arxiv.org/abs/2401.04720v1"}
{"created":"2024-01-09 18:40:52","title":"Low-Resource Vision Challenges for Foundation Models","abstract":"Low-resource settings are well-established in natural language processing, where many languages lack sufficient data for machine learning at scale. However, low-resource problems are under-explored in computer vision. In this paper, we strive to address this gap and explore the challenges of low-resource image tasks with vision foundation models. Thus, we first collect a benchmark of genuinely low-resource image data, covering historic maps, circuit diagrams, and mechanical drawings. These low-resource settings all share the three challenges of data scarcity, fine-grained differences, and the distribution shift from natural images to the specialized domain of interest. While existing foundation models have shown impressive generalizability, we find they cannot transfer well to our low-resource tasks. To begin to tackle the challenges of low-resource vision, we introduce one simple baseline per challenge. Specifically, we propose to i) enlarge the data space by generative models, ii) adopt the best sub-kernels to encode local regions for fine-grained difference discovery and iii) learn attention for specialized domains. Experiments on the three low-resource data sources in our benchmark demonstrate our proposals already provide a better baseline than common transfer learning, data augmentation, and fine-grained methods. This highlights the unique characteristics and challenges of low-resource vision for foundation models that warrant further investigation. Project website: https://xiaobai1217.github.io/Low-Resource-Vision/.","sentences":["Low-resource settings are well-established in natural language processing, where many languages lack sufficient data for machine learning at scale.","However, low-resource problems are under-explored in computer vision.","In this paper, we strive to address this gap and explore the challenges of low-resource image tasks with vision foundation models.","Thus, we first collect a benchmark of genuinely low-resource image data, covering historic maps, circuit diagrams, and mechanical drawings.","These low-resource settings all share the three challenges of data scarcity, fine-grained differences, and the distribution shift from natural images to the specialized domain of interest.","While existing foundation models have shown impressive generalizability, we find they cannot transfer well to our low-resource tasks.","To begin to tackle the challenges of low-resource vision, we introduce one simple baseline per challenge.","Specifically, we propose to i) enlarge the data space by generative models, ii) adopt the best sub-kernels to encode local regions for fine-grained difference discovery and iii) learn attention for specialized domains.","Experiments on the three low-resource data sources in our benchmark demonstrate our proposals already provide a better baseline than common transfer learning, data augmentation, and fine-grained methods.","This highlights the unique characteristics and challenges of low-resource vision for foundation models that warrant further investigation.","Project website: https://xiaobai1217.github.io/Low-Resource-Vision/."],"url":"http://arxiv.org/abs/2401.04716v1"}
{"created":"2024-01-09 18:39:42","title":"Bin Packing under Random-Order: Breaking the Barrier of 3/2","abstract":"Best-Fit is one of the most prominent and practically used algorithms for the bin packing problem, where a set of items with associated sizes needs to be packed in the minimum number of unit-capacity bins. Kenyon [SODA '96] studied online bin packing under random-order arrival, where the adversary chooses the list of items, but the items arrive one by one according to an arrival order drawn uniformly randomly from the set of all permutations of the items. Kenyon's seminal result established an upper bound of $1.5$ and a lower bound of $1.08$ on the random-order ratio of Best-Fit, and it was conjectured that the true ratio is $\\approx 1.15$. The conjecture, if true, will also imply that Best-Fit (on randomly permuted input) has the best performance guarantee among all the widely-used simple algorithms for (offline) bin packing. This conjecture has remained one of the major open problems in the area, as highlighted in the recent survey on random-order models by Gupta and Singla [Beyond the Worst-Case Analysis of Algorithms '20]. Recently, Albers et al. [Algorithmica '21] improved the upper bound to $1.25$ for the special case when all the item sizes are greater than $1/3$, and they improve the lower bound to $1.1$. Ayyadevara et al. [ICALP '22] obtained an improved result for the special case when all the item sizes lie in $(1/4, 1/2]$, which corresponds to the $3$-partition problem. The upper bound of $3/2$ for the general case, however, has remained unimproved.   In this paper, we make the first progress towards the conjecture, by showing that Best-Fit achieves a random-order ratio of at most $1.5 - \\varepsilon$, for a small constant $\\varepsilon>0$. Furthermore, we establish an improved lower bound of $1.144$ on the random-order ratio of Best-Fit, nearly reaching the conjectured ratio.","sentences":["Best-Fit is one of the most prominent and practically used algorithms for the bin packing problem, where a set of items with associated sizes needs to be packed in the minimum number of unit-capacity bins.","Kenyon [SODA '96] studied online bin packing under random-order arrival, where the adversary chooses the list of items, but the items arrive one by one according to an arrival order drawn uniformly randomly from the set of all permutations of the items.","Kenyon's seminal result established an upper bound of $1.5$ and a lower bound of $1.08$ on the random-order ratio of Best-Fit, and it was conjectured that the true ratio is $\\approx 1.15$.","The conjecture, if true, will also imply that Best-Fit (on randomly permuted input) has the best performance guarantee among all the widely-used simple algorithms for (offline) bin packing.","This conjecture has remained one of the major open problems in the area, as highlighted in the recent survey on random-order models by Gupta and Singla","[Beyond the Worst-Case Analysis of Algorithms '20].","Recently, Albers et al.","[Algorithmica '21] improved the upper bound to $1.25$ for the special case when all the item sizes are greater than $1/3$, and they improve the lower bound to $1.1$. Ayyadevara et al.","[ICALP '22] obtained an improved result for the special case when all the item sizes lie in $(1/4, 1/2]$, which corresponds to the $3$-partition problem.","The upper bound of $3/2$ for the general case, however, has remained unimproved.   ","In this paper, we make the first progress towards the conjecture, by showing that Best-Fit achieves a random-order ratio of at most $1.5 - \\varepsilon$, for a small constant $\\varepsilon>0$. Furthermore, we establish an improved lower bound of $1.144$ on the random-order ratio of Best-Fit, nearly reaching the conjectured ratio."],"url":"http://arxiv.org/abs/2401.04714v1"}
{"created":"2024-01-09 18:04:18","title":"HiRace: Accurate and Fast Source-Level Race Checking of GPU Programs","abstract":"Data races are egregious parallel programming bugs on CPUs. They are even worse on GPUs due to the hierarchical thread and memory structure, which makes it possible to write code that is correctly synchronized within a thread group while not being correct across groups. Thus far, all major data-race checkers for GPUs suffer from at least one of the following problems: they do not check races in global memory, do not work on recent GPUs, scale poorly, have not been extensively tested, miss simple data races, or are not dependable without detailed knowledge of the compiler.   Our new data-race detection tool, HiRace, overcomes these limitations. Its key novelty is an innovative parallel finite-state machine that condenses an arbitrarily long access history into a constant-length state, thus allowing it to handle large and long-running programs. HiRace is a dynamic tool that checks for thread-group shared memory and global device memory races. It utilizes source-code instrumentation, thus avoiding driver, compiler, and hardware dependencies. We evaluate it on a modern calibrated data-race benchmark suite. On the 580 tested CUDA kernels, 346 of which contain data races, HiRace finds races missed by other tools without false alarms and is more than 10 times faster on average than the current state of the art, while incurring only half the memory overhead.","sentences":["Data races are egregious parallel programming bugs on CPUs.","They are even worse on GPUs due to the hierarchical thread and memory structure, which makes it possible to write code that is correctly synchronized within a thread group while not being correct across groups.","Thus far, all major data-race checkers for GPUs suffer from at least one of the following problems: they do not check races in global memory, do not work on recent GPUs, scale poorly, have not been extensively tested, miss simple data races, or are not dependable without detailed knowledge of the compiler.   ","Our new data-race detection tool, HiRace, overcomes these limitations.","Its key novelty is an innovative parallel finite-state machine that condenses an arbitrarily long access history into a constant-length state, thus allowing it to handle large and long-running programs.","HiRace is a dynamic tool that checks for thread-group shared memory and global device memory races.","It utilizes source-code instrumentation, thus avoiding driver, compiler, and hardware dependencies.","We evaluate it on a modern calibrated data-race benchmark suite.","On the 580 tested CUDA kernels, 346 of which contain data races, HiRace finds races missed by other tools without false alarms and is more than 10 times faster on average than the current state of the art, while incurring only half the memory overhead."],"url":"http://arxiv.org/abs/2401.04701v1"}
{"created":"2024-01-09 17:39:45","title":"Comparative Evaluation of Animated Scatter Plot Transitions","abstract":"Scatter plots are popular for displaying 2D data, but in practice, many data sets have more than two dimensions. For the analysis of such multivariate data, it is often necessary to switch between scatter plots of different dimension pairs, e.g., in a scatter plot matrix (SPLOM). Alternative approaches include a \"grand tour\" for an overview of the entire data set or creating artificial axes from dimensionality reduction (DR). A cross-cutting concern in all techniques is the ability of viewers to find correspondence between data points in different views. Previous work proposed animations to preserve the mental map between view changes and to trace points as well as clusters between scatter plots of the same underlying data set. In this paper, we evaluate a variety of spline- and rotation-based view transitions in a crowdsourced user study focusing on ecological validity. Using the study results, we assess each animation's suitability for tracing points and clusters across view changes. We evaluate whether the order of horizontal and vertical rotation is relevant for task accuracy. The results show that rotations with an orthographic camera or staged expansion of a depth axis significantly outperform all other animation techniques for the traceability of individual points. Further, we provide a ranking of the animated transition techniques for traceability of individual points. However, we could not find any significant differences for the traceability of clusters. Furthermore, we identified differences by animation direction that could guide further studies to determine potential confounds for these differences. We publish the study data for reuse and provide the animation framework as a D3.js plug-in.","sentences":["Scatter plots are popular for displaying 2D data, but in practice, many data sets have more than two dimensions.","For the analysis of such multivariate data, it is often necessary to switch between scatter plots of different dimension pairs, e.g., in a scatter plot matrix (SPLOM).","Alternative approaches include a \"grand tour\" for an overview of the entire data set or creating artificial axes from dimensionality reduction (DR).","A cross-cutting concern in all techniques is the ability of viewers to find correspondence between data points in different views.","Previous work proposed animations to preserve the mental map between view changes and to trace points as well as clusters between scatter plots of the same underlying data set.","In this paper, we evaluate a variety of spline- and rotation-based view transitions in a crowdsourced user study focusing on ecological validity.","Using the study results, we assess each animation's suitability for tracing points and clusters across view changes.","We evaluate whether the order of horizontal and vertical rotation is relevant for task accuracy.","The results show that rotations with an orthographic camera or staged expansion of a depth axis significantly outperform all other animation techniques for the traceability of individual points.","Further, we provide a ranking of the animated transition techniques for traceability of individual points.","However, we could not find any significant differences for the traceability of clusters.","Furthermore, we identified differences by animation direction that could guide further studies to determine potential confounds for these differences.","We publish the study data for reuse and provide the animation framework as a D3.js plug-in."],"url":"http://arxiv.org/abs/2401.04692v1"}
{"created":"2024-01-09 17:38:19","title":"AI-based Mapping of the Conservation Status of Orchid Assemblages at Global Scale","abstract":"Although increasing threats on biodiversity are now widely recognised, there are no accurate global maps showing whether and where species assemblages are at risk. We hereby assess and map at kilometre resolution the conservation status of the iconic orchid family, and discuss the insights conveyed at multiple scales. We introduce a new Deep Species Distribution Model trained on 1M occurrences of 14K orchid species to predict their assemblages at global scale and at kilometre resolution. We propose two main indicators of the conservation status of the assemblages: (i) the proportion of threatened species, and (ii) the status of the most threatened species in the assemblage. We show and analyze the variation of these indicators at World scale and in relation to currently protected areas in Sumatra island. Global and interactive maps available online show the indicators of conservation status of orchid assemblages, with sharp spatial variations at all scales. The highest level of threat is found at Madagascar and the neighbouring islands. In Sumatra, we found good correspondence of protected areas with our indicators, but supplementing current IUCN assessments with status predictions results in alarming levels of species threat across the island. Recent advances in deep learning enable reliable mapping of the conservation status of species assemblages on a global scale. As an umbrella taxon, orchid family provides a reference for identifying vulnerable ecosystems worldwide, and prioritising conservation actions both at international and local levels.","sentences":["Although increasing threats on biodiversity are now widely recognised, there are no accurate global maps showing whether and where species assemblages are at risk.","We hereby assess and map at kilometre resolution the conservation status of the iconic orchid family, and discuss the insights conveyed at multiple scales.","We introduce a new Deep Species Distribution Model trained on 1M occurrences of 14K orchid species to predict their assemblages at global scale and at kilometre resolution.","We propose two main indicators of the conservation status of the assemblages: (i) the proportion of threatened species, and (ii) the status of the most threatened species in the assemblage.","We show and analyze the variation of these indicators at World scale and in relation to currently protected areas in Sumatra island.","Global and interactive maps available online show the indicators of conservation status of orchid assemblages, with sharp spatial variations at all scales.","The highest level of threat is found at Madagascar and the neighbouring islands.","In Sumatra, we found good correspondence of protected areas with our indicators, but supplementing current IUCN assessments with status predictions results in alarming levels of species threat across the island.","Recent advances in deep learning enable reliable mapping of the conservation status of species assemblages on a global scale.","As an umbrella taxon, orchid family provides a reference for identifying vulnerable ecosystems worldwide, and prioritising conservation actions both at international and local levels."],"url":"http://arxiv.org/abs/2401.04691v1"}
{"created":"2024-01-09 17:15:47","title":"Mixture of multilayer stochastic block models for multiview clustering","abstract":"In this work, we propose an original method for aggregating multiple clustering coming from different sources of information. Each partition is encoded by a co-membership matrix between observations. Our approach uses a mixture of multilayer Stochastic Block Models (SBM) to group co-membership matrices with similar information into components and to partition observations into different clusters, taking into account their specificities within the components. The identifiability of the model parameters is established and a variational Bayesian EM algorithm is proposed for the estimation of these parameters. The Bayesian framework allows for selecting an optimal number of clusters and components. The proposed approach is compared using synthetic data with consensus clustering and tensor-based algorithms for community detection in large-scale complex networks. Finally, the method is utilized to analyze global food trading networks, leading to structures of interest.","sentences":["In this work, we propose an original method for aggregating multiple clustering coming from different sources of information.","Each partition is encoded by a co-membership matrix between observations.","Our approach uses a mixture of multilayer Stochastic Block Models (SBM) to group co-membership matrices with similar information into components and to partition observations into different clusters, taking into account their specificities within the components.","The identifiability of the model parameters is established and a variational Bayesian EM algorithm is proposed for the estimation of these parameters.","The Bayesian framework allows for selecting an optimal number of clusters and components.","The proposed approach is compared using synthetic data with consensus clustering and tensor-based algorithms for community detection in large-scale complex networks.","Finally, the method is utilized to analyze global food trading networks, leading to structures of interest."],"url":"http://arxiv.org/abs/2401.04682v1"}
{"created":"2024-01-09 16:52:57","title":"Transfer-Learning-Based Autotuning Using Gaussian Copula","abstract":"As diverse high-performance computing (HPC) systems are built, many opportunities arise for applications to solve larger problems than ever before. Given the significantly increased complexity of these HPC systems and application tuning, empirical performance tuning, such as autotuning, has emerged as a promising approach in recent years. Despite its effectiveness, autotuning is often a computationally expensive approach. Transfer learning (TL)-based autotuning seeks to address this issue by leveraging the data from prior tuning. Current TL methods for autotuning spend significant time modeling the relationship between parameter configurations and performance, which is ineffective for few-shot (that is, few empirical evaluations) tuning on new tasks. We introduce the first generative TL-based autotuning approach based on the Gaussian copula (GC) to model the high-performing regions of the search space from prior data and then generate high-performing configurations for new tasks. This allows a sampling-based approach that maximizes few-shot performance and provides the first probabilistic estimation of the few-shot budget for effective TL-based autotuning. We compare our generative TL approach with state-of-the-art autotuning techniques on several benchmarks. We find that the GC is capable of achieving 64.37% of peak few-shot performance in its first evaluation. Furthermore, the GC model can determine a few-shot transfer budget that yields up to 33.39$\\times$ speedup, a dramatic improvement over the 20.58$\\times$ speedup using prior techniques.","sentences":["As diverse high-performance computing (HPC) systems are built, many opportunities arise for applications to solve larger problems than ever before.","Given the significantly increased complexity of these HPC systems and application tuning, empirical performance tuning, such as autotuning, has emerged as a promising approach in recent years.","Despite its effectiveness, autotuning is often a computationally expensive approach.","Transfer learning (TL)-based autotuning seeks to address this issue by leveraging the data from prior tuning.","Current TL methods for autotuning spend significant time modeling the relationship between parameter configurations and performance, which is ineffective for few-shot (that is, few empirical evaluations) tuning on new tasks.","We introduce the first generative TL-based autotuning approach based on the Gaussian copula (GC) to model the high-performing regions of the search space from prior data and then generate high-performing configurations for new tasks.","This allows a sampling-based approach that maximizes few-shot performance and provides the first probabilistic estimation of the few-shot budget for effective TL-based autotuning.","We compare our generative TL approach with state-of-the-art autotuning techniques on several benchmarks.","We find that the GC is capable of achieving 64.37% of peak few-shot performance in its first evaluation.","Furthermore, the GC model can determine a few-shot transfer budget that yields up to 33.39$\\times$ speedup, a dramatic improvement over the 20.58$\\times$ speedup using prior techniques."],"url":"http://arxiv.org/abs/2401.04669v1"}
{"created":"2024-01-09 16:48:11","title":"Benchmark Analysis of Various Pre-trained Deep Learning Models on ASSIRA Cats and Dogs Dataset","abstract":"As the most basic application and implementation of deep learning, image classification has grown in popularity. Various datasets are provided by renowned data science communities for benchmarking machine learning algorithms and pre-trained models. The ASSIRA Cats & Dogs dataset is one of them and is being used in this research for its overall acceptance and benchmark standards. A comparison of various pre-trained models is demonstrated by using different types of optimizers and loss functions. Hyper-parameters are changed to gain the best result from a model. By applying this approach, we have got higher accuracy without major changes in the training model. To run the experiment, we used three different computer architectures: a laptop equipped with NVIDIA GeForce GTX 1070, a laptop equipped with NVIDIA GeForce RTX 3080Ti, and a desktop equipped with NVIDIA GeForce RTX 3090. The acquired results demonstrate supremacy in terms of accuracy over the previously done experiments on this dataset. From this experiment, the highest accuracy which is 99.65% is gained using the NASNet Large.","sentences":["As the most basic application and implementation of deep learning, image classification has grown in popularity.","Various datasets are provided by renowned data science communities for benchmarking machine learning algorithms and pre-trained models.","The ASSIRA Cats & Dogs dataset is one of them and is being used in this research for its overall acceptance and benchmark standards.","A comparison of various pre-trained models is demonstrated by using different types of optimizers and loss functions.","Hyper-parameters are changed to gain the best result from a model.","By applying this approach, we have got higher accuracy without major changes in the training model.","To run the experiment, we used three different computer architectures: a laptop equipped with NVIDIA GeForce GTX 1070, a laptop equipped with NVIDIA GeForce RTX 3080Ti, and a desktop equipped with NVIDIA GeForce RTX 3090.","The acquired results demonstrate supremacy in terms of accuracy over the previously done experiments on this dataset.","From this experiment, the highest accuracy which is 99.65% is gained using the NASNet Large."],"url":"http://arxiv.org/abs/2401.04666v1"}
{"created":"2024-01-09 16:16:32","title":"A novel framework for generalization of deep hidden physics models","abstract":"Modelling of systems where the full system information is unknown is an oft encountered problem for various engineering and industrial applications, as it's either impossible to consider all the complex physics involved or simpler models are considered to keep within the limits of the available resources. Recent advances in greybox modelling like the deep hidden physics models address this space by combining data and physics. However, for most real-life applications, model generalizability is a key issue, as retraining a model for every small change in system inputs and parameters or modification in domain configuration can render the model economically unviable. In this work we present a novel enhancement to the idea of hidden physics models which can generalize for changes in system inputs, parameters and domains. We also show that this approach holds promise in system discovery as well and helps learn the hidden physics for the changed system inputs, parameters and domain configuration.","sentences":["Modelling of systems where the full system information is unknown is an oft encountered problem for various engineering and industrial applications, as it's either impossible to consider all the complex physics involved or simpler models are considered to keep within the limits of the available resources.","Recent advances in greybox modelling like the deep hidden physics models address this space by combining data and physics.","However, for most real-life applications, model generalizability is a key issue, as retraining a model for every small change in system inputs and parameters or modification in domain configuration can render the model economically unviable.","In this work we present a novel enhancement to the idea of hidden physics models which can generalize for changes in system inputs, parameters and domains.","We also show that this approach holds promise in system discovery as well and helps learn the hidden physics for the changed system inputs, parameters and domain configuration."],"url":"http://arxiv.org/abs/2401.04648v1"}
{"created":"2024-01-09 16:05:47","title":"Applying Large Language Models API to Issue Classification Problem","abstract":"Effective prioritization of issue reports is crucial in software engineering to optimize resource allocation and address critical problems promptly. However, the manual classification of issue reports for prioritization is laborious and lacks scalability. Alternatively, many open source software (OSS) projects employ automated processes for this task, albeit relying on substantial datasets for adequate training. This research seeks to devise an automated approach that ensures reliability in issue prioritization, even when trained on smaller datasets. Our proposed methodology harnesses the power of Generative Pre-trained Transformers (GPT), recognizing their potential to efficiently handle this task. By leveraging the capabilities of such models, we aim to develop a robust system for prioritizing issue reports accurately, mitigating the necessity for extensive training data while maintaining reliability. In our research, we have developed a reliable GPT-based approach to accurately label and prioritize issue reports with a reduced training dataset. By reducing reliance on massive data requirements and focusing on few-shot fine-tuning, our methodology offers a more accessible and efficient solution for issue prioritization in software engineering. Our model predicted issue types in individual projects up to 93.2% in precision, 95% in recall, and 89.3% in F1-score.","sentences":["Effective prioritization of issue reports is crucial in software engineering to optimize resource allocation and address critical problems promptly.","However, the manual classification of issue reports for prioritization is laborious and lacks scalability.","Alternatively, many open source software (OSS) projects employ automated processes for this task, albeit relying on substantial datasets for adequate training.","This research seeks to devise an automated approach that ensures reliability in issue prioritization, even when trained on smaller datasets.","Our proposed methodology harnesses the power of Generative Pre-trained Transformers (GPT), recognizing their potential to efficiently handle this task.","By leveraging the capabilities of such models, we aim to develop a robust system for prioritizing issue reports accurately, mitigating the necessity for extensive training data while maintaining reliability.","In our research, we have developed a reliable GPT-based approach to accurately label and prioritize issue reports with a reduced training dataset.","By reducing reliance on massive data requirements and focusing on few-shot fine-tuning, our methodology offers a more accessible and efficient solution for issue prioritization in software engineering.","Our model predicted issue types in individual projects up to 93.2% in precision, 95% in recall, and 89.3% in F1-score."],"url":"http://arxiv.org/abs/2401.04637v1"}
{"created":"2024-01-09 15:59:43","title":"Hypercomplex neural network in time series forecasting of stock data","abstract":"The three classes of architectures for time series prediction were tested. They differ by input layers which contain either convolutional, LSTM, or dense hypercomplex layers for 4D algebras. The input was four related Stock Market time series, and the prediction of one of them is expected. The optimization of hyperparameters related to the classes of architectures was performed in order to compare the best neural networks within the class. The results show that in most cases, the architecture with a hypercomplex dense layer provides similar MAE accuracy to other architectures, however, with considerably less trainable parameters. Thanks to it, hypercomplex neural networks can be learned and process data faster than the other tested architectures. Moreover, the order of the input time series has an impact on effectively.","sentences":["The three classes of architectures for time series prediction were tested.","They differ by input layers which contain either convolutional, LSTM, or dense hypercomplex layers for 4D algebras.","The input was four related Stock Market time series, and the prediction of one of them is expected.","The optimization of hyperparameters related to the classes of architectures was performed in order to compare the best neural networks within the class.","The results show that in most cases, the architecture with a hypercomplex dense layer provides similar MAE accuracy to other architectures, however, with considerably less trainable parameters.","Thanks to it, hypercomplex neural networks can be learned and process data faster than the other tested architectures.","Moreover, the order of the input time series has an impact on effectively."],"url":"http://arxiv.org/abs/2401.04632v1"}
{"created":"2024-01-09 15:56:43","title":"Multi-Neuron Representations of Hierarchical Concepts in Spiking Neural Networks","abstract":"We describe how hierarchical concepts can be represented in three types of layered neural networks. The aim is to support recognition of the concepts when partial information about the concepts is presented, and also when some of the neurons in the network might fail. Our failure model involves initial random failures. The three types of networks are: feed-forward networks with high connectivity, feed-forward networks with low connectivity, and layered networks with low connectivity and with both forward edges and \"lateral\" edges within layers. In order to achieve fault-tolerance, the representations all use multiple representative neurons for each concept. We show how recognition can work in all three of these settings, and quantify how the probability of correct recognition depends on several parameters, including the number of representatives and the neuron failure probability. We also discuss how these representations might be learned, in all three types of networks. For the feed-forward networks, the learning algorithms are similar to ones used in [4], whereas for networks with lateral edges, the algorithms are generally inspired by work on the assembly calculus [3, 6, 7].","sentences":["We describe how hierarchical concepts can be represented in three types of layered neural networks.","The aim is to support recognition of the concepts when partial information about the concepts is presented, and also when some of the neurons in the network might fail.","Our failure model involves initial random failures.","The three types of networks are: feed-forward networks with high connectivity, feed-forward networks with low connectivity, and layered networks with low connectivity and with both forward edges and \"lateral\" edges within layers.","In order to achieve fault-tolerance, the representations all use multiple representative neurons for each concept.","We show how recognition can work in all three of these settings, and quantify how the probability of correct recognition depends on several parameters, including the number of representatives and the neuron failure probability.","We also discuss how these representations might be learned, in all three types of networks.","For the feed-forward networks, the learning algorithms are similar to ones used in [4], whereas for networks with lateral edges, the algorithms are generally inspired by work on the assembly calculus [3, 6, 7]."],"url":"http://arxiv.org/abs/2401.04628v1"}
{"created":"2024-01-09 15:55:08","title":"A Novel OMNeT++-based Simulation Tool for Vehicular Cloud Computing in ETSI MEC-compliant 5G Environments","abstract":"Vehicular cloud computing is gaining popularity thanks to the rapid advancements in next generation wireless communication networks. Similarly, Edge Computing, along with its standard proposals such as European Telecommunications Standards Institute (ETSI) Multi-access Edge Computing (MEC), will play a vital role in these scenarios, by enabling the execution of cloud-based services at the edge of the network. Together, these solutions have the potential to create real micro-datacenters at the network edge, favoring several benefits like minimal latency, real-time data processing, and data locality. However, the research community has not yet the opportunity to use integrated simulation frameworks for the easy testing of applications that exploit both the vehicular cloud paradigm and MEC-compliant 5G deployment environments. In this paper, we present our simulation tool as a platform for researchers and engineers to design, test, and enhance applications utilizing the concepts of vehicular and edge cloud. Our platform significantly extends OMNet++ and Simu5G, and implements our ETSI MEC-compliant architecture that leverages resources provided by far-edge nodes. In addition, the paper analyzes and reports performance results for our simulation platform, as well as provides a use case where our simulator is used to support the design, test, and validation of an algorithm to distribute MEC application components on vehicular cloud resources.","sentences":["Vehicular cloud computing is gaining popularity thanks to the rapid advancements in next generation wireless communication networks.","Similarly, Edge Computing, along with its standard proposals such as European Telecommunications Standards Institute (ETSI) Multi-access Edge Computing (MEC), will play a vital role in these scenarios, by enabling the execution of cloud-based services at the edge of the network.","Together, these solutions have the potential to create real micro-datacenters at the network edge, favoring several benefits like minimal latency, real-time data processing, and data locality.","However, the research community has not yet the opportunity to use integrated simulation frameworks for the easy testing of applications that exploit both the vehicular cloud paradigm and MEC-compliant 5G deployment environments.","In this paper, we present our simulation tool as a platform for researchers and engineers to design, test, and enhance applications utilizing the concepts of vehicular and edge cloud.","Our platform significantly extends OMNet++ and Simu5G, and implements our ETSI MEC-compliant architecture that leverages resources provided by far-edge nodes.","In addition, the paper analyzes and reports performance results for our simulation platform, as well as provides a use case where our simulator is used to support the design, test, and validation of an algorithm to distribute MEC application components on vehicular cloud resources."],"url":"http://arxiv.org/abs/2401.04626v1"}
{"created":"2024-01-09 15:46:38","title":"DebugBench: Evaluating Debugging Capability of Large Language Models","abstract":"Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and three open-source models in a zero-shot scenario. We find that (1) while closed-source models like GPT-4 exhibit inferior debugging performance compared to humans, open-source models such as Code Llama fail to attain any pass rate scores; (2) the complexity of debugging notably fluctuates depending on the bug category; (3) incorporating runtime feedback has a clear impact on debugging performance which is not always helpful. As an extension, we also compare LLM debugging and code generation, revealing a strong correlation between them for closed-source models. These findings will benefit the development of LLMs in debugging.","sentences":["Large Language Models (LLMs) have demonstrated exceptional coding capability.","However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored.","Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs.","To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances.","It covers four major bug categories and 18 minor types in C++, Java, and Python.","To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks.","We evaluate two commercial and three open-source models in a zero-shot scenario.","We find that (1) while closed-source models like GPT-4 exhibit inferior debugging performance compared to humans, open-source models such as Code Llama fail to attain any pass rate scores; (2) the complexity of debugging notably fluctuates depending on the bug category; (3) incorporating runtime feedback has a clear impact on debugging performance which is not always helpful.","As an extension, we also compare LLM debugging and code generation, revealing a strong correlation between them for closed-source models.","These findings will benefit the development of LLMs in debugging."],"url":"http://arxiv.org/abs/2401.04621v1"}
{"created":"2024-01-09 15:28:29","title":"Distribution-Free Conformal Joint Prediction Regions for Neural Marked Temporal Point Processes","abstract":"Sequences of labeled events observed at irregular intervals in continuous time are ubiquitous across various fields. Temporal Point Processes (TPPs) provide a mathematical framework for modeling these sequences, enabling inferences such as predicting the arrival time of future events and their associated label, called mark. However, due to model misspecification or lack of training data, these probabilistic models may provide a poor approximation of the true, unknown underlying process, with prediction regions extracted from them being unreliable estimates of the underlying uncertainty. This paper develops more reliable methods for uncertainty quantification in neural TPP models via the framework of conformal prediction. A primary objective is to generate a distribution-free joint prediction region for the arrival time and mark, with a finite-sample marginal coverage guarantee. A key challenge is to handle both a strictly positive, continuous response and a categorical response, without distributional assumptions. We first consider a simple but overly conservative approach that combines individual prediction regions for the event arrival time and mark. Then, we introduce a more effective method based on bivariate highest density regions derived from the joint predictive density of event arrival time and mark. By leveraging the dependencies between these two variables, this method exclude unlikely combinations of the two, resulting in sharper prediction regions while still attaining the pre-specified coverage level. We also explore the generation of individual univariate prediction regions for arrival times and marks through conformal regression and classification techniques. Moreover, we investigate the stronger notion of conditional coverage. Finally, through extensive experimentation on both simulated and real-world datasets, we assess the validity and efficiency of these methods.","sentences":["Sequences of labeled events observed at irregular intervals in continuous time are ubiquitous across various fields.","Temporal Point Processes (TPPs) provide a mathematical framework for modeling these sequences, enabling inferences such as predicting the arrival time of future events and their associated label, called mark.","However, due to model misspecification or lack of training data, these probabilistic models may provide a poor approximation of the true, unknown underlying process, with prediction regions extracted from them being unreliable estimates of the underlying uncertainty.","This paper develops more reliable methods for uncertainty quantification in neural TPP models via the framework of conformal prediction.","A primary objective is to generate a distribution-free joint prediction region for the arrival time and mark, with a finite-sample marginal coverage guarantee.","A key challenge is to handle both a strictly positive, continuous response and a categorical response, without distributional assumptions.","We first consider a simple but overly conservative approach that combines individual prediction regions for the event arrival time and mark.","Then, we introduce a more effective method based on bivariate highest density regions derived from the joint predictive density of event arrival time and mark.","By leveraging the dependencies between these two variables, this method exclude unlikely combinations of the two, resulting in sharper prediction regions while still attaining the pre-specified coverage level.","We also explore the generation of individual univariate prediction regions for arrival times and marks through conformal regression and classification techniques.","Moreover, we investigate the stronger notion of conditional coverage.","Finally, through extensive experimentation on both simulated and real-world datasets, we assess the validity and efficiency of these methods."],"url":"http://arxiv.org/abs/2401.04612v1"}
{"created":"2024-01-09 14:58:34","title":"A Multi-Modal Approach Based on Large Vision Model for Close-Range Underwater Target Localization","abstract":"Underwater target localization uses real-time sensory measurements to estimate the position of underwater objects of interest, providing critical feedback information for underwater robots. While acoustic sensing is the most acknowledged method in underwater robots and possibly the only effective approach for long-range underwater target localization, such a sensing modality generally suffers from low resolution, high cost and high energy consumption, thus leading to a mediocre performance when applied to close-range underwater target localization. On the other hand, optical sensing has attracted increasing attention in the underwater robotics community for its advantages of high resolution and low cost, holding a great potential particularly in close-range underwater target localization. However, most existing studies in underwater optical sensing are restricted to specific types of targets due to the limited training data available. In addition, these studies typically focus on the design of estimation algorithms and ignore the influence of illumination conditions on the sensing performance, thus hindering wider applications in the real world. To address the aforementioned issues, this paper proposes a novel target localization method that assimilates both optical and acoustic sensory measurements to estimate the 3D positions of close-range underwater targets. A test platform with controllable illumination conditions is designed and developed to experimentally investigate the proposed multi-modal sensing approach. A large vision model is applied to process the optical imaging measurements, eliminating the requirement for training data acquisition, thus significantly expanding the scope of potential applications. Extensive experiments are conducted, the results of which validate the effectiveness of the proposed underwater target localization method.","sentences":["Underwater target localization uses real-time sensory measurements to estimate the position of underwater objects of interest, providing critical feedback information for underwater robots.","While acoustic sensing is the most acknowledged method in underwater robots and possibly the only effective approach for long-range underwater target localization, such a sensing modality generally suffers from low resolution, high cost and high energy consumption, thus leading to a mediocre performance when applied to close-range underwater target localization.","On the other hand, optical sensing has attracted increasing attention in the underwater robotics community for its advantages of high resolution and low cost, holding a great potential particularly in close-range underwater target localization.","However, most existing studies in underwater optical sensing are restricted to specific types of targets due to the limited training data available.","In addition, these studies typically focus on the design of estimation algorithms and ignore the influence of illumination conditions on the sensing performance, thus hindering wider applications in the real world.","To address the aforementioned issues, this paper proposes a novel target localization method that assimilates both optical and acoustic sensory measurements to estimate the 3D positions of close-range underwater targets.","A test platform with controllable illumination conditions is designed and developed to experimentally investigate the proposed multi-modal sensing approach.","A large vision model is applied to process the optical imaging measurements, eliminating the requirement for training data acquisition, thus significantly expanding the scope of potential applications.","Extensive experiments are conducted, the results of which validate the effectiveness of the proposed underwater target localization method."],"url":"http://arxiv.org/abs/2401.04595v1"}
{"created":"2024-01-09 14:50:04","title":"An Assessment on Comprehending Mental Health through Large Language Models","abstract":"Mental health challenges pose considerable global burdens on individuals and communities. Recent data indicates that more than 20% of adults may encounter at least one mental disorder in their lifetime. On the one hand, the advancements in large language models have facilitated diverse applications, yet a significant research gap persists in understanding and enhancing the potential of large language models within the domain of mental health. On the other hand, across various applications, an outstanding question involves the capacity of large language models to comprehend expressions of human mental health conditions in natural language. This study presents an initial evaluation of large language models in addressing this gap. Due to this, we compare the performance of Llama-2 and ChatGPT with classical Machine as well as Deep learning models. Our results on the DAIC-WOZ dataset show that transformer-based models, like BERT or XLNet, outperform the large language models.","sentences":["Mental health challenges pose considerable global burdens on individuals and communities.","Recent data indicates that more than 20% of adults may encounter at least one mental disorder in their lifetime.","On the one hand, the advancements in large language models have facilitated diverse applications, yet a significant research gap persists in understanding and enhancing the potential of large language models within the domain of mental health.","On the other hand, across various applications, an outstanding question involves the capacity of large language models to comprehend expressions of human mental health conditions in natural language.","This study presents an initial evaluation of large language models in addressing this gap.","Due to this, we compare the performance of Llama-2 and ChatGPT with classical Machine as well as Deep learning models.","Our results on the DAIC-WOZ dataset show that transformer-based models, like BERT or XLNet, outperform the large language models."],"url":"http://arxiv.org/abs/2401.04592v1"}
{"created":"2024-01-09 14:32:24","title":"Effective pruning of web-scale datasets based on complexity of concept clusters","abstract":"Utilizing massive web-scale datasets has led to unprecedented performance gains in machine learning models, but also imposes outlandish compute requirements for their training. In order to improve training and data efficiency, we here push the limits of pruning large-scale multimodal datasets for training CLIP-style models. Today's most effective pruning method on ImageNet clusters data samples into separate concepts according to their embedding and prunes away the most prototypical samples. We scale this approach to LAION and improve it by noting that the pruning rate should be concept-specific and adapted to the complexity of the concept. Using a simple and intuitive complexity measure, we are able to reduce the training cost to a quarter of regular training. By filtering from the LAION dataset, we find that training on a smaller set of high-quality data can lead to higher performance with significantly lower training costs. More specifically, we are able to outperform the LAION-trained OpenCLIP-ViT-B32 model on ImageNet zero-shot accuracy by 1.1p.p. while only using 27.7% of the data and training compute. Despite a strong reduction in training cost, we also see improvements on ImageNet dist. shifts, retrieval tasks and VTAB. On the DataComp Medium benchmark, we achieve a new state-of-the-art ImageNet zero-shot accuracy and a competitive average zero-shot accuracy on 38 evaluation tasks.","sentences":["Utilizing massive web-scale datasets has led to unprecedented performance gains in machine learning models, but also imposes outlandish compute requirements for their training.","In order to improve training and data efficiency, we here push the limits of pruning large-scale multimodal datasets for training CLIP-style models.","Today's most effective pruning method on ImageNet clusters data samples into separate concepts according to their embedding and prunes away the most prototypical samples.","We scale this approach to LAION and improve it by noting that the pruning rate should be concept-specific and adapted to the complexity of the concept.","Using a simple and intuitive complexity measure, we are able to reduce the training cost to a quarter of regular training.","By filtering from the LAION dataset, we find that training on a smaller set of high-quality data can lead to higher performance with significantly lower training costs.","More specifically, we are able to outperform the LAION-trained OpenCLIP-ViT-B32 model on ImageNet zero-shot accuracy by 1.1p.p.","while only using 27.7% of the data and training compute.","Despite a strong reduction in training cost, we also see improvements on ImageNet dist.","shifts, retrieval tasks and VTAB.","On the DataComp Medium benchmark, we achieve a new state-of-the-art ImageNet zero-shot accuracy and a competitive average zero-shot accuracy on 38 evaluation tasks."],"url":"http://arxiv.org/abs/2401.04578v1"}
{"created":"2024-01-09 14:24:29","title":"Let's Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual Concept Understanding","abstract":"Vision and vision-language applications of neural networks, such as image classification and captioning, rely on large-scale annotated datasets that require non-trivial data-collecting processes. This time-consuming endeavor hinders the emergence of large-scale datasets, limiting researchers and practitioners to a small number of choices. Therefore, we seek more efficient ways to collect and annotate images. Previous initiatives have gathered captions from HTML alt-texts and crawled social media postings, but these data sources suffer from noise, sparsity, or subjectivity. For this reason, we turn to commercial shopping websites whose data meet three criteria: cleanliness, informativeness, and fluency. We introduce the Let's Go Shopping (LGS) dataset, a large-scale public dataset with 15 million image-caption pairs from publicly available e-commerce websites. When compared with existing general-domain datasets, the LGS images focus on the foreground object and have less complex backgrounds. Our experiments on LGS show that the classifiers trained on existing benchmark datasets do not readily generalize to e-commerce data, while specific self-supervised visual feature extractors can better generalize. Furthermore, LGS's high-quality e-commerce-focused images and bimodal nature make it advantageous for vision-language bi-modal tasks: LGS enables image-captioning models to generate richer captions and helps text-to-image generation models achieve e-commerce style transfer.","sentences":["Vision and vision-language applications of neural networks, such as image classification and captioning, rely on large-scale annotated datasets that require non-trivial data-collecting processes.","This time-consuming endeavor hinders the emergence of large-scale datasets, limiting researchers and practitioners to a small number of choices.","Therefore, we seek more efficient ways to collect and annotate images.","Previous initiatives have gathered captions from HTML alt-texts and crawled social media postings, but these data sources suffer from noise, sparsity, or subjectivity.","For this reason, we turn to commercial shopping websites whose data meet three criteria: cleanliness, informativeness, and fluency.","We introduce the Let's Go Shopping (LGS) dataset, a large-scale public dataset with 15 million image-caption pairs from publicly available e-commerce websites.","When compared with existing general-domain datasets, the LGS images focus on the foreground object and have less complex backgrounds.","Our experiments on LGS show that the classifiers trained on existing benchmark datasets do not readily generalize to e-commerce data, while specific self-supervised visual feature extractors can better generalize.","Furthermore, LGS's high-quality e-commerce-focused images and bimodal nature make it advantageous for vision-language bi-modal tasks: LGS enables image-captioning models to generate richer captions and helps text-to-image generation models achieve e-commerce style transfer."],"url":"http://arxiv.org/abs/2401.04575v1"}
{"created":"2024-01-09 13:56:37","title":"Phase-shifted remote photoplethysmography for estimating heart rate and blood pressure from facial video","abstract":"Human health can be critically affected by cardiovascular diseases, such as hypertension, arrhythmias, and stroke. Heart rate and blood pressure are important biometric information for the monitoring of cardiovascular system and early diagnosis of cardiovascular diseases. Existing methods for estimating the heart rate are based on electrocardiography and photoplethyomography, which require contacting the sensor to the skin surface. Moreover, catheter and cuff-based methods for measuring blood pressure cause inconvenience and have limited applicability. Therefore, in this thesis, we propose a vision-based method for estimating the heart rate and blood pressure. This thesis proposes a 2-stage deep learning framework consisting of a dual remote photoplethysmography network (DRP-Net) and bounded blood pressure network (BBP-Net). In the first stage, DRP-Net infers remote photoplethysmography (rPPG) signals for the acral and facial regions, and these phase-shifted rPPG signals are utilized to estimate the heart rate. In the second stage, BBP-Net integrates temporal features and analyzes phase discrepancy between the acral and facial rPPG signals to estimate SBP and DBP values. To improve the accuracy of estimating the heart rate, we employed a data augmentation method based on a frame interpolation model. Moreover, we designed BBP-Net to infer blood pressure within a predefined range by incorporating a scaled sigmoid function. Our method resulted in estimating the heart rate with the mean absolute error (MAE) of 1.78 BPM, reducing the MAE by 34.31 % compared to the recent method, on the MMSE-HR dataset. The MAE for estimating the systolic blood pressure (SBP) and diastolic blood pressure (DBP) were 10.19 mmHg and 7.09 mmHg. On the V4V dataset, the MAE for the heart rate, SBP, and DBP were 3.83 BPM, 13.64 mmHg, and 9.4 mmHg, respectively.","sentences":["Human health can be critically affected by cardiovascular diseases, such as hypertension, arrhythmias, and stroke.","Heart rate and blood pressure are important biometric information for the monitoring of cardiovascular system and early diagnosis of cardiovascular diseases.","Existing methods for estimating the heart rate are based on electrocardiography and photoplethyomography, which require contacting the sensor to the skin surface.","Moreover, catheter and cuff-based methods for measuring blood pressure cause inconvenience and have limited applicability.","Therefore, in this thesis, we propose a vision-based method for estimating the heart rate and blood pressure.","This thesis proposes a 2-stage deep learning framework consisting of a dual remote photoplethysmography network (DRP-Net) and bounded blood pressure network (BBP-Net).","In the first stage, DRP-Net infers remote photoplethysmography (rPPG) signals for the acral and facial regions, and these phase-shifted rPPG signals are utilized to estimate the heart rate.","In the second stage, BBP-Net integrates temporal features and analyzes phase discrepancy between the acral and facial rPPG signals to estimate SBP and DBP values.","To improve the accuracy of estimating the heart rate, we employed a data augmentation method based on a frame interpolation model.","Moreover, we designed BBP-Net to infer blood pressure within a predefined range by incorporating a scaled sigmoid function.","Our method resulted in estimating the heart rate with the mean absolute error (MAE) of 1.78 BPM, reducing the MAE by 34.31 % compared to the recent method, on the MMSE-HR dataset.","The MAE for estimating the systolic blood pressure (SBP) and diastolic blood pressure (DBP) were 10.19 mmHg and 7.09 mmHg.","On the V4V dataset, the MAE for the heart rate, SBP, and DBP were 3.83 BPM, 13.64 mmHg, and 9.4 mmHg, respectively."],"url":"http://arxiv.org/abs/2401.04560v1"}
{"created":"2024-01-09 13:42:21","title":"WaveletFormerNet: A Transformer-based Wavelet Network for Real-world Non-homogeneous and Dense Fog Removal","abstract":"Although deep convolutional neural networks have achieved remarkable success in removing synthetic fog, it is essential to be able to process images taken in complex foggy conditions, such as dense or non-homogeneous fog, in the real world. However, the haze distribution in the real world is complex, and downsampling can lead to color distortion or loss of detail in the output results as the resolution of a feature map or image resolution decreases. In addition to the challenges of obtaining sufficient training data, overfitting can also arise in deep learning techniques for foggy image processing, which can limit the generalization abilities of the model, posing challenges for its practical applications in real-world scenarios. Considering these issues, this paper proposes a Transformer-based wavelet network (WaveletFormerNet) for real-world foggy image recovery. We embed the discrete wavelet transform into the Vision Transformer by proposing the WaveletFormer and IWaveletFormer blocks, aiming to alleviate texture detail loss and color distortion in the image due to downsampling. We introduce parallel convolution in the Transformer block, which allows for the capture of multi-frequency information in a lightweight mechanism. Additionally, we have implemented a feature aggregation module (FAM) to maintain image resolution and enhance the feature extraction capacity of our model, further contributing to its impressive performance in real-world foggy image recovery tasks. Extensive experiments demonstrate that our WaveletFormerNet performs better than state-of-the-art methods, as shown through quantitative and qualitative evaluations of minor model complexity. Additionally, our satisfactory results on real-world dust removal and application tests showcase the superior generalization ability and improved performance of WaveletFormerNet in computer vision-related applications.","sentences":["Although deep convolutional neural networks have achieved remarkable success in removing synthetic fog, it is essential to be able to process images taken in complex foggy conditions, such as dense or non-homogeneous fog, in the real world.","However, the haze distribution in the real world is complex, and downsampling can lead to color distortion or loss of detail in the output results as the resolution of a feature map or image resolution decreases.","In addition to the challenges of obtaining sufficient training data, overfitting can also arise in deep learning techniques for foggy image processing, which can limit the generalization abilities of the model, posing challenges for its practical applications in real-world scenarios.","Considering these issues, this paper proposes a Transformer-based wavelet network (WaveletFormerNet) for real-world foggy image recovery.","We embed the discrete wavelet transform into the Vision Transformer by proposing the WaveletFormer and IWaveletFormer blocks, aiming to alleviate texture detail loss and color distortion in the image due to downsampling.","We introduce parallel convolution in the Transformer block, which allows for the capture of multi-frequency information in a lightweight mechanism.","Additionally, we have implemented a feature aggregation module (FAM) to maintain image resolution and enhance the feature extraction capacity of our model, further contributing to its impressive performance in real-world foggy image recovery tasks.","Extensive experiments demonstrate that our WaveletFormerNet performs better than state-of-the-art methods, as shown through quantitative and qualitative evaluations of minor model complexity.","Additionally, our satisfactory results on real-world dust removal and application tests showcase the superior generalization ability and improved performance of WaveletFormerNet in computer vision-related applications."],"url":"http://arxiv.org/abs/2401.04550v1"}
{"created":"2024-01-09 13:35:09","title":"Evaluating Gesture Recognition in Virtual Reality","abstract":"Human-Robot Interaction (HRI) has become increasingly important as robots are being integrated into various aspects of daily life. One key aspect of HRI is gesture recognition, which allows robots to interpret and respond to human gestures in real-time. Gesture recognition plays an important role in non-verbal communication in HRI. To this aim, there is ongoing research on how such non-verbal communication can strengthen verbal communication and improve the system's overall efficiency, thereby enhancing the user experience with the robot. However, several challenges need to be addressed in gesture recognition systems, which include data generation, transferability, scalability, generalizability, standardization, and lack of benchmarking of the gestural systems. In this preliminary paper, we want to address the challenges of data generation using virtual reality simulations and standardization issues by presenting gestures to some commands that can be used as a standard in ground robots.","sentences":["Human-Robot Interaction (HRI) has become increasingly important as robots are being integrated into various aspects of daily life.","One key aspect of HRI is gesture recognition, which allows robots to interpret and respond to human gestures in real-time.","Gesture recognition plays an important role in non-verbal communication in HRI.","To this aim, there is ongoing research on how such non-verbal communication can strengthen verbal communication and improve the system's overall efficiency, thereby enhancing the user experience with the robot.","However, several challenges need to be addressed in gesture recognition systems, which include data generation, transferability, scalability, generalizability, standardization, and lack of benchmarking of the gestural systems.","In this preliminary paper, we want to address the challenges of data generation using virtual reality simulations and standardization issues by presenting gestures to some commands that can be used as a standard in ground robots."],"url":"http://arxiv.org/abs/2401.04545v1"}
{"created":"2024-01-09 13:19:37","title":"Evaluating Language Model Agency through Negotiations","abstract":"Companies, organizations, and governments increasingly exploit Language Models' (LM) remarkable capability to display agent-like behavior. As LMs are adopted to perform tasks with growing autonomy, there exists an urgent need for reliable and scalable evaluation benchmarks. Current, predominantly static LM benchmarks are ill-suited to evaluate such dynamic applications. Thus, we propose jointly evaluating LM performance and alignment through the lenses of negotiation games. We argue that this common task better reflects real-world deployment conditions while offering insights into LMs' decision-making processes. Crucially, negotiation games allow us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental data leakage in evaluation. We report results for six publicly accessible LMs from several major providers on a variety of negotiation games, evaluating both self-play and cross-play performance. Noteworthy findings include: (i) open-source models are currently unable to complete these tasks; (ii) cooperative bargaining games prove challenging; and (iii) the most powerful models do not always \"win\".","sentences":["Companies, organizations, and governments increasingly exploit Language Models' (LM) remarkable capability to display agent-like behavior.","As LMs are adopted to perform tasks with growing autonomy, there exists an urgent need for reliable and scalable evaluation benchmarks.","Current, predominantly static LM benchmarks are ill-suited to evaluate such dynamic applications.","Thus, we propose jointly evaluating LM performance and alignment through the lenses of negotiation games.","We argue that this common task better reflects real-world deployment conditions while offering insights into LMs' decision-making processes.","Crucially, negotiation games allow us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental data leakage in evaluation.","We report results for six publicly accessible LMs from several major providers on a variety of negotiation games, evaluating both self-play and cross-play performance.","Noteworthy findings include: (i) open-source models are currently unable to complete these tasks; (ii) cooperative bargaining games prove challenging; and (iii) the most powerful models do not always \"win\"."],"url":"http://arxiv.org/abs/2401.04536v1"}
{"created":"2024-01-09 12:55:21","title":"MERA: A Comprehensive LLM Evaluation in Russian","abstract":"Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs). As the models' size increases, LMs demonstrate enhancements in measurable aspects and the development of new qualitative features. However, despite researchers' attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood. To address these issues, we introduce an open Multimodal Evaluation of Russian-language Architectures (MERA), a new instruction benchmark for evaluating foundation models oriented towards the Russian language. The benchmark encompasses 21 evaluation tasks for generative models in 11 skill domains and is designed as a black-box test to ensure the exclusion of data leakage. The paper introduces a methodology to evaluate FMs and LMs in zero- and few-shot fixed instruction settings that can be extended to other modalities. We propose an evaluation methodology, an open-source code base for the MERA assessment, and a leaderboard with a submission system. We evaluate open LMs as baselines and find that they are still far behind the human level. We publicly release MERA to guide forthcoming research, anticipate groundbreaking model features, standardize the evaluation procedure, and address potential societal drawbacks.","sentences":["Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs).","As the models' size increases, LMs demonstrate enhancements in measurable aspects and the development of new qualitative features.","However, despite researchers' attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood.","To address these issues, we introduce an open Multimodal Evaluation of Russian-language Architectures (MERA), a new instruction benchmark for evaluating foundation models oriented towards the Russian language.","The benchmark encompasses 21 evaluation tasks for generative models in 11 skill domains and is designed as a black-box test to ensure the exclusion of data leakage.","The paper introduces a methodology to evaluate FMs and LMs in zero- and few-shot fixed instruction settings that can be extended to other modalities.","We propose an evaluation methodology, an open-source code base for the MERA assessment, and a leaderboard with a submission system.","We evaluate open LMs as baselines and find that they are still far behind the human level.","We publicly release MERA to guide forthcoming research, anticipate groundbreaking model features, standardize the evaluation procedure, and address potential societal drawbacks."],"url":"http://arxiv.org/abs/2401.04531v1"}
{"created":"2024-01-09 11:56:05","title":"Linear-size Suffix Tries and Linear-size CDAWGs Simplified and Improved","abstract":"The linear-size suffix tries (LSTries) [Crochemore et al., TCS 2016] are a version of suffix trees in which the edge labels are single characters, yet are able to perform pattern matching queries in optimal time. Instead of explicitly storing the input text, LSTries have some extra non-branching internal nodes called type-2 nodes. The extended techniques are then used in the linear-size compact directed acyclic word graphs (LCDAWGs) [Takagi et al. SPIRE 2017], which can be stored with $O(el(T)+er(T))$ space (i.e. without the text), where $el(T)$ and $er(T)$ are the numbers of left- and right-extensions of the maximal repeats in the input text string $T$, respectively. In this paper, we present simpler alternatives to the aforementioned indexing structures, called the simplified LSTries (simLSTries) and the simplified LCDAWGs (simLCDAWGs), in which most of the type-2 nodes are removed. In particular, our simLCDAWGs require only $O(er(T))$ space and work on a weaker model of computation (i.e. the pointer machine). This contrasts the $O(er(T))$-space CDAWG representation of [Belazzougui \\& Cunial, SPIRE 2017], which works on the word RAM.","sentences":["The linear-size suffix tries (LSTries)","[Crochemore et al., TCS 2016] are a version of suffix trees in which the edge labels are single characters, yet are able to perform pattern matching queries in optimal time.","Instead of explicitly storing the input text, LSTries have some extra non-branching internal nodes called type-2 nodes.","The extended techniques are then used in the linear-size compact directed acyclic word graphs (LCDAWGs)","[Takagi et al. SPIRE 2017], which can be stored with $O(el(T)+er(T))$ space (i.e. without the text), where $el(T)$ and $er(T)$ are the numbers of left- and right-extensions of the maximal repeats in the input text string $T$, respectively.","In this paper, we present simpler alternatives to the aforementioned indexing structures, called the simplified LSTries (simLSTries) and the simplified LCDAWGs (simLCDAWGs), in which most of the type-2 nodes are removed.","In particular, our simLCDAWGs require only $O(er(T))$ space and work on a weaker model of computation (i.e. the pointer machine).","This contrasts the $O(er(T))$-space CDAWG representation of [Belazzougui \\& Cunial, SPIRE 2017], which works on the word RAM."],"url":"http://arxiv.org/abs/2401.04509v1"}
{"created":"2024-01-09 11:52:58","title":"TechGPT-2.0: A large language model project to solve the task of knowledge graph construction","abstract":"Large language models have exhibited robust performance across diverse natural language processing tasks. This report introduces TechGPT-2.0, a project designed to enhance the capabilities of large language models specifically in knowledge graph construction tasks, including named entity recognition (NER) and relationship triple extraction (RTE) tasks in NLP applications. Additionally, it serves as a LLM accessible for research within the Chinese open-source model community. We offer two 7B large language model weights and a QLoRA weight specialized for processing lengthy texts.Notably, TechGPT-2.0 is trained on Huawei's Ascend server. Inheriting all functionalities from TechGPT-1.0, it exhibits robust text processing capabilities, particularly in the domains of medicine and law. Furthermore, we introduce new capabilities to the model, enabling it to process texts in various domains such as geographical areas, transportation, organizations, literary works, biology, natural sciences, astronomical objects, and architecture. These enhancements also fortified the model's adeptness in handling hallucinations, unanswerable queries, and lengthy texts. This report provides a comprehensive and detailed introduction to the full fine-tuning process on Huawei's Ascend servers, encompassing experiences in Ascend server debugging, instruction fine-tuning data processing, and model training. Our code is available at https://github.com/neukg/TechGPT-2.0","sentences":["Large language models have exhibited robust performance across diverse natural language processing tasks.","This report introduces TechGPT-2.0, a project designed to enhance the capabilities of large language models specifically in knowledge graph construction tasks, including named entity recognition (NER) and relationship triple extraction (RTE) tasks in NLP applications.","Additionally, it serves as a LLM accessible for research within the Chinese open-source model community.","We offer two 7B large language model weights and a QLoRA weight specialized for processing lengthy texts.","Notably, TechGPT-2.0 is trained on Huawei's Ascend server.","Inheriting all functionalities from TechGPT-1.0, it exhibits robust text processing capabilities, particularly in the domains of medicine and law.","Furthermore, we introduce new capabilities to the model, enabling it to process texts in various domains such as geographical areas, transportation, organizations, literary works, biology, natural sciences, astronomical objects, and architecture.","These enhancements also fortified the model's adeptness in handling hallucinations, unanswerable queries, and lengthy texts.","This report provides a comprehensive and detailed introduction to the full fine-tuning process on Huawei's Ascend servers, encompassing experiences in Ascend server debugging, instruction fine-tuning data processing, and model training.","Our code is available at https://github.com/neukg/TechGPT-2.0"],"url":"http://arxiv.org/abs/2401.04507v1"}
{"created":"2024-01-09 11:14:42","title":"Adaptive Asynchronous Work-Stealing for distributed load-balancing in heterogeneous systems","abstract":"Supercomputers have revolutionized how industries and scientific fields process large amounts of data. These machines group hundreds or thousands of computing nodes working together to execute time-consuming programs that require a large amount of computational resources. Over the years, supercomputers have expanded to include new and different technologies characterizing them as heterogeneous. However, executing a program in a heterogeneous environment requires attention to a specific aspect of performance degradation: load imbalance. In this research, we address the challenges associated with load imbalance when scheduling many homogeneous tasks in a heterogeneous environment. To address this issue, we introduce the concept of adaptive asynchronous work-stealing. This approach collects information about the nodes and utilizes it to improve work-stealing aspects, such as victim selection and task offloading. Additionally, the proposed approach eliminates the need for extra threads to communicate information, thereby reducing overhead when implementing a fully asynchronous approach. Our experimental results demonstrate a performance improvement of approximately 10.1\\% compared to other conventional and state-of-the-art implementations.","sentences":["Supercomputers have revolutionized how industries and scientific fields process large amounts of data.","These machines group hundreds or thousands of computing nodes working together to execute time-consuming programs that require a large amount of computational resources.","Over the years, supercomputers have expanded to include new and different technologies characterizing them as heterogeneous.","However, executing a program in a heterogeneous environment requires attention to a specific aspect of performance degradation: load imbalance.","In this research, we address the challenges associated with load imbalance when scheduling many homogeneous tasks in a heterogeneous environment.","To address this issue, we introduce the concept of adaptive asynchronous work-stealing.","This approach collects information about the nodes and utilizes it to improve work-stealing aspects, such as victim selection and task offloading.","Additionally, the proposed approach eliminates the need for extra threads to communicate information, thereby reducing overhead when implementing a fully asynchronous approach.","Our experimental results demonstrate a performance improvement of approximately 10.1\\% compared to other conventional and state-of-the-art implementations."],"url":"http://arxiv.org/abs/2401.04494v1"}
{"created":"2024-01-09 11:07:48","title":"SpiNNaker2: A Large-Scale Neuromorphic System for Event-Based and Asynchronous Machine Learning","abstract":"The joint progress of artificial neural networks (ANNs) and domain specific hardware accelerators such as GPUs and TPUs took over many domains of machine learning research. This development is accompanied by a rapid growth of the required computational demands for larger models and more data. Concurrently, emerging properties of foundation models such as in-context learning drive new opportunities for machine learning applications. However, the computational cost of such applications is a limiting factor of the technology in data centers, and more importantly in mobile devices and edge systems. To mediate the energy footprint and non-trivial latency of contemporary systems, neuromorphic computing systems deeply integrate computational principles of neurobiological systems by leveraging low-power analog and digital technologies. SpiNNaker2 is a digital neuromorphic chip developed for scalable machine learning. The event-based and asynchronous design of SpiNNaker2 allows the composition of large-scale systems involving thousands of chips. This work features the operating principles of SpiNNaker2 systems, outlining the prototype of novel machine learning applications. These applications range from ANNs over bio-inspired spiking neural networks to generalized event-based neural networks. With the successful development and deployment of SpiNNaker2, we aim to facilitate the advancement of event-based and asynchronous algorithms for future generations of machine learning systems.","sentences":["The joint progress of artificial neural networks (ANNs) and domain specific hardware accelerators such as GPUs and TPUs took over many domains of machine learning research.","This development is accompanied by a rapid growth of the required computational demands for larger models and more data.","Concurrently, emerging properties of foundation models such as in-context learning drive new opportunities for machine learning applications.","However, the computational cost of such applications is a limiting factor of the technology in data centers, and more importantly in mobile devices and edge systems.","To mediate the energy footprint and non-trivial latency of contemporary systems, neuromorphic computing systems deeply integrate computational principles of neurobiological systems by leveraging low-power analog and digital technologies.","SpiNNaker2 is a digital neuromorphic chip developed for scalable machine learning.","The event-based and asynchronous design of SpiNNaker2 allows the composition of large-scale systems involving thousands of chips.","This work features the operating principles of SpiNNaker2 systems, outlining the prototype of novel machine learning applications.","These applications range from ANNs over bio-inspired spiking neural networks to generalized event-based neural networks.","With the successful development and deployment of SpiNNaker2, we aim to facilitate the advancement of event-based and asynchronous algorithms for future generations of machine learning systems."],"url":"http://arxiv.org/abs/2401.04491v1"}
{"created":"2024-01-09 11:01:11","title":"Optimal Survival Trees: A Dynamic Programming Approach","abstract":"Survival analysis studies and predicts the time of death, or other singular unrepeated events, based on historical data, while the true time of death for some instances is unknown. Survival trees enable the discovery of complex nonlinear relations in a compact human comprehensible model, by recursively splitting the population and predicting a distinct survival distribution in each leaf node. We use dynamic programming to provide the first survival tree method with optimality guarantees, enabling the assessment of the optimality gap of heuristics. We improve the scalability of our method through a special algorithm for computing trees up to depth two. The experiments show that our method's run time even outperforms some heuristics for realistic cases while obtaining similar out-of-sample performance with the state-of-the-art.","sentences":["Survival analysis studies and predicts the time of death, or other singular unrepeated events, based on historical data, while the true time of death for some instances is unknown.","Survival trees enable the discovery of complex nonlinear relations in a compact human comprehensible model, by recursively splitting the population and predicting a distinct survival distribution in each leaf node.","We use dynamic programming to provide the first survival tree method with optimality guarantees, enabling the assessment of the optimality gap of heuristics.","We improve the scalability of our method through a special algorithm for computing trees up to depth two.","The experiments show that our method's run time even outperforms some heuristics for realistic cases while obtaining similar out-of-sample performance with the state-of-the-art."],"url":"http://arxiv.org/abs/2401.04489v1"}
{"created":"2024-01-09 10:39:17","title":"Continuously Learning New Words in Automatic Speech Recognition","abstract":"Despite recent advances, Automatic Speech Recognition (ASR) systems are still far from perfect. Typical errors include acronyms, named entities and domain-specific special words for which little or no data is available. To address the problem of recognizing these words, we propose an self-supervised continual learning approach. Given the audio of a lecture talk with corresponding slides, we bias the model towards decoding new words from the slides by using a memory-enhanced ASR model from previous work. Then, we perform inference on the talk, collecting utterances that contain detected new words into an adaptation dataset. Continual learning is then performed on this set by adapting low-rank matrix weights added to each weight matrix of the model. The whole procedure is iterated for many talks. We show that with this approach, we obtain increasing performance on the new words when they occur more frequently (more than 80% recall) while preserving the general performance of the model.","sentences":["Despite recent advances, Automatic Speech Recognition (ASR) systems are still far from perfect.","Typical errors include acronyms, named entities and domain-specific special words for which little or no data is available.","To address the problem of recognizing these words, we propose an self-supervised continual learning approach.","Given the audio of a lecture talk with corresponding slides, we bias the model towards decoding new words from the slides by using a memory-enhanced ASR model from previous work.","Then, we perform inference on the talk, collecting utterances that contain detected new words into an adaptation dataset.","Continual learning is then performed on this set by adapting low-rank matrix weights added to each weight matrix of the model.","The whole procedure is iterated for many talks.","We show that with this approach, we obtain increasing performance on the new words when they occur more frequently (more than 80% recall) while preserving the general performance of the model."],"url":"http://arxiv.org/abs/2401.04482v1"}
{"created":"2024-01-09 10:38:13","title":"Fighting Fire with Fire: Adversarial Prompting to Generate a Misinformation Detection Dataset","abstract":"The recent success in language generation capabilities of large language models (LLMs), such as GPT, Bard, Llama etc., can potentially lead to concerns about their possible misuse in inducing mass agitation and communal hatred via generating fake news and spreading misinformation. Traditional means of developing a misinformation ground-truth dataset does not scale well because of the extensive manual effort required to annotate the data. In this paper, we propose an LLM-based approach of creating silver-standard ground-truth datasets for identifying misinformation. Specifically speaking, given a trusted news article, our proposed approach involves prompting LLMs to automatically generate a summarised version of the original article. The prompts in our proposed approach act as a controlling mechanism to generate specific types of factual incorrectness in the generated summaries, e.g., incorrect quantities, false attributions etc. To investigate the usefulness of this dataset, we conduct a set of experiments where we train a range of supervised models for the task of misinformation detection.","sentences":["The recent success in language generation capabilities of large language models (LLMs), such as GPT, Bard, Llama etc., can potentially lead to concerns about their possible misuse in inducing mass agitation and communal hatred via generating fake news and spreading misinformation.","Traditional means of developing a misinformation ground-truth dataset does not scale well because of the extensive manual effort required to annotate the data.","In this paper, we propose an LLM-based approach of creating silver-standard ground-truth datasets for identifying misinformation.","Specifically speaking, given a trusted news article, our proposed approach involves prompting LLMs to automatically generate a summarised version of the original article.","The prompts in our proposed approach act as a controlling mechanism to generate specific types of factual incorrectness in the generated summaries, e.g., incorrect quantities, false attributions etc.","To investigate the usefulness of this dataset, we conduct a set of experiments where we train a range of supervised models for the task of misinformation detection."],"url":"http://arxiv.org/abs/2401.04481v1"}
{"created":"2024-01-09 10:24:46","title":"Combining Embedding-Based and Semantic-Based Models for Post-hoc Explanations in Recommender Systems","abstract":"In today's data-rich environment, recommender systems play a crucial role in decision support systems. They provide to users personalized recommendations and explanations about these recommendations. Embedding-based models, despite their widespread use, often suffer from a lack of interpretability, which can undermine trust and user engagement. This paper presents an approach that combines embedding-based and semantic-based models to generate post-hoc explanations in recommender systems, leveraging ontology-based knowledge graphs to improve interpretability and explainability. By organizing data within a structured framework, ontologies enable the modeling of intricate relationships between entities, which is essential for generating explanations. By combining embedding-based and semantic based models for post-hoc explanations in recommender systems, the framework we defined aims at producing meaningful and easy-to-understand explanations, enhancing user trust and satisfaction, and potentially promoting the adoption of recommender systems across the e-commerce sector.","sentences":["In today's data-rich environment, recommender systems play a crucial role in decision support systems.","They provide to users personalized recommendations and explanations about these recommendations.","Embedding-based models, despite their widespread use, often suffer from a lack of interpretability, which can undermine trust and user engagement.","This paper presents an approach that combines embedding-based and semantic-based models to generate post-hoc explanations in recommender systems, leveraging ontology-based knowledge graphs to improve interpretability and explainability.","By organizing data within a structured framework, ontologies enable the modeling of intricate relationships between entities, which is essential for generating explanations.","By combining embedding-based and semantic based models for post-hoc explanations in recommender systems, the framework we defined aims at producing meaningful and easy-to-understand explanations, enhancing user trust and satisfaction, and potentially promoting the adoption of recommender systems across the e-commerce sector."],"url":"http://arxiv.org/abs/2401.04474v1"}
{"created":"2024-01-09 09:58:42","title":"PhilEO Bench: Evaluating Geo-Spatial Foundation Models","abstract":"Massive amounts of unlabelled data are captured by Earth Observation (EO) satellites, with the Sentinel-2 constellation generating 1.6 TB of data daily. This makes Remote Sensing a data-rich domain well suited to Machine Learning (ML) solutions. However, a bottleneck in applying ML models to EO is the lack of annotated data as annotation is a labour-intensive and costly process. As a result, research in this domain has focused on Self-Supervised Learning and Foundation Model approaches. This paper addresses the need to evaluate different Foundation Models on a fair and uniform benchmark by introducing the PhilEO Bench, a novel evaluation framework for EO Foundation Models. The framework comprises of a testbed and a novel 400 GB Sentinel-2 dataset containing labels for three downstream tasks, building density estimation, road segmentation, and land cover classification. We present experiments using our framework evaluating different Foundation Models, including Prithvi and SatMAE, at multiple n-shots and convergence rates.","sentences":["Massive amounts of unlabelled data are captured by Earth Observation (EO) satellites, with the Sentinel-2 constellation generating 1.6 TB of data daily.","This makes Remote Sensing a data-rich domain well suited to Machine Learning (ML) solutions.","However, a bottleneck in applying ML models to EO is the lack of annotated data as annotation is a labour-intensive and costly process.","As a result, research in this domain has focused on Self-Supervised Learning and Foundation Model approaches.","This paper addresses the need to evaluate different Foundation Models on a fair and uniform benchmark by introducing the PhilEO Bench, a novel evaluation framework for EO Foundation Models.","The framework comprises of a testbed and a novel 400 GB Sentinel-2 dataset containing labels for three downstream tasks, building density estimation, road segmentation, and land cover classification.","We present experiments using our framework evaluating different Foundation Models, including Prithvi and SatMAE, at multiple n-shots and convergence rates."],"url":"http://arxiv.org/abs/2401.04464v1"}
{"created":"2024-01-09 09:57:38","title":"D3AD: Dynamic Denoising Diffusion Probabilistic Model for Anomaly Detection","abstract":"Diffusion models have found valuable applications in anomaly detection by capturing the nominal data distribution and identifying anomalies via reconstruction. Despite their merits, they struggle to localize anomalies of varying scales, especially larger anomalies like entire missing components. Addressing this, we present a novel framework that enhances the capability of diffusion models, by extending the previous introduced implicit conditioning approach Meng et al. (2022) in three significant ways. First, we incorporate a dynamic step size computation that allows for variable noising steps in the forward process guided by an initial anomaly prediction. Second, we demonstrate that denoising an only scaled input, without any added noise, outperforms conventional denoising process. Third, we project images in a latent space to abstract away from fine details that interfere with reconstruction of large missing components. Additionally, we propose a fine-tuning mechanism that facilitates the model to effectively grasp the nuances of the target domain. Our method undergoes rigorous evaluation on two prominent anomaly detection datasets VISA and BTAD, yielding state-of-the-art performance. Importantly, our framework effectively localizes anomalies regardless of their scale, marking a pivotal advancement in diffusion-based anomaly detection.","sentences":["Diffusion models have found valuable applications in anomaly detection by capturing the nominal data distribution and identifying anomalies via reconstruction.","Despite their merits, they struggle to localize anomalies of varying scales, especially larger anomalies like entire missing components.","Addressing this, we present a novel framework that enhances the capability of diffusion models, by extending the previous introduced implicit conditioning approach Meng et al.","(2022) in three significant ways.","First, we incorporate a dynamic step size computation that allows for variable noising steps in the forward process guided by an initial anomaly prediction.","Second, we demonstrate that denoising an only scaled input, without any added noise, outperforms conventional denoising process.","Third, we project images in a latent space to abstract away from fine details that interfere with reconstruction of large missing components.","Additionally, we propose a fine-tuning mechanism that facilitates the model to effectively grasp the nuances of the target domain.","Our method undergoes rigorous evaluation on two prominent anomaly detection datasets VISA and BTAD, yielding state-of-the-art performance.","Importantly, our framework effectively localizes anomalies regardless of their scale, marking a pivotal advancement in diffusion-based anomaly detection."],"url":"http://arxiv.org/abs/2401.04463v1"}
{"created":"2024-01-09 09:37:44","title":"Character comes from practice: longitudinal practice-based ethics training in data science","abstract":"In this chapter, we propose a non-traditional RCR training in data science that is grounded into a virtue theory framework. First, we delineate the approach in more theoretical detail, by discussing how the goal of RCR training is to foster the cultivation of certain moral abilities. We specify the nature of these abilities: while the ideal is the cultivation of virtues, the limited space allowed by RCR modules can only facilitate the cultivation of superficial abilities or proto-virtues, which help students to familiarize with moral and political issues in the data science environment. Third, we operationalize our approach by stressing that (proto-)virtue acquisition (like skill acquisition) occurs through the technical and social tasks of daily data science activities, where these repetitive tasks provide the opportunities to develop (proto-)virtue capacity and to support the development of ethically robust data systems. Finally, we discuss a concrete example of how this approach has been implemented. In particular, we describe how this method is applied to teach data ethics to students participating in the CODATA-RDA Data Science Summer Schools.","sentences":["In this chapter, we propose a non-traditional RCR training in data science that is grounded into a virtue theory framework.","First, we delineate the approach in more theoretical detail, by discussing how the goal of RCR training is to foster the cultivation of certain moral abilities.","We specify the nature of these abilities: while the ideal is the cultivation of virtues, the limited space allowed by RCR modules can only facilitate the cultivation of superficial abilities or proto-virtues, which help students to familiarize with moral and political issues in the data science environment.","Third, we operationalize our approach by stressing that (proto-)virtue acquisition (like skill acquisition) occurs through the technical and social tasks of daily data science activities, where these repetitive tasks provide the opportunities to develop (proto-)virtue capacity and to support the development of ethically robust data systems.","Finally, we discuss a concrete example of how this approach has been implemented.","In particular, we describe how this method is applied to teach data ethics to students participating in the CODATA-RDA Data Science Summer Schools."],"url":"http://arxiv.org/abs/2401.04454v1"}
{"created":"2024-01-09 09:25:58","title":"A Novel Dataset for Non-Destructive Inspection of Handwritten Documents","abstract":"Forensic handwriting examination is a branch of Forensic Science that aims to examine handwritten documents in order to properly define or hypothesize the manuscript's author. These analysis involves comparing two or more (digitized) documents through a comprehensive comparison of intrinsic local and global features. If a correlation exists and specific best practices are satisfied, then it will be possible to affirm that the documents under analysis were written by the same individual. The need to create sophisticated tools capable of extracting and comparing significant features has led to the development of cutting-edge software with almost entirely automated processes, improving the forensic examination of handwriting and achieving increasingly objective evaluations. This is made possible by algorithmic solutions based on purely mathematical concepts. Machine Learning and Deep Learning models trained with specific datasets could turn out to be the key elements to best solve the task at hand. In this paper, we proposed a new and challenging dataset consisting of two subsets: the first consists of 21 documents written either by the classic ``pen and paper\" approach (and later digitized) and directly acquired on common devices such as tablets; the second consists of 362 handwritten manuscripts by 124 different people, acquired following a specific pipeline. Our study pioneered a comparison between traditionally handwritten documents and those produced with digital tools (e.g., tablets). Preliminary results on the proposed datasets show that 90% classification accuracy can be achieved on the first subset (documents written on both paper and pen and later digitized and on tablets) and 96% on the second portion of the data. The datasets are available at https://iplab.dmi.unict.it/mfs/forensic-handwriting-analysis/novel-dataset-2023/.","sentences":["Forensic handwriting examination is a branch of Forensic Science that aims to examine handwritten documents in order to properly define or hypothesize the manuscript's author.","These analysis involves comparing two or more (digitized) documents through a comprehensive comparison of intrinsic local and global features.","If a correlation exists and specific best practices are satisfied, then it will be possible to affirm that the documents under analysis were written by the same individual.","The need to create sophisticated tools capable of extracting and comparing significant features has led to the development of cutting-edge software with almost entirely automated processes, improving the forensic examination of handwriting and achieving increasingly objective evaluations.","This is made possible by algorithmic solutions based on purely mathematical concepts.","Machine Learning and Deep Learning models trained with specific datasets could turn out to be the key elements to best solve the task at hand.","In this paper, we proposed a new and challenging dataset consisting of two subsets: the first consists of 21 documents written either by the classic ``pen and paper\" approach (and later digitized) and directly acquired on common devices such as tablets; the second consists of 362 handwritten manuscripts by 124 different people, acquired following a specific pipeline.","Our study pioneered a comparison between traditionally handwritten documents and those produced with digital tools (e.g., tablets).","Preliminary results on the proposed datasets show that 90% classification accuracy can be achieved on the first subset (documents written on both paper and pen and later digitized and on tablets) and 96% on the second portion of the data.","The datasets are available at https://iplab.dmi.unict.it/mfs/forensic-handwriting-analysis/novel-dataset-2023/."],"url":"http://arxiv.org/abs/2401.04448v1"}
{"created":"2024-01-09 09:11:41","title":"Image classification network enhancement methods based on knowledge injection","abstract":"The current deep neural network algorithm still stays in the end-to-end training supervision method like Image-Label pairs, which makes traditional algorithm is difficult to explain the reason for the results, and the prediction logic is difficult to understand and analyze. The current algorithm does not use the existing human knowledge information, which makes the model not in line with the human cognition model and makes the model not suitable for human use. In order to solve the above problems, the present invention provides a deep neural network training method based on the human knowledge, which uses the human cognition model to construct the deep neural network training model, and uses the existing human knowledge information to construct the deep neural network training model. This paper proposes a multi-level hierarchical deep learning algorithm, which is composed of multi-level hierarchical deep neural network architecture and multi-level hierarchical deep learning framework. The experimental results show that the proposed algorithm can effectively explain the hidden information of the neural network. The goal of our study is to improve the interpretability of deep neural networks (DNNs) by providing an analysis of the impact of knowledge injection on the classification task. We constructed a knowledge injection dataset with matching knowledge data and image classification data. The knowledge injection dataset is the benchmark dataset for the experiments in the paper. Our model expresses the improvement in interpretability and classification task performance of hidden layers at different scales.","sentences":["The current deep neural network algorithm still stays in the end-to-end training supervision method like Image-Label pairs, which makes traditional algorithm is difficult to explain the reason for the results, and the prediction logic is difficult to understand and analyze.","The current algorithm does not use the existing human knowledge information, which makes the model not in line with the human cognition model and makes the model not suitable for human use.","In order to solve the above problems, the present invention provides a deep neural network training method based on the human knowledge, which uses the human cognition model to construct the deep neural network training model, and uses the existing human knowledge information to construct the deep neural network training model.","This paper proposes a multi-level hierarchical deep learning algorithm, which is composed of multi-level hierarchical deep neural network architecture and multi-level hierarchical deep learning framework.","The experimental results show that the proposed algorithm can effectively explain the hidden information of the neural network.","The goal of our study is to improve the interpretability of deep neural networks (DNNs) by providing an analysis of the impact of knowledge injection on the classification task.","We constructed a knowledge injection dataset with matching knowledge data and image classification data.","The knowledge injection dataset is the benchmark dataset for the experiments in the paper.","Our model expresses the improvement in interpretability and classification task performance of hidden layers at different scales."],"url":"http://arxiv.org/abs/2401.04441v1"}
{"created":"2024-01-09 08:59:39","title":"Uncertainty-aware Sampling for Long-tailed Semi-supervised Learning","abstract":"For semi-supervised learning with imbalance classes, the long-tailed distribution of data will increase the model prediction bias toward dominant classes, undermining performance on less frequent classes. Existing methods also face challenges in ensuring the selection of sufficiently reliable pseudo-labels for model training and there is a lack of mechanisms to adjust the selection of more reliable pseudo-labels based on different training stages. To mitigate this issue, we introduce uncertainty into the modeling process for pseudo-label sampling, taking into account that the model performance on the tailed classes varies over different training stages. For example, at the early stage of model training, the limited predictive accuracy of model results in a higher rate of uncertain pseudo-labels. To counter this, we propose an Uncertainty-Aware Dynamic Threshold Selection (UDTS) approach. This approach allows the model to perceive the uncertainty of pseudo-labels at different training stages, thereby adaptively adjusting the selection thresholds for different classes. Compared to other methods such as the baseline method FixMatch, UDTS achieves an increase in accuracy of at least approximately 5.26%, 1.75%, 9.96%, and 1.28% on the natural scene image datasets CIFAR10-LT, CIFAR100-LT, STL-10-LT, and the medical image dataset TissueMNIST, respectively. The source code of UDTS is publicly available at: https://github.com/yangk/UDTS.","sentences":["For semi-supervised learning with imbalance classes, the long-tailed distribution of data will increase the model prediction bias toward dominant classes, undermining performance on less frequent classes.","Existing methods also face challenges in ensuring the selection of sufficiently reliable pseudo-labels for model training and there is a lack of mechanisms to adjust the selection of more reliable pseudo-labels based on different training stages.","To mitigate this issue, we introduce uncertainty into the modeling process for pseudo-label sampling, taking into account that the model performance on the tailed classes varies over different training stages.","For example, at the early stage of model training, the limited predictive accuracy of model results in a higher rate of uncertain pseudo-labels.","To counter this, we propose an Uncertainty-Aware Dynamic Threshold Selection (UDTS) approach.","This approach allows the model to perceive the uncertainty of pseudo-labels at different training stages, thereby adaptively adjusting the selection thresholds for different classes.","Compared to other methods such as the baseline method FixMatch, UDTS achieves an increase in accuracy of at least approximately 5.26%, 1.75%, 9.96%, and 1.28% on the natural scene image datasets CIFAR10-LT, CIFAR100-LT, STL-10-LT, and the medical image dataset TissueMNIST, respectively.","The source code of UDTS is publicly available at: https://github.com/yangk/UDTS."],"url":"http://arxiv.org/abs/2401.04435v1"}
{"created":"2024-01-09 08:51:56","title":"i-Rebalance: Personalized Vehicle Repositioning for Supply Demand Balance","abstract":"Ride-hailing platforms have been facing the challenge of balancing demand and supply. Existing vehicle reposition techniques often treat drivers as homogeneous agents and relocate them deterministically, assuming compliance with the reposition. In this paper, we consider a more realistic and driver-centric scenario where drivers have unique cruising preferences and can decide whether to take the recommendation or not on their own. We propose i-Rebalance, a personalized vehicle reposition technique with deep reinforcement learning (DRL). i-Rebalance estimates drivers' decisions on accepting reposition recommendations through an on-field user study involving 99 real drivers. To optimize supply-demand balance and enhance preference satisfaction simultaneously, i-Rebalance has a sequential reposition strategy with dual DRL agents: Grid Agent to determine the reposition order of idle vehicles, and Vehicle Agent to provide personalized recommendations to each vehicle in the pre-defined order. This sequential learning strategy facilitates more effective policy training within a smaller action space compared to traditional joint-action methods. Evaluation of real-world trajectory data shows that i-Rebalance improves driver acceptance rate by 38.07% and total driver income by 9.97%.","sentences":["Ride-hailing platforms have been facing the challenge of balancing demand and supply.","Existing vehicle reposition techniques often treat drivers as homogeneous agents and relocate them deterministically, assuming compliance with the reposition.","In this paper, we consider a more realistic and driver-centric scenario where drivers have unique cruising preferences and can decide whether to take the recommendation or not on their own.","We propose i-Rebalance, a personalized vehicle reposition technique with deep reinforcement learning (DRL).","i-Rebalance estimates drivers' decisions on accepting reposition recommendations through an on-field user study involving 99 real drivers.","To optimize supply-demand balance and enhance preference satisfaction simultaneously, i-Rebalance has a sequential reposition strategy with dual DRL agents:","Grid Agent to determine the reposition order of idle vehicles, and Vehicle Agent to provide personalized recommendations to each vehicle in the pre-defined order.","This sequential learning strategy facilitates more effective policy training within a smaller action space compared to traditional joint-action methods.","Evaluation of real-world trajectory data shows that i-Rebalance improves driver acceptance rate by 38.07% and total driver income by 9.97%."],"url":"http://arxiv.org/abs/2401.04429v1"}
{"created":"2024-01-09 08:34:50","title":"Meta-forests: Domain generalization on random forests with meta-learning","abstract":"Domain generalization is a popular machine learning technique that enables models to perform well on the unseen target domain, by learning from multiple source domains. Domain generalization is useful in cases where data is limited, difficult, or expensive to collect, such as in object recognition and biomedicine. In this paper, we propose a novel domain generalization algorithm called \"meta-forests\", which builds upon the basic random forests model by incorporating the meta-learning strategy and maximum mean discrepancy measure. The aim of meta-forests is to enhance the generalization ability of classifiers by reducing the correlation among trees and increasing their strength. More specifically, meta-forests conducts meta-learning optimization during each meta-task, while also utilizing the maximum mean discrepancy as a regularization term to penalize poor generalization performance in the meta-test process. To evaluate the effectiveness of our algorithm, we test it on two publicly object recognition datasets and a glucose monitoring dataset that we have used in a previous study. Our results show that meta-forests outperforms state-of-the-art approaches in terms of generalization performance on both object recognition and glucose monitoring datasets.","sentences":["Domain generalization is a popular machine learning technique that enables models to perform well on the unseen target domain, by learning from multiple source domains.","Domain generalization is useful in cases where data is limited, difficult, or expensive to collect, such as in object recognition and biomedicine.","In this paper, we propose a novel domain generalization algorithm called \"meta-forests\", which builds upon the basic random forests model by incorporating the meta-learning strategy and maximum mean discrepancy measure.","The aim of meta-forests is to enhance the generalization ability of classifiers by reducing the correlation among trees and increasing their strength.","More specifically, meta-forests conducts meta-learning optimization during each meta-task, while also utilizing the maximum mean discrepancy as a regularization term to penalize poor generalization performance in the meta-test process.","To evaluate the effectiveness of our algorithm, we test it on two publicly object recognition datasets and a glucose monitoring dataset that we have used in a previous study.","Our results show that meta-forests outperforms state-of-the-art approaches in terms of generalization performance on both object recognition and glucose monitoring datasets."],"url":"http://arxiv.org/abs/2401.04425v1"}
{"created":"2024-01-09 08:30:50","title":"Privacy-Preserving Sequential Recommendation with Collaborative Confusion","abstract":"Sequential recommendation has attracted a lot of attention from both academia and industry, however the privacy risks associated to gathering and transferring users' personal interaction data are often underestimated or ignored. Existing privacy-preserving studies are mainly applied to traditional collaborative filtering or matrix factorization rather than sequential recommendation. Moreover, these studies are mostly based on differential privacy or federated learning, which often leads to significant performance degradation, or has high requirements for communication. In this work, we address privacy-preserving from a different perspective. Unlike existing research, we capture collaborative signals of neighbor interaction sequences and directly inject indistinguishable items into the target sequence before the recommendation process begins, thereby increasing the perplexity of the target sequence. Even if the target interaction sequence is obtained by attackers, it is difficult to discern which ones are the actual user interaction records. To achieve this goal, we propose a CoLlaborative-cOnfusion seqUential recommenDer, namely CLOUD, which incorporates a collaborative confusion mechanism to edit the raw interaction sequences before conducting recommendation. Specifically, CLOUD first calculates the similarity between the target interaction sequence and other neighbor sequences to find similar sequences. Then, CLOUD considers the shared representation of the target sequence and similar sequences to determine the operation to be performed: keep, delete, or insert. We design a copy mechanism to make items from similar sequences have a higher probability to be inserted into the target sequence. Finally, the modified sequence is used to train the recommender and predict the next item.","sentences":["Sequential recommendation has attracted a lot of attention from both academia and industry, however the privacy risks associated to gathering and transferring users' personal interaction data are often underestimated or ignored.","Existing privacy-preserving studies are mainly applied to traditional collaborative filtering or matrix factorization rather than sequential recommendation.","Moreover, these studies are mostly based on differential privacy or federated learning, which often leads to significant performance degradation, or has high requirements for communication.","In this work, we address privacy-preserving from a different perspective.","Unlike existing research, we capture collaborative signals of neighbor interaction sequences and directly inject indistinguishable items into the target sequence before the recommendation process begins, thereby increasing the perplexity of the target sequence.","Even if the target interaction sequence is obtained by attackers, it is difficult to discern which ones are the actual user interaction records.","To achieve this goal, we propose a CoLlaborative-cOnfusion seqUential recommenDer, namely CLOUD, which incorporates a collaborative confusion mechanism to edit the raw interaction sequences before conducting recommendation.","Specifically, CLOUD first calculates the similarity between the target interaction sequence and other neighbor sequences to find similar sequences.","Then, CLOUD considers the shared representation of the target sequence and similar sequences to determine the operation to be performed: keep, delete, or insert.","We design a copy mechanism to make items from similar sequences have a higher probability to be inserted into the target sequence.","Finally, the modified sequence is used to train the recommender and predict the next item."],"url":"http://arxiv.org/abs/2401.04423v1"}
{"created":"2024-01-09 08:08:38","title":"Hiding Information for Secure and Covert Data Storage in Commercial ReRAM Chips","abstract":"This article introduces a novel, low-cost technique for hiding data in commercially available resistive-RAM (ReRAM) chips. The data is kept hidden in ReRAM cells by manipulating its analog physical properties through switching ($\\textit{set/reset}$) operations. This hidden data, later, is retrieved by sensing the changes in cells' physical properties (i.e., $\\textit{set/reset}$ time of the memory cells). The proposed system-level hiding technique does not affect the normal memory operations and does not require any hardware modifications. Furthermore, the proposed hiding approach is robust against temperature variations and the aging of the devices through normal read/write operation. The silicon results show that our proposed data hiding technique is acceptably fast with ${\\sim}0.4bit/min$ of encoding and ${\\sim}15.625bits/s$ of retrieval rates, and the hidden message is unrecoverable without the knowledge of the secret key, which is used to enhance the security of hidden information.","sentences":["This article introduces a novel, low-cost technique for hiding data in commercially available resistive-RAM (ReRAM) chips.","The data is kept hidden in ReRAM cells by manipulating its analog physical properties through switching ($\\textit{set/reset}$) operations.","This hidden data, later, is retrieved by sensing the changes in cells' physical properties (i.e., $\\textit{set/reset}$ time of the memory cells).","The proposed system-level hiding technique does not affect the normal memory operations and does not require any hardware modifications.","Furthermore, the proposed hiding approach is robust against temperature variations and the aging of the devices through normal read/write operation.","The silicon results show that our proposed data hiding technique is acceptably fast with ${\\sim}0.4bit/min$ of encoding and ${\\sim}15.625bits/s$ of retrieval rates, and the hidden message is unrecoverable without the knowledge of the secret key, which is used to enhance the security of hidden information."],"url":"http://arxiv.org/abs/2401.04411v1"}
{"created":"2024-01-09 08:04:11","title":"Fine-Grained Embedding Dimension Optimization During Training for Recommender Systems","abstract":"Huge embedding tables in modern Deep Learning Recommender Models (DLRM) require prohibitively large memory during training and inference. Aiming to reduce the memory footprint of training, this paper proposes FIne-grained In-Training Embedding Dimension optimization (FIITED). Given the observation that embedding vectors are not equally important, FIITED adjusts the dimension of each individual embedding vector continuously during training, assigning longer dimensions to more important embeddings while adapting to dynamic changes in data. A novel embedding storage system based on virtually-hashed physically-indexed hash tables is designed to efficiently implement the embedding dimension adjustment and effectively enable memory saving. Experiments on two industry models show that FIITED is able to reduce the size of embeddings by more than 65% while maintaining the trained model's quality, saving significantly more memory than a state-of-the-art in-training embedding pruning method. On public click-through rate prediction datasets, FIITED is able to prune up to 93.75%-99.75% embeddings without significant accuracy loss.","sentences":["Huge embedding tables in modern Deep Learning Recommender Models (DLRM) require prohibitively large memory during training and inference.","Aiming to reduce the memory footprint of training, this paper proposes FIne-grained In-Training Embedding Dimension optimization (FIITED).","Given the observation that embedding vectors are not equally important, FIITED adjusts the dimension of each individual embedding vector continuously during training, assigning longer dimensions to more important embeddings while adapting to dynamic changes in data.","A novel embedding storage system based on virtually-hashed physically-indexed hash tables is designed to efficiently implement the embedding dimension adjustment and effectively enable memory saving.","Experiments on two industry models show that FIITED is able to reduce the size of embeddings by more than 65% while maintaining the trained model's quality, saving significantly more memory than a state-of-the-art in-training embedding pruning method.","On public click-through rate prediction datasets, FIITED is able to prune up to 93.75%-99.75% embeddings without significant accuracy loss."],"url":"http://arxiv.org/abs/2401.04408v1"}
{"created":"2024-01-09 08:02:00","title":"MapAI: Precision in Building Segmentation","abstract":"MapAI: Precision in Building Segmentation is a competition arranged with the Norwegian Artificial Intelligence Research Consortium (NORA) in collaboration with Centre for Artificial Intelligence Research at the University of Agder (CAIR), the Norwegian Mapping Authority, AI:Hub, Norkart, and the Danish Agency for Data Supply and Infrastructure. The competition will be held in the fall of 2022. It will be concluded at the Northern Lights Deep Learning conference focusing on the segmentation of buildings using aerial images and laser data. We propose two different tasks to segment buildings, where the first task can only utilize aerial images, while the second must use laser data (LiDAR) with or without aerial images. Furthermore, we use IoU and Boundary IoU to properly evaluate the precision of the models, with the latter being an IoU measure that evaluates the results' boundaries. We provide the participants with a training dataset and keep a test dataset for evaluation.","sentences":["MapAI: Precision in Building Segmentation is a competition arranged with the Norwegian Artificial Intelligence Research Consortium (NORA) in collaboration with Centre for Artificial Intelligence Research at the University of Agder (CAIR), the Norwegian Mapping Authority, AI:Hub, Norkart, and the Danish Agency for Data Supply and Infrastructure.","The competition will be held in the fall of 2022.","It will be concluded at the Northern Lights Deep Learning conference focusing on the segmentation of buildings using aerial images and laser data.","We propose two different tasks to segment buildings, where the first task can only utilize aerial images, while the second must use laser data (LiDAR) with or without aerial images.","Furthermore, we use IoU and Boundary IoU to properly evaluate the precision of the models, with the latter being an IoU measure that evaluates the results' boundaries.","We provide the participants with a training dataset and keep a test dataset for evaluation."],"url":"http://arxiv.org/abs/2401.04406v1"}
{"created":"2024-01-09 08:01:47","title":"Optimal Transcoding Resolution Prediction for Efficient Per-Title Bitrate Ladder Estimation","abstract":"Adaptive video streaming requires efficient bitrate ladder construction to meet heterogeneous network conditions and end-user demands. Per-title optimized encoding typically traverses numerous encoding parameters to search the Pareto-optimal operating points for each video. Recently, researchers have attempted to predict the content-optimized bitrate ladder for pre-encoding overhead reduction. However, existing methods commonly estimate the encoding parameters on the Pareto front and still require subsequent pre-encodings. In this paper, we propose to directly predict the optimal transcoding resolution at each preset bitrate for efficient bitrate ladder construction. We adopt a Temporal Attentive Gated Recurrent Network to capture spatial-temporal features and predict transcoding resolutions as a multi-task classification problem. We demonstrate that content-optimized bitrate ladders can thus be efficiently determined without any pre-encoding. Our method well approximates the ground-truth bitrate-resolution pairs with a slight Bj{\\o}ntegaard Delta rate loss of 1.21% and significantly outperforms the state-of-the-art fixed ladder.","sentences":["Adaptive video streaming requires efficient bitrate ladder construction to meet heterogeneous network conditions and end-user demands.","Per-title optimized encoding typically traverses numerous encoding parameters to search the Pareto-optimal operating points for each video.","Recently, researchers have attempted to predict the content-optimized bitrate ladder for pre-encoding overhead reduction.","However, existing methods commonly estimate the encoding parameters on the Pareto front and still require subsequent pre-encodings.","In this paper, we propose to directly predict the optimal transcoding resolution at each preset bitrate for efficient bitrate ladder construction.","We adopt a Temporal Attentive Gated Recurrent Network to capture spatial-temporal features and predict transcoding resolutions as a multi-task classification problem.","We demonstrate that content-optimized bitrate ladders can thus be efficiently determined without any pre-encoding.","Our method well approximates the ground-truth bitrate-resolution pairs with a slight Bj{\\o}ntegaard","Delta rate loss of 1.21% and significantly outperforms the state-of-the-art fixed ladder."],"url":"http://arxiv.org/abs/2401.04405v1"}
{"created":"2024-01-09 07:59:42","title":"MST: Adaptive Multi-Scale Tokens Guided Interactive Segmentation","abstract":"In the field of Industrial Informatics, interactive segmentation has gained significant attention for its application in human-computer interaction and data annotation. Existing algorithms, however, face challenges in balancing the segmentation accuracy between large and small targets, often leading to an increased number of user interactions. To tackle this, a novel multi-scale token adaptation algorithm, leveraging token similarity, has been devised to enhance segmentation across varying target sizes. This algorithm utilizes a differentiable top-k tokens selection mechanism, allowing for fewer tokens to be used while maintaining efficient multi-scale token interaction. Furthermore, a contrastive loss is introduced to better discriminate between target and background tokens, improving the correctness and robustness of the tokens similar to the target. Extensive benchmarking shows that the algorithm achieves state-of-the-art (SOTA) performance compared to current methods. An interactive demo and all reproducible codes will be released at https://github.com/hahamyt/mst.","sentences":["In the field of Industrial Informatics, interactive segmentation has gained significant attention for its application in human-computer interaction and data annotation.","Existing algorithms, however, face challenges in balancing the segmentation accuracy between large and small targets, often leading to an increased number of user interactions.","To tackle this, a novel multi-scale token adaptation algorithm, leveraging token similarity, has been devised to enhance segmentation across varying target sizes.","This algorithm utilizes a differentiable top-k tokens selection mechanism, allowing for fewer tokens to be used while maintaining efficient multi-scale token interaction.","Furthermore, a contrastive loss is introduced to better discriminate between target and background tokens, improving the correctness and robustness of the tokens similar to the target.","Extensive benchmarking shows that the algorithm achieves state-of-the-art (SOTA) performance compared to current methods.","An interactive demo and all reproducible codes will be released at https://github.com/hahamyt/mst."],"url":"http://arxiv.org/abs/2401.04403v1"}
{"created":"2024-01-09 07:57:21","title":"IGNITE: Individualized GeNeration of Imputations in Time-series Electronic health records","abstract":"Electronic Health Records present a valuable modality for driving personalized medicine, where treatment is tailored to fit individual-level differences. For this purpose, many data-driven machine learning and statistical models rely on the wealth of longitudinal EHRs to study patients' physiological and treatment effects. However, longitudinal EHRs tend to be sparse and highly missing, where missingness could also be informative and reflect the underlying patient's health status. Therefore, the success of data-driven models for personalized medicine highly depends on how the EHR data is represented from physiological data, treatments, and the missing values in the data. To this end, we propose a novel deep-learning model that learns the underlying patient dynamics over time across multivariate data to generate personalized realistic values conditioning on an individual's demographic characteristics and treatments. Our proposed model, IGNITE (Individualized GeNeration of Imputations in Time-series Electronic health records), utilises a conditional dual-variational autoencoder augmented with dual-stage attention to generate missing values for an individual. In IGNITE, we further propose a novel individualized missingness mask (IMM), which helps our model generate values based on the individual's observed data and missingness patterns. We further extend the use of IGNITE from imputing missingness to a personalized data synthesizer, where it generates missing EHRs that were never observed prior or even generates new patients for various applications. We validate our model on three large publicly available datasets and show that IGNITE outperforms state-of-the-art approaches in missing data reconstruction and task prediction.","sentences":["Electronic Health Records present a valuable modality for driving personalized medicine, where treatment is tailored to fit individual-level differences.","For this purpose, many data-driven machine learning and statistical models rely on the wealth of longitudinal EHRs to study patients' physiological and treatment effects.","However, longitudinal EHRs tend to be sparse and highly missing, where missingness could also be informative and reflect the underlying patient's health status.","Therefore, the success of data-driven models for personalized medicine highly depends on how the EHR data is represented from physiological data, treatments, and the missing values in the data.","To this end, we propose a novel deep-learning model that learns the underlying patient dynamics over time across multivariate data to generate personalized realistic values conditioning on an individual's demographic characteristics and treatments.","Our proposed model, IGNITE (Individualized GeNeration of Imputations in Time-series Electronic health records), utilises a conditional dual-variational autoencoder augmented with dual-stage attention to generate missing values for an individual.","In IGNITE, we further propose a novel individualized missingness mask (IMM), which helps our model generate values based on the individual's observed data and missingness patterns.","We further extend the use of IGNITE from imputing missingness to a personalized data synthesizer, where it generates missing EHRs that were never observed prior or even generates new patients for various applications.","We validate our model on three large publicly available datasets and show that IGNITE outperforms state-of-the-art approaches in missing data reconstruction and task prediction."],"url":"http://arxiv.org/abs/2401.04402v1"}
{"created":"2024-01-09 07:46:26","title":"Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding","abstract":"Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the Chain-of-Table framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, showing the reasoning process for a given tabular problem. The chain carries structured information of the intermediate results, enabling more accurate and reliable predictions. Chain-of-Table achieves new state-of-the-art performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM choices.","sentences":["Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification.","Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data.","Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain.","We propose the Chain-of-Table framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts.","Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain.","LLMs can therefore dynamically plan the next operation based on the results of the previous ones.","This continuous evolution of the table forms a chain, showing the reasoning process for a given tabular problem.","The chain carries structured information of the intermediate results, enabling more accurate and reliable predictions.","Chain-of-Table achieves new state-of-the-art performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM choices."],"url":"http://arxiv.org/abs/2401.04398v1"}
{"created":"2024-01-09 07:39:36","title":"The Role of Higher-Order Cognitive Models in Active Learning","abstract":"Building machines capable of efficiently collaborating with humans has been a longstanding goal in artificial intelligence. Especially in the presence of uncertainties, optimal cooperation often requires that humans and artificial agents model each other's behavior and use these models to infer underlying goals, beliefs or intentions, potentially involving multiple levels of recursion. Empirical evidence for such higher-order cognition in human behavior is also provided by previous works in cognitive science, linguistics, and robotics. We advocate for a new paradigm for active learning for human feedback that utilises humans as active data sources while accounting for their higher levels of agency. In particular, we discuss how increasing level of agency results in qualitatively different forms of rational communication between an active learning system and a teacher. Additionally, we provide a practical example of active learning using a higher-order cognitive model. This is accompanied by a computational study that underscores the unique behaviors that this model produces.","sentences":["Building machines capable of efficiently collaborating with humans has been a longstanding goal in artificial intelligence.","Especially in the presence of uncertainties, optimal cooperation often requires that humans and artificial agents model each other's behavior and use these models to infer underlying goals, beliefs or intentions, potentially involving multiple levels of recursion.","Empirical evidence for such higher-order cognition in human behavior is also provided by previous works in cognitive science, linguistics, and robotics.","We advocate for a new paradigm for active learning for human feedback that utilises humans as active data sources while accounting for their higher levels of agency.","In particular, we discuss how increasing level of agency results in qualitatively different forms of rational communication between an active learning system and a teacher.","Additionally, we provide a practical example of active learning using a higher-order cognitive model.","This is accompanied by a computational study that underscores the unique behaviors that this model produces."],"url":"http://arxiv.org/abs/2401.04397v1"}
{"created":"2024-01-09 07:22:30","title":"Learning with Noisy Labels: Interconnection of Two Expectation-Maximizations","abstract":"Labor-intensive labeling becomes a bottleneck in developing computer vision algorithms based on deep learning. For this reason, dealing with imperfect labels has increasingly gained attention and has become an active field of study. We address learning with noisy labels (LNL) problem, which is formalized as a task of finding a structured manifold in the midst of noisy data. In this framework, we provide a proper objective function and an optimization algorithm based on two expectation-maximization (EM) cycles. The separate networks associated with the two EM cycles collaborate to optimize the objective function, where one model is for distinguishing clean labels from corrupted ones while the other is for refurbishing the corrupted labels. This approach results in a non-collapsing LNL-flywheel model in the end. Experiments show that our algorithm achieves state-of-the-art performance in multiple standard benchmarks with substantial margins under various types of label noise.","sentences":["Labor-intensive labeling becomes a bottleneck in developing computer vision algorithms based on deep learning.","For this reason, dealing with imperfect labels has increasingly gained attention and has become an active field of study.","We address learning with noisy labels (LNL) problem, which is formalized as a task of finding a structured manifold in the midst of noisy data.","In this framework, we provide a proper objective function and an optimization algorithm based on two expectation-maximization (EM) cycles.","The separate networks associated with the two EM cycles collaborate to optimize the objective function, where one model is for distinguishing clean labels from corrupted ones while the other is for refurbishing the corrupted labels.","This approach results in a non-collapsing LNL-flywheel model in the end.","Experiments show that our algorithm achieves state-of-the-art performance in multiple standard benchmarks with substantial margins under various types of label noise."],"url":"http://arxiv.org/abs/2401.04390v1"}
{"created":"2024-01-09 07:14:45","title":"Machine unlearning through fine-grained model parameters perturbation","abstract":"Machine unlearning techniques, which involve retracting data records and reducing influence of said data on trained models, help with the user privacy protection objective but incur significant computational costs. Weight perturbation-based unlearning is a general approach, but it typically involves globally modifying the parameters. We propose fine-grained Top-K and Random-k parameters perturbed inexact machine unlearning strategies that address the privacy needs while keeping the computational costs tractable.   In order to demonstrate the efficacy of our strategies we also tackle the challenge of evaluating the effectiveness of machine unlearning by considering the model's generalization performance across both unlearning and remaining data. To better assess the unlearning effect and model generalization, we propose novel metrics, namely, the forgetting rate and memory retention rate. However, for inexact machine unlearning, current metrics are inadequate in quantifying the degree of forgetting that occurs after unlearning strategies are applied. To address this, we introduce SPD-GAN, which subtly perturbs the distribution of data targeted for unlearning. Then, we evaluate the degree of unlearning by measuring the performance difference of the models on the perturbed unlearning data before and after the unlearning process. By implementing these innovative techniques and metrics, we achieve computationally efficacious privacy protection in machine learning applications without significant sacrifice of model performance. Furthermore, this approach provides a novel method for evaluating the degree of unlearning.","sentences":["Machine unlearning techniques, which involve retracting data records and reducing influence of said data on trained models, help with the user privacy protection objective but incur significant computational costs.","Weight perturbation-based unlearning is a general approach, but it typically involves globally modifying the parameters.","We propose fine-grained Top-K and Random-k parameters perturbed inexact machine unlearning strategies that address the privacy needs while keeping the computational costs tractable.   ","In order to demonstrate the efficacy of our strategies we also tackle the challenge of evaluating the effectiveness of machine unlearning by considering the model's generalization performance across both unlearning and remaining data.","To better assess the unlearning effect and model generalization, we propose novel metrics, namely, the forgetting rate and memory retention rate.","However, for inexact machine unlearning, current metrics are inadequate in quantifying the degree of forgetting that occurs after unlearning strategies are applied.","To address this, we introduce SPD-GAN, which subtly perturbs the distribution of data targeted for unlearning.","Then, we evaluate the degree of unlearning by measuring the performance difference of the models on the perturbed unlearning data before and after the unlearning process.","By implementing these innovative techniques and metrics, we achieve computationally efficacious privacy protection in machine learning applications without significant sacrifice of model performance.","Furthermore, this approach provides a novel method for evaluating the degree of unlearning."],"url":"http://arxiv.org/abs/2401.04385v1"}
{"created":"2024-01-09 06:27:09","title":"Towards Explainable Artificial Intelligence (XAI): A Data Mining Perspective","abstract":"Given the complexity and lack of transparency in deep neural networks (DNNs), extensive efforts have been made to make these systems more interpretable or explain their behaviors in accessible terms. Unlike most reviews, which focus on algorithmic and model-centric perspectives, this work takes a \"data-centric\" view, examining how data collection, processing, and analysis contribute to explainable AI (XAI). We categorize existing work into three categories subject to their purposes: interpretations of deep models, referring to feature attributions and reasoning processes that correlate data points with model outputs; influences of training data, examining the impact of training data nuances, such as data valuation and sample anomalies, on decision-making processes; and insights of domain knowledge, discovering latent patterns and fostering new knowledge from data and models to advance social values and scientific discovery. Specifically, we distill XAI methodologies into data mining operations on training and testing data across modalities, such as images, text, and tabular data, as well as on training logs, checkpoints, models and other DNN behavior descriptors. In this way, our study offers a comprehensive, data-centric examination of XAI from a lens of data mining methods and applications.","sentences":["Given the complexity and lack of transparency in deep neural networks (DNNs), extensive efforts have been made to make these systems more interpretable or explain their behaviors in accessible terms.","Unlike most reviews, which focus on algorithmic and model-centric perspectives, this work takes a \"data-centric\" view, examining how data collection, processing, and analysis contribute to explainable AI (XAI).","We categorize existing work into three categories subject to their purposes: interpretations of deep models, referring to feature attributions and reasoning processes that correlate data points with model outputs; influences of training data, examining the impact of training data nuances, such as data valuation and sample anomalies, on decision-making processes; and insights of domain knowledge, discovering latent patterns and fostering new knowledge from data and models to advance social values and scientific discovery.","Specifically, we distill XAI methodologies into data mining operations on training and testing data across modalities, such as images, text, and tabular data, as well as on training logs, checkpoints, models and other DNN behavior descriptors.","In this way, our study offers a comprehensive, data-centric examination of XAI from a lens of data mining methods and applications."],"url":"http://arxiv.org/abs/2401.04374v1"}
{"created":"2024-01-09 05:52:02","title":"Air Quality Forecasting Using Machine Learning: A Global perspective with Relevance to Low-Resource Settings","abstract":"Air pollution stands as the fourth leading cause of death globally. While extensive research has been conducted in this domain, most approaches rely on large datasets when it comes to prediction. This limits their applicability in low-resource settings though more vulnerable. This study addresses this gap by proposing a novel machine learning approach for accurate air quality prediction using two months of air quality data. By leveraging the World Weather Repository, the meteorological, air pollutant, and Air Quality Index features from 197 capital cities were considered to predict air quality for the next day. The evaluation of several machine learning models demonstrates the effectiveness of the Random Forest algorithm in generating reliable predictions, particularly when applied to classification rather than regression, approach which enhances the model's generalizability by 42%, achieving a cross-validation score of 0.38 for regression and 0.89 for classification. To instill confidence in the predictions, interpretable machine learning was considered. Finally, a cost estimation comparing the implementation of this solution in high-resource and low-resource settings is presented including a tentative of technology licensing business model. This research highlights the potential for resource-limited countries to independently predict air quality while awaiting larger datasets to further refine their predictions.","sentences":["Air pollution stands as the fourth leading cause of death globally.","While extensive research has been conducted in this domain, most approaches rely on large datasets when it comes to prediction.","This limits their applicability in low-resource settings though more vulnerable.","This study addresses this gap by proposing a novel machine learning approach for accurate air quality prediction using two months of air quality data.","By leveraging the World Weather Repository, the meteorological, air pollutant, and Air Quality Index features from 197 capital cities were considered to predict air quality for the next day.","The evaluation of several machine learning models demonstrates the effectiveness of the Random Forest algorithm in generating reliable predictions, particularly when applied to classification rather than regression, approach which enhances the model's generalizability by 42%, achieving a cross-validation score of 0.38 for regression and 0.89 for classification.","To instill confidence in the predictions, interpretable machine learning was considered.","Finally, a cost estimation comparing the implementation of this solution in high-resource and low-resource settings is presented including a tentative of technology licensing business model.","This research highlights the potential for resource-limited countries to independently predict air quality while awaiting larger datasets to further refine their predictions."],"url":"http://arxiv.org/abs/2401.04369v1"}
{"created":"2024-01-09 05:42:32","title":"Enhancing Acute Kidney Injury Prediction through Integration of Drug Features in Intensive Care Units","abstract":"The relationship between acute kidney injury (AKI) prediction and nephrotoxic drugs, or drugs that adversely affect kidney function, is one that has yet to be explored in the critical care setting. One contributing factor to this gap in research is the limited investigation of drug modalities in the intensive care unit (ICU) context, due to the challenges of processing prescription data into the corresponding drug representations and a lack in the comprehensive understanding of these drug representations. This study addresses this gap by proposing a novel approach that leverages patient prescription data as a modality to improve existing models for AKI prediction. We base our research on Electronic Health Record (EHR) data, extracting the relevant patient prescription information and converting it into the selected drug representation for our research, the extended-connectivity fingerprint (ECFP). Furthermore, we adopt a unique multimodal approach, developing machine learning models and 1D Convolutional Neural Networks (CNN) applied to clinical drug representations, establishing a procedure which has not been used by any previous studies predicting AKI. The findings showcase a notable improvement in AKI prediction through the integration of drug embeddings and other patient cohort features. By using drug features represented as ECFP molecular fingerprints along with common cohort features such as demographics and lab test values, we achieved a considerable improvement in model performance for the AKI prediction task over the baseline model which does not include the drug representations as features, indicating that our distinct approach enhances existing baseline techniques and highlights the relevance of drug data in predicting AKI in the ICU setting","sentences":["The relationship between acute kidney injury (AKI) prediction and nephrotoxic drugs, or drugs that adversely affect kidney function, is one that has yet to be explored in the critical care setting.","One contributing factor to this gap in research is the limited investigation of drug modalities in the intensive care unit (ICU) context, due to the challenges of processing prescription data into the corresponding drug representations and a lack in the comprehensive understanding of these drug representations.","This study addresses this gap by proposing a novel approach that leverages patient prescription data as a modality to improve existing models for AKI prediction.","We base our research on Electronic Health Record (EHR) data, extracting the relevant patient prescription information and converting it into the selected drug representation for our research, the extended-connectivity fingerprint (ECFP).","Furthermore, we adopt a unique multimodal approach, developing machine learning models and 1D Convolutional Neural Networks (CNN) applied to clinical drug representations, establishing a procedure which has not been used by any previous studies predicting AKI.","The findings showcase a notable improvement in AKI prediction through the integration of drug embeddings and other patient cohort features.","By using drug features represented as ECFP molecular fingerprints along with common cohort features such as demographics and lab test values, we achieved a considerable improvement in model performance for the AKI prediction task over the baseline model which does not include the drug representations as features, indicating that our distinct approach enhances existing baseline techniques and highlights the relevance of drug data in predicting AKI in the ICU setting"],"url":"http://arxiv.org/abs/2401.04368v1"}
{"created":"2024-01-09 04:44:41","title":"Message-Passing Receiver for OCDM over Multi-Lag Multi-Doppler Channels","abstract":"As a new candidate waveform for the next generation wireless communications, orthogonal chirp division multiplexing (OCDM) has attracted growing attention for its ability to achieve full diversity in uncoded transmission, and its robustness to narrow-band interference or impulsive noise. Under high mobility channels with multiple lags and multiple Doppler-shifts (MLMD), the signal suffers doubly selective (DS) fadings in time and frequency domain, and data symbols modulated on orthogonal chirps are interfered by each other. To address the problem of symbol detection of OCDM over MLMD channel, under the assumption that path attenuation factors, delays, and Doppler shifts of the channel are available, we first derive the closed-form channel matrix in Fresnel domain, and then propose a low-complexity method to approximate it as a sparse matrix. Based on the approximated Fresnel-domain channel, we propose a message passing (MP) based detector to estimate the transmit symbols iteratively. Finally, under two MLMD channels (an underspread channel for terrestrial vehicular communication, and an overspread channel for narrow-band underwater acoustic communications), Monte Carlo simulation results and analysis are provided to validate its advantages as a promising detector for OCDM.","sentences":["As a new candidate waveform for the next generation wireless communications, orthogonal chirp division multiplexing (OCDM) has attracted growing attention for its ability to achieve full diversity in uncoded transmission, and its robustness to narrow-band interference or impulsive noise.","Under high mobility channels with multiple lags and multiple Doppler-shifts (MLMD), the signal suffers doubly selective (DS) fadings in time and frequency domain, and data symbols modulated on orthogonal chirps are interfered by each other.","To address the problem of symbol detection of OCDM over MLMD channel, under the assumption that path attenuation factors, delays, and Doppler shifts of the channel are available, we first derive the closed-form channel matrix in Fresnel domain, and then propose a low-complexity method to approximate it as a sparse matrix.","Based on the approximated Fresnel-domain channel, we propose a message passing (MP) based detector to estimate the transmit symbols iteratively.","Finally, under two MLMD channels (an underspread channel for terrestrial vehicular communication, and an overspread channel for narrow-band underwater acoustic communications), Monte Carlo simulation results and analysis are provided to validate its advantages as a promising detector for OCDM."],"url":"http://arxiv.org/abs/2401.04358v1"}
{"created":"2024-01-09 04:37:10","title":"Knowledge-enhanced Multi-perspective Video Representation Learning for Scene Recognition","abstract":"With the explosive growth of video data in real-world applications, a comprehensive representation of videos becomes increasingly important. In this paper, we address the problem of video scene recognition, whose goal is to learn a high-level video representation to classify scenes in videos. Due to the diversity and complexity of video contents in realistic scenarios, this task remains a challenge. Most existing works identify scenes for videos only from visual or textual information in a temporal perspective, ignoring the valuable information hidden in single frames, while several earlier studies only recognize scenes for separate images in a non-temporal perspective. We argue that these two perspectives are both meaningful for this task and complementary to each other, meanwhile, externally introduced knowledge can also promote the comprehension of videos. We propose a novel two-stream framework to model video representations from multiple perspectives, i.e. temporal and non-temporal perspectives, and integrate the two perspectives in an end-to-end manner by self-distillation. Besides, we design a knowledge-enhanced feature fusion and label prediction method that contributes to naturally introducing knowledge into the task of video scene recognition. Experiments conducted on a real-world dataset demonstrate the effectiveness of our proposed method.","sentences":["With the explosive growth of video data in real-world applications, a comprehensive representation of videos becomes increasingly important.","In this paper, we address the problem of video scene recognition, whose goal is to learn a high-level video representation to classify scenes in videos.","Due to the diversity and complexity of video contents in realistic scenarios, this task remains a challenge.","Most existing works identify scenes for videos only from visual or textual information in a temporal perspective, ignoring the valuable information hidden in single frames, while several earlier studies only recognize scenes for separate images in a non-temporal perspective.","We argue that these two perspectives are both meaningful for this task and complementary to each other, meanwhile, externally introduced knowledge can also promote the comprehension of videos.","We propose a novel two-stream framework to model video representations from multiple perspectives, i.e. temporal and non-temporal perspectives, and integrate the two perspectives in an end-to-end manner by self-distillation.","Besides, we design a knowledge-enhanced feature fusion and label prediction method that contributes to naturally introducing knowledge into the task of video scene recognition.","Experiments conducted on a real-world dataset demonstrate the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2401.04354v1"}
{"created":"2024-01-09 04:35:17","title":"A Change Point Detection Integrated Remaining Useful Life Estimation Model under Variable Operating Conditions","abstract":"By informing the onset of the degradation process, health status evaluation serves as a significant preliminary step for reliable remaining useful life (RUL) estimation of complex equipment. This paper proposes a novel temporal dynamics learning-based model for detecting change points of individual devices, even under variable operating conditions, and utilises the learnt change points to improve the RUL estimation accuracy. During offline model development, the multivariate sensor data are decomposed to learn fused temporal correlation features that are generalisable and representative of normal operation dynamics across multiple operating conditions. Monitoring statistics and control limit thresholds for normal behaviour are dynamically constructed from these learnt temporal features for the unsupervised detection of device-level change points. The detected change points then inform the degradation data labelling for training a long short-term memory (LSTM)-based RUL estimation model. During online monitoring, the temporal correlation dynamics of a query device is monitored for breach of the control limit derived in offline training. If a change point is detected, the device's RUL is estimated with the well-trained offline model for early preventive action. Using C-MAPSS turbofan engines as the case study, the proposed method improved the accuracy by 5.6\\% and 7.5\\% for two scenarios with six operating conditions, when compared to existing LSTM-based RUL estimation models that do not consider heterogeneous change points.","sentences":["By informing the onset of the degradation process, health status evaluation serves as a significant preliminary step for reliable remaining useful life (RUL) estimation of complex equipment.","This paper proposes a novel temporal dynamics learning-based model for detecting change points of individual devices, even under variable operating conditions, and utilises the learnt change points to improve the RUL estimation accuracy.","During offline model development, the multivariate sensor data are decomposed to learn fused temporal correlation features that are generalisable and representative of normal operation dynamics across multiple operating conditions.","Monitoring statistics and control limit thresholds for normal behaviour are dynamically constructed from these learnt temporal features for the unsupervised detection of device-level change points.","The detected change points then inform the degradation data labelling for training a long short-term memory (LSTM)-based RUL estimation model.","During online monitoring, the temporal correlation dynamics of a query device is monitored for breach of the control limit derived in offline training.","If a change point is detected, the device's RUL is estimated with the well-trained offline model for early preventive action.","Using C-MAPSS turbofan engines as the case study, the proposed method improved the accuracy by 5.6\\% and 7.5\\% for two scenarios with six operating conditions, when compared to existing LSTM-based RUL estimation models that do not consider heterogeneous change points."],"url":"http://arxiv.org/abs/2401.04351v1"}
{"created":"2024-01-09 04:19:16","title":"LAMPAT: Low-Rank Adaption for Multilingual Paraphrasing Using Adversarial Training","abstract":"Paraphrases are texts that convey the same meaning while using different words or sentence structures. It can be used as an automatic data augmentation tool for many Natural Language Processing tasks, especially when dealing with low-resource languages, where data shortage is a significant problem. To generate a paraphrase in multilingual settings, previous studies have leveraged the knowledge from the machine translation field, i.e., forming a paraphrase through zero-shot machine translation in the same language. Despite good performance on human evaluation, those methods still require parallel translation datasets, thus making them inapplicable to languages that do not have parallel corpora. To mitigate that problem, we proposed the first unsupervised multilingual paraphrasing model, LAMPAT ($\\textbf{L}$ow-rank $\\textbf{A}$daptation for $\\textbf{M}$ultilingual $\\textbf{P}$araphrasing using $\\textbf{A}$dversarial $\\textbf{T}$raining), by which monolingual dataset is sufficient enough to generate a human-like and diverse sentence. Throughout the experiments, we found out that our method not only works well for English but can generalize on unseen languages as well. Data and code are available at https://github.com/phkhanhtrinh23/LAMPAT.","sentences":["Paraphrases are texts that convey the same meaning while using different words or sentence structures.","It can be used as an automatic data augmentation tool for many Natural Language Processing tasks, especially when dealing with low-resource languages, where data shortage is a significant problem.","To generate a paraphrase in multilingual settings, previous studies have leveraged the knowledge from the machine translation field, i.e., forming a paraphrase through zero-shot machine translation in the same language.","Despite good performance on human evaluation, those methods still require parallel translation datasets, thus making them inapplicable to languages that do not have parallel corpora.","To mitigate that problem, we proposed the first unsupervised multilingual paraphrasing model, LAMPAT ($\\textbf{L}$ow-rank $\\textbf{A}$daptation for $\\textbf{M}$ultilingual $\\textbf{P}$araphrasing using $\\textbf{A}$dversarial $\\textbf{T}$raining), by which monolingual dataset is sufficient enough to generate a human-like and diverse sentence.","Throughout the experiments, we found out that our method not only works well for English but can generalize on unseen languages as well.","Data and code are available at https://github.com/phkhanhtrinh23/LAMPAT."],"url":"http://arxiv.org/abs/2401.04348v1"}
{"created":"2024-01-09 03:53:59","title":"Private Fine-tuning of Large Language Models with Zeroth-order Optimization","abstract":"Fine-tuning large pretrained models on private datasets may run the risk of violating privacy. Differential privacy is a framework for mitigating privacy risks by enforcing algorithmic stability. DP-SGD enables training models with private data in a privacy-preserving manner, but raises new obstacles in the form of performance loss and significant engineering challenges. We introduce DP-ZO, a new method for fine-tuning large language models that preserves the privacy of training data by privatizing zeroth-order optimization. A key insight into the design of our method is that the direction of the gradient in SPSA, the zeroth-order algorithm we use, is always random and the only information that depends on private data is the step size, i.e., a scalar. Therefore, we only need to privatize the scalar step size, which is memory-efficient. DP-ZO, which can be instantiated with either Laplace or Gaussian noise, provides a strong privacy-utility trade-off across different tasks, and model sizes, under conservative privacy budgets. One noteworthy result is that DP-ZO exhibits just $1.86\\%$ performance degradation due to privacy at $(1,10^{-5})$-DP when fine-tuning OPT-66B on 1000 training samples from SQuAD.","sentences":["Fine-tuning large pretrained models on private datasets may run the risk of violating privacy.","Differential privacy is a framework for mitigating privacy risks by enforcing algorithmic stability.","DP-SGD enables training models with private data in a privacy-preserving manner, but raises new obstacles in the form of performance loss and significant engineering challenges.","We introduce DP-ZO, a new method for fine-tuning large language models that preserves the privacy of training data by privatizing zeroth-order optimization.","A key insight into the design of our method is that the direction of the gradient in SPSA, the zeroth-order algorithm we use, is always random and the only information that depends on private data is the step size, i.e., a scalar.","Therefore, we only need to privatize the scalar step size, which is memory-efficient.","DP-ZO, which can be instantiated with either Laplace or Gaussian noise, provides a strong privacy-utility trade-off across different tasks, and model sizes, under conservative privacy budgets.","One noteworthy result is that DP-ZO exhibits just $1.86\\%$ performance degradation due to privacy at $(1,10^{-5})$-DP when fine-tuning OPT-66B on 1000 training samples from SQuAD."],"url":"http://arxiv.org/abs/2401.04343v1"}
{"created":"2024-01-09 03:35:43","title":"G-Meta: Distributed Meta Learning in GPU Clusters for Large-Scale Recommender Systems","abstract":"Recently, a new paradigm, meta learning, has been widely applied to Deep Learning Recommendation Models (DLRM) and significantly improves statistical performance, especially in cold-start scenarios. However, the existing systems are not tailored for meta learning based DLRM models and have critical problems regarding efficiency in distributed training in the GPU cluster. It is because the conventional deep learning pipeline is not optimized for two task-specific datasets and two update loops in meta learning. This paper provides a high-performance framework for large-scale training for Optimization-based Meta DLRM models over the \\textbf{G}PU cluster, namely \\textbf{G}-Meta. Firstly, G-Meta utilizes both data parallelism and model parallelism with careful orchestration regarding computation and communication efficiency, to enable high-speed distributed training. Secondly, it proposes a Meta-IO pipeline for efficient data ingestion to alleviate the I/O bottleneck. Various experimental results show that G-Meta achieves notable training speed without loss of statistical performance. Since early 2022, G-Meta has been deployed in Alipay's core advertising and recommender system, shrinking the continuous delivery of models by four times. It also obtains 6.48\\% improvement in Conversion Rate (CVR) and 1.06\\% increase in CPM (Cost Per Mille) in Alipay's homepage display advertising, with the benefit of larger training samples and tasks.","sentences":["Recently, a new paradigm, meta learning, has been widely applied to Deep Learning Recommendation Models (DLRM) and significantly improves statistical performance, especially in cold-start scenarios.","However, the existing systems are not tailored for meta learning based DLRM models and have critical problems regarding efficiency in distributed training in the GPU cluster.","It is because the conventional deep learning pipeline is not optimized for two task-specific datasets and two update loops in meta learning.","This paper provides a high-performance framework for large-scale training for Optimization-based Meta DLRM models over the \\textbf{G}PU cluster, namely \\textbf{G}-Meta.","Firstly, G-Meta utilizes both data parallelism and model parallelism with careful orchestration regarding computation and communication efficiency, to enable high-speed distributed training.","Secondly, it proposes a Meta-IO pipeline for efficient data ingestion to alleviate the I/O bottleneck.","Various experimental results show that G-Meta achieves notable training speed without loss of statistical performance.","Since early 2022, G-Meta has been deployed in Alipay's core advertising and recommender system, shrinking the continuous delivery of models by four times.","It also obtains 6.48\\% improvement in Conversion Rate (CVR) and 1.06\\% increase in CPM (Cost Per Mille) in Alipay's homepage display advertising, with the benefit of larger training samples and tasks."],"url":"http://arxiv.org/abs/2401.04338v1"}
{"created":"2024-01-09 03:29:40","title":"Deep Efficient Private Neighbor Generation for Subgraph Federated Learning","abstract":"Behemoth graphs are often fragmented and separately stored by multiple data owners as distributed subgraphs in many realistic applications. Without harming data privacy, it is natural to consider the subgraph federated learning (subgraph FL) scenario, where each local client holds a subgraph of the entire global graph, to obtain globally generalized graph mining models. To overcome the unique challenge of incomplete information propagation on local subgraphs due to missing cross-subgraph neighbors, previous works resort to the augmentation of local neighborhoods through the joint FL of missing neighbor generators and GNNs. Yet their technical designs have profound limitations regarding the utility, efficiency, and privacy goals of FL. In this work, we propose FedDEP to comprehensively tackle these challenges in subgraph FL. FedDEP consists of a series of novel technical designs: (1) Deep neighbor generation through leveraging the GNN embeddings of potential missing neighbors; (2) Efficient pseudo-FL for neighbor generation through embedding prototyping; and (3) Privacy protection through noise-less edge-local-differential-privacy.   We analyze the correctness and efficiency of FedDEP, and provide theoretical guarantees on its privacy.   Empirical results on four real-world datasets justify the clear benefits of proposed techniques.","sentences":["Behemoth graphs are often fragmented and separately stored by multiple data owners as distributed subgraphs in many realistic applications.","Without harming data privacy, it is natural to consider the subgraph federated learning (subgraph FL) scenario, where each local client holds a subgraph of the entire global graph, to obtain globally generalized graph mining models.","To overcome the unique challenge of incomplete information propagation on local subgraphs due to missing cross-subgraph neighbors, previous works resort to the augmentation of local neighborhoods through the joint FL of missing neighbor generators and GNNs.","Yet their technical designs have profound limitations regarding the utility, efficiency, and privacy goals of FL.","In this work, we propose FedDEP to comprehensively tackle these challenges in subgraph FL.","FedDEP consists of a series of novel technical designs: (1) Deep neighbor generation through leveraging the GNN embeddings of potential missing neighbors; (2) Efficient pseudo-FL for neighbor generation through embedding prototyping; and (3) Privacy protection through noise-less edge-local-differential-privacy.   ","We analyze the correctness and efficiency of FedDEP, and provide theoretical guarantees on its privacy.   ","Empirical results on four real-world datasets justify the clear benefits of proposed techniques."],"url":"http://arxiv.org/abs/2401.04336v1"}
{"created":"2024-01-09 03:05:53","title":"Mix-GENEO: A flexible filtration for multiparameter persistent homology detects digital images","abstract":"Two important problems in the field of Topological Data Analysis are defining practical multifiltrations on objects and showing ability of TDA to detect the geometry. Motivated by the problems, we constuct three multifiltrations named multi-GENEO, multi-DGENEO and mix-GENEO, and prove the stability of both the interleaving distance and multiparameter persistence landscape of multi-GENEO with respect to the pseudometric of the subspace of bounded functions. We also give the estimations of upper bound for multi-DGENEO and mix-GENEO. Finally, we provide experiment results on MNIST dataset to demonstrate our bifiltrations have ability to detect geometric and topological differences of digital images.","sentences":["Two important problems in the field of Topological Data Analysis are defining practical multifiltrations on objects and showing ability of TDA to detect the geometry.","Motivated by the problems, we constuct three multifiltrations named multi-GENEO, multi-DGENEO and mix-GENEO, and prove the stability of both the interleaving distance and multiparameter persistence landscape of multi-GENEO with respect to the pseudometric of the subspace of bounded functions.","We also give the estimations of upper bound for multi-DGENEO and mix-GENEO.","Finally, we provide experiment results on MNIST dataset to demonstrate our bifiltrations have ability to detect geometric and topological differences of digital images."],"url":"http://arxiv.org/abs/2401.04332v1"}
{"created":"2024-01-09 02:40:03","title":"RadarCam-Depth: Radar-Camera Fusion for Depth Estimation with Learned Metric Scale","abstract":"We present a novel approach for metric dense depth estimation based on the fusion of a single-view image and a sparse, noisy Radar point cloud. The direct fusion of heterogeneous Radar and image data, or their encodings, tends to yield dense depth maps with significant artifacts, blurred boundaries, and suboptimal accuracy. To circumvent this issue, we learn to augment versatile and robust monocular depth prediction with the dense metric scale induced from sparse and noisy Radar data. We propose a Radar-Camera framework for highly accurate and fine-detailed dense depth estimation with four stages, including monocular depth prediction, global scale alignment of monocular depth with sparse Radar points, quasi-dense scale estimation through learning the association between Radar points and image patches, and local scale refinement of dense depth using a scale map learner. Our proposed method significantly outperforms the state-of-the-art Radar-Camera depth estimation methods by reducing the mean absolute error (MAE) of depth estimation by 25.6% and 40.2% on the challenging nuScenes dataset and our self-collected ZJU-4DRadarCam dataset, respectively.","sentences":["We present a novel approach for metric dense depth estimation based on the fusion of a single-view image and a sparse, noisy Radar point cloud.","The direct fusion of heterogeneous Radar and image data, or their encodings, tends to yield dense depth maps with significant artifacts, blurred boundaries, and suboptimal accuracy.","To circumvent this issue, we learn to augment versatile and robust monocular depth prediction with the dense metric scale induced from sparse and noisy Radar data.","We propose a Radar-Camera framework for highly accurate and fine-detailed dense depth estimation with four stages, including monocular depth prediction, global scale alignment of monocular depth with sparse Radar points, quasi-dense scale estimation through learning the association between Radar points and image patches, and local scale refinement of dense depth using a scale map learner.","Our proposed method significantly outperforms the state-of-the-art Radar-Camera depth estimation methods by reducing the mean absolute error (MAE) of depth estimation by 25.6% and 40.2% on the challenging nuScenes dataset and our self-collected ZJU-4DRadarCam dataset, respectively."],"url":"http://arxiv.org/abs/2401.04325v1"}
{"created":"2024-01-09 02:37:40","title":"Divergent Characteristics of Biomedical Research across Publication Types: A Quantitative Analysis on the Aging-related Research","abstract":"This paper investigates differences in characteristics across publication types for aging-related genetic research. We utilized bibliometric data for five model species retrieved from authoritative databases including PubMed. Publications are classified into types according to PubMed. Results indicate substantial divergence across publication types in attention paid to aging-related research, scopes of studied genes, and topical preferences. For instance, comparative studies and meta-analyses show a greater focus on aging than validation studies. Reviews concentrate more on cell biology while clinical studies emphasize translational topics. Publication types also manifest variations in highly studied genes, like APOE for reviews versus GH1 for clinical studies. Despite differences, top genes like insulin are universally emphasized. Publication types demonstrate similar levels of imbalance in research efforts to genes. Differences also exist in bibliometrics like authorship numbers, citation counts, etc. Publication types show distinct preferences for journals of certain topical specialties and scope of readership. Overall, findings showcase distinct characteristics of publication types in studying aging-related genetics, owing to their unique nature and objectives. This study is the first endeavor to systematically depict the inherent structure of a biomedical research field from the perspective of publication types and provides insights into knowledge production and evaluation patterns across biomedical communities.","sentences":["This paper investigates differences in characteristics across publication types for aging-related genetic research.","We utilized bibliometric data for five model species retrieved from authoritative databases including PubMed.","Publications are classified into types according to PubMed.","Results indicate substantial divergence across publication types in attention paid to aging-related research, scopes of studied genes, and topical preferences.","For instance, comparative studies and meta-analyses show a greater focus on aging than validation studies.","Reviews concentrate more on cell biology while clinical studies emphasize translational topics.","Publication types also manifest variations in highly studied genes, like APOE for reviews versus GH1 for clinical studies.","Despite differences, top genes like insulin are universally emphasized.","Publication types demonstrate similar levels of imbalance in research efforts to genes.","Differences also exist in bibliometrics like authorship numbers, citation counts, etc.","Publication types show distinct preferences for journals of certain topical specialties and scope of readership.","Overall, findings showcase distinct characteristics of publication types in studying aging-related genetics, owing to their unique nature and objectives.","This study is the first endeavor to systematically depict the inherent structure of a biomedical research field from the perspective of publication types and provides insights into knowledge production and evaluation patterns across biomedical communities."],"url":"http://arxiv.org/abs/2401.04323v1"}
{"created":"2024-01-09 02:20:30","title":"Vision Reimagined: AI-Powered Breakthroughs in WiFi Indoor Imaging","abstract":"Indoor imaging is a critical task for robotics and internet-of-things. WiFi as an omnipresent signal is a promising candidate for carrying out passive imaging and synchronizing the up-to-date information to all connected devices. This is the first research work to consider WiFi indoor imaging as a multi-modal image generation task that converts the measured WiFi power into a high-resolution indoor image. Our proposed WiFi-GEN network achieves a shape reconstruction accuracy that is 275% of that achieved by physical model-based inversion methods. Additionally, the Frechet Inception Distance score has been significantly reduced by 82%. To examine the effectiveness of models for this task, the first large-scale dataset is released containing 80,000 pairs of WiFi signal and imaging target. Our model absorbs challenges for the model-based methods including the non-linearity, ill-posedness and non-certainty into massive parameters of our generative AI network. The network is also designed to best fit measured WiFi signals and the desired imaging output. For reproducibility, we will release the data and code upon acceptance.","sentences":["Indoor imaging is a critical task for robotics and internet-of-things.","WiFi as an omnipresent signal is a promising candidate for carrying out passive imaging and synchronizing the up-to-date information to all connected devices.","This is the first research work to consider WiFi indoor imaging as a multi-modal image generation task that converts the measured WiFi power into a high-resolution indoor image.","Our proposed WiFi-GEN network achieves a shape reconstruction accuracy that is 275% of that achieved by physical model-based inversion methods.","Additionally, the Frechet Inception Distance score has been significantly reduced by 82%.","To examine the effectiveness of models for this task, the first large-scale dataset is released containing 80,000 pairs of WiFi signal and imaging target.","Our model absorbs challenges for the model-based methods including the non-linearity, ill-posedness and non-certainty into massive parameters of our generative AI network.","The network is also designed to best fit measured WiFi signals and the desired imaging output.","For reproducibility, we will release the data and code upon acceptance."],"url":"http://arxiv.org/abs/2401.04317v1"}
{"created":"2024-01-09 01:41:36","title":"Advancing Deep Active Learning & Data Subset Selection: Unifying Principles with Information-Theory Intuitions","abstract":"At its core, this thesis aims to enhance the practicality of deep learning by improving the label and training efficiency of deep learning models. To this end, we investigate data subset selection techniques, specifically active learning and active sampling, grounded in information-theoretic principles. Active learning improves label efficiency, while active sampling enhances training efficiency. Supervised deep learning models often require extensive training with labeled data. Label acquisition can be expensive and time-consuming, and training large models is resource-intensive, hindering the adoption outside academic research and ``big tech.'' Existing methods for data subset selection in deep learning often rely on heuristics or lack a principled information-theoretic foundation. In contrast, this thesis examines several objectives for data subset selection and their applications within deep learning, striving for a more principled approach inspired by information theory. We begin by disentangling epistemic and aleatoric uncertainty in single forward-pass deep neural networks, which provides helpful intuitions and insights into different forms of uncertainty and their relevance for data subset selection. We then propose and investigate various approaches for active learning and data subset selection in (Bayesian) deep learning. Finally, we relate various existing and proposed approaches to approximations of information quantities in weight or prediction space. Underpinning this work is a principled and practical notation for information-theoretic quantities that includes both random variables and observed outcomes. This thesis demonstrates the benefits of working from a unified perspective and highlights the potential impact of our contributions to the practical application of deep learning.","sentences":["At its core, this thesis aims to enhance the practicality of deep learning by improving the label and training efficiency of deep learning models.","To this end, we investigate data subset selection techniques, specifically active learning and active sampling, grounded in information-theoretic principles.","Active learning improves label efficiency, while active sampling enhances training efficiency.","Supervised deep learning models often require extensive training with labeled data.","Label acquisition can be expensive and time-consuming, and training large models is resource-intensive, hindering the adoption outside academic research and ``big tech.''","Existing methods for data subset selection in deep learning often rely on heuristics or lack a principled information-theoretic foundation.","In contrast, this thesis examines several objectives for data subset selection and their applications within deep learning, striving for a more principled approach inspired by information theory.","We begin by disentangling epistemic and aleatoric uncertainty in single forward-pass deep neural networks, which provides helpful intuitions and insights into different forms of uncertainty and their relevance for data subset selection.","We then propose and investigate various approaches for active learning and data subset selection in (Bayesian) deep learning.","Finally, we relate various existing and proposed approaches to approximations of information quantities in weight or prediction space.","Underpinning this work is a principled and practical notation for information-theoretic quantities that includes both random variables and observed outcomes.","This thesis demonstrates the benefits of working from a unified perspective and highlights the potential impact of our contributions to the practical application of deep learning."],"url":"http://arxiv.org/abs/2401.04305v1"}
{"created":"2024-01-09 01:19:03","title":"Setting the Record Straight on Transformer Oversmoothing","abstract":"Transformer-based models have recently become wildly successful across a diverse set of domains. At the same time, recent work has shown that Transformers are inherently low-pass filters that gradually oversmooth the inputs, reducing the expressivity of their representations. A natural question is: How can Transformers achieve these successes given this shortcoming? In this work we show that in fact Transformers are not inherently low-pass filters. Instead, whether Transformers oversmooth or not depends on the eigenspectrum of their update equations. Our analysis extends prior work in oversmoothing and in the closely-related phenomenon of rank collapse. We show that many successful Transformer models have attention and weights which satisfy conditions that avoid oversmoothing. Based on this analysis, we derive a simple way to parameterize the weights of the Transformer update equations that allows for control over its spectrum, ensuring that oversmoothing does not occur. Compared to a recent solution for oversmoothing, our approach improves generalization, even when training with more layers, fewer datapoints, and data that is corrupted.","sentences":["Transformer-based models have recently become wildly successful across a diverse set of domains.","At the same time, recent work has shown that Transformers are inherently low-pass filters that gradually oversmooth the inputs, reducing the expressivity of their representations.","A natural question is: How can Transformers achieve these successes given this shortcoming?","In this work we show that in fact Transformers are not inherently low-pass filters.","Instead, whether Transformers oversmooth or not depends on the eigenspectrum of their update equations.","Our analysis extends prior work in oversmoothing and in the closely-related phenomenon of rank collapse.","We show that many successful Transformer models have attention and weights which satisfy conditions that avoid oversmoothing.","Based on this analysis, we derive a simple way to parameterize the weights of the Transformer update equations that allows for control over its spectrum, ensuring that oversmoothing does not occur.","Compared to a recent solution for oversmoothing, our approach improves generalization, even when training with more layers, fewer datapoints, and data that is corrupted."],"url":"http://arxiv.org/abs/2401.04301v1"}
{"created":"2024-01-09 00:05:56","title":"StarCraftImage: A Dataset For Prototyping Spatial Reasoning Methods For Multi-Agent Environments","abstract":"Spatial reasoning tasks in multi-agent environments such as event prediction, agent type identification, or missing data imputation are important for multiple applications (e.g., autonomous surveillance over sensor networks and subtasks for reinforcement learning (RL)). StarCraft II game replays encode intelligent (and adversarial) multi-agent behavior and could provide a testbed for these tasks; however, extracting simple and standardized representations for prototyping these tasks is laborious and hinders reproducibility. In contrast, MNIST and CIFAR10, despite their extreme simplicity, have enabled rapid prototyping and reproducibility of ML methods. Following the simplicity of these datasets, we construct a benchmark spatial reasoning dataset based on StarCraft II replays that exhibit complex multi-agent behaviors, while still being as easy to use as MNIST and CIFAR10. Specifically, we carefully summarize a window of 255 consecutive game states to create 3.6 million summary images from 60,000 replays, including all relevant metadata such as game outcome and player races. We develop three formats of decreasing complexity: Hyperspectral images that include one channel for every unit type (similar to multispectral geospatial images), RGB images that mimic CIFAR10, and grayscale images that mimic MNIST. We show how this dataset can be used for prototyping spatial reasoning methods. All datasets, code for extraction, and code for dataset loading can be found at https://starcraftdata.davidinouye.com","sentences":["Spatial reasoning tasks in multi-agent environments such as event prediction, agent type identification, or missing data imputation are important for multiple applications (e.g., autonomous surveillance over sensor networks and subtasks for reinforcement learning (RL)).","StarCraft II game replays encode intelligent (and adversarial) multi-agent behavior and could provide a testbed for these tasks; however, extracting simple and standardized representations for prototyping these tasks is laborious and hinders reproducibility.","In contrast, MNIST and CIFAR10, despite their extreme simplicity, have enabled rapid prototyping and reproducibility of ML methods.","Following the simplicity of these datasets, we construct a benchmark spatial reasoning dataset based on StarCraft II replays that exhibit complex multi-agent behaviors, while still being as easy to use as MNIST and CIFAR10.","Specifically, we carefully summarize a window of 255 consecutive game states to create 3.6 million summary images from 60,000 replays, including all relevant metadata such as game outcome and player races.","We develop three formats of decreasing complexity: Hyperspectral images that include one channel for every unit type (similar to multispectral geospatial images), RGB images that mimic CIFAR10, and grayscale images that mimic MNIST.","We show how this dataset can be used for prototyping spatial reasoning methods.","All datasets, code for extraction, and code for dataset loading can be found at https://starcraftdata.davidinouye.com"],"url":"http://arxiv.org/abs/2401.04290v1"}
{"created":"2024-01-08 23:36:25","title":"A Fast Graph Search Algorithm with Dynamic Optimization and Reduced Histogram for Discrimination of Binary Classification Problem","abstract":"This study develops a graph search algorithm to find the optimal discrimination path for the binary classification problem. The objective function is defined as the difference of variations between the true positive (TP) and false positive (FP). It uses the depth first search (DFS) algorithm to find the top-down paths for discrimination. It proposes a dynamic optimization procedure to optimize TP at the upper levels and then reduce FP at the lower levels. To accelerate computing speed with improving accuracy, it proposes a reduced histogram algorithm with variable bin size instead of looping over all data points, to find the feature threshold of discrimination. The algorithm is applied on top of a Support Vector Machine (SVM) model for a binary classification problem on whether a person is fit or unfit. It significantly improves TP and reduces FP of the SVM results (e.g., reduced FP by 90% with a loss of only\\ 5% TP). The graph search auto-generates 39 ranked discrimination paths within 9 seconds on an input of total 328,464 objects, using a dual-core Laptop computer with a processor of 2.59 GHz.","sentences":["This study develops a graph search algorithm to find the optimal discrimination path for the binary classification problem.","The objective function is defined as the difference of variations between the true positive (TP) and false positive (FP).","It uses the depth first search (DFS) algorithm to find the top-down paths for discrimination.","It proposes a dynamic optimization procedure to optimize TP at the upper levels and then reduce FP at the lower levels.","To accelerate computing speed with improving accuracy, it proposes a reduced histogram algorithm with variable bin size instead of looping over all data points, to find the feature threshold of discrimination.","The algorithm is applied on top of a Support Vector Machine (SVM) model for a binary classification problem on whether a person is fit or unfit.","It significantly improves TP and reduces FP of the SVM results (e.g., reduced FP by 90% with a loss of only\\ 5% TP).","The graph search auto-generates 39 ranked discrimination paths within 9 seconds on an input of total 328,464 objects, using a dual-core Laptop computer with a processor of 2.59 GHz."],"url":"http://arxiv.org/abs/2401.04282v1"}
{"created":"2024-01-08 22:36:05","title":"Attention versus Contrastive Learning of Tabular Data -- A Data-centric Benchmarking","abstract":"Despite groundbreaking success in image and text learning, deep learning has not achieved significant improvements against traditional machine learning (ML) when it comes to tabular data. This performance gap underscores the need for data-centric treatment and benchmarking of learning algorithms. Recently, attention and contrastive learning breakthroughs have shifted computer vision and natural language processing paradigms. However, the effectiveness of these advanced deep models on tabular data is sparsely studied using a few data sets with very large sample sizes, reporting mixed findings after benchmarking against a limited number of baselines. We argue that the heterogeneity of tabular data sets and selective baselines in the literature can bias the benchmarking outcomes. This article extensively evaluates state-of-the-art attention and contrastive learning methods on a wide selection of 28 tabular data sets (14 easy and 14 hard-to-classify) against traditional deep and machine learning. Our data-centric benchmarking demonstrates when traditional ML is preferred over deep learning and vice versa because no best learning method exists for all tabular data sets. Combining between-sample and between-feature attentions conquers the invincible traditional ML on tabular data sets by a significant margin but fails on high dimensional data, where contrastive learning takes a robust lead. While a hybrid attention-contrastive learning strategy mostly wins on hard-to-classify data sets, traditional methods are frequently superior on easy-to-classify data sets with presumably simpler decision boundaries. To the best of our knowledge, this is the first benchmarking paper with statistical analyses of attention and contrastive learning performances on a diverse selection of tabular data sets against traditional deep and machine learning baselines to facilitate further advances in this field.","sentences":["Despite groundbreaking success in image and text learning, deep learning has not achieved significant improvements against traditional machine learning (ML) when it comes to tabular data.","This performance gap underscores the need for data-centric treatment and benchmarking of learning algorithms.","Recently, attention and contrastive learning breakthroughs have shifted computer vision and natural language processing paradigms.","However, the effectiveness of these advanced deep models on tabular data is sparsely studied using a few data sets with very large sample sizes, reporting mixed findings after benchmarking against a limited number of baselines.","We argue that the heterogeneity of tabular data sets and selective baselines in the literature can bias the benchmarking outcomes.","This article extensively evaluates state-of-the-art attention and contrastive learning methods on a wide selection of 28 tabular data sets (14 easy and 14 hard-to-classify) against traditional deep and machine learning.","Our data-centric benchmarking demonstrates when traditional ML is preferred over deep learning and vice versa because no best learning method exists for all tabular data sets.","Combining between-sample and between-feature attentions conquers the invincible traditional ML on tabular data sets by a significant margin but fails on high dimensional data, where contrastive learning takes a robust lead.","While a hybrid attention-contrastive learning strategy mostly wins on hard-to-classify data sets, traditional methods are frequently superior on easy-to-classify data sets with presumably simpler decision boundaries.","To the best of our knowledge, this is the first benchmarking paper with statistical analyses of attention and contrastive learning performances on a diverse selection of tabular data sets against traditional deep and machine learning baselines to facilitate further advances in this field."],"url":"http://arxiv.org/abs/2401.04266v1"}
{"created":"2024-01-08 22:25:07","title":"A Statically and Dynamically Scalable Soft GPGPU","abstract":"Current soft processor architectures for FPGAs do not utilize the potential of the massive parallelism available. FPGAs now support many thousands of embedded floating point operators, and have similar computational densities to GPGPUs. Several soft GPGPU or SIMT processors have been published, but the reported large areas and modest Fmax makes their widespread use unlikely for commercial designs. In this paper we take an alternative approach, building the soft GPU microarchitecture around the FPGA resource mix available. We demonstrate a statically scalable soft GPGPU processor (where both parameters and feature set can be determined at configuration time) that always closes timing at the peak speed of the slowest embedded component in the FPGA (DSP or hard memory), with a completely unconstrained compile into a current Intel Agilex FPGA. We also show dynamic scalability, where a subset of the thread space can be specified on an instruction-by-instruction basis.   For one example core type, we show a logic range -- depending on the configuration -- of 4k to 10k ALMs, along with 24 to 32 DSP Blocks, and 50 to 250 M20K memories. All of these instances close timing at 771 MHz, a performance level limited only by the DSP Blocks. We describe our methodology for reliably achieving this clock rate by matching the processor pipeline structure to the physical structure of the FPGA fabric. We also benchmark several algorithms across a range of data sizes, and compare to a commercial soft RISC processor.","sentences":["Current soft processor architectures for FPGAs do not utilize the potential of the massive parallelism available.","FPGAs now support many thousands of embedded floating point operators, and have similar computational densities to GPGPUs.","Several soft GPGPU or SIMT processors have been published, but the reported large areas and modest Fmax makes their widespread use unlikely for commercial designs.","In this paper we take an alternative approach, building the soft GPU microarchitecture around the FPGA resource mix available.","We demonstrate a statically scalable soft GPGPU processor (where both parameters and feature set can be determined at configuration time) that always closes timing at the peak speed of the slowest embedded component in the FPGA (DSP or hard memory), with a completely unconstrained compile into a current Intel Agilex FPGA.","We also show dynamic scalability, where a subset of the thread space can be specified on an instruction-by-instruction basis.   ","For one example core type, we show a logic range -- depending on the configuration -- of 4k to 10k ALMs, along with 24 to 32 DSP Blocks, and 50 to 250 M20K memories.","All of these instances close timing at 771 MHz, a performance level limited only by the DSP Blocks.","We describe our methodology for reliably achieving this clock rate by matching the processor pipeline structure to the physical structure of the FPGA fabric.","We also benchmark several algorithms across a range of data sizes, and compare to a commercial soft RISC processor."],"url":"http://arxiv.org/abs/2401.04261v1"}
{"created":"2024-01-08 21:47:35","title":"Explaining the Power of Topological Data Analysis in Graph Machine Learning","abstract":"Topological Data Analysis (TDA) has been praised by researchers for its ability to capture intricate shapes and structures within data. TDA is considered robust in handling noisy and high-dimensional datasets, and its interpretability is believed to promote an intuitive understanding of model behavior. However, claims regarding the power and usefulness of TDA have only been partially tested in application domains where TDA-based models are compared to other graph machine learning approaches, such as graph neural networks. We meticulously test claims on TDA through a comprehensive set of experiments and validate their merits. Our results affirm TDA's robustness against outliers and its interpretability, aligning with proponents' arguments. However, we find that TDA does not significantly enhance the predictive power of existing methods in our specific experiments, while incurring significant computational costs. We investigate phenomena related to graph characteristics, such as small diameters and high clustering coefficients, to mitigate the computational expenses of TDA computations. Our results offer valuable perspectives on integrating TDA into graph machine learning tasks.","sentences":["Topological Data Analysis (TDA) has been praised by researchers for its ability to capture intricate shapes and structures within data.","TDA is considered robust in handling noisy and high-dimensional datasets, and its interpretability is believed to promote an intuitive understanding of model behavior.","However, claims regarding the power and usefulness of TDA have only been partially tested in application domains where TDA-based models are compared to other graph machine learning approaches, such as graph neural networks.","We meticulously test claims on TDA through a comprehensive set of experiments and validate their merits.","Our results affirm TDA's robustness against outliers and its interpretability, aligning with proponents' arguments.","However, we find that TDA does not significantly enhance the predictive power of existing methods in our specific experiments, while incurring significant computational costs.","We investigate phenomena related to graph characteristics, such as small diameters and high clustering coefficients, to mitigate the computational expenses of TDA computations.","Our results offer valuable perspectives on integrating TDA into graph machine learning tasks."],"url":"http://arxiv.org/abs/2401.04250v1"}
{"created":"2024-01-08 21:23:23","title":"Data-Agnostic Face Image Synthesis Detection Using Bayesian CNNs","abstract":"Face image synthesis detection is considerably gaining attention because of the potential negative impact on society that this type of synthetic data brings. In this paper, we propose a data-agnostic solution to detect the face image synthesis process. Specifically, our solution is based on an anomaly detection framework that requires only real data to learn the inference process. It is therefore data-agnostic in the sense that it requires no synthetic face images. The solution uses the posterior probability with respect to the reference data to determine if new samples are synthetic or not. Our evaluation results using different synthesizers show that our solution is very competitive against the state-of-the-art, which requires synthetic data for training.","sentences":["Face image synthesis detection is considerably gaining attention because of the potential negative impact on society that this type of synthetic data brings.","In this paper, we propose a data-agnostic solution to detect the face image synthesis process.","Specifically, our solution is based on an anomaly detection framework that requires only real data to learn the inference process.","It is therefore data-agnostic in the sense that it requires no synthetic face images.","The solution uses the posterior probability with respect to the reference data to determine if new samples are synthetic or not.","Our evaluation results using different synthesizers show that our solution is very competitive against the state-of-the-art, which requires synthetic data for training."],"url":"http://arxiv.org/abs/2401.04241v1"}
{"created":"2024-01-08 20:59:56","title":"High-precision Voice Search Query Correction via Retrievable Speech-text Embedings","abstract":"Automatic speech recognition (ASR) systems can suffer from poor recall for various reasons, such as noisy audio, lack of sufficient training data, etc.   Previous work has shown that recall can be improved by retrieving rewrite candidates from a large database of likely, contextually-relevant alternatives to the hypothesis text using nearest-neighbors search over embeddings of the ASR hypothesis text to correct and candidate corrections.   However, ASR-hypothesis-based retrieval can yield poor precision if the textual hypotheses are too phonetically dissimilar to the transcript truth. In this paper, we eliminate the hypothesis-audio mismatch problem by querying the correction database directly using embeddings derived from the utterance audio; the embeddings of the utterance audio and candidate corrections are produced by multimodal speech-text embedding networks trained to place the embedding of the audio of an utterance and the embedding of its corresponding textual transcript close together.   After locating an appropriate correction candidate using nearest-neighbor search, we score the candidate with its speech-text embedding distance before adding the candidate to the original n-best list.   We show a relative word error rate (WER) reduction of 6% on utterances whose transcripts appear in the candidate set, without increasing WER on general utterances.","sentences":["Automatic speech recognition (ASR) systems can suffer from poor recall for various reasons, such as noisy audio, lack of sufficient training data, etc.   ","Previous work has shown that recall can be improved by retrieving rewrite candidates from a large database of likely, contextually-relevant alternatives to the hypothesis text using nearest-neighbors search over embeddings of the ASR hypothesis text to correct and candidate corrections.   ","However, ASR-hypothesis-based retrieval can yield poor precision if the textual hypotheses are too phonetically dissimilar to the transcript truth.","In this paper, we eliminate the hypothesis-audio mismatch problem by querying the correction database directly using embeddings derived from the utterance audio; the embeddings of the utterance audio and candidate corrections are produced by multimodal speech-text embedding networks trained to place the embedding of the audio of an utterance and the embedding of its corresponding textual transcript close together.   ","After locating an appropriate correction candidate using nearest-neighbor search, we score the candidate with its speech-text embedding distance before adding the candidate to the original n-best list.   ","We show a relative word error rate (WER) reduction of 6% on utterances whose transcripts appear in the candidate set, without increasing WER on general utterances."],"url":"http://arxiv.org/abs/2401.04235v1"}
{"created":"2024-01-08 20:25:14","title":"RaceFixer -- An Automated Data Race Fixer","abstract":"Fixing software bugs has always been an essential and time-consuming process in software development. Fixing concurrency bugs has become especially critical in the multicore era. However, fixing concurrency bugs is challenging due to non-deterministic failures and tricky parallel reasoning. Beyond correctly fixing the original problem in the software, a good patch should also avoid introducing new bugs, degrading performance unnecessarily, or damaging software readability. Existing tools cannot automate the whole fixing process and provide good-quality patches. We present RaceFixer, a tool that automates the process of fixing one common type of concurrency bug: single-variable atomicity violations. RaceFixer starts from the bug reports of an existing bug-detection tool ThreadSanitizer. It augments these with static analysis to construct a suitable patch for each bug report. It tries to combine the patches of multiple bugs for better performance and code readability. Finally, we test RaceFixer on benchmarks from TheadSanitizer.","sentences":["Fixing software bugs has always been an essential and time-consuming process in software development.","Fixing concurrency bugs has become especially critical in the multicore era.","However, fixing concurrency bugs is challenging due to non-deterministic failures and tricky parallel reasoning.","Beyond correctly fixing the original problem in the software, a good patch should also avoid introducing new bugs, degrading performance unnecessarily, or damaging software readability.","Existing tools cannot automate the whole fixing process and provide good-quality patches.","We present RaceFixer, a tool that automates the process of fixing one common type of concurrency bug: single-variable atomicity violations.","RaceFixer starts from the bug reports of an existing bug-detection tool ThreadSanitizer.","It augments these with static analysis to construct a suitable patch for each bug report.","It tries to combine the patches of multiple bugs for better performance and code readability.","Finally, we test RaceFixer on benchmarks from TheadSanitizer."],"url":"http://arxiv.org/abs/2401.04221v1"}
{"created":"2024-01-08 20:08:04","title":"Distortions in Judged Spatial Relations in Large Language Models: The Dawn of Natural Language Geographic Data?","abstract":"We present a benchmark for assessing the capability of Large Language Models (LLMs) to discern intercardinal directions between geographic locations and apply it to three prominent LLMs: GPT-3.5, GPT-4, and Llama-2. This benchmark specifically evaluates whether LLMs exhibit a hierarchical spatial bias similar to humans, where judgments about individual locations' spatial relationships are influenced by the perceived relationships of the larger groups that contain them. To investigate this, we formulated 14 questions focusing on well-known American cities. Seven questions were designed to challenge the LLMs with scenarios potentially influenced by the orientation of larger geographical units, such as states or countries, while the remaining seven targeted locations less susceptible to such hierarchical categorization. Among the tested models, GPT-4 exhibited superior performance with 55.3% accuracy, followed by GPT-3.5 at 47.3%, and Llama-2 at 44.7%. The models showed significantly reduced accuracy on tasks with suspected hierarchical bias. For example, GPT-4's accuracy dropped to 32.9% on these tasks, compared to 85.7% on others. Despite these inaccuracies, the models identified the nearest cardinal direction in most cases, suggesting associative learning, embodying human-like misconceptions. We discuss the potential of text-based data representing geographic relationships directly to improve the spatial reasoning capabilities of LLMs.","sentences":["We present a benchmark for assessing the capability of Large Language Models (LLMs) to discern intercardinal directions between geographic locations and apply it to three prominent LLMs: GPT-3.5, GPT-4, and Llama-2.","This benchmark specifically evaluates whether LLMs exhibit a hierarchical spatial bias similar to humans, where judgments about individual locations' spatial relationships are influenced by the perceived relationships of the larger groups that contain them.","To investigate this, we formulated 14 questions focusing on well-known American cities.","Seven questions were designed to challenge the LLMs with scenarios potentially influenced by the orientation of larger geographical units, such as states or countries, while the remaining seven targeted locations less susceptible to such hierarchical categorization.","Among the tested models, GPT-4 exhibited superior performance with 55.3% accuracy, followed by GPT-3.5 at 47.3%, and Llama-2 at 44.7%.","The models showed significantly reduced accuracy on tasks with suspected hierarchical bias.","For example, GPT-4's accuracy dropped to 32.9% on these tasks, compared to 85.7% on others.","Despite these inaccuracies, the models identified the nearest cardinal direction in most cases, suggesting associative learning, embodying human-like misconceptions.","We discuss the potential of text-based data representing geographic relationships directly to improve the spatial reasoning capabilities of LLMs."],"url":"http://arxiv.org/abs/2401.04218v1"}
{"created":"2024-01-08 19:39:36","title":"FunnyNet-W: Multimodal Learning of Funny Moments in Videos in the Wild","abstract":"Automatically understanding funny moments (i.e., the moments that make people laugh) when watching comedy is challenging, as they relate to various features, such as body language, dialogues and culture. In this paper, we propose FunnyNet-W, a model that relies on cross- and self-attention for visual, audio and text data to predict funny moments in videos. Unlike most methods that rely on ground truth data in the form of subtitles, in this work we exploit modalities that come naturally with videos: (a) video frames as they contain visual information indispensable for scene understanding, (b) audio as it contains higher-level cues associated with funny moments, such as intonation, pitch and pauses and (c) text automatically extracted with a speech-to-text model as it can provide rich information when processed by a Large Language Model. To acquire labels for training, we propose an unsupervised approach that spots and labels funny audio moments. We provide experiments on five datasets: the sitcoms TBBT, MHD, MUStARD, Friends, and the TED talk UR-Funny. Extensive experiments and analysis show that FunnyNet-W successfully exploits visual, auditory and textual cues to identify funny moments, while our findings reveal FunnyNet-W's ability to predict funny moments in the wild. FunnyNet-W sets the new state of the art for funny moment detection with multimodal cues on all datasets with and without using ground truth information.","sentences":["Automatically understanding funny moments (i.e., the moments that make people laugh) when watching comedy is challenging, as they relate to various features, such as body language, dialogues and culture.","In this paper, we propose FunnyNet-W, a model that relies on cross- and self-attention for visual, audio and text data to predict funny moments in videos.","Unlike most methods that rely on ground truth data in the form of subtitles, in this work we exploit modalities that come naturally with videos: (a) video frames as they contain visual information indispensable for scene understanding, (b) audio as it contains higher-level cues associated with funny moments, such as intonation, pitch and pauses and (c) text automatically extracted with a speech-to-text model as it can provide rich information when processed by a Large Language Model.","To acquire labels for training, we propose an unsupervised approach that spots and labels funny audio moments.","We provide experiments on five datasets: the sitcoms TBBT, MHD, MUStARD, Friends, and the TED talk UR-Funny.","Extensive experiments and analysis show that FunnyNet-W successfully exploits visual, auditory and textual cues to identify funny moments, while our findings reveal FunnyNet-W's ability to predict funny moments in the wild.","FunnyNet-W sets the new state of the art for funny moment detection with multimodal cues on all datasets with and without using ground truth information."],"url":"http://arxiv.org/abs/2401.04210v1"}
