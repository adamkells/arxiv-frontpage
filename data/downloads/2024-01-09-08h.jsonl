{"created":"2024-01-08 18:59:31","title":"Dr$^2$Net: Dynamic Reversible Dual-Residual Networks for Memory-Efficient Finetuning","abstract":"Large pretrained models are increasingly crucial in modern computer vision tasks. These models are typically used in downstream tasks by end-to-end finetuning, which is highly memory-intensive for tasks with high-resolution data, e.g., video understanding, small object detection, and point cloud analysis. In this paper, we propose Dynamic Reversible Dual-Residual Networks, or Dr$^2$Net, a novel family of network architectures that acts as a surrogate network to finetune a pretrained model with substantially reduced memory consumption. Dr$^2$Net contains two types of residual connections, one maintaining the residual structure in the pretrained models, and the other making the network reversible. Due to its reversibility, intermediate activations, which can be reconstructed from output, are cleared from memory during training. We use two coefficients on either type of residual connections respectively, and introduce a dynamic training strategy that seamlessly transitions the pretrained model to a reversible network with much higher numerical precision. We evaluate Dr$^2$Net on various pretrained models and various tasks, and show that it can reach comparable performance to conventional finetuning but with significantly less memory usage.","sentences":["Large pretrained models are increasingly crucial in modern computer vision tasks.","These models are typically used in downstream tasks by end-to-end finetuning, which is highly memory-intensive for tasks with high-resolution data, e.g., video understanding, small object detection, and point cloud analysis.","In this paper, we propose Dynamic Reversible Dual-Residual Networks, or Dr$^2$Net, a novel family of network architectures that acts as a surrogate network to finetune a pretrained model with substantially reduced memory consumption.","Dr$^2$Net contains two types of residual connections, one maintaining the residual structure in the pretrained models, and the other making the network reversible.","Due to its reversibility, intermediate activations, which can be reconstructed from output, are cleared from memory during training.","We use two coefficients on either type of residual connections respectively, and introduce a dynamic training strategy that seamlessly transitions the pretrained model to a reversible network with much higher numerical precision.","We evaluate Dr$^2$Net on various pretrained models and various tasks, and show that it can reach comparable performance to conventional finetuning but with significantly less memory usage."],"url":"http://arxiv.org/abs/2401.04105v1"}
{"created":"2024-01-08 18:56:33","title":"AGG: Amortized Generative 3D Gaussians for Single Image to 3D","abstract":"Given the growing need for automatic 3D content creation pipelines, various 3D representations have been studied to generate 3D objects from a single image. Due to its superior rendering efficiency, 3D Gaussian splatting-based models have recently excelled in both 3D reconstruction and generation. 3D Gaussian splatting approaches for image to 3D generation are often optimization-based, requiring many computationally expensive score-distillation steps. To overcome these challenges, we introduce an Amortized Generative 3D Gaussian framework (AGG) that instantly produces 3D Gaussians from a single image, eliminating the need for per-instance optimization. Utilizing an intermediate hybrid representation, AGG decomposes the generation of 3D Gaussian locations and other appearance attributes for joint optimization. Moreover, we propose a cascaded pipeline that first generates a coarse representation of the 3D data and later upsamples it with a 3D Gaussian super-resolution module. Our method is evaluated against existing optimization-based 3D Gaussian frameworks and sampling-based pipelines utilizing other 3D representations, where AGG showcases competitive generation abilities both qualitatively and quantitatively while being several orders of magnitude faster. Project page: https://ir1d.github.io/AGG/","sentences":["Given the growing need for automatic 3D content creation pipelines, various 3D representations have been studied to generate 3D objects from a single image.","Due to its superior rendering efficiency, 3D Gaussian splatting-based models have recently excelled in both 3D reconstruction and generation.","3D Gaussian splatting approaches for image to 3D generation are often optimization-based, requiring many computationally expensive score-distillation steps.","To overcome these challenges, we introduce an Amortized Generative 3D Gaussian framework (AGG) that instantly produces 3D Gaussians from a single image, eliminating the need for per-instance optimization.","Utilizing an intermediate hybrid representation, AGG decomposes the generation of 3D Gaussian locations and other appearance attributes for joint optimization.","Moreover, we propose a cascaded pipeline that first generates a coarse representation of the 3D data and later upsamples it with a 3D Gaussian super-resolution module.","Our method is evaluated against existing optimization-based 3D Gaussian frameworks and sampling-based pipelines utilizing other 3D representations, where AGG showcases competitive generation abilities both qualitatively and quantitatively while being several orders of magnitude faster.","Project page: https://ir1d.github.io/AGG/"],"url":"http://arxiv.org/abs/2401.04099v1"}
{"created":"2024-01-08 18:27:57","title":"Security and Privacy Issues in Cloud Storage","abstract":"Even with the vast potential that cloud computing has, so far, it has not been adopted by the consumers with the enthusiasm and pace that it be worthy; this is a very reason statement why consumers still hesitated of using cloud computing for their sensitive data and the threats that prevent the consumers from shifting to use cloud computing in general and cloud storage in particular. The cloud computing inherits the traditional potential security and privacy threats besides its own issues due to its unique structures. Some threats related to cloud computing are the insider malicious attacks from the employees that even sometime the provider unconscious about, the lack of transparency of agreement between consumer and provider, data loss, traffic hijacking, shared technology and insecure application interface. Such threats need remedies to make the consumer use its features in secure way. In this review, we spot the light on the most security and privacy issues which can be attributed as gaps that sometimes the consumers or even the enterprises are not aware of. We also define the parties that involve in scenario of cloud computing that also may attack the entire cloud systems. We also show the consequences of these threats.","sentences":["Even with the vast potential that cloud computing has, so far, it has not been adopted by the consumers with the enthusiasm and pace that it be worthy; this is a very reason statement why consumers still hesitated of using cloud computing for their sensitive data and the threats that prevent the consumers from shifting to use cloud computing in general and cloud storage in particular.","The cloud computing inherits the traditional potential security and privacy threats besides its own issues due to its unique structures.","Some threats related to cloud computing are the insider malicious attacks from the employees that even sometime the provider unconscious about, the lack of transparency of agreement between consumer and provider, data loss, traffic hijacking, shared technology and insecure application interface.","Such threats need remedies to make the consumer use its features in secure way.","In this review, we spot the light on the most security and privacy issues which can be attributed as gaps that sometimes the consumers or even the enterprises are not aware of.","We also define the parties that involve in scenario of cloud computing that also may attack the entire cloud systems.","We also show the consequences of these threats."],"url":"http://arxiv.org/abs/2401.04076v1"}
{"created":"2024-01-08 18:18:02","title":"Fun with Flags: Robust Principal Directions via Flag Manifolds","abstract":"Principal component analysis (PCA), along with its extensions to manifolds and outlier contaminated data, have been indispensable in computer vision and machine learning. In this work, we present a unifying formalism for PCA and its variants, and introduce a framework based on the flags of linear subspaces, \\ie a hierarchy of nested linear subspaces of increasing dimension, which not only allows for a common implementation but also yields novel variants, not explored previously. We begin by generalizing traditional PCA methods that either maximize variance or minimize reconstruction error. We expand these interpretations to develop a wide array of new dimensionality reduction algorithms by accounting for outliers and the data manifold. To devise a common computational approach, we recast robust and dual forms of PCA as optimization problems on flag manifolds. We then integrate tangent space approximations of principal geodesic analysis (tangent-PCA) into this flag-based framework, creating novel robust and dual geodesic PCA variations. The remarkable flexibility offered by the 'flagification' introduced here enables even more algorithmic variants identified by specific flag types. Last but not least, we propose an effective convergent solver for these flag-formulations employing the Stiefel manifold. Our empirical results on both real-world and synthetic scenarios, demonstrate the superiority of our novel algorithms, especially in terms of robustness to outliers on manifolds.","sentences":["Principal component analysis (PCA), along with its extensions to manifolds and outlier contaminated data, have been indispensable in computer vision and machine learning.","In this work, we present a unifying formalism for PCA and its variants, and introduce a framework based on the flags of linear subspaces, \\ie a hierarchy of nested linear subspaces of increasing dimension, which not only allows for a common implementation but also yields novel variants, not explored previously.","We begin by generalizing traditional PCA methods that either maximize variance or minimize reconstruction error.","We expand these interpretations to develop a wide array of new dimensionality reduction algorithms by accounting for outliers and the data manifold.","To devise a common computational approach, we recast robust and dual forms of PCA as optimization problems on flag manifolds.","We then integrate tangent space approximations of principal geodesic analysis (tangent-PCA) into this flag-based framework, creating novel robust and dual geodesic PCA variations.","The remarkable flexibility offered by the 'flagification' introduced here enables even more algorithmic variants identified by specific flag types.","Last but not least, we propose an effective convergent solver for these flag-formulations employing the Stiefel manifold.","Our empirical results on both real-world and synthetic scenarios, demonstrate the superiority of our novel algorithms, especially in terms of robustness to outliers on manifolds."],"url":"http://arxiv.org/abs/2401.04071v1"}
{"created":"2024-01-08 18:01:09","title":"Variance Reduction in Ratio Metrics for Efficient Online Experiments","abstract":"Online controlled experiments, such as A/B-tests, are commonly used by modern tech companies to enable continuous system improvements. Despite their paramount importance, A/B-tests are expensive: by their very definition, a percentage of traffic is assigned an inferior system variant. To ensure statistical significance on top-level metrics, online experiments typically run for several weeks. Even then, a considerable amount of experiments will lead to inconclusive results (i.e. false negatives, or type-II error). The main culprit for this inefficiency is the variance of the online metrics. Variance reduction techniques have been proposed in the literature, but their direct applicability to commonly used ratio metrics (e.g. click-through rate or user retention) is limited.   In this work, we successfully apply variance reduction techniques to ratio metrics on a large-scale short-video platform: ShareChat. Our empirical results show that we can either improve A/B-test confidence in 77% of cases, or can retain the same level of confidence with 30% fewer data points. Importantly, we show that the common approach of including as many covariates as possible in regression is counter-productive, highlighting that control variates based on Gradient-Boosted Decision Tree predictors are most effective. We discuss the practicalities of implementing these methods at scale and showcase the cost reduction they beget.","sentences":["Online controlled experiments, such as A/B-tests, are commonly used by modern tech companies to enable continuous system improvements.","Despite their paramount importance, A/B-tests are expensive: by their very definition, a percentage of traffic is assigned an inferior system variant.","To ensure statistical significance on top-level metrics, online experiments typically run for several weeks.","Even then, a considerable amount of experiments will lead to inconclusive results (i.e. false negatives, or type-II error).","The main culprit for this inefficiency is the variance of the online metrics.","Variance reduction techniques have been proposed in the literature, but their direct applicability to commonly used ratio metrics (e.g. click-through rate or user retention) is limited.   ","In this work, we successfully apply variance reduction techniques to ratio metrics on a large-scale short-video platform: ShareChat.","Our empirical results show that we can either improve A/B-test confidence in 77% of cases, or can retain the same level of confidence with 30% fewer data points.","Importantly, we show that the common approach of including as many covariates as possible in regression is counter-productive, highlighting that control variates based on Gradient-Boosted Decision Tree predictors are most effective.","We discuss the practicalities of implementing these methods at scale and showcase the cost reduction they beget."],"url":"http://arxiv.org/abs/2401.04062v1"}
{"created":"2024-01-08 17:45:01","title":"The Role of Text in Visualizations: How Annotations Shape Perceptions of Bias and Influence Predictions","abstract":"This paper investigates the role of text in visualizations, specifically the impact of text position, semantic content, and biased wording. Two empirical studies were conducted based on two tasks (predicting data trends and appraising bias) using two visualization types (bar and line charts). While the addition of text had a minimal effect on how people perceive data trends, there was a significant impact on how biased they perceive the authors to be. This finding revealed a relationship between the degree of bias in textual information and the perception of the authors' bias. Exploratory analyses support an interaction between a person's prediction and the degree of bias they perceived. This paper also develops a crowdsourced method for creating chart annotations that range from neutral to highly biased. This research highlights the need for designers to mitigate potential polarization of readers' opinions based on how authors' ideas are expressed.","sentences":["This paper investigates the role of text in visualizations, specifically the impact of text position, semantic content, and biased wording.","Two empirical studies were conducted based on two tasks (predicting data trends and appraising bias) using two visualization types (bar and line charts).","While the addition of text had a minimal effect on how people perceive data trends, there was a significant impact on how biased they perceive the authors to be.","This finding revealed a relationship between the degree of bias in textual information and the perception of the authors' bias.","Exploratory analyses support an interaction between a person's prediction and the degree of bias they perceived.","This paper also develops a crowdsourced method for creating chart annotations that range from neutral to highly biased.","This research highlights the need for designers to mitigate potential polarization of readers' opinions based on how authors' ideas are expressed."],"url":"http://arxiv.org/abs/2401.04052v1"}
{"created":"2024-01-08 17:44:43","title":"Empirical Analysis of Efficient Fine-Tuning Methods for Large Pre-Trained Language Models","abstract":"Fine-tuning large pre-trained language models for downstream tasks remains a critical challenge in natural language processing. This paper presents an empirical analysis comparing two efficient fine-tuning methods - BitFit and adapter modules - to standard full model fine-tuning. Experiments conducted on GLUE benchmark datasets (MRPC, COLA, STS-B) reveal several key insights. The BitFit approach, which trains only bias terms and task heads, matches full fine-tuning performance across varying amounts of training data and time constraints. It demonstrates remarkable stability even with only 30\\% of data, outperforming full fine-tuning at intermediate data levels. Adapter modules exhibit high variability, with inconsistent gains over default models. The findings indicate BitFit offers an attractive balance between performance and parameter efficiency. Our work provides valuable perspectives on model tuning, emphasizing robustness and highlighting BitFit as a promising alternative for resource-constrained or streaming task settings. The analysis offers actionable guidelines for efficient adaptation of large pre-trained models, while illustrating open challenges in stabilizing techniques like adapter modules.","sentences":["Fine-tuning large pre-trained language models for downstream tasks remains a critical challenge in natural language processing.","This paper presents an empirical analysis comparing two efficient fine-tuning methods - BitFit and adapter modules - to standard full model fine-tuning.","Experiments conducted on GLUE benchmark datasets (MRPC, COLA, STS-B) reveal several key insights.","The BitFit approach, which trains only bias terms and task heads, matches full fine-tuning performance across varying amounts of training data and time constraints.","It demonstrates remarkable stability even with only 30\\% of data, outperforming full fine-tuning at intermediate data levels.","Adapter modules exhibit high variability, with inconsistent gains over default models.","The findings indicate BitFit offers an attractive balance between performance and parameter efficiency.","Our work provides valuable perspectives on model tuning, emphasizing robustness and highlighting BitFit as a promising alternative for resource-constrained or streaming task settings.","The analysis offers actionable guidelines for efficient adaptation of large pre-trained models, while illustrating open challenges in stabilizing techniques like adapter modules."],"url":"http://arxiv.org/abs/2401.04051v1"}
{"created":"2024-01-08 17:17:08","title":"Digital Twin for Autonomous Surface Vessels for Safe Maritime Navigation","abstract":"Autonomous surface vessels (ASVs) play an increasingly important role in the safety and sustainability of open sea operations. Since most maritime accidents are related to human failure, intelligent algorithms for autonomous collision avoidance and path following can drastically reduce the risk in the maritime sector. A DT is a virtual representative of a real physical system and can enhance the situational awareness (SITAW) of such an ASV to generate optimal decisions. This work builds on an existing DT framework for ASVs and demonstrates foundations for enabling predictive, prescriptive, and autonomous capabilities. In this context, sophisticated target tracking approaches are crucial for estimating and predicting the position and motion of other dynamic objects. The applied tracking method is enabled by real-time automatic identification system (AIS) data and synthetic light detection and ranging (Lidar) measurements. To guarantee safety during autonomous operations, we applied a predictive safety filter, based on the concept of nonlinear model predictive control (NMPC). The approaches are implemented into a DT built with the Unity game engine. As a result, this work demonstrates the potential of a DT capable of making predictions, playing through various what-if scenarios, and providing optimal control decisions according to its enhanced SITAW.","sentences":["Autonomous surface vessels (ASVs) play an increasingly important role in the safety and sustainability of open sea operations.","Since most maritime accidents are related to human failure, intelligent algorithms for autonomous collision avoidance and path following can drastically reduce the risk in the maritime sector.","A DT is a virtual representative of a real physical system and can enhance the situational awareness (SITAW) of such an ASV to generate optimal decisions.","This work builds on an existing DT framework for ASVs and demonstrates foundations for enabling predictive, prescriptive, and autonomous capabilities.","In this context, sophisticated target tracking approaches are crucial for estimating and predicting the position and motion of other dynamic objects.","The applied tracking method is enabled by real-time automatic identification system (AIS) data and synthetic light detection and ranging (Lidar) measurements.","To guarantee safety during autonomous operations, we applied a predictive safety filter, based on the concept of nonlinear model predictive control (NMPC).","The approaches are implemented into a DT built with the Unity game engine.","As a result, this work demonstrates the potential of a DT capable of making predictions, playing through various what-if scenarios, and providing optimal control decisions according to its enhanced SITAW."],"url":"http://arxiv.org/abs/2401.04032v1"}
{"created":"2024-01-08 17:07:37","title":"IDoFew: Intermediate Training Using Dual-Clustering in Language Models for Few Labels Text Classification","abstract":"Language models such as Bidirectional Encoder Representations from Transformers (BERT) have been very effective in various Natural Language Processing (NLP) and text mining tasks including text classification. However, some tasks still pose challenges for these models, including text classification with limited labels. This can result in a cold-start problem. Although some approaches have attempted to address this problem through single-stage clustering as an intermediate training step coupled with a pre-trained language model, which generates pseudo-labels to improve classification, these methods are often error-prone due to the limitations of the clustering algorithms. To overcome this, we have developed a novel two-stage intermediate clustering with subsequent fine-tuning that models the pseudo-labels reliably, resulting in reduced prediction errors. The key novelty in our model, IDoFew, is that the two-stage clustering coupled with two different clustering algorithms helps exploit the advantages of the complementary algorithms that reduce the errors in generating reliable pseudo-labels for fine-tuning. Our approach has shown significant improvements compared to strong comparative models.","sentences":["Language models such as Bidirectional Encoder Representations from Transformers (BERT) have been very effective in various Natural Language Processing (NLP) and text mining tasks including text classification.","However, some tasks still pose challenges for these models, including text classification with limited labels.","This can result in a cold-start problem.","Although some approaches have attempted to address this problem through single-stage clustering as an intermediate training step coupled with a pre-trained language model, which generates pseudo-labels to improve classification, these methods are often error-prone due to the limitations of the clustering algorithms.","To overcome this, we have developed a novel two-stage intermediate clustering with subsequent fine-tuning that models the pseudo-labels reliably, resulting in reduced prediction errors.","The key novelty in our model, IDoFew, is that the two-stage clustering coupled with two different clustering algorithms helps exploit the advantages of the complementary algorithms that reduce the errors in generating reliable pseudo-labels for fine-tuning.","Our approach has shown significant improvements compared to strong comparative models."],"url":"http://arxiv.org/abs/2401.04025v1"}
{"created":"2024-01-08 17:02:25","title":"Efficient Multiscale Multimodal Bottleneck Transformer for Audio-Video Classification","abstract":"In recent years, researchers combine both audio and video signals to deal with challenges where actions are not well represented or captured by visual cues. However, how to effectively leverage the two modalities is still under development. In this work, we develop a multiscale multimodal Transformer (MMT) that leverages hierarchical representation learning. Particularly, MMT is composed of a novel multiscale audio Transformer (MAT) and a multiscale video Transformer [43]. To learn a discriminative cross-modality fusion, we further design multimodal supervised contrastive objectives called audio-video contrastive loss (AVC) and intra-modal contrastive loss (IMC) that robustly align the two modalities. MMT surpasses previous state-of-the-art approaches by 7.3% and 2.1% on Kinetics-Sounds and VGGSound in terms of the top-1 accuracy without external training data. Moreover, the proposed MAT significantly outperforms AST [28] by 22.2%, 4.4% and 4.7% on three public benchmark datasets, and is about 3% more efficient based on the number of FLOPs and 9.8% more efficient based on GPU memory usage.","sentences":["In recent years, researchers combine both audio and video signals to deal with challenges where actions are not well represented or captured by visual cues.","However, how to effectively leverage the two modalities is still under development.","In this work, we develop a multiscale multimodal Transformer (MMT) that leverages hierarchical representation learning.","Particularly, MMT is composed of a novel multiscale audio Transformer (MAT) and a multiscale video Transformer [43].","To learn a discriminative cross-modality fusion, we further design multimodal supervised contrastive objectives called audio-video contrastive loss (AVC) and intra-modal contrastive loss (IMC) that robustly align the two modalities.","MMT surpasses previous state-of-the-art approaches by 7.3% and 2.1% on Kinetics-Sounds and VGGSound in terms of the top-1 accuracy without external training data.","Moreover, the proposed MAT significantly outperforms AST","[28] by 22.2%, 4.4% and 4.7% on three public benchmark datasets, and is about 3% more efficient based on the number of FLOPs and 9.8% more efficient based on GPU memory usage."],"url":"http://arxiv.org/abs/2401.04023v1"}
{"created":"2024-01-08 17:02:12","title":"Identifying Fabricated Networks within Authorship-for-Sale Enterprises","abstract":"Fabricated papers do not just need text, images, and data, they also require a fabricated or partially fabricated network of authors. Most `authors' on a fabricated paper have not been associated with the research, but rather are added through a transaction. This lack of deeper connection means that there is a low likelihood that co-authors on fabricated papers will ever appear together on the same paper more than once. This paper constructs a model that encodes some of the key characteristics of this activity in an `authorship-for-sale' network with the aim to create a robust method to detect this type of activity. A characteristic network fingerprint arises from this model that provides a robust statistical approach to the detection of paper-mill networks. The model suggested in this paper detects networks that have a statistically significant overlap with other approaches that principally rely on textual analysis for the detection of fraudulent papers. Researchers connected to networks identified using the methodology outlined in this paper are shown to be connected with 37% of papers identified through the tortured-phrase and clay-feet methods deployed in the Problematic Paper Screener website. Finally, methods to limit the expansion and propagation of these networks is discussed both in technological and social terms.","sentences":["Fabricated papers do not just need text, images, and data, they also require a fabricated or partially fabricated network of authors.","Most `authors' on a fabricated paper have not been associated with the research, but rather are added through a transaction.","This lack of deeper connection means that there is a low likelihood that co-authors on fabricated papers will ever appear together on the same paper more than once.","This paper constructs a model that encodes some of the key characteristics of this activity in an `authorship-for-sale' network with the aim to create a robust method to detect this type of activity.","A characteristic network fingerprint arises from this model that provides a robust statistical approach to the detection of paper-mill networks.","The model suggested in this paper detects networks that have a statistically significant overlap with other approaches that principally rely on textual analysis for the detection of fraudulent papers.","Researchers connected to networks identified using the methodology outlined in this paper are shown to be connected with 37% of papers identified through the tortured-phrase and clay-feet methods deployed in the Problematic Paper Screener website.","Finally, methods to limit the expansion and propagation of these networks is discussed both in technological and social terms."],"url":"http://arxiv.org/abs/2401.04022v1"}
{"created":"2024-01-08 16:55:33","title":"On the Long-Term behavior of $k$-tuples Frequencies in Mutation Systems","abstract":"In response to the evolving landscape of data storage, researchers have increasingly explored non-traditional platforms, with DNA-based storage emerging as a cutting-edge solution. Our work is motivated by the potential of in-vivo DNA storage, known for its capacity to store vast amounts of information efficiently and confidentially within an organism's native DNA. While promising, in-vivo DNA storage faces challenges, including susceptibility to errors introduced by mutations. To understand the long-term behavior of such mutation systems, we investigate the frequency of $k$-tuples after multiple mutation applications.   Drawing inspiration from related works, we generalize results from the study of mutation systems, particularly focusing on the frequency of $k$-tuples. In this work, we provide a broad analysis through the construction of a specialized matrix and the identification of its eigenvectors. In the context of substitution and duplication systems, we leverage previous results on almost sure convergence, equating the expected frequency to the limiting frequency. Moreover, we demonstrate convergence in probability under certain assumptions.","sentences":["In response to the evolving landscape of data storage, researchers have increasingly explored non-traditional platforms, with DNA-based storage emerging as a cutting-edge solution.","Our work is motivated by the potential of in-vivo DNA storage, known for its capacity to store vast amounts of information efficiently and confidentially within an organism's native DNA.","While promising, in-vivo DNA storage faces challenges, including susceptibility to errors introduced by mutations.","To understand the long-term behavior of such mutation systems, we investigate the frequency of $k$-tuples after multiple mutation applications.   ","Drawing inspiration from related works, we generalize results from the study of mutation systems, particularly focusing on the frequency of $k$-tuples.","In this work, we provide a broad analysis through the construction of a specialized matrix and the identification of its eigenvectors.","In the context of substitution and duplication systems, we leverage previous results on almost sure convergence, equating the expected frequency to the limiting frequency.","Moreover, we demonstrate convergence in probability under certain assumptions."],"url":"http://arxiv.org/abs/2401.04020v1"}
{"created":"2024-01-08 16:44:21","title":"MX: Enhancing RISC-V's Vector ISA for Ultra-Low Overhead, Energy-Efficient Matrix Multiplication","abstract":"Dense Matrix Multiplication (MatMul) is arguably one of the most ubiquitous compute-intensive kernels, spanning linear algebra, DSP, graphics, and machine learning applications. Thus, MatMul optimization is crucial not only in high-performance processors but also in embedded low-power platforms. Several Instruction Set Architectures (ISAs) have recently included matrix extensions to improve MatMul performance and efficiency at the cost of added matrix register files and units. In this paper, we propose Matrix eXtension (MX), a lightweight approach that builds upon the open-source RISC-V Vector (RVV) ISA to boost MatMul energy efficiency. Instead of adding expensive dedicated hardware, MX uses the pre-existing vector register file and functional units to create a hybrid vector/matrix engine at a negligible area cost (< 3%), which comes from a compact near-FPU tile buffer for higher data reuse, and no clock frequency overhead. We implement MX on a compact and highly energy-optimized RVV processor and evaluate it in both a Dual- and 64-Core cluster in a 12-nm technology node. MX boosts the Dual-Core's energy efficiency by 10% for a double-precision 64x64x64 matrix multiplication with the same FPU utilization (~97%) and by 25% on the 64-Core cluster for the same benchmark on 32-bit data, with a 56% performance gain.","sentences":["Dense Matrix Multiplication (MatMul) is arguably one of the most ubiquitous compute-intensive kernels, spanning linear algebra, DSP, graphics, and machine learning applications.","Thus, MatMul optimization is crucial not only in high-performance processors but also in embedded low-power platforms.","Several Instruction Set Architectures (ISAs) have recently included matrix extensions to improve MatMul performance and efficiency at the cost of added matrix register files and units.","In this paper, we propose Matrix eXtension (MX), a lightweight approach that builds upon the open-source RISC-V Vector (RVV) ISA to boost MatMul energy efficiency.","Instead of adding expensive dedicated hardware, MX uses the pre-existing vector register file and functional units to create a hybrid vector/matrix engine at a negligible area cost (< 3%), which comes from a compact near-FPU tile buffer for higher data reuse, and no clock frequency overhead.","We implement MX on a compact and highly energy-optimized RVV processor and evaluate it in both a Dual- and 64-Core cluster in a 12-nm technology node.","MX boosts the Dual-Core's energy efficiency by 10% for a double-precision 64x64x64 matrix multiplication with the same FPU utilization (~97%) and by 25% on the 64-Core cluster for the same benchmark on 32-bit data, with a 56% performance gain."],"url":"http://arxiv.org/abs/2401.04012v1"}
{"created":"2024-01-08 16:37:55","title":"Task-Oriented Active Learning of Model Preconditions for Inaccurate Dynamics Models","abstract":"When planning with an inaccurate dynamics model, a practical strategy is to restrict planning to regions of state-action space where the model is accurate: also known as a model precondition. Empirical real-world trajectory data is valuable for defining data-driven model preconditions regardless of the model form (analytical, simulator, learned, etc...). However, real-world data is often expensive and dangerous to collect. In order to achieve data efficiency, this paper presents an algorithm for actively selecting trajectories to learn a model precondition for an inaccurate pre-specified dynamics model. Our proposed techniques address challenges arising from the sequential nature of trajectories, and potential benefit of prioritizing task-relevant data. The experimental analysis shows how algorithmic properties affect performance in three planning scenarios: icy gridworld, simulated plant watering, and real-world plant watering. Results demonstrate an improvement of approximately 80% after only four real-world trajectories when using our proposed techniques.","sentences":["When planning with an inaccurate dynamics model, a practical strategy is to restrict planning to regions of state-action space where the model is accurate: also known as a model precondition.","Empirical real-world trajectory data is valuable for defining data-driven model preconditions regardless of the model form (analytical, simulator, learned, etc...).","However, real-world data is often expensive and dangerous to collect.","In order to achieve data efficiency, this paper presents an algorithm for actively selecting trajectories to learn a model precondition for an inaccurate pre-specified dynamics model.","Our proposed techniques address challenges arising from the sequential nature of trajectories, and potential benefit of prioritizing task-relevant data.","The experimental analysis shows how algorithmic properties affect performance in three planning scenarios: icy gridworld, simulated plant watering, and real-world plant watering.","Results demonstrate an improvement of approximately 80% after only four real-world trajectories when using our proposed techniques."],"url":"http://arxiv.org/abs/2401.04007v1"}
{"created":"2024-01-08 16:36:47","title":"Generative adversarial wavelet neural operator: Application to fault detection and isolation of multivariate time series data","abstract":"Fault detection and isolation in complex systems are critical to ensure reliable and efficient operation. However, traditional fault detection methods often struggle with issues such as nonlinearity and multivariate characteristics of the time series variables. This article proposes a generative adversarial wavelet neural operator (GAWNO) as a novel unsupervised deep learning approach for fault detection and isolation of multivariate time series processes.The GAWNO combines the strengths of wavelet neural operators and generative adversarial networks (GANs) to effectively capture both the temporal distributions and the spatial dependencies among different variables of an underlying system. The approach of fault detection and isolation using GAWNO consists of two main stages. In the first stage, the GAWNO is trained on a dataset of normal operating conditions to learn the underlying data distribution. In the second stage, a reconstruction error-based threshold approach using the trained GAWNO is employed to detect and isolate faults based on the discrepancy values. We validate the proposed approach using the Tennessee Eastman Process (TEP) dataset and Avedore wastewater treatment plant (WWTP) and N2O emissions named as WWTPN2O datasets. Overall, we showcase that the idea of harnessing the power of wavelet analysis, neural operators, and generative models in a single framework to detect and isolate faults has shown promising results compared to various well-established baselines in the literature.","sentences":["Fault detection and isolation in complex systems are critical to ensure reliable and efficient operation.","However, traditional fault detection methods often struggle with issues such as nonlinearity and multivariate characteristics of the time series variables.","This article proposes a generative adversarial wavelet neural operator (GAWNO) as a novel unsupervised deep learning approach for fault detection and isolation of multivariate time series processes.","The GAWNO combines the strengths of wavelet neural operators and generative adversarial networks (GANs) to effectively capture both the temporal distributions and the spatial dependencies among different variables of an underlying system.","The approach of fault detection and isolation using GAWNO consists of two main stages.","In the first stage, the GAWNO is trained on a dataset of normal operating conditions to learn the underlying data distribution.","In the second stage, a reconstruction error-based threshold approach using the trained GAWNO is employed to detect and isolate faults based on the discrepancy values.","We validate the proposed approach using the Tennessee Eastman Process (TEP) dataset and Avedore wastewater treatment plant (WWTP) and N2O emissions named as WWTPN2O datasets.","Overall, we showcase that the idea of harnessing the power of wavelet analysis, neural operators, and generative models in a single framework to detect and isolate faults has shown promising results compared to various well-established baselines in the literature."],"url":"http://arxiv.org/abs/2401.04004v1"}
{"created":"2024-01-08 16:15:43","title":"Behavioural Cloning in VizDoom","abstract":"This paper describes methods for training autonomous agents to play the game \"Doom 2\" through Imitation Learning (IL) using only pixel data as input. We also explore how Reinforcement Learning (RL) compares to IL for humanness by comparing camera movement and trajectory data. Through behavioural cloning, we examine the ability of individual models to learn varying behavioural traits. We attempt to mimic the behaviour of real players with different play styles, and find we can train agents that behave aggressively, passively, or simply more human-like than traditional AIs. We propose these methods of introducing more depth and human-like behaviour to agents in video games. The trained IL agents perform on par with the average players in our dataset, whilst outperforming the worst players. While performance was not as strong as common RL approaches, it provides much stronger human-like behavioural traits to the agent.","sentences":["This paper describes methods for training autonomous agents to play the game \"Doom 2\" through Imitation Learning (IL) using only pixel data as input.","We also explore how Reinforcement Learning (RL) compares to IL for humanness by comparing camera movement and trajectory data.","Through behavioural cloning, we examine the ability of individual models to learn varying behavioural traits.","We attempt to mimic the behaviour of real players with different play styles, and find we can train agents that behave aggressively, passively, or simply more human-like than traditional AIs.","We propose these methods of introducing more depth and human-like behaviour to agents in video games.","The trained IL agents perform on par with the average players in our dataset, whilst outperforming the worst players.","While performance was not as strong as common RL approaches, it provides much stronger human-like behavioural traits to the agent."],"url":"http://arxiv.org/abs/2401.03993v1"}
{"created":"2024-01-08 15:29:23","title":"Comparing Data-Driven and Mechanistic Models for Predicting Phenology in Deciduous Broadleaf Forests","abstract":"Understanding the future climate is crucial for informed policy decisions on climate change prevention and mitigation. Earth system models play an important role in predicting future climate, requiring accurate representation of complex sub-processes that span multiple time scales and spatial scales. One such process that links seasonal and interannual climate variability to cyclical biological events is tree phenology in deciduous broadleaf forests. Phenological dates, such as the start and end of the growing season, are critical for understanding the exchange of carbon and water between the biosphere and the atmosphere. Mechanistic prediction of these dates is challenging. Hybrid modelling, which integrates data-driven approaches into complex models, offers a solution. In this work, as a first step towards this goal, train a deep neural network to predict a phenological index from meteorological time series. We find that this approach outperforms traditional process-based models. This highlights the potential of data-driven methods to improve climate predictions. We also analyze which variables and aspects of the time series influence the predicted onset of the season, in order to gain a better understanding of the advantages and limitations of our model.","sentences":["Understanding the future climate is crucial for informed policy decisions on climate change prevention and mitigation.","Earth system models play an important role in predicting future climate, requiring accurate representation of complex sub-processes that span multiple time scales and spatial scales.","One such process that links seasonal and interannual climate variability to cyclical biological events is tree phenology in deciduous broadleaf forests.","Phenological dates, such as the start and end of the growing season, are critical for understanding the exchange of carbon and water between the biosphere and the atmosphere.","Mechanistic prediction of these dates is challenging.","Hybrid modelling, which integrates data-driven approaches into complex models, offers a solution.","In this work, as a first step towards this goal, train a deep neural network to predict a phenological index from meteorological time series.","We find that this approach outperforms traditional process-based models.","This highlights the potential of data-driven methods to improve climate predictions.","We also analyze which variables and aspects of the time series influence the predicted onset of the season, in order to gain a better understanding of the advantages and limitations of our model."],"url":"http://arxiv.org/abs/2401.03960v1"}
{"created":"2024-01-08 15:21:21","title":"TTMs: Fast Multi-level Tiny Time Mixers for Improved Zero-shot and Few-shot Forecasting of Multivariate Time Series","abstract":"Large Pretrained models for Zero/Few-shot learning excel in language and vision domains but encounter challenges in multivariate time series (TS) due to the diverse nature and scarcity of publicly available pretraining data. Consequently, there has been a recent surge in utilizing pretrained large language models (LLMs) with various adaptations for time series forecasting. These approaches employ cross-domain transfer learning, yielding highly impressive results. However, these models are typically very large ($\\sim$ billion parameters), exhibit slow execution, and do not consider cross-channel correlations. To address this, we present Multi-level Tiny Time Mixers (TTM), a significantly smaller model based on the lightweight TSMixer architecture. TTM marks the first success in developing tiny pretrained models ($\\le$1 million parameters), exclusively trained on public TS data with effective transfer learning capabilities. To tackle the complexity of pretraining on multiple datasets with varied temporal resolutions, we introduce several novel enhancements such as adaptive patching, dataset augmentation via downsampling, and resolution prefix tuning. Moreover, we employ a multi-level modeling strategy to effectively model channel correlations and incorporate exogenous signals during finetuning, a crucial capability lacking in existing benchmarks. TTM excels in few/zero-shot forecasting, demonstrating significant accuracy gains (12-38%) over existing benchmarks. Further, it achieves a remarkable 14-106X reduction in model parameters, enabling 54-65X faster training/inference as compared to the LLM-TS benchmarks. In fact, TTM's zero-shot results often surpass the few-shot results in many benchmarks, highlighting the efficacy of our approach. Code and Pretrained Models will be open-sourced.","sentences":["Large Pretrained models for Zero/Few-shot learning excel in language and vision domains but encounter challenges in multivariate time series (TS) due to the diverse nature and scarcity of publicly available pretraining data.","Consequently, there has been a recent surge in utilizing pretrained large language models (LLMs) with various adaptations for time series forecasting.","These approaches employ cross-domain transfer learning, yielding highly impressive results.","However, these models are typically very large ($\\sim$ billion parameters), exhibit slow execution, and do not consider cross-channel correlations.","To address this, we present Multi-level Tiny Time Mixers (TTM), a significantly smaller model based on the lightweight TSMixer architecture.","TTM marks the first success in developing tiny pretrained models ($\\le$1 million parameters), exclusively trained on public TS data with effective transfer learning capabilities.","To tackle the complexity of pretraining on multiple datasets with varied temporal resolutions, we introduce several novel enhancements such as adaptive patching, dataset augmentation via downsampling, and resolution prefix tuning.","Moreover, we employ a multi-level modeling strategy to effectively model channel correlations and incorporate exogenous signals during finetuning, a crucial capability lacking in existing benchmarks.","TTM excels in few/zero-shot forecasting, demonstrating significant accuracy gains (12-38%) over existing benchmarks.","Further, it achieves a remarkable 14-106X reduction in model parameters, enabling 54-65X faster training/inference as compared to the LLM-TS benchmarks.","In fact, TTM's zero-shot results often surpass the few-shot results in many benchmarks, highlighting the efficacy of our approach.","Code and Pretrained Models will be open-sourced."],"url":"http://arxiv.org/abs/2401.03955v1"}
{"created":"2024-01-08 14:57:22","title":"Recovering the 3D UUV Position using UAV Imagery in Shallow-Water Environments","abstract":"In this paper we propose a novel approach aimed at recovering the 3D position of an UUV from UAV imagery in shallow-water environments. Through combination of UAV and UUV measurements, we show that our method can be utilized as an accurate and cost-effective alternative when compared to acoustic sensing methods, typically required to obtain ground truth information in underwater localization problems. Furthermore, our approach allows for a seamless conversion to geo-referenced coordinates which can be utilized for navigation purposes. To validate our method, we present the results with data collected through a simulation environment and field experiments, demonstrating the ability to successfully recover the UUV position with sub-meter accuracy.","sentences":["In this paper we propose a novel approach aimed at recovering the 3D position of an UUV from UAV imagery in shallow-water environments.","Through combination of UAV and UUV measurements, we show that our method can be utilized as an accurate and cost-effective alternative when compared to acoustic sensing methods, typically required to obtain ground truth information in underwater localization problems.","Furthermore, our approach allows for a seamless conversion to geo-referenced coordinates which can be utilized for navigation purposes.","To validate our method, we present the results with data collected through a simulation environment and field experiments, demonstrating the ability to successfully recover the UUV position with sub-meter accuracy."],"url":"http://arxiv.org/abs/2401.03938v1"}
{"created":"2024-01-08 14:45:15","title":"Using reinforcement learning to improve drone-based inference of greenhouse gas fluxes","abstract":"Accurate mapping of greenhouse gas fluxes at the Earth's surface is essential for the validation and calibration of climate models. In this study, we present a framework for surface flux estimation with drones. Our approach uses data assimilation (DA) to infer fluxes from drone-based observations, and reinforcement learning (RL) to optimize the drone's sampling strategy. Herein, we demonstrate that a RL-trained drone can quantify a CO2 hotspot more accurately than a drone sampling along a predefined flight path that traverses the emission plume. We find that information-based reward functions can match the performance of an error-based reward function that quantifies the difference between the estimated surface flux and the true value. Reward functions based on information gain and information entropy can motivate actions that increase the drone's confidence in its updated belief, without requiring knowledge of the true surface flux. These findings provide valuable insights for further development of the framework for the mapping of more complex surface flux fields.","sentences":["Accurate mapping of greenhouse gas fluxes at the Earth's surface is essential for the validation and calibration of climate models.","In this study, we present a framework for surface flux estimation with drones.","Our approach uses data assimilation (DA) to infer fluxes from drone-based observations, and reinforcement learning (RL) to optimize the drone's sampling strategy.","Herein, we demonstrate that a RL-trained drone can quantify a CO2 hotspot more accurately than a drone sampling along a predefined flight path that traverses the emission plume.","We find that information-based reward functions can match the performance of an error-based reward function that quantifies the difference between the estimated surface flux and the true value.","Reward functions based on information gain and information entropy can motivate actions that increase the drone's confidence in its updated belief, without requiring knowledge of the true surface flux.","These findings provide valuable insights for further development of the framework for the mapping of more complex surface flux fields."],"url":"http://arxiv.org/abs/2401.03932v1"}
{"created":"2024-01-08 14:39:21","title":"Rastro-DM: data mining with a trail","abstract":"This paper proposes a methodology for documenting data mining (DM) projects, Rastro-DM (Trail Data Mining), with a focus not on the model that is generated, but on the processes behind its construction, in order to leave a trail (Rastro in Portuguese) of planned actions, training completed, results obtained, and lessons learned. The proposed practices are complementary to structuring methodologies of DM, such as CRISP-DM, which establish a methodological and paradigmatic framework for the DM process. The application of best practices and their benefits is illustrated in a project called 'Cladop' that was created for the classification of PDF documents associated with the investigative process of damages to the Brazilian Federal Public Treasury. Building the Rastro-DM kit in the context of a project is a small step that can lead to an institutional leap to be achieved by sharing and using the trail across the enterprise.","sentences":["This paper proposes a methodology for documenting data mining (DM) projects, Rastro-DM (Trail Data Mining), with a focus not on the model that is generated, but on the processes behind its construction, in order to leave a trail (Rastro in Portuguese) of planned actions, training completed, results obtained, and lessons learned.","The proposed practices are complementary to structuring methodologies of DM, such as CRISP-DM, which establish a methodological and paradigmatic framework for the DM process.","The application of best practices and their benefits is illustrated in a project called 'Cladop' that was created for the classification of PDF documents associated with the investigative process of damages to the Brazilian Federal Public Treasury.","Building the Rastro-DM kit in the context of a project is a small step that can lead to an institutional leap to be achieved by sharing and using the trail across the enterprise."],"url":"http://arxiv.org/abs/2401.03925v1"}
{"created":"2024-01-08 14:21:02","title":"D3PRefiner: A Diffusion-based Denoise Method for 3D Human Pose Refinement","abstract":"Three-dimensional (3D) human pose estimation using a monocular camera has gained increasing attention due to its ease of implementation and the abundance of data available from daily life. However, owing to the inherent depth ambiguity in images, the accuracy of existing monocular camera-based 3D pose estimation methods remains unsatisfactory, and the estimated 3D poses usually include much noise. By observing the histogram of this noise, we find each dimension of the noise follows a certain distribution, which indicates the possibility for a neural network to learn the mapping between noisy poses and ground truth poses. In this work, in order to obtain more accurate 3D poses, a Diffusion-based 3D Pose Refiner (D3PRefiner) is proposed to refine the output of any existing 3D pose estimator. We first introduce a conditional multivariate Gaussian distribution to model the distribution of noisy 3D poses, using paired 2D poses and noisy 3D poses as conditions to achieve greater accuracy. Additionally, we leverage the architecture of current diffusion models to convert the distribution of noisy 3D poses into ground truth 3D poses. To evaluate the effectiveness of the proposed method, two state-of-the-art sequence-to-sequence 3D pose estimators are used as basic 3D pose estimation models, and the proposed method is evaluated on different types of 2D poses and different lengths of the input sequence. Experimental results demonstrate the proposed architecture can significantly improve the performance of current sequence-to-sequence 3D pose estimators, with a reduction of at least 10.3% in the mean per joint position error (MPJPE) and at least 11.0% in the Procrustes MPJPE (P-MPJPE).","sentences":["Three-dimensional (3D) human pose estimation using a monocular camera has gained increasing attention due to its ease of implementation and the abundance of data available from daily life.","However, owing to the inherent depth ambiguity in images, the accuracy of existing monocular camera-based 3D pose estimation methods remains unsatisfactory, and the estimated 3D poses usually include much noise.","By observing the histogram of this noise, we find each dimension of the noise follows a certain distribution, which indicates the possibility for a neural network to learn the mapping between noisy poses and ground truth poses.","In this work, in order to obtain more accurate 3D poses, a Diffusion-based 3D Pose Refiner (D3PRefiner) is proposed to refine the output of any existing 3D pose estimator.","We first introduce a conditional multivariate Gaussian distribution to model the distribution of noisy 3D poses, using paired 2D poses and noisy 3D poses as conditions to achieve greater accuracy.","Additionally, we leverage the architecture of current diffusion models to convert the distribution of noisy 3D poses into ground truth 3D poses.","To evaluate the effectiveness of the proposed method, two state-of-the-art sequence-to-sequence 3D pose estimators are used as basic 3D pose estimation models, and the proposed method is evaluated on different types of 2D poses and different lengths of the input sequence.","Experimental results demonstrate the proposed architecture can significantly improve the performance of current sequence-to-sequence 3D pose estimators, with a reduction of at least 10.3% in the mean per joint position error (MPJPE) and at least 11.0% in the Procrustes MPJPE (P-MPJPE)."],"url":"http://arxiv.org/abs/2401.03914v1"}
{"created":"2024-01-08 14:16:55","title":"A Wasserstein Graph Distance Based on Distributions of Probabilistic Node Embeddings","abstract":"Distance measures between graphs are important primitives for a variety of learning tasks. In this work, we describe an unsupervised, optimal transport based approach to define a distance between graphs. Our idea is to derive representations of graphs as Gaussian mixture models, fitted to distributions of sampled node embeddings over the same space. The Wasserstein distance between these Gaussian mixture distributions then yields an interpretable and easily computable distance measure, which can further be tailored for the comparison at hand by choosing appropriate embeddings. We propose two embeddings for this framework and show that under certain assumptions about the shape of the resulting Gaussian mixture components, further computational improvements of this Wasserstein distance can be achieved. An empirical validation of our findings on synthetic data and real-world Functional Brain Connectivity networks shows promising performance compared to existing embedding methods.","sentences":["Distance measures between graphs are important primitives for a variety of learning tasks.","In this work, we describe an unsupervised, optimal transport based approach to define a distance between graphs.","Our idea is to derive representations of graphs as Gaussian mixture models, fitted to distributions of sampled node embeddings over the same space.","The Wasserstein distance between these Gaussian mixture distributions then yields an interpretable and easily computable distance measure, which can further be tailored for the comparison at hand by choosing appropriate embeddings.","We propose two embeddings for this framework and show that under certain assumptions about the shape of the resulting Gaussian mixture components, further computational improvements of this Wasserstein distance can be achieved.","An empirical validation of our findings on synthetic data and real-world Functional Brain Connectivity networks shows promising performance compared to existing embedding methods."],"url":"http://arxiv.org/abs/2401.03913v1"}
{"created":"2024-01-08 13:52:59","title":"Ultra-Dense Cell-Free Massive MIMO for 6G: Technical Overview and Open Questions","abstract":"Ultra-dense cell-free massive multiple-input multiple-output (CF-MMIMO) has emerged as a promising technology expected to meet the future ubiquitous connectivity requirements and ever-growing data traffic demands in 6G. This article provides a contemporary overview of ultra-dense CF-MMIMO networks, and addresses important unresolved questions on their future deployment. We first present a comprehensive survey of state-of-the-art research on CF-MMIMO and ultra-dense networks. Then, we discuss the key challenges of CF-MMIMO under ultra-dense scenarios such as low-complexity architecture and processing, low-complexity/scalable resource allocation, fronthaul limitation, massive access, synchronization, and channel acquisition. Finally, we answer key open questions, considering different design comparisons and discussing suitable methods dealing with the key challenges of ultra-dense CF-MMIMO. The discussion aims to provide a valuable roadmap for interesting future research directions in this area, facilitating the development of CF-MMIMO MIMO for 6G.","sentences":["Ultra-dense cell-free massive multiple-input multiple-output (CF-MMIMO) has emerged as a promising technology expected to meet the future ubiquitous connectivity requirements and ever-growing data traffic demands in 6G. This article provides a contemporary overview of ultra-dense CF-MMIMO networks, and addresses important unresolved questions on their future deployment.","We first present a comprehensive survey of state-of-the-art research on CF-MMIMO and ultra-dense networks.","Then, we discuss the key challenges of CF-MMIMO under ultra-dense scenarios such as low-complexity architecture and processing, low-complexity/scalable resource allocation, fronthaul limitation, massive access, synchronization, and channel acquisition.","Finally, we answer key open questions, considering different design comparisons and discussing suitable methods dealing with the key challenges of ultra-dense CF-MMIMO.","The discussion aims to provide a valuable roadmap for interesting future research directions in this area, facilitating the development of CF-MMIMO MIMO for 6G."],"url":"http://arxiv.org/abs/2401.03898v1"}
{"created":"2024-01-08 13:31:02","title":"The Impact of Differential Privacy on Recommendation Accuracy and Popularity Bias","abstract":"Collaborative filtering-based recommender systems leverage vast amounts of behavioral user data, which poses severe privacy risks. Thus, often, random noise is added to the data to ensure Differential Privacy (DP). However, to date, it is not well understood, in which ways this impacts personalized recommendations. In this work, we study how DP impacts recommendation accuracy and popularity bias, when applied to the training data of state-of-the-art recommendation models. Our findings are three-fold: First, we find that nearly all users' recommendations change when DP is applied. Second, recommendation accuracy drops substantially while recommended item popularity experiences a sharp increase, suggesting that popularity bias worsens. Third, we find that DP exacerbates popularity bias more severely for users who prefer unpopular items than for users that prefer popular items.","sentences":["Collaborative filtering-based recommender systems leverage vast amounts of behavioral user data, which poses severe privacy risks.","Thus, often, random noise is added to the data to ensure Differential Privacy (DP).","However, to date, it is not well understood, in which ways this impacts personalized recommendations.","In this work, we study how DP impacts recommendation accuracy and popularity bias, when applied to the training data of state-of-the-art recommendation models.","Our findings are three-fold: First, we find that nearly all users' recommendations change when DP is applied.","Second, recommendation accuracy drops substantially while recommended item popularity experiences a sharp increase, suggesting that popularity bias worsens.","Third, we find that DP exacerbates popularity bias more severely for users who prefer unpopular items than for users that prefer popular items."],"url":"http://arxiv.org/abs/2401.03883v1"}
{"created":"2024-01-08 12:54:22","title":"Incremental Learning of Stock Trends via Meta-Learning with Dynamic Adaptation","abstract":"Forecasting the trend of stock prices is an enduring topic at the intersection of finance and computer science. Incremental updates to forecasters have proven effective in alleviating the impacts of concept drift arising from non-stationary stock markets. However, there is a need for refinement in the incremental learning of stock trends, as existing methods disregard recurring patterns. To address this issue, we propose meta-learning with dynamic adaptation (MetaDA) for the incremental learning of stock trends, which performs dynamic model adaptation considering both the recurring and emerging patterns. We initially organize the stock trend forecasting into meta-learning tasks and train a forecasting model following meta-learning protocols. During model adaptation, MetaDA adapts the forecasting model with the latest data and a selected portion of historical data, which is dynamically identified by a task inference module. The task inference module first extracts task-level embeddings from the historical tasks, and then identifies the informative data with a task inference network. MetaDA has been evaluated on real-world stock datasets, achieving state-of-the-art performance with satisfactory efficiency.","sentences":["Forecasting the trend of stock prices is an enduring topic at the intersection of finance and computer science.","Incremental updates to forecasters have proven effective in alleviating the impacts of concept drift arising from non-stationary stock markets.","However, there is a need for refinement in the incremental learning of stock trends, as existing methods disregard recurring patterns.","To address this issue, we propose meta-learning with dynamic adaptation (MetaDA) for the incremental learning of stock trends, which performs dynamic model adaptation considering both the recurring and emerging patterns.","We initially organize the stock trend forecasting into meta-learning tasks and train a forecasting model following meta-learning protocols.","During model adaptation, MetaDA adapts the forecasting model with the latest data and a selected portion of historical data, which is dynamically identified by a task inference module.","The task inference module first extracts task-level embeddings from the historical tasks, and then identifies the informative data with a task inference network.","MetaDA has been evaluated on real-world stock datasets, achieving state-of-the-art performance with satisfactory efficiency."],"url":"http://arxiv.org/abs/2401.03865v1"}
{"created":"2024-01-08 12:37:25","title":"DJCM: A Deep Joint Cascade Model for Singing Voice Separation and Vocal Pitch Estimation","abstract":"Singing voice separation and vocal pitch estimation are pivotal tasks in music information retrieval. Existing methods for simultaneous extraction of clean vocals and vocal pitches can be classified into two categories: pipeline methods and naive joint learning methods. However, the efficacy of these methods is limited by the following problems: On the one hand, pipeline methods train models for each task independently, resulting a mismatch between the data distributions at the training and testing time. On the other hand, naive joint learning methods simply add the losses of both tasks, possibly leading to a misalignment between the distinct objectives of each task. To solve these problems, we propose a Deep Joint Cascade Model (DJCM) for singing voice separation and vocal pitch estimation. DJCM employs a novel joint cascade model structure to concurrently train both tasks. Moreover, task-specific weights are used to align different objectives of both tasks. Experimental results show that DJCM achieves state-of-the-art performance on both tasks, with great improvements of 0.45 in terms of Signal-to-Distortion Ratio (SDR) for singing voice separation and 2.86% in terms of Overall Accuracy (OA) for vocal pitch estimation. Furthermore, extensive ablation studies validate the effectiveness of each design of our proposed model. The code of DJCM is available at https://github.com/Dream-High/DJCM .","sentences":["Singing voice separation and vocal pitch estimation are pivotal tasks in music information retrieval.","Existing methods for simultaneous extraction of clean vocals and vocal pitches can be classified into two categories: pipeline methods and naive joint learning methods.","However, the efficacy of these methods is limited by the following problems: On the one hand, pipeline methods train models for each task independently, resulting a mismatch between the data distributions at the training and testing time.","On the other hand, naive joint learning methods simply add the losses of both tasks, possibly leading to a misalignment between the distinct objectives of each task.","To solve these problems, we propose a Deep Joint Cascade Model (DJCM) for singing voice separation and vocal pitch estimation.","DJCM employs a novel joint cascade model structure to concurrently train both tasks.","Moreover, task-specific weights are used to align different objectives of both tasks.","Experimental results show that DJCM achieves state-of-the-art performance on both tasks, with great improvements of 0.45 in terms of Signal-to-Distortion Ratio (SDR) for singing voice separation and 2.86% in terms of Overall Accuracy (OA) for vocal pitch estimation.","Furthermore, extensive ablation studies validate the effectiveness of each design of our proposed model.","The code of DJCM is available at https://github.com/Dream-High/DJCM ."],"url":"http://arxiv.org/abs/2401.03856v1"}
{"created":"2024-01-08 12:30:23","title":"Aligned with LLM: a new multi-modal training paradigm for encoding fMRI activity in visual cortex","abstract":"Recently, there has been a surge in the popularity of pre trained large language models (LLMs) (such as GPT-4), sweeping across the entire Natural Language Processing (NLP) and Computer Vision (CV) communities. These LLMs have demonstrated advanced multi-modal understanding capabilities and showcased strong performance across various benchmarks. The LLM has started to embody traits of artificial general intelligence, which holds vital guidance for enhancing brain-like characteristics within visual encoding models. Hence, This paper proposes a new multi-modal training paradigm, aligning with LLM, for encoding fMRI activity in visual cortex. Based on this paradigm, we trained an encoding model in fMRI data named the LLM-Visual Encoding Model (LLM-VEM). Specifically, we utilize LLM (miniGPT4) to generate descriptive text for all stimulus images, forming a high-quality textual description set. Moreover, we use the pre-trained text encoder (CLIP) to process these detailed descriptions, obtaining the text embedding features. Next, we use the contrast loss function to minimize the distance between the image embedding features and the text embedding features to complete the alignment operation of the stimulus image and text information. With the assistance of the pre-trained LLM, this alignment process facilitates better learning of the visual encoding model, resulting in higher precision. The final experimental results indicate that our training paradigm has significantly aided in enhancing the performance of the visual encoding model.","sentences":["Recently, there has been a surge in the popularity of pre trained large language models (LLMs) (such as GPT-4), sweeping across the entire Natural Language Processing (NLP) and Computer Vision (CV) communities.","These LLMs have demonstrated advanced multi-modal understanding capabilities and showcased strong performance across various benchmarks.","The LLM has started to embody traits of artificial general intelligence, which holds vital guidance for enhancing brain-like characteristics within visual encoding models.","Hence, This paper proposes a new multi-modal training paradigm, aligning with LLM, for encoding fMRI activity in visual cortex.","Based on this paradigm, we trained an encoding model in fMRI data named the LLM-Visual Encoding Model (LLM-VEM).","Specifically, we utilize LLM (miniGPT4) to generate descriptive text for all stimulus images, forming a high-quality textual description set.","Moreover, we use the pre-trained text encoder (CLIP) to process these detailed descriptions, obtaining the text embedding features.","Next, we use the contrast loss function to minimize the distance between the image embedding features and the text embedding features to complete the alignment operation of the stimulus image and text information.","With the assistance of the pre-trained LLM, this alignment process facilitates better learning of the visual encoding model, resulting in higher precision.","The final experimental results indicate that our training paradigm has significantly aided in enhancing the performance of the visual encoding model."],"url":"http://arxiv.org/abs/2401.03851v1"}
{"created":"2024-01-08 12:19:53","title":"Analysis of Blockchain Integration in the e-Healthcare Ecosystem","abstract":"No one can dispute the disruptive impact of blockchain technology, which has long been considered one of the major revolutions of contemporary times. Its integration into the healthcare ecosystem has helped overcome numerous difficulties and constraints faced by healthcare systems. This has been notably demonstrated in the meticulous management of electronic health records (EHR) and their access rights, as well as in its capabilities in terms of security, scalability, flexibility, and interoperability with other systems. This article undertakes the study and analysis of the most commonly adopted approaches in healthcare data management systems using blockchain technology. An evaluation is then conducted based on a set of observed common characteristics, distinguishing one approach from the others. The results of this analysis highlight the advantages and limitations of each approach, thus facilitating the choice of the method best suited to the readers' specific case study. Furthermore, for effective implementation in the context of e-health, we emphasize the existence of crucial challenges, such as the incomplete representation of major stakeholders in the blockchain network, the lack of regulatory flexibility to ensure legal interoperability by country, and the insufficient integration of an official regulatory authority ensuring compliance with ethical and legal standards. To address these challenges, it is necessary to establish close collaboration between regulators, technology developers, and healthcare stakeholders.","sentences":["No one can dispute the disruptive impact of blockchain technology, which has long been considered one of the major revolutions of contemporary times.","Its integration into the healthcare ecosystem has helped overcome numerous difficulties and constraints faced by healthcare systems.","This has been notably demonstrated in the meticulous management of electronic health records (EHR) and their access rights, as well as in its capabilities in terms of security, scalability, flexibility, and interoperability with other systems.","This article undertakes the study and analysis of the most commonly adopted approaches in healthcare data management systems using blockchain technology.","An evaluation is then conducted based on a set of observed common characteristics, distinguishing one approach from the others.","The results of this analysis highlight the advantages and limitations of each approach, thus facilitating the choice of the method best suited to the readers' specific case study.","Furthermore, for effective implementation in the context of e-health, we emphasize the existence of crucial challenges, such as the incomplete representation of major stakeholders in the blockchain network, the lack of regulatory flexibility to ensure legal interoperability by country, and the insufficient integration of an official regulatory authority ensuring compliance with ethical and legal standards.","To address these challenges, it is necessary to establish close collaboration between regulators, technology developers, and healthcare stakeholders."],"url":"http://arxiv.org/abs/2401.03848v1"}
{"created":"2024-01-08 12:14:15","title":"Fully Attentional Networks with Self-emerging Token Labeling","abstract":"Recent studies indicate that Vision Transformers (ViTs) are robust against out-of-distribution scenarios. In particular, the Fully Attentional Network (FAN) - a family of ViT backbones, has achieved state-of-the-art robustness. In this paper, we revisit the FAN models and improve their pre-training with a self-emerging token labeling (STL) framework. Our method contains a two-stage training framework. Specifically, we first train a FAN token labeler (FAN-TL) to generate semantically meaningful patch token labels, followed by a FAN student model training stage that uses both the token labels and the original class label. With the proposed STL framework, our best model based on FAN-L-Hybrid (77.3M parameters) achieves 84.8% Top-1 accuracy and 42.1% mCE on ImageNet-1K and ImageNet-C, and sets a new state-of-the-art for ImageNet-A (46.1%) and ImageNet-R (56.6%) without using extra data, outperforming the original FAN counterpart by significant margins. The proposed framework also demonstrates significantly enhanced performance on downstream tasks such as semantic segmentation, with up to 1.7% improvement in robustness over the counterpart model. Code is available at https://github.com/NVlabs/STL.","sentences":["Recent studies indicate that Vision Transformers (ViTs) are robust against out-of-distribution scenarios.","In particular, the Fully Attentional Network (FAN) - a family of ViT backbones, has achieved state-of-the-art robustness.","In this paper, we revisit the FAN models and improve their pre-training with a self-emerging token labeling (STL) framework.","Our method contains a two-stage training framework.","Specifically, we first train a FAN token labeler (FAN-TL) to generate semantically meaningful patch token labels, followed by a FAN student model training stage that uses both the token labels and the original class label.","With the proposed STL framework, our best model based on FAN-L-Hybrid (77.3M parameters) achieves 84.8% Top-1 accuracy and 42.1% mCE on ImageNet-1K and ImageNet-C, and sets a new state-of-the-art for ImageNet-A (46.1%) and ImageNet-R (56.6%) without using extra data, outperforming the original FAN counterpart by significant margins.","The proposed framework also demonstrates significantly enhanced performance on downstream tasks such as semantic segmentation, with up to 1.7% improvement in robustness over the counterpart model.","Code is available at https://github.com/NVlabs/STL."],"url":"http://arxiv.org/abs/2401.03844v1"}
{"created":"2024-01-08 11:46:45","title":"Limitations of Data-Driven Spectral Reconstruction -- An Optics-Aware Analysis","abstract":"Hyperspectral imaging empowers computer vision systems with the distinct capability of identifying materials through recording their spectral signatures. Recent efforts in data-driven spectral reconstruction aim at extracting spectral information from RGB images captured by cost-effective RGB cameras, instead of dedicated hardware.   In this paper we systematically analyze the performance of such methods, evaluating both the practical limitations with respect to current datasets and overfitting, as well as fundamental limits with respect to the nature of the information encoded in the RGB images, and the dependency of this information on the optical system of the camera.   We find that the current models are not robust under slight variations, e.g., in noise level or compression of the RGB file. Both the methods and the datasets are also limited in their ability to cope with metameric colors. This issue can in part be overcome with metameric data augmentation. Moreover, optical lens aberrations can help to improve the encoding of the metameric information into the RGB image, which paves the road towards higher performing spectral imaging and reconstruction approaches.","sentences":["Hyperspectral imaging empowers computer vision systems with the distinct capability of identifying materials through recording their spectral signatures.","Recent efforts in data-driven spectral reconstruction aim at extracting spectral information from RGB images captured by cost-effective RGB cameras, instead of dedicated hardware.   ","In this paper we systematically analyze the performance of such methods, evaluating both the practical limitations with respect to current datasets and overfitting, as well as fundamental limits with respect to the nature of the information encoded in the RGB images, and the dependency of this information on the optical system of the camera.   ","We find that the current models are not robust under slight variations, e.g., in noise level or compression of the RGB file.","Both the methods and the datasets are also limited in their ability to cope with metameric colors.","This issue can in part be overcome with metameric data augmentation.","Moreover, optical lens aberrations can help to improve the encoding of the metameric information into the RGB image, which paves the road towards higher performing spectral imaging and reconstruction approaches."],"url":"http://arxiv.org/abs/2401.03835v1"}
{"created":"2024-01-08 11:43:03","title":"T-FREX: A Transformer-based Feature Extraction Method from Mobile App Reviews","abstract":"Mobile app reviews are a large-scale data source for software-related knowledge generation activities, including software maintenance, evolution and feedback analysis. Effective extraction of features (i.e., functionalities or characteristics) from these reviews is key to support analysis on the acceptance of these features, identification of relevant new feature requests and prioritization of feature development, among others. Traditional methods focus on syntactic pattern-based approaches, typically context-agnostic, evaluated on a closed set of apps, difficult to replicate and limited to a reduced set and domain of apps. Meanwhile, the pervasiveness of Large Language Models (LLMs) based on the Transformer architecture in software engineering tasks lays the groundwork for empirical evaluation of the performance of these models to support feature extraction. In this study, we present T-FREX, a Transformer-based, fully automatic approach for mobile app review feature extraction. First, we collect a set of ground truth features from users in a real crowdsourced software recommendation platform and transfer them automatically into a dataset of app reviews. Then, we use this newly created dataset to fine-tune multiple LLMs on a named entity recognition task under different data configurations. We assess the performance of T-FREX with respect to this ground truth, and we complement our analysis by comparing T-FREX with a baseline method from the field. Finally, we assess the quality of new features predicted by T-FREX through an external human evaluation. Results show that T-FREX outperforms on average the traditional syntactic-based method, especially when discovering new features from a domain for which the model has been fine-tuned.","sentences":["Mobile app reviews are a large-scale data source for software-related knowledge generation activities, including software maintenance, evolution and feedback analysis.","Effective extraction of features (i.e., functionalities or characteristics) from these reviews is key to support analysis on the acceptance of these features, identification of relevant new feature requests and prioritization of feature development, among others.","Traditional methods focus on syntactic pattern-based approaches, typically context-agnostic, evaluated on a closed set of apps, difficult to replicate and limited to a reduced set and domain of apps.","Meanwhile, the pervasiveness of Large Language Models (LLMs) based on the Transformer architecture in software engineering tasks lays the groundwork for empirical evaluation of the performance of these models to support feature extraction.","In this study, we present T-FREX, a Transformer-based, fully automatic approach for mobile app review feature extraction.","First, we collect a set of ground truth features from users in a real crowdsourced software recommendation platform and transfer them automatically into a dataset of app reviews.","Then, we use this newly created dataset to fine-tune multiple LLMs on a named entity recognition task under different data configurations.","We assess the performance of T-FREX with respect to this ground truth, and we complement our analysis by comparing T-FREX with a baseline method from the field.","Finally, we assess the quality of new features predicted by T-FREX through an external human evaluation.","Results show that T-FREX outperforms on average the traditional syntactic-based method, especially when discovering new features from a domain for which the model has been fine-tuned."],"url":"http://arxiv.org/abs/2401.03833v1"}
{"created":"2024-01-08 10:43:19","title":"TeleChat Technical Report","abstract":"In this technical report, we present TeleChat, a collection of large language models (LLMs) with parameters of 3 billion, 7 billion and 12 billion. It includes pretrained language models as well as fine-tuned chat models that is aligned with human preferences. TeleChat is initially pretrained on an extensive corpus containing a diverse collection of texts from both English and Chinese languages, including trillions of tokens. Subsequently, the model undergoes fine-tuning to align with human preferences, following a detailed methodology that we describe. We evaluate the performance of TeleChat on various tasks, including language understanding, mathematics, reasoning, code generation, and knowledge-based question answering. Our findings indicate that TeleChat achieves comparable performance to other open-source models of similar size across a wide range of public benchmarks. To support future research and applications utilizing LLMs, we release the fine-tuned model checkpoints of TeleChat's 7B and 12B variant, along with code and a portion of our pretraining data, to the public community.","sentences":["In this technical report, we present TeleChat, a collection of large language models (LLMs) with parameters of 3 billion, 7 billion and 12 billion.","It includes pretrained language models as well as fine-tuned chat models that is aligned with human preferences.","TeleChat is initially pretrained on an extensive corpus containing a diverse collection of texts from both English and Chinese languages, including trillions of tokens.","Subsequently, the model undergoes fine-tuning to align with human preferences, following a detailed methodology that we describe.","We evaluate the performance of TeleChat on various tasks, including language understanding, mathematics, reasoning, code generation, and knowledge-based question answering.","Our findings indicate that TeleChat achieves comparable performance to other open-source models of similar size across a wide range of public benchmarks.","To support future research and applications utilizing LLMs, we release the fine-tuned model checkpoints of TeleChat's 7B and 12B variant, along with code and a portion of our pretraining data, to the public community."],"url":"http://arxiv.org/abs/2401.03804v1"}
{"created":"2024-01-08 10:20:34","title":"Monitoring water contaminants in coastal areas through ML algorithms leveraging atmospherically corrected Sentinel-2 data","abstract":"Monitoring water contaminants is of paramount importance, ensuring public health and environmental well-being. Turbidity, a key parameter, poses a significant problem, affecting water quality. Its accurate assessment is crucial for safeguarding ecosystems and human consumption, demanding meticulous attention and action. For this, our study pioneers a novel approach to monitor the Turbidity contaminant, integrating CatBoost Machine Learning (ML) with high-resolution data from Sentinel-2 Level-2A. Traditional methods are labor-intensive while CatBoost offers an efficient solution, excelling in predictive accuracy. Leveraging atmospherically corrected Sentinel-2 data through the Google Earth Engine (GEE), our study contributes to scalable and precise Turbidity monitoring. A specific tabular dataset derived from Hong Kong contaminants monitoring stations enriches our study, providing region-specific insights. Results showcase the viability of this integrated approach, laying the foundation for adopting advanced techniques in global water quality management.","sentences":["Monitoring water contaminants is of paramount importance, ensuring public health and environmental well-being.","Turbidity, a key parameter, poses a significant problem, affecting water quality.","Its accurate assessment is crucial for safeguarding ecosystems and human consumption, demanding meticulous attention and action.","For this, our study pioneers a novel approach to monitor the Turbidity contaminant, integrating CatBoost Machine Learning (ML) with high-resolution data from Sentinel-2 Level-2A. Traditional methods are labor-intensive while CatBoost offers an efficient solution, excelling in predictive accuracy.","Leveraging atmospherically corrected Sentinel-2 data through the Google Earth Engine (GEE), our study contributes to scalable and precise Turbidity monitoring.","A specific tabular dataset derived from Hong Kong contaminants monitoring stations enriches our study, providing region-specific insights.","Results showcase the viability of this integrated approach, laying the foundation for adopting advanced techniques in global water quality management."],"url":"http://arxiv.org/abs/2401.03792v1"}
{"created":"2024-01-08 09:50:54","title":"NeRFmentation: NeRF-based Augmentation for Monocular Depth Estimation","abstract":"The capabilities of monocular depth estimation (MDE) models are limited by the availability of sufficient and diverse datasets. In the case of MDE models for autonomous driving, this issue is exacerbated by the linearity of the captured data trajectories. We propose a NeRF-based data augmentation pipeline to introduce synthetic data with more diverse viewing directions into training datasets and demonstrate the benefits of our approach to model performance and robustness. Our data augmentation pipeline, which we call \"NeRFmentation\", trains NeRFs on each scene in the dataset, filters out subpar NeRFs based on relevant metrics, and uses them to generate synthetic RGB-D images captured from new viewing directions. In this work, we apply our technique in conjunction with three state-of-the-art MDE architectures on the popular autonomous driving dataset KITTI, augmenting its training set of the Eigen split. We evaluate the resulting performance gain on the original test set, a separate popular driving set, and our own synthetic test set.","sentences":["The capabilities of monocular depth estimation (MDE) models are limited by the availability of sufficient and diverse datasets.","In the case of MDE models for autonomous driving, this issue is exacerbated by the linearity of the captured data trajectories.","We propose a NeRF-based data augmentation pipeline to introduce synthetic data with more diverse viewing directions into training datasets and demonstrate the benefits of our approach to model performance and robustness.","Our data augmentation pipeline, which we call \"NeRFmentation\", trains NeRFs on each scene in the dataset, filters out subpar NeRFs based on relevant metrics, and uses them to generate synthetic RGB-D images captured from new viewing directions.","In this work, we apply our technique in conjunction with three state-of-the-art MDE architectures on the popular autonomous driving dataset KITTI, augmenting its training set of the Eigen split.","We evaluate the resulting performance gain on the original test set, a separate popular driving set, and our own synthetic test set."],"url":"http://arxiv.org/abs/2401.03771v1"}
{"created":"2024-01-08 09:47:19","title":"Corn Yield Prediction Model with Deep Neural Networks for Smallholder Farmer Decision Support System","abstract":"Given the nonlinearity of the interaction between weather and soil variables, a novel deep neural network regressor (DNNR) was carefully designed with considerations to the depth, number of neurons of the hidden layers, and the hyperparameters with their optimizations. Additionally, a new metric, the average of absolute root squared error (ARSE) was proposed to address the shortcomings of root mean square error (RMSE) and mean absolute error (MAE) while combining their strengths. Using the ARSE metric, the random forest regressor (RFR) and the extreme gradient boosting regressor (XGBR), were compared with DNNR. The RFR and XGBR achieved yield errors of 0.0000294 t/ha, and 0.000792 t/ha, respectively, compared to the DNNR(s) which achieved 0.0146 t/ha and 0.0209 t/ha, respectively. All errors were impressively small. However, with changes to the explanatory variables to ensure generalizability to unforeseen data, DNNR(s) performed best. The unforeseen data, different from unseen data, is coined to represent sudden and unexplainable change to weather and soil variables due to climate change. Further analysis reveals that a strong interaction does exist between weather and soil variables. Using precipitation and silt, which are strong-negatively and strong-positively correlated with yield, respectively, yield was observed to increase when precipitation was reduced and silt increased, and vice-versa.","sentences":["Given the nonlinearity of the interaction between weather and soil variables, a novel deep neural network regressor (DNNR) was carefully designed with considerations to the depth, number of neurons of the hidden layers, and the hyperparameters with their optimizations.","Additionally, a new metric, the average of absolute root squared error (ARSE) was proposed to address the shortcomings of root mean square error (RMSE) and mean absolute error (MAE) while combining their strengths.","Using the ARSE metric, the random forest regressor (RFR) and the extreme gradient boosting regressor (XGBR), were compared with DNNR.","The RFR and XGBR achieved yield errors of 0.0000294 t/ha, and 0.000792 t/ha, respectively, compared to the DNNR(s) which achieved 0.0146 t/ha and 0.0209 t/ha, respectively.","All errors were impressively small.","However, with changes to the explanatory variables to ensure generalizability to unforeseen data, DNNR(s) performed best.","The unforeseen data, different from unseen data, is coined to represent sudden and unexplainable change to weather and soil variables due to climate change.","Further analysis reveals that a strong interaction does exist between weather and soil variables.","Using precipitation and silt, which are strong-negatively and strong-positively correlated with yield, respectively, yield was observed to increase when precipitation was reduced and silt increased, and vice-versa."],"url":"http://arxiv.org/abs/2401.03768v1"}
{"created":"2024-01-08 09:41:22","title":"InvariantOODG: Learning Invariant Features of Point Clouds for Out-of-Distribution Generalization","abstract":"The convenience of 3D sensors has led to an increase in the use of 3D point clouds in various applications. However, the differences in acquisition devices or scenarios lead to divergence in the data distribution of point clouds, which requires good generalization of point cloud representation learning methods. While most previous methods rely on domain adaptation, which involves fine-tuning pre-trained models on target domain data, this may not always be feasible in real-world scenarios where target domain data may be unavailable. To address this issue, we propose InvariantOODG, which learns invariability between point clouds with different distributions using a two-branch network to extract local-to-global features from original and augmented point clouds. Specifically, to enhance local feature learning of point clouds, we define a set of learnable anchor points that locate the most useful local regions and two types of transformations to augment the input point clouds. The experimental results demonstrate the effectiveness of the proposed model on 3D domain generalization benchmarks.","sentences":["The convenience of 3D sensors has led to an increase in the use of 3D point clouds in various applications.","However, the differences in acquisition devices or scenarios lead to divergence in the data distribution of point clouds, which requires good generalization of point cloud representation learning methods.","While most previous methods rely on domain adaptation, which involves fine-tuning pre-trained models on target domain data, this may not always be feasible in real-world scenarios where target domain data may be unavailable.","To address this issue, we propose InvariantOODG, which learns invariability between point clouds with different distributions using a two-branch network to extract local-to-global features from original and augmented point clouds.","Specifically, to enhance local feature learning of point clouds, we define a set of learnable anchor points that locate the most useful local regions and two types of transformations to augment the input point clouds.","The experimental results demonstrate the effectiveness of the proposed model on 3D domain generalization benchmarks."],"url":"http://arxiv.org/abs/2401.03765v1"}
{"created":"2024-01-08 09:36:42","title":"Range Reporting for Time Series via Rectangle Stabbing","abstract":"We study the Fr\\'echet queries problem. It is a data structure problem, where we are given a set $S$ of $n$ polygonal curves and a distance threshold $\\rho$. The data structure should support queries with a polygonal curve $q$ for the elements of $S$, for which the continuous Fr\\'echet distance to $q$ is at most $\\rho$. Afshani and Driemel in 2018 studied this problem for two-dimensional polygonal curves and gave upper and lower bounds on the space-query time tradeoff. We study the case that the ambient space of the curves is one-dimensional and show an intimate connection to the well-studied rectangle stabbing problem. Here, we are given a set of hyperrectangles as input and a query with a point $q$ should return all input rectangles that contain this point. Using known data structures for rectangle stabbing or orthogonal range searching this directly leads to a data structure with $\\mathcal{O}(n \\log ^{t-1} n)$ storage and $\\mathcal{O}(\\log^{t-1} n+k)$ query time, where $k$ denotes the output size and $t$ can be chosen as the maximum number of vertices of either (a) the stored curves or (b) the query curves. The resulting bounds improve upon the bounds by Afshani and Driemel in both the storage and query time. In addition, we show that known lower bounds for rectangle stabbing and orthogonal range reporting with dimension parameter $d= \\lfloor t/2 \\rfloor$ can be applied to our problem via reduction. .","sentences":["We study the Fr\\'echet queries problem.","It is a data structure problem, where we are given a set $S$ of $n$ polygonal curves and a distance threshold $\\rho$. The data structure should support queries with a polygonal curve $q$ for the elements of $S$, for which the continuous Fr\\'echet distance to $q$ is at most $\\rho$. Afshani and Driemel in 2018 studied this problem for two-dimensional polygonal curves and gave upper and lower bounds on the space-query time tradeoff.","We study the case that the ambient space of the curves is one-dimensional and show an intimate connection to the well-studied rectangle stabbing problem.","Here, we are given a set of hyperrectangles as input and a query with a point $q$ should return all input rectangles that contain this point.","Using known data structures for rectangle stabbing or orthogonal range searching this directly leads to a data structure with $\\mathcal{O}(n \\log ^{t-1} n)$ storage and $\\mathcal{O}(\\log^{t-1} n+k)$ query time, where $k$ denotes the output size and $t$ can be chosen as the maximum number of vertices of either (a) the stored curves or (b) the query curves.","The resulting bounds improve upon the bounds by Afshani and Driemel in both the storage and query time.","In addition, we show that known lower bounds for rectangle stabbing and orthogonal range reporting with dimension parameter $d= \\lfloor t/2 \\rfloor$ can be applied to our problem via reduction. ."],"url":"http://arxiv.org/abs/2401.03762v1"}
{"created":"2024-01-08 09:21:55","title":"How Participants Respond to Computer Delays","abstract":"Reaction time studies with computers investigate how and how quickly participants respond to changing sensory input. They promise simple and precise measurement of time and inputs and offer interesting insights into human behavior. However, several previous studies have discovered imprecisions in timing appearing as delays, depending on the browser, software and programming used for conducting such studies. Since the accuaracy of the collected data is widely discussed, we aim to provide new results on the effect of unintended delays on participants' behavior. For this purpose, a new reaction time study was conducted. Computer delays were added to the experiment to investigate their effects on participants' performance and repulsion. Minimal changes in participants' behavior did occur and should be furtherly investigated, as the power of this study was rather low and might not have uncovered all underlying effects. The following report details our study design and results and offers several suggestions for improvements in further studies.","sentences":["Reaction time studies with computers investigate how and how quickly participants respond to changing sensory input.","They promise simple and precise measurement of time and inputs and offer interesting insights into human behavior.","However, several previous studies have discovered imprecisions in timing appearing as delays, depending on the browser, software and programming used for conducting such studies.","Since the accuaracy of the collected data is widely discussed, we aim to provide new results on the effect of unintended delays on participants' behavior.","For this purpose, a new reaction time study was conducted.","Computer delays were added to the experiment to investigate their effects on participants' performance and repulsion.","Minimal changes in participants' behavior did occur and should be furtherly investigated, as the power of this study was rather low and might not have uncovered all underlying effects.","The following report details our study design and results and offers several suggestions for improvements in further studies."],"url":"http://arxiv.org/abs/2401.03751v1"}
{"created":"2024-01-08 09:20:46","title":"Flying Bird Object Detection Algorithm in Surveillance Video","abstract":"Aiming at the characteristics of the flying bird object in surveillance video, such as the single frame image feature is not obvious, the size is small in most cases, and asymmetric, this paper proposes a Flying Bird Object Detection method for Surveillance Video (FBOD-SV). Firstly, a new feature aggregation module, the Correlation Attention Feature Aggregation (Co-Attention-FA) module, is designed to aggregate the features of the flying bird object according to the bird object's correlation on multiple consecutive frames of images. Secondly, a Flying Bird Object Detection Network (FBOD-Net) with down-sampling and then up-sampling is designed, which uses a large feature layer that fuses fine spatial information and large receptive field information to detect special multi-scale (mostly small-scale) bird objects. Finally, the SimOTA dynamic label allocation method is applied to One-Category object detection, and the SimOTA-OC dynamic label strategy is proposed to solve the difficult problem of label allocation caused by irregular flying bird objects. In this paper, the algorithm's performance is verified by the experimental data set of the surveillance video of the flying bird object of the traction substation. The experimental results show that the surveillance video flying bird object detection method proposed in this paper effectively improves the detection performance of flying bird objects.","sentences":["Aiming at the characteristics of the flying bird object in surveillance video, such as the single frame image feature is not obvious, the size is small in most cases, and asymmetric, this paper proposes a Flying Bird Object Detection method for Surveillance Video (FBOD-SV).","Firstly, a new feature aggregation module, the Correlation Attention Feature Aggregation (Co-Attention-FA) module, is designed to aggregate the features of the flying bird object according to the bird object's correlation on multiple consecutive frames of images.","Secondly, a Flying Bird Object Detection Network (FBOD-Net) with down-sampling and then up-sampling is designed, which uses a large feature layer that fuses fine spatial information and large receptive field information to detect special multi-scale (mostly small-scale) bird objects.","Finally, the SimOTA dynamic label allocation method is applied to One-Category object detection, and the SimOTA-OC dynamic label strategy is proposed to solve the difficult problem of label allocation caused by irregular flying bird objects.","In this paper, the algorithm's performance is verified by the experimental data set of the surveillance video of the flying bird object of the traction substation.","The experimental results show that the surveillance video flying bird object detection method proposed in this paper effectively improves the detection performance of flying bird objects."],"url":"http://arxiv.org/abs/2401.03749v1"}
{"created":"2024-01-08 08:28:08","title":"The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance","abstract":"Large Language Models (LLMs) are regularly being used to label data across many domains and for myriad tasks. By simply asking the LLM for an answer, or ``prompting,'' practitioners are able to use LLMs to quickly get a response for an arbitrary task. This prompting is done through a series of decisions by the practitioner, from simple wording of the prompt, to requesting the output in a certain data format, to jailbreaking in the case of prompts that address more sensitive topics. In this work, we ask: do variations in the way a prompt is constructed change the ultimate decision of the LLM? We answer this using a series of prompt variations across a variety of text classification tasks. We find that even the smallest of perturbations, such as adding a space at the end of a prompt, can cause the LLM to change its answer. Further, we find that requesting responses in XML and commonly used jailbreaks can have cataclysmic effects on the data labeled by LLMs.","sentences":["Large Language Models (LLMs) are regularly being used to label data across many domains and for myriad tasks.","By simply asking the LLM for an answer, or ``prompting,'' practitioners are able to use LLMs to quickly get a response for an arbitrary task.","This prompting is done through a series of decisions by the practitioner, from simple wording of the prompt, to requesting the output in a certain data format, to jailbreaking in the case of prompts that address more sensitive topics.","In this work, we ask: do variations in the way a prompt is constructed change the ultimate decision of the LLM?","We answer this using a series of prompt variations across a variety of text classification tasks.","We find that even the smallest of perturbations, such as adding a space at the end of a prompt, can cause the LLM to change its answer.","Further, we find that requesting responses in XML and commonly used jailbreaks can have cataclysmic effects on the data labeled by LLMs."],"url":"http://arxiv.org/abs/2401.03729v1"}
{"created":"2024-01-08 08:10:37","title":"From Data to Insights: A Comprehensive Survey on Advanced Applications in Thyroid Cancer Research","abstract":"Thyroid cancer, the most prevalent endocrine cancer, has gained significant global attention due to its impact on public health. Extensive research efforts have been dedicated to leveraging artificial intelligence (AI) methods for the early detection of this disease, aiming to reduce its morbidity rates. However, a comprehensive understanding of the structured organization of research applications in this particular field remains elusive. To address this knowledge gap, we conducted a systematic review and developed a comprehensive taxonomy of machine learning-based applications in thyroid cancer pathogenesis, diagnosis, and prognosis. Our primary objective was to facilitate the research community's ability to stay abreast of technological advancements and potentially lead the emerging trends in this field. This survey presents a coherent literature review framework for interpreting the advanced techniques used in thyroid cancer research. A total of 758 related studies were identified and scrutinized. To the best of our knowledge, this is the first review that provides an in-depth analysis of the various aspects of AI applications employed in the context of thyroid cancer. Furthermore, we highlight key challenges encountered in this domain and propose future research opportunities for those interested in studying the latest trends or exploring less-investigated aspects of thyroid cancer research. By presenting this comprehensive review and taxonomy, we contribute to the existing knowledge in the field, while providing valuable insights for researchers, clinicians, and stakeholders in advancing the understanding and management of this disease.","sentences":["Thyroid cancer, the most prevalent endocrine cancer, has gained significant global attention due to its impact on public health.","Extensive research efforts have been dedicated to leveraging artificial intelligence (AI) methods for the early detection of this disease, aiming to reduce its morbidity rates.","However, a comprehensive understanding of the structured organization of research applications in this particular field remains elusive.","To address this knowledge gap, we conducted a systematic review and developed a comprehensive taxonomy of machine learning-based applications in thyroid cancer pathogenesis, diagnosis, and prognosis.","Our primary objective was to facilitate the research community's ability to stay abreast of technological advancements and potentially lead the emerging trends in this field.","This survey presents a coherent literature review framework for interpreting the advanced techniques used in thyroid cancer research.","A total of 758 related studies were identified and scrutinized.","To the best of our knowledge, this is the first review that provides an in-depth analysis of the various aspects of AI applications employed in the context of thyroid cancer.","Furthermore, we highlight key challenges encountered in this domain and propose future research opportunities for those interested in studying the latest trends or exploring less-investigated aspects of thyroid cancer research.","By presenting this comprehensive review and taxonomy, we contribute to the existing knowledge in the field, while providing valuable insights for researchers, clinicians, and stakeholders in advancing the understanding and management of this disease."],"url":"http://arxiv.org/abs/2401.03722v1"}
{"created":"2024-01-08 08:05:34","title":"Enhancing Adaptive History Reserving by Spiking Convolutional Block Attention Module in Recurrent Neural Networks","abstract":"Spiking neural networks (SNNs) serve as one type of efficient model to process spatio-temporal patterns in time series, such as the Address-Event Representation data collected from Dynamic Vision Sensor (DVS). Although convolutional SNNs have achieved remarkable performance on these AER datasets, benefiting from the predominant spatial feature extraction ability of convolutional structure, they ignore temporal features related to sequential time points. In this paper, we develop a recurrent spiking neural network (RSNN) model embedded with an advanced spiking convolutional block attention module (SCBAM) component to combine both spatial and temporal features of spatio-temporal patterns. It invokes the history information in spatial and temporal channels adaptively through SCBAM, which brings the advantages of efficient memory calling and history redundancy elimination. The performance of our model was evaluated in DVS128-Gesture dataset and other time-series datasets. The experimental results show that the proposed SRNN-SCBAM model makes better use of the history information in spatial and temporal dimensions with less memory space, and achieves higher accuracy compared to other models.","sentences":["Spiking neural networks (SNNs) serve as one type of efficient model to process spatio-temporal patterns in time series, such as the Address-Event Representation data collected from Dynamic Vision Sensor (DVS).","Although convolutional SNNs have achieved remarkable performance on these AER datasets, benefiting from the predominant spatial feature extraction ability of convolutional structure, they ignore temporal features related to sequential time points.","In this paper, we develop a recurrent spiking neural network (RSNN) model embedded with an advanced spiking convolutional block attention module (SCBAM) component to combine both spatial and temporal features of spatio-temporal patterns.","It invokes the history information in spatial and temporal channels adaptively through SCBAM, which brings the advantages of efficient memory calling and history redundancy elimination.","The performance of our model was evaluated in DVS128-Gesture dataset and other time-series datasets.","The experimental results show that the proposed SRNN-SCBAM model makes better use of the history information in spatial and temporal dimensions with less memory space, and achieves higher accuracy compared to other models."],"url":"http://arxiv.org/abs/2401.03719v1"}
{"created":"2024-01-08 08:00:04","title":"Universal Time-Series Representation Learning: A Survey","abstract":"Time-series data exists in every corner of real-world systems and services, ranging from satellites in the sky to wearable devices on human bodies. Learning representations by extracting and inferring valuable information from these time series is crucial for understanding the complex dynamics of particular phenomena and enabling informed decisions. With the learned representations, we can perform numerous downstream analyses more effectively. Among several approaches, deep learning has demonstrated remarkable performance in extracting hidden patterns and features from time-series data without manual feature engineering. This survey first presents a novel taxonomy based on three fundamental elements in designing state-of-the-art universal representation learning methods for time series. According to the proposed taxonomy, we comprehensively review existing studies and discuss their intuitions and insights into how these methods enhance the quality of learned representations. Finally, as a guideline for future studies, we summarize commonly used experimental setups and datasets and discuss several promising research directions. An up-to-date corresponding resource is available at https://github.com/itouchz/awesome-deep-time-series-representations.","sentences":["Time-series data exists in every corner of real-world systems and services, ranging from satellites in the sky to wearable devices on human bodies.","Learning representations by extracting and inferring valuable information from these time series is crucial for understanding the complex dynamics of particular phenomena and enabling informed decisions.","With the learned representations, we can perform numerous downstream analyses more effectively.","Among several approaches, deep learning has demonstrated remarkable performance in extracting hidden patterns and features from time-series data without manual feature engineering.","This survey first presents a novel taxonomy based on three fundamental elements in designing state-of-the-art universal representation learning methods for time series.","According to the proposed taxonomy, we comprehensively review existing studies and discuss their intuitions and insights into how these methods enhance the quality of learned representations.","Finally, as a guideline for future studies, we summarize commonly used experimental setups and datasets and discuss several promising research directions.","An up-to-date corresponding resource is available at https://github.com/itouchz/awesome-deep-time-series-representations."],"url":"http://arxiv.org/abs/2401.03717v1"}
{"created":"2024-01-08 06:18:46","title":"Logits Poisoning Attack in Federated Distillation","abstract":"Federated Distillation (FD) is a novel and promising distributed machine learning paradigm, where knowledge distillation is leveraged to facilitate a more efficient and flexible cross-device knowledge transfer in federated learning. By optimizing local models with knowledge distillation, FD circumvents the necessity of uploading large-scale model parameters to the central server, simultaneously preserving the raw data on local clients. Despite the growing popularity of FD, there is a noticeable gap in previous works concerning the exploration of poisoning attacks within this framework. This can lead to a scant understanding of the vulnerabilities to potential adversarial actions. To this end, we introduce FDLA, a poisoning attack method tailored for FD. FDLA manipulates logit communications in FD, aiming to significantly degrade model performance on clients through misleading the discrimination of private samples. Through extensive simulation experiments across a variety of datasets, attack scenarios, and FD configurations, we demonstrate that LPA effectively compromises client model accuracy, outperforming established baseline algorithms in this regard. Our findings underscore the critical need for robust defense mechanisms in FD settings to mitigate such adversarial threats.","sentences":["Federated Distillation (FD) is a novel and promising distributed machine learning paradigm, where knowledge distillation is leveraged to facilitate a more efficient and flexible cross-device knowledge transfer in federated learning.","By optimizing local models with knowledge distillation, FD circumvents the necessity of uploading large-scale model parameters to the central server, simultaneously preserving the raw data on local clients.","Despite the growing popularity of FD, there is a noticeable gap in previous works concerning the exploration of poisoning attacks within this framework.","This can lead to a scant understanding of the vulnerabilities to potential adversarial actions.","To this end, we introduce FDLA, a poisoning attack method tailored for FD.","FDLA manipulates logit communications in FD, aiming to significantly degrade model performance on clients through misleading the discrimination of private samples.","Through extensive simulation experiments across a variety of datasets, attack scenarios, and FD configurations, we demonstrate that LPA effectively compromises client model accuracy, outperforming established baseline algorithms in this regard.","Our findings underscore the critical need for robust defense mechanisms in FD settings to mitigate such adversarial threats."],"url":"http://arxiv.org/abs/2401.03685v1"}
{"created":"2024-01-08 05:46:43","title":"Comparing discriminating abilities of evaluation metrics in link prediction","abstract":"Link prediction aims to predict the potential existence of links between two unconnected nodes within a network based on the known topological characteristics. Evaluation metrics are used to assess the effectiveness of algorithms in link prediction. The discriminating ability of these evaluation metrics is vitally important for accurately evaluating link prediction algorithms. In this study, we propose an artificial network model, based on which one can adjust a single parameter to monotonically and continuously turn the prediction accuracy of the specifically designed link prediction algorithm. Building upon this foundation, we show a framework to depict the effectiveness of evaluating metrics by focusing on their discriminating ability. Specifically, a quantitative comparison in the abilities of correctly discerning varying prediction accuracies was conducted encompassing nine evaluation metrics: Precision, Recall, F1-Measure, Matthews Correlation Coefficient (MCC), Balanced Precision (BP), the Area Under the receiver operating characteristic Curve (AUC), the Area Under the Precision-Recall curve (AUPR), Normalized Discounted Cumulative Gain (NDCG), and the Area Under the magnified ROC (AUC-mROC). The results indicate that the discriminating abilities of the three metrics, AUC, AUPR, and NDCG, are significantly higher than those of other metrics.","sentences":["Link prediction aims to predict the potential existence of links between two unconnected nodes within a network based on the known topological characteristics.","Evaluation metrics are used to assess the effectiveness of algorithms in link prediction.","The discriminating ability of these evaluation metrics is vitally important for accurately evaluating link prediction algorithms.","In this study, we propose an artificial network model, based on which one can adjust a single parameter to monotonically and continuously turn the prediction accuracy of the specifically designed link prediction algorithm.","Building upon this foundation, we show a framework to depict the effectiveness of evaluating metrics by focusing on their discriminating ability.","Specifically, a quantitative comparison in the abilities of correctly discerning varying prediction accuracies was conducted encompassing nine evaluation metrics: Precision, Recall, F1-Measure, Matthews Correlation Coefficient (MCC), Balanced Precision (BP), the Area Under the receiver operating characteristic Curve (AUC), the Area Under the Precision-Recall curve (AUPR), Normalized Discounted Cumulative Gain (NDCG), and the Area Under the magnified ROC (AUC-mROC).","The results indicate that the discriminating abilities of the three metrics, AUC, AUPR, and NDCG, are significantly higher than those of other metrics."],"url":"http://arxiv.org/abs/2401.03673v1"}
{"created":"2024-01-08 05:36:48","title":"Receiver-Oriented Cheap Talk Design","abstract":"This paper considers the dynamics of cheap talk interactions between a sender and receiver, departing from conventional models by focusing on the receiver's perspective. We study two models, one with transparent motives and another one in which the receiver can \\emph{filter} the information that is accessible by the sender. We give a geometric characterization of the best receiver equilibrium under transparent motives and prove that the receiver does not benefit from filtering information in this case. However, in general, we show that the receiver can strictly benefit from filtering and provide efficient algorithms for computing optimal equilibria. This innovative analysis aligns with user-based platforms where receivers (users) control information accessible to senders (sellers). Our findings provide insights into communication dynamics, leveling the sender's inherent advantage, and offering strategic interaction predictions.","sentences":["This paper considers the dynamics of cheap talk interactions between a sender and receiver, departing from conventional models by focusing on the receiver's perspective.","We study two models, one with transparent motives and another one in which the receiver can \\emph{filter} the information that is accessible by the sender.","We give a geometric characterization of the best receiver equilibrium under transparent motives and prove that the receiver does not benefit from filtering information in this case.","However, in general, we show that the receiver can strictly benefit from filtering and provide efficient algorithms for computing optimal equilibria.","This innovative analysis aligns with user-based platforms where receivers (users) control information accessible to senders (sellers).","Our findings provide insights into communication dynamics, leveling the sender's inherent advantage, and offering strategic interaction predictions."],"url":"http://arxiv.org/abs/2401.03671v1"}
{"created":"2024-01-08 04:37:35","title":"Primitive Geometry Segment Pre-training for 3D Medical Image Segmentation","abstract":"The construction of 3D medical image datasets presents several issues, including requiring significant financial costs in data collection and specialized expertise for annotation, as well as strict privacy concerns for patient confidentiality compared to natural image datasets. Therefore, it has become a pressing issue in 3D medical image segmentation to enable data-efficient learning with limited 3D medical data and supervision. A promising approach is pre-training, but improving its performance in 3D medical image segmentation is difficult due to the small size of existing 3D medical image datasets. We thus present the Primitive Geometry Segment Pre-training (PrimGeoSeg) method to enable the learning of 3D semantic features by pre-training segmentation tasks using only primitive geometric objects for 3D medical image segmentation. PrimGeoSeg performs more accurate and efficient 3D medical image segmentation without manual data collection and annotation. Further, experimental results show that PrimGeoSeg on SwinUNETR improves performance over learning from scratch on BTCV, MSD (Task06), and BraTS datasets by 3.7%, 4.4%, and 0.3%, respectively. Remarkably, the performance was equal to or better than state-of-the-art self-supervised learning despite the equal number of pre-training data. From experimental results, we conclude that effective pre-training can be achieved by looking at primitive geometric objects only. Code and dataset are available at https://github.com/SUPER-TADORY/PrimGeoSeg.","sentences":["The construction of 3D medical image datasets presents several issues, including requiring significant financial costs in data collection and specialized expertise for annotation, as well as strict privacy concerns for patient confidentiality compared to natural image datasets.","Therefore, it has become a pressing issue in 3D medical image segmentation to enable data-efficient learning with limited 3D medical data and supervision.","A promising approach is pre-training, but improving its performance in 3D medical image segmentation is difficult due to the small size of existing 3D medical image datasets.","We thus present the Primitive Geometry Segment Pre-training (PrimGeoSeg) method to enable the learning of 3D semantic features by pre-training segmentation tasks using only primitive geometric objects for 3D medical image segmentation.","PrimGeoSeg performs more accurate and efficient 3D medical image segmentation without manual data collection and annotation.","Further, experimental results show that PrimGeoSeg on SwinUNETR improves performance over learning from scratch on BTCV, MSD (Task06), and BraTS datasets by 3.7%, 4.4%, and 0.3%, respectively.","Remarkably, the performance was equal to or better than state-of-the-art self-supervised learning despite the equal number of pre-training data.","From experimental results, we conclude that effective pre-training can be achieved by looking at primitive geometric objects only.","Code and dataset are available at https://github.com/SUPER-TADORY/PrimGeoSeg."],"url":"http://arxiv.org/abs/2401.03665v1"}
{"created":"2024-01-08 03:37:43","title":"Reproducibility Analysis and Enhancements for Multi-Aspect Dense Retriever with Aspect Learning","abstract":"Multi-aspect dense retrieval aims to incorporate aspect information (e.g., brand and category) into dual encoders to facilitate relevance matching. As an early and representative multi-aspect dense retriever, MADRAL learns several extra aspect embeddings and fuses the explicit aspects with an implicit aspect \"OTHER\" for final representation. MADRAL was evaluated on proprietary data and its code was not released, making it challenging to validate its effectiveness on other datasets. We failed to reproduce its effectiveness on the public MA-Amazon data, motivating us to probe the reasons and re-examine its components. We propose several component alternatives for comparisons, including replacing \"OTHER\" with \"CLS\" and representing aspects with the first several content tokens. Through extensive experiments, we confirm that learning \"OTHER\" from scratch in aspect fusion is harmful. In contrast, our proposed variants can greatly enhance the retrieval performance. Our research not only sheds light on the limitations of MADRAL but also provides valuable insights for future studies on more powerful multi-aspect dense retrieval models. Code will be released at: https://github.com/sunxiaojie99/Reproducibility-for-MADRAL.","sentences":["Multi-aspect dense retrieval aims to incorporate aspect information (e.g., brand and category) into dual encoders to facilitate relevance matching.","As an early and representative multi-aspect dense retriever, MADRAL learns several extra aspect embeddings and fuses the explicit aspects with an implicit aspect \"OTHER\" for final representation.","MADRAL was evaluated on proprietary data and its code was not released, making it challenging to validate its effectiveness on other datasets.","We failed to reproduce its effectiveness on the public MA-Amazon data, motivating us to probe the reasons and re-examine its components.","We propose several component alternatives for comparisons, including replacing \"OTHER\" with \"CLS\" and representing aspects with the first several content tokens.","Through extensive experiments, we confirm that learning \"OTHER\" from scratch in aspect fusion is harmful.","In contrast, our proposed variants can greatly enhance the retrieval performance.","Our research not only sheds light on the limitations of MADRAL but also provides valuable insights for future studies on more powerful multi-aspect dense retrieval models.","Code will be released at: https://github.com/sunxiaojie99/Reproducibility-for-MADRAL."],"url":"http://arxiv.org/abs/2401.03648v1"}
{"created":"2024-01-08 02:49:16","title":"Unifying Graph Contrastive Learning via Graph Message Augmentation","abstract":"Graph contrastive learning is usually performed by first conducting Graph Data Augmentation (GDA) and then employing a contrastive learning pipeline to train GNNs. As we know that GDA is an important issue for graph contrastive learning. Various GDAs have been developed recently which mainly involve dropping or perturbing edges, nodes, node attributes and edge attributes. However, to our knowledge, it still lacks a universal and effective augmentor that is suitable for different types of graph data. To address this issue, in this paper, we first introduce the graph message representation of graph data. Based on it, we then propose a novel Graph Message Augmentation (GMA), a universal scheme for reformulating many existing GDAs. The proposed unified GMA not only gives a new perspective to understand many existing GDAs but also provides a universal and more effective graph data augmentation for graph self-supervised learning tasks. Moreover, GMA introduces an easy way to implement the mixup augmentor which is natural for images but usually challengeable for graphs. Based on the proposed GMA, we then propose a unified graph contrastive learning, termed Graph Message Contrastive Learning (GMCL), that employs attribution-guided universal GMA for graph contrastive learning. Experiments on many graph learning tasks demonstrate the effectiveness and benefits of the proposed GMA and GMCL approaches.","sentences":["Graph contrastive learning is usually performed by first conducting Graph Data Augmentation (GDA) and then employing a contrastive learning pipeline to train GNNs.","As we know that GDA is an important issue for graph contrastive learning.","Various GDAs have been developed recently which mainly involve dropping or perturbing edges, nodes, node attributes and edge attributes.","However, to our knowledge, it still lacks a universal and effective augmentor that is suitable for different types of graph data.","To address this issue, in this paper, we first introduce the graph message representation of graph data.","Based on it, we then propose a novel Graph Message Augmentation (GMA), a universal scheme for reformulating many existing GDAs.","The proposed unified GMA not only gives a new perspective to understand many existing GDAs but also provides a universal and more effective graph data augmentation for graph self-supervised learning tasks.","Moreover, GMA introduces an easy way to implement the mixup augmentor which is natural for images but usually challengeable for graphs.","Based on the proposed GMA, we then propose a unified graph contrastive learning, termed Graph Message Contrastive Learning (GMCL), that employs attribution-guided universal GMA for graph contrastive learning.","Experiments on many graph learning tasks demonstrate the effectiveness and benefits of the proposed GMA and GMCL approaches."],"url":"http://arxiv.org/abs/2401.03638v1"}
{"created":"2024-01-08 02:17:09","title":"DDM-Lag : A Diffusion-based Decision-making Model for Autonomous Vehicles with Lagrangian Safety Enhancement","abstract":"Decision-making stands as a pivotal component in the realm of autonomous vehicles (AVs), playing a crucial role in navigating the intricacies of autonomous driving. Amidst the evolving landscape of data-driven methodologies, enhancing decision-making performance in complex scenarios has emerged as a prominent research focus. Despite considerable advancements, current learning-based decision-making approaches exhibit potential for refinement, particularly in aspects of policy articulation and safety assurance. To address these challenges, we introduce DDM-Lag, a Diffusion Decision Model,augmented with Lagrangian-based safety enhancements.In our approach, the autonomous driving decision-making conundrum is conceptualized as a Constrained Markov Decision Process (CMDP). We have crafted an Actor-Critic framework, wherein the diffusion model is employed as the actor,facilitating policy exploration and learning. The integration of safety constraints in the CMDP and the adoption of a Lagrangian relaxation-based policy optimization technique ensure enhanced decision safety. A PID controller is employed for the stable updating of model parameters. The effectiveness of DDM-Lag is evaluated through different driving tasks, showcasing improvements in decision-making safety and overall performance compared to baselines.","sentences":["Decision-making stands as a pivotal component in the realm of autonomous vehicles (AVs), playing a crucial role in navigating the intricacies of autonomous driving.","Amidst the evolving landscape of data-driven methodologies, enhancing decision-making performance in complex scenarios has emerged as a prominent research focus.","Despite considerable advancements, current learning-based decision-making approaches exhibit potential for refinement, particularly in aspects of policy articulation and safety assurance.","To address these challenges, we introduce DDM-Lag, a Diffusion Decision Model,augmented with Lagrangian-based safety enhancements.","In our approach, the autonomous driving decision-making conundrum is conceptualized as a Constrained Markov Decision Process (CMDP).","We have crafted an Actor-Critic framework, wherein the diffusion model is employed as the actor,facilitating policy exploration and learning.","The integration of safety constraints in the CMDP and the adoption of a Lagrangian relaxation-based policy optimization technique ensure enhanced decision safety.","A PID controller is employed for the stable updating of model parameters.","The effectiveness of DDM-Lag is evaluated through different driving tasks, showcasing improvements in decision-making safety and overall performance compared to baselines."],"url":"http://arxiv.org/abs/2401.03629v1"}
{"created":"2024-01-07 23:45:01","title":"Multi-Modal Federated Learning for Cancer Staging over Non-IID Datasets with Unbalanced Modalities","abstract":"The use of machine learning (ML) for cancer staging through medical image analysis has gained substantial interest across medical disciplines. When accompanied by the innovative federated learning (FL) framework, ML techniques can further overcome privacy concerns related to patient data exposure. Given the frequent presence of diverse data modalities within patient records, leveraging FL in a multi-modal learning framework holds considerable promise for cancer staging. However, existing works on multi-modal FL often presume that all data-collecting institutions have access to all data modalities. This oversimplified approach neglects institutions that have access to only a portion of data modalities within the system. In this work, we introduce a novel FL architecture designed to accommodate not only the heterogeneity of data samples, but also the inherent heterogeneity/non-uniformity of data modalities across institutions. We shed light on the challenges associated with varying convergence speeds observed across different data modalities within our FL system. Subsequently, we propose a solution to tackle these challenges by devising a distributed gradient blending and proximity-aware client weighting strategy tailored for multi-modal FL. To show the superiority of our method, we conduct experiments using The Cancer Genome Atlas program (TCGA) datalake considering different cancer types and three modalities of data: mRNA sequences, histopathological image data, and clinical information.","sentences":["The use of machine learning (ML) for cancer staging through medical image analysis has gained substantial interest across medical disciplines.","When accompanied by the innovative federated learning (FL) framework, ML techniques can further overcome privacy concerns related to patient data exposure.","Given the frequent presence of diverse data modalities within patient records, leveraging FL in a multi-modal learning framework holds considerable promise for cancer staging.","However, existing works on multi-modal FL often presume that all data-collecting institutions have access to all data modalities.","This oversimplified approach neglects institutions that have access to only a portion of data modalities within the system.","In this work, we introduce a novel FL architecture designed to accommodate not only the heterogeneity of data samples, but also the inherent heterogeneity/non-uniformity of data modalities across institutions.","We shed light on the challenges associated with varying convergence speeds observed across different data modalities within our FL system.","Subsequently, we propose a solution to tackle these challenges by devising a distributed gradient blending and proximity-aware client weighting strategy tailored for multi-modal FL.","To show the superiority of our method, we conduct experiments using The Cancer Genome Atlas program (TCGA) datalake considering different cancer types and three modalities of data: mRNA sequences, histopathological image data, and clinical information."],"url":"http://arxiv.org/abs/2401.03609v1"}
{"created":"2024-01-07 23:13:51","title":"Amirkabir campus dataset: Real-world challenges and scenarios of Visual Inertial Odometry (VIO) for visually impaired people","abstract":"Visual Inertial Odometry (VIO) algorithms estimate the accurate camera trajectory by using camera and Inertial Measurement Unit (IMU) sensors. The applications of VIO span a diverse range, including augmented reality and indoor navigation. VIO algorithms hold the potential to facilitate navigation for visually impaired individuals in both indoor and outdoor settings. Nevertheless, state-of-the-art VIO algorithms encounter substantial challenges in dynamic environments, particularly in densely populated corridors. Existing VIO datasets, e.g., ADVIO, typically fail to effectively exploit these challenges. In this paper, we introduce the Amirkabir campus dataset (AUT-VI) to address the mentioned problem and improve the navigation systems. AUT-VI is a novel and super-challenging dataset with 126 diverse sequences in 17 different locations. This dataset contains dynamic objects, challenging loop-closure/map-reuse, different lighting conditions, reflections, and sudden camera movements to cover all extreme navigation scenarios. Moreover, in support of ongoing development efforts, we have released the Android application for data capture to the public. This allows fellow researchers to easily capture their customized VIO dataset variations. In addition, we evaluate state-of-the-art Visual Inertial Odometry (VIO) and Visual Odometry (VO) methods on our dataset, emphasizing the essential need for this challenging dataset.","sentences":["Visual Inertial Odometry (VIO) algorithms estimate the accurate camera trajectory by using camera and Inertial Measurement Unit (IMU) sensors.","The applications of VIO span a diverse range, including augmented reality and indoor navigation.","VIO algorithms hold the potential to facilitate navigation for visually impaired individuals in both indoor and outdoor settings.","Nevertheless, state-of-the-art VIO algorithms encounter substantial challenges in dynamic environments, particularly in densely populated corridors.","Existing VIO datasets, e.g., ADVIO, typically fail to effectively exploit these challenges.","In this paper, we introduce the Amirkabir campus dataset (AUT-VI) to address the mentioned problem and improve the navigation systems.","AUT-VI is a novel and super-challenging dataset with 126 diverse sequences in 17 different locations.","This dataset contains dynamic objects, challenging loop-closure/map-reuse, different lighting conditions, reflections, and sudden camera movements to cover all extreme navigation scenarios.","Moreover, in support of ongoing development efforts, we have released the Android application for data capture to the public.","This allows fellow researchers to easily capture their customized VIO dataset variations.","In addition, we evaluate state-of-the-art Visual Inertial Odometry (VIO) and Visual Odometry (VO) methods on our dataset, emphasizing the essential need for this challenging dataset."],"url":"http://arxiv.org/abs/2401.03604v1"}
{"created":"2024-01-07 22:47:38","title":"Few-Shot Causal Representation Learning for Out-of-Distribution Generalization on Heterogeneous Graphs","abstract":"Heterogeneous graph few-shot learning (HGFL) has been developed to address the label sparsity issue in heterogeneous graphs (HGs), which consist of various types of nodes and edges. The core concept of HGFL is to extract knowledge from rich-labeled classes in a source HG, transfer this knowledge to a target HG to facilitate learning new classes with few-labeled training data, and finally make predictions on unlabeled testing data. Existing methods typically assume that the source HG, training data, and testing data all share the same distribution. However, in practice, distribution shifts among these three types of data are inevitable due to two reasons: (1) the limited availability of the source HG that matches the target HG distribution, and (2) the unpredictable data generation mechanism of the target HG. Such distribution shifts result in ineffective knowledge transfer and poor learning performance in existing methods, thereby leading to a novel problem of out-of-distribution (OOD) generalization in HGFL. To address this challenging problem, we propose a novel Causal OOD Heterogeneous graph Few-shot learning model, namely COHF. In COHF, we first characterize distribution shifts in HGs with a structural causal model, establishing an invariance principle for OOD generalization in HGFL. Then, following this invariance principle, we propose a new variational autoencoder-based heterogeneous graph neural network to mitigate the impact of distribution shifts. Finally, by integrating this network with a novel meta-learning framework, COHF effectively transfers knowledge to the target HG to predict new classes with few-labeled data. Extensive experiments on seven real-world datasets have demonstrated the superior performance of COHF over the state-of-the-art methods.","sentences":["Heterogeneous graph few-shot learning (HGFL) has been developed to address the label sparsity issue in heterogeneous graphs (HGs), which consist of various types of nodes and edges.","The core concept of HGFL is to extract knowledge from rich-labeled classes in a source HG, transfer this knowledge to a target HG to facilitate learning new classes with few-labeled training data, and finally make predictions on unlabeled testing data.","Existing methods typically assume that the source HG, training data, and testing data all share the same distribution.","However, in practice, distribution shifts among these three types of data are inevitable due to two reasons: (1) the limited availability of the source HG that matches the target HG distribution, and (2) the unpredictable data generation mechanism of the target HG.","Such distribution shifts result in ineffective knowledge transfer and poor learning performance in existing methods, thereby leading to a novel problem of out-of-distribution (OOD) generalization in HGFL.","To address this challenging problem, we propose a novel Causal OOD Heterogeneous graph Few-shot learning model, namely COHF.","In COHF, we first characterize distribution shifts in HGs with a structural causal model, establishing an invariance principle for OOD generalization in HGFL.","Then, following this invariance principle, we propose a new variational autoencoder-based heterogeneous graph neural network to mitigate the impact of distribution shifts.","Finally, by integrating this network with a novel meta-learning framework, COHF effectively transfers knowledge to the target HG to predict new classes with few-labeled data.","Extensive experiments on seven real-world datasets have demonstrated the superior performance of COHF over the state-of-the-art methods."],"url":"http://arxiv.org/abs/2401.03597v1"}
{"created":"2024-01-07 21:50:24","title":"Big Data and Deep Learning in Smart Cities: A Comprehensive Dataset for AI-Driven Traffic Accident Detection and Computer Vision Systems","abstract":"In the dynamic urban landscape, where the interplay of vehicles and pedestrians defines the rhythm of life, integrating advanced technology for safety and efficiency is increasingly crucial. This study delves into the application of cutting-edge technological methods in smart cities, focusing on enhancing public safety through improved traffic accident detection. Action recognition plays a pivotal role in interpreting visual data and tracking object motion such as human pose estimation in video sequences. The challenges of action recognition include variability in rapid actions, limited dataset, and environmental factors such as (Weather, Illumination, and Occlusions). In this paper, we present a novel comprehensive dataset for traffic accident detection. This datasets is specifically designed to bolster computer vision and action recognition systems in predicting and detecting road traffic accidents. We integrated datasets from wide variety of data sources, road networks, weather conditions, and regions across the globe. This approach is underpinned by empirical studies, aiming to contribute to the discourse on how technology can enhance the quality of life in densely populated areas. This research aims to bridge existing research gaps by introducing benchmark datasets that leverage state-of-the-art algorithms tailored for traffic accident detection in smart cities. These dataset is expected to advance academic research and also enhance real-time accident detection applications, contributing significantly to the evolution of smart urban environments. Our study marks a pivotal step towards safer, more efficient smart cities, harnessing the power of AI and machine learning to transform urban living.","sentences":["In the dynamic urban landscape, where the interplay of vehicles and pedestrians defines the rhythm of life, integrating advanced technology for safety and efficiency is increasingly crucial.","This study delves into the application of cutting-edge technological methods in smart cities, focusing on enhancing public safety through improved traffic accident detection.","Action recognition plays a pivotal role in interpreting visual data and tracking object motion such as human pose estimation in video sequences.","The challenges of action recognition include variability in rapid actions, limited dataset, and environmental factors such as (Weather, Illumination, and Occlusions).","In this paper, we present a novel comprehensive dataset for traffic accident detection.","This datasets is specifically designed to bolster computer vision and action recognition systems in predicting and detecting road traffic accidents.","We integrated datasets from wide variety of data sources, road networks, weather conditions, and regions across the globe.","This approach is underpinned by empirical studies, aiming to contribute to the discourse on how technology can enhance the quality of life in densely populated areas.","This research aims to bridge existing research gaps by introducing benchmark datasets that leverage state-of-the-art algorithms tailored for traffic accident detection in smart cities.","These dataset is expected to advance academic research and also enhance real-time accident detection applications, contributing significantly to the evolution of smart urban environments.","Our study marks a pivotal step towards safer, more efficient smart cities, harnessing the power of AI and machine learning to transform urban living."],"url":"http://arxiv.org/abs/2401.03587v1"}
{"created":"2024-01-07 20:08:17","title":"Involution Fused ConvNet for Classifying Eye-Tracking Patterns of Children with Autism Spectrum Disorder","abstract":"Autism Spectrum Disorder (ASD) is a complicated neurological condition which is challenging to diagnose. Numerous studies demonstrate that children diagnosed with autism struggle with maintaining attention spans and have less focused vision. The eye-tracking technology has drawn special attention in the context of ASD since anomalies in gaze have long been acknowledged as a defining feature of autism in general. Deep Learning (DL) approaches coupled with eye-tracking sensors are exploiting additional capabilities to advance the diagnostic and its applications. By learning intricate nonlinear input-output relations, DL can accurately recognize the various gaze and eye-tracking patterns and adjust to the data. Convolutions alone are insufficient to capture the important spatial information in gaze patterns or eye tracking. The dynamic kernel-based process known as involutions can improve the efficiency of classifying gaze patterns or eye tracking data. In this paper, we utilise two different image-processing operations to see how these processes learn eye-tracking patterns. Since these patterns are primarily based on spatial information, we use involution with convolution making it a hybrid, which adds location-specific capability to a deep learning model. Our proposed model is implemented in a simple yet effective approach, which makes it easier for applying in real life. We investigate the reasons why our approach works well for classifying eye-tracking patterns. For comparative analysis, we experiment with two separate datasets as well as a combined version of both. The results show that IC with three involution layers outperforms the previous approaches.","sentences":["Autism Spectrum Disorder (ASD) is a complicated neurological condition which is challenging to diagnose.","Numerous studies demonstrate that children diagnosed with autism struggle with maintaining attention spans and have less focused vision.","The eye-tracking technology has drawn special attention in the context of ASD since anomalies in gaze have long been acknowledged as a defining feature of autism in general.","Deep Learning (DL) approaches coupled with eye-tracking sensors are exploiting additional capabilities to advance the diagnostic and its applications.","By learning intricate nonlinear input-output relations, DL can accurately recognize the various gaze and eye-tracking patterns and adjust to the data.","Convolutions alone are insufficient to capture the important spatial information in gaze patterns or eye tracking.","The dynamic kernel-based process known as involutions can improve the efficiency of classifying gaze patterns or eye tracking data.","In this paper, we utilise two different image-processing operations to see how these processes learn eye-tracking patterns.","Since these patterns are primarily based on spatial information, we use involution with convolution making it a hybrid, which adds location-specific capability to a deep learning model.","Our proposed model is implemented in a simple yet effective approach, which makes it easier for applying in real life.","We investigate the reasons why our approach works well for classifying eye-tracking patterns.","For comparative analysis, we experiment with two separate datasets as well as a combined version of both.","The results show that IC with three involution layers outperforms the previous approaches."],"url":"http://arxiv.org/abs/2401.03575v1"}
{"created":"2024-01-07 19:32:55","title":"A Note on Dynamic Bidirected Dyck-Reachability with Cycles","abstract":"Recently, Li et al. [2022] presented a dynamic Dyck-reachability algorithm for bidirected graphs. The basic idea is based on updating edge weights in a data structure called the merged graph $G_m$. As noted in Krishna et al. [2023], the edge deletion procedure described in the algorithm of Li et al. [2022] cannot properly update the weights in the presence of cycles in $G_m$. This note discusses the cycle case and the time complexity.","sentences":["Recently, Li et al.","[2022] presented a dynamic Dyck-reachability algorithm for bidirected graphs.","The basic idea is based on updating edge weights in a data structure called the merged graph $G_m$. As noted in Krishna et al.","[2023], the edge deletion procedure described in the algorithm of Li et al.","[2022] cannot properly update the weights in the presence of cycles in $G_m$. This note discusses the cycle case and the time complexity."],"url":"http://arxiv.org/abs/2401.03570v1"}
{"created":"2024-01-07 19:11:18","title":"Agent AI: Surveying the Horizons of Multimodal Interaction","abstract":"Multi-modal AI systems will likely become a ubiquitous presence in our everyday lives. A promising approach to making these systems more interactive is to embody them as agents within physical and virtual environments. At present, systems leverage existing foundation models as the basic building blocks for the creation of embodied agents. Embedding agents within such environments facilitates the ability of models to process and interpret visual and contextual data, which is critical for the creation of more sophisticated and context-aware AI systems. For example, a system that can perceive user actions, human behavior, environmental objects, audio expressions, and the collective sentiment of a scene can be used to inform and direct agent responses within the given environment. To accelerate research on agent-based multimodal intelligence, we define \"Agent AI\" as a class of interactive systems that can perceive visual stimuli, language inputs, and other environmentally-grounded data, and can produce meaningful embodied action with infinite agent. In particular, we explore systems that aim to improve agents based on next-embodied action prediction by incorporating external knowledge, multi-sensory inputs, and human feedback. We argue that by developing agentic AI systems in grounded environments, one can also mitigate the hallucinations of large foundation models and their tendency to generate environmentally incorrect outputs. The emerging field of Agent AI subsumes the broader embodied and agentic aspects of multimodal interactions. Beyond agents acting and interacting in the physical world, we envision a future where people can easily create any virtual reality or simulated scene and interact with agents embodied within the virtual environment.","sentences":["Multi-modal AI systems will likely become a ubiquitous presence in our everyday lives.","A promising approach to making these systems more interactive is to embody them as agents within physical and virtual environments.","At present, systems leverage existing foundation models as the basic building blocks for the creation of embodied agents.","Embedding agents within such environments facilitates the ability of models to process and interpret visual and contextual data, which is critical for the creation of more sophisticated and context-aware AI systems.","For example, a system that can perceive user actions, human behavior, environmental objects, audio expressions, and the collective sentiment of a scene can be used to inform and direct agent responses within the given environment.","To accelerate research on agent-based multimodal intelligence, we define \"Agent AI\" as a class of interactive systems that can perceive visual stimuli, language inputs, and other environmentally-grounded data, and can produce meaningful embodied action with infinite agent.","In particular, we explore systems that aim to improve agents based on next-embodied action prediction by incorporating external knowledge, multi-sensory inputs, and human feedback.","We argue that by developing agentic AI systems in grounded environments, one can also mitigate the hallucinations of large foundation models and their tendency to generate environmentally incorrect outputs.","The emerging field of Agent AI subsumes the broader embodied and agentic aspects of multimodal interactions.","Beyond agents acting and interacting in the physical world, we envision a future where people can easily create any virtual reality or simulated scene and interact with agents embodied within the virtual environment."],"url":"http://arxiv.org/abs/2401.03568v1"}
{"created":"2024-01-07 18:12:20","title":"Data-CUBE: Data Curriculum for Instruction-based Sentence Representation Learning","abstract":"Recently, multi-task instruction tuning has been applied into sentence representation learning, which endows the capability of generating specific representations with the guidance of task instruction, exhibiting strong generalization ability on new tasks. However, these methods mostly neglect the potential interference problems across different tasks and instances, which may affect the training and convergence of the model. To address it, we propose a data curriculum method, namely Data-CUBE, that arranges the orders of all the multi-task data for training, to minimize the interference risks from the two views. In the task level, we aim to find the optimal task order to minimize the total cross-task interference risk, which is exactly the traveling salesman problem, hence we utilize a simulated annealing algorithm to find its solution. In the instance level, we measure the difficulty of all instances per task, then divide them into the easy-to-difficult mini-batches for training. Experiments on MTEB sentence representation evaluation tasks show that our approach can boost the performance of state-of-the-art methods. Our code and data are publicly available at the link: \\url{https://github.com/RUCAIBox/Data-CUBE}.","sentences":["Recently, multi-task instruction tuning has been applied into sentence representation learning, which endows the capability of generating specific representations with the guidance of task instruction, exhibiting strong generalization ability on new tasks.","However, these methods mostly neglect the potential interference problems across different tasks and instances, which may affect the training and convergence of the model.","To address it, we propose a data curriculum method, namely Data-CUBE, that arranges the orders of all the multi-task data for training, to minimize the interference risks from the two views.","In the task level, we aim to find the optimal task order to minimize the total cross-task interference risk, which is exactly the traveling salesman problem, hence we utilize a simulated annealing algorithm to find its solution.","In the instance level, we measure the difficulty of all instances per task, then divide them into the easy-to-difficult mini-batches for training.","Experiments on MTEB sentence representation evaluation tasks show that our approach can boost the performance of state-of-the-art methods.","Our code and data are publicly available at the link: \\url{https://github.com/RUCAIBox/Data-CUBE}."],"url":"http://arxiv.org/abs/2401.03563v1"}
{"created":"2024-01-07 18:10:14","title":"GLOCALFAIR: Jointly Improving Global and Local Group Fairness in Federated Learning","abstract":"Federated learning (FL) has emerged as a prospective solution for collaboratively learning a shared model across clients without sacrificing their data privacy. However, the federated learned model tends to be biased against certain demographic groups (e.g., racial and gender groups) due to the inherent FL properties, such as data heterogeneity and party selection. Unlike centralized learning, mitigating bias in FL is particularly challenging as private training datasets and their sensitive attributes are typically not directly accessible. Most prior research in this field only focuses on global fairness while overlooking the local fairness of individual clients. Moreover, existing methods often require sensitive information about the client's local datasets to be shared, which is not desirable. To address these issues, we propose GLOCALFAIR, a client-server co-design fairness framework that can jointly improve global and local group fairness in FL without the need for sensitive statistics about the client's private datasets. Specifically, we utilize constrained optimization to enforce local fairness on the client side and adopt a fairness-aware clustering-based aggregation on the server to further ensure the global model fairness across different sensitive groups while maintaining high utility. Experiments on two image datasets and one tabular dataset with various state-of-the-art fairness baselines show that GLOCALFAIR can achieve enhanced fairness under both global and local data distributions while maintaining a good level of utility and client fairness.","sentences":["Federated learning (FL) has emerged as a prospective solution for collaboratively learning a shared model across clients without sacrificing their data privacy.","However, the federated learned model tends to be biased against certain demographic groups (e.g., racial and gender groups) due to the inherent FL properties, such as data heterogeneity and party selection.","Unlike centralized learning, mitigating bias in FL is particularly challenging as private training datasets and their sensitive attributes are typically not directly accessible.","Most prior research in this field only focuses on global fairness while overlooking the local fairness of individual clients.","Moreover, existing methods often require sensitive information about the client's local datasets to be shared, which is not desirable.","To address these issues, we propose GLOCALFAIR, a client-server co-design fairness framework that can jointly improve global and local group fairness in FL without the need for sensitive statistics about the client's private datasets.","Specifically, we utilize constrained optimization to enforce local fairness on the client side and adopt a fairness-aware clustering-based aggregation on the server to further ensure the global model fairness across different sensitive groups while maintaining high utility.","Experiments on two image datasets and one tabular dataset with various state-of-the-art fairness baselines show that GLOCALFAIR can achieve enhanced fairness under both global and local data distributions while maintaining a good level of utility and client fairness."],"url":"http://arxiv.org/abs/2401.03562v1"}
{"created":"2024-01-07 17:52:41","title":"Improving Transferability of Network Intrusion Detection in a Federated Learning Setup","abstract":"Network Intrusion Detection Systems (IDS) aim to detect the presence of an intruder by analyzing network packets arriving at an internet connected device. Data-driven deep learning systems, popular due to their superior performance compared to traditional IDS, depend on availability of high quality training data for diverse intrusion classes. A way to overcome this limitation is through transferable learning, where training for one intrusion class can lead to detection of unseen intrusion classes after deployment. In this paper, we provide a detailed study on the transferability of intrusion detection. We investigate practical federated learning configurations to enhance the transferability of intrusion detection. We propose two techniques to significantly improve the transferability of a federated intrusion detection system. The code for this work can be found at https://github.com/ghosh64/transferability.","sentences":["Network Intrusion Detection Systems (IDS) aim to detect the presence of an intruder by analyzing network packets arriving at an internet connected device.","Data-driven deep learning systems, popular due to their superior performance compared to traditional IDS, depend on availability of high quality training data for diverse intrusion classes.","A way to overcome this limitation is through transferable learning, where training for one intrusion class can lead to detection of unseen intrusion classes after deployment.","In this paper, we provide a detailed study on the transferability of intrusion detection.","We investigate practical federated learning configurations to enhance the transferability of intrusion detection.","We propose two techniques to significantly improve the transferability of a federated intrusion detection system.","The code for this work can be found at https://github.com/ghosh64/transferability."],"url":"http://arxiv.org/abs/2401.03560v1"}
{"created":"2024-01-07 17:23:55","title":"Privacy-Preserving in Blockchain-based Federated Learning Systems","abstract":"Federated Learning (FL) has recently arisen as a revolutionary approach to collaborative training Machine Learning models. According to this novel framework, multiple participants train a global model collaboratively, coordinating with a central aggregator without sharing their local data. As FL gains popularity in diverse domains, security, and privacy concerns arise due to the distributed nature of this solution. Therefore, integrating this strategy with Blockchain technology has been consolidated as a preferred choice to ensure the privacy and security of participants.   This paper explores the research efforts carried out by the scientific community to define privacy solutions in scenarios adopting Blockchain-Enabled FL. It comprehensively summarizes the background related to FL and Blockchain, evaluates existing architectures for their integration, and the primary attacks and possible countermeasures to guarantee privacy in this setting. Finally, it reviews the main application scenarios where Blockchain-Enabled FL approaches have been proficiently applied. This survey can help academia and industry practitioners understand which theories and techniques exist to improve the performance of FL through Blockchain to preserve privacy and which are the main challenges and future directions in this novel and still under-explored context. We believe this work provides a novel contribution respect to the previous surveys and is a valuable tool to explore the current landscape, understand perspectives, and pave the way for advancements or improvements in this amalgamation of Blockchain and Federated Learning.","sentences":["Federated Learning (FL) has recently arisen as a revolutionary approach to collaborative training Machine Learning models.","According to this novel framework, multiple participants train a global model collaboratively, coordinating with a central aggregator without sharing their local data.","As FL gains popularity in diverse domains, security, and privacy concerns arise due to the distributed nature of this solution.","Therefore, integrating this strategy with Blockchain technology has been consolidated as a preferred choice to ensure the privacy and security of participants.   ","This paper explores the research efforts carried out by the scientific community to define privacy solutions in scenarios adopting Blockchain-Enabled FL.","It comprehensively summarizes the background related to FL and Blockchain, evaluates existing architectures for their integration, and the primary attacks and possible countermeasures to guarantee privacy in this setting.","Finally, it reviews the main application scenarios where Blockchain-Enabled FL approaches have been proficiently applied.","This survey can help academia and industry practitioners understand which theories and techniques exist to improve the performance of FL through Blockchain to preserve privacy and which are the main challenges and future directions in this novel and still under-explored context.","We believe this work provides a novel contribution respect to the previous surveys and is a valuable tool to explore the current landscape, understand perspectives, and pave the way for advancements or improvements in this amalgamation of Blockchain and Federated Learning."],"url":"http://arxiv.org/abs/2401.03552v1"}
{"created":"2024-01-07 16:39:34","title":"Transfer the linguistic representations from TTS to accent conversion with non-parallel data","abstract":"Accent conversion aims to convert the accent of a source speech to a target accent, meanwhile preserving the speaker's identity. This paper introduces a novel non-autoregressive framework for accent conversion that learns accent-agnostic linguistic representations and employs them to convert the accent in the source speech. Specifically, the proposed system aligns speech representations with linguistic representations obtained from Text-to-Speech (TTS) systems, enabling training of the accent voice conversion model on non-parallel data. Furthermore, we investigate the effectiveness of a pretraining strategy on native data and different acoustic features within our proposed framework. We conduct a comprehensive evaluation using both subjective and objective metrics to assess the performance of our approach. The evaluation results highlight the benefits of the pretraining strategy and the incorporation of richer semantic features, resulting in significantly enhanced audio quality and intelligibility.","sentences":["Accent conversion aims to convert the accent of a source speech to a target accent, meanwhile preserving the speaker's identity.","This paper introduces a novel non-autoregressive framework for accent conversion that learns accent-agnostic linguistic representations and employs them to convert the accent in the source speech.","Specifically, the proposed system aligns speech representations with linguistic representations obtained from Text-to-Speech (TTS) systems, enabling training of the accent voice conversion model on non-parallel data.","Furthermore, we investigate the effectiveness of a pretraining strategy on native data and different acoustic features within our proposed framework.","We conduct a comprehensive evaluation using both subjective and objective metrics to assess the performance of our approach.","The evaluation results highlight the benefits of the pretraining strategy and the incorporation of richer semantic features, resulting in significantly enhanced audio quality and intelligibility."],"url":"http://arxiv.org/abs/2401.03538v1"}
{"created":"2024-01-07 16:19:28","title":"Physics-informed Neural Networks for Encoding Dynamics in Real Physical Systems","abstract":"This dissertation investigates physics-informed neural networks (PINNs) as candidate models for encoding governing equations, and assesses their performance on experimental data from two different systems. The first system is a simple nonlinear pendulum, and the second is 2D heat diffusion across the surface of a metal block. We show that for the pendulum system the PINNs outperformed equivalent uninformed neural networks (NNs) in the ideal data case, with accuracy improvements of 18x and 6x for 10 linearly-spaced and 10 uniformly-distributed random training points respectively. In similar test cases with real data collected from an experiment, PINNs outperformed NNs with 9.3x and 9.1x accuracy improvements for 67 linearly-spaced and uniformly-distributed random points respectively. For the 2D heat diffusion, we show that both PINNs and NNs do not fare very well in reconstructing the heating regime due to difficulties in optimizing the network parameters over a large domain in both time and space. We highlight that data denoising and smoothing, reducing the size of the optimization problem, and using LBFGS as the optimizer are all ways to improve the accuracy of the predicted solution for both PINNs and NNs. Additionally, we address the viability of deploying physics-informed models within physical systems, and we choose FPGAs as the compute substrate for deployment. In light of this, we perform our experiments using a PYNQ-Z1 FPGA and identify issues related to time-coherent sensing and spatial data alignment. We discuss the insights gained from this work and list future work items based on the proposed architecture for the system that our methods work to develop.","sentences":["This dissertation investigates physics-informed neural networks (PINNs) as candidate models for encoding governing equations, and assesses their performance on experimental data from two different systems.","The first system is a simple nonlinear pendulum, and the second is 2D heat diffusion across the surface of a metal block.","We show that for the pendulum system the PINNs outperformed equivalent uninformed neural networks (NNs) in the ideal data case, with accuracy improvements of 18x and 6x for 10 linearly-spaced and 10 uniformly-distributed random training points respectively.","In similar test cases with real data collected from an experiment, PINNs outperformed NNs with 9.3x and 9.1x accuracy improvements for 67 linearly-spaced and uniformly-distributed random points respectively.","For the 2D heat diffusion, we show that both PINNs and NNs do not fare very well in reconstructing the heating regime due to difficulties in optimizing the network parameters over a large domain in both time and space.","We highlight that data denoising and smoothing, reducing the size of the optimization problem, and using LBFGS as the optimizer are all ways to improve the accuracy of the predicted solution for both PINNs and NNs.","Additionally, we address the viability of deploying physics-informed models within physical systems, and we choose FPGAs as the compute substrate for deployment.","In light of this, we perform our experiments using a PYNQ-Z1 FPGA and identify issues related to time-coherent sensing and spatial data alignment.","We discuss the insights gained from this work and list future work items based on the proposed architecture for the system that our methods work to develop."],"url":"http://arxiv.org/abs/2401.03534v1"}
{"created":"2024-01-07 16:01:51","title":"Detecting Anomalies in Blockchain Transactions using Machine Learning Classifiers and Explainability Analysis","abstract":"As the use of Blockchain for digital payments continues to rise in popularity, it also becomes susceptible to various malicious attacks. Successfully detecting anomalies within Blockchain transactions is essential for bolstering trust in digital payments. However, the task of anomaly detection in Blockchain transaction data is challenging due to the infrequent occurrence of illicit transactions. Although several studies have been conducted in the field, a limitation persists: the lack of explanations for the model's predictions. This study seeks to overcome this limitation by integrating eXplainable Artificial Intelligence (XAI) techniques and anomaly rules into tree-based ensemble classifiers for detecting anomalous Bitcoin transactions. The Shapley Additive exPlanation (SHAP) method is employed to measure the contribution of each feature, and it is compatible with ensemble models. Moreover, we present rules for interpreting whether a Bitcoin transaction is anomalous or not. Additionally, we have introduced an under-sampling algorithm named XGBCLUS, designed to balance anomalous and non-anomalous transaction data. This algorithm is compared against other commonly used under-sampling and over-sampling techniques. Finally, the outcomes of various tree-based single classifiers are compared with those of stacking and voting ensemble classifiers. Our experimental results demonstrate that: (i) XGBCLUS enhances TPR and ROC-AUC scores compared to state-of-the-art under-sampling and over-sampling techniques, and (ii) our proposed ensemble classifiers outperform traditional single tree-based machine learning classifiers in terms of accuracy, TPR, and FPR scores.","sentences":["As the use of Blockchain for digital payments continues to rise in popularity, it also becomes susceptible to various malicious attacks.","Successfully detecting anomalies within Blockchain transactions is essential for bolstering trust in digital payments.","However, the task of anomaly detection in Blockchain transaction data is challenging due to the infrequent occurrence of illicit transactions.","Although several studies have been conducted in the field, a limitation persists: the lack of explanations for the model's predictions.","This study seeks to overcome this limitation by integrating eXplainable Artificial Intelligence (XAI) techniques and anomaly rules into tree-based ensemble classifiers for detecting anomalous Bitcoin transactions.","The Shapley Additive exPlanation (SHAP) method is employed to measure the contribution of each feature, and it is compatible with ensemble models.","Moreover, we present rules for interpreting whether a Bitcoin transaction is anomalous or not.","Additionally, we have introduced an under-sampling algorithm named XGBCLUS, designed to balance anomalous and non-anomalous transaction data.","This algorithm is compared against other commonly used under-sampling and over-sampling techniques.","Finally, the outcomes of various tree-based single classifiers are compared with those of stacking and voting ensemble classifiers.","Our experimental results demonstrate that: (i) XGBCLUS enhances TPR and ROC-AUC scores compared to state-of-the-art under-sampling and over-sampling techniques, and (ii) our proposed ensemble classifiers outperform traditional single tree-based machine learning classifiers in terms of accuracy, TPR, and FPR scores."],"url":"http://arxiv.org/abs/2401.03530v1"}
{"created":"2024-01-07 15:13:24","title":"RoBERTurk: Adjusting RoBERTa for Turkish","abstract":"We pretrain RoBERTa on a Turkish corpora using BPE tokenizer. Our model outperforms BERTurk family models on the BOUN dataset for the POS task while resulting in underperformance on the IMST dataset for the same task and achieving competitive scores on the Turkish split of the XTREME dataset for the NER task - all while being pretrained on smaller data than its competitors. We release our pretrained model and tokenizer.","sentences":["We pretrain RoBERTa on a Turkish corpora using BPE tokenizer.","Our model outperforms BERTurk family models on the BOUN dataset for the POS task while resulting in underperformance on the IMST dataset for the same task and achieving competitive scores on the Turkish split of the XTREME dataset for the NER task - all while being pretrained on smaller data than its competitors.","We release our pretrained model and tokenizer."],"url":"http://arxiv.org/abs/2401.03515v1"}
{"created":"2024-01-07 14:34:34","title":"Re:Draw -- Context Aware Translation as a Controllable Method for Artistic Production","abstract":"We introduce context-aware translation, a novel method that combines the benefits of inpainting and image-to-image translation, respecting simultaneously the original input and contextual relevance -- where existing methods fall short. By doing so, our method opens new avenues for the controllable use of AI within artistic creation, from animation to digital art.   As an use case, we apply our method to redraw any hand-drawn animated character eyes based on any design specifications - eyes serve as a focal point that captures viewer attention and conveys a range of emotions, however, the labor-intensive nature of traditional animation often leads to compromises in the complexity and consistency of eye design. Furthermore, we remove the need for production data for training and introduce a new character recognition method that surpasses existing work by not requiring fine-tuning to specific productions. This proposed use case could help maintain consistency throughout production and unlock bolder and more detailed design choices without the production cost drawbacks. A user study shows context-aware translation is preferred over existing work 95.16% of the time.","sentences":["We introduce context-aware translation, a novel method that combines the benefits of inpainting and image-to-image translation, respecting simultaneously the original input and contextual relevance -- where existing methods fall short.","By doing so, our method opens new avenues for the controllable use of AI within artistic creation, from animation to digital art.   ","As an use case, we apply our method to redraw any hand-drawn animated character eyes based on any design specifications - eyes serve as a focal point that captures viewer attention and conveys a range of emotions, however, the labor-intensive nature of traditional animation often leads to compromises in the complexity and consistency of eye design.","Furthermore, we remove the need for production data for training and introduce a new character recognition method that surpasses existing work by not requiring fine-tuning to specific productions.","This proposed use case could help maintain consistency throughout production and unlock bolder and more detailed design choices without the production cost drawbacks.","A user study shows context-aware translation is preferred over existing work 95.16% of the time."],"url":"http://arxiv.org/abs/2401.03499v1"}
{"created":"2024-01-07 14:07:00","title":"Ensemble Defense System: A Hybrid IDS Approach for Effective Cyber Threat Detection","abstract":"Sophisticated cyber attacks present significant challenges for organizations in detecting and preventing such threats. To address this critical need for advanced defense mechanisms, we propose an Ensemble Defense System (EDS). An EDS is a cybersecurity framework aggregating multiple security tools designed to monitor and alert an organization during cyber attacks. The proposed EDS leverages a comprehensive range of Intrusion Detection System (IDS) capabilities by introducing a hybrid of signature-based IDS and anomaly-based IDS tools. It also incorporates Elasticsearch, an open-source Security Information and Event Management (SIEM) tool, to facilitate data analysis and interactive visualization of alerts generated from IDSs. The effectiveness of the EDS is evaluated through a payload from a bash script that executes various attacks, including port scanning, privilege escalation, and Denial-of-Service (DoS). The evaluation demonstrates the EDS's ability to detect diverse cyber attacks.","sentences":["Sophisticated cyber attacks present significant challenges for organizations in detecting and preventing such threats.","To address this critical need for advanced defense mechanisms, we propose an Ensemble Defense System (EDS).","An EDS is a cybersecurity framework aggregating multiple security tools designed to monitor and alert an organization during cyber attacks.","The proposed EDS leverages a comprehensive range of Intrusion Detection System (IDS) capabilities by introducing a hybrid of signature-based IDS and anomaly-based IDS tools.","It also incorporates Elasticsearch, an open-source Security Information and Event Management (SIEM) tool, to facilitate data analysis and interactive visualization of alerts generated from IDSs.","The effectiveness of the EDS is evaluated through a payload from a bash script that executes various attacks, including port scanning, privilege escalation, and Denial-of-Service (DoS).","The evaluation demonstrates the EDS's ability to detect diverse cyber attacks."],"url":"http://arxiv.org/abs/2401.03491v1"}
{"created":"2024-01-07 14:02:22","title":"Data-Driven Subsampling in the Presence of an Adversarial Actor","abstract":"Deep learning based automatic modulation classification (AMC) has received significant attention owing to its potential applications in both military and civilian use cases. Recently, data-driven subsampling techniques have been utilized to overcome the challenges associated with computational complexity and training time for AMC. Beyond these direct advantages of data-driven subsampling, these methods also have regularizing properties that may improve the adversarial robustness of the modulation classifier. In this paper, we investigate the effects of an adversarial attack on an AMC system that employs deep learning models both for AMC and for subsampling. Our analysis shows that subsampling itself is an effective deterrent to adversarial attacks. We also uncover the most efficient subsampling strategy when an adversarial attack on both the classifier and the subsampler is anticipated.","sentences":["Deep learning based automatic modulation classification (AMC) has received significant attention owing to its potential applications in both military and civilian use cases.","Recently, data-driven subsampling techniques have been utilized to overcome the challenges associated with computational complexity and training time for AMC.","Beyond these direct advantages of data-driven subsampling, these methods also have regularizing properties that may improve the adversarial robustness of the modulation classifier.","In this paper, we investigate the effects of an adversarial attack on an AMC system that employs deep learning models both for AMC and for subsampling.","Our analysis shows that subsampling itself is an effective deterrent to adversarial attacks.","We also uncover the most efficient subsampling strategy when an adversarial attack on both the classifier and the subsampler is anticipated."],"url":"http://arxiv.org/abs/2401.03488v1"}
{"created":"2024-01-07 13:01:29","title":"Freetalker: Controllable Speech and Text-Driven Gesture Generation Based on Diffusion Models for Enhanced Speaker Naturalness","abstract":"Current talking avatars mostly generate co-speech gestures based on audio and text of the utterance, without considering the non-speaking motion of the speaker. Furthermore, previous works on co-speech gesture generation have designed network structures based on individual gesture datasets, which results in limited data volume, compromised generalizability, and restricted speaker movements. To tackle these issues, we introduce FreeTalker, which, to the best of our knowledge, is the first framework for the generation of both spontaneous (e.g., co-speech gesture) and non-spontaneous (e.g., moving around the podium) speaker motions. Specifically, we train a diffusion-based model for speaker motion generation that employs unified representations of both speech-driven gestures and text-driven motions, utilizing heterogeneous data sourced from various motion datasets. During inference, we utilize classifier-free guidance to highly control the style in the clips. Additionally, to create smooth transitions between clips, we utilize DoubleTake, a method that leverages a generative prior and ensures seamless motion blending. Extensive experiments show that our method generates natural and controllable speaker movements. Our code, model, and demo are are available at \\url{https://youngseng.github.io/FreeTalker/}.","sentences":["Current talking avatars mostly generate co-speech gestures based on audio and text of the utterance, without considering the non-speaking motion of the speaker.","Furthermore, previous works on co-speech gesture generation have designed network structures based on individual gesture datasets, which results in limited data volume, compromised generalizability, and restricted speaker movements.","To tackle these issues, we introduce FreeTalker, which, to the best of our knowledge, is the first framework for the generation of both spontaneous (e.g., co-speech gesture) and non-spontaneous (e.g., moving around the podium) speaker motions.","Specifically, we train a diffusion-based model for speaker motion generation that employs unified representations of both speech-driven gestures and text-driven motions, utilizing heterogeneous data sourced from various motion datasets.","During inference, we utilize classifier-free guidance to highly control the style in the clips.","Additionally, to create smooth transitions between clips, we utilize DoubleTake, a method that leverages a generative prior and ensures seamless motion blending.","Extensive experiments show that our method generates natural and controllable speaker movements.","Our code, model, and demo are are available at \\url{https://youngseng.github.io/FreeTalker/}."],"url":"http://arxiv.org/abs/2401.03476v1"}
{"created":"2024-01-07 12:51:42","title":"ICMC-ASR: The ICASSP 2024 In-Car Multi-Channel Automatic Speech Recognition Challenge","abstract":"To promote speech processing and recognition research in driving scenarios, we build on the success of the Intelligent Cockpit Speech Recognition Challenge (ICSRC) held at ISCSLP 2022 and launch the ICASSP 2024 In-Car Multi-Channel Automatic Speech Recognition (ICMC-ASR) Challenge. This challenge collects over 100 hours of multi-channel speech data recorded inside a new energy vehicle and 40 hours of noise for data augmentation. Two tracks, including automatic speech recognition (ASR) and automatic speech diarization and recognition (ASDR) are set up, using character error rate (CER) and concatenated minimum permutation character error rate (cpCER) as evaluation metrics, respectively. Overall, the ICMC-ASR Challenge attracts 98 participating teams and receives 53 valid results in both tracks. In the end, first-place team USTCiflytek achieves a CER of 13.16% in the ASR track and a cpCER of 21.48% in the ASDR track, showing an absolute improvement of 13.08% and 51.4% compared to our challenge baseline, respectively.","sentences":["To promote speech processing and recognition research in driving scenarios, we build on the success of the Intelligent Cockpit Speech Recognition Challenge (ICSRC) held at ISCSLP 2022 and launch the ICASSP 2024 In-Car Multi-Channel Automatic Speech Recognition (ICMC-ASR) Challenge.","This challenge collects over 100 hours of multi-channel speech data recorded inside a new energy vehicle and 40 hours of noise for data augmentation.","Two tracks, including automatic speech recognition (ASR) and automatic speech diarization and recognition (ASDR) are set up, using character error rate (CER) and concatenated minimum permutation character error rate (cpCER) as evaluation metrics, respectively.","Overall, the ICMC-ASR Challenge attracts 98 participating teams and receives 53 valid results in both tracks.","In the end, first-place team USTCiflytek achieves a CER of 13.16% in the ASR track and a cpCER of 21.48% in the ASDR track, showing an absolute improvement of 13.08% and 51.4% compared to our challenge baseline, respectively."],"url":"http://arxiv.org/abs/2401.03473v1"}
{"created":"2024-01-07 12:31:36","title":"Efficient Test Data Generation for MC/DC with OCL and Search","abstract":"System-level testing of avionics software systems requires compliance with different international safety standards such as DO-178C. An important consideration of the avionics industry is automated test data generation according to the criteria suggested by safety standards. One of the recommended criteria by DO-178C is the modified condition/decision coverage (MC/DC) criterion. The current model-based test data generation approaches use constraints written in Object Constraint Language (OCL), and apply search techniques to generate test data. These approaches either do not support MC/DC criterion or suffer from performance issues while generating test data for large-scale avionics systems. In this paper, we propose an effective way to automate MC/DC test data generation during model-based testing. We develop a strategy that utilizes case-based reasoning (CBR) and range reduction heuristics designed to solve MC/DC-tailored OCL constraints. We performed an empirical study to compare our proposed strategy for MC/DC test data generation using CBR, range reduction, both CBR and range reduction, with an original search algorithm, and random search. We also empirically compared our strategy with existing constraint-solving approaches. The results show that both CBR and range reduction for MC/DC test data generation outperform the baseline approach. Moreover, the combination of both CBR and range reduction for MC/DC test data generation is an effective approach compared to existing constraint solvers.","sentences":["System-level testing of avionics software systems requires compliance with different international safety standards such as DO-178C. An important consideration of the avionics industry is automated test data generation according to the criteria suggested by safety standards.","One of the recommended criteria by DO-178C is the modified condition/decision coverage (MC/DC) criterion.","The current model-based test data generation approaches use constraints written in Object Constraint Language (OCL), and apply search techniques to generate test data.","These approaches either do not support MC/DC criterion or suffer from performance issues while generating test data for large-scale avionics systems.","In this paper, we propose an effective way to automate MC/DC test data generation during model-based testing.","We develop a strategy that utilizes case-based reasoning (CBR) and range reduction heuristics designed to solve MC/DC-tailored OCL constraints.","We performed an empirical study to compare our proposed strategy for MC/DC test data generation using CBR, range reduction, both CBR and range reduction, with an original search algorithm, and random search.","We also empirically compared our strategy with existing constraint-solving approaches.","The results show that both CBR and range reduction for MC/DC test data generation outperform the baseline approach.","Moreover, the combination of both CBR and range reduction for MC/DC test data generation is an effective approach compared to existing constraint solvers."],"url":"http://arxiv.org/abs/2401.03469v1"}
{"created":"2024-01-07 11:57:40","title":"Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon","abstract":"The utilization of long contexts poses a big challenge for large language models due to their limited context window length. Although the context window can be extended through fine-tuning, it will result in a considerable cost at both training and inference time, and exert an unfavorable impact to the LLM's original capabilities. In this work, we propose Activation Beacon, which condenses LLM's raw activations into more compact forms such that it can perceive a much longer context with a limited context window. Activation Beacon is introduced as a plug-and-play module for the LLM. It fully preserves the LLM's original capability on short contexts while extending the new capability on processing longer contexts. Besides, it works with short sliding windows to process the long context, which achieves a competitive memory and time efficiency in both training and inference. Activation Beacon is learned by the auto-regression task conditioned on a mixture of beacons with diversified condensing ratios. Thanks to such a treatment, it can be efficiently trained purely with short-sequence data in just 10K steps, which consumes less than 9 hours on a single 8xA800 GPU machine. The experimental studies show that Activation Beacon is able to extend Llama-2-7B's context length by $\\times100$ times (from 4K to 400K), meanwhile achieving a superior result on both long-context generation and understanding tasks. Our model and code will be available at the BGE repository.","sentences":["The utilization of long contexts poses a big challenge for large language models due to their limited context window length.","Although the context window can be extended through fine-tuning, it will result in a considerable cost at both training and inference time, and exert an unfavorable impact to the LLM's original capabilities.","In this work, we propose Activation Beacon, which condenses LLM's raw activations into more compact forms such that it can perceive a much longer context with a limited context window.","Activation Beacon is introduced as a plug-and-play module for the LLM.","It fully preserves the LLM's original capability on short contexts while extending the new capability on processing longer contexts.","Besides, it works with short sliding windows to process the long context, which achieves a competitive memory and time efficiency in both training and inference.","Activation Beacon is learned by the auto-regression task conditioned on a mixture of beacons with diversified condensing ratios.","Thanks to such a treatment, it can be efficiently trained purely with short-sequence data in just 10K steps, which consumes less than 9 hours on a single 8xA800 GPU machine.","The experimental studies show that Activation Beacon is able to extend Llama-2-7B's context length by $\\times100$ times (from 4K to 400K), meanwhile achieving a superior result on both long-context generation and understanding tasks.","Our model and code will be available at the BGE repository."],"url":"http://arxiv.org/abs/2401.03462v1"}
{"created":"2024-01-07 10:30:51","title":"Towards a Unified Method for Network Dynamic via Adversarial Weighted Link Prediction","abstract":"Network dynamic (e.g., traffic burst in data center networks and channel fading in cellular WiFi networks) has a great impact on the performance of communication networks (e.g., throughput, capacity, delay, and jitter). This article proposes a unified prediction-based method to handle the dynamic of various network systems. From the view of graph deep learning, I generally formulate the dynamic prediction of networks as a temporal link prediction task and analyze the possible challenges of the prediction of weighted networks, where link weights have the wide-value-range and sparsity issues. Inspired by the high-resolution video frame prediction with generative adversarial network (GAN), I try to adopt adversarial learning to generate high-quality predicted snapshots for network dynamic, which is expected to support the precise and fine-grained network control. A novel high-quality temporal link prediction (HQ-TLP) model with GAN is then developed to illustrate the potential of my basic idea. Extensive experiments for various application scenarios further demonstrate the powerful capability of HQ-TLP.","sentences":["Network dynamic (e.g., traffic burst in data center networks and channel fading in cellular WiFi networks) has a great impact on the performance of communication networks (e.g., throughput, capacity, delay, and jitter).","This article proposes a unified prediction-based method to handle the dynamic of various network systems.","From the view of graph deep learning, I generally formulate the dynamic prediction of networks as a temporal link prediction task and analyze the possible challenges of the prediction of weighted networks, where link weights have the wide-value-range and sparsity issues.","Inspired by the high-resolution video frame prediction with generative adversarial network (GAN), I try to adopt adversarial learning to generate high-quality predicted snapshots for network dynamic, which is expected to support the precise and fine-grained network control.","A novel high-quality temporal link prediction (HQ-TLP) model with GAN is then developed to illustrate the potential of my basic idea.","Extensive experiments for various application scenarios further demonstrate the powerful capability of HQ-TLP."],"url":"http://arxiv.org/abs/2401.03444v1"}
{"created":"2024-01-07 08:06:23","title":"N$^{3}$-Mapping: Normal Guided Neural Non-Projective Signed Distance Fields for Large-scale 3D Mapping","abstract":"Accurate and dense mapping in large-scale environments is essential for various robot applications. Recently, implicit neural signed distance fields (SDFs) have shown promising advances in this task. However, most existing approaches employ projective distances from range data as SDF supervision, introducing approximation errors and thus degrading the mapping quality. To address this problem, we introduce N3-Mapping, an implicit neural mapping system featuring normal-guided neural non-projective signed distance fields. Specifically, we directly sample points along the surface normal, instead of the ray, to obtain more accurate non-projective distance values from range data. Then these distance values are used as supervision to train the implicit map. For large-scale mapping, we apply a voxel-oriented sliding window mechanism to alleviate the forgetting issue with a bounded memory footprint. Besides, considering the uneven distribution of measured point clouds, a hierarchical sampling strategy is designed to improve training efficiency. Experiments demonstrate that our method effectively mitigates SDF approximation errors and achieves state-of-the-art mapping quality compared to existing approaches.","sentences":["Accurate and dense mapping in large-scale environments is essential for various robot applications.","Recently, implicit neural signed distance fields (SDFs) have shown promising advances in this task.","However, most existing approaches employ projective distances from range data as SDF supervision, introducing approximation errors and thus degrading the mapping quality.","To address this problem, we introduce N3-Mapping, an implicit neural mapping system featuring normal-guided neural non-projective signed distance fields.","Specifically, we directly sample points along the surface normal, instead of the ray, to obtain more accurate non-projective distance values from range data.","Then these distance values are used as supervision to train the implicit map.","For large-scale mapping, we apply a voxel-oriented sliding window mechanism to alleviate the forgetting issue with a bounded memory footprint.","Besides, considering the uneven distribution of measured point clouds, a hierarchical sampling strategy is designed to improve training efficiency.","Experiments demonstrate that our method effectively mitigates SDF approximation errors and achieves state-of-the-art mapping quality compared to existing approaches."],"url":"http://arxiv.org/abs/2401.03412v1"}
{"created":"2024-01-07 08:01:25","title":"Engineering Features to Improve Pass Prediction in Soccer Simulation 2D Games","abstract":"Soccer Simulation 2D (SS2D) is a simulation of a real soccer game in two dimensions. In soccer, passing behavior is an essential action for keeping the ball in possession of our team and creating goal opportunities. Similarly, for SS2D, predicting the passing behaviors of both opponents and our teammates helps manage resources and score more goals. Therefore, in this research, we have tried to address the modeling of passing behavior of soccer 2D players using Deep Neural Networks (DNN) and Random Forest (RF). We propose an embedded data extraction module that can record the decision-making of agents in an online format. Afterward, we apply four data sorting techniques for training data preparation. After, we evaluate the trained models' performance playing against 6 top teams of RoboCup 2019 that have distinctive playing strategies. Finally, we examine the importance of different feature groups on the prediction of a passing strategy. All results in each step of this work prove our suggested methodology's effectiveness and improve the performance of the pass prediction in Soccer Simulation 2D games ranging from 5\\% (e.g., playing against the same team) to 10\\% (e.g., playing against Robocup top teams).","sentences":["Soccer Simulation 2D (SS2D) is a simulation of a real soccer game in two dimensions.","In soccer, passing behavior is an essential action for keeping the ball in possession of our team and creating goal opportunities.","Similarly, for SS2D, predicting the passing behaviors of both opponents and our teammates helps manage resources and score more goals.","Therefore, in this research, we have tried to address the modeling of passing behavior of soccer 2D players using Deep Neural Networks (DNN) and Random Forest (RF).","We propose an embedded data extraction module that can record the decision-making of agents in an online format.","Afterward, we apply four data sorting techniques for training data preparation.","After, we evaluate the trained models' performance playing against 6 top teams of RoboCup 2019 that have distinctive playing strategies.","Finally, we examine the importance of different feature groups on the prediction of a passing strategy.","All results in each step of this work prove our suggested methodology's effectiveness and improve the performance of the pass prediction in Soccer Simulation 2D games ranging from 5\\% (e.g., playing against the same team) to 10\\% (e.g., playing against Robocup top teams)."],"url":"http://arxiv.org/abs/2401.03410v1"}
{"created":"2024-01-07 07:13:50","title":"Empirical Study of Large Language Models as Automated Essay Scoring Tools in English Composition__Taking TOEFL Independent Writing Task for Example","abstract":"Large language models have demonstrated exceptional capabilities in tasks involving natural language generation, reasoning, and comprehension. This study aims to construct prompts and comments grounded in the diverse scoring criteria delineated within the official TOEFL guide. The primary objective is to assess the capabilities and constraints of ChatGPT, a prominent representative of large language models, within the context of automated essay scoring. The prevailing methodologies for automated essay scoring involve the utilization of deep neural networks, statistical machine learning techniques, and fine-tuning pre-trained models. However, these techniques face challenges when applied to different contexts or subjects, primarily due to their substantial data requirements and limited adaptability to small sample sizes. In contrast, this study employs ChatGPT to conduct an automated evaluation of English essays, even with a small sample size, employing an experimental approach. The empirical findings indicate that ChatGPT can provide operational functionality for automated essay scoring, although the results exhibit a regression effect. It is imperative to underscore that the effective design and implementation of ChatGPT prompts necessitate a profound domain expertise and technical proficiency, as these prompts are subject to specific threshold criteria. Keywords: ChatGPT, Automated Essay Scoring, Prompt Learning, TOEFL Independent Writing Task","sentences":["Large language models have demonstrated exceptional capabilities in tasks involving natural language generation, reasoning, and comprehension.","This study aims to construct prompts and comments grounded in the diverse scoring criteria delineated within the official TOEFL guide.","The primary objective is to assess the capabilities and constraints of ChatGPT, a prominent representative of large language models, within the context of automated essay scoring.","The prevailing methodologies for automated essay scoring involve the utilization of deep neural networks, statistical machine learning techniques, and fine-tuning pre-trained models.","However, these techniques face challenges when applied to different contexts or subjects, primarily due to their substantial data requirements and limited adaptability to small sample sizes.","In contrast, this study employs ChatGPT to conduct an automated evaluation of English essays, even with a small sample size, employing an experimental approach.","The empirical findings indicate that ChatGPT can provide operational functionality for automated essay scoring, although the results exhibit a regression effect.","It is imperative to underscore that the effective design and implementation of ChatGPT prompts necessitate a profound domain expertise and technical proficiency, as these prompts are subject to specific threshold criteria.","Keywords: ChatGPT, Automated Essay Scoring, Prompt Learning, TOEFL Independent Writing Task"],"url":"http://arxiv.org/abs/2401.03401v1"}
{"created":"2024-01-07 06:51:26","title":"Predicting the Skies: A Novel Model for Flight-Level Passenger Traffic Forecasting","abstract":"Accurate prediction of flight-level passenger traffic is of paramount importance in airline operations, influencing key decisions from pricing to route optimization. This study introduces a novel, multimodal deep learning approach to the challenge of predicting flight-level passenger traffic, yielding substantial accuracy improvements compared to traditional models. Leveraging an extensive dataset from American Airlines, our model ingests historical traffic data, fare closure information, and seasonality attributes specific to each flight. Our proposed neural network integrates the strengths of Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN), exploiting the temporal patterns and spatial relationships within the data to enhance prediction performance. Crucial to the success of our model is a comprehensive data processing strategy. We construct 3D tensors to represent data, apply careful masking strategies to mirror real-world dynamics, and employ data augmentation techniques to enrich the diversity of our training set. The efficacy of our approach is borne out in the results: our model demonstrates an approximate 33\\% improvement in Mean Squared Error (MSE) compared to traditional benchmarks. This study, therefore, highlights the significant potential of deep learning techniques and meticulous data processing in advancing the field of flight traffic prediction.","sentences":["Accurate prediction of flight-level passenger traffic is of paramount importance in airline operations, influencing key decisions from pricing to route optimization.","This study introduces a novel, multimodal deep learning approach to the challenge of predicting flight-level passenger traffic, yielding substantial accuracy improvements compared to traditional models.","Leveraging an extensive dataset from American Airlines, our model ingests historical traffic data, fare closure information, and seasonality attributes specific to each flight.","Our proposed neural network integrates the strengths of Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN), exploiting the temporal patterns and spatial relationships within the data to enhance prediction performance.","Crucial to the success of our model is a comprehensive data processing strategy.","We construct 3D tensors to represent data, apply careful masking strategies to mirror real-world dynamics, and employ data augmentation techniques to enrich the diversity of our training set.","The efficacy of our approach is borne out in the results: our model demonstrates an approximate 33\\% improvement in Mean Squared Error (MSE) compared to traditional benchmarks.","This study, therefore, highlights the significant potential of deep learning techniques and meticulous data processing in advancing the field of flight traffic prediction."],"url":"http://arxiv.org/abs/2401.03397v1"}
{"created":"2024-01-07 05:18:38","title":"More MDS codes of non-Reed-Solomon type","abstract":"MDS codes have diverse practical applications in communication systems, data storage, and quantum codes due to their algebraic properties and optimal error-correcting capability. In this paper, we focus on a class of linear codes and establish some sufficient and necessary conditions for them being MDS. Notably, these codes differ from Reed-Solomon codes up to monomial equivalence. Additionally, we also explore the cases in which these codes are almost MDS or near MDS. Applying our main results, we determine the covering radii and deep holes of the dual codes associated with specific Roth-Lempel codes and discover an infinite family of (almost) optimally extendable codes with dimension three.","sentences":["MDS codes have diverse practical applications in communication systems, data storage, and quantum codes due to their algebraic properties and optimal error-correcting capability.","In this paper, we focus on a class of linear codes and establish some sufficient and necessary conditions for them being MDS.","Notably, these codes differ from Reed-Solomon codes up to monomial equivalence.","Additionally, we also explore the cases in which these codes are almost MDS or near MDS.","Applying our main results, we determine the covering radii and deep holes of the dual codes associated with specific Roth-Lempel codes and discover an infinite family of (almost) optimally extendable codes with dimension three."],"url":"http://arxiv.org/abs/2401.03391v1"}
{"created":"2024-01-07 04:32:29","title":"Grimoire is All You Need for Enhancing Large Language Models","abstract":"In-context learning (ICL) is one of the key methods for enhancing the performance of large language models on specific tasks by providing a set of few-shot question and answer examples. However, the ICL capability of different types of models shows significant variation due to factors such as model architecture, volume of learning data, and the size of parameters. Generally, the larger the model's parameter size and the more extensive the learning data, the stronger its ICL capability. In this paper, we propose a method SLEICL (Strong LLM Enhanced ICL) that involves learning from examples using strong language models and then summarizing and transferring these learned skills to weak language models for inference and application. This ensures the stability and effectiveness of ICL. Compared to directly enabling weak language models to learn from prompt examples, SLEICL reduces the difficulty of ICL for these models. Our experiments, conducted on up to eight datasets with five language models, demonstrate that weak language models achieve consistent improvement over their own zero-shot or few-shot capabilities using the SLEICL method. Some weak language models even surpass the performance of GPT4-1106-preview (zero-shot) with the aid of SLEICL.","sentences":["In-context learning (ICL) is one of the key methods for enhancing the performance of large language models on specific tasks by providing a set of few-shot question and answer examples.","However, the ICL capability of different types of models shows significant variation due to factors such as model architecture, volume of learning data, and the size of parameters.","Generally, the larger the model's parameter size and the more extensive the learning data, the stronger its ICL capability.","In this paper, we propose a method SLEICL (Strong LLM Enhanced ICL) that involves learning from examples using strong language models and then summarizing and transferring these learned skills to weak language models for inference and application.","This ensures the stability and effectiveness of ICL.","Compared to directly enabling weak language models to learn from prompt examples, SLEICL reduces the difficulty of ICL for these models.","Our experiments, conducted on up to eight datasets with five language models, demonstrate that weak language models achieve consistent improvement over their own zero-shot or few-shot capabilities using the SLEICL method.","Some weak language models even surpass the performance of GPT4-1106-preview (zero-shot) with the aid of SLEICL."],"url":"http://arxiv.org/abs/2401.03385v1"}
{"created":"2024-01-07 01:57:41","title":"In-Database Data Imputation","abstract":"Missing data is a widespread problem in many domains, creating challenges in data analysis and decision making. Traditional techniques for dealing with missing data, such as excluding incomplete records or imputing simple estimates (e.g., mean), are computationally efficient but may introduce bias and disrupt variable relationships, leading to inaccurate analyses. Model-based imputation techniques offer a more robust solution that preserves the variability and relationships in the data, but they demand significantly more computation time, limiting their applicability to small datasets.   This work enables efficient, high-quality, and scalable data imputation within a database system using the widely used MICE method. We adapt this method to exploit computation sharing and a ring abstraction for faster model training. To impute both continuous and categorical values, we develop techniques for in-database learning of stochastic linear regression and Gaussian discriminant analysis models. Our MICE implementations in PostgreSQL and DuckDB outperform alternative MICE implementations and model-based imputation techniques by up to two orders of magnitude in terms of computation time, while maintaining high imputation quality.","sentences":["Missing data is a widespread problem in many domains, creating challenges in data analysis and decision making.","Traditional techniques for dealing with missing data, such as excluding incomplete records or imputing simple estimates (e.g., mean), are computationally efficient but may introduce bias and disrupt variable relationships, leading to inaccurate analyses.","Model-based imputation techniques offer a more robust solution that preserves the variability and relationships in the data, but they demand significantly more computation time, limiting their applicability to small datasets.   ","This work enables efficient, high-quality, and scalable data imputation within a database system using the widely used MICE method.","We adapt this method to exploit computation sharing and a ring abstraction for faster model training.","To impute both continuous and categorical values, we develop techniques for in-database learning of stochastic linear regression and Gaussian discriminant analysis models.","Our MICE implementations in PostgreSQL and DuckDB outperform alternative MICE implementations and model-based imputation techniques by up to two orders of magnitude in terms of computation time, while maintaining high imputation quality."],"url":"http://arxiv.org/abs/2401.03359v1"}
{"created":"2024-01-07 00:58:33","title":"Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks","abstract":"While graph neural networks (GNNs) are widely used for node and graph representation learning tasks, the reliability of GNN uncertainty estimates under distribution shifts remains relatively under-explored. Indeed, while post-hoc calibration strategies can be used to improve in-distribution calibration, they need not also improve calibration under distribution shift. However, techniques which produce GNNs with better intrinsic uncertainty estimates are particularly valuable, as they can always be combined with post-hoc strategies later. Therefore, in this work, we propose G-$\\Delta$UQ, a novel training framework designed to improve intrinsic GNN uncertainty estimates. Our framework adapts the principle of stochastic data centering to graph data through novel graph anchoring strategies, and is able to support partially stochastic GNNs. While, the prevalent wisdom is that fully stochastic networks are necessary to obtain reliable estimates, we find that the functional diversity induced by our anchoring strategies when sampling hypotheses renders this unnecessary and allows us to support G-$\\Delta$UQ on pretrained models. Indeed, through extensive evaluation under covariate, concept and graph size shifts, we show that G-$\\Delta$UQ leads to better calibrated GNNs for node and graph classification. Further, it also improves performance on the uncertainty-based tasks of out-of-distribution detection and generalization gap estimation. Overall, our work provides insights into uncertainty estimation for GNNs, and demonstrates the utility of G-$\\Delta$UQ in obtaining reliable estimates.","sentences":["While graph neural networks (GNNs) are widely used for node and graph representation learning tasks, the reliability of GNN uncertainty estimates under distribution shifts remains relatively under-explored.","Indeed, while post-hoc calibration strategies can be used to improve in-distribution calibration, they need not also improve calibration under distribution shift.","However, techniques which produce GNNs with better intrinsic uncertainty estimates are particularly valuable, as they can always be combined with post-hoc strategies later.","Therefore, in this work, we propose G-$\\Delta$UQ, a novel training framework designed to improve intrinsic GNN uncertainty estimates.","Our framework adapts the principle of stochastic data centering to graph data through novel graph anchoring strategies, and is able to support partially stochastic GNNs.","While, the prevalent wisdom is that fully stochastic networks are necessary to obtain reliable estimates, we find that the functional diversity induced by our anchoring strategies when sampling hypotheses renders this unnecessary and allows us to support G-$\\Delta$UQ on pretrained models.","Indeed, through extensive evaluation under covariate, concept and graph size shifts, we show that G-$\\Delta$UQ leads to better calibrated GNNs for node and graph classification.","Further, it also improves performance on the uncertainty-based tasks of out-of-distribution detection and generalization gap estimation.","Overall, our work provides insights into uncertainty estimation for GNNs, and demonstrates the utility of G-$\\Delta$UQ in obtaining reliable estimates."],"url":"http://arxiv.org/abs/2401.03350v1"}
{"created":"2024-01-07 00:39:33","title":"An Investigation of Large Language Models for Real-World Hate Speech Detection","abstract":"Hate speech has emerged as a major problem plaguing our social spaces today. While there have been significant efforts to address this problem, existing methods are still significantly limited in effectively detecting hate speech online. A major limitation of existing methods is that hate speech detection is a highly contextual problem, and these methods cannot fully capture the context of hate speech to make accurate predictions. Recently, large language models (LLMs) have demonstrated state-of-the-art performance in several natural language tasks. LLMs have undergone extensive training using vast amounts of natural language data, enabling them to grasp intricate contextual details. Hence, they could be used as knowledge bases for context-aware hate speech detection. However, a fundamental problem with using LLMs to detect hate speech is that there are no studies on effectively prompting LLMs for context-aware hate speech detection. In this study, we conduct a large-scale study of hate speech detection, employing five established hate speech datasets. We discover that LLMs not only match but often surpass the performance of current benchmark machine learning models in identifying hate speech. By proposing four diverse prompting strategies that optimize the use of LLMs in detecting hate speech. Our study reveals that a meticulously crafted reasoning prompt can effectively capture the context of hate speech by fully utilizing the knowledge base in LLMs, significantly outperforming existing techniques. Furthermore, although LLMs can provide a rich knowledge base for the contextual detection of hate speech, suitable prompting strategies play a crucial role in effectively leveraging this knowledge base for efficient detection.","sentences":["Hate speech has emerged as a major problem plaguing our social spaces today.","While there have been significant efforts to address this problem, existing methods are still significantly limited in effectively detecting hate speech online.","A major limitation of existing methods is that hate speech detection is a highly contextual problem, and these methods cannot fully capture the context of hate speech to make accurate predictions.","Recently, large language models (LLMs) have demonstrated state-of-the-art performance in several natural language tasks.","LLMs have undergone extensive training using vast amounts of natural language data, enabling them to grasp intricate contextual details.","Hence, they could be used as knowledge bases for context-aware hate speech detection.","However, a fundamental problem with using LLMs to detect hate speech is that there are no studies on effectively prompting LLMs for context-aware hate speech detection.","In this study, we conduct a large-scale study of hate speech detection, employing five established hate speech datasets.","We discover that LLMs not only match but often surpass the performance of current benchmark machine learning models in identifying hate speech.","By proposing four diverse prompting strategies that optimize the use of LLMs in detecting hate speech.","Our study reveals that a meticulously crafted reasoning prompt can effectively capture the context of hate speech by fully utilizing the knowledge base in LLMs, significantly outperforming existing techniques.","Furthermore, although LLMs can provide a rich knowledge base for the contextual detection of hate speech, suitable prompting strategies play a crucial role in effectively leveraging this knowledge base for efficient detection."],"url":"http://arxiv.org/abs/2401.03346v1"}
{"created":"2024-01-07 00:23:05","title":"Weakly Augmented Variational Autoencoder in Time Series Anomaly Detection","abstract":"Due to their unsupervised training and uncertainty estimation, deep Variational Autoencoders (VAEs) have become powerful tools for reconstruction-based Time Series Anomaly Detection (TSAD). Existing VAE-based TSAD methods, either statistical or deep, tune meta-priors to estimate the likelihood probability for effectively capturing spatiotemporal dependencies in the data. However, these methods confront the challenge of inherent data scarcity, which is often the case in anomaly detection tasks. Such scarcity easily leads to latent holes, discontinuous regions in latent space, resulting in non-robust reconstructions on these discontinuous spaces. We propose a novel generative framework that combines VAEs with self-supervised learning (SSL) to address this issue.","sentences":["Due to their unsupervised training and uncertainty estimation, deep Variational Autoencoders (VAEs) have become powerful tools for reconstruction-based Time Series Anomaly Detection (TSAD).","Existing VAE-based TSAD methods, either statistical or deep, tune meta-priors to estimate the likelihood probability for effectively capturing spatiotemporal dependencies in the data.","However, these methods confront the challenge of inherent data scarcity, which is often the case in anomaly detection tasks.","Such scarcity easily leads to latent holes, discontinuous regions in latent space, resulting in non-robust reconstructions on these discontinuous spaces.","We propose a novel generative framework that combines VAEs with self-supervised learning (SSL) to address this issue."],"url":"http://arxiv.org/abs/2401.03341v1"}
{"created":"2024-01-06 22:32:34","title":"Spatiotemporally adaptive compression for scientific dataset with feature preservation -- a case study on simulation data with extreme climate events analysis","abstract":"Scientific discoveries are increasingly constrained by limited storage space and I/O capacities. For time-series simulations and experiments, their data often need to be decimated over timesteps to accommodate storage and I/O limitations. In this paper, we propose a technique that addresses storage costs while improving post-analysis accuracy through spatiotemporal adaptive, error-controlled lossy compression. We investigate the trade-off between data precision and temporal output rates, revealing that reducing data precision and increasing timestep frequency lead to more accurate analysis outcomes. Additionally, we integrate spatiotemporal feature detection with data compression and demonstrate that performing adaptive error-bounded compression in higher dimensional space enables greater compression ratios, leveraging the error propagation theory of a transformation-based compressor.   To evaluate our approach, we conduct experiments using the well-known E3SM climate simulation code and apply our method to compress variables used for cyclone tracking. Our results show a significant reduction in storage size while enhancing the quality of cyclone tracking analysis, both quantitatively and qualitatively, in comparison to the prevalent timestep decimation approach. Compared to three state-of-the-art lossy compressors lacking feature preservation capabilities, our adaptive compression framework improves perfectly matched cases in TC tracking by 26.4-51.3% at medium compression ratios and by 77.3-571.1% at large compression ratios, with a merely 5-11% computational overhead.","sentences":["Scientific discoveries are increasingly constrained by limited storage space and I/O capacities.","For time-series simulations and experiments, their data often need to be decimated over timesteps to accommodate storage and I/O limitations.","In this paper, we propose a technique that addresses storage costs while improving post-analysis accuracy through spatiotemporal adaptive, error-controlled lossy compression.","We investigate the trade-off between data precision and temporal output rates, revealing that reducing data precision and increasing timestep frequency lead to more accurate analysis outcomes.","Additionally, we integrate spatiotemporal feature detection with data compression and demonstrate that performing adaptive error-bounded compression in higher dimensional space enables greater compression ratios, leveraging the error propagation theory of a transformation-based compressor.   ","To evaluate our approach, we conduct experiments using the well-known E3SM climate simulation code and apply our method to compress variables used for cyclone tracking.","Our results show a significant reduction in storage size while enhancing the quality of cyclone tracking analysis, both quantitatively and qualitatively, in comparison to the prevalent timestep decimation approach.","Compared to three state-of-the-art lossy compressors lacking feature preservation capabilities, our adaptive compression framework improves perfectly matched cases in TC tracking by 26.4-51.3% at medium compression ratios and by 77.3-571.1% at large compression ratios, with a merely 5-11% computational overhead."],"url":"http://arxiv.org/abs/2401.03317v1"}
{"created":"2024-01-06 22:13:51","title":"Enhancing Context Through Contrast","abstract":"Neural machine translation benefits from semantically rich representations. Considerable progress in learning such representations has been achieved by language modelling and mutual information maximization objectives using contrastive learning. The language-dependent nature of language modelling introduces a trade-off between the universality of the learned representations and the model's performance on the language modelling tasks. Although contrastive learning improves performance, its success cannot be attributed to mutual information alone. We propose a novel Context Enhancement step to improve performance on neural machine translation by maximizing mutual information using the Barlow Twins loss. Unlike other approaches, we do not explicitly augment the data but view languages as implicit augmentations, eradicating the risk of disrupting semantic information. Further, our method does not learn embeddings from scratch and can be generalised to any set of pre-trained embeddings. Finally, we evaluate the language-agnosticism of our embeddings through language classification and use them for neural machine translation to compare with state-of-the-art approaches.","sentences":["Neural machine translation benefits from semantically rich representations.","Considerable progress in learning such representations has been achieved by language modelling and mutual information maximization objectives using contrastive learning.","The language-dependent nature of language modelling introduces a trade-off between the universality of the learned representations and the model's performance on the language modelling tasks.","Although contrastive learning improves performance, its success cannot be attributed to mutual information alone.","We propose a novel Context Enhancement step to improve performance on neural machine translation by maximizing mutual information using the Barlow Twins loss.","Unlike other approaches, we do not explicitly augment the data but view languages as implicit augmentations, eradicating the risk of disrupting semantic information.","Further, our method does not learn embeddings from scratch and can be generalised to any set of pre-trained embeddings.","Finally, we evaluate the language-agnosticism of our embeddings through language classification and use them for neural machine translation to compare with state-of-the-art approaches."],"url":"http://arxiv.org/abs/2401.03314v1"}
{"created":"2024-01-06 21:47:49","title":"Exploiting Data Hierarchy as a New Modality for Contrastive Learning","abstract":"This work investigates how hierarchically structured data can help neural networks learn conceptual representations of cathedrals. The underlying WikiScenes dataset provides a spatially organized hierarchical structure of cathedral components. We propose a novel hierarchical contrastive training approach that leverages a triplet margin loss to represent the data's spatial hierarchy in the encoder's latent space. As such, the proposed approach investigates if the dataset structure provides valuable information for self-supervised learning. We apply t-SNE to visualize the resultant latent space and evaluate the proposed approach by comparing it with other dataset-specific contrastive learning methods using a common downstream classification task. The proposed method outperforms the comparable weakly-supervised and baseline methods. Our findings suggest that dataset structure is a valuable modality for weakly-supervised learning.","sentences":["This work investigates how hierarchically structured data can help neural networks learn conceptual representations of cathedrals.","The underlying WikiScenes dataset provides a spatially organized hierarchical structure of cathedral components.","We propose a novel hierarchical contrastive training approach that leverages a triplet margin loss to represent the data's spatial hierarchy in the encoder's latent space.","As such, the proposed approach investigates if the dataset structure provides valuable information for self-supervised learning.","We apply t-SNE to visualize the resultant latent space and evaluate the proposed approach by comparing it with other dataset-specific contrastive learning methods using a common downstream classification task.","The proposed method outperforms the comparable weakly-supervised and baseline methods.","Our findings suggest that dataset structure is a valuable modality for weakly-supervised learning."],"url":"http://arxiv.org/abs/2401.03312v1"}
{"created":"2024-01-06 21:22:18","title":"CAVIAR: Co-simulation of 6G Communications, 3D Scenarios and AI for Digital Twins","abstract":"Digital twins are an important technology for advancing mobile communications, specially in use cases that require simultaneously simulating the wireless channel, 3D scenes and machine learning. Aiming at providing a solution to this demand, this work describes a modular co-simulation methodology called CAVIAR. Here, CAVIAR is upgraded to support a message passing library and enable the virtual counterpart of a digital twin system using different 6G-related simulators. The main contributions of this work are the detailed description of different CAVIAR architectures, the implementation of this methodology to assess a 6G use case of UAV-based search and rescue mission (SAR), and the generation of benchmarking data about the computational resource usage. For executing the SAR co-simulation we adopt five open-source solutions: the physical and link level network simulator Sionna, the simulator for autonomous vehicles AirSim, scikit-learn for training a decision tree for MIMO beam selection, Yolov8 for the detection of rescue targets and NATS for message passing. Results for the implemented SAR use case suggest that the methodology can run in a single machine, with the main demanded resources being the CPU processing and the GPU memory.","sentences":["Digital twins are an important technology for advancing mobile communications, specially in use cases that require simultaneously simulating the wireless channel, 3D scenes and machine learning.","Aiming at providing a solution to this demand, this work describes a modular co-simulation methodology called CAVIAR.","Here, CAVIAR is upgraded to support a message passing library and enable the virtual counterpart of a digital twin system using different 6G-related simulators.","The main contributions of this work are the detailed description of different CAVIAR architectures, the implementation of this methodology to assess a 6G use case of UAV-based search and rescue mission (SAR), and the generation of benchmarking data about the computational resource usage.","For executing the SAR co-simulation we adopt five open-source solutions: the physical and link level network simulator Sionna, the simulator for autonomous vehicles AirSim, scikit-learn for training a decision tree for MIMO beam selection, Yolov8 for the detection of rescue targets and NATS for message passing.","Results for the implemented SAR use case suggest that the methodology can run in a single machine, with the main demanded resources being the CPU processing and the GPU memory."],"url":"http://arxiv.org/abs/2401.03310v1"}
{"created":"2024-01-06 21:04:31","title":"MOTO: Offline Pre-training to Online Fine-tuning for Model-based Robot Learning","abstract":"We study the problem of offline pre-training and online fine-tuning for reinforcement learning from high-dimensional observations in the context of realistic robot tasks. Recent offline model-free approaches successfully use online fine-tuning to either improve the performance of the agent over the data collection policy or adapt to novel tasks. At the same time, model-based RL algorithms have achieved significant progress in sample efficiency and the complexity of the tasks they can solve, yet remain under-utilized in the fine-tuning setting. In this work, we argue that existing model-based offline RL methods are not suitable for offline-to-online fine-tuning in high-dimensional domains due to issues with distribution shifts, off-dynamics data, and non-stationary rewards. We propose an on-policy model-based method that can efficiently reuse prior data through model-based value expansion and policy regularization, while preventing model exploitation by controlling epistemic uncertainty. We find that our approach successfully solves tasks from the MetaWorld benchmark, as well as the Franka Kitchen robot manipulation environment completely from images. To the best of our knowledge, MOTO is the first method to solve this environment from pixels.","sentences":["We study the problem of offline pre-training and online fine-tuning for reinforcement learning from high-dimensional observations in the context of realistic robot tasks.","Recent offline model-free approaches successfully use online fine-tuning to either improve the performance of the agent over the data collection policy or adapt to novel tasks.","At the same time, model-based RL algorithms have achieved significant progress in sample efficiency and the complexity of the tasks they can solve, yet remain under-utilized in the fine-tuning setting.","In this work, we argue that existing model-based offline RL methods are not suitable for offline-to-online fine-tuning in high-dimensional domains due to issues with distribution shifts, off-dynamics data, and non-stationary rewards.","We propose an on-policy model-based method that can efficiently reuse prior data through model-based value expansion and policy regularization, while preventing model exploitation by controlling epistemic uncertainty.","We find that our approach successfully solves tasks from the MetaWorld benchmark, as well as the Franka Kitchen robot manipulation environment completely from images.","To the best of our knowledge, MOTO is the first method to solve this environment from pixels."],"url":"http://arxiv.org/abs/2401.03306v1"}
{"created":"2024-01-06 20:52:04","title":"On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling, and Beyond","abstract":"We seek to understand what facilitates sample-efficient learning from historical datasets for sequential decision-making, a problem that is popularly known as offline reinforcement learning (RL). Further, we are interested in algorithms that enjoy sample efficiency while leveraging (value) function approximation. In this paper, we address these fundamental questions by (i) proposing a notion of data diversity that subsumes the previous notions of coverage measures in offline RL and (ii) using this notion to {unify} three distinct classes of offline RL algorithms based on version spaces (VS), regularized optimization (RO), and posterior sampling (PS). We establish that VS-based, RO-based, and PS-based algorithms, under standard assumptions, achieve \\emph{comparable} sample efficiency, which recovers the state-of-the-art sub-optimality bounds for finite and linear model classes with the standard assumptions. This result is surprising, given that the prior work suggested an unfavorable sample complexity of the RO-based algorithm compared to the VS-based algorithm, whereas posterior sampling is rarely considered in offline RL due to its explorative nature. Notably, our proposed model-free PS-based algorithm for offline RL is {novel}, with sub-optimality bounds that are {frequentist} (i.e., worst-case) in nature.","sentences":["We seek to understand what facilitates sample-efficient learning from historical datasets for sequential decision-making, a problem that is popularly known as offline reinforcement learning (RL).","Further, we are interested in algorithms that enjoy sample efficiency while leveraging (value) function approximation.","In this paper, we address these fundamental questions by (i) proposing a notion of data diversity that subsumes the previous notions of coverage measures in offline RL and (ii) using this notion to {unify} three distinct classes of offline RL algorithms based on version spaces (VS), regularized optimization (RO), and posterior sampling (PS).","We establish that VS-based, RO-based, and PS-based algorithms, under standard assumptions, achieve \\emph{comparable} sample efficiency, which recovers the state-of-the-art sub-optimality bounds for finite and linear model classes with the standard assumptions.","This result is surprising, given that the prior work suggested an unfavorable sample complexity of the RO-based algorithm compared to the VS-based algorithm, whereas posterior sampling is rarely considered in offline RL due to its explorative nature.","Notably, our proposed model-free PS-based algorithm for offline RL is {novel}, with sub-optimality bounds that are {frequentist} (i.e., worst-case) in nature."],"url":"http://arxiv.org/abs/2401.03301v1"}
{"created":"2024-01-06 20:39:20","title":"Multi-View 3D Instance Segmentation of Structural Anomalies for Enhanced Structural Inspection of Concrete Bridges","abstract":"For effective structural damage assessment, the instances of damages need to be localized in the world of a 3D model. Due to a lack of data, the detection of structural anomalies can currently not be directly learned and performed in 3D space. In this work, a three-stage approach is presented, which uses the good performance of detection models on image level to segment instances of anomalies in the 3D space. In the detection stage, semantic segmentation predictions are produced on image level. The mapping stage transfers the image-level prediction onto the respective point cloud. In the extraction stage, 3D anomaly instances are extracted from the segmented point cloud. Cloud contraction is used to transform cracks into their medial axis representation. For areal anomalies the bounding polygon is extracted by means of alpha shapes. The approach covers the classes crack, spalling, and corrosion and the three image-level segmentation models TopoCrack, nnU-Net, and DetectionHMA are compared. Granted a localization tolerance of 4cm, IoUs of over 90% can be achieved for crack and corrosion and 41% for spalling, which appears to be a specifically challenging class. Detection on instance-level measured in AP is about 45% for crack and spalling and 73% for corrosion.","sentences":["For effective structural damage assessment, the instances of damages need to be localized in the world of a 3D model.","Due to a lack of data, the detection of structural anomalies can currently not be directly learned and performed in 3D space.","In this work, a three-stage approach is presented, which uses the good performance of detection models on image level to segment instances of anomalies in the 3D space.","In the detection stage, semantic segmentation predictions are produced on image level.","The mapping stage transfers the image-level prediction onto the respective point cloud.","In the extraction stage, 3D anomaly instances are extracted from the segmented point cloud.","Cloud contraction is used to transform cracks into their medial axis representation.","For areal anomalies the bounding polygon is extracted by means of alpha shapes.","The approach covers the classes crack, spalling, and corrosion and the three image-level segmentation models TopoCrack, nnU-Net, and DetectionHMA are compared.","Granted a localization tolerance of 4cm, IoUs of over 90% can be achieved for crack and corrosion and 41% for spalling, which appears to be a specifically challenging class.","Detection on instance-level measured in AP is about 45% for crack and spalling and 73% for corrosion."],"url":"http://arxiv.org/abs/2401.03298v1"}
