{"created":"2024-04-09 17:59:04","title":"Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?","abstract":"Enhancing semantic grounding abilities in Vision-Language Models (VLMs) often involves collecting domain-specific training data, refining the network architectures, or modifying the training recipes. In this work, we venture into an orthogonal direction and explore whether VLMs can improve their semantic grounding by \"receiving\" feedback, without requiring in-domain data, fine-tuning, or modifications to the network architectures. We systematically analyze this hypothesis using a feedback mechanism composed of a binary signal. We find that if prompted appropriately, VLMs can utilize feedback both in a single step and iteratively, showcasing the potential of feedback as an alternative technique to improve grounding in internet-scale VLMs. Furthermore, VLMs, like LLMs, struggle to self-correct errors out-of-the-box. However, we find that this issue can be mitigated via a binary verification mechanism. Finally, we explore the potential and limitations of amalgamating these findings and applying them iteratively to automatically enhance VLMs' grounding performance, showing grounding accuracy consistently improves using automated feedback across all models in all settings investigated. Overall, our iterative framework improves semantic grounding in VLMs by more than 15 accuracy points under noise-free feedback and up to 5 accuracy points under a simple automated binary verification mechanism. The project website is hosted at https://andrewliao11.github.io/vlms_feedback","sentences":["Enhancing semantic grounding abilities in Vision-Language Models (VLMs) often involves collecting domain-specific training data, refining the network architectures, or modifying the training recipes.","In this work, we venture into an orthogonal direction and explore whether VLMs can improve their semantic grounding by \"receiving\" feedback, without requiring in-domain data, fine-tuning, or modifications to the network architectures.","We systematically analyze this hypothesis using a feedback mechanism composed of a binary signal.","We find that if prompted appropriately, VLMs can utilize feedback both in a single step and iteratively, showcasing the potential of feedback as an alternative technique to improve grounding in internet-scale VLMs.","Furthermore, VLMs, like LLMs, struggle to self-correct errors out-of-the-box.","However, we find that this issue can be mitigated via a binary verification mechanism.","Finally, we explore the potential and limitations of amalgamating these findings and applying them iteratively to automatically enhance VLMs' grounding performance, showing grounding accuracy consistently improves using automated feedback across all models in all settings investigated.","Overall, our iterative framework improves semantic grounding in VLMs by more than 15 accuracy points under noise-free feedback and up to 5 accuracy points under a simple automated binary verification mechanism.","The project website is hosted at https://andrewliao11.github.io/vlms_feedback"],"url":"http://arxiv.org/abs/2404.06510v1"}
{"created":"2024-04-09 17:57:29","title":"On the Effect of (Near) Duplicate Subwords in Language Modelling","abstract":"Tokenisation is a core part of language models (LMs). It involves splitting a character sequence into subwords which are assigned arbitrary indices before being served to the LM. While typically lossless, however, this process may lead to less sample efficient LM training: as it removes character-level information, it could make it harder for LMs to generalise across similar subwords, such as now and Now. We refer to such subwords as near duplicates. In this paper, we study the impact of near duplicate subwords on LM training efficiency. First, we design an experiment that gives us an upper bound to how much we should expect a model to improve if we could perfectly generalise across near duplicates. We do this by duplicating each subword in our LM's vocabulary, creating perfectly equivalent classes of subwords. Experimentally, we find that LMs need roughly 17% more data when trained in a fully duplicated setting. Second, we investigate the impact of naturally occurring near duplicates on LMs. Here, we see that merging them considerably hurts LM performance. Therefore, although subword duplication negatively impacts LM training efficiency, naturally occurring near duplicates may not be as similar as anticipated, limiting the potential for performance improvements.","sentences":["Tokenisation is a core part of language models (LMs).","It involves splitting a character sequence into subwords which are assigned arbitrary indices before being served to the LM.","While typically lossless, however, this process may lead to less sample efficient LM training: as it removes character-level information, it could make it harder for LMs to generalise across similar subwords, such as now and Now.","We refer to such subwords as near duplicates.","In this paper, we study the impact of near duplicate subwords on LM training efficiency.","First, we design an experiment that gives us an upper bound to how much we should expect a model to improve if we could perfectly generalise across near duplicates.","We do this by duplicating each subword in our LM's vocabulary, creating perfectly equivalent classes of subwords.","Experimentally, we find that LMs need roughly 17% more data when trained in a fully duplicated setting.","Second, we investigate the impact of naturally occurring near duplicates on LMs.","Here, we see that merging them considerably hurts LM performance.","Therefore, although subword duplication negatively impacts LM training efficiency, naturally occurring near duplicates may not be as similar as anticipated, limiting the potential for performance improvements."],"url":"http://arxiv.org/abs/2404.06508v1"}
{"created":"2024-04-09 17:54:10","title":"Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?","abstract":"Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note. A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology. The relevant information can then be extracted and organized according to the structure of the SOAP note. In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency. The first approach generates the sections independently, while the second method generates them all together. In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1% difference) and have no difference in terms of the Factuality metric. We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators. Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively. With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics. This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.","sentences":["Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note.","A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology.","The relevant information can then be extracted and organized according to the structure of the SOAP note.","In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency.","The first approach generates the sections independently, while the second method generates them all together.","In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1% difference) and have no difference in terms of the Factuality metric.","We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators.","Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively.","With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics.","This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections."],"url":"http://arxiv.org/abs/2404.06503v1"}
{"created":"2024-04-09 17:51:42","title":"The Rise and Fall of the Initial Era","abstract":"Bibliographic data is a rich source of information that goes beyond the use cases of location and citation -- it also encodes both cultural and technological context. For most of its existence, the scholarly record has changed slowly and hence provides an opportunity to gain insight through its reflection of the cultural norms of the research community over the last four centuries. While it is often difficult to distinguish the originating driver of change, it is still valuable to consider the motivating influences that have led to changes in the structure of the scholarly record. An ``initial era'' is identified during which initials were used in preference to full names by authors on scholarly communications. Causes of the emergence and demise of this era are considered as well as the implications of this era on research culture and practice.","sentences":["Bibliographic data is a rich source of information that goes beyond the use cases of location and citation -- it also encodes both cultural and technological context.","For most of its existence, the scholarly record has changed slowly and hence provides an opportunity to gain insight through its reflection of the cultural norms of the research community over the last four centuries.","While it is often difficult to distinguish the originating driver of change, it is still valuable to consider the motivating influences that have led to changes in the structure of the scholarly record.","An ``initial era'' is identified during which initials were used in preference to full names by authors on scholarly communications.","Causes of the emergence and demise of this era are considered as well as the implications of this era on research culture and practice."],"url":"http://arxiv.org/abs/2404.06500v1"}
{"created":"2024-04-09 17:28:07","title":"Mechanised Hypersafety Proofs about Structured Data: Extended Version","abstract":"Arrays are a fundamental abstraction to represent collections of data. It is often possible to exploit structural properties of the data stored in an array (e.g., repetition or sparsity) to develop a specialised representation optimised for space efficiency. Formally reasoning about correctness of manipulations with such structured data is challenging, as they are often composed of multiple loops with non-trivial invariants.   In this work, we observe that specifications for structured data manipulations can be phrased as hypersafety properties, i.e., predicates that relate traces of $k$ programs. To turn this observation into an effective verification methodology, we developed the Logic for Graceful Tensor Manipulation (LGTM), a new Hoare-style relational separation logic for specifying and verifying computations over structured data. The key enabling idea of LGTM is that of parametrised hypersafety specifications that allow the number $k$ of the program components to depend on the program variables. We implemented LGTM as a foundational embedding into Coq, mechanising its rules, meta-theory, and the proof of soundness. Furthermore, we developed a library of domain-specific tactics that automate computer-aided hypersafety reasoning, resulting in pleasantly short proof scripts that enjoy a high degree of reuse. We argue for the effectiveness of relational reasoning about structured data in LGTM by specifying and mechanically proving correctness of 13 case studies including computations on compressed arrays and efficient operations over multiple kinds of sparse tensors.","sentences":["Arrays are a fundamental abstraction to represent collections of data.","It is often possible to exploit structural properties of the data stored in an array (e.g., repetition or sparsity) to develop a specialised representation optimised for space efficiency.","Formally reasoning about correctness of manipulations with such structured data is challenging, as they are often composed of multiple loops with non-trivial invariants.   ","In this work, we observe that specifications for structured data manipulations can be phrased as hypersafety properties, i.e., predicates that relate traces of $k$ programs.","To turn this observation into an effective verification methodology, we developed the Logic for Graceful Tensor Manipulation (LGTM), a new Hoare-style relational separation logic for specifying and verifying computations over structured data.","The key enabling idea of LGTM is that of parametrised hypersafety specifications that allow the number $k$ of the program components to depend on the program variables.","We implemented LGTM as a foundational embedding into Coq, mechanising its rules, meta-theory, and the proof of soundness.","Furthermore, we developed a library of domain-specific tactics that automate computer-aided hypersafety reasoning, resulting in pleasantly short proof scripts that enjoy a high degree of reuse.","We argue for the effectiveness of relational reasoning about structured data in LGTM by specifying and mechanically proving correctness of 13 case studies including computations on compressed arrays and efficient operations over multiple kinds of sparse tensors."],"url":"http://arxiv.org/abs/2404.06477v1"}
{"created":"2024-04-09 17:15:37","title":"Scaling to 32 GPUs on a Novel Composable System Architecture","abstract":"The development of composable systems architecture marks a significant shift in resource allocation and utilization within data centers. This paper presents a composable architecture scaling up to 32 GPUs on a single node, addressing the technical challenges encountered and the innovative solutions implemented. This design introduces a flexible and dynamic resource distribution mechanism, particularly for GPUs, enabling tailored allocation to meet varying node demands. The architecture's dynamic nature allows for the flexible assignment and reassignment of hardware resources, such as GPUs, to different nodes as required, offering unprecedented capability and flexibility.","sentences":["The development of composable systems architecture marks a significant shift in resource allocation and utilization within data centers.","This paper presents a composable architecture scaling up to 32 GPUs on a single node, addressing the technical challenges encountered and the innovative solutions implemented.","This design introduces a flexible and dynamic resource distribution mechanism, particularly for GPUs, enabling tailored allocation to meet varying node demands.","The architecture's dynamic nature allows for the flexible assignment and reassignment of hardware resources, such as GPUs, to different nodes as required, offering unprecedented capability and flexibility."],"url":"http://arxiv.org/abs/2404.06467v1"}
{"created":"2024-04-09 17:14:41","title":"Hyperparameter Selection in Continual Learning","abstract":"In continual learning (CL) -- where a learner trains on a stream of data -- standard hyperparameter optimisation (HPO) cannot be applied, as a learner does not have access to all of the data at the same time. This has prompted the development of CL-specific HPO frameworks. The most popular way to tune hyperparameters in CL is to repeatedly train over the whole data stream with different hyperparameter settings. However, this end-of-training HPO is unrealistic as in practice a learner can only see the stream once. Hence, there is an open question: what HPO framework should a practitioner use for a CL problem in reality? This paper answers this question by evaluating several realistic HPO frameworks. We find that all the HPO frameworks considered, including end-of-training HPO, perform similarly. We therefore advocate using the realistic and most computationally efficient method: fitting the hyperparameters on the first task and then fixing them throughout training.","sentences":["In continual learning (CL) -- where a learner trains on a stream of data -- standard hyperparameter optimisation (HPO) cannot be applied, as a learner does not have access to all of the data at the same time.","This has prompted the development of CL-specific HPO frameworks.","The most popular way to tune hyperparameters in CL is to repeatedly train over the whole data stream with different hyperparameter settings.","However, this end-of-training HPO is unrealistic as in practice a learner can only see the stream once.","Hence, there is an open question: what HPO framework should a practitioner use for a CL problem in reality?","This paper answers this question by evaluating several realistic HPO frameworks.","We find that all the HPO frameworks considered, including end-of-training HPO, perform similarly.","We therefore advocate using the realistic and most computationally efficient method: fitting the hyperparameters on the first task and then fixing them throughout training."],"url":"http://arxiv.org/abs/2404.06466v1"}
{"created":"2024-04-09 17:00:53","title":"Analysis of Distributed Algorithms for Big-data","abstract":"The parallel and distributed processing are becoming de facto industry standard, and a large part of the current research is targeted on how to make computing scalable and distributed, dynamically, without allocating the resources on permanent basis. The present article focuses on the study and performance of distributed and parallel algorithms their file systems, to achieve scalability at local level (OpenMP platform), and at global level where computing and file systems are distributed. Various applications, algorithms,file systems have been used to demonstrate the areas, and their performance studies have been presented. The systems and applications chosen here are of open-source nature, due to their wider applicability.","sentences":["The parallel and distributed processing are becoming de facto industry standard, and a large part of the current research is targeted on how to make computing scalable and distributed, dynamically, without allocating the resources on permanent basis.","The present article focuses on the study and performance of distributed and parallel algorithms their file systems, to achieve scalability at local level (OpenMP platform), and at global level where computing and file systems are distributed.","Various applications, algorithms,file systems have been used to demonstrate the areas, and their performance studies have been presented.","The systems and applications chosen here are of open-source nature, due to their wider applicability."],"url":"http://arxiv.org/abs/2404.06461v1"}
{"created":"2024-04-09 16:50:30","title":"Automated Federated Pipeline for Parameter-Efficient Fine-Tuning of Large Language Models","abstract":"Recently, there has been a surge in the development of advanced intelligent generative content (AIGC), especially large language models (LLMs). However, for many downstream tasks, it is necessary to fine-tune LLMs using private data. While federated learning offers a promising privacy-preserving solution to LLM fine-tuning, the substantial size of an LLM, combined with high computational and communication demands, makes it hard to apply to downstream tasks. More importantly, private edge servers often possess varying computing and network resources in real-world scenarios, introducing additional complexities to LLM fine-tuning. To tackle these problems, we design and implement an automated federated pipeline, named FedPipe, to fine-tune LLMs with minimal training cost but without adding any inference latency. FedPipe firstly identifies the weights to be fine-tuned based on their contributions to the LLM training. It then configures a low-rank adapter for each selected weight to train local low-rank adapters on an edge server, and aggregate local adapters of all edge servers to fine-tune the whole LLM. Finally, it appropriately quantizes the parameters of LLM to reduce memory space according to the requirements of edge servers. Extensive experiments demonstrate that FedPipe expedites the model training and achieves higher accuracy than state-of-the-art benchmarks.","sentences":["Recently, there has been a surge in the development of advanced intelligent generative content (AIGC), especially large language models (LLMs).","However, for many downstream tasks, it is necessary to fine-tune LLMs using private data.","While federated learning offers a promising privacy-preserving solution to LLM fine-tuning, the substantial size of an LLM, combined with high computational and communication demands, makes it hard to apply to downstream tasks.","More importantly, private edge servers often possess varying computing and network resources in real-world scenarios, introducing additional complexities to LLM fine-tuning.","To tackle these problems, we design and implement an automated federated pipeline, named FedPipe, to fine-tune LLMs with minimal training cost but without adding any inference latency.","FedPipe firstly identifies the weights to be fine-tuned based on their contributions to the LLM training.","It then configures a low-rank adapter for each selected weight to train local low-rank adapters on an edge server, and aggregate local adapters of all edge servers to fine-tune the whole LLM.","Finally, it appropriately quantizes the parameters of LLM to reduce memory space according to the requirements of edge servers.","Extensive experiments demonstrate that FedPipe expedites the model training and achieves higher accuracy than state-of-the-art benchmarks."],"url":"http://arxiv.org/abs/2404.06448v1"}
{"created":"2024-04-09 16:49:42","title":"The Central Spanning Tree Problem","abstract":"Spanning trees are an important primitive in many data analysis tasks, when a data set needs to be summarized in terms of its \"skeleton\", or when a tree-shaped graph over all observations is required for downstream processing. Popular definitions of spanning trees include the minimum spanning tree and the optimum distance spanning tree, a.k.a. the minimum routing cost tree. When searching for the shortest spanning tree but admitting additional branching points, even shorter spanning trees can be realized: Steiner trees. Unfortunately, both minimum spanning and Steiner trees are not robust with respect to noise in the observations; that is, small perturbations of the original data set often lead to drastic changes in the associated spanning trees. In response, we make two contributions when the data lies in a Euclidean space: on the theoretical side, we introduce a new optimization problem, the \"(branched) central spanning tree\", which subsumes all previously mentioned definitions as special cases. On the practical side, we show empirically that the (branched) central spanning tree is more robust to noise in the data, and as such is better suited to summarize a data set in terms of its skeleton. We also propose a heuristic to address the NP-hard optimization problem, and illustrate its use on single cell RNA expression data from biology and 3D point clouds of plants.","sentences":["Spanning trees are an important primitive in many data analysis tasks, when a data set needs to be summarized in terms of its \"skeleton\", or when a tree-shaped graph over all observations is required for downstream processing.","Popular definitions of spanning trees include the minimum spanning tree and the optimum distance spanning tree, a.k.a.","the minimum routing cost tree.","When searching for the shortest spanning tree but admitting additional branching points, even shorter spanning trees can be realized: Steiner trees.","Unfortunately, both minimum spanning and Steiner trees are not robust with respect to noise in the observations; that is, small perturbations of the original data set often lead to drastic changes in the associated spanning trees.","In response, we make two contributions when the data lies in a Euclidean space: on the theoretical side, we introduce a new optimization problem, the \"(branched) central spanning tree\", which subsumes all previously mentioned definitions as special cases.","On the practical side, we show empirically that the (branched) central spanning tree is more robust to noise in the data, and as such is better suited to summarize a data set in terms of its skeleton.","We also propose a heuristic to address the NP-hard optimization problem, and illustrate its use on single cell RNA expression data from biology and 3D point clouds of plants."],"url":"http://arxiv.org/abs/2404.06447v1"}
{"created":"2024-04-09 16:23:01","title":"pfl-research: simulation framework for accelerating research in Private Federated Learning","abstract":"Federated learning (FL) is an emerging machine learning (ML) training paradigm where clients own their data and collaborate to train a global model, without revealing any data to the server and other participants. Researchers commonly perform experiments in a simulation environment to quickly iterate on ideas. However, existing open-source tools do not offer the efficiency required to simulate FL on larger and more realistic FL datasets. We introduce pfl-research, a fast, modular, and easy-to-use Python framework for simulating FL. It supports TensorFlow, PyTorch, and non-neural network models, and is tightly integrated with state-of-the-art privacy algorithms. We study the speed of open-source FL frameworks and show that pfl-research is 7-72$\\times$ faster than alternative open-source frameworks on common cross-device setups. Such speedup will significantly boost the productivity of the FL research community and enable testing hypotheses on realistic FL datasets that were previously too resource intensive. We release a suite of benchmarks that evaluates an algorithm's overall performance on a diverse set of realistic scenarios. The code is available on GitHub at https://github.com/apple/pfl-research.","sentences":["Federated learning (FL) is an emerging machine learning (ML) training paradigm where clients own their data and collaborate to train a global model, without revealing any data to the server and other participants.","Researchers commonly perform experiments in a simulation environment to quickly iterate on ideas.","However, existing open-source tools do not offer the efficiency required to simulate FL on larger and more realistic FL datasets.","We introduce pfl-research, a fast, modular, and easy-to-use Python framework for simulating FL.","It supports TensorFlow, PyTorch, and non-neural network models, and is tightly integrated with state-of-the-art privacy algorithms.","We study the speed of open-source FL frameworks and show that pfl-research is 7-72$\\times$ faster than alternative open-source frameworks on common cross-device setups.","Such speedup will significantly boost the productivity of the FL research community and enable testing hypotheses on realistic FL datasets that were previously too resource intensive.","We release a suite of benchmarks that evaluates an algorithm's overall performance on a diverse set of realistic scenarios.","The code is available on GitHub at https://github.com/apple/pfl-research."],"url":"http://arxiv.org/abs/2404.06430v1"}
{"created":"2024-04-09 16:18:13","title":"A universal sequence of tensors for the asymptotic rank conjecture","abstract":"The exponent $\\sigma(T)$ of a tensor $T\\in\\mathbb{F}^d\\otimes\\mathbb{F}^d\\otimes\\mathbb{F}^d$ over a field $\\mathbb{F}$ captures the base of the exponential growth rate of the tensor rank of $T$ under Kronecker powers. Tensor exponents are fundamental from the standpoint of algorithms and computational complexity theory; for example, the exponent $\\omega$ of matrix multiplication can be characterized as $\\omega=2\\sigma(\\mathrm{MM}_2)$, where $\\mathrm{MM}_2\\in\\mathbb{F}^4\\otimes\\mathbb{F}^4\\otimes\\mathbb{F}^4$ is the tensor that represents $2\\times 2$ matrix multiplication.   Our main result is an explicit construction of a sequence $\\mathcal{U}_d$ of zero-one-valued tensors that is universal for the worst-case tensor exponent; more precisely, we show that $\\sigma(\\mathcal{U}_d)=\\sigma(d)$ where $\\sigma(d)=\\sup_{T\\in\\mathbb{F}^d\\otimes\\mathbb{F}^d\\otimes\\mathbb{F}^d}\\sigma(T)$. We also supply an explicit universal sequence $\\mathcal{U}_\\Delta$ localised to capture the worst-case exponent $\\sigma(\\Delta)$ of tensors with support contained in $\\Delta\\subseteq [d]\\times[d]\\times [d]$; by combining such sequences, we obtain a universal sequence $\\mathcal{T}_d$ such that $\\sigma(\\mathcal{T}_d)=1$ holds if and only if Strassen's asymptotic rank conjecture [Progr. Math. 120 (1994)] holds for $d$. Finally, we show that the limit $\\lim_{d\\rightarrow\\infty}\\sigma(d)$ exists and can be captured as $\\lim_{d\\rightarrow\\infty} \\sigma(D_d)$ for an explicit sequence $(D_d)_{d=1}^\\infty$ of tensors obtained by diagonalisation of the sequences $\\mathcal{U}_d$. As our second result we relate the absence of polynomials of fixed degree vanishing on tensors of low rank, or more generally asymptotic rank, with upper bounds on the exponent $\\sigma(d)$. Using this technique, one may bound asymptotic rank for all tensors of a given format, knowing enough specific tensors of low asymptotic rank.","sentences":["The exponent $\\sigma(T)$ of a tensor $T\\in\\mathbb{F}^d\\otimes\\mathbb{F}^d\\otimes\\mathbb{F}^d$ over a field $\\mathbb{F}$ captures the base of the exponential growth rate of the tensor rank of $T$ under Kronecker powers.","Tensor exponents are fundamental from the standpoint of algorithms and computational complexity theory; for example, the exponent $\\omega$ of matrix multiplication can be characterized as $\\omega=2\\sigma(\\mathrm{MM}_2)$, where $\\mathrm{MM}_2\\in\\mathbb{F}^4\\otimes\\mathbb{F}^4\\otimes\\mathbb{F}^4$ is the tensor that represents $2\\times 2$ matrix multiplication.   ","Our main result is an explicit construction of a sequence $\\mathcal{U}_d$ of zero-one-valued tensors that is universal for the worst-case tensor exponent; more precisely, we show that $\\sigma(\\mathcal{U}_d)=\\sigma(d)$ where $\\sigma(d)=\\sup_{T\\in\\mathbb{F}^d\\otimes\\mathbb{F}^d\\otimes\\mathbb{F}^d}\\sigma(T)$. We also supply an explicit universal sequence $\\mathcal{U}_\\Delta$ localised to capture the worst-case exponent $\\sigma(\\Delta)$ of tensors with support contained in $\\Delta\\subseteq [d]\\times[d]\\times [d]$; by combining such sequences, we obtain a universal sequence $\\mathcal{T}_d$ such that $\\sigma(\\mathcal{T}_d)=1$ holds if and only if Strassen's asymptotic rank conjecture","[Progr.","Math. 120 (1994)] holds for $d$. Finally, we show that the limit $\\lim_{d\\rightarrow\\infty}\\sigma(d)$ exists and can be captured as $\\lim_{d\\rightarrow\\infty} \\sigma(D_d)$ for an explicit sequence $(D_d)_{d=1}^\\infty$ of tensors obtained by diagonalisation of the sequences $\\mathcal{U}_d$. As our second result we relate the absence of polynomials of fixed degree vanishing on tensors of low rank, or more generally asymptotic rank, with upper bounds on the exponent $\\sigma(d)$. Using this technique, one may bound asymptotic rank for all tensors of a given format, knowing enough specific tensors of low asymptotic rank."],"url":"http://arxiv.org/abs/2404.06427v1"}
{"created":"2024-04-09 16:03:26","title":"Large Language Models to the Rescue: Deadlock Resolution in Multi-Robot Systems","abstract":"Multi-agent robotic systems are prone to deadlocks in an obstacle environment where the system can get stuck away from its desired location under a smooth low-level control policy. Without an external intervention, often in terms of a high-level command, it is not possible to guarantee that just a low-level control policy can resolve such deadlocks. Utilizing the generalizability and low data requirements of large language models (LLMs), this paper explores the possibility of using LLMs for deadlock resolution. We propose a hierarchical control framework where an LLM resolves deadlocks by assigning a leader and direction for the leader to move along. A graph neural network (GNN) based low-level distributed control policy executes the assigned plan. We systematically study various prompting techniques to improve LLM's performance in resolving deadlocks. In particular, as part of prompt engineering, we provide in-context examples for LLMs. We conducted extensive experiments on various multi-robot environments with up to 15 agents and 40 obstacles. Our results demonstrate that LLM-based high-level planners are effective in resolving deadlocks in MRS.","sentences":["Multi-agent robotic systems are prone to deadlocks in an obstacle environment where the system can get stuck away from its desired location under a smooth low-level control policy.","Without an external intervention, often in terms of a high-level command, it is not possible to guarantee that just a low-level control policy can resolve such deadlocks.","Utilizing the generalizability and low data requirements of large language models (LLMs), this paper explores the possibility of using LLMs for deadlock resolution.","We propose a hierarchical control framework where an LLM resolves deadlocks by assigning a leader and direction for the leader to move along.","A graph neural network (GNN) based low-level distributed control policy executes the assigned plan.","We systematically study various prompting techniques to improve LLM's performance in resolving deadlocks.","In particular, as part of prompt engineering, we provide in-context examples for LLMs.","We conducted extensive experiments on various multi-robot environments with up to 15 agents and 40 obstacles.","Our results demonstrate that LLM-based high-level planners are effective in resolving deadlocks in MRS."],"url":"http://arxiv.org/abs/2404.06413v1"}
{"created":"2024-04-09 15:53:06","title":"Apprentices to Research Assistants: Advancing Research with Large Language Models","abstract":"Large Language Models (LLMs) have emerged as powerful tools in various research domains. This article examines their potential through a literature review and firsthand experimentation. While LLMs offer benefits like cost-effectiveness and efficiency, challenges such as prompt tuning, biases, and subjectivity must be addressed. The study presents insights from experiments utilizing LLMs for qualitative analysis, highlighting successes and limitations. Additionally, it discusses strategies for mitigating challenges, such as prompt optimization techniques and leveraging human expertise. This study aligns with the 'LLMs as Research Tools' workshop's focus on integrating LLMs into HCI data work critically and ethically. By addressing both opportunities and challenges, our work contributes to the ongoing dialogue on their responsible application in research.","sentences":["Large Language Models (LLMs) have emerged as powerful tools in various research domains.","This article examines their potential through a literature review and firsthand experimentation.","While LLMs offer benefits like cost-effectiveness and efficiency, challenges such as prompt tuning, biases, and subjectivity must be addressed.","The study presents insights from experiments utilizing LLMs for qualitative analysis, highlighting successes and limitations.","Additionally, it discusses strategies for mitigating challenges, such as prompt optimization techniques and leveraging human expertise.","This study aligns with the 'LLMs as Research Tools' workshop's focus on integrating LLMs into HCI data work critically and ethically.","By addressing both opportunities and challenges, our work contributes to the ongoing dialogue on their responsible application in research."],"url":"http://arxiv.org/abs/2404.06404v1"}
{"created":"2024-04-09 15:53:02","title":"Online Learning of Decision Trees with Thompson Sampling","abstract":"Decision Trees are prominent prediction models for interpretable Machine Learning. They have been thoroughly researched, mostly in the batch setting with a fixed labelled dataset, leading to popular algorithms such as C4.5, ID3 and CART. Unfortunately, these methods are of heuristic nature, they rely on greedy splits offering no guarantees of global optimality and often leading to unnecessarily complex and hard-to-interpret Decision Trees. Recent breakthroughs addressed this suboptimality issue in the batch setting, but no such work has considered the online setting with data arriving in a stream. To this end, we devise a new Monte Carlo Tree Search algorithm, Thompson Sampling Decision Trees (TSDT), able to produce optimal Decision Trees in an online setting. We analyse our algorithm and prove its almost sure convergence to the optimal tree. Furthermore, we conduct extensive experiments to validate our findings empirically. The proposed TSDT outperforms existing algorithms on several benchmarks, all while presenting the practical advantage of being tailored to the online setting.","sentences":["Decision Trees are prominent prediction models for interpretable Machine Learning.","They have been thoroughly researched, mostly in the batch setting with a fixed labelled dataset, leading to popular algorithms such as C4.5, ID3 and CART.","Unfortunately, these methods are of heuristic nature, they rely on greedy splits offering no guarantees of global optimality and often leading to unnecessarily complex and hard-to-interpret Decision Trees.","Recent breakthroughs addressed this suboptimality issue in the batch setting, but no such work has considered the online setting with data arriving in a stream.","To this end, we devise a new Monte Carlo Tree Search algorithm, Thompson Sampling Decision Trees (TSDT), able to produce optimal Decision Trees in an online setting.","We analyse our algorithm and prove its almost sure convergence to the optimal tree.","Furthermore, we conduct extensive experiments to validate our findings empirically.","The proposed TSDT outperforms existing algorithms on several benchmarks, all while presenting the practical advantage of being tailored to the online setting."],"url":"http://arxiv.org/abs/2404.06403v1"}
{"created":"2024-04-09 15:48:31","title":"Bounded Edit Distance: Optimal Static and Dynamic Algorithms for Small Integer Weights","abstract":"The edit distance of two strings is the minimum number of insertions, deletions, and substitutions needed to transform one string into the other. The textbook algorithm determines the edit distance of length-$n$ strings in $O(n^2)$ time, which is optimal up to subpolynomial factors under Orthogonal Vectors Hypothesis. In the bounded version of the problem, parameterized by the edit distance $k$, the algorithm of Landau and Vishkin [JCSS'88] achieves $O(n+k^2)$ time, which is optimal as a function of $n$ and $k$.   The dynamic version of the problem asks to maintain the edit distance of two strings that change dynamically, with each update modeled as an edit. A folklore approach supports updates in $\\tilde O(k^2)$ time, where $\\tilde O(\\cdot)$ hides polylogarithmic factors. Recently, Charalampopoulos, Kociumaka, and Mozes [CPM'20] showed an algorithm with update time $\\tilde O(n)$, which is optimal under OVH in terms of $n$. The update time of $\\tilde O(\\min\\{n,k^2\\})$ raised an exciting open question of whether $\\tilde O(k)$ is possible; we answer it affirmatively.   Our solution relies on tools originating from weighted edit distance, where the weight of each edit depends on the edit type and the characters involved. The textbook algorithm supports weights, but the Landau-Vishkin approach does not, and a simple $O(nk)$-time procedure long remained the fastest for bounded weighted edit distance. Only recently, Das et al. [STOC'23] provided an $O(n+k^5)$-time algorithm, whereas Cassis, Kociumaka, and Wellnitz [FOCS'23] presented an $\\tilde O(n+\\sqrt{nk^3})$-time solution and a matching conditional lower bound. In this paper, we show that, for integer edit weights between $0$ and $W$, weighted edit distance can be computed in $\\tilde O(n+Wk^2)$ time and maintained dynamically in $\\tilde O(W^2k)$ time per update. Our static algorithm can also be implemented in $\\tilde O(n+k^{2.5})$ time.","sentences":["The edit distance of two strings is the minimum number of insertions, deletions, and substitutions needed to transform one string into the other.","The textbook algorithm determines the edit distance of length-$n$ strings in $O(n^2)$ time, which is optimal up to subpolynomial factors under Orthogonal Vectors Hypothesis.","In the bounded version of the problem, parameterized by the edit distance $k$, the algorithm of Landau and Vishkin","[JCSS'88] achieves $O(n+k^2)$ time, which is optimal as a function of $n$ and $k$.   The dynamic version of the problem asks to maintain the edit distance of two strings that change dynamically, with each update modeled as an edit.","A folklore approach supports updates in $\\tilde O(k^2)$ time, where $\\tilde O(\\cdot)$ hides polylogarithmic factors.","Recently, Charalampopoulos, Kociumaka, and Mozes [CPM'20] showed an algorithm with update time $\\tilde O(n)$, which is optimal under OVH in terms of $n$. The update time of $\\tilde O(\\min\\{n,k^2\\})$ raised an exciting open question of whether $\\tilde O(k)$ is possible; we answer it affirmatively.   ","Our solution relies on tools originating from weighted edit distance, where the weight of each edit depends on the edit type and the characters involved.","The textbook algorithm supports weights, but the Landau-Vishkin approach does not, and a simple $O(nk)$-time procedure long remained the fastest for bounded weighted edit distance.","Only recently, Das et al.","[STOC'23] provided an $O(n+k^5)$-time algorithm, whereas Cassis, Kociumaka, and Wellnitz","[FOCS'23] presented an $\\tilde O(n+\\sqrt{nk^3})$-time solution and a matching conditional lower bound.","In this paper, we show that, for integer edit weights between $0$ and $W$, weighted edit distance can be computed in $\\tilde O(n+Wk^2)$ time and maintained dynamically in $\\tilde O(W^2k)$ time per update.","Our static algorithm can also be implemented in $\\tilde O(n+k^{2.5})$ time."],"url":"http://arxiv.org/abs/2404.06401v1"}
{"created":"2024-04-09 15:36:50","title":"MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies","abstract":"The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM .","sentences":["The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation.","This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative.","In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs.","While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research.","Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling.","For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation.","We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS.","With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal.","Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications.","MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM ."],"url":"http://arxiv.org/abs/2404.06395v1"}
{"created":"2024-04-09 15:35:52","title":"MuPT: A Generative Symbolic Music Pretrained Transformer","abstract":"In this paper, we explore the application of Large Language Models (LLMs) to the pre-training of music. While the prevalent use of MIDI in music modeling is well-established, our findings suggest that LLMs are inherently more compatible with ABC Notation, which aligns more closely with their design and strengths, thereby enhancing the model's performance in musical composition. To address the challenges associated with misaligned measures from different tracks during generation, we propose the development of a \\underline{S}ynchronized \\underline{M}ulti-\\underline{T}rack ABC Notation (\\textbf{SMT-ABC Notation}), which aims to preserve coherence across multiple musical tracks. Our contributions include a series of models capable of handling up to 8192 tokens, covering 90\\% of the symbolic music data in our training set. Furthermore, we explore the implications of the \\underline{S}ymbolic \\underline{M}usic \\underline{S}caling Law (\\textbf{SMS Law}) on model performance. The results indicate a promising direction for future research in music generation, offering extensive resources for community-led research through our open-source contributions.","sentences":["In this paper, we explore the application of Large Language Models (LLMs) to the pre-training of music.","While the prevalent use of MIDI in music modeling is well-established, our findings suggest that LLMs are inherently more compatible with ABC Notation, which aligns more closely with their design and strengths, thereby enhancing the model's performance in musical composition.","To address the challenges associated with misaligned measures from different tracks during generation, we propose the development of a \\underline{S}ynchronized \\underline{M}ulti-\\underline{T}rack ABC Notation (\\textbf{SMT-ABC Notation}), which aims to preserve coherence across multiple musical tracks.","Our contributions include a series of models capable of handling up to 8192 tokens, covering 90\\% of the symbolic music data in our training set.","Furthermore, we explore the implications of the \\underline{S}ymbolic \\underline{M}usic \\underline{S}caling Law (\\textbf{SMS Law}) on model performance.","The results indicate a promising direction for future research in music generation, offering extensive resources for community-led research through our open-source contributions."],"url":"http://arxiv.org/abs/2404.06393v1"}
{"created":"2024-04-09 15:05:48","title":"VISION2UI: A Real-World Dataset with Layout for Code Generation from UI Designs","abstract":"Automatically generating UI code from webpage design visions can significantly alleviate the burden of developers, enabling beginner developers or designers to directly generate Web pages from design diagrams. Currently, prior research has accomplished the objective of generating UI code from rudimentary design visions or sketches through designing deep neural networks. Inspired by the groundbreaking advancements achieved by Multimodal Large Language Models (MLLMs), the automatic generation of UI code from high-fidelity design images is now emerging as a viable possibility. Nevertheless, our investigation reveals that existing MLLMs are hampered by the scarcity of authentic, high-quality, and large-scale datasets, leading to unsatisfactory performance in automated UI code generation. To mitigate this gap, we present a novel dataset, termed VISION2UI, extracted from real-world scenarios, augmented with comprehensive layout information, tailored specifically for finetuning MLLMs in UI code generation. Specifically, this dataset is derived through a series of operations, encompassing collecting, cleaning, and filtering of the open-source Common Crawl dataset. In order to uphold its quality, a neural scorer trained on labeled samples is utilized to refine the data, retaining higher-quality instances. Ultimately, this process yields a dataset comprising 2,000 (Much more is coming soon) parallel samples encompassing design visions and UI code. The dataset is available at https://huggingface.co/datasets/xcodemind/vision2ui.","sentences":["Automatically generating UI code from webpage design visions can significantly alleviate the burden of developers, enabling beginner developers or designers to directly generate Web pages from design diagrams.","Currently, prior research has accomplished the objective of generating UI code from rudimentary design visions or sketches through designing deep neural networks.","Inspired by the groundbreaking advancements achieved by Multimodal Large Language Models (MLLMs), the automatic generation of UI code from high-fidelity design images is now emerging as a viable possibility.","Nevertheless, our investigation reveals that existing MLLMs are hampered by the scarcity of authentic, high-quality, and large-scale datasets, leading to unsatisfactory performance in automated UI code generation.","To mitigate this gap, we present a novel dataset, termed VISION2UI, extracted from real-world scenarios, augmented with comprehensive layout information, tailored specifically for finetuning MLLMs in UI code generation.","Specifically, this dataset is derived through a series of operations, encompassing collecting, cleaning, and filtering of the open-source Common Crawl dataset.","In order to uphold its quality, a neural scorer trained on labeled samples is utilized to refine the data, retaining higher-quality instances.","Ultimately, this process yields a dataset comprising 2,000 (Much more is coming soon) parallel samples encompassing design visions and UI code.","The dataset is available at https://huggingface.co/datasets/xcodemind/vision2ui."],"url":"http://arxiv.org/abs/2404.06369v1"}
{"created":"2024-04-09 15:04:27","title":"ClinLinker: Medical Entity Linking of Clinical Concept Mentions in Spanish","abstract":"Advances in natural language processing techniques, such as named entity recognition and normalization to widely used standardized terminologies like UMLS or SNOMED-CT, along with the digitalization of electronic health records, have significantly advanced clinical text analysis. This study presents ClinLinker, a novel approach employing a two-phase pipeline for medical entity linking that leverages the potential of in-domain adapted language models for biomedical text mining: initial candidate retrieval using a SapBERT-based bi-encoder and subsequent re-ranking with a cross-encoder, trained by following a contrastive-learning strategy to be tailored to medical concepts in Spanish. This methodology, focused initially on content in Spanish, substantially outperforming multilingual language models designed for the same purpose. This is true even for complex scenarios involving heterogeneous medical terminologies and being trained on a subset of the original data. Our results, evaluated using top-k accuracy at 25 and other top-k metrics, demonstrate our approach's performance on two distinct clinical entity linking Gold Standard corpora, DisTEMIST (diseases) and MedProcNER (clinical procedures), outperforming previous benchmarks by 40 points in DisTEMIST and 43 points in MedProcNER, both normalized to SNOMED-CT codes. These findings highlight our approach's ability to address language-specific nuances and set a new benchmark in entity linking, offering a potent tool for enhancing the utility of digital medical records. The resulting system is of practical value, both for large scale automatic generation of structured data derived from clinical records, as well as for exhaustive extraction and harmonization of predefined clinical variables of interest.","sentences":["Advances in natural language processing techniques, such as named entity recognition and normalization to widely used standardized terminologies like UMLS or SNOMED-CT, along with the digitalization of electronic health records, have significantly advanced clinical text analysis.","This study presents ClinLinker, a novel approach employing a two-phase pipeline for medical entity linking that leverages the potential of in-domain adapted language models for biomedical text mining: initial candidate retrieval using a SapBERT-based bi-encoder and subsequent re-ranking with a cross-encoder, trained by following a contrastive-learning strategy to be tailored to medical concepts in Spanish.","This methodology, focused initially on content in Spanish, substantially outperforming multilingual language models designed for the same purpose.","This is true even for complex scenarios involving heterogeneous medical terminologies and being trained on a subset of the original data.","Our results, evaluated using top-k accuracy at 25 and other top-k metrics, demonstrate our approach's performance on two distinct clinical entity linking Gold Standard corpora, DisTEMIST (diseases) and MedProcNER (clinical procedures), outperforming previous benchmarks by 40 points in DisTEMIST and 43 points in MedProcNER, both normalized to SNOMED-CT codes.","These findings highlight our approach's ability to address language-specific nuances and set a new benchmark in entity linking, offering a potent tool for enhancing the utility of digital medical records.","The resulting system is of practical value, both for large scale automatic generation of structured data derived from clinical records, as well as for exhaustive extraction and harmonization of predefined clinical variables of interest."],"url":"http://arxiv.org/abs/2404.06367v1"}
{"created":"2024-04-09 14:56:34","title":"Test-Time Adaptation with SaLIP: A Cascade of SAM and CLIP for Zero shot Medical Image Segmentation","abstract":"The Segment Anything Model (SAM) and CLIP are remarkable vision foundation models (VFMs). SAM, a prompt driven segmentation model, excels in segmentation tasks across diverse domains, while CLIP is renowned for its zero shot recognition capabilities. However, their unified potential has not yet been explored in medical image segmentation. To adapt SAM to medical imaging, existing methods primarily rely on tuning strategies that require extensive data or prior prompts tailored to the specific task, making it particularly challenging when only a limited number of data samples are available. This work presents an in depth exploration of integrating SAM and CLIP into a unified framework for medical image segmentation. Specifically, we propose a simple unified framework, SaLIP, for organ segmentation. Initially, SAM is used for part based segmentation within the image, followed by CLIP to retrieve the mask corresponding to the region of interest (ROI) from the pool of SAM generated masks. Finally, SAM is prompted by the retrieved ROI to segment a specific organ. Thus, SaLIP is training and fine tuning free and does not rely on domain expertise or labeled data for prompt engineering. Our method shows substantial enhancements in zero shot segmentation, showcasing notable improvements in DICE scores across diverse segmentation tasks like brain (63.46%), lung (50.11%), and fetal head (30.82%), when compared to un prompted SAM. Code and text prompts will be available online.","sentences":["The Segment Anything Model (SAM) and CLIP are remarkable vision foundation models (VFMs).","SAM, a prompt driven segmentation model, excels in segmentation tasks across diverse domains, while CLIP is renowned for its zero shot recognition capabilities.","However, their unified potential has not yet been explored in medical image segmentation.","To adapt SAM to medical imaging, existing methods primarily rely on tuning strategies that require extensive data or prior prompts tailored to the specific task, making it particularly challenging when only a limited number of data samples are available.","This work presents an in depth exploration of integrating SAM and CLIP into a unified framework for medical image segmentation.","Specifically, we propose a simple unified framework, SaLIP, for organ segmentation.","Initially, SAM is used for part based segmentation within the image, followed by CLIP to retrieve the mask corresponding to the region of interest (ROI) from the pool of SAM generated masks.","Finally, SAM is prompted by the retrieved ROI to segment a specific organ.","Thus, SaLIP is training and fine tuning free and does not rely on domain expertise or labeled data for prompt engineering.","Our method shows substantial enhancements in zero shot segmentation, showcasing notable improvements in DICE scores across diverse segmentation tasks like brain (63.46%), lung (50.11%), and fetal head (30.82%), when compared to un prompted SAM.","Code and text prompts will be available online."],"url":"http://arxiv.org/abs/2404.06362v1"}
{"created":"2024-04-09 14:53:59","title":"Towards Practical Meshlet Compression","abstract":"We propose a codec specifically designed for meshlet compression, optimized for rapid data-parallel GPU decompression within a mesh shader. Our compression strategy orders triangles in optimal generalized triangle strips (GTSs), which we generate by formulating the creation as a mixed integer linear program (MILP). Our method achieves index buffer compression rates of 16:1 compared to the vertex pipeline and crack-free vertex attribute quantization based on user preference. The 15.5 million triangles of our teaser image decompress and render in 0.59 ms on an AMD Radeon RX 7900 XTX.","sentences":["We propose a codec specifically designed for meshlet compression, optimized for rapid data-parallel GPU decompression within a mesh shader.","Our compression strategy orders triangles in optimal generalized triangle strips (GTSs), which we generate by formulating the creation as a mixed integer linear program (MILP).","Our method achieves index buffer compression rates of 16:1 compared to the vertex pipeline and crack-free vertex attribute quantization based on user preference.","The 15.5 million triangles of our teaser image decompress and render in 0.59 ms on an AMD Radeon RX 7900 XTX."],"url":"http://arxiv.org/abs/2404.06359v1"}
{"created":"2024-04-09 14:46:48","title":"Policy-Guided Diffusion","abstract":"In many real-world settings, agents must learn from an offline dataset gathered by some prior behavior policy. Such a setting naturally leads to distribution shift between the behavior policy and the target policy being trained - requiring policy conservatism to avoid instability and overestimation bias. Autoregressive world models offer a different solution to this by generating synthetic, on-policy experience. However, in practice, model rollouts must be severely truncated to avoid compounding error. As an alternative, we propose policy-guided diffusion. Our method uses diffusion models to generate entire trajectories under the behavior distribution, applying guidance from the target policy to move synthetic experience further on-policy. We show that policy-guided diffusion models a regularized form of the target distribution that balances action likelihood under both the target and behavior policies, leading to plausible trajectories with high target policy probability, while retaining a lower dynamics error than an offline world model baseline. Using synthetic experience from policy-guided diffusion as a drop-in substitute for real data, we demonstrate significant improvements in performance across a range of standard offline reinforcement learning algorithms and environments. Our approach provides an effective alternative to autoregressive offline world models, opening the door to the controllable generation of synthetic training data.","sentences":["In many real-world settings, agents must learn from an offline dataset gathered by some prior behavior policy.","Such a setting naturally leads to distribution shift between the behavior policy and the target policy being trained - requiring policy conservatism to avoid instability and overestimation bias.","Autoregressive world models offer a different solution to this by generating synthetic, on-policy experience.","However, in practice, model rollouts must be severely truncated to avoid compounding error.","As an alternative, we propose policy-guided diffusion.","Our method uses diffusion models to generate entire trajectories under the behavior distribution, applying guidance from the target policy to move synthetic experience further on-policy.","We show that policy-guided diffusion models a regularized form of the target distribution that balances action likelihood under both the target and behavior policies, leading to plausible trajectories with high target policy probability, while retaining a lower dynamics error than an offline world model baseline.","Using synthetic experience from policy-guided diffusion as a drop-in substitute for real data, we demonstrate significant improvements in performance across a range of standard offline reinforcement learning algorithms and environments.","Our approach provides an effective alternative to autoregressive offline world models, opening the door to the controllable generation of synthetic training data."],"url":"http://arxiv.org/abs/2404.06356v1"}
{"created":"2024-04-09 14:40:08","title":"CausalBench: A Comprehensive Benchmark for Causal Learning Capability of Large Language Models","abstract":"Causality reveals fundamental principles behind data distributions in real-world scenarios, and the capability of large language models (LLMs) to understand causality directly impacts their efficacy across explaining outputs, adapting to new evidence, and generating counterfactuals. With the proliferation of LLMs, the evaluation of this capacity is increasingly garnering attention. However, the absence of a comprehensive benchmark has rendered existing evaluation studies being straightforward, undiversified, and homogeneous. To address these challenges, this paper proposes a comprehensive benchmark, namely CausalBench, to evaluate the causality understanding capabilities of LLMs. Originating from the causal research community, CausalBench encompasses three causal learning-related tasks, which facilitate a convenient comparison of LLMs' performance with classic causal learning algorithms. Meanwhile, causal networks of varying scales and densities are integrated in CausalBench, to explore the upper limits of LLMs' capabilities across task scenarios of varying difficulty. Notably, background knowledge and structured data are also incorporated into CausalBench to thoroughly unlock the underlying potential of LLMs for long-text comprehension and prior information utilization. Based on CausalBench, this paper evaluates nineteen leading LLMs and unveils insightful conclusions in diverse aspects. Firstly, we present the strengths and weaknesses of LLMs and quantitatively explore the upper limits of their capabilities across various scenarios. Meanwhile, we further discern the adaptability and abilities of LLMs to specific structural networks and complex chain of thought structures. Moreover, this paper quantitatively presents the differences across diverse information sources and uncovers the gap between LLMs' capabilities in causal understanding within textual contexts and numerical domains.","sentences":["Causality reveals fundamental principles behind data distributions in real-world scenarios, and the capability of large language models (LLMs) to understand causality directly impacts their efficacy across explaining outputs, adapting to new evidence, and generating counterfactuals.","With the proliferation of LLMs, the evaluation of this capacity is increasingly garnering attention.","However, the absence of a comprehensive benchmark has rendered existing evaluation studies being straightforward, undiversified, and homogeneous.","To address these challenges, this paper proposes a comprehensive benchmark, namely CausalBench, to evaluate the causality understanding capabilities of LLMs.","Originating from the causal research community, CausalBench encompasses three causal learning-related tasks, which facilitate a convenient comparison of LLMs' performance with classic causal learning algorithms.","Meanwhile, causal networks of varying scales and densities are integrated in CausalBench, to explore the upper limits of LLMs' capabilities across task scenarios of varying difficulty.","Notably, background knowledge and structured data are also incorporated into CausalBench to thoroughly unlock the underlying potential of LLMs for long-text comprehension and prior information utilization.","Based on CausalBench, this paper evaluates nineteen leading LLMs and unveils insightful conclusions in diverse aspects.","Firstly, we present the strengths and weaknesses of LLMs and quantitatively explore the upper limits of their capabilities across various scenarios.","Meanwhile, we further discern the adaptability and abilities of LLMs to specific structural networks and complex chain of thought structures.","Moreover, this paper quantitatively presents the differences across diverse information sources and uncovers the gap between LLMs' capabilities in causal understanding within textual contexts and numerical domains."],"url":"http://arxiv.org/abs/2404.06349v1"}
{"created":"2024-04-09 14:33:16","title":"AgentsCoDriver: Large Language Model Empowered Collaborative Driving with Lifelong Learning","abstract":"Connected and autonomous driving is developing rapidly in recent years. However, current autonomous driving systems, which are primarily based on data-driven approaches, exhibit deficiencies in interpretability, generalization, and continuing learning capabilities. In addition, the single-vehicle autonomous driving systems lack of the ability of collaboration and negotiation with other vehicles, which is crucial for the safety and efficiency of autonomous driving systems. In order to address these issues, we leverage large language models (LLMs) to develop a novel framework, AgentsCoDriver, to enable multiple vehicles to conduct collaborative driving. AgentsCoDriver consists of five modules: observation module, reasoning engine, cognitive memory module, reinforcement reflection module, and communication module. It can accumulate knowledge, lessons, and experiences over time by continuously interacting with the environment, thereby making itself capable of lifelong learning. In addition, by leveraging the communication module, different agents can exchange information and realize negotiation and collaboration in complex traffic environments. Extensive experiments are conducted and show the superiority of AgentsCoDriver.","sentences":["Connected and autonomous driving is developing rapidly in recent years.","However, current autonomous driving systems, which are primarily based on data-driven approaches, exhibit deficiencies in interpretability, generalization, and continuing learning capabilities.","In addition, the single-vehicle autonomous driving systems lack of the ability of collaboration and negotiation with other vehicles, which is crucial for the safety and efficiency of autonomous driving systems.","In order to address these issues, we leverage large language models (LLMs) to develop a novel framework, AgentsCoDriver, to enable multiple vehicles to conduct collaborative driving.","AgentsCoDriver consists of five modules: observation module, reasoning engine, cognitive memory module, reinforcement reflection module, and communication module.","It can accumulate knowledge, lessons, and experiences over time by continuously interacting with the environment, thereby making itself capable of lifelong learning.","In addition, by leveraging the communication module, different agents can exchange information and realize negotiation and collaboration in complex traffic environments.","Extensive experiments are conducted and show the superiority of AgentsCoDriver."],"url":"http://arxiv.org/abs/2404.06345v1"}
{"created":"2024-04-09 14:33:03","title":"Synaptogen: A cross-domain generative device model for large-scale neuromorphic circuit design","abstract":"We present a fast generative modeling approach for resistive memories that reproduces the complex statistical properties of real-world devices. To enable efficient modeling of analog circuits, the model is implemented in Verilog-A. By training on extensive measurement data of integrated 1T1R arrays (6,000 cycles of 512 devices), an autoregressive stochastic process accurately accounts for the cross-correlations between the switching parameters, while non-linear transformations ensure agreement with both cycle-to-cycle (C2C) and device-to-device (D2D) variability. Benchmarks show that this statistically comprehensive model achieves read/write throughputs exceeding those of even highly simplified and deterministic compact models.","sentences":["We present a fast generative modeling approach for resistive memories that reproduces the complex statistical properties of real-world devices.","To enable efficient modeling of analog circuits, the model is implemented in Verilog-A. By training on extensive measurement data of integrated 1T1R arrays (6,000 cycles of 512 devices), an autoregressive stochastic process accurately accounts for the cross-correlations between the switching parameters, while non-linear transformations ensure agreement with both cycle-to-cycle (C2C) and device-to-device (D2D) variability.","Benchmarks show that this statistically comprehensive model achieves read/write throughputs exceeding those of even highly simplified and deterministic compact models."],"url":"http://arxiv.org/abs/2404.06344v1"}
{"created":"2024-04-09 14:25:27","title":"Finding fake reviews in e-commerce platforms by using hybrid algorithms","abstract":"Sentiment analysis, a vital component in natural language processing, plays a crucial role in understanding the underlying emotions and opinions expressed in textual data. In this paper, we propose an innovative ensemble approach for sentiment analysis for finding fake reviews that amalgamate the predictive capabilities of Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Decision Tree classifiers. Our ensemble architecture strategically combines these diverse models to capitalize on their strengths while mitigating inherent weaknesses, thereby achieving superior accuracy and robustness in fake review prediction. By combining all the models of our classifiers, the predictive performance is boosted and it also fosters adaptability to varied linguistic patterns and nuances present in real-world datasets. The metrics accounted for on fake reviews demonstrate the efficacy and competitiveness of the proposed ensemble method against traditional single-model approaches. Our findings underscore the potential of ensemble techniques in advancing the state-of-the-art in finding fake reviews using hybrid algorithms, with implications for various applications in different social media and e-platforms to find the best reviews and neglect the fake ones, eliminating puffery and bluffs.","sentences":["Sentiment analysis, a vital component in natural language processing, plays a crucial role in understanding the underlying emotions and opinions expressed in textual data.","In this paper, we propose an innovative ensemble approach for sentiment analysis for finding fake reviews that amalgamate the predictive capabilities of Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Decision Tree classifiers.","Our ensemble architecture strategically combines these diverse models to capitalize on their strengths while mitigating inherent weaknesses, thereby achieving superior accuracy and robustness in fake review prediction.","By combining all the models of our classifiers, the predictive performance is boosted and it also fosters adaptability to varied linguistic patterns and nuances present in real-world datasets.","The metrics accounted for on fake reviews demonstrate the efficacy and competitiveness of the proposed ensemble method against traditional single-model approaches.","Our findings underscore the potential of ensemble techniques in advancing the state-of-the-art in finding fake reviews using hybrid algorithms, with implications for various applications in different social media and e-platforms to find the best reviews and neglect the fake ones, eliminating puffery and bluffs."],"url":"http://arxiv.org/abs/2404.06339v1"}
{"created":"2024-04-09 14:08:47","title":"Generative Pre-Trained Transformer for Symbolic Regression Base In-Context Reinforcement Learning","abstract":"The mathematical formula is the human language to describe nature and is the essence of scientific research. Finding mathematical formulas from observational data is a major demand of scientific research and a major challenge of artificial intelligence. This area is called symbolic regression. Originally symbolic regression was often formulated as a combinatorial optimization problem and solved using GP or reinforcement learning algorithms. These two kinds of algorithms have strong noise robustness ability and good Versatility. However, inference time usually takes a long time, so the search efficiency is relatively low. Later, based on large-scale pre-training data proposed, such methods use a large number of synthetic data points and expression pairs to train a Generative Pre-Trained Transformer(GPT). Then this GPT can only need to perform one forward propagation to obtain the results, the advantage is that the inference speed is very fast. However, its performance is very dependent on the training data and performs poorly on data outside the training set, which leads to poor noise robustness and Versatility of such methods. So, can we combine the advantages of the above two categories of SR algorithms? In this paper, we propose \\textbf{FormulaGPT}, which trains a GPT using massive sparse reward learning histories of reinforcement learning-based SR algorithms as training data. After training, the SR algorithm based on reinforcement learning is distilled into a Transformer. When new test data comes, FormulaGPT can directly generate a \"reinforcement learning process\" and automatically update the learning policy in context. Tested on more than ten datasets including SRBench, formulaGPT achieves the state-of-the-art performance in fitting ability compared with four baselines. In addition, it achieves satisfactory results in noise robustness, versatility, and inference efficiency.","sentences":["The mathematical formula is the human language to describe nature and is the essence of scientific research.","Finding mathematical formulas from observational data is a major demand of scientific research and a major challenge of artificial intelligence.","This area is called symbolic regression.","Originally symbolic regression was often formulated as a combinatorial optimization problem and solved using GP or reinforcement learning algorithms.","These two kinds of algorithms have strong noise robustness ability and good Versatility.","However, inference time usually takes a long time, so the search efficiency is relatively low.","Later, based on large-scale pre-training data proposed, such methods use a large number of synthetic data points and expression pairs to train a Generative Pre-Trained Transformer(GPT).","Then this GPT can only need to perform one forward propagation to obtain the results, the advantage is that the inference speed is very fast.","However, its performance is very dependent on the training data and performs poorly on data outside the training set, which leads to poor noise robustness and Versatility of such methods.","So, can we combine the advantages of the above two categories of SR algorithms?","In this paper, we propose \\textbf{FormulaGPT}, which trains a GPT using massive sparse reward learning histories of reinforcement learning-based SR algorithms as training data.","After training, the SR algorithm based on reinforcement learning is distilled into a Transformer.","When new test data comes, FormulaGPT can directly generate a \"reinforcement learning process\" and automatically update the learning policy in context.","Tested on more than ten datasets including SRBench, formulaGPT achieves the state-of-the-art performance in fitting ability compared with four baselines.","In addition, it achieves satisfactory results in noise robustness, versatility, and inference efficiency."],"url":"http://arxiv.org/abs/2404.06330v1"}
{"created":"2024-04-09 14:04:33","title":"Characterizations of Sparsifiability for Affine CSPs and Symmetric CSPs","abstract":"CSP sparsification, introduced by Kogan and Krauthgamer (ITCS 2015), considers the following question: when can an instance of a constraint satisfaction problem be sparsified (by retaining a weighted subset of the constraints) while still roughly capturing the weight of constraints satisfied by {\\em every} assignment. CSP sparsification generalizes and abstracts other commonly studied problems including graph cut-sparsification, hypergraph cut-sparsification and hypergraph XOR-sparsification. A central question here is to understand what properties of a constraint predicate $P:\\Sigma^r \\to \\{0,1\\}$ (where variables are assigned values in $\\Sigma$) allow for nearly linear-size sparsifiers (in the number of variables). In this work (1) we significantly extend the class of CSPs for which nearly linear-size, and other non-trivial, sparsifications exist and give classifications in some broad settings and (2) give a polynomial-time algorithm to extract this sparsification.   Our results captured in item (1) completely classify all symmetric Boolean predicates $P$ (i.e., on the Boolean domain $\\Sigma = \\{0,1\\}$) that allow nearly-linear-size sparsifications. Symmetric Boolean CSPs already capture all the special classes of sparisifcation listed above including hypergraph cut-sparsification and variants. Our study of symmetric CSPs reveals an inherent, previously undetected, number-theoretic phenomenon that determines near-linear size sparsifiability. We also completely classify the set of Boolean predicates $P$ that allow non-trivial ($o(n^r)$-size) sparsifications, thus answering an open question from the work of Kogan and Krauthgamer.","sentences":["CSP sparsification, introduced by Kogan and Krauthgamer (ITCS 2015), considers the following question: when can an instance of a constraint satisfaction problem be sparsified (by retaining a weighted subset of the constraints) while still roughly capturing the weight of constraints satisfied by {\\em every} assignment.","CSP sparsification generalizes and abstracts other commonly studied problems including graph cut-sparsification, hypergraph cut-sparsification and hypergraph XOR-sparsification.","A central question here is to understand what properties of a constraint predicate $P:\\Sigma^r \\to \\{0,1\\}$ (where variables are assigned values in $\\Sigma$) allow for nearly linear-size sparsifiers (in the number of variables).","In this work (1) we significantly extend the class of CSPs for which nearly linear-size, and other non-trivial, sparsifications exist and give classifications in some broad settings and (2) give a polynomial-time algorithm to extract this sparsification.   ","Our results captured in item (1) completely classify all symmetric Boolean predicates $P$ (i.e., on the Boolean domain $\\Sigma = \\{0,1\\}$) that allow nearly-linear-size sparsifications.","Symmetric Boolean CSPs already capture all the special classes of sparisifcation listed above including hypergraph cut-sparsification and variants.","Our study of symmetric CSPs reveals an inherent, previously undetected, number-theoretic phenomenon that determines near-linear size sparsifiability.","We also completely classify the set of Boolean predicates $P$ that allow non-trivial ($o(n^r)$-size) sparsifications, thus answering an open question from the work of Kogan and Krauthgamer."],"url":"http://arxiv.org/abs/2404.06327v1"}
{"created":"2024-04-09 14:04:26","title":"What is the $\\textit{intrinsic}$ dimension of your binary data? -- and how to compute it quickly","abstract":"Dimensionality is an important aspect for analyzing and understanding (high-dimensional) data. In their 2006 ICDM paper Tatti et al. answered the question for a (interpretable) dimension of binary data tables by introducing a normalized correlation dimension. In the present work we revisit their results and contrast them with a concept based notion of intrinsic dimension (ID) recently introduced for geometric data sets. To do this, we present a novel approximation for this ID that is based on computing concepts only up to a certain support value. We demonstrate and evaluate our approximation using all available datasets from Tatti et al., which have between 469 and 41271 extrinsic dimensions.","sentences":["Dimensionality is an important aspect for analyzing and understanding (high-dimensional) data.","In their 2006 ICDM paper Tatti et al. answered the question for a (interpretable) dimension of binary data tables by introducing a normalized correlation dimension.","In the present work we revisit their results and contrast them with a concept based notion of intrinsic dimension (ID) recently introduced for geometric data sets.","To do this, we present a novel approximation for this ID that is based on computing concepts only up to a certain support value.","We demonstrate and evaluate our approximation using all available datasets from Tatti et al., which have between 469 and 41271 extrinsic dimensions."],"url":"http://arxiv.org/abs/2404.06326v1"}
{"created":"2024-04-09 13:44:16","title":"DRE: Generating Recommendation Explanations by Aligning Large Language Models at Data-level","abstract":"Recommendation systems play a crucial role in various domains, suggesting items based on user behavior.However, the lack of transparency in presenting recommendations can lead to user confusion. In this paper, we introduce Data-level Recommendation Explanation (DRE), a non-intrusive explanation framework for black-box recommendation models.Different from existing methods, DRE does not require any intermediary representations of the recommendation model or latent alignment training, mitigating potential performance issues.We propose a data-level alignment method, leveraging large language models to reason relationships between user data and recommended items.Additionally, we address the challenge of enriching the details of the explanation by introducing target-aware user preference distillation, utilizing item reviews. Experimental results on benchmark datasets demonstrate the effectiveness of the DRE in providing accurate and user-centric explanations, enhancing user engagement with recommended item.","sentences":["Recommendation systems play a crucial role in various domains, suggesting items based on user behavior.","However, the lack of transparency in presenting recommendations can lead to user confusion.","In this paper, we introduce Data-level Recommendation Explanation (DRE), a non-intrusive explanation framework for black-box recommendation models.","Different from existing methods, DRE does not require any intermediary representations of the recommendation model or latent alignment training, mitigating potential performance issues.","We propose a data-level alignment method, leveraging large language models to reason relationships between user data and recommended items.","Additionally, we address the challenge of enriching the details of the explanation by introducing target-aware user preference distillation, utilizing item reviews.","Experimental results on benchmark datasets demonstrate the effectiveness of the DRE in providing accurate and user-centric explanations, enhancing user engagement with recommended item."],"url":"http://arxiv.org/abs/2404.06311v1"}
{"created":"2024-04-09 13:39:37","title":"Audio-Visual Generalized Zero-Shot Learning using Pre-Trained Large Multi-Modal Models","abstract":"Audio-visual zero-shot learning methods commonly build on features extracted from pre-trained models, e.g. video or audio classification models. However, existing benchmarks predate the popularization of large multi-modal models, such as CLIP and CLAP. In this work, we explore such large pre-trained models to obtain features, i.e. CLIP for visual features, and CLAP for audio features. Furthermore, the CLIP and CLAP text encoders provide class label embeddings which are combined to boost the performance of the system. We propose a simple yet effective model that only relies on feed-forward neural networks, exploiting the strong generalization capabilities of the new audio, visual and textual features. Our framework achieves state-of-the-art performance on VGGSound-GZSL, UCF-GZSL, and ActivityNet-GZSL with our new features. Code and data available at: https://github.com/dkurzend/ClipClap-GZSL.","sentences":["Audio-visual zero-shot learning methods commonly build on features extracted from pre-trained models, e.g. video or audio classification models.","However, existing benchmarks predate the popularization of large multi-modal models, such as CLIP and CLAP.","In this work, we explore such large pre-trained models to obtain features, i.e. CLIP for visual features, and CLAP for audio features.","Furthermore, the CLIP and CLAP text encoders provide class label embeddings which are combined to boost the performance of the system.","We propose a simple yet effective model that only relies on feed-forward neural networks, exploiting the strong generalization capabilities of the new audio, visual and textual features.","Our framework achieves state-of-the-art performance on VGGSound-GZSL, UCF-GZSL, and ActivityNet-GZSL with our new features.","Code and data available at: https://github.com/dkurzend/ClipClap-GZSL."],"url":"http://arxiv.org/abs/2404.06309v1"}
{"created":"2024-04-09 13:19:25","title":"Optimal Stopping with Interdependent Values","abstract":"We study online selection problems in both the prophet and secretary settings, when arriving agents have interdependent values. In the interdependent values model, introduced in the seminal work of Milgrom and Weber [1982], each agent has a private signal and the value of an agent is a function of the signals held by all agents. Results in online selection crucially rely on some degree of independence of values, which is conceptually at odds with the interdependent values model. For prophet and secretary models under the standard independent values assumption, prior works provide constant factor approximations to the welfare. On the other hand, when agents have interdependent values, prior works in Economics and Computer Science provide truthful mechanisms that obtain optimal and approximately optimal welfare under certain assumptions on the valuation functions.   We bring together these two important lines of work and provide the first constant factor approximations for prophet and secretary problems with interdependent values. We consider both the algorithmic setting, where agents are non-strategic (but have interdependent values), and the mechanism design setting with strategic agents. All our results are constructive and use simple stopping rules.","sentences":["We study online selection problems in both the prophet and secretary settings, when arriving agents have interdependent values.","In the interdependent values model, introduced in the seminal work of Milgrom and Weber [1982], each agent has a private signal and the value of an agent is a function of the signals held by all agents.","Results in online selection crucially rely on some degree of independence of values, which is conceptually at odds with the interdependent values model.","For prophet and secretary models under the standard independent values assumption, prior works provide constant factor approximations to the welfare.","On the other hand, when agents have interdependent values, prior works in Economics and Computer Science provide truthful mechanisms that obtain optimal and approximately optimal welfare under certain assumptions on the valuation functions.   ","We bring together these two important lines of work and provide the first constant factor approximations for prophet and secretary problems with interdependent values.","We consider both the algorithmic setting, where agents are non-strategic (but have interdependent values), and the mechanism design setting with strategic agents.","All our results are constructive and use simple stopping rules."],"url":"http://arxiv.org/abs/2404.06293v1"}
{"created":"2024-04-09 13:15:23","title":"Statistical Modelling of Driving Scenarios in Road Traffic using Fleet Data of Production Vehicles","abstract":"Ensuring the safety of road vehicles at an acceptable level requires the absence of any unreasonable risk arising from all potential hazards linked to the intended au-tomated driving function and its implementation. The assurance that there are no unreasonable risks stemming from hazardous behaviours associated to functional insufficiencies is denoted as safety of intended functionality (SOTIF), a concept outlined in the ISO 21448 standard. In this context, the acquisition of real driving data is considered essential for the verification and validation. For this purpose, we are currently developing a method with which data collect-ed representatively from production vehicles can be modelled into a knowledge-based system in the future. A system that represents the probabilities of occur-rence of concrete driving scenarios over the statistical population of road traffic and makes them usable. The method includes the qualitative and quantitative ab-straction of the drives recorded by the sensors in the vehicles, the possibility of subsequent wireless transmission of the abstracted data from the vehicles and the derivation of the distributions and correlations of scenario parameters. This paper provides a summary of the research project and outlines its central idea. To this end, among other things, the needs for statistical information and da-ta from road traffic are elaborated from ISO 21448, the current state of research is addressed, and methodical aspects are discussed.","sentences":["Ensuring the safety of road vehicles at an acceptable level requires the absence of any unreasonable risk arising from all potential hazards linked to the intended au-tomated driving function and its implementation.","The assurance that there are no unreasonable risks stemming from hazardous behaviours associated to functional insufficiencies is denoted as safety of intended functionality (SOTIF), a concept outlined in the ISO 21448 standard.","In this context, the acquisition of real driving data is considered essential for the verification and validation.","For this purpose, we are currently developing a method with which data collect-ed representatively from production vehicles can be modelled into a knowledge-based system in the future.","A system that represents the probabilities of occur-rence of concrete driving scenarios over the statistical population of road traffic and makes them usable.","The method includes the qualitative and quantitative ab-straction of the drives recorded by the sensors in the vehicles, the possibility of subsequent wireless transmission of the abstracted data from the vehicles and the derivation of the distributions and correlations of scenario parameters.","This paper provides a summary of the research project and outlines its central idea.","To this end, among other things, the needs for statistical information and da-ta from road traffic are elaborated from ISO 21448, the current state of research is addressed, and methodical aspects are discussed."],"url":"http://arxiv.org/abs/2404.06288v1"}
{"created":"2024-04-09 13:08:56","title":"LLMs' Reading Comprehension Is Affected by Parametric Knowledge and Struggles with Hypothetical Statements","abstract":"The task of reading comprehension (RC), often implemented as context-based question answering (QA), provides a primary means to assess language models' natural language understanding (NLU) capabilities. Yet, when applied to large language models (LLMs) with extensive built-in world knowledge, this method can be deceptive. If the context aligns with the LLMs' internal knowledge, it is hard to discern whether the models' answers stem from context comprehension or from LLMs' internal information. Conversely, using data that conflicts with the models' knowledge creates erroneous trends which distort the results. To address this issue, we suggest to use RC on imaginary data, based on fictitious facts and entities. This task is entirely independent of the models' world knowledge, enabling us to evaluate LLMs' linguistic abilities without the interference of parametric knowledge. Testing ChatGPT, GPT-4, LLaMA 2 and Mixtral on such imaginary data, we uncover a class of linguistic phenomena posing a challenge to current LLMs, involving thinking in terms of alternative, hypothetical scenarios. While all the models handle simple affirmative and negative contexts with high accuracy, they are much more prone to error when dealing with modal and conditional contexts. Crucially, these phenomena also trigger the LLMs' vulnerability to knowledge-conflicts again. In particular, while some models prove virtually unaffected by knowledge conflicts in affirmative and negative contexts, when faced with more semantically involved modal and conditional environments, they often fail to separate the text from their internal knowledge.","sentences":["The task of reading comprehension (RC), often implemented as context-based question answering (QA), provides a primary means to assess language models' natural language understanding (NLU) capabilities.","Yet, when applied to large language models (LLMs) with extensive built-in world knowledge, this method can be deceptive.","If the context aligns with the LLMs' internal knowledge, it is hard to discern whether the models' answers stem from context comprehension or from LLMs' internal information.","Conversely, using data that conflicts with the models' knowledge creates erroneous trends which distort the results.","To address this issue, we suggest to use RC on imaginary data, based on fictitious facts and entities.","This task is entirely independent of the models' world knowledge, enabling us to evaluate LLMs' linguistic abilities without the interference of parametric knowledge.","Testing ChatGPT, GPT-4, LLaMA 2 and Mixtral on such imaginary data, we uncover a class of linguistic phenomena posing a challenge to current LLMs, involving thinking in terms of alternative, hypothetical scenarios.","While all the models handle simple affirmative and negative contexts with high accuracy, they are much more prone to error when dealing with modal and conditional contexts.","Crucially, these phenomena also trigger the LLMs' vulnerability to knowledge-conflicts again.","In particular, while some models prove virtually unaffected by knowledge conflicts in affirmative and negative contexts, when faced with more semantically involved modal and conditional environments, they often fail to separate the text from their internal knowledge."],"url":"http://arxiv.org/abs/2404.06283v1"}
{"created":"2024-04-09 13:02:40","title":"Algorithms for Caching and MTS with reduced number of predictions","abstract":"ML-augmented algorithms utilize predictions to achieve performance beyond their worst-case bounds. Producing these predictions might be a costly operation -- this motivated Im et al. '22 to introduce the study of algorithms which use predictions parsimoniously. We design parsimonious algorithms for caching and MTS with action predictions, proposed by Antoniadis et al. '20, focusing on the parameters of consistency (performance with perfect predictions) and smoothness (dependence of their performance on the prediction error). Our algorithm for caching is 1-consistent, robust, and its smoothness deteriorates with the decreasing number of available predictions. We propose an algorithm for general MTS whose consistency and smoothness both scale linearly with the decreasing number of predictions. Without the restriction on the number of available predictions, both algorithms match the earlier guarantees achieved by Antoniadis et al. '20.","sentences":["ML-augmented algorithms utilize predictions to achieve performance beyond their worst-case bounds.","Producing these predictions might be a costly operation -- this motivated Im et al.","'22 to introduce the study of algorithms which use predictions parsimoniously.","We design parsimonious algorithms for caching and MTS with action predictions, proposed by Antoniadis et al. '20, focusing on the parameters of consistency (performance with perfect predictions) and smoothness (dependence of their performance on the prediction error).","Our algorithm for caching is 1-consistent, robust, and its smoothness deteriorates with the decreasing number of available predictions.","We propose an algorithm for general MTS whose consistency and smoothness both scale linearly with the decreasing number of predictions.","Without the restriction on the number of available predictions, both algorithms match the earlier guarantees achieved by Antoniadis et al. '20."],"url":"http://arxiv.org/abs/2404.06280v1"}
{"created":"2024-04-09 13:02:22","title":"Dimensionality Reduction in Sentence Transformer Vector Databases with Fast Fourier Transform","abstract":"Dimensionality reduction in vector databases is pivotal for streamlining AI data management, enabling efficient storage, faster computation, and improved model performance. This paper explores the benefits of reducing vector database dimensions, with a focus on computational efficiency and overcoming the curse of dimensionality. We introduce a novel application of Fast Fourier Transform (FFT) to dimensionality reduction, a method previously underexploited in this context. By demonstrating its utility across various AI domains, including Retrieval-Augmented Generation (RAG) models and image processing, this FFT-based approach promises to improve data retrieval processes and enhance the efficiency and scalability of AI solutions. The incorporation of FFT may not only optimize operations in real-time processing and recommendation systems but also extend to advanced image processing techniques, where dimensionality reduction can significantly improve performance and analysis efficiency. This paper advocates for the broader adoption of FFT in vector database management, marking a significant stride towards addressing the challenges of data volume and complexity in AI research and applications. Unlike many existing approaches, we directly handle the embedding vectors produced by the model after processing a test input.","sentences":["Dimensionality reduction in vector databases is pivotal for streamlining AI data management, enabling efficient storage, faster computation, and improved model performance.","This paper explores the benefits of reducing vector database dimensions, with a focus on computational efficiency and overcoming the curse of dimensionality.","We introduce a novel application of Fast Fourier Transform (FFT) to dimensionality reduction, a method previously underexploited in this context.","By demonstrating its utility across various AI domains, including Retrieval-Augmented Generation (RAG) models and image processing, this FFT-based approach promises to improve data retrieval processes and enhance the efficiency and scalability of AI solutions.","The incorporation of FFT may not only optimize operations in real-time processing and recommendation systems but also extend to advanced image processing techniques, where dimensionality reduction can significantly improve performance and analysis efficiency.","This paper advocates for the broader adoption of FFT in vector database management, marking a significant stride towards addressing the challenges of data volume and complexity in AI research and applications.","Unlike many existing approaches, we directly handle the embedding vectors produced by the model after processing a test input."],"url":"http://arxiv.org/abs/2404.06278v1"}
{"created":"2024-04-09 13:01:26","title":"Learning Embeddings with Centroid Triplet Loss for Object Identification in Robotic Grasping","abstract":"Foundation models are a strong trend in deep learning and computer vision. These models serve as a base for applications as they require minor or no further fine-tuning by developers to integrate into their applications. Foundation models for zero-shot object segmentation such as Segment Anything (SAM) output segmentation masks from images without any further object information. When they are followed in a pipeline by an object identification model, they can perform object detection without training. Here, we focus on training such an object identification model. A crucial practical aspect for an object identification model is to be flexible in input size. As object identification is an image retrieval problem, a suitable method should handle multi-query multi-gallery situations without constraining the number of input images (e.g. by having fixed-size aggregation layers). The key solution to train such a model is the centroid triplet loss (CTL), which aggregates image features to their centroids. CTL yields high accuracy, avoids misleading training signals and keeps the model input size flexible. In our experiments, we establish a new state of the art on the ArmBench object identification task, which shows general applicability of our model. We furthermore demonstrate an integrated unseen object detection pipeline on the challenging HOPE dataset, which requires fine-grained detection. There, our pipeline matches and surpasses related methods which have been trained on dataset-specific data.","sentences":["Foundation models are a strong trend in deep learning and computer vision.","These models serve as a base for applications as they require minor or no further fine-tuning by developers to integrate into their applications.","Foundation models for zero-shot object segmentation such as Segment Anything (SAM) output segmentation masks from images without any further object information.","When they are followed in a pipeline by an object identification model, they can perform object detection without training.","Here, we focus on training such an object identification model.","A crucial practical aspect for an object identification model is to be flexible in input size.","As object identification is an image retrieval problem, a suitable method should handle multi-query multi-gallery situations without constraining the number of input images (e.g. by having fixed-size aggregation layers).","The key solution to train such a model is the centroid triplet loss (CTL), which aggregates image features to their centroids.","CTL yields high accuracy, avoids misleading training signals and keeps the model input size flexible.","In our experiments, we establish a new state of the art on the ArmBench object identification task, which shows general applicability of our model.","We furthermore demonstrate an integrated unseen object detection pipeline on the challenging HOPE dataset, which requires fine-grained detection.","There, our pipeline matches and surpasses related methods which have been trained on dataset-specific data."],"url":"http://arxiv.org/abs/2404.06277v1"}
{"created":"2024-04-09 12:45:17","title":"PGTNet: A Process Graph Transformer Network for Remaining Time Prediction of Business Process Instances","abstract":"We present PGTNet, an approach that transforms event logs into graph datasets and leverages graph-oriented data for training Process Graph Transformer Networks to predict the remaining time of business process instances. PGTNet consistently outperforms state-of-the-art deep learning approaches across a diverse range of 20 publicly available real-world event logs. Notably, our approach is most promising for highly complex processes, where existing deep learning approaches encounter difficulties stemming from their limited ability to learn control-flow relationships among process activities and capture long-range dependencies. PGTNet addresses these challenges, while also being able to consider multiple process perspectives during the learning process.","sentences":["We present PGTNet, an approach that transforms event logs into graph datasets and leverages graph-oriented data for training Process Graph Transformer Networks to predict the remaining time of business process instances.","PGTNet consistently outperforms state-of-the-art deep learning approaches across a diverse range of 20 publicly available real-world event logs.","Notably, our approach is most promising for highly complex processes, where existing deep learning approaches encounter difficulties stemming from their limited ability to learn control-flow relationships among process activities and capture long-range dependencies.","PGTNet addresses these challenges, while also being able to consider multiple process perspectives during the learning process."],"url":"http://arxiv.org/abs/2404.06267v1"}
{"created":"2024-04-09 12:29:56","title":"DDPG-E2E: A Novel Policy Gradient Approach for End-to-End Communication Systems","abstract":"The End-to-end (E2E) learning-based approach has great potential to reshape the existing communication systems by replacing the transceivers with deep neural networks. To this end, the E2E learning approach needs to assume the availability of prior channel information to mathematically formulate a differentiable channel layer for the backpropagation (BP) of the error gradients, thereby jointly optimizing the transmitter and the receiver. However, accurate and instantaneous channel state information is hardly obtained in practical wireless communication scenarios. Moreover, the existing E2E learning-based solutions exhibit limited performance in data transmissions with large block lengths. In this article, these practical issues are addressed by our proposed deep deterministic policy gradient-based E2E communication system. In particular, the proposed solution utilizes a reward feedback mechanism to train both the transmitter and the receiver, which alleviates the information loss of error gradients during BP. In addition, a convolutional neural network (CNN)-based architecture is developed to mitigate the curse of dimensionality problem when transmitting messages with large block lengths. Extensive simulations then demonstrate that our proposed solution can not only jointly train the transmitter and the receiver simultaneously without requiring the prior channel knowledge but also can obtain significant performance improvement on block error rate compared to state-of-the-art solutions.","sentences":["The End-to-end (E2E) learning-based approach has great potential to reshape the existing communication systems by replacing the transceivers with deep neural networks.","To this end, the E2E learning approach needs to assume the availability of prior channel information to mathematically formulate a differentiable channel layer for the backpropagation (BP) of the error gradients, thereby jointly optimizing the transmitter and the receiver.","However, accurate and instantaneous channel state information is hardly obtained in practical wireless communication scenarios.","Moreover, the existing E2E learning-based solutions exhibit limited performance in data transmissions with large block lengths.","In this article, these practical issues are addressed by our proposed deep deterministic policy gradient-based E2E communication system.","In particular, the proposed solution utilizes a reward feedback mechanism to train both the transmitter and the receiver, which alleviates the information loss of error gradients during BP.","In addition, a convolutional neural network (CNN)-based architecture is developed to mitigate the curse of dimensionality problem when transmitting messages with large block lengths.","Extensive simulations then demonstrate that our proposed solution can not only jointly train the transmitter and the receiver simultaneously without requiring the prior channel knowledge but also can obtain significant performance improvement on block error rate compared to state-of-the-art solutions."],"url":"http://arxiv.org/abs/2404.06257v1"}
{"created":"2024-04-09 12:29:16","title":"Label-Efficient 3D Object Detection For Road-Side Units","abstract":"Occlusion presents a significant challenge for safety-critical applications such as autonomous driving. Collaborative perception has recently attracted a large research interest thanks to the ability to enhance the perception of autonomous vehicles via deep information fusion with intelligent roadside units (RSU), thus minimizing the impact of occlusion. While significant advancement has been made, the data-hungry nature of these methods creates a major hurdle for their real-world deployment, particularly due to the need for annotated RSU data. Manually annotating the vast amount of RSU data required for training is prohibitively expensive, given the sheer number of intersections and the effort involved in annotating point clouds. We address this challenge by devising a label-efficient object detection method for RSU based on unsupervised object discovery. Our paper introduces two new modules: one for object discovery based on a spatial-temporal aggregation of point clouds, and another for refinement. Furthermore, we demonstrate that fine-tuning on a small portion of annotated data allows our object discovery models to narrow the performance gap with, or even surpass, fully supervised models. Extensive experiments are carried out in simulated and real-world datasets to evaluate our method.","sentences":["Occlusion presents a significant challenge for safety-critical applications such as autonomous driving.","Collaborative perception has recently attracted a large research interest thanks to the ability to enhance the perception of autonomous vehicles via deep information fusion with intelligent roadside units (RSU), thus minimizing the impact of occlusion.","While significant advancement has been made, the data-hungry nature of these methods creates a major hurdle for their real-world deployment, particularly due to the need for annotated RSU data.","Manually annotating the vast amount of RSU data required for training is prohibitively expensive, given the sheer number of intersections and the effort involved in annotating point clouds.","We address this challenge by devising a label-efficient object detection method for RSU based on unsupervised object discovery.","Our paper introduces two new modules: one for object discovery based on a spatial-temporal aggregation of point clouds, and another for refinement.","Furthermore, we demonstrate that fine-tuning on a small portion of annotated data allows our object discovery models to narrow the performance gap with, or even surpass, fully supervised models.","Extensive experiments are carried out in simulated and real-world datasets to evaluate our method."],"url":"http://arxiv.org/abs/2404.06256v1"}
{"created":"2024-04-09 12:25:06","title":"From Barlow Twins to Triplet Training: Differentiating Dementia with Limited Data","abstract":"Differential diagnosis of dementia is challenging due to overlapping symptoms, with structural magnetic resonance imaging (MRI) being the primary method for diagnosis. Despite the clinical value of computer-aided differential diagnosis, research has been limited, mainly due to the absence of public datasets that contain diverse types of dementia. This leaves researchers with small in-house datasets that are insufficient for training deep neural networks (DNNs). Self-supervised learning shows promise for utilizing unlabeled MRI scans in training, but small batch sizes for volumetric brain scans make its application challenging. To address these issues, we propose Triplet Training for differential diagnosis with limited target data. It consists of three key stages: (i) self-supervised pre-training on unlabeled data with Barlow Twins, (ii) self-distillation on task-related data, and (iii) fine-tuning on the target dataset. Our approach significantly outperforms traditional training strategies, achieving a balanced accuracy of 75.6%. We further provide insights into the training process by visualizing changes in the latent space after each step. Finally, we validate the robustness of Triplet Training in terms of its individual components in a comprehensive ablation study. Our code is available at https://github.com/ai-med/TripletTraining.","sentences":["Differential diagnosis of dementia is challenging due to overlapping symptoms, with structural magnetic resonance imaging (MRI) being the primary method for diagnosis.","Despite the clinical value of computer-aided differential diagnosis, research has been limited, mainly due to the absence of public datasets that contain diverse types of dementia.","This leaves researchers with small in-house datasets that are insufficient for training deep neural networks (DNNs).","Self-supervised learning shows promise for utilizing unlabeled MRI scans in training, but small batch sizes for volumetric brain scans make its application challenging.","To address these issues, we propose Triplet Training for differential diagnosis with limited target data.","It consists of three key stages: (i) self-supervised pre-training on unlabeled data with Barlow Twins, (ii) self-distillation on task-related data, and (iii) fine-tuning on the target dataset.","Our approach significantly outperforms traditional training strategies, achieving a balanced accuracy of 75.6%.","We further provide insights into the training process by visualizing changes in the latent space after each step.","Finally, we validate the robustness of Triplet Training in terms of its individual components in a comprehensive ablation study.","Our code is available at https://github.com/ai-med/TripletTraining."],"url":"http://arxiv.org/abs/2404.06253v1"}
{"created":"2024-04-09 12:13:40","title":"LRR: Language-Driven Resamplable Continuous Representation against Adversarial Tracking Attacks","abstract":"Visual object tracking plays a critical role in visual-based autonomous systems, as it aims to estimate the position and size of the object of interest within a live video. Despite significant progress made in this field, state-of-the-art (SOTA) trackers often fail when faced with adversarial perturbations in the incoming frames. This can lead to significant robustness and security issues when these trackers are deployed in the real world. To achieve high accuracy on both clean and adversarial data, we propose building a spatial-temporal continuous representation using the semantic text guidance of the object of interest. This novel continuous representation enables us to reconstruct incoming frames to maintain semantic and appearance consistency with the object of interest and its clean counterparts. As a result, our proposed method successfully defends against different SOTA adversarial tracking attacks while maintaining high accuracy on clean data. In particular, our method significantly increases tracking accuracy under adversarial attacks with around 90% relative improvement on UAV123, which is even higher than the accuracy on clean data.","sentences":["Visual object tracking plays a critical role in visual-based autonomous systems, as it aims to estimate the position and size of the object of interest within a live video.","Despite significant progress made in this field, state-of-the-art (SOTA) trackers often fail when faced with adversarial perturbations in the incoming frames.","This can lead to significant robustness and security issues when these trackers are deployed in the real world.","To achieve high accuracy on both clean and adversarial data, we propose building a spatial-temporal continuous representation using the semantic text guidance of the object of interest.","This novel continuous representation enables us to reconstruct incoming frames to maintain semantic and appearance consistency with the object of interest and its clean counterparts.","As a result, our proposed method successfully defends against different SOTA adversarial tracking attacks while maintaining high accuracy on clean data.","In particular, our method significantly increases tracking accuracy under adversarial attacks with around 90% relative improvement on UAV123, which is even higher than the accuracy on clean data."],"url":"http://arxiv.org/abs/2404.06247v1"}
{"created":"2024-04-09 12:10:54","title":"Anchor-based Robust Finetuning of Vision-Language Models","abstract":"We aim at finetuning a vision-language model without hurting its out-of-distribution (OOD) generalization. We address two types of OOD generalization, i.e., i) domain shift such as natural to sketch images, and ii) zero-shot capability to recognize the category that was not contained in the finetune data. Arguably, the diminished OOD generalization after finetuning stems from the excessively simplified finetuning target, which only provides the class information, such as ``a photo of a [CLASS]''. This is distinct from the process in that CLIP was pretrained, where there is abundant text supervision with rich semantic information. Therefore, we propose to compensate for the finetune process using auxiliary supervision with rich semantic information, which acts as anchors to preserve the OOD generalization. Specifically, two types of anchors are elaborated in our method, including i) text-compensated anchor which uses the images from the finetune set but enriches the text supervision from a pretrained captioner, ii) image-text-pair anchor which is retrieved from the dataset similar to pretraining data of CLIP according to the downstream task, associating with the original CLIP text with rich semantics. Those anchors are utilized as auxiliary semantic information to maintain the original feature space of CLIP, thereby preserving the OOD generalization capabilities. Comprehensive experiments demonstrate that our method achieves in-distribution performance akin to conventional finetuning while attaining new state-of-the-art results on domain shift and zero-shot learning benchmarks.","sentences":["We aim at finetuning a vision-language model without hurting its out-of-distribution (OOD) generalization.","We address two types of OOD generalization, i.e., i) domain shift such as natural to sketch images, and ii) zero-shot capability to recognize the category that was not contained in the finetune data.","Arguably, the diminished OOD generalization after finetuning stems from the excessively simplified finetuning target, which only provides the class information, such as ``a photo of a [CLASS]''.","This is distinct from the process in that CLIP was pretrained, where there is abundant text supervision with rich semantic information.","Therefore, we propose to compensate for the finetune process using auxiliary supervision with rich semantic information, which acts as anchors to preserve the OOD generalization.","Specifically, two types of anchors are elaborated in our method, including i) text-compensated anchor which uses the images from the finetune set but enriches the text supervision from a pretrained captioner, ii) image-text-pair anchor which is retrieved from the dataset similar to pretraining data of CLIP according to the downstream task, associating with the original CLIP text with rich semantics.","Those anchors are utilized as auxiliary semantic information to maintain the original feature space of CLIP, thereby preserving the OOD generalization capabilities.","Comprehensive experiments demonstrate that our method achieves in-distribution performance akin to conventional finetuning while attaining new state-of-the-art results on domain shift and zero-shot learning benchmarks."],"url":"http://arxiv.org/abs/2404.06244v1"}
{"created":"2024-04-09 12:09:56","title":"ActNetFormer: Transformer-ResNet Hybrid Method for Semi-Supervised Action Recognition in Videos","abstract":"Human action or activity recognition in videos is a fundamental task in computer vision with applications in surveillance and monitoring, self-driving cars, sports analytics, human-robot interaction and many more. Traditional supervised methods require large annotated datasets for training, which are expensive and time-consuming to acquire. This work proposes a novel approach using Cross-Architecture Pseudo-Labeling with contrastive learning for semi-supervised action recognition. Our framework leverages both labeled and unlabelled data to robustly learn action representations in videos, combining pseudo-labeling with contrastive learning for effective learning from both types of samples. We introduce a novel cross-architecture approach where 3D Convolutional Neural Networks (3D CNNs) and video transformers (VIT) are utilised to capture different aspects of action representations; hence we call it ActNetFormer. The 3D CNNs excel at capturing spatial features and local dependencies in the temporal domain, while VIT excels at capturing long-range dependencies across frames. By integrating these complementary architectures within the ActNetFormer framework, our approach can effectively capture both local and global contextual information of an action. This comprehensive representation learning enables the model to achieve better performance in semi-supervised action recognition tasks by leveraging the strengths of each of these architectures. Experimental results on standard action recognition datasets demonstrate that our approach performs better than the existing methods, achieving state-of-the-art performance with only a fraction of labeled data. The official website of this work is available at: https://github.com/rana2149/ActNetFormer.","sentences":["Human action or activity recognition in videos is a fundamental task in computer vision with applications in surveillance and monitoring, self-driving cars, sports analytics, human-robot interaction and many more.","Traditional supervised methods require large annotated datasets for training, which are expensive and time-consuming to acquire.","This work proposes a novel approach using Cross-Architecture Pseudo-Labeling with contrastive learning for semi-supervised action recognition.","Our framework leverages both labeled and unlabelled data to robustly learn action representations in videos, combining pseudo-labeling with contrastive learning for effective learning from both types of samples.","We introduce a novel cross-architecture approach where 3D Convolutional Neural Networks (3D CNNs) and video transformers (VIT) are utilised to capture different aspects of action representations; hence we call it ActNetFormer.","The 3D CNNs excel at capturing spatial features and local dependencies in the temporal domain, while VIT excels at capturing long-range dependencies across frames.","By integrating these complementary architectures within the ActNetFormer framework, our approach can effectively capture both local and global contextual information of an action.","This comprehensive representation learning enables the model to achieve better performance in semi-supervised action recognition tasks by leveraging the strengths of each of these architectures.","Experimental results on standard action recognition datasets demonstrate that our approach performs better than the existing methods, achieving state-of-the-art performance with only a fraction of labeled data.","The official website of this work is available at: https://github.com/rana2149/ActNetFormer."],"url":"http://arxiv.org/abs/2404.06243v1"}
{"created":"2024-04-09 12:06:21","title":"Hyperparameter-Free Medical Image Synthesis for Sharing Data and Improving Site-Specific Segmentation","abstract":"Sharing synthetic medical images is a promising alternative to sharing real images that can improve patient privacy and data security. To get good results, existing methods for medical image synthesis must be manually adjusted when they are applied to unseen data. To remove this manual burden, we introduce a Hyperparameter-Free distributed learning method for automatic medical image Synthesis, Sharing, and Segmentation called HyFree-S3. For three diverse segmentation settings (pelvic MRIs, lung X-rays, polyp photos), the use of HyFree-S3 results in improved performance over training only with site-specific data (in the majority of cases). The hyperparameter-free nature of the method should make data synthesis and sharing easier, potentially leading to an increase in the quantity of available data and consequently the quality of the models trained that may ultimately be applied in the clinic. Our code is available at https://github.com/AwesomeLemon/HyFree-S3","sentences":["Sharing synthetic medical images is a promising alternative to sharing real images that can improve patient privacy and data security.","To get good results, existing methods for medical image synthesis must be manually adjusted when they are applied to unseen data.","To remove this manual burden, we introduce a Hyperparameter-Free distributed learning method for automatic medical image Synthesis, Sharing, and Segmentation called HyFree-S3.","For three diverse segmentation settings (pelvic MRIs, lung X-rays, polyp photos), the use of HyFree-S3 results in improved performance over training only with site-specific data (in the majority of cases).","The hyperparameter-free nature of the method should make data synthesis and sharing easier, potentially leading to an increase in the quantity of available data and consequently the quality of the models trained that may ultimately be applied in the clinic.","Our code is available at https://github.com/AwesomeLemon/HyFree-S3"],"url":"http://arxiv.org/abs/2404.06240v1"}
{"created":"2024-04-09 11:14:45","title":"Zero-Shot Relational Learning for Multimodal Knowledge Graphs","abstract":"Relational learning is an essential task in the domain of knowledge representation, particularly in knowledge graph completion (KGC).While relational learning in traditional single-modal settings has been extensively studied, exploring it within a multimodal KGC context presents distinct challenges and opportunities. One of the major challenges is inference on newly discovered relations without any associated training data. This zero-shot relational learning scenario poses unique requirements for multimodal KGC, i.e., utilizing multimodality to facilitate relational learning. However, existing works fail to support the leverage of multimodal information and leave the problem unexplored. In this paper, we propose a novel end-to-end framework, consisting of three components, i.e., multimodal learner, structure consolidator, and relation embedding generator, to integrate diverse multimodal information and knowledge graph structures to facilitate the zero-shot relational learning. Evaluation results on two multimodal knowledge graphs demonstrate the superior performance of our proposed method.","sentences":["Relational learning is an essential task in the domain of knowledge representation, particularly in knowledge graph completion (KGC).While relational learning in traditional single-modal settings has been extensively studied, exploring it within a multimodal KGC context presents distinct challenges and opportunities.","One of the major challenges is inference on newly discovered relations without any associated training data.","This zero-shot relational learning scenario poses unique requirements for multimodal KGC, i.e., utilizing multimodality to facilitate relational learning.","However, existing works fail to support the leverage of multimodal information and leave the problem unexplored.","In this paper, we propose a novel end-to-end framework, consisting of three components, i.e., multimodal learner, structure consolidator, and relation embedding generator, to integrate diverse multimodal information and knowledge graph structures to facilitate the zero-shot relational learning.","Evaluation results on two multimodal knowledge graphs demonstrate the superior performance of our proposed method."],"url":"http://arxiv.org/abs/2404.06220v1"}
{"created":"2024-04-09 11:12:39","title":"Quantum Circuit $C^*$-algebra Net","abstract":"This paper introduces quantum circuit $C^*$-algebra net, which provides a connection between $C^*$-algebra nets proposed in classical machine learning and quantum circuits. Using $C^*$-algebra, a generalization of the space of complex numbers, we can represent quantum gates as weight parameters of a neural network. By introducing additional parameters, we can induce interaction among multiple circuits constructed by quantum gates. This interaction enables the circuits to share information among them, which contributes to improved generalization performance in machine learning tasks. As an application, we propose to use the quantum circuit $C^*$-algebra net to encode classical data into quantum states, which enables us to integrate classical data into quantum algorithms. Numerical results demonstrate that the interaction among circuits improves performance significantly in image classification, and encoded data by the quantum circuit $C^*$-algebra net are useful for downstream quantum machine learning tasks.","sentences":["This paper introduces quantum circuit $C^*$-algebra net, which provides a connection between $C^*$-algebra nets proposed in classical machine learning and quantum circuits.","Using $C^*$-algebra, a generalization of the space of complex numbers, we can represent quantum gates as weight parameters of a neural network.","By introducing additional parameters, we can induce interaction among multiple circuits constructed by quantum gates.","This interaction enables the circuits to share information among them, which contributes to improved generalization performance in machine learning tasks.","As an application, we propose to use the quantum circuit $C^*$-algebra net to encode classical data into quantum states, which enables us to integrate classical data into quantum algorithms.","Numerical results demonstrate that the interaction among circuits improves performance significantly in image classification, and encoded data by the quantum circuit $C^*$-algebra net are useful for downstream quantum machine learning tasks."],"url":"http://arxiv.org/abs/2404.06218v1"}
{"created":"2024-04-09 11:10:00","title":"VI-OOD: A Unified Representation Learning Framework for Textual Out-of-distribution Detection","abstract":"Out-of-distribution (OOD) detection plays a crucial role in ensuring the safety and reliability of deep neural networks in various applications. While there has been a growing focus on OOD detection in visual data, the field of textual OOD detection has received less attention. Only a few attempts have been made to directly apply general OOD detection methods to natural language processing (NLP) tasks, without adequately considering the characteristics of textual data. In this paper, we delve into textual OOD detection with Transformers. We first identify a key problem prevalent in existing OOD detection methods: the biased representation learned through the maximization of the conditional likelihood $p(y\\mid x)$ can potentially result in subpar performance. We then propose a novel variational inference framework for OOD detection (VI-OOD), which maximizes the likelihood of the joint distribution $p(x, y)$ instead of $p(y\\mid x)$. VI-OOD is tailored for textual OOD detection by efficiently exploiting the representations of pre-trained Transformers. Through comprehensive experiments on various text classification tasks, VI-OOD demonstrates its effectiveness and wide applicability. Our code has been released at \\url{https://github.com/liam0949/LLM-OOD}.","sentences":["Out-of-distribution (OOD) detection plays a crucial role in ensuring the safety and reliability of deep neural networks in various applications.","While there has been a growing focus on OOD detection in visual data, the field of textual OOD detection has received less attention.","Only a few attempts have been made to directly apply general OOD detection methods to natural language processing (NLP) tasks, without adequately considering the characteristics of textual data.","In this paper, we delve into textual OOD detection with Transformers.","We first identify a key problem prevalent in existing OOD detection methods: the biased representation learned through the maximization of the conditional likelihood $p(y\\mid x)$ can potentially result in subpar performance.","We then propose a novel variational inference framework for OOD detection (VI-OOD), which maximizes the likelihood of the joint distribution $p(x, y)$ instead of $p(y\\mid x)$. VI-OOD is tailored for textual OOD detection by efficiently exploiting the representations of pre-trained Transformers.","Through comprehensive experiments on various text classification tasks, VI-OOD demonstrates its effectiveness and wide applicability.","Our code has been released at \\url{https://github.com/liam0949/LLM-OOD}."],"url":"http://arxiv.org/abs/2404.06217v1"}
{"created":"2024-04-09 11:07:57","title":"Privacy-preserving Scanpath Comparison for Pervasive Eye Tracking","abstract":"As eye tracking becomes pervasive with screen-based devices and head-mounted displays, privacy concerns regarding eye-tracking data have escalated. While state-of-the-art approaches for privacy-preserving eye tracking mostly involve differential privacy and empirical data manipulations, previous research has not focused on methods for scanpaths. We introduce a novel privacy-preserving scanpath comparison protocol designed for the widely used Needleman-Wunsch algorithm, a generalized version of the edit distance algorithm. Particularly, by incorporating the Paillier homomorphic encryption scheme, our protocol ensures that no private information is revealed. Furthermore, we introduce a random processing strategy and a multi-layered masking method to obfuscate the values while preserving the original order of encrypted editing operation costs. This minimizes communication overhead, requiring a single communication round for each iteration of the Needleman-Wunsch process. We demonstrate the efficiency and applicability of our protocol on three publicly available datasets with comprehensive computational performance analyses and make our source code publicly accessible.","sentences":["As eye tracking becomes pervasive with screen-based devices and head-mounted displays, privacy concerns regarding eye-tracking data have escalated.","While state-of-the-art approaches for privacy-preserving eye tracking mostly involve differential privacy and empirical data manipulations, previous research has not focused on methods for scanpaths.","We introduce a novel privacy-preserving scanpath comparison protocol designed for the widely used Needleman-Wunsch algorithm, a generalized version of the edit distance algorithm.","Particularly, by incorporating the Paillier homomorphic encryption scheme, our protocol ensures that no private information is revealed.","Furthermore, we introduce a random processing strategy and a multi-layered masking method to obfuscate the values while preserving the original order of encrypted editing operation costs.","This minimizes communication overhead, requiring a single communication round for each iteration of the Needleman-Wunsch process.","We demonstrate the efficiency and applicability of our protocol on three publicly available datasets with comprehensive computational performance analyses and make our source code publicly accessible."],"url":"http://arxiv.org/abs/2404.06216v1"}
{"created":"2024-04-09 11:04:50","title":"[Call for Papers] The 2nd BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus","abstract":"After last year's successful BabyLM Challenge, the competition will be hosted again in 2024/2025. The overarching goals of the challenge remain the same; however, some of the competition rules will be different. The big changes for this year's competition are as follows: First, we replace the loose track with a paper track, which allows (for example) non-model-based submissions, novel cognitively-inspired benchmarks, or analysis techniques. Second, we are relaxing the rules around pretraining data, and will now allow participants to construct their own datasets provided they stay within the 100M-word or 10M-word budget. Third, we introduce a multimodal vision-and-language track, and will release a corpus of 50% text-only and 50% image-text multimodal data as a starting point for LM model training. The purpose of this CfP is to provide rules for this year's challenge, explain these rule changes and their rationale in greater detail, give a timeline of this year's competition, and provide answers to frequently asked questions from last year's challenge.","sentences":["After last year's successful BabyLM Challenge, the competition will be hosted again in 2024/2025.","The overarching goals of the challenge remain the same; however, some of the competition rules will be different.","The big changes for this year's competition are as follows:","First, we replace the loose track with a paper track, which allows (for example) non-model-based submissions, novel cognitively-inspired benchmarks, or analysis techniques.","Second, we are relaxing the rules around pretraining data, and will now allow participants to construct their own datasets provided they stay within the 100M-word or 10M-word budget.","Third, we introduce a multimodal vision-and-language track, and will release a corpus of 50% text-only and 50% image-text multimodal data as a starting point for LM model training.","The purpose of this CfP is to provide rules for this year's challenge, explain these rule changes and their rationale in greater detail, give a timeline of this year's competition, and provide answers to frequently asked questions from last year's challenge."],"url":"http://arxiv.org/abs/2404.06214v1"}
{"created":"2024-04-09 11:00:19","title":"OmniFusion Technical Report","abstract":"Last year, multimodal architectures served up a revolution in AI-based approaches and solutions, extending the capabilities of large language models (LLM). We propose an \\textit{OmniFusion} model based on a pretrained LLM and adapters for visual modality. We evaluated and compared several architecture design principles for better text and visual data coupling: MLP and transformer adapters, various CLIP ViT-based encoders (SigLIP, InternVIT, etc.), and their fusing approach, image encoding method (whole image or tiles encoding) and two 7B LLMs (the proprietary one and open-source Mistral). Experiments on 8 visual-language benchmarks show the top score for the best OmniFusion setup in terms of different VQA tasks in comparison with open-source LLaVA-like solutions: VizWiz, Pope, MM-Vet, ScienceQA, MMBench, TextVQA, VQAv2, MMMU. We also propose a variety of situations, where OmniFusion provides highly-detailed answers in different domains: housekeeping, sightseeing, culture, medicine, handwritten and scanned equations recognition, etc. Mistral-based OmniFusion model is an open-source solution with weights, training and inference scripts available at https://github.com/AIRI-Institute/OmniFusion.","sentences":["Last year, multimodal architectures served up a revolution in AI-based approaches and solutions, extending the capabilities of large language models (LLM).","We propose an \\textit{OmniFusion} model based on a pretrained LLM and adapters for visual modality.","We evaluated and compared several architecture design principles for better text and visual data coupling: MLP and transformer adapters, various CLIP ViT-based encoders (SigLIP, InternVIT, etc.), and their fusing approach, image encoding method (whole image or tiles encoding) and two 7B LLMs (the proprietary one and open-source Mistral).","Experiments on 8 visual-language benchmarks show the top score for the best OmniFusion setup in terms of different VQA tasks in comparison with open-source LLaVA-like solutions: VizWiz, Pope, MM-Vet, ScienceQA, MMBench, TextVQA, VQAv2, MMMU.","We also propose a variety of situations, where OmniFusion provides highly-detailed answers in different domains: housekeeping, sightseeing, culture, medicine, handwritten and scanned equations recognition, etc. Mistral-based OmniFusion model is an open-source solution with weights, training and inference scripts available at https://github.com/AIRI-Institute/OmniFusion."],"url":"http://arxiv.org/abs/2404.06212v1"}
{"created":"2024-04-09 10:58:21","title":"Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models","abstract":"While many have shown how Large Language Models (LLMs) can be applied to a diverse set of tasks, the critical issues of data contamination and memorization are often glossed over. In this work, we address this concern for tabular data. Specifically, we introduce a variety of different techniques to assess whether a language model has seen a tabular dataset during training. This investigation reveals that LLMs have memorized many popular tabular datasets verbatim. We then compare the few-shot learning performance of LLMs on datasets that were seen during training to the performance on datasets released after training. We find that LLMs perform better on datasets seen during training, indicating that memorization leads to overfitting. At the same time, LLMs show non-trivial performance on novel datasets and are surprisingly robust to data transformations. We then investigate the in-context statistical learning abilities of LLMs. Without fine-tuning, we find them to be limited. This suggests that much of the few-shot performance on novel datasets is due to the LLM's world knowledge. Overall, our results highlight the importance of testing whether an LLM has seen an evaluation dataset during pre-training. We make the exposure tests we developed available as the tabmemcheck Python package at https://github.com/interpretml/LLM-Tabular-Memorization-Checker","sentences":["While many have shown how Large Language Models (LLMs) can be applied to a diverse set of tasks, the critical issues of data contamination and memorization are often glossed over.","In this work, we address this concern for tabular data.","Specifically, we introduce a variety of different techniques to assess whether a language model has seen a tabular dataset during training.","This investigation reveals that LLMs have memorized many popular tabular datasets verbatim.","We then compare the few-shot learning performance of LLMs on datasets that were seen during training to the performance on datasets released after training.","We find that LLMs perform better on datasets seen during training, indicating that memorization leads to overfitting.","At the same time, LLMs show non-trivial performance on novel datasets and are surprisingly robust to data transformations.","We then investigate the in-context statistical learning abilities of LLMs.","Without fine-tuning, we find them to be limited.","This suggests that much of the few-shot performance on novel datasets is due to the LLM's world knowledge.","Overall, our results highlight the importance of testing whether an LLM has seen an evaluation dataset during pre-training.","We make the exposure tests we developed available as the tabmemcheck Python package at https://github.com/interpretml/LLM-Tabular-Memorization-Checker"],"url":"http://arxiv.org/abs/2404.06209v1"}
{"created":"2024-04-09 10:49:23","title":"A Comprehensive Benchmarking Analysis of Fault Recovery in Stream Processing Frameworks","abstract":"Nowadays, several software systems rely on stream processing architectures to deliver scalable performance and handle large volumes of data in near real time. Stream processing frameworks facilitate scalable computing by distributing the application's execution across multiple machines. Despite performance being extensively studied, the measurement of fault tolerance-a key and most appealing feature offered by stream processing frameworks-has still not been measured properly with updated and comprehensive testbeds. Moreover, the impact that fault recovery can have on performance is mostly ignored. This paper provides a comprehensive analysis of fault recovery performance, stability, and recovery time in a cloud-native environment with modern open-source frameworks, namely Flink, Kafka Streams, and Spark Structured Streaming. Our benchmarking analysis is inspired by chaos engineering to inject failures. Generally, our results indicate that much has changed compared to previous studies on fault recovery in distributed stream processing. In particular, the results indicate that Flink can be the fastest and stablest under failures. Moreover, Kafka Streams shows performance instabilities after failures, which is due to its current repartitioning strategy that can be suboptimal in terms of load balancing. Spark Structured Streaming shows suitable fault recovery performance and stability, but with higher event latency. Our study intends to (i) help industry practitioners in choosing the most suitable stream processing framework for efficient and reliable executions of data-intensive applications; (ii) support researchers in applying and extending our research method as well as our benchmark; (iii) identify, prevent, and assist in solving potential issues in production deployments.","sentences":["Nowadays, several software systems rely on stream processing architectures to deliver scalable performance and handle large volumes of data in near real time.","Stream processing frameworks facilitate scalable computing by distributing the application's execution across multiple machines.","Despite performance being extensively studied, the measurement of fault tolerance-a key and most appealing feature offered by stream processing frameworks-has still not been measured properly with updated and comprehensive testbeds.","Moreover, the impact that fault recovery can have on performance is mostly ignored.","This paper provides a comprehensive analysis of fault recovery performance, stability, and recovery time in a cloud-native environment with modern open-source frameworks, namely Flink, Kafka Streams, and Spark Structured Streaming.","Our benchmarking analysis is inspired by chaos engineering to inject failures.","Generally, our results indicate that much has changed compared to previous studies on fault recovery in distributed stream processing.","In particular, the results indicate that Flink can be the fastest and stablest under failures.","Moreover, Kafka Streams shows performance instabilities after failures, which is due to its current repartitioning strategy that can be suboptimal in terms of load balancing.","Spark Structured Streaming shows suitable fault recovery performance and stability, but with higher event latency.","Our study intends to (i) help industry practitioners in choosing the most suitable stream processing framework for efficient and reliable executions of data-intensive applications; (ii) support researchers in applying and extending our research method as well as our benchmark; (iii) identify, prevent, and assist in solving potential issues in production deployments."],"url":"http://arxiv.org/abs/2404.06203v1"}
{"created":"2024-04-09 10:47:02","title":"Open-Source AI-based SE Tools: Opportunities and Challenges of Collaborative Software Learning","abstract":"Large Language Models (LLMs) have become instrumental in advancing software engineering (SE) tasks, showcasing their efficacy in code understanding and beyond. Like traditional SE tools, open-source collaboration is key in realising the excellent products. However, with AI models, the essential need is in data. The collaboration of these AI-based SE models hinges on maximising the sources of high-quality data. However, data especially of high quality, often holds commercial or sensitive value, making it less accessible for open-source AI-based SE projects. This reality presents a significant barrier to the development and enhancement of AI-based SE tools within the software engineering community. Therefore, researchers need to find solutions for enabling open-source AI-based SE models to tap into resources by different organisations. Addressing this challenge, our position paper investigates one solution to facilitate access to diverse organizational resources for open-source AI models, ensuring privacy and commercial sensitivities are respected. We introduce a governance framework centered on federated learning (FL), designed to foster the joint development and maintenance of open-source AI code models while safeguarding data privacy and security. Additionally, we present guidelines for developers on AI-based SE tool collaboration, covering data requirements, model architecture, updating strategies, and version control. Given the significant influence of data characteristics on FL, our research examines the effect of code data heterogeneity on FL performance.","sentences":["Large Language Models (LLMs) have become instrumental in advancing software engineering (SE) tasks, showcasing their efficacy in code understanding and beyond.","Like traditional SE tools, open-source collaboration is key in realising the excellent products.","However, with AI models, the essential need is in data.","The collaboration of these AI-based SE models hinges on maximising the sources of high-quality data.","However, data especially of high quality, often holds commercial or sensitive value, making it less accessible for open-source AI-based SE projects.","This reality presents a significant barrier to the development and enhancement of AI-based SE tools within the software engineering community.","Therefore, researchers need to find solutions for enabling open-source AI-based SE models to tap into resources by different organisations.","Addressing this challenge, our position paper investigates one solution to facilitate access to diverse organizational resources for open-source AI models, ensuring privacy and commercial sensitivities are respected.","We introduce a governance framework centered on federated learning (FL), designed to foster the joint development and maintenance of open-source AI code models while safeguarding data privacy and security.","Additionally, we present guidelines for developers on AI-based SE tool collaboration, covering data requirements, model architecture, updating strategies, and version control.","Given the significant influence of data characteristics on FL, our research examines the effect of code data heterogeneity on FL performance."],"url":"http://arxiv.org/abs/2404.06201v1"}
{"created":"2024-04-09 10:41:59","title":"The impact of data set similarity and diversity on transfer learning success in time series forecasting","abstract":"Models, pre-trained on a similar or diverse source data set, have become pivotal in enhancing the efficiency and accuracy of time series forecasting on target data sets by leveraging transfer learning. While benchmarks validate the performance of model generalization on various target data sets, there is no structured research providing similarity and diversity measures explaining which characteristics of source and target data lead to transfer learning success. Our study pioneers in systematically evaluating the impact of source-target similarity and source diversity on zero-shot and fine-tuned forecasting outcomes in terms of accuracy, bias, and uncertainty estimation. We investigate these dynamics using pre-trained neural networks across five public source datasets, applied in forecasting five target data sets, including real-world wholesales data. We identify two feature-based similarity and diversity measures showing: Source-target similarity enhances forecasting accuracy and reduces bias, while source diversity enhances forecasting accuracy and uncertainty estimation and increases the bias.","sentences":["Models, pre-trained on a similar or diverse source data set, have become pivotal in enhancing the efficiency and accuracy of time series forecasting on target data sets by leveraging transfer learning.","While benchmarks validate the performance of model generalization on various target data sets, there is no structured research providing similarity and diversity measures explaining which characteristics of source and target data lead to transfer learning success.","Our study pioneers in systematically evaluating the impact of source-target similarity and source diversity on zero-shot and fine-tuned forecasting outcomes in terms of accuracy, bias, and uncertainty estimation.","We investigate these dynamics using pre-trained neural networks across five public source datasets, applied in forecasting five target data sets, including real-world wholesales data.","We identify two feature-based similarity and diversity measures showing: Source-target similarity enhances forecasting accuracy and reduces bias, while source diversity enhances forecasting accuracy and uncertainty estimation and increases the bias."],"url":"http://arxiv.org/abs/2404.06198v1"}
{"created":"2024-04-09 10:15:18","title":"Diverse Randomized Value Functions: A Provably Pessimistic Approach for Offline Reinforcement Learning","abstract":"Offline Reinforcement Learning (RL) faces distributional shift and unreliable value estimation, especially for out-of-distribution (OOD) actions. To address this, existing uncertainty-based methods penalize the value function with uncertainty quantification and demand numerous ensemble networks, posing computational challenges and suboptimal outcomes. In this paper, we introduce a novel strategy employing diverse randomized value functions to estimate the posterior distribution of $Q$-values. It provides robust uncertainty quantification and estimates lower confidence bounds (LCB) of $Q$-values. By applying moderate value penalties for OOD actions, our method fosters a provably pessimistic approach. We also emphasize on diversity within randomized value functions and enhance efficiency by introducing a diversity regularization method, reducing the requisite number of networks. These modules lead to reliable value estimation and efficient policy learning from offline data. Theoretical analysis shows that our method recovers the provably efficient LCB-penalty under linear MDP assumptions. Extensive empirical results also demonstrate that our proposed method significantly outperforms baseline methods in terms of performance and parametric efficiency.","sentences":["Offline Reinforcement Learning (RL) faces distributional shift and unreliable value estimation, especially for out-of-distribution (OOD) actions.","To address this, existing uncertainty-based methods penalize the value function with uncertainty quantification and demand numerous ensemble networks, posing computational challenges and suboptimal outcomes.","In this paper, we introduce a novel strategy employing diverse randomized value functions to estimate the posterior distribution of $Q$-values.","It provides robust uncertainty quantification and estimates lower confidence bounds (LCB) of $Q$-values.","By applying moderate value penalties for OOD actions, our method fosters a provably pessimistic approach.","We also emphasize on diversity within randomized value functions and enhance efficiency by introducing a diversity regularization method, reducing the requisite number of networks.","These modules lead to reliable value estimation and efficient policy learning from offline data.","Theoretical analysis shows that our method recovers the provably efficient LCB-penalty under linear MDP assumptions.","Extensive empirical results also demonstrate that our proposed method significantly outperforms baseline methods in terms of performance and parametric efficiency."],"url":"http://arxiv.org/abs/2404.06188v1"}
{"created":"2024-04-09 10:06:43","title":"Streamlined Transmission: A Semantic-Aware XR Deployment Framework Enhanced by Generative AI","abstract":"In the era of 6G, featuring compelling visions of digital twins and metaverses, Extended Reality (XR) has emerged as a vital conduit connecting the digital and physical realms, garnering widespread interest. Ensuring a fully immersive wireless XR experience stands as a paramount technical necessity, demanding the liberation of XR from the confines of wired connections. In this paper, we first introduce the technologies applied in the wireless XR domain, delve into their benefits and limitations, and highlight the ongoing challenges. We then propose a novel deployment framework for a broad XR pipeline, termed \"GeSa-XRF\", inspired by the core philosophy of Semantic Communication (SemCom) which shifts the concern from \"how\" to transmit to \"what\" to transmit. Particularly, the framework comprises three stages: data collection, data analysis, and data delivery. In each stage, we integrate semantic awareness to achieve streamlined transmission and employ Generative Artificial Intelligence (GAI) to achieve collaborative refinements. For the data collection of multi-modal data with differentiated data volumes and heterogeneous latency requirements, we propose a novel SemCom paradigm based on multi-modal fusion and separation and a GAI-based robust superposition scheme. To perform a comprehensive data analysis, we employ multi-task learning to perform the prediction of field of view and personalized attention and discuss the possible preprocessing approaches assisted by GAI. Lastly, for the data delivery stage, we present a semantic-aware multicast-based delivery strategy aimed at reducing pixel level redundant transmissions and introduce the GAI collaborative refinement approach. The performance gain of the proposed GeSa-XRF is preliminarily demonstrated through a case study.","sentences":["In the era of 6G, featuring compelling visions of digital twins and metaverses, Extended Reality (XR) has emerged as a vital conduit connecting the digital and physical realms, garnering widespread interest.","Ensuring a fully immersive wireless XR experience stands as a paramount technical necessity, demanding the liberation of XR from the confines of wired connections.","In this paper, we first introduce the technologies applied in the wireless XR domain, delve into their benefits and limitations, and highlight the ongoing challenges.","We then propose a novel deployment framework for a broad XR pipeline, termed \"GeSa-XRF\", inspired by the core philosophy of Semantic Communication (SemCom) which shifts the concern from \"how\" to transmit to \"what\" to transmit.","Particularly, the framework comprises three stages: data collection, data analysis, and data delivery.","In each stage, we integrate semantic awareness to achieve streamlined transmission and employ Generative Artificial Intelligence (GAI) to achieve collaborative refinements.","For the data collection of multi-modal data with differentiated data volumes and heterogeneous latency requirements, we propose a novel SemCom paradigm based on multi-modal fusion and separation and a GAI-based robust superposition scheme.","To perform a comprehensive data analysis, we employ multi-task learning to perform the prediction of field of view and personalized attention and discuss the possible preprocessing approaches assisted by GAI.","Lastly, for the data delivery stage, we present a semantic-aware multicast-based delivery strategy aimed at reducing pixel level redundant transmissions and introduce the GAI collaborative refinement approach.","The performance gain of the proposed GeSa-XRF is preliminarily demonstrated through a case study."],"url":"http://arxiv.org/abs/2404.06182v1"}
{"created":"2024-04-09 10:04:06","title":"EPL: Evidential Prototype Learning for Semi-supervised Medical Image Segmentation","abstract":"Although current semi-supervised medical segmentation methods can achieve decent performance, they are still affected by the uncertainty in unlabeled data and model predictions, and there is currently a lack of effective strategies that can explore the uncertain aspects of both simultaneously. To address the aforementioned issues, we propose Evidential Prototype Learning (EPL), which utilizes an extended probabilistic framework to effectively fuse voxel probability predictions from different sources and achieves prototype fusion utilization of labeled and unlabeled data under a generalized evidential framework, leveraging voxel-level dual uncertainty masking. The uncertainty not only enables the model to self-correct predictions but also improves the guided learning process with pseudo-labels and is able to feed back into the construction of hidden features. The method proposed in this paper has been experimented on LA, Pancreas-CT and TBAD datasets, achieving the state-of-the-art performance in three different labeled ratios, which strongly demonstrates the effectiveness of our strategy.","sentences":["Although current semi-supervised medical segmentation methods can achieve decent performance, they are still affected by the uncertainty in unlabeled data and model predictions, and there is currently a lack of effective strategies that can explore the uncertain aspects of both simultaneously.","To address the aforementioned issues, we propose Evidential Prototype Learning (EPL), which utilizes an extended probabilistic framework to effectively fuse voxel probability predictions from different sources and achieves prototype fusion utilization of labeled and unlabeled data under a generalized evidential framework, leveraging voxel-level dual uncertainty masking.","The uncertainty not only enables the model to self-correct predictions but also improves the guided learning process with pseudo-labels and is able to feed back into the construction of hidden features.","The method proposed in this paper has been experimented on LA, Pancreas-CT and TBAD datasets, achieving the state-of-the-art performance in three different labeled ratios, which strongly demonstrates the effectiveness of our strategy."],"url":"http://arxiv.org/abs/2404.06181v1"}
{"created":"2024-04-09 10:03:35","title":"AI-MOLE: Autonomous Iterative Motion Learning for Unknown Nonlinear Dynamics with Extensive Experimental Validation","abstract":"This work proposes Autonomous Iterative Motion Learning (AI-MOLE), a method that enables systems with unknown, nonlinear dynamics to autonomously learn to solve reference tracking tasks. The method iteratively applies an input trajectory to the unknown dynamics, trains a Gaussian process model based on the experimental data, and utilizes the model to update the input trajectory until desired tracking performance is achieved. Unlike existing approaches, the proposed method determines necessary parameters automatically, i.e., AI-MOLE works plug-and-play and without manual parameter tuning. Furthermore, AI-MOLE only requires input/output information, but can also exploit available state information to accelerate learning.   While other approaches are typically only validated in simulation or on a single real-world testbed using manually tuned parameters, we present the unprecedented result of validating the proposed method on three different real-world robots and a total of nine different reference tracking tasks without requiring any a priori model information or manual parameter tuning. Over all systems and tasks, AI-MOLE rapidly learns to track the references without requiring any manual parameter tuning at all, even if only input/output information is available.","sentences":["This work proposes Autonomous Iterative Motion Learning (AI-MOLE), a method that enables systems with unknown, nonlinear dynamics to autonomously learn to solve reference tracking tasks.","The method iteratively applies an input trajectory to the unknown dynamics, trains a Gaussian process model based on the experimental data, and utilizes the model to update the input trajectory until desired tracking performance is achieved.","Unlike existing approaches, the proposed method determines necessary parameters automatically, i.e., AI-MOLE works plug-and-play and without manual parameter tuning.","Furthermore, AI-MOLE only requires input/output information, but can also exploit available state information to accelerate learning.   ","While other approaches are typically only validated in simulation or on a single real-world testbed using manually tuned parameters, we present the unprecedented result of validating the proposed method on three different real-world robots and a total of nine different reference tracking tasks without requiring any a priori model information or manual parameter tuning.","Over all systems and tasks, AI-MOLE rapidly learns to track the references without requiring any manual parameter tuning at all, even if only input/output information is available."],"url":"http://arxiv.org/abs/2404.06179v1"}
{"created":"2024-04-09 09:46:17","title":"scCDCG: Efficient Deep Structural Clustering for single-cell RNA-seq via Deep Cut-informed Graph Embedding","abstract":"Single-cell RNA sequencing (scRNA-seq) is essential for unraveling cellular heterogeneity and diversity, offering invaluable insights for bioinformatics advancements. Despite its potential, traditional clustering methods in scRNA-seq data analysis often neglect the structural information embedded in gene expression profiles, crucial for understanding cellular correlations and dependencies. Existing strategies, including graph neural networks, face challenges in handling the inefficiency due to scRNA-seq data's intrinsic high-dimension and high-sparsity. Addressing these limitations, we introduce scCDCG (single-cell RNA-seq Clustering via Deep Cut-informed Graph), a novel framework designed for efficient and accurate clustering of scRNA-seq data that simultaneously utilizes intercellular high-order structural information. scCDCG comprises three main components: (i) A graph embedding module utilizing deep cut-informed techniques, which effectively captures intercellular high-order structural information, overcoming the over-smoothing and inefficiency issues prevalent in prior graph neural network methods. (ii) A self-supervised learning module guided by optimal transport, tailored to accommodate the unique complexities of scRNA-seq data, specifically its high-dimension and high-sparsity. (iii) An autoencoder-based feature learning module that simplifies model complexity through effective dimension reduction and feature extraction. Our extensive experiments on 6 datasets demonstrate scCDCG's superior performance and efficiency compared to 7 established models, underscoring scCDCG's potential as a transformative tool in scRNA-seq data analysis. Our code is available at: https://github.com/XPgogogo/scCDCG.","sentences":["Single-cell RNA sequencing (scRNA-seq) is essential for unraveling cellular heterogeneity and diversity, offering invaluable insights for bioinformatics advancements.","Despite its potential, traditional clustering methods in scRNA-seq data analysis often neglect the structural information embedded in gene expression profiles, crucial for understanding cellular correlations and dependencies.","Existing strategies, including graph neural networks, face challenges in handling the inefficiency due to scRNA-seq data's intrinsic high-dimension and high-sparsity.","Addressing these limitations, we introduce scCDCG (single-cell RNA-seq Clustering via Deep Cut-informed Graph), a novel framework designed for efficient and accurate clustering of scRNA-seq data that simultaneously utilizes intercellular high-order structural information.","scCDCG comprises three main components: (i) A graph embedding module utilizing deep cut-informed techniques, which effectively captures intercellular high-order structural information, overcoming the over-smoothing and inefficiency issues prevalent in prior graph neural network methods.","(ii) A self-supervised learning module guided by optimal transport, tailored to accommodate the unique complexities of scRNA-seq data, specifically its high-dimension and high-sparsity.","(iii) An autoencoder-based feature learning module that simplifies model complexity through effective dimension reduction and feature extraction.","Our extensive experiments on 6 datasets demonstrate scCDCG's superior performance and efficiency compared to 7 established models, underscoring scCDCG's potential as a transformative tool in scRNA-seq data analysis.","Our code is available at: https://github.com/XPgogogo/scCDCG."],"url":"http://arxiv.org/abs/2404.06167v1"}
{"created":"2024-04-09 09:42:18","title":"Enhanced Radar Perception via Multi-Task Learning: Towards Refined Data for Sensor Fusion Applications","abstract":"Radar and camera fusion yields robustness in perception tasks by leveraging the strength of both sensors. The typical extracted radar point cloud is 2D without height information due to insufficient antennas along the elevation axis, which challenges the network performance. This work introduces a learning-based approach to infer the height of radar points associated with 3D objects. A novel robust regression loss is introduced to address the sparse target challenge. In addition, a multi-task training strategy is employed, emphasizing important features. The average radar absolute height error decreases from 1.69 to 0.25 meters compared to the state-of-the-art height extension method. The estimated target height values are used to preprocess and enrich radar data for downstream perception tasks. Integrating this refined radar information further enhances the performance of existing radar camera fusion models for object detection and depth estimation tasks.","sentences":["Radar and camera fusion yields robustness in perception tasks by leveraging the strength of both sensors.","The typical extracted radar point cloud is 2D without height information due to insufficient antennas along the elevation axis, which challenges the network performance.","This work introduces a learning-based approach to infer the height of radar points associated with 3D objects.","A novel robust regression loss is introduced to address the sparse target challenge.","In addition, a multi-task training strategy is employed, emphasizing important features.","The average radar absolute height error decreases from 1.69 to 0.25 meters compared to the state-of-the-art height extension method.","The estimated target height values are used to preprocess and enrich radar data for downstream perception tasks.","Integrating this refined radar information further enhances the performance of existing radar camera fusion models for object detection and depth estimation tasks."],"url":"http://arxiv.org/abs/2404.06165v1"}
{"created":"2024-04-09 09:34:25","title":"Characterizing Multimodal Long-form Summarization: A Case Study on Financial Reports","abstract":"As large language models (LLMs) expand the power of natural language processing to handle long inputs, rigorous and systematic analyses are necessary to understand their abilities and behavior. A salient application is summarization, due to its ubiquity and controversy (e.g., researchers have declared the death of summarization). In this paper, we use financial report summarization as a case study because financial reports not only are long but also use numbers and tables extensively. We propose a computational framework for characterizing multimodal long-form summarization and investigate the behavior of Claude 2.0/2.1, GPT-4/3.5, and Command. We find that GPT-3.5 and Command fail to perform this summarization task meaningfully. For Claude 2 and GPT-4, we analyze the extractiveness of the summary and identify a position bias in LLMs. This position bias disappears after shuffling the input for Claude, which suggests that Claude has the ability to recognize important information. We also conduct a comprehensive investigation on the use of numeric data in LLM-generated summaries and offer a taxonomy of numeric hallucination. We employ prompt engineering to improve GPT-4's use of numbers with limited success. Overall, our analyses highlight the strong capability of Claude 2 in handling long multimodal inputs compared to GPT-4.","sentences":["As large language models (LLMs) expand the power of natural language processing to handle long inputs, rigorous and systematic analyses are necessary to understand their abilities and behavior.","A salient application is summarization, due to its ubiquity and controversy (e.g., researchers have declared the death of summarization).","In this paper, we use financial report summarization as a case study because financial reports not only are long but also use numbers and tables extensively.","We propose a computational framework for characterizing multimodal long-form summarization and investigate the behavior of Claude 2.0/2.1, GPT-4/3.5, and Command.","We find that GPT-3.5 and Command fail to perform this summarization task meaningfully.","For Claude 2 and GPT-4, we analyze the extractiveness of the summary and identify a position bias in LLMs.","This position bias disappears after shuffling the input for Claude, which suggests that Claude has the ability to recognize important information.","We also conduct a comprehensive investigation on the use of numeric data in LLM-generated summaries and offer a taxonomy of numeric hallucination.","We employ prompt engineering to improve GPT-4's use of numbers with limited success.","Overall, our analyses highlight the strong capability of Claude 2 in handling long multimodal inputs compared to GPT-4."],"url":"http://arxiv.org/abs/2404.06162v1"}
{"created":"2024-04-09 09:25:16","title":"scRDiT: Generating single-cell RNA-seq data by diffusion transformers and accelerating sampling","abstract":"Motivation: Single-cell RNA sequencing (scRNA-seq) is a groundbreaking technology extensively utilized in biological research, facilitating the examination of gene expression at the individual cell level within a given tissue sample. While numerous tools have been developed for scRNA-seq data analysis, the challenge persists in capturing the distinct features of such data and replicating virtual datasets that share analogous statistical properties. Results: Our study introduces a generative approach termed scRNA-seq Diffusion Transformer (scRDiT). This method generates virtual scRNA-seq data by leveraging a real dataset. The method is a neural network constructed based on Denoising Diffusion Probabilistic Models (DDPMs) and Diffusion Transformers (DiTs). This involves subjecting Gaussian noises to the real dataset through iterative noise-adding steps and ultimately restoring the noises to form scRNA-seq samples. This scheme allows us to learn data features from actual scRNA-seq samples during model training. Our experiments, conducted on two distinct scRNA-seq datasets, demonstrate superior performance. Additionally, the model sampling process is expedited by incorporating Denoising Diffusion Implicit Models (DDIM). scRDiT presents a unified methodology empowering users to train neural network models with their unique scRNA-seq datasets, enabling the generation of numerous high-quality scRNA-seq samples. Availability and implementation: https://github.com/DongShengze/scRDiT","sentences":["Motivation: Single-cell RNA sequencing (scRNA-seq) is a groundbreaking technology extensively utilized in biological research, facilitating the examination of gene expression at the individual cell level within a given tissue sample.","While numerous tools have been developed for scRNA-seq data analysis, the challenge persists in capturing the distinct features of such data and replicating virtual datasets that share analogous statistical properties.","Results:","Our study introduces a generative approach termed scRNA-seq Diffusion Transformer (scRDiT).","This method generates virtual scRNA-seq data by leveraging a real dataset.","The method is a neural network constructed based on Denoising Diffusion Probabilistic Models (DDPMs) and Diffusion Transformers (DiTs).","This involves subjecting Gaussian noises to the real dataset through iterative noise-adding steps and ultimately restoring the noises to form scRNA-seq samples.","This scheme allows us to learn data features from actual scRNA-seq samples during model training.","Our experiments, conducted on two distinct scRNA-seq datasets, demonstrate superior performance.","Additionally, the model sampling process is expedited by incorporating Denoising Diffusion Implicit Models (DDIM).","scRDiT presents a unified methodology empowering users to train neural network models with their unique scRNA-seq datasets, enabling the generation of numerous high-quality scRNA-seq samples.","Availability and implementation: https://github.com/DongShengze/scRDiT"],"url":"http://arxiv.org/abs/2404.06153v1"}
{"created":"2024-04-09 09:09:36","title":"Differential Privacy for Anomaly Detection: Analyzing the Trade-off Between Privacy and Explainability","abstract":"Anomaly detection (AD), also referred to as outlier detection, is a statistical process aimed at identifying observations within a dataset that significantly deviate from the expected pattern of the majority of the data. Such a process finds wide application in various fields, such as finance and healthcare. While the primary objective of AD is to yield high detection accuracy, the requirements of explainability and privacy are also paramount. The first ensures the transparency of the AD process, while the second guarantees that no sensitive information is leaked to untrusted parties. In this work, we exploit the trade-off of applying Explainable AI (XAI) through SHapley Additive exPlanations (SHAP) and differential privacy (DP). We perform AD with different models and on various datasets, and we thoroughly evaluate the cost of privacy in terms of decreased accuracy and explainability. Our results show that the enforcement of privacy through DP has a significant impact on detection accuracy and explainability, which depends on both the dataset and the considered AD model. We further show that the visual interpretation of explanations is also influenced by the choice of the AD algorithm.","sentences":["Anomaly detection (AD), also referred to as outlier detection, is a statistical process aimed at identifying observations within a dataset that significantly deviate from the expected pattern of the majority of the data.","Such a process finds wide application in various fields, such as finance and healthcare.","While the primary objective of AD is to yield high detection accuracy, the requirements of explainability and privacy are also paramount.","The first ensures the transparency of the AD process, while the second guarantees that no sensitive information is leaked to untrusted parties.","In this work, we exploit the trade-off of applying Explainable AI (XAI) through SHapley Additive exPlanations (SHAP) and differential privacy (DP).","We perform AD with different models and on various datasets, and we thoroughly evaluate the cost of privacy in terms of decreased accuracy and explainability.","Our results show that the enforcement of privacy through DP has a significant impact on detection accuracy and explainability, which depends on both the dataset and the considered AD model.","We further show that the visual interpretation of explanations is also influenced by the choice of the AD algorithm."],"url":"http://arxiv.org/abs/2404.06144v1"}
{"created":"2024-04-09 09:03:44","title":"SmurfCat at SemEval-2024 Task 6: Leveraging Synthetic Data for Hallucination Detection","abstract":"In this paper, we present our novel systems developed for the SemEval-2024 hallucination detection task. Our investigation spans a range of strategies to compare model predictions with reference standards, encompassing diverse baselines, the refinement of pre-trained encoders through supervised learning, and an ensemble approaches utilizing several high-performing models. Through these explorations, we introduce three distinct methods that exhibit strong performance metrics. To amplify our training data, we generate additional training samples from unlabelled training subset. Furthermore, we provide a detailed comparative analysis of our approaches. Notably, our premier method achieved a commendable 9th place in the competition's model-agnostic track and 17th place in model-aware track, highlighting its effectiveness and potential.","sentences":["In this paper, we present our novel systems developed for the SemEval-2024 hallucination detection task.","Our investigation spans a range of strategies to compare model predictions with reference standards, encompassing diverse baselines, the refinement of pre-trained encoders through supervised learning, and an ensemble approaches utilizing several high-performing models.","Through these explorations, we introduce three distinct methods that exhibit strong performance metrics.","To amplify our training data, we generate additional training samples from unlabelled training subset.","Furthermore, we provide a detailed comparative analysis of our approaches.","Notably, our premier method achieved a commendable 9th place in the competition's model-agnostic track and 17th place in model-aware track, highlighting its effectiveness and potential."],"url":"http://arxiv.org/abs/2404.06137v1"}
{"created":"2024-04-09 08:51:05","title":"FLEX: FLEXible Federated Learning Framework","abstract":"In the realm of Artificial Intelligence (AI), the need for privacy and security in data processing has become paramount. As AI applications continue to expand, the collection and handling of sensitive data raise concerns about individual privacy protection. Federated Learning (FL) emerges as a promising solution to address these challenges by enabling decentralized model training on local devices, thus preserving data privacy. This paper introduces FLEX: a FLEXible Federated Learning Framework designed to provide maximum flexibility in FL research experiments. By offering customizable features for data distribution, privacy parameters, and communication strategies, FLEX empowers researchers to innovate and develop novel FL techniques. The framework also includes libraries for specific FL implementations including: (1) anomalies, (2) blockchain, (3) adversarial attacks and defences, (4) natural language processing and (5) decision trees, enhancing its versatility and applicability in various domains. Overall, FLEX represents a significant advancement in FL research, facilitating the development of robust and efficient FL applications.","sentences":["In the realm of Artificial Intelligence (AI), the need for privacy and security in data processing has become paramount.","As AI applications continue to expand, the collection and handling of sensitive data raise concerns about individual privacy protection.","Federated Learning (FL) emerges as a promising solution to address these challenges by enabling decentralized model training on local devices, thus preserving data privacy.","This paper introduces FLEX: a FLEXible Federated Learning Framework designed to provide maximum flexibility in FL research experiments.","By offering customizable features for data distribution, privacy parameters, and communication strategies, FLEX empowers researchers to innovate and develop novel FL techniques.","The framework also includes libraries for specific FL implementations including: (1) anomalies, (2) blockchain, (3) adversarial attacks and defences, (4) natural language processing and (5) decision trees, enhancing its versatility and applicability in various domains.","Overall, FLEX represents a significant advancement in FL research, facilitating the development of robust and efficient FL applications."],"url":"http://arxiv.org/abs/2404.06127v1"}
{"created":"2024-04-09 08:35:04","title":"Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive Survey","abstract":"With the rapid growth in the volume of data sets, models, and devices in the domain of deep learning, there is increasing attention on large-scale distributed deep learning. In contrast to traditional distributed deep learning, the large-scale scenario poses new challenges that include fault tolerance, scalability of algorithms and infrastructures, and heterogeneity in data sets, models, and resources. Due to intensive synchronization of models and sharing of data across GPUs and computing nodes during distributed training and inference processes, communication efficiency becomes the bottleneck for achieving high performance at a large scale. This article surveys the literature over the period of 2018-2023 on algorithms and technologies aimed at achieving efficient communication in large-scale distributed deep learning at various levels, including algorithms, frameworks, and infrastructures. Specifically, we first introduce efficient algorithms for model synchronization and communication data compression in the context of large-scale distributed training. Next, we introduce efficient strategies related to resource allocation and task scheduling for use in distributed training and inference. After that, we present the latest technologies pertaining to modern communication infrastructures used in distributed deep learning with a focus on examining the impact of the communication overhead in a large-scale and heterogeneous setting. Finally, we conduct a case study on the distributed training of large language models at a large scale to illustrate how to apply these technologies in real cases. This article aims to offer researchers a comprehensive understanding of the current landscape of large-scale distributed deep learning and to reveal promising future research directions toward communication-efficient solutions in this scope.","sentences":["With the rapid growth in the volume of data sets, models, and devices in the domain of deep learning, there is increasing attention on large-scale distributed deep learning.","In contrast to traditional distributed deep learning, the large-scale scenario poses new challenges that include fault tolerance, scalability of algorithms and infrastructures, and heterogeneity in data sets, models, and resources.","Due to intensive synchronization of models and sharing of data across GPUs and computing nodes during distributed training and inference processes, communication efficiency becomes the bottleneck for achieving high performance at a large scale.","This article surveys the literature over the period of 2018-2023 on algorithms and technologies aimed at achieving efficient communication in large-scale distributed deep learning at various levels, including algorithms, frameworks, and infrastructures.","Specifically, we first introduce efficient algorithms for model synchronization and communication data compression in the context of large-scale distributed training.","Next, we introduce efficient strategies related to resource allocation and task scheduling for use in distributed training and inference.","After that, we present the latest technologies pertaining to modern communication infrastructures used in distributed deep learning with a focus on examining the impact of the communication overhead in a large-scale and heterogeneous setting.","Finally, we conduct a case study on the distributed training of large language models at a large scale to illustrate how to apply these technologies in real cases.","This article aims to offer researchers a comprehensive understanding of the current landscape of large-scale distributed deep learning and to reveal promising future research directions toward communication-efficient solutions in this scope."],"url":"http://arxiv.org/abs/2404.06114v1"}
{"created":"2024-04-09 07:48:49","title":"EVE: Enabling Anyone to Train Robot using Augmented Reality","abstract":"The increasing affordability of robot hardware is accelerating the integration of robots into everyday activities. However, training a robot to automate a task typically requires physical robots and expensive demonstration data from trained human annotators. Consequently, only those with access to physical robots produce demonstrations to train robots. To mitigate this issue, we introduce EVE, an iOS app that enables everyday users to train robots using intuitive augmented reality visualizations without needing a physical robot. With EVE, users can collect demonstrations by specifying waypoints with their hands, visually inspecting the environment for obstacles, modifying existing waypoints, and verifying collected trajectories. In a user study ($N=14$, $D=30$) consisting of three common tabletop tasks, EVE outperformed three state-of-the-art interfaces in success rate and was comparable to kinesthetic teaching-physically moving a real robot-in completion time, usability, motion intent communication, enjoyment, and preference ($mean_{p}=0.30$). We conclude by enumerating limitations and design considerations for future AR-based demonstration collection systems for robotics.","sentences":["The increasing affordability of robot hardware is accelerating the integration of robots into everyday activities.","However, training a robot to automate a task typically requires physical robots and expensive demonstration data from trained human annotators.","Consequently, only those with access to physical robots produce demonstrations to train robots.","To mitigate this issue, we introduce EVE, an iOS app that enables everyday users to train robots using intuitive augmented reality visualizations without needing a physical robot.","With EVE, users can collect demonstrations by specifying waypoints with their hands, visually inspecting the environment for obstacles, modifying existing waypoints, and verifying collected trajectories.","In a user study ($N=14$, $D=30$) consisting of three common tabletop tasks, EVE outperformed three state-of-the-art interfaces in success rate and was comparable to kinesthetic teaching-physically moving a real robot-in completion time, usability, motion intent communication, enjoyment, and preference ($mean_{p}=0.30$).","We conclude by enumerating limitations and design considerations for future AR-based demonstration collection systems for robotics."],"url":"http://arxiv.org/abs/2404.06089v1"}
{"created":"2024-04-09 07:32:30","title":"Is Your AI Truly Yours? Leveraging Blockchain for Copyrights, Provenance, and Lineage","abstract":"As Artificial Intelligence (AI) integrates into diverse areas, particularly in content generation, ensuring rightful ownership and ethical use becomes paramount. AI service providers are expected to prioritize responsibly sourcing training data and obtaining licenses from data owners. However, existing studies primarily center on safeguarding static copyrights, which simply treats metadata/datasets as non-fungible items with transferable/trading capabilities, neglecting the dynamic nature of training procedures that can shape an ongoing trajectory.   In this paper, we present \\textsc{IBis}, a blockchain-based framework tailored for AI model training workflows. \\textsc{IBis} integrates on-chain registries for datasets, licenses and models, alongside off-chain signing services to facilitate collaboration among multiple participants. Our framework addresses concerns regarding data and model provenance and copyright compliance. \\textsc{IBis} enables iterative model retraining and fine-tuning, and offers flexible license checks and renewals. Further, \\textsc{IBis} provides APIs designed for seamless integration with existing contract management software, minimizing disruptions to established model training processes. We implement \\textsc{IBis} using Daml on the Canton blockchain. Evaluation results showcase the feasibility and scalability of \\textsc{IBis} across varying numbers of users, datasets, models, and licenses.","sentences":["As Artificial Intelligence (AI) integrates into diverse areas, particularly in content generation, ensuring rightful ownership and ethical use becomes paramount.","AI service providers are expected to prioritize responsibly sourcing training data and obtaining licenses from data owners.","However, existing studies primarily center on safeguarding static copyrights, which simply treats metadata/datasets as non-fungible items with transferable/trading capabilities, neglecting the dynamic nature of training procedures that can shape an ongoing trajectory.   ","In this paper, we present \\textsc{IBis}, a blockchain-based framework tailored for AI model training workflows.","\\textsc{IBis} integrates on-chain registries for datasets, licenses and models, alongside off-chain signing services to facilitate collaboration among multiple participants.","Our framework addresses concerns regarding data and model provenance and copyright compliance.","\\textsc{IBis} enables iterative model retraining and fine-tuning, and offers flexible license checks and renewals.","Further, \\textsc{IBis} provides APIs designed for seamless integration with existing contract management software, minimizing disruptions to established model training processes.","We implement \\textsc{IBis} using Daml on the Canton blockchain.","Evaluation results showcase the feasibility and scalability of \\textsc{IBis} across varying numbers of users, datasets, models, and licenses."],"url":"http://arxiv.org/abs/2404.06077v1"}
{"created":"2024-04-09 07:15:53","title":"Fully Dynamic Matching and Ordered Ruzsa-Szemer\u00e9di Graphs","abstract":"We study the fully dynamic maximum matching problem. In this problem, the goal is to efficiently maintain an approximate maximum matching of a graph that is subject to edge insertions and deletions. Our focus is particularly on algorithms that maintain the edges of a $(1-\\epsilon)$-approximate maximum matching for an arbitrarily small constant $\\epsilon > 0$. Until recently, the fastest known algorithm for this problem required $\\Theta(n)$ time per update where $n$ is the number of vertices. This bound was slightly improved to $n/(\\log^* n)^{\\Omega(1)}$ by Assadi, Behnezhad, Khanna, and Li [STOC'23] and very recently to $n/2^{\\Omega(\\sqrt{\\log n})}$ by Liu [ArXiv'24]. Whether this can be improved to $n^{1-\\Omega(1)}$ remains a major open problem.   In this paper, we present a new algorithm that maintains a $(1-\\epsilon)$-approximate maximum matching. The update-time of our algorithm is parametrized based on the density of a certain class of graphs that we call Ordered Ruzsa-Szemer\\'edi (ORS) graphs, a generalization of the well-known Ruzsa-Szemer\\'edi graphs. While determining the density of ORS (or RS) remains a hard problem in combinatorics, we prove that if the existing constructions of ORS graphs are optimal, then our algorithm runs in $n^{1/2+O(\\epsilon)}$ time for any fixed $\\epsilon > 0$ which would be significantly faster than existing near-linear in $n$ time algorithms.   Our second main contribution is a better upper bound on density of both ORS and RS graphs with linear size matchings. The previous best upper bound was due to a proof of the triangle-removal lemma from more than a decade ago due to Fox [Annals of Mathematics '11].","sentences":["We study the fully dynamic maximum matching problem.","In this problem, the goal is to efficiently maintain an approximate maximum matching of a graph that is subject to edge insertions and deletions.","Our focus is particularly on algorithms that maintain the edges of a $(1-\\epsilon)$-approximate maximum matching for an arbitrarily small constant $\\epsilon > 0$.","Until recently, the fastest known algorithm for this problem required $\\Theta(n)$ time per update where $n$ is the number of vertices.","This bound was slightly improved to $n/(\\log^* n)^{\\Omega(1)}$ by Assadi, Behnezhad, Khanna, and Li","[STOC'23] and very recently to $n/2^{\\Omega(\\sqrt{\\log n})}$ by Liu","[ArXiv'24].","Whether this can be improved to $n^{1-\\Omega(1)}$ remains a major open problem.   ","In this paper, we present a new algorithm that maintains a $(1-\\epsilon)$-approximate maximum matching.","The update-time of our algorithm is parametrized based on the density of a certain class of graphs that we call Ordered Ruzsa-Szemer\\'edi (ORS) graphs, a generalization of the well-known Ruzsa-Szemer\\'edi graphs.","While determining the density of ORS (or RS) remains a hard problem in combinatorics, we prove that if the existing constructions of ORS graphs are optimal, then our algorithm runs in $n^{1/2+O(\\epsilon)}$ time for any fixed $\\epsilon > 0$ which would be significantly faster than existing near-linear in $n$ time algorithms.   ","Our second main contribution is a better upper bound on density of both ORS and RS graphs with linear size matchings.","The previous best upper bound was due to a proof of the triangle-removal lemma from more than a decade ago due to Fox","[Annals of Mathematics '11]."],"url":"http://arxiv.org/abs/2404.06069v1"}
{"created":"2024-04-09 07:14:40","title":"The Voronoi Diagram of Weakly Smooth Planar Point Sets in $O(\\log n)$ Deterministic Rounds on the Congested Clique","abstract":"We study the problem of computing the Voronoi diagram of a set of $n^2$ points with $O(\\log n)$-bit coordinates in the Euclidean plane in a substantially sublinear in $n$ number of rounds in the congested clique model with $n$ nodes. Recently, Jansson et al. have shown that if the points are uniformly at random distributed in a unit square then their Voronoi diagram within the square can be computed in $O(1)$ rounds with high probability (w.h.p.). We show that if a very weak smoothness condition is satisfied by an input set of $n^2$ points with $O(\\log n)$-bit coordinates in the unit square then the Voronoi diagram of the point set within the unit square can be computed in $O(\\log n)$ rounds in this model.","sentences":["We study the problem of computing the Voronoi diagram of a set of $n^2$ points with $O(\\log n)$-bit coordinates in the Euclidean plane in a substantially sublinear in $n$ number of rounds in the congested clique model with $n$ nodes.","Recently, Jansson et al. have shown that if the points are uniformly at random distributed in a unit square then their Voronoi diagram within the square can be computed in $O(1)$ rounds with high probability (w.h.p.).","We show that if a very weak smoothness condition is satisfied by an input set of $n^2$ points with $O(\\log n)$-bit coordinates in the unit square then the Voronoi diagram of the point set within the unit square can be computed in $O(\\log n)$ rounds in this model."],"url":"http://arxiv.org/abs/2404.06068v1"}
{"created":"2024-04-09 07:08:00","title":"Unified Entropy Optimization for Open-Set Test-Time Adaptation","abstract":"Test-time adaptation (TTA) aims at adapting a model pre-trained on the labeled source domain to the unlabeled target domain. Existing methods usually focus on improving TTA performance under covariate shifts, while neglecting semantic shifts. In this paper, we delve into a realistic open-set TTA setting where the target domain may contain samples from unknown classes. Many state-of-the-art closed-set TTA methods perform poorly when applied to open-set scenarios, which can be attributed to the inaccurate estimation of data distribution and model confidence. To address these issues, we propose a simple but effective framework called unified entropy optimization (UniEnt), which is capable of simultaneously adapting to covariate-shifted in-distribution (csID) data and detecting covariate-shifted out-of-distribution (csOOD) data. Specifically, UniEnt first mines pseudo-csID and pseudo-csOOD samples from test data, followed by entropy minimization on the pseudo-csID data and entropy maximization on the pseudo-csOOD data. Furthermore, we introduce UniEnt+ to alleviate the noise caused by hard data partition leveraging sample-level confidence. Extensive experiments on CIFAR benchmarks and Tiny-ImageNet-C show the superiority of our framework. The code is available at https://github.com/gaozhengqing/UniEnt","sentences":["Test-time adaptation (TTA) aims at adapting a model pre-trained on the labeled source domain to the unlabeled target domain.","Existing methods usually focus on improving TTA performance under covariate shifts, while neglecting semantic shifts.","In this paper, we delve into a realistic open-set TTA setting where the target domain may contain samples from unknown classes.","Many state-of-the-art closed-set TTA methods perform poorly when applied to open-set scenarios, which can be attributed to the inaccurate estimation of data distribution and model confidence.","To address these issues, we propose a simple but effective framework called unified entropy optimization (UniEnt), which is capable of simultaneously adapting to covariate-shifted in-distribution (csID) data and detecting covariate-shifted out-of-distribution (csOOD) data.","Specifically, UniEnt first mines pseudo-csID and pseudo-csOOD samples from test data, followed by entropy minimization on the pseudo-csID data and entropy maximization on the pseudo-csOOD data.","Furthermore, we introduce UniEnt+ to alleviate the noise caused by hard data partition leveraging sample-level confidence.","Extensive experiments on CIFAR benchmarks and Tiny-ImageNet-C show the superiority of our framework.","The code is available at https://github.com/gaozhengqing/UniEnt"],"url":"http://arxiv.org/abs/2404.06065v1"}
{"created":"2024-04-09 07:02:14","title":"All in One: An Empirical Study of GPT for Few-Shot Aspect-Based Sentiment Anlaysis","abstract":"Aspect-Based Sentiment Analysis (ABSA) is an indispensable and highly challenging task in natural language processing. Current efforts have focused on specific sub-tasks, making it difficult to comprehensively cover all sub-tasks within the ABSA domain. With the development of Generative Pre-trained Transformers (GPTs), there came inspiration for a one-stop solution to sentiment analysis. In this study, we used GPTs for all sub-tasks of few-shot ABSA while defining a general learning paradigm for this application. We propose the All in One (AiO) model, a simple yet effective two-stage model for all ABSA sub-tasks. In the first stage, a specific backbone network learns the semantic information of the review and generates heuristically enhanced candidates. In the second stage, AiO leverages GPT contextual learning capabilities to generate predictions. The study conducted comprehensive comparative and ablation experiments on five benchmark datasets, and the results show that AiO can effectively handle all ABSA sub-tasks, even with few-shot data.","sentences":["Aspect-Based Sentiment Analysis (ABSA) is an indispensable and highly challenging task in natural language processing.","Current efforts have focused on specific sub-tasks, making it difficult to comprehensively cover all sub-tasks within the ABSA domain.","With the development of Generative Pre-trained Transformers (GPTs), there came inspiration for a one-stop solution to sentiment analysis.","In this study, we used GPTs for all sub-tasks of few-shot ABSA while defining a general learning paradigm for this application.","We propose the All in One (AiO) model, a simple yet effective two-stage model for all ABSA sub-tasks.","In the first stage, a specific backbone network learns the semantic information of the review and generates heuristically enhanced candidates.","In the second stage, AiO leverages GPT contextual learning capabilities to generate predictions.","The study conducted comprehensive comparative and ablation experiments on five benchmark datasets, and the results show that AiO can effectively handle all ABSA sub-tasks, even with few-shot data."],"url":"http://arxiv.org/abs/2404.06063v1"}
{"created":"2024-04-09 06:47:44","title":"Unified Multi-modal Diagnostic Framework with Reconstruction Pre-training and Heterogeneity-combat Tuning","abstract":"Medical multi-modal pre-training has revealed promise in computer-aided diagnosis by leveraging large-scale unlabeled datasets. However, existing methods based on masked autoencoders mainly rely on data-level reconstruction tasks, but lack high-level semantic information. Furthermore, two significant heterogeneity challenges hinder the transfer of pre-trained knowledge to downstream tasks, \\textit{i.e.}, the distribution heterogeneity between pre-training data and downstream data, and the modality heterogeneity within downstream data. To address these challenges, we propose a Unified Medical Multi-modal Diagnostic (UMD) framework with tailored pre-training and downstream tuning strategies. Specifically, to enhance the representation abilities of vision and language encoders, we propose the Multi-level Reconstruction Pre-training (MR-Pretrain) strategy, including a feature-level and data-level reconstruction, which guides models to capture the semantic information from masked inputs of different modalities. Moreover, to tackle two kinds of heterogeneities during the downstream tuning, we present the heterogeneity-combat downstream tuning strategy, which consists of a Task-oriented Distribution Calibration (TD-Calib) and a Gradient-guided Modality Coordination (GM-Coord). In particular, TD-Calib fine-tunes the pre-trained model regarding the distribution of downstream datasets, and GM-Coord adjusts the gradient weights according to the dynamic optimization status of different modalities. Extensive experiments on five public medical datasets demonstrate the effectiveness of our UMD framework, which remarkably outperforms existing approaches on three kinds of downstream tasks.","sentences":["Medical multi-modal pre-training has revealed promise in computer-aided diagnosis by leveraging large-scale unlabeled datasets.","However, existing methods based on masked autoencoders mainly rely on data-level reconstruction tasks, but lack high-level semantic information.","Furthermore, two significant heterogeneity challenges hinder the transfer of pre-trained knowledge to downstream tasks, \\textit{i.e.}, the distribution heterogeneity between pre-training data and downstream data, and the modality heterogeneity within downstream data.","To address these challenges, we propose a Unified Medical Multi-modal Diagnostic (UMD) framework with tailored pre-training and downstream tuning strategies.","Specifically, to enhance the representation abilities of vision and language encoders, we propose the Multi-level Reconstruction Pre-training (MR-Pretrain) strategy, including a feature-level and data-level reconstruction, which guides models to capture the semantic information from masked inputs of different modalities.","Moreover, to tackle two kinds of heterogeneities during the downstream tuning, we present the heterogeneity-combat downstream tuning strategy, which consists of a Task-oriented Distribution Calibration (TD-Calib) and a Gradient-guided Modality Coordination (GM-Coord).","In particular, TD-Calib fine-tunes the pre-trained model regarding the distribution of downstream datasets, and GM-Coord adjusts the gradient weights according to the dynamic optimization status of different modalities.","Extensive experiments on five public medical datasets demonstrate the effectiveness of our UMD framework, which remarkably outperforms existing approaches on three kinds of downstream tasks."],"url":"http://arxiv.org/abs/2404.06057v1"}
{"created":"2024-04-09 06:07:03","title":"Automatic Configuration Tuning on Cloud Database: A Survey","abstract":"Faced with the challenges of big data, modern cloud database management systems are designed to efficiently store, organize, and retrieve data, supporting optimal performance, scalability, and reliability for complex data processing and analysis. However, achieving good performance in modern databases is non-trivial as they are notorious for having dozens of configurable knobs, such as hardware setup, software setup, database physical and logical design, etc., that control runtime behaviors and impact database performance. To find the optimal configuration for achieving optimal performance, extensive research has been conducted on automatic parameter tuning in DBMS. This paper provides a comprehensive survey of predominant configuration tuning techniques, including Bayesian optimization-based solutions, Neural network-based solutions, Reinforcement learning-based solutions, and Search-based solutions. Moreover, it investigates the fundamental aspects of parameter tuning pipeline, including tuning objective, workload characterization, feature pruning, knowledge from experience, configuration recommendation, and experimental settings. We highlight technique comparisons in each component, corresponding solutions, and introduce the experimental setting for performance evaluation. Finally, we conclude this paper and present future research opportunities. This paper aims to assist future researchers and practitioners in gaining a better understanding of automatic parameter tuning in cloud databases by providing state-of-the-art existing solutions, research directions, and evaluation benchmarks.","sentences":["Faced with the challenges of big data, modern cloud database management systems are designed to efficiently store, organize, and retrieve data, supporting optimal performance, scalability, and reliability for complex data processing and analysis.","However, achieving good performance in modern databases is non-trivial as they are notorious for having dozens of configurable knobs, such as hardware setup, software setup, database physical and logical design, etc., that control runtime behaviors and impact database performance.","To find the optimal configuration for achieving optimal performance, extensive research has been conducted on automatic parameter tuning in DBMS.","This paper provides a comprehensive survey of predominant configuration tuning techniques, including Bayesian optimization-based solutions, Neural network-based solutions, Reinforcement learning-based solutions, and Search-based solutions.","Moreover, it investigates the fundamental aspects of parameter tuning pipeline, including tuning objective, workload characterization, feature pruning, knowledge from experience, configuration recommendation, and experimental settings.","We highlight technique comparisons in each component, corresponding solutions, and introduce the experimental setting for performance evaluation.","Finally, we conclude this paper and present future research opportunities.","This paper aims to assist future researchers and practitioners in gaining a better understanding of automatic parameter tuning in cloud databases by providing state-of-the-art existing solutions, research directions, and evaluation benchmarks."],"url":"http://arxiv.org/abs/2404.06043v1"}
{"created":"2024-04-09 05:53:33","title":"A Survey of Distributed Graph Algorithms on Massive Graphs","abstract":"Distributed processing of large-scale graph data has many practical applications and has been widely studied. In recent years, a lot of distributed graph processing frameworks and algorithms have been proposed. While many efforts have been devoted to analyzing these, with most analyzing them based on programming models, less research focuses on understanding their challenges in distributed environments. Applying graph tasks to distributed environments is not easy, often facing numerous challenges through our analysis, including parallelism, load balancing, communication overhead, and bandwidth. In this paper, we provide an extensive overview of the current state-of-the-art in this field by outlining the challenges and solutions of distributed graph algorithms. We first conduct a systematic analysis of the inherent challenges in distributed graph processing, followed by presenting an overview of existing general solutions. Subsequently, we survey the challenges highlighted in recent distributed graph processing papers and the strategies adopted to address them. Finally, we discuss the current research trends and identify potential future opportunities.","sentences":["Distributed processing of large-scale graph data has many practical applications and has been widely studied.","In recent years, a lot of distributed graph processing frameworks and algorithms have been proposed.","While many efforts have been devoted to analyzing these, with most analyzing them based on programming models, less research focuses on understanding their challenges in distributed environments.","Applying graph tasks to distributed environments is not easy, often facing numerous challenges through our analysis, including parallelism, load balancing, communication overhead, and bandwidth.","In this paper, we provide an extensive overview of the current state-of-the-art in this field by outlining the challenges and solutions of distributed graph algorithms.","We first conduct a systematic analysis of the inherent challenges in distributed graph processing, followed by presenting an overview of existing general solutions.","Subsequently, we survey the challenges highlighted in recent distributed graph processing papers and the strategies adopted to address them.","Finally, we discuss the current research trends and identify potential future opportunities."],"url":"http://arxiv.org/abs/2404.06037v1"}
{"created":"2024-04-09 05:07:26","title":"Combinational Nonuniform Timeslicing of Dynamic Networks","abstract":"Dynamic networks represent the complex and evolving interrelationships between real-world entities. Given the scale and variability of these networks, finding an optimal slicing interval is essential for meaningful analysis. Nonuniform timeslicing, which adapts to density changes within the network, is drawing attention as a solution to this problem. In this research, we categorized existing algorithms into two domains -- data mining and visualization -- according to their approach to the problem. Data mining approach focuses on capturing temporal patterns of dynamic networks, while visualization approach emphasizes lessening the burden of analysis. We then introduce a novel nonuniform timeslicing method that synthesizes the strengths of both approaches, demonstrating its efficacy with a real-world data. The findings suggest that combining the two approaches offers the potential for more effective network analysis.","sentences":["Dynamic networks represent the complex and evolving interrelationships between real-world entities.","Given the scale and variability of these networks, finding an optimal slicing interval is essential for meaningful analysis.","Nonuniform timeslicing, which adapts to density changes within the network, is drawing attention as a solution to this problem.","In this research, we categorized existing algorithms into two domains -- data mining and visualization -- according to their approach to the problem.","Data mining approach focuses on capturing temporal patterns of dynamic networks, while visualization approach emphasizes lessening the burden of analysis.","We then introduce a novel nonuniform timeslicing method that synthesizes the strengths of both approaches, demonstrating its efficacy with a real-world data.","The findings suggest that combining the two approaches offers the potential for more effective network analysis."],"url":"http://arxiv.org/abs/2404.06021v1"}
{"created":"2024-04-09 04:45:18","title":"Feel-Good Thompson Sampling for Contextual Dueling Bandits","abstract":"Contextual dueling bandits, where a learner compares two options based on context and receives feedback indicating which was preferred, extends classic dueling bandits by incorporating contextual information for decision-making and preference learning. Several algorithms based on the upper confidence bound (UCB) have been proposed for linear contextual dueling bandits. However, no algorithm based on posterior sampling has been developed in this setting, despite the empirical success observed in traditional contextual bandits. In this paper, we propose a Thompson sampling algorithm, named FGTS.CDB, for linear contextual dueling bandits. At the core of our algorithm is a new Feel-Good exploration term specifically tailored for dueling bandits. This term leverages the independence of the two selected arms, thereby avoiding a cross term in the analysis. We show that our algorithm achieves nearly minimax-optimal regret, i.e., $\\tilde{\\mathcal{O}}(d\\sqrt T)$, where $d$ is the model dimension and $T$ is the time horizon. Finally, we evaluate our algorithm on synthetic data and observe that FGTS.CDB outperforms existing algorithms by a large margin.","sentences":["Contextual dueling bandits, where a learner compares two options based on context and receives feedback indicating which was preferred, extends classic dueling bandits by incorporating contextual information for decision-making and preference learning.","Several algorithms based on the upper confidence bound (UCB) have been proposed for linear contextual dueling bandits.","However, no algorithm based on posterior sampling has been developed in this setting, despite the empirical success observed in traditional contextual bandits.","In this paper, we propose a Thompson sampling algorithm, named FGTS.CDB, for linear contextual dueling bandits.","At the core of our algorithm is a new Feel-Good exploration term specifically tailored for dueling bandits.","This term leverages the independence of the two selected arms, thereby avoiding a cross term in the analysis.","We show that our algorithm achieves nearly minimax-optimal regret, i.e., $\\tilde{\\mathcal{O}}(d\\sqrt T)$, where $d$ is the model dimension and $T$ is the time horizon.","Finally, we evaluate our algorithm on synthetic data and observe that FGTS.CDB outperforms existing algorithms by a large margin."],"url":"http://arxiv.org/abs/2404.06013v1"}
{"created":"2024-04-09 04:41:05","title":"Diffusion-Based Point Cloud Super-Resolution for mmWave Radar Data","abstract":"The millimeter-wave radar sensor maintains stable performance under adverse environmental conditions, making it a promising solution for all-weather perception tasks, such as outdoor mobile robotics. However, the radar point clouds are relatively sparse and contain massive ghost points, which greatly limits the development of mmWave radar technology. In this paper, we propose a novel point cloud super-resolution approach for 3D mmWave radar data, named Radar-diffusion. Our approach employs the diffusion model defined by mean-reverting stochastic differential equations(SDE). Using our proposed new objective function with supervision from corresponding LiDAR point clouds, our approach efficiently handles radar ghost points and enhances the sparse mmWave radar point clouds to dense LiDAR-like point clouds. We evaluate our approach on two different datasets, and the experimental results show that our method outperforms the state-of-the-art baseline methods in 3D radar super-resolution tasks. Furthermore, we demonstrate that our enhanced radar point cloud is capable of downstream radar point-based registration tasks.","sentences":["The millimeter-wave radar sensor maintains stable performance under adverse environmental conditions, making it a promising solution for all-weather perception tasks, such as outdoor mobile robotics.","However, the radar point clouds are relatively sparse and contain massive ghost points, which greatly limits the development of mmWave radar technology.","In this paper, we propose a novel point cloud super-resolution approach for 3D mmWave radar data, named Radar-diffusion.","Our approach employs the diffusion model defined by mean-reverting stochastic differential equations(SDE).","Using our proposed new objective function with supervision from corresponding LiDAR point clouds, our approach efficiently handles radar ghost points and enhances the sparse mmWave radar point clouds to dense LiDAR-like point clouds.","We evaluate our approach on two different datasets, and the experimental results show that our method outperforms the state-of-the-art baseline methods in 3D radar super-resolution tasks.","Furthermore, we demonstrate that our enhanced radar point cloud is capable of downstream radar point-based registration tasks."],"url":"http://arxiv.org/abs/2404.06012v1"}
{"created":"2024-04-09 04:26:16","title":"Collaborative Edge AI Inference over Cloud-RAN","abstract":"In this paper, a cloud radio access network (Cloud-RAN) based collaborative edge AI inference architecture is proposed. Specifically, geographically distributed devices capture real-time noise-corrupted sensory data samples and extract the noisy local feature vectors, which are then aggregated at each remote radio head (RRH) to suppress sensing noise. To realize efficient uplink feature aggregation, we allow each RRH receives local feature vectors from all devices over the same resource blocks simultaneously by leveraging an over-the-air computation (AirComp) technique. Thereafter, these aggregated feature vectors are quantized and transmitted to a central processor (CP) for further aggregation and downstream inference tasks. Our aim in this work is to maximize the inference accuracy via a surrogate accuracy metric called discriminant gain, which measures the discernibility of different classes in the feature space. The key challenges lie on simultaneously suppressing the coupled sensing noise, AirComp distortion caused by hostile wireless channels, and the quantization error resulting from the limited capacity of fronthaul links. To address these challenges, this work proposes a joint transmit precoding, receive beamforming, and quantization error control scheme to enhance the inference accuracy. Extensive numerical experiments demonstrate the effectiveness and superiority of our proposed optimization algorithm compared to various baselines.","sentences":["In this paper, a cloud radio access network (Cloud-RAN) based collaborative edge AI inference architecture is proposed.","Specifically, geographically distributed devices capture real-time noise-corrupted sensory data samples and extract the noisy local feature vectors, which are then aggregated at each remote radio head (RRH) to suppress sensing noise.","To realize efficient uplink feature aggregation, we allow each RRH receives local feature vectors from all devices over the same resource blocks simultaneously by leveraging an over-the-air computation (AirComp) technique.","Thereafter, these aggregated feature vectors are quantized and transmitted to a central processor (CP) for further aggregation and downstream inference tasks.","Our aim in this work is to maximize the inference accuracy via a surrogate accuracy metric called discriminant gain, which measures the discernibility of different classes in the feature space.","The key challenges lie on simultaneously suppressing the coupled sensing noise, AirComp distortion caused by hostile wireless channels, and the quantization error resulting from the limited capacity of fronthaul links.","To address these challenges, this work proposes a joint transmit precoding, receive beamforming, and quantization error control scheme to enhance the inference accuracy.","Extensive numerical experiments demonstrate the effectiveness and superiority of our proposed optimization algorithm compared to various baselines."],"url":"http://arxiv.org/abs/2404.06007v1"}
{"created":"2024-04-09 04:20:27","title":"AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free Information Retrieval","abstract":"In approximate nearest neighbor search (ANNS) methods based on approximate proximity graphs, DiskANN achieves good recall-speed balance for large-scale datasets using both of RAM and storage. Despite it claims to save memory usage by loading compressed vectors by product quantization (PQ), its memory usage increases in proportion to the scale of datasets. In this paper, we propose All-in-Storage ANNS with Product Quantization (AiSAQ), which offloads the compressed vectors to storage. Our method achieves $\\sim$10 MB memory usage in query search even with billion-scale datasets with minor performance degradation. AiSAQ also reduces the index load time before query search, which enables the index switch between muitiple billion-scale datasets and significantly enhances the flexibility of retrieval-augmented generation (RAG). This method is applicable to all graph-based ANNS algorithms and can be combined with higher-spec ANNS methods in the future.","sentences":["In approximate nearest neighbor search (ANNS) methods based on approximate proximity graphs, DiskANN achieves good recall-speed balance for large-scale datasets using both of RAM and storage.","Despite it claims to save memory usage by loading compressed vectors by product quantization (PQ), its memory usage increases in proportion to the scale of datasets.","In this paper, we propose All-in-Storage ANNS with Product Quantization (AiSAQ), which offloads the compressed vectors to storage.","Our method achieves $\\sim$10 MB memory usage in query search even with billion-scale datasets with minor performance degradation.","AiSAQ also reduces the index load time before query search, which enables the index switch between muitiple billion-scale datasets and significantly enhances the flexibility of retrieval-augmented generation (RAG).","This method is applicable to all graph-based ANNS algorithms and can be combined with higher-spec ANNS methods in the future."],"url":"http://arxiv.org/abs/2404.06004v1"}
{"created":"2024-04-09 04:17:51","title":"FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models","abstract":"The rapid development of large language model (LLM) evaluation methodologies and datasets has led to a profound challenge: integrating state-of-the-art evaluation techniques cost-effectively while ensuring reliability, reproducibility, and efficiency. Currently, there is a notable absence of a unified and adaptable framework that seamlessly integrates various evaluation approaches. Moreover, the reliability of evaluation findings is often questionable due to potential data contamination, with the evaluation efficiency commonly overlooked when facing the substantial costs associated with LLM inference. In response to these challenges, we introduce FreeEval, a modular and scalable framework crafted to enable trustworthy and efficient automatic evaluations of LLMs. Firstly, FreeEval's unified abstractions simplify the integration and improve the transparency of diverse evaluation methodologies, encompassing dynamic evaluation that demand sophisticated LLM interactions. Secondly, the framework integrates meta-evaluation techniques like human evaluation and data contamination detection, which, along with dynamic evaluation modules in the platform, enhance the fairness of the evaluation outcomes. Lastly, FreeEval is designed with a high-performance infrastructure, including distributed computation and caching strategies, enabling extensive evaluations across multi-node, multi-GPU clusters for open-source and proprietary LLMs.","sentences":["The rapid development of large language model (LLM) evaluation methodologies and datasets has led to a profound challenge: integrating state-of-the-art evaluation techniques cost-effectively while ensuring reliability, reproducibility, and efficiency.","Currently, there is a notable absence of a unified and adaptable framework that seamlessly integrates various evaluation approaches.","Moreover, the reliability of evaluation findings is often questionable due to potential data contamination, with the evaluation efficiency commonly overlooked when facing the substantial costs associated with LLM inference.","In response to these challenges, we introduce FreeEval, a modular and scalable framework crafted to enable trustworthy and efficient automatic evaluations of LLMs.","Firstly, FreeEval's unified abstractions simplify the integration and improve the transparency of diverse evaluation methodologies, encompassing dynamic evaluation that demand sophisticated LLM interactions.","Secondly, the framework integrates meta-evaluation techniques like human evaluation and data contamination detection, which, along with dynamic evaluation modules in the platform, enhance the fairness of the evaluation outcomes.","Lastly, FreeEval is designed with a high-performance infrastructure, including distributed computation and caching strategies, enabling extensive evaluations across multi-node, multi-GPU clusters for open-source and proprietary LLMs."],"url":"http://arxiv.org/abs/2404.06003v1"}
{"created":"2024-04-09 03:52:58","title":"Polynomial-time derivation of optimal k-tree topology from Markov networks","abstract":"Characterization of joint probability distribution for large networks of random variables remains a challenging task in data science. Probabilistic graph approximation with simple topologies has practically been resorted to; typically the tree topology makes joint probability computation much simpler and can be effective for statistical inference on insufficient data. However, to characterize network components where multiple variables cooperate closely to influence others, model topologies beyond a tree are needed, which unfortunately are infeasible to acquire. In particular, our previous work has related optimal approximation of Markov networks of tree-width k >=2 closely to the graph-theoretic problem of finding maximum spanning k-tree (MSkT), which is a provably intractable task.   This paper investigates optimal approximation of Markov networks with k-tree topology that retains some designated underlying subgraph. Such a subgraph may encode certain background information that arises in scientific applications, for example, about a known significant pathway in gene networks or the indispensable backbone connectivity in the residue interaction graphs for a biomolecule 3D structure. In particular, it is proved that the \\beta-retaining MSkT problem, for a number of classes \\beta of graphs, admit O(n^{k+1})-time algorithms for every fixed k>= 1. These \\beta-retaining MSkT algorithms offer efficient solutions for approximation of Markov networks with k-tree topology in the situation where certain persistent information needs to be retained.","sentences":["Characterization of joint probability distribution for large networks of random variables remains a challenging task in data science.","Probabilistic graph approximation with simple topologies has practically been resorted to; typically the tree topology makes joint probability computation much simpler and can be effective for statistical inference on insufficient data.","However, to characterize network components where multiple variables cooperate closely to influence others, model topologies beyond a tree are needed, which unfortunately are infeasible to acquire.","In particular, our previous work has related optimal approximation of Markov networks of tree-width k >=2 closely to the graph-theoretic problem of finding maximum spanning k-tree (MSkT), which is a provably intractable task.   ","This paper investigates optimal approximation of Markov networks with k-tree topology that retains some designated underlying subgraph.","Such a subgraph may encode certain background information that arises in scientific applications, for example, about a known significant pathway in gene networks or the indispensable backbone connectivity in the residue interaction graphs for a biomolecule 3D structure.","In particular, it is proved that the \\beta-retaining MSkT problem, for a number of classes \\beta of graphs, admit O(n^{k+1})-time algorithms for every fixed k>= 1.","These \\beta-retaining MSkT algorithms offer efficient solutions for approximation of Markov networks with k-tree topology in the situation where certain persistent information needs to be retained."],"url":"http://arxiv.org/abs/2404.05991v1"}
{"created":"2024-04-09 03:45:15","title":"Comparison of Three Programming Error Measures for Explaining Variability in CS1 Grades","abstract":"Programming courses can be challenging for first year university students, especially for those without prior coding experience. Students initially struggle with code syntax, but as more advanced topics are introduced across a semester, the difficulty in learning to program shifts to learning computational thinking (e.g., debugging strategies). This study examined the relationships between students' rate of programming errors and their grades on two exams. Using an online integrated development environment, data were collected from 280 students in a Java programming course. The course had two parts. The first focused on introductory procedural programming and culminated with exam 1, while the second part covered more complex topics and object-oriented programming and ended with exam 2. To measure students' programming abilities, 51095 code snapshots were collected from students while they completed assignments that were autograded based on unit tests. Compiler and runtime errors were extracted from the snapshots, and three measures -- Error Count, Error Quotient and Repeated Error Density -- were explored to identify the best measure explaining variability in exam grades. Models utilizing Error Quotient outperformed the models using the other two measures, in terms of the explained variability in grades and Bayesian Information Criterion. Compiler errors were significant predictors of exam 1 grades but not exam 2 grades; only runtime errors significantly predicted exam 2 grades. The findings indicate that leveraging Error Quotient with multiple error types (compiler and runtime) may be a better measure of students' introductory programming abilities, though still not explaining most of the observed variability.","sentences":["Programming courses can be challenging for first year university students, especially for those without prior coding experience.","Students initially struggle with code syntax, but as more advanced topics are introduced across a semester, the difficulty in learning to program shifts to learning computational thinking (e.g., debugging strategies).","This study examined the relationships between students' rate of programming errors and their grades on two exams.","Using an online integrated development environment, data were collected from 280 students in a Java programming course.","The course had two parts.","The first focused on introductory procedural programming and culminated with exam 1, while the second part covered more complex topics and object-oriented programming and ended with exam 2.","To measure students' programming abilities, 51095 code snapshots were collected from students while they completed assignments that were autograded based on unit tests.","Compiler and runtime errors were extracted from the snapshots, and three measures -- Error Count, Error Quotient and Repeated Error Density -- were explored to identify the best measure explaining variability in exam grades.","Models utilizing Error Quotient outperformed the models using the other two measures, in terms of the explained variability in grades and Bayesian Information Criterion.","Compiler errors were significant predictors of exam 1 grades but not exam 2 grades; only runtime errors significantly predicted exam 2 grades.","The findings indicate that leveraging Error Quotient with multiple error types (compiler and runtime) may be a better measure of students' introductory programming abilities, though still not explaining most of the observed variability."],"url":"http://arxiv.org/abs/2404.05988v1"}
{"created":"2024-04-09 03:36:39","title":"Boosting Digital Safeguards: Blending Cryptography and Steganography","abstract":"In today's digital age, the internet is essential for communication and the sharing of information, creating a critical need for sophisticated data security measures to prevent unauthorized access and exploitation. Cryptography encrypts messages into a cipher text that is incomprehensible to unauthorized readers, thus safeguarding data during its transmission. Steganography, on the other hand, originates from the Greek term for \"covered writing\" and involves the art of hiding data within another medium, thereby facilitating covert communication by making the message invisible. This proposed approach takes advantage of the latest advancements in Artificial Intelligence (AI) and Deep Learning (DL), especially through the application of Generative Adversarial Networks (GANs), to improve upon traditional steganographic methods. By embedding encrypted data within another medium, our method ensures that the communication remains hidden from prying eyes. The application of GANs enables a smart, secure system that utilizes the inherent sensitivity of neural networks to slight alterations in data, enhancing the protection against detection. By merging the encryption techniques of cryptography with the hiding capabilities of steganography, and augmenting these with the strengths of AI, we introduce a comprehensive security system designed to maintain both the privacy and integrity of information. This system is crafted not just to prevent unauthorized access or modification of data, but also to keep the existence of the data hidden. This fusion of technologies tackles the core challenges of data security in the current era of open digital communication, presenting an advanced solution with the potential to transform the landscape of information security.","sentences":["In today's digital age, the internet is essential for communication and the sharing of information, creating a critical need for sophisticated data security measures to prevent unauthorized access and exploitation.","Cryptography encrypts messages into a cipher text that is incomprehensible to unauthorized readers, thus safeguarding data during its transmission.","Steganography, on the other hand, originates from the Greek term for \"covered writing\" and involves the art of hiding data within another medium, thereby facilitating covert communication by making the message invisible.","This proposed approach takes advantage of the latest advancements in Artificial Intelligence (AI) and Deep Learning (DL), especially through the application of Generative Adversarial Networks (GANs), to improve upon traditional steganographic methods.","By embedding encrypted data within another medium, our method ensures that the communication remains hidden from prying eyes.","The application of GANs enables a smart, secure system that utilizes the inherent sensitivity of neural networks to slight alterations in data, enhancing the protection against detection.","By merging the encryption techniques of cryptography with the hiding capabilities of steganography, and augmenting these with the strengths of AI, we introduce a comprehensive security system designed to maintain both the privacy and integrity of information.","This system is crafted not just to prevent unauthorized access or modification of data, but also to keep the existence of the data hidden.","This fusion of technologies tackles the core challenges of data security in the current era of open digital communication, presenting an advanced solution with the potential to transform the landscape of information security."],"url":"http://arxiv.org/abs/2404.05985v1"}
{"created":"2024-04-09 03:28:59","title":"On Achievable Covert Communication Performance under CSI Estimation Error and Feedback Delay","abstract":"Covert communication's effectiveness critically depends on precise channel state information (CSI). This paper investigates the impact of imperfect CSI on achievable covert communication performance in a two-hop relay system. Firstly, we introduce a two-hop covert transmission scheme utilizing channel inversion power control (CIPC) to manage opportunistic interference, eliminating the receiver's self-interference. Given that CSI estimation error (CEE) and feedback delay (FD) are the two primary factors leading to imperfect CSI, we construct a comprehensive theoretical model to accurately characterize their effects on CSI quality. With the aid of this model, we then derive closed-form solutions for detection error probability (DEP) and covert rate (CR), establishing an analytical framework to delineate the inherent relationship between CEE, FD, and covert performance. Furthermore, to mitigate the adverse effects of imperfect CSI on achievable covert performance, we investigate the joint optimization of channel inversion power and data symbol length to maximize CR under DEP constraints and propose an iterative alternating algorithm to solve the bi-dimensional non-convex optimization problem. Finally, extensive experimental results validate our theoretical framework and illustrate the impact of imperfect CSI on achievable covert performance.","sentences":["Covert communication's effectiveness critically depends on precise channel state information (CSI).","This paper investigates the impact of imperfect CSI on achievable covert communication performance in a two-hop relay system.","Firstly, we introduce a two-hop covert transmission scheme utilizing channel inversion power control (CIPC) to manage opportunistic interference, eliminating the receiver's self-interference.","Given that CSI estimation error (CEE) and feedback delay (FD) are the two primary factors leading to imperfect CSI, we construct a comprehensive theoretical model to accurately characterize their effects on CSI quality.","With the aid of this model, we then derive closed-form solutions for detection error probability (DEP) and covert rate (CR), establishing an analytical framework to delineate the inherent relationship between CEE, FD, and covert performance.","Furthermore, to mitigate the adverse effects of imperfect CSI on achievable covert performance, we investigate the joint optimization of channel inversion power and data symbol length to maximize CR under DEP constraints and propose an iterative alternating algorithm to solve the bi-dimensional non-convex optimization problem.","Finally, extensive experimental results validate our theoretical framework and illustrate the impact of imperfect CSI on achievable covert performance."],"url":"http://arxiv.org/abs/2404.05983v1"}
{"created":"2024-04-09 03:10:45","title":"A Cyber Manufacturing IoT System for Adaptive Machine Learning Model Deployment by Interactive Causality Enabled Self-Labeling","abstract":"Machine Learning (ML) has been demonstrated to improve productivity in many manufacturing applications. To host these ML applications, several software and Industrial Internet of Things (IIoT) systems have been proposed for manufacturing applications to deploy ML applications and provide real-time intelligence. Recently, an interactive causality enabled self-labeling method has been proposed to advance adaptive ML applications in cyber-physical systems, especially manufacturing, by automatically adapting and personalizing ML models after deployment to counter data distribution shifts. The unique features of the self-labeling method require a novel software system to support dynamism at various levels.   This paper proposes the AdaptIoT system, comprised of an end-to-end data streaming pipeline, ML service integration, and an automated self-labeling service. The self-labeling service consists of causal knowledge bases and automated full-cycle self-labeling workflows to adapt multiple ML models simultaneously. AdaptIoT employs a containerized microservice architecture to deliver a scalable and portable solution for small and medium-sized manufacturers. A field demonstration of a self-labeling adaptive ML application is conducted with a makerspace and shows reliable performance.","sentences":["Machine Learning (ML) has been demonstrated to improve productivity in many manufacturing applications.","To host these ML applications, several software and Industrial Internet of Things (IIoT) systems have been proposed for manufacturing applications to deploy ML applications and provide real-time intelligence.","Recently, an interactive causality enabled self-labeling method has been proposed to advance adaptive ML applications in cyber-physical systems, especially manufacturing, by automatically adapting and personalizing ML models after deployment to counter data distribution shifts.","The unique features of the self-labeling method require a novel software system to support dynamism at various levels.   ","This paper proposes the AdaptIoT system, comprised of an end-to-end data streaming pipeline, ML service integration, and an automated self-labeling service.","The self-labeling service consists of causal knowledge bases and automated full-cycle self-labeling workflows to adapt multiple ML models simultaneously.","AdaptIoT employs a containerized microservice architecture to deliver a scalable and portable solution for small and medium-sized manufacturers.","A field demonstration of a self-labeling adaptive ML application is conducted with a makerspace and shows reliable performance."],"url":"http://arxiv.org/abs/2404.05976v1"}
{"created":"2024-04-09 02:55:12","title":"JSTR: Judgment Improves Scene Text Recognition","abstract":"In this paper, we present a method for enhancing the accuracy of scene text recognition tasks by judging whether the image and text match each other. While previous studies focused on generating the recognition results from input images, our approach also considers the model's misrecognition results to understand its error tendencies, thus improving the text recognition pipeline. This method boosts text recognition accuracy by providing explicit feedback on the data that the model is likely to misrecognize by predicting correct or incorrect between the image and text. The experimental results on publicly available datasets demonstrate that our proposed method outperforms the baseline and state-of-the-art methods in scene text recognition.","sentences":["In this paper, we present a method for enhancing the accuracy of scene text recognition tasks by judging whether the image and text match each other.","While previous studies focused on generating the recognition results from input images, our approach also considers the model's misrecognition results to understand its error tendencies, thus improving the text recognition pipeline.","This method boosts text recognition accuracy by providing explicit feedback on the data that the model is likely to misrecognize by predicting correct or incorrect between the image and text.","The experimental results on publicly available datasets demonstrate that our proposed method outperforms the baseline and state-of-the-art methods in scene text recognition."],"url":"http://arxiv.org/abs/2404.05967v1"}
{"created":"2024-04-09 02:53:14","title":"THOUGHTSCULPT: Reasoning with Intermediate Revision and Search","abstract":"We present THOUGHTSCULPT, a general reasoning and search method for tasks with outputs that can be decomposed into components. THOUGHTSCULPT explores a search tree of potential solutions using Monte Carlo Tree Search (MCTS), building solutions one action at a time and evaluating according to any domain-specific heuristic, which in practice is often simply an LLM evaluator. Critically, our action space includes revision actions: THOUGHTSCULPT may choose to revise part of its previous output rather than continuing to build the rest of its output. Empirically, THOUGHTSCULPT outperforms state-of-the-art reasoning methods across three challenging tasks: Story Outline Improvement (up to +30% interestingness), Mini-Crosswords Solving (up to +16% word success rate), and Constrained Generation (up to +10% concept coverage).","sentences":["We present THOUGHTSCULPT, a general reasoning and search method for tasks with outputs that can be decomposed into components.","THOUGHTSCULPT explores a search tree of potential solutions using Monte Carlo Tree Search (MCTS), building solutions one action at a time and evaluating according to any domain-specific heuristic, which in practice is often simply an LLM evaluator.","Critically, our action space includes revision actions: THOUGHTSCULPT may choose to revise part of its previous output rather than continuing to build the rest of its output.","Empirically, THOUGHTSCULPT outperforms state-of-the-art reasoning methods across three challenging tasks: Story Outline Improvement (up to +30% interestingness), Mini-Crosswords Solving (up to +16% word success rate), and Constrained Generation (up to +10% concept coverage)."],"url":"http://arxiv.org/abs/2404.05966v1"}
{"created":"2024-04-09 02:52:55","title":"Deep Learning-Based Out-of-distribution Source Code Data Identification: How Far We Have Gone?","abstract":"Software vulnerabilities (SVs) have become a common, serious, and crucial concern to safety-critical security systems. That leads to significant progress in the use of AI-based methods for software vulnerability detection (SVD). In practice, although AI-based methods have been achieving promising performances in SVD and other domain applications (e.g., computer vision), they are well-known to fail in detecting the ground-truth label of input data (referred to as out-of-distribution, OOD, data) lying far away from the training data distribution (i.e., in-distribution, ID). This drawback leads to serious issues where the models fail to indicate when they are likely mistaken. To address this problem, OOD detectors (i.e., determining whether an input is ID or OOD) have been applied before feeding the input data to the downstream AI-based modules. While OOD detection has been widely designed for computer vision and medical diagnosis applications, automated AI-based techniques for OOD source code data detection have not yet been well-studied and explored. To this end, in this paper, we propose an innovative deep learning-based approach addressing the OOD source code data identification problem. Our method is derived from an information-theoretic perspective with the use of innovative cluster-contrastive learning to effectively learn and leverage source code characteristics, enhancing data representation learning for solving the problem. The rigorous and comprehensive experiments on real-world source code datasets show the effectiveness and advancement of our approach compared to state-of-the-art baselines by a wide margin. In short, on average, our method achieves a significantly higher performance from around 15.27%, 7.39%, and 4.93% on the FPR, AUROC, and AUPR measures, respectively, in comparison with the baselines.","sentences":["Software vulnerabilities (SVs) have become a common, serious, and crucial concern to safety-critical security systems.","That leads to significant progress in the use of AI-based methods for software vulnerability detection (SVD).","In practice, although AI-based methods have been achieving promising performances in SVD and other domain applications (e.g., computer vision), they are well-known to fail in detecting the ground-truth label of input data (referred to as out-of-distribution, OOD, data) lying far away from the training data distribution (i.e., in-distribution, ID).","This drawback leads to serious issues where the models fail to indicate when they are likely mistaken.","To address this problem, OOD detectors (i.e., determining whether an input is ID or OOD) have been applied before feeding the input data to the downstream AI-based modules.","While OOD detection has been widely designed for computer vision and medical diagnosis applications, automated AI-based techniques for OOD source code data detection have not yet been well-studied and explored.","To this end, in this paper, we propose an innovative deep learning-based approach addressing the OOD source code data identification problem.","Our method is derived from an information-theoretic perspective with the use of innovative cluster-contrastive learning to effectively learn and leverage source code characteristics, enhancing data representation learning for solving the problem.","The rigorous and comprehensive experiments on real-world source code datasets show the effectiveness and advancement of our approach compared to state-of-the-art baselines by a wide margin.","In short, on average, our method achieves a significantly higher performance from around 15.27%, 7.39%, and 4.93% on the FPR, AUROC, and AUPR measures, respectively, in comparison with the baselines."],"url":"http://arxiv.org/abs/2404.05964v1"}
{"created":"2024-04-09 02:51:05","title":"LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders","abstract":"Large decoder-only language models (LLMs) are the state-of-the-art models on most of today's NLP tasks and benchmarks. Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations. In this work, we introduce LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning. We demonstrate the effectiveness of LLM2Vec by applying it to 3 popular LLMs ranging from 1.3B to 7B parameters and evaluate the transformed models on English word- and sequence-level tasks. We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB). Moreover, when combining LLM2Vec with supervised contrastive learning, we achieve state-of-the-art performance on MTEB among models that train only on publicly available data. Our strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic GPT-4 generated data.","sentences":["Large decoder-only language models (LLMs) are the state-of-the-art models on most of today's NLP tasks and benchmarks.","Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations.","In this work, we introduce LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder.","LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning.","We demonstrate the effectiveness of LLM2Vec by applying it to 3 popular LLMs ranging from 1.3B to 7B parameters and evaluate the transformed models on English word- and sequence-level tasks.","We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB).","Moreover, when combining LLM2Vec with supervised contrastive learning, we achieve state-of-the-art performance on MTEB among models that train only on publicly available data.","Our strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic GPT-4 generated data."],"url":"http://arxiv.org/abs/2404.05961v1"}
{"created":"2024-04-09 02:21:23","title":"3D Branch Point Cloud Completion for Robotic Pruning in Apple Orchards","abstract":"Robotic branch pruning is a significantly growing research area to cope with the shortage of labor force in the context of agriculture. One fundamental requirement in robotic pruning is the perception of detailed geometry and topology of branches. However, the point clouds obtained in agricultural settings often exhibit incompleteness due to several constraints, thereby restricting the accuracy of downstream robotic pruning. In this work, we addressed the issue of point cloud quality through a simulation-based deep neural network, leveraging a Real-to-Simulation (Real2Sim) data generation pipeline that not only eliminates the need for manual parameterization but also guarantees the realism of simulated data. The simulation-based neural network was applied to jointly perform point cloud completion and skeletonization on real-world partial branches, without additional real-world training. The Sim2Real qualitative completion and skeletonization results showed the model's remarkable capability for geometry reconstruction and topology prediction. Additionally, we quantitatively evaluated the Sim2Real performance by comparing branch-level trait characterization errors using raw incomplete data and complete data. The Mean Absolute Error (MAE) reduced by 75% and 8% for branch diameter and branch angle estimation, respectively, using the best complete data, which indicates the effectiveness of the Real2Sim data in a zero-shot generalization setting. The characterization improvements contributed to the precision and efficacy of robotic branch pruning.","sentences":["Robotic branch pruning is a significantly growing research area to cope with the shortage of labor force in the context of agriculture.","One fundamental requirement in robotic pruning is the perception of detailed geometry and topology of branches.","However, the point clouds obtained in agricultural settings often exhibit incompleteness due to several constraints, thereby restricting the accuracy of downstream robotic pruning.","In this work, we addressed the issue of point cloud quality through a simulation-based deep neural network, leveraging a Real-to-Simulation (Real2Sim) data generation pipeline that not only eliminates the need for manual parameterization but also guarantees the realism of simulated data.","The simulation-based neural network was applied to jointly perform point cloud completion and skeletonization on real-world partial branches, without additional real-world training.","The Sim2Real qualitative completion and skeletonization results showed the model's remarkable capability for geometry reconstruction and topology prediction.","Additionally, we quantitatively evaluated the Sim2Real performance by comparing branch-level trait characterization errors using raw incomplete data and complete data.","The Mean Absolute Error (MAE) reduced by 75% and 8% for branch diameter and branch angle estimation, respectively, using the best complete data, which indicates the effectiveness of the Real2Sim data in a zero-shot generalization setting.","The characterization improvements contributed to the precision and efficacy of robotic branch pruning."],"url":"http://arxiv.org/abs/2404.05953v1"}
{"created":"2024-04-09 02:06:08","title":"Balanced Partitioning for Optimizing Big Graph Computation: Complexities and Approximation Algorithms","abstract":"Graph partitioning is a key fundamental problem in the area of big graph computation. Previous works do not consider the practical requirements when optimizing the big data analysis in real applications. In this paper, motivated by optimizing the big data computing applications, two typical problems of graph partitioning are studied. The first problem is to optimize the performance of specific workloads by graph partitioning, which lacks of algorithms with performance guarantees. The second problem is to optimize the computation of motifs by graph partitioning, which has not been focused by previous works. First, the formal definitions of the above two problems are introduced, and the semidefinite programming representations are also designed based on the analysis of the properties of the two problems. For the motif based partitioning problem, it is proved to be NP-complete even for the special case of $k=2$ and the motif is a triangle, and its inapproximability is also shown by proving that there are no efficient algorithms with finite approximation ratio. Finally, using the semidefinite programming and sophisticated rounding techniques, the bi-criteria $O(\\sqrt{\\log n\\log k})$-approximation algorithms with polynomial time cost are designed and analyzed for them.","sentences":["Graph partitioning is a key fundamental problem in the area of big graph computation.","Previous works do not consider the practical requirements when optimizing the big data analysis in real applications.","In this paper, motivated by optimizing the big data computing applications, two typical problems of graph partitioning are studied.","The first problem is to optimize the performance of specific workloads by graph partitioning, which lacks of algorithms with performance guarantees.","The second problem is to optimize the computation of motifs by graph partitioning, which has not been focused by previous works.","First, the formal definitions of the above two problems are introduced, and the semidefinite programming representations are also designed based on the analysis of the properties of the two problems.","For the motif based partitioning problem, it is proved to be NP-complete even for the special case of $k=2$ and the motif is a triangle, and its inapproximability is also shown by proving that there are no efficient algorithms with finite approximation ratio.","Finally, using the semidefinite programming and sophisticated rounding techniques, the bi-criteria $O(\\sqrt{\\log n\\log k})$-approximation algorithms with polynomial time cost are designed and analyzed for them."],"url":"http://arxiv.org/abs/2404.05949v1"}
{"created":"2024-04-09 01:55:05","title":"Interplay of Machine Translation, Diacritics, and Diacritization","abstract":"We investigate two research questions: (1) how do machine translation (MT) and diacritization influence the performance of each other in a multi-task learning setting (2) the effect of keeping (vs. removing) diacritics on MT performance. We examine these two questions in both high-resource (HR) and low-resource (LR) settings across 55 different languages (36 African languages and 19 European languages). For (1), results show that diacritization significantly benefits MT in the LR scenario, doubling or even tripling performance for some languages, but harms MT in the HR scenario. We find that MT harms diacritization in LR but benefits significantly in HR for some languages. For (2), MT performance is similar regardless of diacritics being kept or removed. In addition, we propose two classes of metrics to measure the complexity of a diacritical system, finding these metrics to correlate positively with the performance of our diacritization models. Overall, our work provides insights for developing MT and diacritization systems under different data size conditions and may have implications that generalize beyond the 55 languages we investigate.","sentences":["We investigate two research questions: (1) how do machine translation (MT) and diacritization influence the performance of each other in a multi-task learning setting (2) the effect of keeping (vs. removing) diacritics on MT performance.","We examine these two questions in both high-resource (HR) and low-resource (LR) settings across 55 different languages (36 African languages and 19 European languages).","For (1), results show that diacritization significantly benefits MT in the LR scenario, doubling or even tripling performance for some languages, but harms MT in the HR scenario.","We find that MT harms diacritization in LR but benefits significantly in HR for some languages.","For (2), MT performance is similar regardless of diacritics being kept or removed.","In addition, we propose two classes of metrics to measure the complexity of a diacritical system, finding these metrics to correlate positively with the performance of our diacritization models.","Overall, our work provides insights for developing MT and diacritization systems under different data size conditions and may have implications that generalize beyond the 55 languages we investigate."],"url":"http://arxiv.org/abs/2404.05943v1"}
{"created":"2024-04-09 00:30:16","title":"Prompt-driven Universal Model for View-Agnostic Echocardiography Analysis","abstract":"Echocardiography segmentation for cardiac analysis is time-consuming and resource-intensive due to the variability in image quality and the necessity to process scans from various standard views. While current automated segmentation methods in echocardiography show promising performance, they are trained on specific scan views to analyze corresponding data. However, this solution has a limitation as the number of required models increases with the number of standard views. To address this, in this paper, we present a prompt-driven universal method for view-agnostic echocardiography analysis. Considering the domain shift between standard views, we first introduce a method called prompt matching, aimed at learning prompts specific to different views by matching prompts and querying input embeddings using a pre-trained vision model. Then, we utilized a pre-trained medical language model to align textual information with pixel data for accurate segmentation. Extensive experiments on three standard views showed that our approach significantly outperforms the state-of-the-art universal methods and achieves comparable or even better performances over the segmentation model trained and tested on same views.","sentences":["Echocardiography segmentation for cardiac analysis is time-consuming and resource-intensive due to the variability in image quality and the necessity to process scans from various standard views.","While current automated segmentation methods in echocardiography show promising performance, they are trained on specific scan views to analyze corresponding data.","However, this solution has a limitation as the number of required models increases with the number of standard views.","To address this, in this paper, we present a prompt-driven universal method for view-agnostic echocardiography analysis.","Considering the domain shift between standard views, we first introduce a method called prompt matching, aimed at learning prompts specific to different views by matching prompts and querying input embeddings using a pre-trained vision model.","Then, we utilized a pre-trained medical language model to align textual information with pixel data for accurate segmentation.","Extensive experiments on three standard views showed that our approach significantly outperforms the state-of-the-art universal methods and achieves comparable or even better performances over the segmentation model trained and tested on same views."],"url":"http://arxiv.org/abs/2404.05916v1"}
{"created":"2024-04-09 00:07:16","title":"Deep Reinforcement Learning for Personalized Diagnostic Decision Pathways Using Electronic Health Records: A Comparative Study on Anemia and Systemic Lupus Erythematosus","abstract":"Background: Clinical diagnosis is typically reached by following a series of steps recommended by guidelines authored by colleges of experts. Accordingly, guidelines play a crucial role in rationalizing clinical decisions but suffer from limitations as they are built to cover the majority of the population and fail at covering patients with uncommon conditions. Moreover, their updates are long and expensive, making them unsuitable for emerging diseases and practices.   Methods: Inspired by guidelines, we formulate the task of diagnosis as a sequential decision-making problem and study the use of Deep Reinforcement Learning (DRL) algorithms to learn the optimal sequence of actions to perform in order to obtain a correct diagnosis from Electronic Health Records (EHRs). We apply DRL on synthetic, but realistic EHRs and develop two clinical use cases: Anemia diagnosis, where the decision pathways follow the schema of a decision tree; and Systemic Lupus Erythematosus (SLE) diagnosis, which follows a weighted criteria score. We particularly evaluate the robustness of our approaches to noisy and missing data since these frequently occur in EHRs.   Results: In both use cases, and in the presence of imperfect data, our best DRL algorithms exhibit competitive performance when compared to the traditional classifiers, with the added advantage that they enable the progressive generation of a pathway to the suggested diagnosis which can both guide and explain the decision-making process.   Conclusion: DRL offers the opportunity to learn personalized decision pathways to diagnosis. We illustrate with our two use cases their advantages: they generate step-by-step pathways that are self-explanatory; and their correctness is competitive when compared to state-of-the-art approaches.","sentences":["Background: Clinical diagnosis is typically reached by following a series of steps recommended by guidelines authored by colleges of experts.","Accordingly, guidelines play a crucial role in rationalizing clinical decisions but suffer from limitations as they are built to cover the majority of the population and fail at covering patients with uncommon conditions.","Moreover, their updates are long and expensive, making them unsuitable for emerging diseases and practices.   ","Methods: Inspired by guidelines, we formulate the task of diagnosis as a sequential decision-making problem and study the use of Deep Reinforcement Learning (DRL) algorithms to learn the optimal sequence of actions to perform in order to obtain a correct diagnosis from Electronic Health Records (EHRs).","We apply DRL on synthetic, but realistic EHRs and develop two clinical use cases: Anemia diagnosis, where the decision pathways follow the schema of a decision tree; and Systemic Lupus Erythematosus (SLE) diagnosis, which follows a weighted criteria score.","We particularly evaluate the robustness of our approaches to noisy and missing data since these frequently occur in EHRs.   ","Results:","In both use cases, and in the presence of imperfect data, our best DRL algorithms exhibit competitive performance when compared to the traditional classifiers, with the added advantage that they enable the progressive generation of a pathway to the suggested diagnosis which can both guide and explain the decision-making process.   ","Conclusion: DRL offers the opportunity to learn personalized decision pathways to diagnosis.","We illustrate with our two use cases their advantages: they generate step-by-step pathways that are self-explanatory; and their correctness is competitive when compared to state-of-the-art approaches."],"url":"http://arxiv.org/abs/2404.05913v1"}
{"created":"2024-04-08 23:46:59","title":"Interpretability in Symbolic Regression: a benchmark of Explanatory Methods using the Feynman data set","abstract":"In some situations, the interpretability of the machine learning models plays a role as important as the model accuracy. Interpretability comes from the need to trust the prediction model, verify some of its properties, or even enforce them to improve fairness. Many model-agnostic explanatory methods exists to provide explanations for black-box models. In the regression task, the practitioner can use white-boxes or gray-boxes models to achieve more interpretable results, which is the case of symbolic regression. When using an explanatory method, and since interpretability lacks a rigorous definition, there is a need to evaluate and compare the quality and different explainers. This paper proposes a benchmark scheme to evaluate explanatory methods to explain regression models, mainly symbolic regression models. Experiments were performed using 100 physics equations with different interpretable and non-interpretable regression methods and popular explanation methods, evaluating the performance of the explainers performance with several explanation measures. In addition, we further analyzed four benchmarks from the GP community. The results have shown that Symbolic Regression models can be an interesting alternative to white-box and black-box models that is capable of returning accurate models with appropriate explanations. Regarding the explainers, we observed that Partial Effects and SHAP were the most robust explanation models, with Integrated Gradients being unstable only with tree-based models. This benchmark is publicly available for further experiments.","sentences":["In some situations, the interpretability of the machine learning models plays a role as important as the model accuracy.","Interpretability comes from the need to trust the prediction model, verify some of its properties, or even enforce them to improve fairness.","Many model-agnostic explanatory methods exists to provide explanations for black-box models.","In the regression task, the practitioner can use white-boxes or gray-boxes models to achieve more interpretable results, which is the case of symbolic regression.","When using an explanatory method, and since interpretability lacks a rigorous definition, there is a need to evaluate and compare the quality and different explainers.","This paper proposes a benchmark scheme to evaluate explanatory methods to explain regression models, mainly symbolic regression models.","Experiments were performed using 100 physics equations with different interpretable and non-interpretable regression methods and popular explanation methods, evaluating the performance of the explainers performance with several explanation measures.","In addition, we further analyzed four benchmarks from the GP community.","The results have shown that Symbolic Regression models can be an interesting alternative to white-box and black-box models that is capable of returning accurate models with appropriate explanations.","Regarding the explainers, we observed that Partial Effects and SHAP were the most robust explanation models, with Integrated Gradients being unstable only with tree-based models.","This benchmark is publicly available for further experiments."],"url":"http://arxiv.org/abs/2404.05908v1"}
{"created":"2024-04-08 23:10:47","title":"WILBUR: Adaptive In-Context Learning for Robust and Accurate Web Agents","abstract":"In the realm of web agent research, achieving both generalization and accuracy remains a challenging problem. Due to high variance in website structure, existing approaches often fail. Moreover, existing fine-tuning and in-context learning techniques fail to generalize across multiple websites. We introduce Wilbur, an approach that uses a differentiable ranking model and a novel instruction synthesis technique to optimally populate a black-box large language model's prompt with task demonstrations from previous runs. To maximize end-to-end success rates, we also propose an intelligent backtracking mechanism that learns and recovers from its mistakes. Finally, we show that our ranking model can be trained on data from a generative auto-curriculum which samples representative goals from an LLM, runs the agent, and automatically evaluates it, with no manual annotation. Wilbur achieves state-of-the-art results on the WebVoyager benchmark, beating text-only models by 8% overall, and up to 36% on certain websites. On the same benchmark, Wilbur is within 5% of a strong multi-modal model despite only receiving textual inputs, and further analysis reveals a substantial number of failures are due to engineering challenges of operating the web.","sentences":["In the realm of web agent research, achieving both generalization and accuracy remains a challenging problem.","Due to high variance in website structure, existing approaches often fail.","Moreover, existing fine-tuning and in-context learning techniques fail to generalize across multiple websites.","We introduce Wilbur, an approach that uses a differentiable ranking model and a novel instruction synthesis technique to optimally populate a black-box large language model's prompt with task demonstrations from previous runs.","To maximize end-to-end success rates, we also propose an intelligent backtracking mechanism that learns and recovers from its mistakes.","Finally, we show that our ranking model can be trained on data from a generative auto-curriculum which samples representative goals from an LLM, runs the agent, and automatically evaluates it, with no manual annotation.","Wilbur achieves state-of-the-art results on the WebVoyager benchmark, beating text-only models by 8% overall, and up to 36% on certain websites.","On the same benchmark, Wilbur is within 5% of a strong multi-modal model despite only receiving textual inputs, and further analysis reveals a substantial number of failures are due to engineering challenges of operating the web."],"url":"http://arxiv.org/abs/2404.05902v1"}
{"created":"2024-04-08 22:52:37","title":"ClusterRadar: an Interactive Web-Tool for the Multi-Method Exploration of Spatial Clusters Over Time","abstract":"Spatial cluster analysis, the detection of localized patterns of similarity in geospatial data, has a wide-range of applications for scientific discovery and practical decision making. One way to detect spatial clusters is by using local indicators of spatial association, such as Local Moran's I or Getis-Ord Gi*. However, different indicators tend to produce substantially different results due to their distinct operational characteristics. Choosing a suitable method or comparing results from multiple methods is a complex task. Furthermore, spatial clusters are dynamic and it is often useful to track their evolution over time, which adds an additional layer of complexity. ClusterRadar is a web-tool designed to address these analytical challenges. The tool allows users to easily perform spatial clustering and analyze the results in an interactive environment, uniquely prioritizing temporal analysis and the comparison of multiple methods. The tool's interactive dashboard presents several visualizations, each offering a distinct perspective of the temporal and methodological aspects of the spatial clustering results. ClusterRadar has several features designed to maximize its utility to a broad user-base, including support for various geospatial formats, and a fully in-browser execution environment to preserve the privacy of sensitive data. Feedback from a varied set of researchers suggests ClusterRadar's potential for enhancing the temporal analysis of spatial clusters.","sentences":["Spatial cluster analysis, the detection of localized patterns of similarity in geospatial data, has a wide-range of applications for scientific discovery and practical decision making.","One way to detect spatial clusters is by using local indicators of spatial association, such as Local Moran's I or Getis-Ord Gi*.","However, different indicators tend to produce substantially different results due to their distinct operational characteristics.","Choosing a suitable method or comparing results from multiple methods is a complex task.","Furthermore, spatial clusters are dynamic and it is often useful to track their evolution over time, which adds an additional layer of complexity.","ClusterRadar is a web-tool designed to address these analytical challenges.","The tool allows users to easily perform spatial clustering and analyze the results in an interactive environment, uniquely prioritizing temporal analysis and the comparison of multiple methods.","The tool's interactive dashboard presents several visualizations, each offering a distinct perspective of the temporal and methodological aspects of the spatial clustering results.","ClusterRadar has several features designed to maximize its utility to a broad user-base, including support for various geospatial formats, and a fully in-browser execution environment to preserve the privacy of sensitive data.","Feedback from a varied set of researchers suggests ClusterRadar's potential for enhancing the temporal analysis of spatial clusters."],"url":"http://arxiv.org/abs/2404.05897v1"}
{"created":"2024-04-08 22:45:27","title":"Interference Reduction Design for Improved Multitarget Detection in ISAC Systems","abstract":"The advancement of wireless communication systems toward 5G and beyond is spurred by the demand for high data rates, exceedingly dependable low-latency communication, and extensive connectivity that aligns with sensing requisites such as advanced high-resolution sensing and target detection. Consequently, embedding sensing into communication has gained considerable attention. In this work, we propose an alternative approach for optimizing integrated sensing and communication (ISAC) waveform for target detection by concurrently maximizing the power of the communication signal at an intended user and minimizing the multi-user and sensing interference. We formulate the problem as a non-disciplined convex programming (NDCP) optimization and we use a distribution-based approach for interference cancellation. Precisely, we establish the distribution of the communication signal and the multi-user communication interference received by the intended user, and thereafter, we establish that the sensing interference can be distributed as a centralized Chi-squared if the sensing covariance matrix is idempotent. We design such a matrix based on the symmetrical idempotent property. Additionally, we propose a disciplined convex programming (DCP) form of the problem, and using successive convex approximation (SCA), we show that the solutions can reach a stable waveform for efficient target detection. Furthermore, we compare the proposed waveform with state of the art radar-communication waveform designs and demonstrate its superior performance by computer simulations.","sentences":["The advancement of wireless communication systems toward 5G and beyond is spurred by the demand for high data rates, exceedingly dependable low-latency communication, and extensive connectivity that aligns with sensing requisites such as advanced high-resolution sensing and target detection.","Consequently, embedding sensing into communication has gained considerable attention.","In this work, we propose an alternative approach for optimizing integrated sensing and communication (ISAC) waveform for target detection by concurrently maximizing the power of the communication signal at an intended user and minimizing the multi-user and sensing interference.","We formulate the problem as a non-disciplined convex programming (NDCP) optimization and we use a distribution-based approach for interference cancellation.","Precisely, we establish the distribution of the communication signal and the multi-user communication interference received by the intended user, and thereafter, we establish that the sensing interference can be distributed as a centralized Chi-squared if the sensing covariance matrix is idempotent.","We design such a matrix based on the symmetrical idempotent property.","Additionally, we propose a disciplined convex programming (DCP) form of the problem, and using successive convex approximation (SCA), we show that the solutions can reach a stable waveform for efficient target detection.","Furthermore, we compare the proposed waveform with state of the art radar-communication waveform designs and demonstrate its superior performance by computer simulations."],"url":"http://arxiv.org/abs/2404.05895v1"}
{"created":"2024-04-08 22:29:53","title":"Use of a Structured Knowledge Base Enhances Metadata Curation by Large Language Models","abstract":"Metadata play a crucial role in ensuring the findability, accessibility, interoperability, and reusability of datasets. This paper investigates the potential of large language models (LLMs), specifically GPT-4, to improve adherence to metadata standards. We conducted experiments on 200 random data records describing human samples relating to lung cancer from the NCBI BioSample repository, evaluating GPT-4's ability to suggest edits for adherence to metadata standards. We computed the adherence accuracy of field name-field value pairs through a peer review process, and we observed a marginal average improvement in adherence to the standard data dictionary from 79% to 80% (p<0.01). We then prompted GPT-4 with domain information in the form of the textual descriptions of CEDAR templates and recorded a significant improvement to 97% from 79% (p<0.01). These results indicate that, while LLMs may not be able to correct legacy metadata to ensure satisfactory adherence to standards when unaided, they do show promise for use in automated metadata curation when integrated with a structured knowledge base.","sentences":["Metadata play a crucial role in ensuring the findability, accessibility, interoperability, and reusability of datasets.","This paper investigates the potential of large language models (LLMs), specifically GPT-4, to improve adherence to metadata standards.","We conducted experiments on 200 random data records describing human samples relating to lung cancer from the NCBI BioSample repository, evaluating GPT-4's ability to suggest edits for adherence to metadata standards.","We computed the adherence accuracy of field name-field value pairs through a peer review process, and we observed a marginal average improvement in adherence to the standard data dictionary from 79% to 80% (p<0.01).","We then prompted GPT-4 with domain information in the form of the textual descriptions of CEDAR templates and recorded a significant improvement to 97% from 79% (p<0.01).","These results indicate that, while LLMs may not be able to correct legacy metadata to ensure satisfactory adherence to standards when unaided, they do show promise for use in automated metadata curation when integrated with a structured knowledge base."],"url":"http://arxiv.org/abs/2404.05893v1"}
{"created":"2024-04-08 21:48:36","title":"GBEC: Geometry-Based Hand-Eye Calibration","abstract":"Hand-eye calibration is the problem of solving the transformation from the end-effector of a robot to the sensor attached to it. Commonly employed techniques, such as AXXB or AXZB formulations, rely on regression methods that require collecting pose data from different robot configurations, which can produce low accuracy and repeatability. However, the derived transformation should solely depend on the geometry of the end-effector and the sensor attachment. We propose Geometry-Based End-Effector Calibration (GBEC) that enhances the repeatability and accuracy of the derived transformation compared to traditional hand-eye calibrations. To demonstrate improvements, we apply the approach to two different robot-assisted procedures: Transcranial Magnetic Stimulation (TMS) and femoroplasty. We also discuss the generalizability of GBEC for camera-in-hand and marker-in-hand sensor mounting methods. In the experiments, we perform GBEC between the robot end-effector and an optical tracker's rigid body marker attached to the TMS coil or femoroplasty drill guide. Previous research documents low repeatability and accuracy of the conventional methods for robot-assisted TMS hand-eye calibration. When compared to some existing methods, the proposed method relies solely on the geometry of the flange and the pose of the rigid-body marker, making it independent of workspace constraints or robot accuracy, without sacrificing the orthogonality of the rotation matrix. Our results validate the accuracy and applicability of the approach, providing a new and generalizable methodology for obtaining the transformation from the end-effector to a sensor.","sentences":["Hand-eye calibration is the problem of solving the transformation from the end-effector of a robot to the sensor attached to it.","Commonly employed techniques, such as AXXB or AXZB formulations, rely on regression methods that require collecting pose data from different robot configurations, which can produce low accuracy and repeatability.","However, the derived transformation should solely depend on the geometry of the end-effector and the sensor attachment.","We propose Geometry-Based End-Effector Calibration (GBEC) that enhances the repeatability and accuracy of the derived transformation compared to traditional hand-eye calibrations.","To demonstrate improvements, we apply the approach to two different robot-assisted procedures:","Transcranial Magnetic Stimulation (TMS) and femoroplasty.","We also discuss the generalizability of GBEC for camera-in-hand and marker-in-hand sensor mounting methods.","In the experiments, we perform GBEC between the robot end-effector and an optical tracker's rigid body marker attached to the TMS coil or femoroplasty drill guide.","Previous research documents low repeatability and accuracy of the conventional methods for robot-assisted TMS hand-eye calibration.","When compared to some existing methods, the proposed method relies solely on the geometry of the flange and the pose of the rigid-body marker, making it independent of workspace constraints or robot accuracy, without sacrificing the orthogonality of the rotation matrix.","Our results validate the accuracy and applicability of the approach, providing a new and generalizable methodology for obtaining the transformation from the end-effector to a sensor."],"url":"http://arxiv.org/abs/2404.05884v1"}
{"created":"2024-04-08 21:26:04","title":"Rapid and Precise Topological Comparison with Merge Tree Neural Networks","abstract":"Merge trees are a valuable tool in scientific visualization of scalar fields; however, current methods for merge tree comparisons are computationally expensive, primarily due to the exhaustive matching between tree nodes. To address this challenge, we introduce the merge tree neural networks (MTNN), a learned neural network model designed for merge tree comparison. The MTNN enables rapid and high-quality similarity computation. We first demonstrate how graph neural networks (GNNs), which emerged as an effective encoder for graphs, can be trained to produce embeddings of merge trees in vector spaces that enable efficient similarity comparison. Next, we formulate the novel MTNN model that further improves the similarity comparisons by integrating the tree and node embeddings with a new topological attention mechanism. We demonstrate the effectiveness of our model on real-world data in different domains and examine our model's generalizability across various datasets. Our experimental analysis demonstrates our approach's superiority in accuracy and efficiency. In particular, we speed up the prior state-of-the-art by more than 100x on the benchmark datasets while maintaining an error rate below 0.1%.","sentences":["Merge trees are a valuable tool in scientific visualization of scalar fields; however, current methods for merge tree comparisons are computationally expensive, primarily due to the exhaustive matching between tree nodes.","To address this challenge, we introduce the merge tree neural networks (MTNN), a learned neural network model designed for merge tree comparison.","The MTNN enables rapid and high-quality similarity computation.","We first demonstrate how graph neural networks (GNNs), which emerged as an effective encoder for graphs, can be trained to produce embeddings of merge trees in vector spaces that enable efficient similarity comparison.","Next, we formulate the novel MTNN model that further improves the similarity comparisons by integrating the tree and node embeddings with a new topological attention mechanism.","We demonstrate the effectiveness of our model on real-world data in different domains and examine our model's generalizability across various datasets.","Our experimental analysis demonstrates our approach's superiority in accuracy and efficiency.","In particular, we speed up the prior state-of-the-art by more than 100x on the benchmark datasets while maintaining an error rate below 0.1%."],"url":"http://arxiv.org/abs/2404.05879v1"}
{"created":"2024-04-08 21:19:10","title":"Privacy and Security of Women's Reproductive Health Apps in a Changing Legal Landscape","abstract":"FemTech, a rising trend in mobile apps, empowers women to digitally manage their health and family planning. However, privacy and security vulnerabilities in period-tracking and fertility-monitoring apps present significant risks, such as unintended pregnancies and legal consequences. Our approach involves manual observations of privacy policies and app permissions, along with dynamic and static analysis using multiple evaluation frameworks. Our research reveals that many of these apps gather personally identifiable information (PII) and sensitive healthcare data. Furthermore, our analysis identifies that 61% of the code vulnerabilities found in the apps are classified under the top-ten Open Web Application Security Project (OWASP) vulnerabilities. Our research emphasizes the significance of tackling the privacy and security vulnerabilities present in period-tracking and fertility-monitoring mobile apps. By highlighting these crucial risks, we aim to initiate a vital discussion and advocate for increased accountability and transparency of digital tools for women's health. We encourage the industry to prioritize user privacy and security, ultimately promoting a safer and more secure environment for women's health management.","sentences":["FemTech, a rising trend in mobile apps, empowers women to digitally manage their health and family planning.","However, privacy and security vulnerabilities in period-tracking and fertility-monitoring apps present significant risks, such as unintended pregnancies and legal consequences.","Our approach involves manual observations of privacy policies and app permissions, along with dynamic and static analysis using multiple evaluation frameworks.","Our research reveals that many of these apps gather personally identifiable information (PII) and sensitive healthcare data.","Furthermore, our analysis identifies that 61% of the code vulnerabilities found in the apps are classified under the top-ten Open Web Application Security Project (OWASP) vulnerabilities.","Our research emphasizes the significance of tackling the privacy and security vulnerabilities present in period-tracking and fertility-monitoring mobile apps.","By highlighting these crucial risks, we aim to initiate a vital discussion and advocate for increased accountability and transparency of digital tools for women's health.","We encourage the industry to prioritize user privacy and security, ultimately promoting a safer and more secure environment for women's health management."],"url":"http://arxiv.org/abs/2404.05876v1"}
{"created":"2024-04-08 21:15:36","title":"CodecLM: Aligning Language Models with Tailored Synthetic Data","abstract":"Instruction tuning has emerged as the key in aligning large language models (LLMs) with specific task instructions, thereby mitigating the discrepancy between the next-token prediction objective and users' actual goals. To reduce the labor and time cost to collect or annotate data by humans, researchers start to explore the use of LLMs to generate instruction-aligned synthetic data. Recent works focus on generating diverse instructions and applying LLM to increase instruction complexity, often neglecting downstream use cases. It remains unclear how to tailor high-quality data to elicit better instruction-following abilities in different target instruction distributions and LLMs. To this end, we introduce CodecLM, a general framework for adaptively generating high-quality synthetic data for LLM alignment with different downstream instruction distributions and LLMs. Drawing on the Encode-Decode principles, we use LLMs as codecs to guide the data generation process. We first encode seed instructions into metadata, which are concise keywords generated on-the-fly to capture the target instruction distribution, and then decode metadata to create tailored instructions. We also introduce Self-Rubrics and Contrastive Filtering during decoding to tailor data-efficient samples. Extensive experiments on four open-domain instruction following benchmarks validate the effectiveness of CodecLM over the current state-of-the-arts.","sentences":["Instruction tuning has emerged as the key in aligning large language models (LLMs) with specific task instructions, thereby mitigating the discrepancy between the next-token prediction objective and users' actual goals.","To reduce the labor and time cost to collect or annotate data by humans, researchers start to explore the use of LLMs to generate instruction-aligned synthetic data.","Recent works focus on generating diverse instructions and applying LLM to increase instruction complexity, often neglecting downstream use cases.","It remains unclear how to tailor high-quality data to elicit better instruction-following abilities in different target instruction distributions and LLMs.","To this end, we introduce CodecLM, a general framework for adaptively generating high-quality synthetic data for LLM alignment with different downstream instruction distributions and LLMs.","Drawing on the Encode-Decode principles, we use LLMs as codecs to guide the data generation process.","We first encode seed instructions into metadata, which are concise keywords generated on-the-fly to capture the target instruction distribution, and then decode metadata to create tailored instructions.","We also introduce Self-Rubrics and Contrastive Filtering during decoding to tailor data-efficient samples.","Extensive experiments on four open-domain instruction following benchmarks validate the effectiveness of CodecLM over the current state-of-the-arts."],"url":"http://arxiv.org/abs/2404.05875v1"}
{"created":"2024-04-08 21:08:13","title":"CoBT: Collaborative Programming of Behaviour Trees from One Demonstration for Robot Manipulation","abstract":"Mass customization and shorter manufacturing cycles are becoming more important among small and medium-sized companies. However, classical industrial robots struggle to cope with product variation and dynamic environments. In this paper, we present CoBT, a collaborative programming by demonstration framework for generating reactive and modular behavior trees. CoBT relies on a single demonstration and a combination of data-driven machine learning methods with logic-based declarative learning to learn a task, thus eliminating the need for programming expertise or long development times. The proposed framework is experimentally validated on 7 manipulation tasks and we show that CoBT achieves approx. 93% success rate overall with an average of 7.5s programming time. We conduct a pilot study with non-expert users to provide feedback regarding the usability of CoBT.","sentences":["Mass customization and shorter manufacturing cycles are becoming more important among small and medium-sized companies.","However, classical industrial robots struggle to cope with product variation and dynamic environments.","In this paper, we present CoBT, a collaborative programming by demonstration framework for generating reactive and modular behavior trees.","CoBT relies on a single demonstration and a combination of data-driven machine learning methods with logic-based declarative learning to learn a task, thus eliminating the need for programming expertise or long development times.","The proposed framework is experimentally validated on 7 manipulation tasks and we show that CoBT achieves approx.","93% success rate overall with an average of 7.5s programming time.","We conduct a pilot study with non-expert users to provide feedback regarding the usability of CoBT."],"url":"http://arxiv.org/abs/2404.05870v1"}
{"created":"2024-04-08 21:05:42","title":"Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning","abstract":"Large Language Models (LLMs) often memorize sensitive, private, or copyrighted data during pre-training. LLM unlearning aims to eliminate the influence of undesirable data from the pre-trained model while preserving the model's utilities on other tasks. Several practical methods have recently been proposed for LLM unlearning, mostly based on gradient ascent (GA) on the loss of undesirable data. However, on certain unlearning tasks, these methods either fail to effectively unlearn the target data or suffer from catastrophic collapse -- a drastic degradation of the model's utilities.   In this paper, we propose Negative Preference Optimization (NPO), a simple alignment-inspired method that could efficiently and effectively unlearn a target dataset. We theoretically show that the progression toward catastrophic collapse by minimizing the NPO loss is exponentially slower than GA. Through experiments on synthetic data and the benchmark TOFU dataset, we demonstrate that NPO-based methods achieve a better balance between unlearning the undesirable data and maintaining the model's utilities. We also observe that NPO-based methods generate more sensible outputs than GA-based methods, whose outputs are often gibberish. Remarkably, on TOFU, NPO-based methods are the first to achieve reasonable unlearning results in forgetting 50% (or more) of the training data, whereas existing methods already struggle with forgetting 10% of training data.","sentences":["Large Language Models (LLMs) often memorize sensitive, private, or copyrighted data during pre-training.","LLM unlearning aims to eliminate the influence of undesirable data from the pre-trained model while preserving the model's utilities on other tasks.","Several practical methods have recently been proposed for LLM unlearning, mostly based on gradient ascent (GA) on the loss of undesirable data.","However, on certain unlearning tasks, these methods either fail to effectively unlearn the target data or suffer from catastrophic collapse -- a drastic degradation of the model's utilities.   ","In this paper, we propose Negative Preference Optimization (NPO), a simple alignment-inspired method that could efficiently and effectively unlearn a target dataset.","We theoretically show that the progression toward catastrophic collapse by minimizing the NPO loss is exponentially slower than GA.","Through experiments on synthetic data and the benchmark TOFU dataset, we demonstrate that NPO-based methods achieve a better balance between unlearning the undesirable data and maintaining the model's utilities.","We also observe that NPO-based methods generate more sensible outputs than GA-based methods, whose outputs are often gibberish.","Remarkably, on TOFU, NPO-based methods are the first to achieve reasonable unlearning results in forgetting 50% (or more) of the training data, whereas existing methods already struggle with forgetting 10% of training data."],"url":"http://arxiv.org/abs/2404.05868v1"}
{"created":"2024-04-08 20:58:06","title":"GeniL: A Multilingual Dataset on Generalizing Language","abstract":"LLMs are increasingly transforming our digital ecosystem, but they often inherit societal biases learned from their training data, for instance stereotypes associating certain attributes with specific identity groups. While whether and how these biases are mitigated may depend on the specific use cases, being able to effectively detect instances of stereotype perpetuation is a crucial first step. Current methods to assess presence of stereotypes in generated language rely on simple template or co-occurrence based measures, without accounting for the variety of sentential contexts they manifest in. We argue that understanding the sentential context is crucial for detecting instances of generalization. We distinguish two types of generalizations: (1) language that merely mentions the presence of a generalization (\"people think the French are very rude\"), and (2) language that reinforces such a generalization (\"as French they must be rude\"), from non-generalizing context (\"My French friends think I am rude\"). For meaningful stereotype evaluations, we need to reliably distinguish such instances of generalizations. We introduce the new task of detecting generalization in language, and build GeniL, a multilingual dataset of over 50K sentences from 9 languages (English, Arabic, Bengali, Spanish, French, Hindi, Indonesian, Malay, and Portuguese) annotated for instances of generalizations. We demonstrate that the likelihood of a co-occurrence being an instance of generalization is usually low, and varies across different languages, identity groups, and attributes. We build classifiers to detect generalization in language with an overall PR-AUC of 58.7, with varying degrees of performance across languages. Our research provides data and tools to enable a nuanced understanding of stereotype perpetuation, a crucial step towards more inclusive and responsible language technologies.","sentences":["LLMs are increasingly transforming our digital ecosystem, but they often inherit societal biases learned from their training data, for instance stereotypes associating certain attributes with specific identity groups.","While whether and how these biases are mitigated may depend on the specific use cases, being able to effectively detect instances of stereotype perpetuation is a crucial first step.","Current methods to assess presence of stereotypes in generated language rely on simple template or co-occurrence based measures, without accounting for the variety of sentential contexts they manifest in.","We argue that understanding the sentential context is crucial for detecting instances of generalization.","We distinguish two types of generalizations: (1) language that merely mentions the presence of a generalization (\"people think the French are very rude\"), and (2) language that reinforces such a generalization (\"as French they must be rude\"), from non-generalizing context (\"My French friends think I am rude\").","For meaningful stereotype evaluations, we need to reliably distinguish such instances of generalizations.","We introduce the new task of detecting generalization in language, and build GeniL, a multilingual dataset of over 50K sentences from 9 languages (English, Arabic, Bengali, Spanish, French, Hindi, Indonesian, Malay, and Portuguese) annotated for instances of generalizations.","We demonstrate that the likelihood of a co-occurrence being an instance of generalization is usually low, and varies across different languages, identity groups, and attributes.","We build classifiers to detect generalization in language with an overall PR-AUC of 58.7, with varying degrees of performance across languages.","Our research provides data and tools to enable a nuanced understanding of stereotype perpetuation, a crucial step towards more inclusive and responsible language technologies."],"url":"http://arxiv.org/abs/2404.05866v1"}
{"created":"2024-04-08 20:51:30","title":"Towards Improved Semiconductor Defect Inspection for high-NA EUVL based on SEMI-SuperYOLO-NAS","abstract":"Due to potential pitch reduction, the semiconductor industry is adopting High-NA EUVL technology. However, its low depth of focus presents challenges for High Volume Manufacturing. To address this, suppliers are exploring thinner photoresists and new underlayers/hardmasks. These may suffer from poor SNR, complicating defect detection. Vision-based ML algorithms offer a promising solution for semiconductor defect inspection. However, developing a robust ML model across various image resolutions without explicit training remains a challenge for nano-scale defect inspection. This research's goal is to propose a scale-invariant ADCD framework capable to upscale images, addressing this issue. We propose an improvised ADCD framework as SEMI-SuperYOLO-NAS, which builds upon the baseline YOLO-NAS architecture. This framework integrates a SR assisted branch to aid in learning HR features by the defect detection backbone, particularly for detecting nano-scale defect instances from LR images. Additionally, the SR-assisted branch can recursively generate upscaled images from their corresponding downscaled counterparts, enabling defect detection inference across various image resolutions without requiring explicit training. Moreover, we investigate improved data augmentation strategy aimed at generating diverse and realistic training datasets to enhance model performance. We have evaluated our proposed approach using two original FAB datasets obtained from two distinct processes and captured using two different imaging tools. Finally, we demonstrate zero-shot inference for our model on a new, originating from a process condition distinct from the training dataset and possessing different Pitch characteristics. Experimental validation demonstrates that our proposed ADCD framework aids in increasing the throughput of imaging tools for defect inspection by reducing the required image pixel resolutions.","sentences":["Due to potential pitch reduction, the semiconductor industry is adopting High-NA EUVL technology.","However, its low depth of focus presents challenges for High Volume Manufacturing.","To address this, suppliers are exploring thinner photoresists and new underlayers/hardmasks.","These may suffer from poor SNR, complicating defect detection.","Vision-based ML algorithms offer a promising solution for semiconductor defect inspection.","However, developing a robust ML model across various image resolutions without explicit training remains a challenge for nano-scale defect inspection.","This research's goal is to propose a scale-invariant ADCD framework capable to upscale images, addressing this issue.","We propose an improvised ADCD framework as SEMI-SuperYOLO-NAS, which builds upon the baseline YOLO-NAS architecture.","This framework integrates a SR assisted branch to aid in learning HR features by the defect detection backbone, particularly for detecting nano-scale defect instances from LR images.","Additionally, the SR-assisted branch can recursively generate upscaled images from their corresponding downscaled counterparts, enabling defect detection inference across various image resolutions without requiring explicit training.","Moreover, we investigate improved data augmentation strategy aimed at generating diverse and realistic training datasets to enhance model performance.","We have evaluated our proposed approach using two original FAB datasets obtained from two distinct processes and captured using two different imaging tools.","Finally, we demonstrate zero-shot inference for our model on a new, originating from a process condition distinct from the training dataset and possessing different Pitch characteristics.","Experimental validation demonstrates that our proposed ADCD framework aids in increasing the throughput of imaging tools for defect inspection by reducing the required image pixel resolutions."],"url":"http://arxiv.org/abs/2404.05862v1"}
{"created":"2024-04-08 20:42:10","title":"A Neuromorphic Approach to Obstacle Avoidance in Robot Manipulation","abstract":"Neuromorphic computing mimics computational principles of the brain in $\\textit{silico}$ and motivates research into event-based vision and spiking neural networks (SNNs). Event cameras (ECs) exclusively capture local intensity changes and offer superior power consumption, response latencies, and dynamic ranges. SNNs replicate biological neuronal dynamics and have demonstrated potential as alternatives to conventional artificial neural networks (ANNs), such as in reducing energy expenditure and inference time in visual classification. Nevertheless, these novel paradigms remain scarcely explored outside the domain of aerial robots.   To investigate the utility of brain-inspired sensing and data processing, we developed a neuromorphic approach to obstacle avoidance on a camera-equipped manipulator. Our approach adapts high-level trajectory plans with reactive maneuvers by processing emulated event data in a convolutional SNN, decoding neural activations into avoidance motions, and adjusting plans using a dynamic motion primitive. We conducted experiments with a Kinova Gen3 arm performing simple reaching tasks that involve obstacles in sets of distinct task scenarios and in comparison to a non-adaptive baseline.   Our neuromorphic approach facilitated reliable avoidance of imminent collisions in simulated and real-world experiments, where the baseline consistently failed. Trajectory adaptations had low impacts on safety and predictability criteria. Among the notable SNN properties were the correlation of computations with the magnitude of perceived motions and a robustness to different event emulation methods. Tests with a DAVIS346 EC showed similar performance, validating our experimental event emulation. Our results motivate incorporating SNN learning, utilizing neuromorphic processors, and further exploring the potential of neuromorphic methods.","sentences":["Neuromorphic computing mimics computational principles of the brain in $\\textit{silico}$ and motivates research into event-based vision and spiking neural networks (SNNs).","Event cameras (ECs) exclusively capture local intensity changes and offer superior power consumption, response latencies, and dynamic ranges.","SNNs replicate biological neuronal dynamics and have demonstrated potential as alternatives to conventional artificial neural networks (ANNs), such as in reducing energy expenditure and inference time in visual classification.","Nevertheless, these novel paradigms remain scarcely explored outside the domain of aerial robots.   ","To investigate the utility of brain-inspired sensing and data processing, we developed a neuromorphic approach to obstacle avoidance on a camera-equipped manipulator.","Our approach adapts high-level trajectory plans with reactive maneuvers by processing emulated event data in a convolutional SNN, decoding neural activations into avoidance motions, and adjusting plans using a dynamic motion primitive.","We conducted experiments with a Kinova Gen3 arm performing simple reaching tasks that involve obstacles in sets of distinct task scenarios and in comparison to a non-adaptive baseline.   ","Our neuromorphic approach facilitated reliable avoidance of imminent collisions in simulated and real-world experiments, where the baseline consistently failed.","Trajectory adaptations had low impacts on safety and predictability criteria.","Among the notable SNN properties were the correlation of computations with the magnitude of perceived motions and a robustness to different event emulation methods.","Tests with a DAVIS346 EC showed similar performance, validating our experimental event emulation.","Our results motivate incorporating SNN learning, utilizing neuromorphic processors, and further exploring the potential of neuromorphic methods."],"url":"http://arxiv.org/abs/2404.05858v1"}
{"created":"2024-04-08 20:25:23","title":"Tree-Based versus Hybrid Graphical-Textual Model Editors: An Empirical Study of Testing Specifications","abstract":"Tree-based model editors and hybrid graphical-textual model editors have advantages and limitations when editing domain models. Data is displayed hierarchically in tree-based model editors, whereas hybrid graphical-textual model editors capture high-level domain concepts graphically and low-level domain details textually. We conducted an empirical user study with 22 participants, to evaluate the implicit assumption of system modellers that hybrid notations are superior, and to investigate the tradeoffs between tree-based and hybrid model editors. The results of the user study indicate that users largely prefer hybrid editors and are more confident with hybrid notations for understanding the meaning of conditions. Furthermore, we found that tree editors provide superior performance for analysing ordered lists of model elements, whereas activities requiring the comprehension or modelling of complex conditions are carried out faster through a hybrid editor.","sentences":["Tree-based model editors and hybrid graphical-textual model editors have advantages and limitations when editing domain models.","Data is displayed hierarchically in tree-based model editors, whereas hybrid graphical-textual model editors capture high-level domain concepts graphically and low-level domain details textually.","We conducted an empirical user study with 22 participants, to evaluate the implicit assumption of system modellers that hybrid notations are superior, and to investigate the tradeoffs between tree-based and hybrid model editors.","The results of the user study indicate that users largely prefer hybrid editors and are more confident with hybrid notations for understanding the meaning of conditions.","Furthermore, we found that tree editors provide superior performance for analysing ordered lists of model elements, whereas activities requiring the comprehension or modelling of complex conditions are carried out faster through a hybrid editor."],"url":"http://arxiv.org/abs/2404.05846v1"}
{"created":"2024-04-08 20:06:33","title":"Attention-Driven Multi-Agent Reinforcement Learning: Enhancing Decisions with Expertise-Informed Tasks","abstract":"In this paper, we introduce an alternative approach to enhancing Multi-Agent Reinforcement Learning (MARL) through the integration of domain knowledge and attention-based policy mechanisms. Our methodology focuses on the incorporation of domain-specific expertise into the learning process, which simplifies the development of collaborative behaviors. This approach aims to reduce the complexity and learning overhead typically associated with MARL by enabling agents to concentrate on essential aspects of complex tasks, thus optimizing the learning curve. The utilization of attention mechanisms plays a key role in our model. It allows for the effective processing of dynamic context data and nuanced agent interactions, leading to more refined decision-making. Applied in standard MARL scenarios, such as the Stanford Intelligent Systems Laboratory (SISL) Pursuit and Multi-Particle Environments (MPE) Simple Spread, our method has been shown to improve both learning efficiency and the effectiveness of collaborative behaviors. The results indicate that our attention-based approach can be a viable approach for improving the efficiency of MARL training process, integrating domain-specific knowledge at the action level.","sentences":["In this paper, we introduce an alternative approach to enhancing Multi-Agent Reinforcement Learning (MARL) through the integration of domain knowledge and attention-based policy mechanisms.","Our methodology focuses on the incorporation of domain-specific expertise into the learning process, which simplifies the development of collaborative behaviors.","This approach aims to reduce the complexity and learning overhead typically associated with MARL by enabling agents to concentrate on essential aspects of complex tasks, thus optimizing the learning curve.","The utilization of attention mechanisms plays a key role in our model.","It allows for the effective processing of dynamic context data and nuanced agent interactions, leading to more refined decision-making.","Applied in standard MARL scenarios, such as the Stanford Intelligent Systems Laboratory (SISL) Pursuit and Multi-Particle Environments (MPE) Simple Spread, our method has been shown to improve both learning efficiency and the effectiveness of collaborative behaviors.","The results indicate that our attention-based approach can be a viable approach for improving the efficiency of MARL training process, integrating domain-specific knowledge at the action level."],"url":"http://arxiv.org/abs/2404.05840v1"}
{"created":"2024-04-08 19:48:36","title":"SambaLingo: Teaching Large Language Models New Languages","abstract":"Despite the widespread availability of LLMs, there remains a substantial gap in their capabilities and availability across diverse languages. One approach to address these issues has been to take an existing pre-trained LLM and continue to train it on new languages. While prior works have experimented with language adaptation, many questions around best practices and methodology have not been covered. In this paper, we present a comprehensive investigation into the adaptation of LLMs to new languages. Our study covers the key components in this process, including vocabulary extension, direct preference optimization and the data scarcity problem for human alignment in low-resource languages. We scale these experiments across 9 languages and 2 parameter scales (7B and 70B). We compare our models against Llama 2, Aya-101, XGLM, BLOOM and existing language experts, outperforming all prior published baselines. Additionally, all evaluation code and checkpoints are made public to facilitate future research.","sentences":["Despite the widespread availability of LLMs, there remains a substantial gap in their capabilities and availability across diverse languages.","One approach to address these issues has been to take an existing pre-trained LLM and continue to train it on new languages.","While prior works have experimented with language adaptation, many questions around best practices and methodology have not been covered.","In this paper, we present a comprehensive investigation into the adaptation of LLMs to new languages.","Our study covers the key components in this process, including vocabulary extension, direct preference optimization and the data scarcity problem for human alignment in low-resource languages.","We scale these experiments across 9 languages and 2 parameter scales (7B and 70B).","We compare our models against Llama 2, Aya-101, XGLM, BLOOM and existing language experts, outperforming all prior published baselines.","Additionally, all evaluation code and checkpoints are made public to facilitate future research."],"url":"http://arxiv.org/abs/2404.05829v1"}
{"created":"2024-04-08 19:46:20","title":"Privacy-Preserving Deep Learning Using Deformable Operators for Secure Task Learning","abstract":"In the era of cloud computing and data-driven applications, it is crucial to protect sensitive information to maintain data privacy, ensuring truly reliable systems. As a result, preserving privacy in deep learning systems has become a critical concern. Existing methods for privacy preservation rely on image encryption or perceptual transformation approaches. However, they often suffer from reduced task performance and high computational costs. To address these challenges, we propose a novel Privacy-Preserving framework that uses a set of deformable operators for secure task learning. Our method involves shuffling pixels during the analog-to-digital conversion process to generate visually protected data. Those are then fed into a well-known network enhanced with deformable operators. Using our approach, users can achieve equivalent performance to original images without additional training using a secret key. Moreover, our method enables access control against unauthorized users. Experimental results demonstrate the efficacy of our approach, showcasing its potential in cloud-based scenarios and privacy-sensitive applications.","sentences":["In the era of cloud computing and data-driven applications, it is crucial to protect sensitive information to maintain data privacy, ensuring truly reliable systems.","As a result, preserving privacy in deep learning systems has become a critical concern.","Existing methods for privacy preservation rely on image encryption or perceptual transformation approaches.","However, they often suffer from reduced task performance and high computational costs.","To address these challenges, we propose a novel Privacy-Preserving framework that uses a set of deformable operators for secure task learning.","Our method involves shuffling pixels during the analog-to-digital conversion process to generate visually protected data.","Those are then fed into a well-known network enhanced with deformable operators.","Using our approach, users can achieve equivalent performance to original images without additional training using a secret key.","Moreover, our method enables access control against unauthorized users.","Experimental results demonstrate the efficacy of our approach, showcasing its potential in cloud-based scenarios and privacy-sensitive applications."],"url":"http://arxiv.org/abs/2404.05828v1"}
{"created":"2024-04-08 19:29:15","title":"Decade-long Utilization Patterns of ICSE Technical Papers and Associated Artifacts","abstract":"Context: Annually, ICSE acknowledges a range of papers, a subset of which are paired with research artifacts such as source code, datasets, and supplementary materials, adhering to the Open Science Policy. However, no prior systematic inquiry dives into gauging the influence of ICSE papers using artifact attributes. Objective: We explore the mutual impact between artifacts and their associated papers presented at ICSE over ten years.   Method: We collect data on usage attributes from papers and their artifacts, conduct a statistical assessment to identify differences, and analyze the top five papers in each attribute category.   Results: There is a significant difference between paper citations and the usage of associated artifacts. While statistical analyses show no notable difference between paper citations and GitHub stars, variations exist in views and/or downloads of papers and artifacts.   Conclusion: We provide a thorough overview of ICSE's accepted papers from the last decade, emphasizing the intricate relationship between research papers and their artifacts. To enhance the assessment of artifact influence in software research, we recommend considering key attributes that may be present in one platform but not in another.","sentences":["Context: Annually, ICSE acknowledges a range of papers, a subset of which are paired with research artifacts such as source code, datasets, and supplementary materials, adhering to the Open Science Policy.","However, no prior systematic inquiry dives into gauging the influence of ICSE papers using artifact attributes.","Objective: We explore the mutual impact between artifacts and their associated papers presented at ICSE over ten years.   ","Method: We collect data on usage attributes from papers and their artifacts, conduct a statistical assessment to identify differences, and analyze the top five papers in each attribute category.   ","Results: There is a significant difference between paper citations and the usage of associated artifacts.","While statistical analyses show no notable difference between paper citations and GitHub stars, variations exist in views and/or downloads of papers and artifacts.   ","Conclusion: We provide a thorough overview of ICSE's accepted papers from the last decade, emphasizing the intricate relationship between research papers and their artifacts.","To enhance the assessment of artifact influence in software research, we recommend considering key attributes that may be present in one platform but not in another."],"url":"http://arxiv.org/abs/2404.05826v1"}
{"created":"2024-04-08 19:23:04","title":"Exploiting CPU Clock Modulation for Covert Communication Channel","abstract":"Covert channel attacks represent a significant threat to system security, leveraging shared resources to clandestinely transmit information from highly secure systems, thereby violating the system's security policies. These attacks exploit shared resources as communication channels, necessitating resource partitioning and isolation techniques as countermeasures. However, mitigating attacks exploiting modern processors' hardware features to leak information is challenging because successful attacks can conceal the channel's existence. In this paper, we unveil a novel covert channel exploiting the duty cycle modulation feature of modern x86 processors. Specifically, we illustrate how two collaborating processes, a sender and a receiver can manipulate this feature to transmit sensitive information surreptitiously. Our live system implementation demonstrates that this covert channel can achieve a data transfer rate of up to 55.24 bits per second.","sentences":["Covert channel attacks represent a significant threat to system security, leveraging shared resources to clandestinely transmit information from highly secure systems, thereby violating the system's security policies.","These attacks exploit shared resources as communication channels, necessitating resource partitioning and isolation techniques as countermeasures.","However, mitigating attacks exploiting modern processors' hardware features to leak information is challenging because successful attacks can conceal the channel's existence.","In this paper, we unveil a novel covert channel exploiting the duty cycle modulation feature of modern x86 processors.","Specifically, we illustrate how two collaborating processes, a sender and a receiver can manipulate this feature to transmit sensitive information surreptitiously.","Our live system implementation demonstrates that this covert channel can achieve a data transfer rate of up to 55.24 bits per second."],"url":"http://arxiv.org/abs/2404.05823v1"}
{"created":"2024-04-08 18:16:22","title":"Self-Labeling in Multivariate Causality and Quantification for Adaptive Machine Learning","abstract":"Adaptive machine learning (ML) aims to allow ML models to adapt to ever-changing environments with potential concept drift after model deployment. Traditionally, adaptive ML requires a new dataset to be manually labeled to tailor deployed models to altered data distributions. Recently, an interactive causality based self-labeling method was proposed to autonomously associate causally related data streams for domain adaptation, showing promising results compared to traditional feature similarity-based semi-supervised learning. Several unanswered research questions remain, including self-labeling's compatibility with multivariate causality and the quantitative analysis of the auxiliary models used in the self-labeling. The auxiliary models, the interaction time model (ITM) and the effect state detector (ESD), are vital to the success of self-labeling. This paper further develops the self-labeling framework and its theoretical foundations to address these research questions. A framework for the application of self-labeling to multivariate causal graphs is proposed using four basic causal relationships, and the impact of non-ideal ITM and ESD performance is analyzed. A simulated experiment is conducted based on a multivariate causal graph, validating the proposed theory.","sentences":["Adaptive machine learning (ML) aims to allow ML models to adapt to ever-changing environments with potential concept drift after model deployment.","Traditionally, adaptive ML requires a new dataset to be manually labeled to tailor deployed models to altered data distributions.","Recently, an interactive causality based self-labeling method was proposed to autonomously associate causally related data streams for domain adaptation, showing promising results compared to traditional feature similarity-based semi-supervised learning.","Several unanswered research questions remain, including self-labeling's compatibility with multivariate causality and the quantitative analysis of the auxiliary models used in the self-labeling.","The auxiliary models, the interaction time model (ITM) and the effect state detector (ESD), are vital to the success of self-labeling.","This paper further develops the self-labeling framework and its theoretical foundations to address these research questions.","A framework for the application of self-labeling to multivariate causal graphs is proposed using four basic causal relationships, and the impact of non-ideal ITM and ESD performance is analyzed.","A simulated experiment is conducted based on a multivariate causal graph, validating the proposed theory."],"url":"http://arxiv.org/abs/2404.05809v1"}
{"created":"2024-04-08 18:05:24","title":"BatSort: Enhanced Battery Classification with Transfer Learning for Battery Sorting and Recycling","abstract":"Battery recycling is a critical process for minimizing environmental harm and resource waste for used batteries. However, it is challenging, largely because sorting batteries is costly and hardly automated to group batteries based on battery types. In this paper, we introduce a machine learning-based approach for battery-type classification and address the daunting problem of data scarcity for the application. We propose BatSort which applies transfer learning to utilize the existing knowledge optimized with large-scale datasets and customizes ResNet to be specialized for classifying battery types. We collected our in-house battery-type dataset of small-scale to guide the knowledge transfer as a case study and evaluate the system performance. We conducted an experimental study and the results show that BatSort can achieve outstanding accuracy of 92.1% on average and up to 96.2% and the performance is stable for battery-type classification. Our solution helps realize fast and automated battery sorting with minimized cost and can be transferred to related industry applications with insufficient data.","sentences":["Battery recycling is a critical process for minimizing environmental harm and resource waste for used batteries.","However, it is challenging, largely because sorting batteries is costly and hardly automated to group batteries based on battery types.","In this paper, we introduce a machine learning-based approach for battery-type classification and address the daunting problem of data scarcity for the application.","We propose BatSort which applies transfer learning to utilize the existing knowledge optimized with large-scale datasets and customizes ResNet to be specialized for classifying battery types.","We collected our in-house battery-type dataset of small-scale to guide the knowledge transfer as a case study and evaluate the system performance.","We conducted an experimental study and the results show that BatSort can achieve outstanding accuracy of 92.1% on average and up to 96.2% and the performance is stable for battery-type classification.","Our solution helps realize fast and automated battery sorting with minimized cost and can be transferred to related industry applications with insufficient data."],"url":"http://arxiv.org/abs/2404.05802v1"}
{"created":"2024-04-08 17:58:22","title":"Predicting Overtakes in Trucks Using CAN Data","abstract":"Safe overtakes in trucks are crucial to prevent accidents, reduce congestion, and ensure efficient traffic flow, making early prediction essential for timely and informed driving decisions. Accordingly, we investigate the detection of truck overtakes from CAN data. Three classifiers, Artificial Neural Networks (ANN), Random Forest, and Support Vector Machines (SVM), are employed for the task. Our analysis covers up to 10 seconds before the overtaking event, using an overlapping sliding window of 1 second to extract CAN features. We observe that the prediction scores of the overtake class tend to increase as we approach the overtake trigger, while the no-overtake class remain stable or oscillates depending on the classifier. Thus, the best accuracy is achieved when approaching the trigger, making early overtaking prediction challenging. The classifiers show good accuracy in classifying overtakes (Recall/TPR > 93%), but accuracy is suboptimal in classifying no-overtakes (TNR typically 80-90% and below 60% for one SVM variant). We further combine two classifiers (Random Forest and linear SVM) by averaging their output scores. The fusion is observed to improve no-overtake classification (TNR > 92%) at the expense of reducing overtake accuracy (TPR). However, the latter is kept above 91% near the overtake trigger. Therefore, the fusion balances TPR and TNR, providing more consistent performance than individual classifiers.","sentences":["Safe overtakes in trucks are crucial to prevent accidents, reduce congestion, and ensure efficient traffic flow, making early prediction essential for timely and informed driving decisions.","Accordingly, we investigate the detection of truck overtakes from CAN data.","Three classifiers, Artificial Neural Networks (ANN), Random Forest, and Support Vector Machines (SVM), are employed for the task.","Our analysis covers up to 10 seconds before the overtaking event, using an overlapping sliding window of 1 second to extract CAN features.","We observe that the prediction scores of the overtake class tend to increase as we approach the overtake trigger, while the no-overtake class remain stable or oscillates depending on the classifier.","Thus, the best accuracy is achieved when approaching the trigger, making early overtaking prediction challenging.","The classifiers show good accuracy in classifying overtakes (Recall/TPR > 93%), but accuracy is suboptimal in classifying no-overtakes (TNR typically 80-90% and below 60% for one SVM variant).","We further combine two classifiers (Random Forest and linear SVM) by averaging their output scores.","The fusion is observed to improve no-overtake classification (TNR > 92%) at the expense of reducing overtake accuracy (TPR).","However, the latter is kept above 91% near the overtake trigger.","Therefore, the fusion balances TPR and TNR, providing more consistent performance than individual classifiers."],"url":"http://arxiv.org/abs/2404.05723v1"}
{"created":"2024-04-08 17:53:21","title":"Responsible Generative AI: What to Generate and What Not","abstract":"In recent years, generative AI (GenAI), like large language models and text-to-image models, has received significant attention across various domains. However, ensuring the responsible generation of content by these models is crucial for their real-world applicability. This raises an interesting question: \\textit{What should responsible GenAI generate, and what should it not?} To answer the question, this paper investigates the practical responsible requirements of both textual and visual generative models, outlining five key considerations: generating truthful content, avoiding toxic content, refusing harmful instruction, leaking no training data-related content, and ensuring generated content identifiable. Specifically, we review recent advancements and challenges in addressing these requirements. Besides, we discuss and emphasize the importance of responsible GenAI across healthcare, education, finance, and artificial general intelligence domains. Through a unified perspective on both textual and visual generative models, this paper aims to provide insights into practical safety-related issues and further benefit the community in building responsible GenAI.","sentences":["In recent years, generative AI (GenAI), like large language models and text-to-image models, has received significant attention across various domains.","However, ensuring the responsible generation of content by these models is crucial for their real-world applicability.","This raises an interesting question: \\textit{What should responsible GenAI generate, and what should it not?} To answer the question, this paper investigates the practical responsible requirements of both textual and visual generative models, outlining five key considerations: generating truthful content, avoiding toxic content, refusing harmful instruction, leaking no training data-related content, and ensuring generated content identifiable.","Specifically, we review recent advancements and challenges in addressing these requirements.","Besides, we discuss and emphasize the importance of responsible GenAI across healthcare, education, finance, and artificial general intelligence domains.","Through a unified perspective on both textual and visual generative models, this paper aims to provide insights into practical safety-related issues and further benefit the community in building responsible GenAI."],"url":"http://arxiv.org/abs/2404.05783v1"}
