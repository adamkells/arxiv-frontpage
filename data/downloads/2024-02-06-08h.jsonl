{"created":"2024-02-05 18:59:52","title":"Test-Time Adaptation for Depth Completion","abstract":"It is common to observe performance degradation when transferring models trained on some (source) datasets to target testing data due to a domain gap between them. Existing methods for bridging this gap, such as domain adaptation (DA), may require the source data on which the model was trained (often not available), while others, i.e., source-free DA, require many passes through the testing data. We propose an online test-time adaptation method for depth completion, the task of inferring a dense depth map from a single image and associated sparse depth map, that closes the performance gap in a single pass. We first present a study on how the domain shift in each data modality affects model performance. Based on our observations that the sparse depth modality exhibits a much smaller covariate shift than the image, we design an embedding module trained in the source domain that preserves a mapping from features encoding only sparse depth to those encoding image and sparse depth. During test time, sparse depth features are projected using this map as a proxy for source domain features and are used as guidance to train a set of auxiliary parameters (i.e., adaptation layer) to align image and sparse depth features from the target test domain to that of the source domain. We evaluate our method on indoor and outdoor scenarios and show that it improves over baselines by an average of 21.1%.","sentences":["It is common to observe performance degradation when transferring models trained on some (source) datasets to target testing data due to a domain gap between them.","Existing methods for bridging this gap, such as domain adaptation (DA), may require the source data on which the model was trained (often not available), while others, i.e., source-free DA, require many passes through the testing data.","We propose an online test-time adaptation method for depth completion, the task of inferring a dense depth map from a single image and associated sparse depth map, that closes the performance gap in a single pass.","We first present a study on how the domain shift in each data modality affects model performance.","Based on our observations that the sparse depth modality exhibits a much smaller covariate shift than the image, we design an embedding module trained in the source domain that preserves a mapping from features encoding only sparse depth to those encoding image and sparse depth.","During test time, sparse depth features are projected using this map as a proxy for source domain features and are used as guidance to train a set of auxiliary parameters (i.e., adaptation layer) to align image and sparse depth features from the target test domain to that of the source domain.","We evaluate our method on indoor and outdoor scenarios and show that it improves over baselines by an average of 21.1%."],"url":"http://arxiv.org/abs/2402.03312v1"}
{"created":"2024-02-05 18:59:36","title":"V-IRL: Grounding Virtual Intelligence in Real Life","abstract":"There is a sensory gulf between the Earth that humans inhabit and the digital realms in which modern AI agents are created. To develop AI agents that can sense, think, and act as flexibly as humans in real-world settings, it is imperative to bridge the realism gap between the digital and physical worlds. How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control? Towards this end, we introduce V-IRL: a platform that enables agents to scalably interact with the real world in a virtual yet realistic environment. Our platform serves as a playground for developing agents that can accomplish various practical tasks and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data across the entire globe.","sentences":["There is a sensory gulf between the Earth that humans inhabit and the digital realms in which modern AI agents are created.","To develop AI agents that can sense, think, and act as flexibly as humans in real-world settings, it is imperative to bridge the realism gap between the digital and physical worlds.","How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control?","Towards this end, we introduce V-IRL: a platform that enables agents to scalably interact with the real world in a virtual yet realistic environment.","Our platform serves as a playground for developing agents that can accomplish various practical tasks and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data across the entire globe."],"url":"http://arxiv.org/abs/2402.03310v1"}
{"created":"2024-02-05 18:58:38","title":"Do Diffusion Models Learn Semantically Meaningful and Efficient Representations?","abstract":"Diffusion models are capable of impressive feats of image generation with uncommon juxtapositions such as astronauts riding horses on the moon with properly placed shadows. These outputs indicate the ability to perform compositional generalization, but how do the models do so? We perform controlled experiments on conditional DDPMs learning to generate 2D spherical Gaussian bumps centered at specified $x$- and $y$-positions. Our results show that the emergence of semantically meaningful latent representations is key to achieving high performance. En route to successful performance over learning, the model traverses three distinct phases of latent representations: (phase A) no latent structure, (phase B) a 2D manifold of disordered states, and (phase C) a 2D ordered manifold. Corresponding to each of these phases, we identify qualitatively different generation behaviors: 1) multiple bumps are generated, 2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is generated at the correct $x$ and y location. Furthermore, we show that even under imbalanced datasets where features ($x$- versus $y$-positions) are represented with skewed frequencies, the learning process for $x$ and $y$ is coupled rather than factorized, demonstrating that simple vanilla-flavored diffusion models cannot learn efficient representations in which localization in $x$ and $y$ are factorized into separate 1D tasks. These findings suggest the need for future work to find inductive biases that will push generative models to discover and exploit factorizable independent structures in their inputs, which will be required to vault these models into more data-efficient regimes.","sentences":["Diffusion models are capable of impressive feats of image generation with uncommon juxtapositions such as astronauts riding horses on the moon with properly placed shadows.","These outputs indicate the ability to perform compositional generalization, but how do the models do so?","We perform controlled experiments on conditional DDPMs learning to generate 2D spherical Gaussian bumps centered at specified $x$- and $y$-positions.","Our results show that the emergence of semantically meaningful latent representations is key to achieving high performance.","En route to successful performance over learning, the model traverses three distinct phases of latent representations: (phase A) no latent structure, (phase B) a 2D manifold of disordered states, and (phase C) a 2D ordered manifold.","Corresponding to each of these phases, we identify qualitatively different generation behaviors: 1) multiple bumps are generated, 2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is generated at the correct $x$ and y location.","Furthermore, we show that even under imbalanced datasets where features ($x$- versus $y$-positions) are represented with skewed frequencies, the learning process for $x$ and $y$ is coupled rather than factorized, demonstrating that simple vanilla-flavored diffusion models cannot learn efficient representations in which localization in $x$ and $y$ are factorized into separate 1D tasks.","These findings suggest the need for future work to find inductive biases that will push generative models to discover and exploit factorizable independent structures in their inputs, which will be required to vault these models into more data-efficient regimes."],"url":"http://arxiv.org/abs/2402.03305v1"}
{"created":"2024-02-05 18:58:11","title":"Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining","abstract":"Accurate medical image segmentation demands the integration of multi-scale information, spanning from local features to global dependencies. However, it is challenging for existing methods to model long-range global information, where convolutional neural networks (CNNs) are constrained by their local receptive fields, and vision transformers (ViTs) suffer from high quadratic complexity of their attention mechanism. Recently, Mamba-based models have gained great attention for their impressive ability in long sequence modeling. Several studies have demonstrated that these models can outperform popular vision models in various tasks, offering higher accuracy, lower memory consumption, and less computational burden. However, existing Mamba-based models are mostly trained from scratch and do not explore the power of pretraining, which has been proven to be quite effective for data-efficient medical image analysis. This paper introduces a novel Mamba-based model, Swin-UMamba, designed specifically for medical image segmentation tasks, leveraging the advantages of ImageNet-based pretraining. Our experimental results reveal the vital role of ImageNet-based training in enhancing the performance of Mamba-based models. Swin-UMamba demonstrates superior performance with a large margin compared to CNNs, ViTs, and latest Mamba-based models. Notably, on AbdomenMRI, Encoscopy, and Microscopy datasets, Swin-UMamba outperforms its closest counterpart U-Mamba by an average score of 3.58%. The code and models of Swin-UMamba are publicly available at: https://github.com/JiarunLiu/Swin-UMamba","sentences":["Accurate medical image segmentation demands the integration of multi-scale information, spanning from local features to global dependencies.","However, it is challenging for existing methods to model long-range global information, where convolutional neural networks (CNNs) are constrained by their local receptive fields, and vision transformers (ViTs) suffer from high quadratic complexity of their attention mechanism.","Recently, Mamba-based models have gained great attention for their impressive ability in long sequence modeling.","Several studies have demonstrated that these models can outperform popular vision models in various tasks, offering higher accuracy, lower memory consumption, and less computational burden.","However, existing Mamba-based models are mostly trained from scratch and do not explore the power of pretraining, which has been proven to be quite effective for data-efficient medical image analysis.","This paper introduces a novel Mamba-based model, Swin-UMamba, designed specifically for medical image segmentation tasks, leveraging the advantages of ImageNet-based pretraining.","Our experimental results reveal the vital role of ImageNet-based training in enhancing the performance of Mamba-based models.","Swin-UMamba demonstrates superior performance with a large margin compared to CNNs, ViTs, and latest Mamba-based models.","Notably, on AbdomenMRI, Encoscopy, and Microscopy datasets, Swin-UMamba outperforms its closest counterpart U-Mamba by an average score of 3.58%.","The code and models of Swin-UMamba are publicly available at: https://github.com/JiarunLiu/Swin-UMamba"],"url":"http://arxiv.org/abs/2402.03302v1"}
{"created":"2024-02-05 18:55:32","title":"DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models","abstract":"Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.","sentences":["Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature.","In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data.","DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4.","Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH.","The mathematical reasoning capability of DeepSeekMath is attributed to two key factors:","First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline.","Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO."],"url":"http://arxiv.org/abs/2402.03300v1"}
{"created":"2024-02-05 18:50:27","title":"Zero-shot Object-Level OOD Detection with Context-Aware Inpainting","abstract":"Machine learning algorithms are increasingly provided as black-box cloud services or pre-trained models, without access to their training data. This motivates the problem of zero-shot out-of-distribution (OOD) detection. Concretely, we aim to detect OOD objects that do not belong to the classifier's label set but are erroneously classified as in-distribution (ID) objects. Our approach, RONIN, uses an off-the-shelf diffusion model to replace detected objects with inpainting. RONIN conditions the inpainting process with the predicted ID label, drawing the input object closer to the in-distribution domain. As a result, the reconstructed object is very close to the original in the ID cases and far in the OOD cases, allowing RONIN to effectively distinguish ID and OOD samples. Throughout extensive experiments, we demonstrate that RONIN achieves competitive results compared to previous approaches across several datasets, both in zero-shot and non-zero-shot settings.","sentences":["Machine learning algorithms are increasingly provided as black-box cloud services or pre-trained models, without access to their training data.","This motivates the problem of zero-shot out-of-distribution (OOD) detection.","Concretely, we aim to detect OOD objects that do not belong to the classifier's label set but are erroneously classified as in-distribution (ID) objects.","Our approach, RONIN, uses an off-the-shelf diffusion model to replace detected objects with inpainting.","RONIN conditions the inpainting process with the predicted ID label, drawing the input object closer to the in-distribution domain.","As a result, the reconstructed object is very close to the original in the ID cases and far in the OOD cases, allowing RONIN to effectively distinguish ID and OOD samples.","Throughout extensive experiments, we demonstrate that RONIN achieves competitive results compared to previous approaches across several datasets, both in zero-shot and non-zero-shot settings."],"url":"http://arxiv.org/abs/2402.03292v1"}
{"created":"2024-02-05 18:49:55","title":"Knowledge Acquisition and Integration with Expert-in-the-loop","abstract":"Constructing and serving knowledge graphs (KGs) is an iterative and human-centered process involving on-demand programming and analysis. In this paper, we present Kyurem, a programmable and interactive widget library that facilitates human-in-the-loop knowledge acquisition and integration to enable continuous curation a knowledge graph (KG). Kyurem provides a seamless environment within computational notebooks where data scientists explore a KG to identify opportunities for acquiring new knowledge and verify recommendations provided by AI agents for integrating the acquired knowledge in the KG. We refined Kyurem through participatory design and conducted case studies in a real-world setting for evaluation. The case-studies show that introduction of Kyurem within an existing HR knowledge graph construction and serving platform improved the user experience of the experts and helped eradicate inefficiencies related to knowledge acquisition and integration tasks","sentences":["Constructing and serving knowledge graphs (KGs) is an iterative and human-centered process involving on-demand programming and analysis.","In this paper, we present Kyurem, a programmable and interactive widget library that facilitates human-in-the-loop knowledge acquisition and integration to enable continuous curation a knowledge graph (KG).","Kyurem provides a seamless environment within computational notebooks where data scientists explore a KG to identify opportunities for acquiring new knowledge and verify recommendations provided by AI agents for integrating the acquired knowledge in the KG.","We refined Kyurem through participatory design and conducted case studies in a real-world setting for evaluation.","The case-studies show that introduction of Kyurem within an existing HR knowledge graph construction and serving platform improved the user experience of the experts and helped eradicate inefficiencies related to knowledge acquisition and integration tasks"],"url":"http://arxiv.org/abs/2402.03291v1"}
{"created":"2024-02-05 18:39:04","title":"Towards a Flexible Scale-out Framework for Efficient Visual Data Query Processing","abstract":"There is growing interest in visual data management systems that support queries with specialized operations ranging from resizing an image to running complex machine learning models. With a plethora of such operations, the basic need to receive query responses in minimal time takes a hit, especially when the client desires to run multiple such operations in a single query. Existing systems provide an ad-hoc approach where different solutions are clubbed together to provide an end-to-end visual data management system. Unlike such solutions, the Visual Data Management System (VDMS) natively executes queries with multiple operations, thus providing an end-to-end solution. However, a fixed subset of native operations and a synchronous threading architecture limit its generality and scalability.   In this paper, we develop VDMS-Async that adds the capability to run user-defined operations with VDMS and execute operations within a query on a remote server. VDMS-Async utilizes an event-driven architecture to create an efficient pipeline for executing operations within a query. Our experiments have shown that VDMS-Async reduces the query execution time by 2-3X compared to existing state-of-the-art systems. Further, remote operations coupled with an event-driven architecture enables VDMS-Async to scale query execution time linearly with the addition of every new remote server. We demonstrate a 64X reduction in query execution time when adding 64 remote servers.","sentences":["There is growing interest in visual data management systems that support queries with specialized operations ranging from resizing an image to running complex machine learning models.","With a plethora of such operations, the basic need to receive query responses in minimal time takes a hit, especially when the client desires to run multiple such operations in a single query.","Existing systems provide an ad-hoc approach where different solutions are clubbed together to provide an end-to-end visual data management system.","Unlike such solutions, the Visual Data Management System (VDMS) natively executes queries with multiple operations, thus providing an end-to-end solution.","However, a fixed subset of native operations and a synchronous threading architecture limit its generality and scalability.   ","In this paper, we develop VDMS-Async that adds the capability to run user-defined operations with VDMS and execute operations within a query on a remote server.","VDMS-Async utilizes an event-driven architecture to create an efficient pipeline for executing operations within a query.","Our experiments have shown that VDMS-Async reduces the query execution time by 2-3X compared to existing state-of-the-art systems.","Further, remote operations coupled with an event-driven architecture enables VDMS-Async to scale query execution time linearly with the addition of every new remote server.","We demonstrate a 64X reduction in query execution time when adding 64 remote servers."],"url":"http://arxiv.org/abs/2402.03283v1"}
{"created":"2024-02-05 18:35:16","title":"Event-based Product Carousel Recommendation with Query-Click Graph","abstract":"Many current recommender systems mainly focus on the product-to-product recommendations and user-to-product recommendations even during the time of events rather than modeling the typical recommendations for the target event (e.g., festivals, seasonal activities, or social activities) without addressing the multiple aspects of the shopping demands for the target event. Product recommendations for the multiple aspects of the target event are usually generated by human curators who manually identify the aspects and select a list of aspect-related products (i.e., product carousel) for each aspect as recommendations. However, building a recommender system with machine learning is non-trivial due to the lack of both the ground truth of event-related aspects and the aspect-related products. To fill this gap, we define the novel problem as the event-based product carousel recommendations in e-commerce and propose an effective recommender system based on the query-click bipartite graph. We apply the iterative clustering algorithm over the query-click bipartite graph and infer the event-related aspects by the clusters of queries. The aspect-related recommendations are powered by the click-through rate of products regarding each aspect. We show through experiments that this approach effectively mines product carousels for the target event.","sentences":["Many current recommender systems mainly focus on the product-to-product recommendations and user-to-product recommendations even during the time of events rather than modeling the typical recommendations for the target event (e.g., festivals, seasonal activities, or social activities) without addressing the multiple aspects of the shopping demands for the target event.","Product recommendations for the multiple aspects of the target event are usually generated by human curators who manually identify the aspects and select a list of aspect-related products (i.e., product carousel) for each aspect as recommendations.","However, building a recommender system with machine learning is non-trivial due to the lack of both the ground truth of event-related aspects and the aspect-related products.","To fill this gap, we define the novel problem as the event-based product carousel recommendations in e-commerce and propose an effective recommender system based on the query-click bipartite graph.","We apply the iterative clustering algorithm over the query-click bipartite graph and infer the event-related aspects by the clusters of queries.","The aspect-related recommendations are powered by the click-through rate of products regarding each aspect.","We show through experiments that this approach effectively mines product carousels for the target event."],"url":"http://arxiv.org/abs/2402.03277v1"}
{"created":"2024-02-05 18:31:00","title":"Algorithms and Complexity of Difference Logic","abstract":"Difference Logic (DL) is a fragment of linear arithmetics where atoms are constraints x+k <= y for variables x,y (ranging over Q or Z) and integer k. We study the complexity of deciding the truth of existential DL sentences. This problem appears in many contexts: examples include verification, bioinformatics, telecommunications, and spatio-temporal reasoning in AI. We begin by considering sentences in CNF with rational-valued variables. We restrict the allowed clauses via two natural parameters: arity and coefficient bounds. The problem is NP-hard for most choices of these parameters. As a response to this, we refine our understanding by analyzing the time complexity and the parameterized complexity (with respect to well-studied parameters such as primal and incidence treewidth). We obtain a comprehensive picture of the complexity landscape in both cases. Finally, we generalize our results to integer domains and sentences that are not in CNF.","sentences":["Difference Logic (DL) is a fragment of linear arithmetics where atoms are constraints x+k <= y for variables x,y (ranging over Q or Z) and","integer k. We study the complexity of deciding the truth of existential DL sentences.","This problem appears in many contexts: examples include verification, bioinformatics, telecommunications, and spatio-temporal reasoning in AI.","We begin by considering sentences in CNF with rational-valued variables.","We restrict the allowed clauses via two natural parameters: arity and coefficient bounds.","The problem is NP-hard for most choices of these parameters.","As a response to this, we refine our understanding by analyzing the time complexity and the parameterized complexity (with respect to well-studied parameters such as primal and incidence treewidth).","We obtain a comprehensive picture of the complexity landscape in both cases.","Finally, we generalize our results to integer domains and sentences that are not in CNF."],"url":"http://arxiv.org/abs/2402.03273v1"}
{"created":"2024-02-05 18:22:21","title":"MobilityGPT: Enhanced Human Mobility Modeling with a GPT model","abstract":"Generative models have shown promising results in capturing human mobility characteristics and generating synthetic trajectories. However, it remains challenging to ensure that the generated geospatial mobility data is semantically realistic, including consistent location sequences, and reflects real-world characteristics, such as constraining on geospatial limits. To address these issues, we reformat human mobility modeling as an autoregressive generation task, leveraging Generative Pre-trained Transformer (GPT). To ensure its controllable generation to alleviate the above challenges, we propose a geospatially-aware generative model, MobilityGPT. We propose a gravity-based sampling method to train a transformer for semantic sequence similarity. Then, we constrained the training process via a road connectivity matrix that provides the connectivity of sequences in trajectory generation, thereby keeping generated trajectories in geospatial limits. Lastly, we constructed a Reinforcement Learning from Trajectory Feedback (RLTF) to minimize the travel distance between training and the synthetically generated trajectories. Our experiments on real-world datasets demonstrate that MobilityGPT outperforms state-of-the-art methods in generating high-quality mobility trajectories that are closest to real data in terms of origin-destination similarity, trip length, travel radius, link, and gravity distributions.","sentences":["Generative models have shown promising results in capturing human mobility characteristics and generating synthetic trajectories.","However, it remains challenging to ensure that the generated geospatial mobility data is semantically realistic, including consistent location sequences, and reflects real-world characteristics, such as constraining on geospatial limits.","To address these issues, we reformat human mobility modeling as an autoregressive generation task, leveraging Generative Pre-trained Transformer (GPT).","To ensure its controllable generation to alleviate the above challenges, we propose a geospatially-aware generative model, MobilityGPT.","We propose a gravity-based sampling method to train a transformer for semantic sequence similarity.","Then, we constrained the training process via a road connectivity matrix that provides the connectivity of sequences in trajectory generation, thereby keeping generated trajectories in geospatial limits.","Lastly, we constructed a Reinforcement Learning from Trajectory Feedback (RLTF) to minimize the travel distance between training and the synthetically generated trajectories.","Our experiments on real-world datasets demonstrate that MobilityGPT outperforms state-of-the-art methods in generating high-quality mobility trajectories that are closest to real data in terms of origin-destination similarity, trip length, travel radius, link, and gravity distributions."],"url":"http://arxiv.org/abs/2402.03264v1"}
{"created":"2024-02-05 18:15:50","title":"Freeze-Tag in $L_1$ has Wake-up Time Five","abstract":"The Freeze-Tag Problem, introduced in Arkin et al. (SODA'02) consists of waking up a swarm of $n$ robots, starting from a single active robot. In the basic geometric version, every robot is given coordinates in the plane. As soon as a robot is awakened, it can move towards inactive robots to wake them up. The goal is to minimize the wake-up time of the last robot, the makespan.   Despite significant progress on the computational complexity of this problem and on approximation algorithms, the characterization of exact bounds on the makespan remains one of the main open questions. In this paper, we settle this question for the $\\ell_1$-norm, showing that a makespan of at most $5r$ can always be achieved, where $r$ is the maximum distance between the initial active robot and any sleeping robot. Moreover, a schedule achieving a makespan of at most $5r$ can be computed in optimal time $O(n)$. Both bounds, the time and the makespan are optimal. This implies a new upper bound of $5\\sqrt{2}r \\approx 7.07r$ on the makespan in the $\\ell_2$-norm, improving the best known bound so far $(5+2\\sqrt{2}+\\sqrt{5})r \\approx 10.06r$.","sentences":["The Freeze-Tag Problem, introduced in Arkin et al.","(SODA'02) consists of waking up a swarm of $n$ robots, starting from a single active robot.","In the basic geometric version, every robot is given coordinates in the plane.","As soon as a robot is awakened, it can move towards inactive robots to wake them up.","The goal is to minimize the wake-up time of the last robot, the makespan.   ","Despite significant progress on the computational complexity of this problem and on approximation algorithms, the characterization of exact bounds on the makespan remains one of the main open questions.","In this paper, we settle this question for the $\\ell_1$-norm, showing that a makespan of at most $5r$ can always be achieved, where $r$ is the maximum distance between the initial active robot and any sleeping robot.","Moreover, a schedule achieving a makespan of at most $5r$ can be computed in optimal time $O(n)$. Both bounds, the time and the makespan are optimal.","This implies a new upper bound of $5\\sqrt{2}r \\approx 7.07r$ on the makespan in the $\\ell_2$-norm, improving the best known bound so far $(5+2\\sqrt{2}+\\sqrt{5})r \\approx 10.06r$."],"url":"http://arxiv.org/abs/2402.03258v1"}
{"created":"2024-02-05 18:09:33","title":"CLIP Can Understand Depth","abstract":"Recent studies on generalizing CLIP for monocular depth estimation reveal that CLIP pre-trained on web-crawled data is inefficient for deriving proper similarities between image patches and depth-related prompts. In this paper, we adapt CLIP for meaningful quality of monocular depth estimation with dense prediction, without fine-tuning its original vision-language alignment. By jointly training a compact deconvolutional decoder with a tiny learnable embedding matrix named mirror, as a static prompt for its text encoder, CLIP is enabled to understand depth. With this approach, our model exhibits impressive performance matching several previous state-of-the-art vision-only models on the NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depth estimation model with a large margin. Experiments on temporal depth consistency and spatial continuity demonstrate that the prior knowledge of CLIP can be effectively refined by our proposed framework. Furthermore, an ablation study on mirror proves that the resulting model estimates depth utilizing knowledge not only from the image encoder but also text encoder despite not being given any prompt written in a human way. This research demonstrates that through minimal adjustments, the prior knowledge of vision-language foundation models, such as CLIP, can be generalized even to domains where learning during pretraining is challenging. We facilitate future works focused on methods to adjust suboptimal prior knowledge of vision-language models using non-human language prompts, achieving performance on par with task-specific state-of-the-art methodologies.","sentences":["Recent studies on generalizing CLIP for monocular depth estimation reveal that CLIP pre-trained on web-crawled data is inefficient for deriving proper similarities between image patches and depth-related prompts.","In this paper, we adapt CLIP for meaningful quality of monocular depth estimation with dense prediction, without fine-tuning its original vision-language alignment.","By jointly training a compact deconvolutional decoder with a tiny learnable embedding matrix named mirror, as a static prompt for its text encoder, CLIP is enabled to understand depth.","With this approach, our model exhibits impressive performance matching several previous state-of-the-art vision-only models on the NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depth estimation model with a large margin.","Experiments on temporal depth consistency and spatial continuity demonstrate that the prior knowledge of CLIP can be effectively refined by our proposed framework.","Furthermore, an ablation study on mirror proves that the resulting model estimates depth utilizing knowledge not only from the image encoder but also text encoder despite not being given any prompt written in a human way.","This research demonstrates that through minimal adjustments, the prior knowledge of vision-language foundation models, such as CLIP, can be generalized even to domains where learning during pretraining is challenging.","We facilitate future works focused on methods to adjust suboptimal prior knowledge of vision-language models using non-human language prompts, achieving performance on par with task-specific state-of-the-art methodologies."],"url":"http://arxiv.org/abs/2402.03251v1"}
{"created":"2024-02-05 17:57:26","title":"JOBSKAPE: A Framework for Generating Synthetic Job Postings to Enhance Skill Matching","abstract":"Recent approaches in skill matching, employing synthetic training data for classification or similarity model training, have shown promising results, reducing the need for time-consuming and expensive annotations. However, previous synthetic datasets have limitations, such as featuring only one skill per sentence and generally comprising short sentences. In this paper, we introduce JobSkape, a framework to generate synthetic data that tackles these limitations, specifically designed to enhance skill-to-taxonomy matching. Within this framework, we create SkillSkape, a comprehensive open-source synthetic dataset of job postings tailored for skill-matching tasks. We introduce several offline metrics that show that our dataset resembles real-world data. Additionally, we present a multi-step pipeline for skill extraction and matching tasks using large language models (LLMs), benchmarking against known supervised methodologies. We outline that the downstream evaluation results on real-world data can beat baselines, underscoring its efficacy and adaptability.","sentences":["Recent approaches in skill matching, employing synthetic training data for classification or similarity model training, have shown promising results, reducing the need for time-consuming and expensive annotations.","However, previous synthetic datasets have limitations, such as featuring only one skill per sentence and generally comprising short sentences.","In this paper, we introduce JobSkape, a framework to generate synthetic data that tackles these limitations, specifically designed to enhance skill-to-taxonomy matching.","Within this framework, we create SkillSkape, a comprehensive open-source synthetic dataset of job postings tailored for skill-matching tasks.","We introduce several offline metrics that show that our dataset resembles real-world data.","Additionally, we present a multi-step pipeline for skill extraction and matching tasks using large language models (LLMs), benchmarking against known supervised methodologies.","We outline that the downstream evaluation results on real-world data can beat baselines, underscoring its efficacy and adaptability."],"url":"http://arxiv.org/abs/2402.03242v1"}
{"created":"2024-02-05 17:52:58","title":"ActiveAnno3D -- An Active Learning Framework for Multi-Modal 3D Object Detection","abstract":"The curation of large-scale datasets is still costly and requires much time and resources. Data is often manually labeled, and the challenge of creating high-quality datasets remains. In this work, we fill the research gap using active learning for multi-modal 3D object detection. We propose ActiveAnno3D, an active learning framework to select data samples for labeling that are of maximum informativeness for training. We explore various continuous training methods and integrate the most efficient method regarding computational demand and detection performance. Furthermore, we perform extensive experiments and ablation studies with BEVFusion and PV-RCNN on the nuScenes and TUM Traffic Intersection dataset. We show that we can achieve almost the same performance with PV-RCNN and the entropy-based query strategy when using only half of the training data (77.25 mAP compared to 83.50 mAP) of the TUM Traffic Intersection dataset. BEVFusion achieved an mAP of 64.31 when using half of the training data and 75.0 mAP when using the complete nuScenes dataset. We integrate our active learning framework into the proAnno labeling tool to enable AI-assisted data selection and labeling and minimize the labeling costs. Finally, we provide code, weights, and visualization results on our website: https://active3d-framework.github.io/active3d-framework.","sentences":["The curation of large-scale datasets is still costly and requires much time and resources.","Data is often manually labeled, and the challenge of creating high-quality datasets remains.","In this work, we fill the research gap using active learning for multi-modal 3D object detection.","We propose ActiveAnno3D, an active learning framework to select data samples for labeling that are of maximum informativeness for training.","We explore various continuous training methods and integrate the most efficient method regarding computational demand and detection performance.","Furthermore, we perform extensive experiments and ablation studies with BEVFusion and PV-RCNN on the nuScenes and TUM Traffic Intersection dataset.","We show that we can achieve almost the same performance with PV-RCNN and the entropy-based query strategy when using only half of the training data (77.25 mAP compared to 83.50 mAP) of the TUM Traffic Intersection dataset.","BEVFusion achieved an mAP of 64.31 when using half of the training data and 75.0 mAP when using the complete nuScenes dataset.","We integrate our active learning framework into the proAnno labeling tool to enable AI-assisted data selection and labeling and minimize the labeling costs.","Finally, we provide code, weights, and visualization results on our website: https://active3d-framework.github.io/active3d-framework."],"url":"http://arxiv.org/abs/2402.03235v1"}
{"created":"2024-02-05 17:45:12","title":"Smart Flow Matching: On The Theory of Flow Matching Algorithms with Applications","abstract":"The paper presents the exact formula for the vector field that minimizes the loss for the standard flow. This formula depends analytically on a given distribution \\rho_0 and an unknown one \\rho_1. Based on the presented formula, a new loss and algorithm for training a vector field model in the style of Conditional Flow Matching are provided. Our loss, in comparison to the standard Conditional Flow Matching approach, exhibits smaller variance when evaluated through Monte Carlo sampling methods. Numerical experiments on synthetic models and models on tabular data of large dimensions demonstrate better learning results with the use of the presented algorithm.","sentences":["The paper presents the exact formula for the vector field that minimizes the loss for the standard flow.","This formula depends analytically on a given distribution \\rho_0 and an unknown one \\rho_1.","Based on the presented formula, a new loss and algorithm for training a vector field model in the style of Conditional Flow Matching are provided.","Our loss, in comparison to the standard Conditional Flow Matching approach, exhibits smaller variance when evaluated through Monte Carlo sampling methods.","Numerical experiments on synthetic models and models on tabular data of large dimensions demonstrate better learning results with the use of the presented algorithm."],"url":"http://arxiv.org/abs/2402.03232v1"}
{"created":"2024-02-05 17:38:49","title":"IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images","abstract":"In MRI studies, the aggregation of imaging data from multiple acquisition sites enhances sample size but may introduce site-related variabilities that hinder consistency in subsequent analyses. Deep learning methods for image translation have emerged as a solution for harmonizing MR images across sites. In this study, we introduce IGUANe (Image Generation with Unified Adversarial Networks), an original 3D model that leverages the strengths of domain translation and straightforward application of style transfer methods for multicenter brain MR image harmonization. IGUANe extends CycleGAN architecture by integrating an arbitrary number of domains for training through a many-to-one strategy. During inference, the model can be applied to any image, even from an unknown acquisition site, making it a universal generator for harmonization. Trained on a dataset comprising T1-weighted images from 11 different scanners, IGUANe was evaluated on data from unseen sites. The assessments included the transformation of MR images with traveling subjects, the preservation of pairwise distances between MR images within domains, the evolution of volumetric patterns related to age and Alzheimer$^\\prime$s disease (AD), and the performance in age regression and patient classification tasks. Comparisons with other harmonization and normalization methods suggest that IGUANe better preserves individual information in MR images and is more suitable for maintaining and reinforcing variabilities related to age and AD. Future studies may further assess IGUANe in other multicenter contexts, either using the same model or retraining it for applications to different image modalities.","sentences":["In MRI studies, the aggregation of imaging data from multiple acquisition sites enhances sample size but may introduce site-related variabilities that hinder consistency in subsequent analyses.","Deep learning methods for image translation have emerged as a solution for harmonizing MR images across sites.","In this study, we introduce IGUANe (Image Generation with Unified Adversarial Networks), an original 3D model that leverages the strengths of domain translation and straightforward application of style transfer methods for multicenter brain MR image harmonization.","IGUANe extends CycleGAN architecture by integrating an arbitrary number of domains for training through a many-to-one strategy.","During inference, the model can be applied to any image, even from an unknown acquisition site, making it a universal generator for harmonization.","Trained on a dataset comprising T1-weighted images from 11 different scanners, IGUANe was evaluated on data from unseen sites.","The assessments included the transformation of MR images with traveling subjects, the preservation of pairwise distances between MR images within domains, the evolution of volumetric patterns related to age and Alzheimer$^\\prime$s disease (AD), and the performance in age regression and patient classification tasks.","Comparisons with other harmonization and normalization methods suggest that IGUANe better preserves individual information in MR images and is more suitable for maintaining and reinforcing variabilities related to age and AD.","Future studies may further assess IGUANe in other multicenter contexts, either using the same model or retraining it for applications to different image modalities."],"url":"http://arxiv.org/abs/2402.03227v1"}
{"created":"2024-02-05 17:37:46","title":"FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion","abstract":"As machine learning models in critical fields increasingly grapple with multimodal data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples. Successfully leveraging this complex data, while overcoming the scarcity of high-quality training samples, is key to improving these models' predictive performance. We introduce ``FuseMoE'', a mixture-of-experts framework incorporated with an innovative gating function. Designed to integrate a diverse number of modalities, FuseMoE is effective in managing scenarios with missing modalities and irregularly sampled data trajectories. Theoretically, our unique gating function contributes to enhanced convergence rates, leading to better performance in multiple downstream tasks. The practical utility of FuseMoE in real world is validated by a challenging set of clinical risk prediction tasks.","sentences":["As machine learning models in critical fields increasingly grapple with multimodal data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples.","Successfully leveraging this complex data, while overcoming the scarcity of high-quality training samples, is key to improving these models' predictive performance.","We introduce ``FuseMoE'', a mixture-of-experts framework incorporated with an innovative gating function.","Designed to integrate a diverse number of modalities, FuseMoE is effective in managing scenarios with missing modalities and irregularly sampled data trajectories.","Theoretically, our unique gating function contributes to enhanced convergence rates, leading to better performance in multiple downstream tasks.","The practical utility of FuseMoE in real world is validated by a challenging set of clinical risk prediction tasks."],"url":"http://arxiv.org/abs/2402.03226v1"}
{"created":"2024-02-05 17:36:19","title":"English Prompts are Better for NLI-based Zero-Shot Emotion Classification than Target-Language Prompts","abstract":"Emotion classification in text is a challenging and subjective task, due to the involved cognitive inference processes that are required to interpret a textual stimulus. In addition, the set of emotion categories is highly domain-specific. For instance, literature analysis might require the use of aesthetic emotions (e.g., finding something beautiful), and social media analysis could benefit from fine-grained sets (e.g., separating anger from annoyance) in contrast to basic emotion categories. This renders the task an interesting field for zero-shot classifications, in which the label set is not known at model development time. Unfortunately, most resources for emotion analysis are English, and therefore, most studies on emotion analysis have been performed in English, including those that involve prompting language models for text labels. This leaves us with a research gap that we address in this paper: In which language should we prompt for emotion labels on non-English texts? This is particularly of interest when we have access to a multilingual large language model, because we could request labels with English prompts even for non-English data. Our experiments with natural language inference-based language models show that it is consistently better to use English prompts even if the data is in a different language.","sentences":["Emotion classification in text is a challenging and subjective task, due to the involved cognitive inference processes that are required to interpret a textual stimulus.","In addition, the set of emotion categories is highly domain-specific.","For instance, literature analysis might require the use of aesthetic emotions (e.g., finding something beautiful), and social media analysis could benefit from fine-grained sets (e.g., separating anger from annoyance) in contrast to basic emotion categories.","This renders the task an interesting field for zero-shot classifications, in which the label set is not known at model development time.","Unfortunately, most resources for emotion analysis are English, and therefore, most studies on emotion analysis have been performed in English, including those that involve prompting language models for text labels.","This leaves us with a research gap that we address in this paper: In which language should we prompt for emotion labels on non-English texts?","This is particularly of interest when we have access to a multilingual large language model, because we could request labels with English prompts even for non-English data.","Our experiments with natural language inference-based language models show that it is consistently better to use English prompts even if the data is in a different language."],"url":"http://arxiv.org/abs/2402.03223v1"}
{"created":"2024-02-05 17:33:22","title":"\"Define Your Terms\" : Enhancing Efficient Offensive Speech Classification with Definition","abstract":"The propagation of offensive content through social media channels has garnered attention of the research community. Multiple works have proposed various semantically related yet subtle distinct categories of offensive speech. In this work, we explore meta-earning approaches to leverage the diversity of offensive speech corpora to enhance their reliable and efficient detection. We propose a joint embedding architecture that incorporates the input's label and definition for classification via Prototypical Network. Our model achieves at least 75% of the maximal F1-score while using less than 10% of the available training data across 4 datasets. Our experimental findings also provide a case study of training strategies valuable to combat resource scarcity.","sentences":["The propagation of offensive content through social media channels has garnered attention of the research community.","Multiple works have proposed various semantically related yet subtle distinct categories of offensive speech.","In this work, we explore meta-earning approaches to leverage the diversity of offensive speech corpora to enhance their reliable and efficient detection.","We propose a joint embedding architecture that incorporates the input's label and definition for classification via Prototypical Network.","Our model achieves at least 75% of the maximal F1-score while using less than 10% of the available training data across 4 datasets.","Our experimental findings also provide a case study of training strategies valuable to combat resource scarcity."],"url":"http://arxiv.org/abs/2402.03221v1"}
{"created":"2024-02-05 17:25:04","title":"Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?","abstract":"The advent of generative AI images has completely disrupted the art world. Identifying AI generated images from human art is a challenging problem whose impact is growing over time. The failure to address this problem allows bad actors to defraud individuals paying a premium for human art, and companies whose stated policies forbid AI imagery. This is also critical for AI model trainers, who need to filter training data to avoid potential model collapse. There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors (5 automated detectors and 3 different human groups including 180 crowdworkers, 4000+ professional artists, and 13 expert artists experienced at detecting AI). Both Hive and expert artists do very well, but make mistakes in different ways (Hive is weaker against adversarial perturbations while Expert artists produce higher false positives). We believe these weaknesses will remain as models continue to evolve, and use our data to demonstrate why a combined team of human and automated detectors provides the best combination of accuracy and robustness.","sentences":["The advent of generative AI images has completely disrupted the art world.","Identifying AI generated images from human art is a challenging problem whose impact is growing over time.","The failure to address this problem allows bad actors to defraud individuals paying a premium for human art, and companies whose stated policies forbid AI imagery.","This is also critical for AI model trainers, who need to filter training data to avoid potential model collapse.","There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques.","In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings.","We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors (5 automated detectors and 3 different human groups including 180 crowdworkers, 4000+ professional artists, and 13 expert artists experienced at detecting AI).","Both Hive and expert artists do very well, but make mistakes in different ways (Hive is weaker against adversarial perturbations while Expert artists produce higher false positives).","We believe these weaknesses will remain as models continue to evolve, and use our data to demonstrate why a combined team of human and automated detectors provides the best combination of accuracy and robustness."],"url":"http://arxiv.org/abs/2402.03214v1"}
{"created":"2024-02-05 17:12:21","title":"Guidance with Spherical Gaussian Constraint for Conditional Diffusion","abstract":"Recent advances in diffusion models attempt to handle conditional generative tasks by utilizing a differentiable loss function for guidance without the need for additional training. While these methods achieved certain success, they often compromise on sample quality and require small guidance step sizes, leading to longer sampling processes. This paper reveals that the fundamental issue lies in the manifold deviation during the sampling process when loss guidance is employed. We theoretically show the existence of manifold deviation by establishing a certain lower bound for the estimation error of the loss guidance. To mitigate this problem, we propose Diffusion with Spherical Gaussian constraint (DSG), drawing inspiration from the concentration phenomenon in high-dimensional Gaussian distributions. DSG effectively constrains the guidance step within the intermediate data manifold through optimization and enables the use of larger guidance steps. Furthermore, we present a closed-form solution for DSG denoising with the Spherical Gaussian constraint. Notably, DSG can seamlessly integrate as a plugin module within existing training-free conditional diffusion methods. Implementing DSG merely involves a few lines of additional code with almost no extra computational overhead, yet it leads to significant performance improvements. Comprehensive experimental results in various conditional generation tasks validate the superiority and adaptability of DSG in terms of both sample quality and time efficiency.","sentences":["Recent advances in diffusion models attempt to handle conditional generative tasks by utilizing a differentiable loss function for guidance without the need for additional training.","While these methods achieved certain success, they often compromise on sample quality and require small guidance step sizes, leading to longer sampling processes.","This paper reveals that the fundamental issue lies in the manifold deviation during the sampling process when loss guidance is employed.","We theoretically show the existence of manifold deviation by establishing a certain lower bound for the estimation error of the loss guidance.","To mitigate this problem, we propose Diffusion with Spherical Gaussian constraint (DSG), drawing inspiration from the concentration phenomenon in high-dimensional Gaussian distributions.","DSG effectively constrains the guidance step within the intermediate data manifold through optimization and enables the use of larger guidance steps.","Furthermore, we present a closed-form solution for DSG denoising with the Spherical Gaussian constraint.","Notably, DSG can seamlessly integrate as a plugin module within existing training-free conditional diffusion methods.","Implementing DSG merely involves a few lines of additional code with almost no extra computational overhead, yet it leads to significant performance improvements.","Comprehensive experimental results in various conditional generation tasks validate the superiority and adaptability of DSG in terms of both sample quality and time efficiency."],"url":"http://arxiv.org/abs/2402.03201v1"}
{"created":"2024-02-05 16:47:13","title":"Predicting Configuration Performance in Multiple Environments with Sequential Meta-learning","abstract":"Learning and predicting the performance of given software configurations are of high importance to many software engineering activities. While configurable software systems will almost certainly face diverse running environments (e.g., version, hardware, and workload), current work often either builds performance models under a single environment or fails to properly handle data from diverse settings, hence restricting their accuracy for new environments. In this paper, we target configuration performance learning under multiple environments. We do so by designing SeMPL - a meta-learning framework that learns the common understanding from configurations measured in distinct (meta) environments and generalizes them to the unforeseen, target environment. What makes it unique is that unlike common meta-learning frameworks (e.g., MAML and MetaSGD) that train the meta environments in parallel, we train them sequentially, one at a time. The order of training naturally allows discriminating the contributions among meta environments in the meta-model built, which fits better with the characteristic of configuration data that is known to dramatically differ between different environments. Through comparing with 15 state-of-the-art models under nine systems, our extensive experimental results demonstrate that SeMPL performs considerably better on 89% of the systems with up to 99% accuracy improvement, while being data-efficient, leading to a maximum of 3.86x speedup. All code and data can be found at our repository: https://github.com/ideas-labo/SeMPL.","sentences":["Learning and predicting the performance of given software configurations are of high importance to many software engineering activities.","While configurable software systems will almost certainly face diverse running environments (e.g., version, hardware, and workload), current work often either builds performance models under a single environment or fails to properly handle data from diverse settings, hence restricting their accuracy for new environments.","In this paper, we target configuration performance learning under multiple environments.","We do so by designing SeMPL - a meta-learning framework that learns the common understanding from configurations measured in distinct (meta) environments and generalizes them to the unforeseen, target environment.","What makes it unique is that unlike common meta-learning frameworks (e.g., MAML and MetaSGD) that train the meta environments in parallel, we train them sequentially, one at a time.","The order of training naturally allows discriminating the contributions among meta environments in the meta-model built, which fits better with the characteristic of configuration data that is known to dramatically differ between different environments.","Through comparing with 15 state-of-the-art models under nine systems, our extensive experimental results demonstrate that SeMPL performs considerably better on 89% of the systems with up to 99% accuracy improvement, while being data-efficient, leading to a maximum of 3.86x speedup.","All code and data can be found at our repository: https://github.com/ideas-labo/SeMPL."],"url":"http://arxiv.org/abs/2402.03183v1"}
{"created":"2024-02-05 16:46:35","title":"Empowering Time Series Analysis with Large Language Models: A Survey","abstract":"Recently, remarkable progress has been made over large language models (LLMs), demonstrating their unprecedented capability in varieties of natural language tasks. However, completely training a large general-purpose model from the scratch is challenging for time series analysis, due to the large volumes and varieties of time series data, as well as the non-stationarity that leads to concept drift impeding continuous model adaptation and re-training. Recent advances have shown that pre-trained LLMs can be exploited to capture complex dependencies in time series data and facilitate various applications. In this survey, we provide a systematic overview of existing methods that leverage LLMs for time series analysis. Specifically, we first state the challenges and motivations of applying language models in the context of time series as well as brief preliminaries of LLMs. Next, we summarize the general pipeline for LLM-based time series analysis, categorize existing methods into different groups (i.e., direct query, tokenization, prompt design, fine-tune, and model integration), and highlight the key ideas within each group. We also discuss the applications of LLMs for both general and spatial-temporal time series data, tailored to specific domains. Finally, we thoroughly discuss future research opportunities to empower time series analysis with LLMs.","sentences":["Recently, remarkable progress has been made over large language models (LLMs), demonstrating their unprecedented capability in varieties of natural language tasks.","However, completely training a large general-purpose model from the scratch is challenging for time series analysis, due to the large volumes and varieties of time series data, as well as the non-stationarity that leads to concept drift impeding continuous model adaptation and re-training.","Recent advances have shown that pre-trained LLMs can be exploited to capture complex dependencies in time series data and facilitate various applications.","In this survey, we provide a systematic overview of existing methods that leverage LLMs for time series analysis.","Specifically, we first state the challenges and motivations of applying language models in the context of time series as well as brief preliminaries of LLMs.","Next, we summarize the general pipeline for LLM-based time series analysis, categorize existing methods into different groups (i.e., direct query, tokenization, prompt design, fine-tune, and model integration), and highlight the key ideas within each group.","We also discuss the applications of LLMs for both general and spatial-temporal time series data, tailored to specific domains.","Finally, we thoroughly discuss future research opportunities to empower time series analysis with LLMs."],"url":"http://arxiv.org/abs/2402.03182v1"}
{"created":"2024-02-05 16:43:53","title":"Comparison of Topic Modelling Approaches in the Banking Context","abstract":"Topic modelling is a prominent task for automatic topic extraction in many applications such as sentiment analysis and recommendation systems. The approach is vital for service industries to monitor their customer discussions. The use of traditional approaches such as Latent Dirichlet Allocation (LDA) for topic discovery has shown great performances, however, they are not consistent in their results as these approaches suffer from data sparseness and inability to model the word order in a document. Thus, this study presents the use of Kernel Principal Component Analysis (KernelPCA) and K-means Clustering in the BERTopic architecture. We have prepared a new dataset using tweets from customers of Nigerian banks and we use this to compare the topic modelling approaches. Our findings showed KernelPCA and K-means in the BERTopic architecture-produced coherent topics with a coherence score of 0.8463.","sentences":["Topic modelling is a prominent task for automatic topic extraction in many applications such as sentiment analysis and recommendation systems.","The approach is vital for service industries to monitor their customer discussions.","The use of traditional approaches such as Latent Dirichlet Allocation (LDA) for topic discovery has shown great performances, however, they are not consistent in their results as these approaches suffer from data sparseness and inability to model the word order in a document.","Thus, this study presents the use of Kernel Principal Component Analysis (KernelPCA) and K-means Clustering in the BERTopic architecture.","We have prepared a new dataset using tweets from customers of Nigerian banks and we use this to compare the topic modelling approaches.","Our findings showed KernelPCA and K-means in the BERTopic architecture-produced coherent topics with a coherence score of 0.8463."],"url":"http://arxiv.org/abs/2402.03176v1"}
{"created":"2024-02-05 16:39:15","title":"Homograph Attacks on Maghreb Sentiment Analyzers","abstract":"We examine the impact of homograph attacks on the Sentiment Analysis (SA) task of different Arabic dialects from the Maghreb North-African countries. Homograph attacks result in a 65.3% decrease in transformer classification from an F1-score of 0.95 to 0.33 when data is written in \"Arabizi\". The goal of this study is to highlight LLMs weaknesses' and to prioritize ethical and responsible Machine Learning.","sentences":["We examine the impact of homograph attacks on the Sentiment Analysis (SA) task of different Arabic dialects from the Maghreb North-African countries.","Homograph attacks result in a 65.3% decrease in transformer classification from an F1-score of 0.95 to 0.33 when data is written in \"Arabizi\".","The goal of this study is to highlight LLMs weaknesses' and to prioritize ethical and responsible Machine Learning."],"url":"http://arxiv.org/abs/2402.03171v1"}
{"created":"2024-02-05 16:31:03","title":"Linguistic features for sentence difficulty prediction in ABSA","abstract":"One of the challenges of natural language understanding is to deal with the subjectivity of sentences, which may express opinions and emotions that add layers of complexity and nuance. Sentiment analysis is a field that aims to extract and analyze these subjective elements from text, and it can be applied at different levels of granularity, such as document, paragraph, sentence, or aspect. Aspect-based sentiment analysis is a well-studied topic with many available data sets and models. However, there is no clear definition of what makes a sentence difficult for aspect-based sentiment analysis. In this paper, we explore this question by conducting an experiment with three data sets: \"Laptops\", \"Restaurants\", and \"MTSC\" (Multi-Target-dependent Sentiment Classification), and a merged version of these three datasets. We study the impact of domain diversity and syntactic diversity on difficulty. We use a combination of classifiers to identify the most difficult sentences and analyze their characteristics. We employ two ways of defining sentence difficulty. The first one is binary and labels a sentence as difficult if the classifiers fail to correctly predict the sentiment polarity. The second one is a six-level scale based on how many of the top five best-performing classifiers can correctly predict the sentiment polarity. We also define 9 linguistic features that, combined, aim at estimating the difficulty at sentence level.","sentences":["One of the challenges of natural language understanding is to deal with the subjectivity of sentences, which may express opinions and emotions that add layers of complexity and nuance.","Sentiment analysis is a field that aims to extract and analyze these subjective elements from text, and it can be applied at different levels of granularity, such as document, paragraph, sentence, or aspect.","Aspect-based sentiment analysis is a well-studied topic with many available data sets and models.","However, there is no clear definition of what makes a sentence difficult for aspect-based sentiment analysis.","In this paper, we explore this question by conducting an experiment with three data sets: \"Laptops\", \"Restaurants\", and \"MTSC\" (Multi-Target-dependent Sentiment Classification), and a merged version of these three datasets.","We study the impact of domain diversity and syntactic diversity on difficulty.","We use a combination of classifiers to identify the most difficult sentences and analyze their characteristics.","We employ two ways of defining sentence difficulty.","The first one is binary and labels a sentence as difficult if the classifiers fail to correctly predict the sentiment polarity.","The second one is a six-level scale based on how many of the top five best-performing classifiers can correctly predict the sentiment polarity.","We also define 9 linguistic features that, combined, aim at estimating the difficulty at sentence level."],"url":"http://arxiv.org/abs/2402.03163v1"}
{"created":"2024-02-05 16:30:49","title":"Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization","abstract":"In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13 multimodal benchmarks in image and video understanding and generation. Our code and models will be available at https://video-lavit.github.io.","sentences":["In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos.","Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics.","In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions.","These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text.","At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content.","Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13 multimodal benchmarks in image and video understanding and generation.","Our code and models will be available at https://video-lavit.github.io."],"url":"http://arxiv.org/abs/2402.03161v1"}
{"created":"2024-02-05 16:27:59","title":"Optimal and Near-Optimal Adaptive Vector Quantization","abstract":"Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of quantization is \\emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case. However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements.   We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity. We also present an even faster near-optimal algorithm for large inputs. Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications.","sentences":["Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets.","The most accurate form of quantization is \\emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case.","However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements.   ","We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity.","We also present an even faster near-optimal algorithm for large inputs.","Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications."],"url":"http://arxiv.org/abs/2402.03158v1"}
{"created":"2024-02-05 16:19:53","title":"Learning solutions of parametric Navier-Stokes with physics-informed neural networks","abstract":"We leverage Physics-Informed Neural Networks (PINNs) to learn solution functions of parametric Navier-Stokes Equations (NSE). Our proposed approach results in a feasible optimization problem setup that bypasses PINNs' limitations in converging to solutions of highly nonlinear parametric-PDEs like NSE. We consider the parameter(s) of interest as inputs of PINNs along with spatio-temporal coordinates, and train PINNs on generated numerical solutions of parametric-PDES for instances of the parameters. We perform experiments on the classical 2D flow past cylinder problem aiming to learn velocities and pressure functions over a range of Reynolds numbers as parameter of interest. Provision of training data from generated numerical simulations allows for interpolation of the solution functions for a range of parameters. Therefore, we compare PINNs with unconstrained conventional Neural Networks (NN) on this problem setup to investigate the effectiveness of considering the PDEs regularization in the loss function. We show that our proposed approach results in optimizing PINN models that learn the solution functions while making sure that flow predictions are in line with conservational laws of mass and momentum. Our results show that PINN results in accurate prediction of gradients compared to NN model, this is clearly visible in predicted vorticity fields given that none of these models were trained on vorticity labels.","sentences":["We leverage Physics-Informed Neural Networks (PINNs) to learn solution functions of parametric Navier-Stokes Equations (NSE).","Our proposed approach results in a feasible optimization problem setup that bypasses PINNs' limitations in converging to solutions of highly nonlinear parametric-PDEs like NSE.","We consider the parameter(s) of interest as inputs of PINNs along with spatio-temporal coordinates, and train PINNs on generated numerical solutions of parametric-PDES for instances of the parameters.","We perform experiments on the classical 2D flow past cylinder problem aiming to learn velocities and pressure functions over a range of Reynolds numbers as parameter of interest.","Provision of training data from generated numerical simulations allows for interpolation of the solution functions for a range of parameters.","Therefore, we compare PINNs with unconstrained conventional Neural Networks (NN) on this problem setup to investigate the effectiveness of considering the PDEs regularization in the loss function.","We show that our proposed approach results in optimizing PINN models that learn the solution functions while making sure that flow predictions are in line with conservational laws of mass and momentum.","Our results show that PINN results in accurate prediction of gradients compared to NN model, this is clearly visible in predicted vorticity fields given that none of these models were trained on vorticity labels."],"url":"http://arxiv.org/abs/2402.03153v1"}
{"created":"2024-02-05 16:13:54","title":"Detecting Scams Using Large Language Models","abstract":"Large Language Models (LLMs) have gained prominence in various applications, including security. This paper explores the utility of LLMs in scam detection, a critical aspect of cybersecurity. Unlike traditional applications, we propose a novel use case for LLMs to identify scams, such as phishing, advance fee fraud, and romance scams. We present notable security applications of LLMs and discuss the unique challenges posed by scams. Specifically, we outline the key steps involved in building an effective scam detector using LLMs, emphasizing data collection, preprocessing, model selection, training, and integration into target systems. Additionally, we conduct a preliminary evaluation using GPT-3.5 and GPT-4 on a duplicated email, highlighting their proficiency in identifying common signs of phishing or scam emails. The results demonstrate the models' effectiveness in recognizing suspicious elements, but we emphasize the need for a comprehensive assessment across various language tasks. The paper concludes by underlining the importance of ongoing refinement and collaboration with cybersecurity experts to adapt to evolving threats.","sentences":["Large Language Models (LLMs) have gained prominence in various applications, including security.","This paper explores the utility of LLMs in scam detection, a critical aspect of cybersecurity.","Unlike traditional applications, we propose a novel use case for LLMs to identify scams, such as phishing, advance fee fraud, and romance scams.","We present notable security applications of LLMs and discuss the unique challenges posed by scams.","Specifically, we outline the key steps involved in building an effective scam detector using LLMs, emphasizing data collection, preprocessing, model selection, training, and integration into target systems.","Additionally, we conduct a preliminary evaluation using GPT-3.5 and GPT-4 on a duplicated email, highlighting their proficiency in identifying common signs of phishing or scam emails.","The results demonstrate the models' effectiveness in recognizing suspicious elements, but we emphasize the need for a comprehensive assessment across various language tasks.","The paper concludes by underlining the importance of ongoing refinement and collaboration with cybersecurity experts to adapt to evolving threats."],"url":"http://arxiv.org/abs/2402.03147v1"}
{"created":"2024-02-05 16:13:00","title":"A Multi-step Loss Function for Robust Learning of the Dynamics in Model-based Reinforcement Learning","abstract":"In model-based reinforcement learning, most algorithms rely on simulating trajectories from one-step models of the dynamics learned on data. A critical challenge of this approach is the compounding of one-step prediction errors as the length of the trajectory grows. In this paper we tackle this issue by using a multi-step objective to train one-step models. Our objective is a weighted sum of the mean squared error (MSE) loss at various future horizons. We find that this new loss is particularly useful when the data is noisy (additive Gaussian noise in the observations), which is often the case in real-life environments. To support the multi-step loss, first we study its properties in two tractable cases: i) uni-dimensional linear system, and ii) two-parameter non-linear system. Second, we show in a variety of tasks (environments or datasets) that the models learned with this loss achieve a significant improvement in terms of the averaged R2-score on future prediction horizons. Finally, in the pure batch reinforcement learning setting, we demonstrate that one-step models serve as strong baselines when dynamics are deterministic, while multi-step models would be more advantageous in the presence of noise, highlighting the potential of our approach in real-world applications.","sentences":["In model-based reinforcement learning, most algorithms rely on simulating trajectories from one-step models of the dynamics learned on data.","A critical challenge of this approach is the compounding of one-step prediction errors as the length of the trajectory grows.","In this paper we tackle this issue by using a multi-step objective to train one-step models.","Our objective is a weighted sum of the mean squared error (MSE) loss at various future horizons.","We find that this new loss is particularly useful when the data is noisy (additive Gaussian noise in the observations), which is often the case in real-life environments.","To support the multi-step loss, first we study its properties in two tractable cases: i) uni-dimensional linear system, and ii) two-parameter non-linear system.","Second, we show in a variety of tasks (environments or datasets) that the models learned with this loss achieve a significant improvement in terms of the averaged R2-score on future prediction horizons.","Finally, in the pure batch reinforcement learning setting, we demonstrate that one-step models serve as strong baselines when dynamics are deterministic, while multi-step models would be more advantageous in the presence of noise, highlighting the potential of our approach in real-world applications."],"url":"http://arxiv.org/abs/2402.03146v1"}
{"created":"2024-02-05 16:12:27","title":"Computing Generic Fibres of Polynomial Ideals with FGLM and Hensel Lifting","abstract":"We describe a version of the FGLM algorithm that can be used to compute generic fibers of positive-dimensional polynomial ideals. It combines the FGLM algorithm with a Hensel lifting strategy. We show that this algorithm has a complexity quasi-linear in the number of lifting steps. Some provided experimental data also demonstrates the practical efficacy of our algorithm. Additionally, we sketch a related Hensel lifting method to compute Gr\\\"obner bases using so-called tracers.","sentences":["We describe a version of the FGLM algorithm that can be used to compute generic fibers of positive-dimensional polynomial ideals.","It combines the FGLM algorithm with a Hensel lifting strategy.","We show that this algorithm has a complexity quasi-linear in the number of lifting steps.","Some provided experimental data also demonstrates the practical efficacy of our algorithm.","Additionally, we sketch a related Hensel lifting method to compute Gr\\\"obner bases using so-called tracers."],"url":"http://arxiv.org/abs/2402.03144v1"}
{"created":"2024-02-05 16:05:32","title":"Sociolinguistically Informed Interpretability: A Case Study on Hinglish Emotion Classification","abstract":"Emotion classification is a challenging task in NLP due to the inherent idiosyncratic and subjective nature of linguistic expression, especially with code-mixed data. Pre-trained language models (PLMs) have achieved high performance for many tasks and languages, but it remains to be seen whether these models learn and are robust to the differences in emotional expression across languages. Sociolinguistic studies have shown that Hinglish speakers switch to Hindi when expressing negative emotions and to English when expressing positive emotions. To understand if language models can learn these associations, we study the effect of language on emotion prediction across 3 PLMs on a Hinglish emotion classification dataset. Using LIME and token level language ID, we find that models do learn these associations between language choice and emotional expression. Moreover, having code-mixed data present in the pre-training can augment that learning when task-specific data is scarce. We also conclude from the misclassifications that the models may overgeneralise this heuristic to other infrequent examples where this sociolinguistic phenomenon does not apply.","sentences":["Emotion classification is a challenging task in NLP due to the inherent idiosyncratic and subjective nature of linguistic expression, especially with code-mixed data.","Pre-trained language models (PLMs) have achieved high performance for many tasks and languages, but it remains to be seen whether these models learn and are robust to the differences in emotional expression across languages.","Sociolinguistic studies have shown that Hinglish speakers switch to Hindi when expressing negative emotions and to English when expressing positive emotions.","To understand if language models can learn these associations, we study the effect of language on emotion prediction across 3 PLMs on a Hinglish emotion classification dataset.","Using LIME and token level language ID, we find that models do learn these associations between language choice and emotional expression.","Moreover, having code-mixed data present in the pre-training can augment that learning when task-specific data is scarce.","We also conclude from the misclassifications that the models may overgeneralise this heuristic to other infrequent examples where this sociolinguistic phenomenon does not apply."],"url":"http://arxiv.org/abs/2402.03137v1"}
{"created":"2024-02-05 15:57:32","title":"Constrained Decoding for Cross-lingual Label Projection","abstract":"Zero-shot cross-lingual transfer utilizing multilingual LLMs has become a popular learning paradigm for low-resource languages with no labeled training data. However, for NLP tasks that involve fine-grained predictions on words and phrases, the performance of zero-shot cross-lingual transfer learning lags far behind supervised fine-tuning methods. Therefore, it is common to exploit translation and label projection to further improve the performance by (1) translating training data that is available in a high-resource language (e.g., English) together with the gold labels into low-resource languages, and/or (2) translating test data in low-resource languages to a high-source language to run inference on, then projecting the predicted span-level labels back onto the original test data. However, state-of-the-art marker-based label projection methods suffer from translation quality degradation due to the extra label markers injected in the input to the translation model. In this work, we explore a new direction that leverages constrained decoding for label projection to overcome the aforementioned issues. Our new method not only can preserve the quality of translated texts but also has the versatility of being applicable to both translating training and translating test data strategies. This versatility is crucial as our experiments reveal that translating test data can lead to a considerable boost in performance compared to translating only training data. We evaluate on two cross-lingual transfer tasks, namely Named Entity Recognition and Event Argument Extraction, spanning 20 languages. The results demonstrate that our approach outperforms the state-of-the-art marker-based method by a large margin and also shows better performance than other label projection methods that rely on external word alignment.","sentences":["Zero-shot cross-lingual transfer utilizing multilingual LLMs has become a popular learning paradigm for low-resource languages with no labeled training data.","However, for NLP tasks that involve fine-grained predictions on words and phrases, the performance of zero-shot cross-lingual transfer learning lags far behind supervised fine-tuning methods.","Therefore, it is common to exploit translation and label projection to further improve the performance by (1) translating training data that is available in a high-resource language (e.g., English) together with the gold labels into low-resource languages, and/or (2) translating test data in low-resource languages to a high-source language to run inference on, then projecting the predicted span-level labels back onto the original test data.","However, state-of-the-art marker-based label projection methods suffer from translation quality degradation due to the extra label markers injected in the input to the translation model.","In this work, we explore a new direction that leverages constrained decoding for label projection to overcome the aforementioned issues.","Our new method not only can preserve the quality of translated texts but also has the versatility of being applicable to both translating training and translating test data strategies.","This versatility is crucial as our experiments reveal that translating test data can lead to a considerable boost in performance compared to translating only training data.","We evaluate on two cross-lingual transfer tasks, namely Named Entity Recognition and Event Argument Extraction, spanning 20 languages.","The results demonstrate that our approach outperforms the state-of-the-art marker-based method by a large margin and also shows better performance than other label projection methods that rely on external word alignment."],"url":"http://arxiv.org/abs/2402.03131v1"}
{"created":"2024-02-05 15:56:19","title":"User-Centric Evaluation of ChatGPT Capability of Generating R Program Code","abstract":"This paper reports an evaluation of ChatGPT's capability of generating R programming language code from natural language input. A dataset specially designed for generating R program code was constructed with metadata to support scenario-based testing and evaluation of code generation capabilities in various usage scenarios of different levels of difficulty and different types of programs. The evaluation takes a multiple attempt process in which the tester tries to complete the code generation task through a number of attempts until a satisfactory solution is obtained or gives up after a fixed number of maximal attempts. In each attempt the tester formulates a natural language input to ChatGPT based on the previous results and the task to be completed. In addition to the metrics of average numbers of attempts and average amount of time taken to complete the tasks, the final generated solutions are then assessed on a number of quality attributes, including accuracy, completeness, conciseness, readability, well structuredness, logic clarity, depth of ex-planation, and coverage of parameters. Our experiments demonstrated that ChatGPT is in general highly capable of generating high quality R program code as well as textual explanations although it may fail on hard programming tasks. The experiment data also shows that human developers can hardly learn from experiences naturally to improve the skill of using ChatGPT to generate code.","sentences":["This paper reports an evaluation of ChatGPT's capability of generating R programming language code from natural language input.","A dataset specially designed for generating R program code was constructed with metadata to support scenario-based testing and evaluation of code generation capabilities in various usage scenarios of different levels of difficulty and different types of programs.","The evaluation takes a multiple attempt process in which the tester tries to complete the code generation task through a number of attempts until a satisfactory solution is obtained or gives up after a fixed number of maximal attempts.","In each attempt the tester formulates a natural language input to ChatGPT based on the previous results and the task to be completed.","In addition to the metrics of average numbers of attempts and average amount of time taken to complete the tasks, the final generated solutions are then assessed on a number of quality attributes, including accuracy, completeness, conciseness, readability, well structuredness, logic clarity, depth of ex-planation, and coverage of parameters.","Our experiments demonstrated that ChatGPT is in general highly capable of generating high quality R program code as well as textual explanations although it may fail on hard programming tasks.","The experiment data also shows that human developers can hardly learn from experiences naturally to improve the skill of using ChatGPT to generate code."],"url":"http://arxiv.org/abs/2402.03130v1"}
{"created":"2024-02-05 15:51:38","title":"Shape Manipulation of Bevel-Tip Needles for Prostate Biopsy Procedures: A Comparison of Two Resolved-Rate Controllers","abstract":"Prostate cancer diagnosis continues to encounter challenges, often due to imprecise needle placement in standard biopsies. Several control strategies have been developed to compensate for needle tip prediction inaccuracies, however none were compared against each other, and it is unclear whether any of them can be safely and universally applied in clinical settings. This paper compares the performance of two resolved-rate controllers, derived from a mechanics-based and a data-driven approach, for bevel-tip needle control using needle shape manipulation through a template. We demonstrate for a simulated 12-core biopsy procedure under model parameter uncertainty that the mechanics-based controller can better reach desired targets when only the final goal configuration is presented even with uncertainty on model parameters estimation, and that providing a feasible needle path is crucial in ensuring safe surgical outcomes when either controller is used for needle shape manipulation.","sentences":["Prostate cancer diagnosis continues to encounter challenges, often due to imprecise needle placement in standard biopsies.","Several control strategies have been developed to compensate for needle tip prediction inaccuracies, however none were compared against each other, and it is unclear whether any of them can be safely and universally applied in clinical settings.","This paper compares the performance of two resolved-rate controllers, derived from a mechanics-based and a data-driven approach, for bevel-tip needle control using needle shape manipulation through a template.","We demonstrate for a simulated 12-core biopsy procedure under model parameter uncertainty that the mechanics-based controller can better reach desired targets when only the final goal configuration is presented even with uncertainty on model parameters estimation, and that providing a feasible needle path is crucial in ensuring safe surgical outcomes when either controller is used for needle shape manipulation."],"url":"http://arxiv.org/abs/2402.03125v1"}
{"created":"2024-02-05 15:51:34","title":"Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks","abstract":"Gradient inversion attacks aim to reconstruct local training data from intermediate gradients exposed in the federated learning framework. Despite successful attacks, all previous methods, starting from reconstructing a single data point and then relaxing the single-image limit to batch level, are only tested under hard label constraints. Even for single-image reconstruction, we still lack an analysis-based algorithm to recover augmented soft labels. In this work, we change the focus from enlarging batchsize to investigating the hard label constraints, considering a more realistic circumstance where label smoothing and mixup techniques are used in the training process. In particular, we are the first to initiate a novel algorithm to simultaneously recover the ground-truth augmented label and the input feature of the last fully-connected layer from single-input gradients, and provide a necessary condition for any analytical-based label recovery methods. Extensive experiments testify to the label recovery accuracy, as well as the benefits to the following image reconstruction. We believe soft labels in classification tasks are worth further attention in gradient inversion attacks.","sentences":["Gradient inversion attacks aim to reconstruct local training data from intermediate gradients exposed in the federated learning framework.","Despite successful attacks, all previous methods, starting from reconstructing a single data point and then relaxing the single-image limit to batch level, are only tested under hard label constraints.","Even for single-image reconstruction, we still lack an analysis-based algorithm to recover augmented soft labels.","In this work, we change the focus from enlarging batchsize to investigating the hard label constraints, considering a more realistic circumstance where label smoothing and mixup techniques are used in the training process.","In particular, we are the first to initiate a novel algorithm to simultaneously recover the ground-truth augmented label and the input feature of the last fully-connected layer from single-input gradients, and provide a necessary condition for any analytical-based label recovery methods.","Extensive experiments testify to the label recovery accuracy, as well as the benefits to the following image reconstruction.","We believe soft labels in classification tasks are worth further attention in gradient inversion attacks."],"url":"http://arxiv.org/abs/2402.03124v1"}
{"created":"2024-02-05 15:47:54","title":"Good Teachers Explain: Explanation-Enhanced Knowledge Distillation","abstract":"Knowledge Distillation (KD) has proven effective for compressing large teacher models into smaller student models. While it is well known that student models can achieve similar accuracies as the teachers, it has also been shown that they nonetheless often do not learn the same function. It is, however, often highly desirable that the student's and teacher's functions share similar properties such as basing the prediction on the same input features, as this ensures that students learn the 'right features' from the teachers. In this work, we explore whether this can be achieved by not only optimizing the classic KD loss but also the similarity of the explanations generated by the teacher and the student. Despite the idea being simple and intuitive, we find that our proposed 'explanation-enhanced' KD (e$^2$KD) (1) consistently provides large gains in terms of accuracy and student-teacher agreement, (2) ensures that the student learns from the teacher to be right for the right reasons and to give similar explanations, and (3) is robust with respect to the model architectures, the amount of training data, and even works with 'approximate', pre-computed explanations.","sentences":["Knowledge Distillation (KD) has proven effective for compressing large teacher models into smaller student models.","While it is well known that student models can achieve similar accuracies as the teachers, it has also been shown that they nonetheless often do not learn the same function.","It is, however, often highly desirable that the student's and teacher's functions share similar properties such as basing the prediction on the same input features, as this ensures that students learn the 'right features' from the teachers.","In this work, we explore whether this can be achieved by not only optimizing the classic KD loss but also the similarity of the explanations generated by the teacher and the student.","Despite the idea being simple and intuitive, we find that our proposed 'explanation-enhanced' KD (e$^2$KD) (1) consistently provides large gains in terms of accuracy and student-teacher agreement, (2) ensures that the student learns from the teacher to be right for the right reasons and to give similar explanations, and (3) is robust with respect to the model architectures, the amount of training data, and even works with 'approximate', pre-computed explanations."],"url":"http://arxiv.org/abs/2402.03119v1"}
{"created":"2024-02-05 15:45:59","title":"Feature-Action Design Patterns for Storytelling Visualizations with Time Series Data","abstract":"We present a method to create storytelling visualization with time series data. Many personal decisions nowadays rely on access to dynamic data regularly, as we have seen during the COVID-19 pandemic. It is thus desirable to construct storytelling visualization for dynamic data that is selected by an individual for a specific context. Because of the need to tell data-dependent stories, predefined storyboards based on known data cannot accommodate dynamic data easily nor scale up to many different individuals and contexts. Motivated initially by the need to communicate time series data during the COVID-19 pandemic, we developed a novel computer-assisted method for meta-authoring of stories, which enables the design of storyboards that include feature-action patterns in anticipation of potential features that may appear in dynamically arrived or selected data. In addition to meta-storyboards involving COVID-19 data, we also present storyboards for telling stories about progress in a machine learning workflow. Our approach is complementary to traditional methods for authoring storytelling visualization, and provides an efficient means to construct data-dependent storyboards for different data-streams of similar contexts.","sentences":["We present a method to create storytelling visualization with time series data.","Many personal decisions nowadays rely on access to dynamic data regularly, as we have seen during the COVID-19 pandemic.","It is thus desirable to construct storytelling visualization for dynamic data that is selected by an individual for a specific context.","Because of the need to tell data-dependent stories, predefined storyboards based on known data cannot accommodate dynamic data easily nor scale up to many different individuals and contexts.","Motivated initially by the need to communicate time series data during the COVID-19 pandemic, we developed a novel computer-assisted method for meta-authoring of stories, which enables the design of storyboards that include feature-action patterns in anticipation of potential features that may appear in dynamically arrived or selected data.","In addition to meta-storyboards involving COVID-19 data, we also present storyboards for telling stories about progress in a machine learning workflow.","Our approach is complementary to traditional methods for authoring storytelling visualization, and provides an efficient means to construct data-dependent storyboards for different data-streams of similar contexts."],"url":"http://arxiv.org/abs/2402.03116v1"}
{"created":"2024-02-05 15:45:55","title":"Discovering interpretable models of scientific image data with deep learning","abstract":"How can we find interpretable, domain-appropriate models of natural phenomena given some complex, raw data such as images? Can we use such models to derive scientific insight from the data? In this paper, we propose some methods for achieving this. In particular, we implement disentangled representation learning, sparse deep neural network training and symbolic regression, and assess their usefulness in forming interpretable models of complex image data. We demonstrate their relevance to the field of bioimaging using a well-studied test problem of classifying cell states in microscopy data. We find that such methods can produce highly parsimonious models that achieve $\\sim98\\%$ of the accuracy of black-box benchmark models, with a tiny fraction of the complexity. We explore the utility of such interpretable models in producing scientific explanations of the underlying biological phenomenon.","sentences":["How can we find interpretable, domain-appropriate models of natural phenomena given some complex, raw data such as images?","Can we use such models to derive scientific insight from the data?","In this paper, we propose some methods for achieving this.","In particular, we implement disentangled representation learning, sparse deep neural network training and symbolic regression, and assess their usefulness in forming interpretable models of complex image data.","We demonstrate their relevance to the field of bioimaging using a well-studied test problem of classifying cell states in microscopy data.","We find that such methods can produce highly parsimonious models that achieve $\\sim98\\%$ of the accuracy of black-box benchmark models, with a tiny fraction of the complexity.","We explore the utility of such interpretable models in producing scientific explanations of the underlying biological phenomenon."],"url":"http://arxiv.org/abs/2402.03115v1"}
{"created":"2024-02-05 15:37:02","title":"Computing with Clocks","abstract":"Clocks are a central part of many computing paradigms, and are mainly used to synchronise the delicate operation of switching, necessary to drive modern computational processes. Unfortunately, this synchronisation process is reaching a natural ``apocalypse''. No longer can clock scaling be used as a blunt tool to accelerate computation, we are up against the natural limits of switching and synchronisation across large processors. Therefore, we need to rethink how time is utilised in computation, using it more naturally in the role of representing data. This can be achieved by using a time interval delineated by discrete start and end events, and by re-casting computational operations into the time domain. With this, computer systems can be developed that are naturally scaleable in time and space, and can use ambient time references built to the best effort of the available technology.   Our ambition is to better manage the energy/computation time trade-off, and to explicitly embed the resolution of the data in the time domain. We aim to recast calculations into the ``for free'' format that time offers, and in addition, perform these calculations at the highest clock or oscillator resolution possible.","sentences":["Clocks are a central part of many computing paradigms, and are mainly used to synchronise the delicate operation of switching, necessary to drive modern computational processes.","Unfortunately, this synchronisation process is reaching a natural ``apocalypse''.","No longer can clock scaling be used as a blunt tool to accelerate computation, we are up against the natural limits of switching and synchronisation across large processors.","Therefore, we need to rethink how time is utilised in computation, using it more naturally in the role of representing data.","This can be achieved by using a time interval delineated by discrete start and end events, and by re-casting computational operations into the time domain.","With this, computer systems can be developed that are naturally scaleable in time and space, and can use ambient time references built to the best effort of the available technology.   ","Our ambition is to better manage the energy/computation time trade-off, and to explicitly embed the resolution of the data in the time domain.","We aim to recast calculations into the ``for free'' format that time offers, and in addition, perform these calculations at the highest clock or oscillator resolution possible."],"url":"http://arxiv.org/abs/2402.03109v1"}
{"created":"2024-02-05 15:28:43","title":"Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases","abstract":"Prompt engineering is a challenging and important task due to the high sensitivity of Large Language Models (LLMs) to the given prompt and the inherent ambiguity of a textual task instruction. Automatic prompt engineering is essential to achieve optimized performance from LLMs. Recent studies have demonstrated the capabilities of LLMs to automatically conduct prompt engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved prompt. However, this requires a high-quality benchmark to compare different prompts, which is difficult and expensive to acquire in many real-world use cases. In this work, we introduce a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent. During the optimization process, the system jointly generates synthetic data of boundary use cases and optimizes the prompt according to the generated dataset. We demonstrate the effectiveness of our method with respect to strong proprietary models on real-world tasks such as moderation and generation. Our method outperforms state-of-the-art methods with a limited number of annotated samples. Furthermore, we validate the advantages of each one of the system's key components. Our system is built in a modular way, facilitating easy adaptation to other tasks. The code is available $\\href{https://github.com/Eladlev/AutoPrompt}{here}$.","sentences":["Prompt engineering is a challenging and important task due to the high sensitivity of Large Language Models (LLMs) to the given prompt and the inherent ambiguity of a textual task instruction.","Automatic prompt engineering is essential to achieve optimized performance from LLMs.","Recent studies have demonstrated the capabilities of LLMs to automatically conduct prompt engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved prompt.","However, this requires a high-quality benchmark to compare different prompts, which is difficult and expensive to acquire in many real-world use cases.","In this work, we introduce a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent.","During the optimization process, the system jointly generates synthetic data of boundary use cases and optimizes the prompt according to the generated dataset.","We demonstrate the effectiveness of our method with respect to strong proprietary models on real-world tasks such as moderation and generation.","Our method outperforms state-of-the-art methods with a limited number of annotated samples.","Furthermore, we validate the advantages of each one of the system's key components.","Our system is built in a modular way, facilitating easy adaptation to other tasks.","The code is available $\\href{https://github.com/Eladlev/AutoPrompt}{here}$."],"url":"http://arxiv.org/abs/2402.03099v1"}
{"created":"2024-02-05 15:24:13","title":"AI-Enhanced Virtual Reality in Medicine: A Comprehensive Survey","abstract":"With the rapid advance of computer graphics and artificial intelligence technologies, the ways we interact with the world have undergone a transformative shift. Virtual Reality (VR) technology, aided by artificial intelligence (AI), has emerged as a dominant interaction media in multiple application areas, thanks to its advantage of providing users with immersive experiences. Among those applications, medicine is considered one of the most promising areas. In this paper, we present a comprehensive examination of the burgeoning field of AI-enhanced VR applications in medical care and services. By introducing a systematic taxonomy, we meticulously classify the pertinent techniques and applications into three well-defined categories based on different phases of medical diagnosis and treatment: Visualization Enhancement, VR-related Medical Data Processing, and VR-assisted Intervention. This categorization enables a structured exploration of the diverse roles that AI-powered VR plays in the medical domain, providing a framework for a more comprehensive understanding and evaluation of these technologies. To our best knowledge, this is the first systematic survey of AI-powered VR systems in medical settings, laying a foundation for future research in this interdisciplinary domain.","sentences":["With the rapid advance of computer graphics and artificial intelligence technologies, the ways we interact with the world have undergone a transformative shift.","Virtual Reality (VR) technology, aided by artificial intelligence (AI), has emerged as a dominant interaction media in multiple application areas, thanks to its advantage of providing users with immersive experiences.","Among those applications, medicine is considered one of the most promising areas.","In this paper, we present a comprehensive examination of the burgeoning field of AI-enhanced VR applications in medical care and services.","By introducing a systematic taxonomy, we meticulously classify the pertinent techniques and applications into three well-defined categories based on different phases of medical diagnosis and treatment: Visualization Enhancement, VR-related Medical Data Processing, and VR-assisted Intervention.","This categorization enables a structured exploration of the diverse roles that AI-powered VR plays in the medical domain, providing a framework for a more comprehensive understanding and evaluation of these technologies.","To our best knowledge, this is the first systematic survey of AI-powered VR systems in medical settings, laying a foundation for future research in this interdisciplinary domain."],"url":"http://arxiv.org/abs/2402.03093v1"}
{"created":"2024-02-05 14:59:35","title":"A Note on Rounding Matchings in General Graphs","abstract":"In this note, we revisit the rounding algorithm of Wajc. Wajc gave a fully-adaptive randomized algorithm that rounds a dynamic fractional matching in an unweighted bipartite graph to an integral matching of nearly the same value in $O(\\text{poly}(\\log n,\\frac{1}{\\varepsilon}))$ update time. We give show that the guarantees of this algorithm hold for general graphs as well. Additionally, we show useful properties of this subroutine which have applications in rounding weighted fractional matchings.","sentences":["In this note, we revisit the rounding algorithm of Wajc.","Wajc gave a fully-adaptive randomized algorithm that rounds a dynamic fractional matching in an unweighted bipartite graph to an integral matching of nearly the same value in $O(\\text{poly}(\\log n,\\frac{1}{\\varepsilon}))$ update time.","We give show that the guarantees of this algorithm hold for general graphs as well.","Additionally, we show useful properties of this subroutine which have applications in rounding weighted fractional matchings."],"url":"http://arxiv.org/abs/2402.03068v1"}
{"created":"2024-02-05 14:53:04","title":"Independent set reconfiguration in H-free graphs","abstract":"Given a graph $G$ and two independent sets of $G$, the independent set reconfiguration problem asks whether one independent set can be transformed into the other by moving a single vertex at a time, such that at each intermediate step we have an independent set of $G$. We study the complexity of this problem for $H$-free graphs under the token sliding and token jumping rule. Our contribution is twofold. First, we prove a reconfiguration analogue of Alekseev's theorem, showing that the problem is PSPACE-complete unless $H$ is a path or a subdivision of the claw. We then show that under the token sliding rule, the problem admits a polynomial-time algorithm if the input graph is fork-free.","sentences":["Given a graph $G$ and two independent sets of $G$, the independent set reconfiguration problem asks whether one independent set can be transformed into the other by moving a single vertex at a time, such that at each intermediate step we have an independent set of $G$. We study the complexity of this problem for $H$-free graphs under the token sliding and token jumping rule.","Our contribution is twofold.","First, we prove a reconfiguration analogue of Alekseev's theorem, showing that the problem is PSPACE-complete unless $H$ is a path or a subdivision of the claw.","We then show that under the token sliding rule, the problem admits a polynomial-time algorithm if the input graph is fork-free."],"url":"http://arxiv.org/abs/2402.03063v1"}
{"created":"2024-02-05 14:52:01","title":"UniHENN: Designing More Versatile Homomorphic Encryption-based CNNs without im2col","abstract":"Homomorphic encryption enables computations on encrypted data without decryption, which is crucial for privacy-preserving cloud services. However, deploying convolutional neural networks (CNNs) with homomorphic encryption encounters significant challenges, particularly in converting input data into a two-dimensional matrix for convolution, typically achieved using the im2col technique. While efficient, this method limits the variety of deployable CNN models due to compatibility constraints with the encrypted data structure. UniHENN, a homomorphic encryption-based CNN architecture, eliminates the need for im2col, ensuring compatibility with a diverse range of CNN models using homomorphic encryption. Our experiments demonstrate that UniHENN surpasses the leading 2D CNN inference architecture, PyCrCNN, in inference time, as evidenced by its performance on the LeNet-1 dataset, where it averages 30.090 seconds--significantly faster than PyCrCNN's 794.064 seconds. Furthermore, UniHENN outperforms TenSEAL, which employs im2col, in processing concurrent images, an essential feature for high-demand cloud applications. The versatility of UniHENN is proven across various CNN architectures, including 1D and six different 2D CNNs, highlighting its flexibility and efficiency. These qualities establish UniHENN as a promising solution for privacy-preserving, cloud-based CNN services, addressing the increasing demand for scalable, secure, and efficient deep learning in cloud computing environments.","sentences":["Homomorphic encryption enables computations on encrypted data without decryption, which is crucial for privacy-preserving cloud services.","However, deploying convolutional neural networks (CNNs) with homomorphic encryption encounters significant challenges, particularly in converting input data into a two-dimensional matrix for convolution, typically achieved using the im2col technique.","While efficient, this method limits the variety of deployable CNN models due to compatibility constraints with the encrypted data structure.","UniHENN, a homomorphic encryption-based CNN architecture, eliminates the need for im2col, ensuring compatibility with a diverse range of CNN models using homomorphic encryption.","Our experiments demonstrate that UniHENN surpasses the leading 2D CNN inference architecture, PyCrCNN, in inference time, as evidenced by its performance on the LeNet-1 dataset, where it averages 30.090 seconds--significantly faster than PyCrCNN's 794.064 seconds.","Furthermore, UniHENN outperforms TenSEAL, which employs im2col, in processing concurrent images, an essential feature for high-demand cloud applications.","The versatility of UniHENN is proven across various CNN architectures, including 1D and six different 2D CNNs, highlighting its flexibility and efficiency.","These qualities establish UniHENN as a promising solution for privacy-preserving, cloud-based CNN services, addressing the increasing demand for scalable, secure, and efficient deep learning in cloud computing environments."],"url":"http://arxiv.org/abs/2402.03060v1"}
{"created":"2024-02-05 14:33:56","title":"EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models","abstract":"In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along with a running demo App at https://huggingface.co/spaces/zjunlp/EasyInstruct for quick-start, calling for broader research centered on instruction data.","sentences":["In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs).","To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality.","Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing.","To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction.","EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along with a running demo App at https://huggingface.co/spaces/zjunlp/EasyInstruct for quick-start, calling for broader research centered on instruction data."],"url":"http://arxiv.org/abs/2402.03049v1"}
{"created":"2024-02-05 14:32:00","title":"Open RL Benchmark: Comprehensive Tracked Experiments for Reinforcement Learning","abstract":"In many Reinforcement Learning (RL) papers, learning curves are useful indicators to measure the effectiveness of RL algorithms. However, the complete raw data of the learning curves are rarely available. As a result, it is usually necessary to reproduce the experiments from scratch, which can be time-consuming and error-prone. We present Open RL Benchmark, a set of fully tracked RL experiments, including not only the usual data such as episodic return, but also all algorithm-specific and system metrics. Open RL Benchmark is community-driven: anyone can download, use, and contribute to the data. At the time of writing, more than 25,000 runs have been tracked, for a cumulative duration of more than 8 years. Open RL Benchmark covers a wide range of RL libraries and reference implementations. Special care is taken to ensure that each experiment is precisely reproducible by providing not only the full parameters, but also the versions of the dependencies used to generate it. In addition, Open RL Benchmark comes with a command-line interface (CLI) for easy fetching and generating figures to present the results. In this document, we include two case studies to demonstrate the usefulness of Open RL Benchmark in practice. To the best of our knowledge, Open RL Benchmark is the first RL benchmark of its kind, and the authors hope that it will improve and facilitate the work of researchers in the field.","sentences":["In many Reinforcement Learning (RL) papers, learning curves are useful indicators to measure the effectiveness of RL algorithms.","However, the complete raw data of the learning curves are rarely available.","As a result, it is usually necessary to reproduce the experiments from scratch, which can be time-consuming and error-prone.","We present Open RL Benchmark, a set of fully tracked RL experiments, including not only the usual data such as episodic return, but also all algorithm-specific and system metrics.","Open RL Benchmark is community-driven: anyone can download, use, and contribute to the data.","At the time of writing, more than 25,000 runs have been tracked, for a cumulative duration of more than 8 years.","Open RL Benchmark covers a wide range of RL libraries and reference implementations.","Special care is taken to ensure that each experiment is precisely reproducible by providing not only the full parameters, but also the versions of the dependencies used to generate it.","In addition, Open RL Benchmark comes with a command-line interface (CLI) for easy fetching and generating figures to present the results.","In this document, we include two case studies to demonstrate the usefulness of Open RL Benchmark in practice.","To the best of our knowledge, Open RL Benchmark is the first RL benchmark of its kind, and the authors hope that it will improve and facilitate the work of researchers in the field."],"url":"http://arxiv.org/abs/2402.03046v1"}
{"created":"2024-02-05 14:29:54","title":"SIDU-TXT: An XAI Algorithm for NLP with a Holistic Assessment Approach","abstract":"Explainable AI (XAI) aids in deciphering 'black-box' models. While several methods have been proposed and evaluated primarily in the image domain, the exploration of explainability in the text domain remains a growing research area. In this paper, we delve into the applicability of XAI methods for the text domain. In this context, the 'Similarity Difference and Uniqueness' (SIDU) XAI method, recognized for its superior capability in localizing entire salient regions in image-based classification is extended to textual data. The extended method, SIDU-TXT, utilizes feature activation maps from 'black-box' models to generate heatmaps at a granular, word-based level, thereby providing explanations that highlight contextually significant textual elements crucial for model predictions. Given the absence of a unified standard for assessing XAI methods, this study applies a holistic three-tiered comprehensive evaluation framework: Functionally-Grounded, Human-Grounded and Application-Grounded, to assess the effectiveness of the proposed SIDU-TXT across various experiments. We find that, in sentiment analysis task of a movie review dataset, SIDU-TXT excels in both functionally and human-grounded evaluations, demonstrating superior performance through quantitative and qualitative analyses compared to benchmarks like Grad-CAM and LIME. In the application-grounded evaluation within the sensitive and complex legal domain of asylum decision-making, SIDU-TXT and Grad-CAM demonstrate comparable performances, each with its own set of strengths and weaknesses. However, both methods fall short of entirely fulfilling the sophisticated criteria of expert expectations, highlighting the imperative need for additional research in XAI methods suitable for such domains.","sentences":["Explainable AI (XAI) aids in deciphering 'black-box' models.","While several methods have been proposed and evaluated primarily in the image domain, the exploration of explainability in the text domain remains a growing research area.","In this paper, we delve into the applicability of XAI methods for the text domain.","In this context, the 'Similarity Difference and Uniqueness' (SIDU) XAI method, recognized for its superior capability in localizing entire salient regions in image-based classification is extended to textual data.","The extended method, SIDU-TXT, utilizes feature activation maps from 'black-box' models to generate heatmaps at a granular, word-based level, thereby providing explanations that highlight contextually significant textual elements crucial for model predictions.","Given the absence of a unified standard for assessing XAI methods, this study applies a holistic three-tiered comprehensive evaluation framework: Functionally-Grounded, Human-Grounded and Application-Grounded, to assess the effectiveness of the proposed SIDU-TXT across various experiments.","We find that, in sentiment analysis task of a movie review dataset, SIDU-TXT excels in both functionally and human-grounded evaluations, demonstrating superior performance through quantitative and qualitative analyses compared to benchmarks like Grad-CAM and LIME.","In the application-grounded evaluation within the sensitive and complex legal domain of asylum decision-making, SIDU-TXT and Grad-CAM demonstrate comparable performances, each with its own set of strengths and weaknesses.","However, both methods fall short of entirely fulfilling the sophisticated criteria of expert expectations, highlighting the imperative need for additional research in XAI methods suitable for such domains."],"url":"http://arxiv.org/abs/2402.03043v1"}
{"created":"2024-02-05 14:22:25","title":"An Investigation of the Compressed Sensing Phase in Unsourced Multiple Access","abstract":"A vast population of low-cost low-power transmitters sporadically sending small amounts of data over a common wireless medium is one of the main scenarios for Internet of things (IoT) data communications. At the medium access, the use of grant-free solutions may be preferred to reduce overhead even at the cost of multiple-access interference. Unsourced multiple access (UMA) has been recently established as relevant framework for energy efficient grant-free protocols. The use of a compressed sensing (CS) transmission phase is key in one of the two main classes of UMA protocols, yet little attention has been posed to sparse greedy algorithms as orthogonal matching pursuit (OMP) and its variants. We analyze their performance and provide relevant guidance on how to optimally setup the CS phase. Minimum average transmission power and minimum number of channel uses are investigated together with the performance in terms of receiver operating characteristic (ROC). Interestingly, we show how the basic OMP and generalized OMP (gOMP) are the most competitive algorithms in their class.","sentences":["A vast population of low-cost low-power transmitters sporadically sending small amounts of data over a common wireless medium is one of the main scenarios for Internet of things (IoT) data communications.","At the medium access, the use of grant-free solutions may be preferred to reduce overhead even at the cost of multiple-access interference.","Unsourced multiple access (UMA) has been recently established as relevant framework for energy efficient grant-free protocols.","The use of a compressed sensing (CS) transmission phase is key in one of the two main classes of UMA protocols, yet little attention has been posed to sparse greedy algorithms as orthogonal matching pursuit (OMP) and its variants.","We analyze their performance and provide relevant guidance on how to optimally setup the CS phase.","Minimum average transmission power and minimum number of channel uses are investigated together with the performance in terms of receiver operating characteristic (ROC).","Interestingly, we show how the basic OMP and generalized OMP (gOMP) are the most competitive algorithms in their class."],"url":"http://arxiv.org/abs/2402.03037v1"}
{"created":"2024-02-05 14:00:53","title":"Data-induced multiscale losses and efficient multirate gradient descent schemes","abstract":"This paper investigates the impact of multiscale data on machine learning algorithms, particularly in the context of deep learning. A dataset is multiscale if its distribution shows large variations in scale across different directions. This paper reveals multiscale structures in the loss landscape, including its gradients and Hessians inherited from the data. Correspondingly, it introduces a novel gradient descent approach, drawing inspiration from multiscale algorithms used in scientific computing. This approach seeks to transcend empirical learning rate selection, offering a more systematic, data-informed strategy to enhance training efficiency, especially in the later stages.","sentences":["This paper investigates the impact of multiscale data on machine learning algorithms, particularly in the context of deep learning.","A dataset is multiscale if its distribution shows large variations in scale across different directions.","This paper reveals multiscale structures in the loss landscape, including its gradients and Hessians inherited from the data.","Correspondingly, it introduces a novel gradient descent approach, drawing inspiration from multiscale algorithms used in scientific computing.","This approach seeks to transcend empirical learning rate selection, offering a more systematic, data-informed strategy to enhance training efficiency, especially in the later stages."],"url":"http://arxiv.org/abs/2402.03021v1"}
{"created":"2024-02-05 13:55:54","title":"Toward Green and Human-Like Artificial Intelligence: A Complete Survey on Contemporary Few-Shot Learning Approaches","abstract":"Despite deep learning's widespread success, its data-hungry and computationally expensive nature makes it impractical for many data-constrained real-world applications. Few-Shot Learning (FSL) aims to address these limitations by enabling rapid adaptation to novel learning tasks, seeing significant growth in recent years. This survey provides a comprehensive overview of the field's latest advancements. Initially, FSL is formally defined, and its relationship with different learning fields is presented. A novel taxonomy is introduced, extending previously proposed ones, and real-world applications in classic and novel fields are described. Finally, recent trends shaping the field, outstanding challenges, and promising future research directions are discussed.","sentences":["Despite deep learning's widespread success, its data-hungry and computationally expensive nature makes it impractical for many data-constrained real-world applications.","Few-Shot Learning (FSL) aims to address these limitations by enabling rapid adaptation to novel learning tasks, seeing significant growth in recent years.","This survey provides a comprehensive overview of the field's latest advancements.","Initially, FSL is formally defined, and its relationship with different learning fields is presented.","A novel taxonomy is introduced, extending previously proposed ones, and real-world applications in classic and novel fields are described.","Finally, recent trends shaping the field, outstanding challenges, and promising future research directions are discussed."],"url":"http://arxiv.org/abs/2402.03017v1"}
{"created":"2024-02-05 13:41:22","title":"[Citation needed] Data usage and citation practices in medical imaging conferences","abstract":"Medical imaging papers often focus on methodology, but the quality of the algorithms and the validity of the conclusions are highly dependent on the datasets used. As creating datasets requires a lot of effort, researchers often use publicly available datasets, there is however no adopted standard for citing the datasets used in scientific papers, leading to difficulty in tracking dataset usage. In this work, we present two open-source tools we created that could help with the detection of dataset usage, a pipeline \\url{https://github.com/TheoSourget/Public_Medical_Datasets_References} using OpenAlex and full-text analysis, and a PDF annotation software \\url{https://github.com/TheoSourget/pdf_annotator} used in our study to manually label the presence of datasets. We applied both tools on a study of the usage of 20 publicly available medical datasets in papers from MICCAI and MIDL. We compute the proportion and the evolution between 2013 and 2023 of 3 types of presence in a paper: cited, mentioned in the full text, cited and mentioned. Our findings demonstrate the concentration of the usage of a limited set of datasets. We also highlight different citing practices, making the automation of tracking difficult.","sentences":["Medical imaging papers often focus on methodology, but the quality of the algorithms and the validity of the conclusions are highly dependent on the datasets used.","As creating datasets requires a lot of effort, researchers often use publicly available datasets, there is however no adopted standard for citing the datasets used in scientific papers, leading to difficulty in tracking dataset usage.","In this work, we present two open-source tools we created that could help with the detection of dataset usage, a pipeline \\url{https://github.com/TheoSourget/Public_Medical_Datasets_References} using OpenAlex and full-text analysis, and a PDF annotation software \\url{https://github.com/TheoSourget/pdf_annotator} used in our study to manually label the presence of datasets.","We applied both tools on a study of the usage of 20 publicly available medical datasets in papers from MICCAI and MIDL.","We compute the proportion and the evolution between 2013 and 2023 of 3 types of presence in a paper: cited, mentioned in the full text, cited and mentioned.","Our findings demonstrate the concentration of the usage of a limited set of datasets.","We also highlight different citing practices, making the automation of tracking difficult."],"url":"http://arxiv.org/abs/2402.03003v1"}
{"created":"2024-02-05 12:56:22","title":"Putting Context in Context: the Impact of Discussion Structure on Text Classification","abstract":"Current text classification approaches usually focus on the content to be classified. Contextual aspects (both linguistic and extra-linguistic) are usually neglected, even in tasks based on online discussions. Still in many cases the multi-party and multi-turn nature of the context from which these elements are selected can be fruitfully exploited. In this work, we propose a series of experiments on a large dataset for stance detection in English, in which we evaluate the contribution of different types of contextual information, i.e. linguistic, structural and temporal, by feeding them as natural language input into a transformer-based model. We also experiment with different amounts of training data and analyse the topology of local discussion networks in a privacy-compliant way. Results show that structural information can be highly beneficial to text classification but only under certain circumstances (e.g. depending on the amount of training data and on discussion chain complexity). Indeed, we show that contextual information on smaller datasets from other classification tasks does not yield significant improvements. Our framework, based on local discussion networks, allows the integration of structural information, while minimising user profiling, thus preserving their privacy.","sentences":["Current text classification approaches usually focus on the content to be classified.","Contextual aspects (both linguistic and extra-linguistic) are usually neglected, even in tasks based on online discussions.","Still in many cases the multi-party and multi-turn nature of the context from which these elements are selected can be fruitfully exploited.","In this work, we propose a series of experiments on a large dataset for stance detection in English, in which we evaluate the contribution of different types of contextual information, i.e. linguistic, structural and temporal, by feeding them as natural language input into a transformer-based model.","We also experiment with different amounts of training data and analyse the topology of local discussion networks in a privacy-compliant way.","Results show that structural information can be highly beneficial to text classification but only under certain circumstances (e.g. depending on the amount of training data and on discussion chain complexity).","Indeed, we show that contextual information on smaller datasets from other classification tasks does not yield significant improvements.","Our framework, based on local discussion networks, allows the integration of structural information, while minimising user profiling, thus preserving their privacy."],"url":"http://arxiv.org/abs/2402.02975v1"}
{"created":"2024-02-05 12:50:30","title":"Retrieval-Augmented Score Distillation for Text-to-3D Generation","abstract":"Text-to-3D generation has achieved significant success by incorporating powerful 2D diffusion models, but insufficient 3D prior knowledge also leads to the inconsistency of 3D geometry. Recently, since large-scale multi-view datasets have been released, fine-tuning the diffusion model on the multi-view datasets becomes a mainstream to solve the 3D inconsistency problem. However, it has confronted with fundamental difficulties regarding the limited quality and diversity of 3D data, compared with 2D data. To sidestep these trade-offs, we explore a retrieval-augmented approach tailored for score distillation, dubbed RetDream. We postulate that both expressiveness of 2D diffusion models and geometric consistency of 3D assets can be fully leveraged by employing the semantically relevant assets directly within the optimization process. To this end, we introduce novel framework for retrieval-based quality enhancement in text-to-3D generation. We leverage the retrieved asset to incorporate its geometric prior in the variational objective and adapt the diffusion model's 2D prior toward view consistency, achieving drastic improvements in both geometry and fidelity of generated scenes. We conduct extensive experiments to demonstrate that RetDream exhibits superior quality with increased geometric consistency. Project page is available at https://ku-cvlab.github.io/RetDream/.","sentences":["Text-to-3D generation has achieved significant success by incorporating powerful 2D diffusion models, but insufficient 3D prior knowledge also leads to the inconsistency of 3D geometry.","Recently, since large-scale multi-view datasets have been released, fine-tuning the diffusion model on the multi-view datasets becomes a mainstream to solve the 3D inconsistency problem.","However, it has confronted with fundamental difficulties regarding the limited quality and diversity of 3D data, compared with 2D data.","To sidestep these trade-offs, we explore a retrieval-augmented approach tailored for score distillation, dubbed RetDream.","We postulate that both expressiveness of 2D diffusion models and geometric consistency of 3D assets can be fully leveraged by employing the semantically relevant assets directly within the optimization process.","To this end, we introduce novel framework for retrieval-based quality enhancement in text-to-3D generation.","We leverage the retrieved asset to incorporate its geometric prior in the variational objective and adapt the diffusion model's 2D prior toward view consistency, achieving drastic improvements in both geometry and fidelity of generated scenes.","We conduct extensive experiments to demonstrate that RetDream exhibits superior quality with increased geometric consistency.","Project page is available at https://ku-cvlab.github.io/RetDream/."],"url":"http://arxiv.org/abs/2402.02972v1"}
{"created":"2024-02-05 12:47:09","title":"Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives","abstract":"Foundation models have indeed made a profound impact on various fields, emerging as pivotal components that significantly shape the capabilities of intelligent systems. In the context of intelligent vehicles, leveraging the power of foundation models has proven to be transformative, offering notable advancements in visual understanding. Equipped with multi-modal and multi-task learning capabilities, multi-modal multi-task visual understanding foundation models (MM-VUFMs) effectively process and fuse data from diverse modalities and simultaneously handle various driving-related tasks with powerful adaptability, contributing to a more holistic understanding of the surrounding scene. In this survey, we present a systematic analysis of MM-VUFMs specifically designed for road scenes. Our objective is not only to provide a comprehensive overview of common practices, referring to task-specific models, unified multi-modal models, unified multi-task models, and foundation model prompting techniques, but also to highlight their advanced capabilities in diverse learning paradigms. These paradigms include open-world understanding, efficient transfer for road scenes, continual learning, interactive and generative capability. Moreover, we provide insights into key challenges and future trends, such as closed-loop driving systems, interpretability, embodied driving agents, and world models. To facilitate researchers in staying abreast of the latest developments in MM-VUFMs for road scenes, we have established a continuously updated repository at https://github.com/rolsheng/MM-VUFM4DS","sentences":["Foundation models have indeed made a profound impact on various fields, emerging as pivotal components that significantly shape the capabilities of intelligent systems.","In the context of intelligent vehicles, leveraging the power of foundation models has proven to be transformative, offering notable advancements in visual understanding.","Equipped with multi-modal and multi-task learning capabilities, multi-modal multi-task visual understanding foundation models (MM-VUFMs) effectively process and fuse data from diverse modalities and simultaneously handle various driving-related tasks with powerful adaptability, contributing to a more holistic understanding of the surrounding scene.","In this survey, we present a systematic analysis of MM-VUFMs specifically designed for road scenes.","Our objective is not only to provide a comprehensive overview of common practices, referring to task-specific models, unified multi-modal models, unified multi-task models, and foundation model prompting techniques, but also to highlight their advanced capabilities in diverse learning paradigms.","These paradigms include open-world understanding, efficient transfer for road scenes, continual learning, interactive and generative capability.","Moreover, we provide insights into key challenges and future trends, such as closed-loop driving systems, interpretability, embodied driving agents, and world models.","To facilitate researchers in staying abreast of the latest developments in MM-VUFMs for road scenes, we have established a continuously updated repository at https://github.com/rolsheng/MM-VUFM4DS"],"url":"http://arxiv.org/abs/2402.02968v1"}
{"created":"2024-02-05 12:42:21","title":"Mixed Noise and Posterior Estimation with Conditional DeepGEM","abstract":"Motivated by indirect measurements and applications from nanometrology with a mixed noise model, we develop a novel algorithm for jointly estimating the posterior and the noise parameters in Bayesian inverse problems. We propose to solve the problem by an expectation maximization (EM) algorithm. Based on the current noise parameters, we learn in the E-step a conditional normalizing flow that approximates the posterior. In the M-step, we propose to find the noise parameter updates again by an EM algorithm, which has analytical formulas. We compare the training of the conditional normalizing flow with the forward and reverse KL, and show that our model is able to incorporate information from many measurements, unlike previous approaches.","sentences":["Motivated by indirect measurements and applications from nanometrology with a mixed noise model, we develop a novel algorithm for jointly estimating the posterior and the noise parameters in Bayesian inverse problems.","We propose to solve the problem by an expectation maximization (EM) algorithm.","Based on the current noise parameters, we learn in the E-step a conditional normalizing flow that approximates the posterior.","In the M-step, we propose to find the noise parameter updates again by an EM algorithm, which has analytical formulas.","We compare the training of the conditional normalizing flow with the forward and reverse KL, and show that our model is able to incorporate information from many measurements, unlike previous approaches."],"url":"http://arxiv.org/abs/2402.02964v1"}
{"created":"2024-02-05 12:25:02","title":"Semantic Entropy Can Simultaneously Benefit Transmission Efficiency and Channel Security of Wireless Semantic Communications","abstract":"Recently proliferated deep learning-based semantic communications (DLSC) focus on how transmitted symbols efficiently convey a desired meaning to the destination. However, the sensitivity of neural models and the openness of wireless channels cause the DLSC system to be extremely fragile to various malicious attacks. This inspires us to ask a question: ``Can we further exploit the advantages of transmission efficiency in wireless semantic communications while also alleviating its security disadvantages?''. Keeping this in mind, we propose SemEntropy, a novel method that answers the above question by exploring the semantics of data for both adaptive transmission and physical layer encryption. Specifically, we first introduce semantic entropy, which indicates the expectation of various semantic scores regarding the transmission goal of the DLSC. Equipped with such semantic entropy, we can dynamically assign informative semantics to Orthogonal Frequency Division Multiplexing (OFDM) subcarriers with better channel conditions in a fine-grained manner. We also use the entropy to guide semantic key generation to safeguard communications over open wireless channels. By doing so, both transmission efficiency and channel security can be simultaneously improved. Extensive experiments over various benchmarks show the effectiveness of the proposed SemEntropy. We discuss the reason why our proposed method benefits secure transmission of DLSC, and also give some interesting findings, e.g., SemEntropy can keep the semantic accuracy remain 95\\% with 60\\% less transmission.","sentences":["Recently proliferated deep learning-based semantic communications (DLSC) focus on how transmitted symbols efficiently convey a desired meaning to the destination.","However, the sensitivity of neural models and the openness of wireless channels cause the DLSC system to be extremely fragile to various malicious attacks.","This inspires us to ask a question: ``Can we further exploit the advantages of transmission efficiency in wireless semantic communications while also alleviating its security disadvantages?''.","Keeping this in mind, we propose SemEntropy, a novel method that answers the above question by exploring the semantics of data for both adaptive transmission and physical layer encryption.","Specifically, we first introduce semantic entropy, which indicates the expectation of various semantic scores regarding the transmission goal of the DLSC.","Equipped with such semantic entropy, we can dynamically assign informative semantics to Orthogonal Frequency Division Multiplexing (OFDM) subcarriers with better channel conditions in a fine-grained manner.","We also use the entropy to guide semantic key generation to safeguard communications over open wireless channels.","By doing so, both transmission efficiency and channel security can be simultaneously improved.","Extensive experiments over various benchmarks show the effectiveness of the proposed SemEntropy.","We discuss the reason why our proposed method benefits secure transmission of DLSC, and also give some interesting findings, e.g., SemEntropy can keep the semantic accuracy remain 95\\% with 60\\% less transmission."],"url":"http://arxiv.org/abs/2402.02950v1"}
{"created":"2024-02-05 12:21:16","title":"Kernel PCA for Out-of-Distribution Detection","abstract":"Out-of-Distribution (OoD) detection is vital for the reliability of Deep Neural Networks (DNNs). Existing works have shown the insufficiency of Principal Component Analysis (PCA) straightforwardly applied on the features of DNNs in detecting OoD data from In-Distribution (InD) data. The failure of PCA suggests that the network features residing in OoD and InD are not well separated by simply proceeding in a linear subspace, which instead can be resolved through proper nonlinear mappings. In this work, we leverage the framework of Kernel PCA (KPCA) for OoD detection, seeking subspaces where OoD and InD features are allocated with significantly different patterns. We devise two feature mappings that induce non-linear kernels in KPCA to advocate the separability between InD and OoD data in the subspace spanned by the principal components. Given any test sample, the reconstruction error in such subspace is then used to efficiently obtain the detection result with $\\mathcal{O}(1)$ time complexity in inference. Extensive empirical results on multiple OoD data sets and network structures verify the superiority of our KPCA-based detector in efficiency and efficacy with state-of-the-art OoD detection performances.","sentences":["Out-of-Distribution (OoD) detection is vital for the reliability of Deep Neural Networks (DNNs).","Existing works have shown the insufficiency of Principal Component Analysis (PCA) straightforwardly applied on the features of DNNs in detecting OoD data from In-Distribution (InD) data.","The failure of PCA suggests that the network features residing in OoD and InD are not well separated by simply proceeding in a linear subspace, which instead can be resolved through proper nonlinear mappings.","In this work, we leverage the framework of Kernel PCA (KPCA) for OoD detection, seeking subspaces where OoD and InD features are allocated with significantly different patterns.","We devise two feature mappings that induce non-linear kernels in KPCA to advocate the separability between InD and OoD data in the subspace spanned by the principal components.","Given any test sample, the reconstruction error in such subspace is then used to efficiently obtain the detection result with $\\mathcal{O}(1)$ time complexity in inference.","Extensive empirical results on multiple OoD data sets and network structures verify the superiority of our KPCA-based detector in efficiency and efficacy with state-of-the-art OoD detection performances."],"url":"http://arxiv.org/abs/2402.02949v1"}
{"created":"2024-02-05 12:00:31","title":"Design and Implementation of an Automated Disaster-recovery System for a Kubernetes Cluster Using LSTM","abstract":"With the increasing importance of data in the modern business environment, effective data man-agement and protection strategies are gaining increasing research attention. Data protection in a cloud environment is crucial for safeguarding information assets and maintaining sustainable services. This study introduces a system structure that integrates Kubernetes management plat-forms with backup and restoration tools. This system is designed to immediately detect disasters and automatically recover applications from another kubernetes cluster. The experimental results show that this system executes the restoration process within 15 s without human intervention, enabling rapid recovery. This, in turn, significantly reduces the potential for delays and errors compared with manual recovery processes, thereby enhancing data management and recovery ef-ficiency in cloud environments. Moreover, our research model predicts the CPU utilization of the cluster using Long Short-Term Memory (LSTM). The necessity of scheduling through this predict is made clearer through comparison with experiments without scheduling, demonstrating its ability to prevent performance degradation. This research highlights the efficiency and necessity of automatic recovery systems in cloud environments, setting a new direction for future research.","sentences":["With the increasing importance of data in the modern business environment, effective data man-agement and protection strategies are gaining increasing research attention.","Data protection in a cloud environment is crucial for safeguarding information assets and maintaining sustainable services.","This study introduces a system structure that integrates Kubernetes management plat-forms with backup and restoration tools.","This system is designed to immediately detect disasters and automatically recover applications from another kubernetes cluster.","The experimental results show that this system executes the restoration process within 15 s without human intervention, enabling rapid recovery.","This, in turn, significantly reduces the potential for delays and errors compared with manual recovery processes, thereby enhancing data management and recovery ef-ficiency in cloud environments.","Moreover, our research model predicts the CPU utilization of the cluster using Long Short-Term Memory (LSTM).","The necessity of scheduling through this predict is made clearer through comparison with experiments without scheduling, demonstrating its ability to prevent performance degradation.","This research highlights the efficiency and necessity of automatic recovery systems in cloud environments, setting a new direction for future research."],"url":"http://arxiv.org/abs/2402.02938v1"}
{"created":"2024-02-05 11:55:50","title":"InterpretCC: Conditional Computation for Inherently Interpretable Neural Networks","abstract":"Real-world interpretability for neural networks is a tradeoff between three concerns: 1) it requires humans to trust the explanation approximation (e.g. post-hoc approaches), 2) it compromises the understandability of the explanation (e.g. automatically identified feature masks), and 3) it compromises the model performance (e.g. decision trees). These shortcomings are unacceptable for human-facing domains, like education, healthcare, or natural language, which require trustworthy explanations, actionable interpretations, and accurate predictions. In this work, we present InterpretCC (interpretable conditional computation), a family of interpretable-by-design neural networks that guarantee human-centric interpretability while maintaining comparable performance to state-of-the-art models by adaptively and sparsely activating features before prediction. We extend this idea into an interpretable mixture-of-experts model, that allows humans to specify topics of interest, discretely separates the feature space for each data point into topical subnetworks, and adaptively and sparsely activates these topical subnetworks. We demonstrate variations of the InterpretCC architecture for text and tabular data across several real-world benchmarks: six online education courses, news classification, breast cancer diagnosis, and review sentiment.","sentences":["Real-world interpretability for neural networks is a tradeoff between three concerns: 1) it requires humans to trust the explanation approximation (e.g. post-hoc approaches), 2) it compromises the understandability of the explanation (e.g. automatically identified feature masks), and 3) it compromises the model performance (e.g. decision trees).","These shortcomings are unacceptable for human-facing domains, like education, healthcare, or natural language, which require trustworthy explanations, actionable interpretations, and accurate predictions.","In this work, we present InterpretCC (interpretable conditional computation), a family of interpretable-by-design neural networks that guarantee human-centric interpretability while maintaining comparable performance to state-of-the-art models by adaptively and sparsely activating features before prediction.","We extend this idea into an interpretable mixture-of-experts model, that allows humans to specify topics of interest, discretely separates the feature space for each data point into topical subnetworks, and adaptively and sparsely activates these topical subnetworks.","We demonstrate variations of the InterpretCC architecture for text and tabular data across several real-world benchmarks: six online education courses, news classification, breast cancer diagnosis, and review sentiment."],"url":"http://arxiv.org/abs/2402.02933v1"}
{"created":"2024-02-05 11:32:13","title":"A Computational Model for the Assessment of Mutual Intelligibility Among Closely Related Languages","abstract":"Closely related languages show linguistic similarities that allow speakers of one language to understand speakers of another language without having actively learned it. Mutual intelligibility varies in degree and is typically tested in psycholinguistic experiments. To study mutual intelligibility computationally, we propose a computer-assisted method using the Linear Discriminative Learner, a computational model developed to approximate the cognitive processes by which humans learn languages, which we expand with multilingual semantic vectors and multilingual sound classes. We test the model on cognate data from German, Dutch, and English, three closely related Germanic languages. We find that our model's comprehension accuracy depends on 1) the automatic trimming of inflections and 2) the language pair for which comprehension is tested. Our multilingual modelling approach does not only offer new methodological findings for automatic testing of mutual intelligibility across languages but also extends the use of Linear Discriminative Learning to multilingual settings.","sentences":["Closely related languages show linguistic similarities that allow speakers of one language to understand speakers of another language without having actively learned it.","Mutual intelligibility varies in degree and is typically tested in psycholinguistic experiments.","To study mutual intelligibility computationally, we propose a computer-assisted method using the Linear Discriminative Learner, a computational model developed to approximate the cognitive processes by which humans learn languages, which we expand with multilingual semantic vectors and multilingual sound classes.","We test the model on cognate data from German, Dutch, and English, three closely related Germanic languages.","We find that our model's comprehension accuracy depends on 1) the automatic trimming of inflections and 2) the language pair for which comprehension is tested.","Our multilingual modelling approach does not only offer new methodological findings for automatic testing of mutual intelligibility across languages but also extends the use of Linear Discriminative Learning to multilingual settings."],"url":"http://arxiv.org/abs/2402.02915v1"}
{"created":"2024-02-05 10:57:48","title":"Exploring Federated Self-Supervised Learning for General Purpose Audio Understanding","abstract":"The integration of Federated Learning (FL) and Self-supervised Learning (SSL) offers a unique and synergetic combination to exploit the audio data for general-purpose audio understanding, without compromising user data privacy. However, rare efforts have been made to investigate the SSL models in the FL regime for general-purpose audio understanding, especially when the training data is generated by large-scale heterogeneous audio sources. In this paper, we evaluate the performance of feature-matching and predictive audio-SSL techniques when integrated into large-scale FL settings simulated with non-independently identically distributed (non-iid) data. We propose a novel Federated SSL (F-SSL) framework, dubbed FASSL, that enables learning intermediate feature representations from large-scale decentralized heterogeneous clients, holding unlabelled audio data. Our study has found that audio F-SSL approaches perform on par with the centralized audio-SSL approaches on the audio-retrieval task. Extensive experiments demonstrate the effectiveness and significance of FASSL as it assists in obtaining the optimal global model for state-of-the-art FL aggregation methods.","sentences":["The integration of Federated Learning (FL) and Self-supervised Learning (SSL) offers a unique and synergetic combination to exploit the audio data for general-purpose audio understanding, without compromising user data privacy.","However, rare efforts have been made to investigate the SSL models in the FL regime for general-purpose audio understanding, especially when the training data is generated by large-scale heterogeneous audio sources.","In this paper, we evaluate the performance of feature-matching and predictive audio-SSL techniques when integrated into large-scale FL settings simulated with non-independently identically distributed (non-iid) data.","We propose a novel Federated SSL (F-SSL) framework, dubbed FASSL, that enables learning intermediate feature representations from large-scale decentralized heterogeneous clients, holding unlabelled audio data.","Our study has found that audio F-SSL approaches perform on par with the centralized audio-SSL approaches on the audio-retrieval task.","Extensive experiments demonstrate the effectiveness and significance of FASSL as it assists in obtaining the optimal global model for state-of-the-art FL aggregation methods."],"url":"http://arxiv.org/abs/2402.02889v1"}
{"created":"2024-02-05 10:54:17","title":"Time-Distributed Backdoor Attacks on Federated Spiking Learning","abstract":"This paper investigates the vulnerability of spiking neural networks (SNNs) and federated learning (FL) to backdoor attacks using neuromorphic data. Despite the efficiency of SNNs and the privacy advantages of FL, particularly in low-powered devices, we demonstrate that these systems are susceptible to such attacks. We first assess the viability of using FL with SNNs using neuromorphic data, showing its potential usage. Then, we evaluate the transferability of known FL attack methods to SNNs, finding that these lead to suboptimal attack performance. Therefore, we explore backdoor attacks involving single and multiple attackers to improve the attack performance. Our primary contribution is developing a novel attack strategy tailored to SNNs and FL, which distributes the backdoor trigger temporally and across malicious devices, enhancing the attack's effectiveness and stealthiness. In the best case, we achieve a 100 attack success rate, 0.13 MSE, and 98.9 SSIM. Moreover, we adapt and evaluate an existing defense against backdoor attacks, revealing its inadequacy in protecting SNNs. This study underscores the need for robust security measures in deploying SNNs and FL, particularly in the context of backdoor attacks.","sentences":["This paper investigates the vulnerability of spiking neural networks (SNNs) and federated learning (FL) to backdoor attacks using neuromorphic data.","Despite the efficiency of SNNs and the privacy advantages of FL, particularly in low-powered devices, we demonstrate that these systems are susceptible to such attacks.","We first assess the viability of using FL with SNNs using neuromorphic data, showing its potential usage.","Then, we evaluate the transferability of known FL attack methods to SNNs, finding that these lead to suboptimal attack performance.","Therefore, we explore backdoor attacks involving single and multiple attackers to improve the attack performance.","Our primary contribution is developing a novel attack strategy tailored to SNNs and FL, which distributes the backdoor trigger temporally and across malicious devices, enhancing the attack's effectiveness and stealthiness.","In the best case, we achieve a 100 attack success rate, 0.13 MSE, and 98.9 SSIM.","Moreover, we adapt and evaluate an existing defense against backdoor attacks, revealing its inadequacy in protecting SNNs.","This study underscores the need for robust security measures in deploying SNNs and FL, particularly in the context of backdoor attacks."],"url":"http://arxiv.org/abs/2402.02886v1"}
{"created":"2024-02-05 10:45:39","title":"Feedback to the European Data Protection Board's Guidelines 2/2023 on Technical Scope of Art. 5(3) of ePrivacy Directive","abstract":"We very much welcome the EDPB's Guidelines. Please find hereunder our feedback to the Guidelines 2/2023 on Technical Scope of Art. 5(3) of ePrivacy Directive. Our comments are presented after a quotation from the proposed text by the EDPB in a box.","sentences":["We very much welcome the EDPB's Guidelines.","Please find hereunder our feedback to the Guidelines 2/2023 on Technical Scope of Art.","5(3) of ePrivacy Directive.","Our comments are presented after a quotation from the proposed text by the EDPB in a box."],"url":"http://arxiv.org/abs/2402.02877v1"}
{"created":"2024-02-05 10:18:15","title":"Deep autoregressive density nets vs neural ensembles for model-based offline reinforcement learning","abstract":"We consider the problem of offline reinforcement learning where only a set of system transitions is made available for policy optimization. Following recent advances in the field, we consider a model-based reinforcement learning algorithm that infers the system dynamics from the available data and performs policy optimization on imaginary model rollouts. This approach is vulnerable to exploiting model errors which can lead to catastrophic failures on the real system. The standard solution is to rely on ensembles for uncertainty heuristics and to avoid exploiting the model where it is too uncertain. We challenge the popular belief that we must resort to ensembles by showing that better performance can be obtained with a single well-calibrated autoregressive model on the D4RL benchmark. We also analyze static metrics of model-learning and conclude on the important model properties for the final performance of the agent.","sentences":["We consider the problem of offline reinforcement learning where only a set of system transitions is made available for policy optimization.","Following recent advances in the field, we consider a model-based reinforcement learning algorithm that infers the system dynamics from the available data and performs policy optimization on imaginary model rollouts.","This approach is vulnerable to exploiting model errors which can lead to catastrophic failures on the real system.","The standard solution is to rely on ensembles for uncertainty heuristics and to avoid exploiting the model where it is too uncertain.","We challenge the popular belief that we must resort to ensembles by showing that better performance can be obtained with a single well-calibrated autoregressive model on the D4RL benchmark.","We also analyze static metrics of model-learning and conclude on the important model properties for the final performance of the agent."],"url":"http://arxiv.org/abs/2402.02858v1"}
{"created":"2024-02-05 10:07:37","title":"Repeated-Root Cyclic Codes with Optimal Parameters or Best Parameters Known","abstract":"Cyclic codes are the most studied subclass of linear codes and widely used in data storage and communication systems. Many cyclic codes have optimal parameters or the best parameters known. They are divided into simple-root cyclic codes and repeated-root cyclic codes. Although there are a huge number of references on cyclic codes, few of them are on repeated-root cyclic codes. Hence, repeated-root cyclic codes are rarely studied. There are a few families of distance-optimal repeated-root binary and $p$-ary cyclic codes for odd prime $p$ in the literature. However, it is open whether there exists an infinite family of distance-optimal repeated-root cyclic codes over $\\bF_q$ for each even $q \\geq 4$.   In this paper, three infinite families of distance-optimal repeated-root cyclic codes with minimum distance 3 or 4 are constructed; two other infinite families of repeated-root cyclic codes with minimum distance 3 or 4 are developed; four infinite families of repeated-root cyclic codes with minimum distance 6 or 8 are presented; and two infinite families of repeated-root binary cyclic codes with parameters $[2n, k, d \\geq (n-1)/\\log_2 n]$, where $n=2^m-1$ and $k \\geq n$, are constructed. In addition, 27 repeated-root cyclic codes of length up to $254$ over $\\bF_q$ for $q \\in \\{2, 4, 8\\}$ with optimal parameters or best parameters known are obtained in this paper. The results of this paper show that repeated-root cyclic codes could be very attractive and are worth of further investigation.","sentences":["Cyclic codes are the most studied subclass of linear codes and widely used in data storage and communication systems.","Many cyclic codes have optimal parameters or the best parameters known.","They are divided into simple-root cyclic codes and repeated-root cyclic codes.","Although there are a huge number of references on cyclic codes, few of them are on repeated-root cyclic codes.","Hence, repeated-root cyclic codes are rarely studied.","There are a few families of distance-optimal repeated-root binary and $p$-ary cyclic codes for odd prime $p$ in the literature.","However, it is open whether there exists an infinite family of distance-optimal repeated-root cyclic codes over $\\bF_q$ for each even $q \\geq 4$.   ","In this paper, three infinite families of distance-optimal repeated-root cyclic codes with minimum distance 3 or 4 are constructed; two other infinite families of repeated-root cyclic codes with minimum distance 3 or 4 are developed; four infinite families of repeated-root cyclic codes with minimum distance 6 or 8 are presented; and two infinite families of repeated-root binary cyclic codes with parameters $[2n, k, d \\geq (n-1)/\\log_2 n]$, where $n=2^m-1$ and $k \\geq n$, are constructed.","In addition, 27 repeated-root cyclic codes of length up to $254$ over $\\bF_q$ for $q \\in \\{2, 4, 8\\}$ with optimal parameters or best parameters known are obtained in this paper.","The results of this paper show that repeated-root cyclic codes could be very attractive and are worth of further investigation."],"url":"http://arxiv.org/abs/2402.02853v1"}
{"created":"2024-02-05 10:06:24","title":"Enhancing Compositional Generalization via Compositional Feature Alignment","abstract":"Real-world applications of machine learning models often confront data distribution shifts, wherein discrepancies exist between the training and test data distributions. In the common multi-domain multi-class setup, as the number of classes and domains scales up, it becomes infeasible to gather training data for every domain-class combination. This challenge naturally leads the quest for models with Compositional Generalization (CG) ability, where models can generalize to unseen domain-class combinations. To delve into the CG challenge, we develop CG-Bench, a suite of CG benchmarks derived from existing real-world image datasets, and observe that the prevalent pretraining-finetuning paradigm on foundational models, such as CLIP and DINOv2, struggles with the challenge. To address this challenge, we propose Compositional Feature Alignment (CFA), a simple two-stage finetuning technique that i) learns two orthogonal linear heads on a pretrained encoder with respect to class and domain labels, and ii) fine-tunes the encoder with the newly learned head frozen. We theoretically and empirically justify that CFA encourages compositional feature learning of pretrained models. We further conduct extensive experiments on CG-Bench for CLIP and DINOv2, two powerful pretrained vision foundation models. Experiment results show that CFA outperforms common finetuning techniques in compositional generalization, corroborating CFA's efficacy in compositional feature learning.","sentences":["Real-world applications of machine learning models often confront data distribution shifts, wherein discrepancies exist between the training and test data distributions.","In the common multi-domain multi-class setup, as the number of classes and domains scales up, it becomes infeasible to gather training data for every domain-class combination.","This challenge naturally leads the quest for models with Compositional Generalization (CG) ability, where models can generalize to unseen domain-class combinations.","To delve into the CG challenge, we develop CG-Bench, a suite of CG benchmarks derived from existing real-world image datasets, and observe that the prevalent pretraining-finetuning paradigm on foundational models, such as CLIP and DINOv2, struggles with the challenge.","To address this challenge, we propose Compositional Feature Alignment (CFA), a simple two-stage finetuning technique that i) learns two orthogonal linear heads on a pretrained encoder with respect to class and domain labels, and ii) fine-tunes the encoder with the newly learned head frozen.","We theoretically and empirically justify that CFA encourages compositional feature learning of pretrained models.","We further conduct extensive experiments on CG-Bench for CLIP and DINOv2, two powerful pretrained vision foundation models.","Experiment results show that CFA outperforms common finetuning techniques in compositional generalization, corroborating CFA's efficacy in compositional feature learning."],"url":"http://arxiv.org/abs/2402.02851v1"}
{"created":"2024-02-05 09:48:46","title":"HAPI-FHIR Server Implementation to Enhancing Interoperability among Primary Care Health Information Systems in Sri Lanka: Review of the Technical Use Case","abstract":"This review underscores the vital role of interoperability in digital health, advocating for a standardized framework. It focuses on implementing a Fast Healthcare Interoperability Resources (FHIR) server, addressing technical, semantic, and process challenges. FHIR's adaptability ensures uniformity within Primary Care Health Information Systems, fostering interoperability. Patient data management complexities highlight the pivotal role of semantic interoperability in seamless patient care. FHIR standards enhance these efforts, offering multiple pathways for data search. The ADR-guided FHIR server implementation systematically addresses challenges related to patient identity, biometrics, and data security. The detailed development phases emphasize architecture, API integration, and security. The concluding stages incorporate forward-looking approaches, including HHIMS Synthetic Dataset testing. Envisioning FHIR integration as transformative, it anticipates a responsive healthcare environment aligned with the evolving digital health landscape, ensuring comprehensive, dynamic, and interconnected systems for efficient data exchange and access.","sentences":["This review underscores the vital role of interoperability in digital health, advocating for a standardized framework.","It focuses on implementing a Fast Healthcare Interoperability Resources (FHIR) server, addressing technical, semantic, and process challenges.","FHIR's adaptability ensures uniformity within Primary Care Health Information Systems, fostering interoperability.","Patient data management complexities highlight the pivotal role of semantic interoperability in seamless patient care.","FHIR standards enhance these efforts, offering multiple pathways for data search.","The ADR-guided FHIR server implementation systematically addresses challenges related to patient identity, biometrics, and data security.","The detailed development phases emphasize architecture, API integration, and security.","The concluding stages incorporate forward-looking approaches, including HHIMS Synthetic Dataset testing.","Envisioning FHIR integration as transformative, it anticipates a responsive healthcare environment aligned with the evolving digital health landscape, ensuring comprehensive, dynamic, and interconnected systems for efficient data exchange and access."],"url":"http://arxiv.org/abs/2402.02838v1"}
{"created":"2024-02-05 09:18:49","title":"SynthVision -- Harnessing Minimal Input for Maximal Output in Computer Vision Models using Synthetic Image data","abstract":"Rapid development of disease detection computer vision models is vital in response to urgent medical crises like epidemics or events of bioterrorism. However, traditional data gathering methods are too slow for these scenarios necessitating innovative approaches to generate reliable models quickly from minimal data. We demonstrate our new approach by building a comprehensive computer vision model for detecting Human Papilloma Virus Genital warts using only synthetic data. In our study, we employed a two phase experimental design using diffusion models. In the first phase diffusion models were utilized to generate a large number of diverse synthetic images from 10 HPV guide images explicitly focusing on accurately depicting genital warts. The second phase involved the training and testing vision model using this synthetic dataset. This method aimed to assess the effectiveness of diffusion models in rapidly generating high quality training data and the subsequent impact on the vision model performance in medical image recognition. The study findings revealed significant insights into the performance of the vision model trained on synthetic images generated through diffusion models. The vision model showed exceptional performance in accurately identifying cases of genital warts. It achieved an accuracy rate of 96% underscoring its effectiveness in medical image classification. For HPV cases the model demonstrated a high precision of 99% and a recall of 94%. In normal cases the precision was 95% with an impressive recall of 99%. These metrics indicate the model capability to correctly identify true positive cases and minimize false positives. The model achieved an F1 Score of 96% for HPV cases and 97% for normal cases. The high F1 Score across both categories highlights the balanced nature of the model precision and recall ensuring reliability and robustness in its predictions.","sentences":["Rapid development of disease detection computer vision models is vital in response to urgent medical crises like epidemics or events of bioterrorism.","However, traditional data gathering methods are too slow for these scenarios necessitating innovative approaches to generate reliable models quickly from minimal data.","We demonstrate our new approach by building a comprehensive computer vision model for detecting Human Papilloma Virus Genital warts using only synthetic data.","In our study, we employed a two phase experimental design using diffusion models.","In the first phase diffusion models were utilized to generate a large number of diverse synthetic images from 10 HPV guide images explicitly focusing on accurately depicting genital warts.","The second phase involved the training and testing vision model using this synthetic dataset.","This method aimed to assess the effectiveness of diffusion models in rapidly generating high quality training data and the subsequent impact on the vision model performance in medical image recognition.","The study findings revealed significant insights into the performance of the vision model trained on synthetic images generated through diffusion models.","The vision model showed exceptional performance in accurately identifying cases of genital warts.","It achieved an accuracy rate of 96% underscoring its effectiveness in medical image classification.","For HPV cases the model demonstrated a high precision of 99% and a recall of 94%.","In normal cases the precision was 95% with an impressive recall of 99%.","These metrics indicate the model capability to correctly identify true positive cases and minimize false positives.","The model achieved an F1 Score of 96% for HPV cases and 97% for normal cases.","The high F1 Score across both categories highlights the balanced nature of the model precision and recall ensuring reliability and robustness in its predictions."],"url":"http://arxiv.org/abs/2402.02826v1"}
{"created":"2024-02-05 09:10:32","title":"Evading Data Contamination Detection for Language Models is (too) Easy","abstract":"Large language models are widespread, with their performance on benchmarks frequently guiding user preferences for one model over another. However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements. While recently developed contamination detection methods try to address this issue, they overlook the possibility of deliberate contamination by malicious model providers aiming to evade detection. We argue that this setting is of crucial importance as it casts doubt on the reliability of public benchmarks. To more rigorously study this issue, we propose a categorization of both model providers and contamination detection methods. This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods.","sentences":["Large language models are widespread, with their performance on benchmarks frequently guiding user preferences for one model over another.","However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements.","While recently developed contamination detection methods try to address this issue, they overlook the possibility of deliberate contamination by malicious model providers aiming to evade detection.","We argue that this setting is of crucial importance as it casts doubt on the reliability of public benchmarks.","To more rigorously study this issue, we propose a categorization of both model providers and contamination detection methods.","This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods."],"url":"http://arxiv.org/abs/2402.02823v1"}
{"created":"2024-02-05 09:06:57","title":"Revisiting VAE for Unsupervised Time Series Anomaly Detection: A Frequency Perspective","abstract":"Time series Anomaly Detection (AD) plays a crucial role for web systems. Various web systems rely on time series data to monitor and identify anomalies in real time, as well as to initiate diagnosis and remediation procedures. Variational Autoencoders (VAEs) have gained popularity in recent decades due to their superior de-noising capabilities, which are useful for anomaly detection. However, our study reveals that VAE-based methods face challenges in capturing long-periodic heterogeneous patterns and detailed short-periodic trends simultaneously. To address these challenges, we propose Frequency-enhanced Conditional Variational Autoencoder (FCVAE), a novel unsupervised AD method for univariate time series. To ensure an accurate AD, FCVAE exploits an innovative approach to concurrently integrate both the global and local frequency features into the condition of Conditional Variational Autoencoder (CVAE) to significantly increase the accuracy of reconstructing the normal data. Together with a carefully designed \"target attention\" mechanism, our approach allows the model to pick the most useful information from the frequency domain for better short-periodic trend construction. Our FCVAE has been evaluated on public datasets and a large-scale cloud system, and the results demonstrate that it outperforms state-of-the-art methods. This confirms the practical applicability of our approach in addressing the limitations of current VAE-based anomaly detection models.","sentences":["Time series Anomaly Detection (AD) plays a crucial role for web systems.","Various web systems rely on time series data to monitor and identify anomalies in real time, as well as to initiate diagnosis and remediation procedures.","Variational Autoencoders (VAEs) have gained popularity in recent decades due to their superior de-noising capabilities, which are useful for anomaly detection.","However, our study reveals that VAE-based methods face challenges in capturing long-periodic heterogeneous patterns and detailed short-periodic trends simultaneously.","To address these challenges, we propose Frequency-enhanced Conditional Variational Autoencoder (FCVAE), a novel unsupervised AD method for univariate time series.","To ensure an accurate AD, FCVAE exploits an innovative approach to concurrently integrate both the global and local frequency features into the condition of Conditional Variational Autoencoder (CVAE) to significantly increase the accuracy of reconstructing the normal data.","Together with a carefully designed \"target attention\" mechanism, our approach allows the model to pick the most useful information from the frequency domain for better short-periodic trend construction.","Our FCVAE has been evaluated on public datasets and a large-scale cloud system, and the results demonstrate that it outperforms state-of-the-art methods.","This confirms the practical applicability of our approach in addressing the limitations of current VAE-based anomaly detection models."],"url":"http://arxiv.org/abs/2402.02820v1"}
{"created":"2024-02-05 08:56:24","title":"Intersectional Two-sided Fairness in Recommendation","abstract":"Fairness of recommender systems (RS) has attracted increasing attention recently. Based on the involved stakeholders, the fairness of RS can be divided into user fairness, item fairness, and two-sided fairness which considers both user and item fairness simultaneously. However, we argue that the intersectional two-sided unfairness may still exist even if the RS is two-sided fair, which is observed and shown by empirical studies on real-world data in this paper, and has not been well-studied previously. To mitigate this problem, we propose a novel approach called Intersectional Two-sided Fairness Recommendation (ITFR). Our method utilizes a sharpness-aware loss to perceive disadvantaged groups, and then uses collaborative loss balance to develop consistent distinguishing abilities for different intersectional groups. Additionally, predicted score normalization is leveraged to align positive predicted scores to fairly treat positives in different intersectional groups. Extensive experiments and analyses on three public datasets show that our proposed approach effectively alleviates the intersectional two-sided unfairness and consistently outperforms previous state-of-the-art methods.","sentences":["Fairness of recommender systems (RS) has attracted increasing attention recently.","Based on the involved stakeholders, the fairness of RS can be divided into user fairness, item fairness, and two-sided fairness which considers both user and item fairness simultaneously.","However, we argue that the intersectional two-sided unfairness may still exist even if the RS is two-sided fair, which is observed and shown by empirical studies on real-world data in this paper, and has not been well-studied previously.","To mitigate this problem, we propose a novel approach called Intersectional Two-sided Fairness Recommendation (ITFR).","Our method utilizes a sharpness-aware loss to perceive disadvantaged groups, and then uses collaborative loss balance to develop consistent distinguishing abilities for different intersectional groups.","Additionally, predicted score normalization is leveraged to align positive predicted scores to fairly treat positives in different intersectional groups.","Extensive experiments and analyses on three public datasets show that our proposed approach effectively alleviates the intersectional two-sided unfairness and consistently outperforms previous state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.02816v1"}
{"created":"2024-02-05 08:42:39","title":"State estimation of urban air pollution with statistical, physical, and super-learning graph models","abstract":"We consider the problem of real-time reconstruction of urban air pollution maps. The task is challenging due to the heterogeneous sources of available data, the scarcity of direct measurements, the presence of noise, and the large surfaces that need to be considered. In this work, we introduce different reconstruction methods based on posing the problem on city graphs. Our strategies can be classified as fully data-driven, physics-driven, or hybrid, and we combine them with super-learning models. The performance of the methods is tested in the case of the inner city of Paris, France.","sentences":["We consider the problem of real-time reconstruction of urban air pollution maps.","The task is challenging due to the heterogeneous sources of available data, the scarcity of direct measurements, the presence of noise, and the large surfaces that need to be considered.","In this work, we introduce different reconstruction methods based on posing the problem on city graphs.","Our strategies can be classified as fully data-driven, physics-driven, or hybrid, and we combine them with super-learning models.","The performance of the methods is tested in the case of the inner city of Paris, France."],"url":"http://arxiv.org/abs/2402.02812v1"}
{"created":"2024-02-05 08:35:33","title":"Are Sounds Sound for Phylogenetic Reconstruction?","abstract":"In traditional studies on language evolution, scholars often emphasize the importance of sound laws and sound correspondences for phylogenetic inference of language family trees. However, to date, computational approaches have typically not taken this potential into account. Most computational studies still rely on lexical cognates as major data source for phylogenetic reconstruction in linguistics, although there do exist a few studies in which authors praise the benefits of comparing words at the level of sound sequences. Building on (a) ten diverse datasets from different language families, and (b) state-of-the-art methods for automated cognate and sound correspondence detection, we test, for the first time, the performance of sound-based versus cognate-based approaches to phylogenetic reconstruction. Our results show that phylogenies reconstructed from lexical cognates are topologically closer, by approximately one third with respect to the generalized quartet distance on average, to the gold standard phylogenies than phylogenies reconstructed from sound correspondences.","sentences":["In traditional studies on language evolution, scholars often emphasize the importance of sound laws and sound correspondences for phylogenetic inference of language family trees.","However, to date, computational approaches have typically not taken this potential into account.","Most computational studies still rely on lexical cognates as major data source for phylogenetic reconstruction in linguistics, although there do exist a few studies in which authors praise the benefits of comparing words at the level of sound sequences.","Building on (a) ten diverse datasets from different language families, and (b) state-of-the-art methods for automated cognate and sound correspondence detection, we test, for the first time, the performance of sound-based versus cognate-based approaches to phylogenetic reconstruction.","Our results show that phylogenies reconstructed from lexical cognates are topologically closer, by approximately one third with respect to the generalized quartet distance on average, to the gold standard phylogenies than phylogenies reconstructed from sound correspondences."],"url":"http://arxiv.org/abs/2402.02807v1"}
{"created":"2024-02-05 08:25:22","title":"Large Language Model Distilling Medication Recommendation Model","abstract":"The recommendation of medication is a vital aspect of intelligent healthcare systems, as it involves prescribing the most suitable drugs based on a patient's specific health needs. Unfortunately, many sophisticated models currently in use tend to overlook the nuanced semantics of medical data, while only relying heavily on identities. Furthermore, these models face significant challenges in handling cases involving patients who are visiting the hospital for the first time, as they lack prior prescription histories to draw upon. To tackle these issues, we harness the powerful semantic comprehension and input-agnostic characteristics of Large Language Models (LLMs). Our research aims to transform existing medication recommendation methodologies using LLMs. In this paper, we introduce a novel approach called Large Language Model Distilling Medication Recommendation (LEADER). We begin by creating appropriate prompt templates that enable LLMs to suggest medications effectively. However, the straightforward integration of LLMs into recommender systems leads to an out-of-corpus issue specific to drugs. We handle it by adapting the LLMs with a novel output layer and a refined tuning loss function. Although LLM-based models exhibit remarkable capabilities, they are plagued by high computational costs during inference, which is impractical for the healthcare sector. To mitigate this, we have developed a feature-level knowledge distillation technique, which transfers the LLM's proficiency to a more compact model. Extensive experiments conducted on two real-world datasets, MIMIC-III and MIMIC-IV, demonstrate that our proposed model not only delivers effective results but also is efficient. To ease the reproducibility of our experiments, we release the implementation code online.","sentences":["The recommendation of medication is a vital aspect of intelligent healthcare systems, as it involves prescribing the most suitable drugs based on a patient's specific health needs.","Unfortunately, many sophisticated models currently in use tend to overlook the nuanced semantics of medical data, while only relying heavily on identities.","Furthermore, these models face significant challenges in handling cases involving patients who are visiting the hospital for the first time, as they lack prior prescription histories to draw upon.","To tackle these issues, we harness the powerful semantic comprehension and input-agnostic characteristics of Large Language Models (LLMs).","Our research aims to transform existing medication recommendation methodologies using LLMs.","In this paper, we introduce a novel approach called Large Language Model Distilling Medication Recommendation (LEADER).","We begin by creating appropriate prompt templates that enable LLMs to suggest medications effectively.","However, the straightforward integration of LLMs into recommender systems leads to an out-of-corpus issue specific to drugs.","We handle it by adapting the LLMs with a novel output layer and a refined tuning loss function.","Although LLM-based models exhibit remarkable capabilities, they are plagued by high computational costs during inference, which is impractical for the healthcare sector.","To mitigate this, we have developed a feature-level knowledge distillation technique, which transfers the LLM's proficiency to a more compact model.","Extensive experiments conducted on two real-world datasets, MIMIC-III and MIMIC-IV, demonstrate that our proposed model not only delivers effective results but also is efficient.","To ease the reproducibility of our experiments, we release the implementation code online."],"url":"http://arxiv.org/abs/2402.02803v1"}
{"created":"2024-02-05 07:59:38","title":"Rethinking Optimization and Architecture for Tiny Language Models","abstract":"The power of large language models (LLMs) has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, i.e., neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\\pi$-1B Pro and PanGu-$\\pi$-1.5B Pro on 1.6T multilingual corpora, following the established formulas. Experimental results demonstrate the improved optimization and architecture yield a notable average improvement of 8.87 on benchmark evaluation sets for PanGu-$\\pi$-1B Pro. Besides, PanGu-$\\pi$-1.5B Pro surpasses a range of SOTA models with larger model sizes, validating its superior performance. The code will be released soon (https://github.com/YuchuanTian/RethinkTinyLM).","sentences":["The power of large language models (LLMs) has been demonstrated through numerous data and computing resources.","However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required.","Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully.","In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component.","Three perspectives are mainly discussed, i.e., neural architecture, parameter initialization, and optimization strategy.","Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training.","Then we train PanGu-$\\pi$-1B Pro and PanGu-$\\pi$-1.5B Pro on 1.6T multilingual corpora, following the established formulas.","Experimental results demonstrate the improved optimization and architecture yield a notable average improvement of 8.87 on benchmark evaluation sets for PanGu-$\\pi$-1B Pro.","Besides, PanGu-$\\pi$-1.5B Pro surpasses a range of SOTA models with larger model sizes, validating its superior performance.","The code will be released soon (https://github.com/YuchuanTian/RethinkTinyLM)."],"url":"http://arxiv.org/abs/2402.02791v1"}
{"created":"2024-02-05 07:14:18","title":"Accelerating Matroid Optimization through Fast Imprecise Oracles","abstract":"Querying complex models for precise information (e.g. traffic models, database systems, large ML models) often entails intense computations and results in long response times. Thus, weaker models which give imprecise results quickly can be advantageous, provided inaccuracies can be resolved using few queries to a stronger model. In the fundamental problem of computing a maximum-weight basis of a matroid, a well-known generalization of many combinatorial optimization problems, algorithms have access to a clean oracle to query matroid information. We additionally equip algorithms with a fast but dirty oracle modelling an unknown, potentially different matroid. We design and analyze practical algorithms which only use few clean queries w.r.t. the quality of the dirty oracle, while maintaining robustness against arbitrarily poor dirty matroids, approaching the performance of classic algorithms for the given problem. Notably, we prove that our algorithms are, in many respects, best-possible. Further, we outline extensions to other matroid oracle types, non-free dirty oracles and other matroid problems.","sentences":["Querying complex models for precise information (e.g. traffic models, database systems, large ML models) often entails intense computations and results in long response times.","Thus, weaker models which give imprecise results quickly can be advantageous, provided inaccuracies can be resolved using few queries to a stronger model.","In the fundamental problem of computing a maximum-weight basis of a matroid, a well-known generalization of many combinatorial optimization problems, algorithms have access to a clean oracle to query matroid information.","We additionally equip algorithms with a fast but dirty oracle modelling an unknown, potentially different matroid.","We design and analyze practical algorithms which only use few clean queries w.r.t.","the quality of the dirty oracle, while maintaining robustness against arbitrarily poor dirty matroids, approaching the performance of classic algorithms for the given problem.","Notably, we prove that our algorithms are, in many respects, best-possible.","Further, we outline extensions to other matroid oracle types, non-free dirty oracles and other matroid problems."],"url":"http://arxiv.org/abs/2402.02774v1"}
{"created":"2024-02-05 07:05:17","title":"Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate","abstract":"Generalization remains a central challenge in machine learning. In this work, we propose Learning from Teaching (LoT), a novel regularization technique for deep neural networks to enhance generalization. Inspired by the human ability to capture concise and abstract patterns, we hypothesize that generalizable correlations are expected to be easier to teach. LoT operationalizes this concept to improve the generalization of the main model with auxiliary student learners. The student learners are trained by the main model and improve the main model to capture more generalizable and teachable correlations by providing feedback. Our experimental results across several domains, including Computer Vision, Natural Language Processing, and Reinforcement Learning, demonstrate that the introduction of LoT brings significant benefits compared to merely training models on the original training data. It suggests the effectiveness of LoT in identifying generalizable information without falling into the swamp of complex patterns in data, making LoT a valuable addition to the current machine learning frameworks.","sentences":["Generalization remains a central challenge in machine learning.","In this work, we propose Learning from Teaching (LoT), a novel regularization technique for deep neural networks to enhance generalization.","Inspired by the human ability to capture concise and abstract patterns, we hypothesize that generalizable correlations are expected to be easier to teach.","LoT operationalizes this concept to improve the generalization of the main model with auxiliary student learners.","The student learners are trained by the main model and improve the main model to capture more generalizable and teachable correlations by providing feedback.","Our experimental results across several domains, including Computer Vision, Natural Language Processing, and Reinforcement Learning, demonstrate that the introduction of LoT brings significant benefits compared to merely training models on the original training data.","It suggests the effectiveness of LoT in identifying generalizable information without falling into the swamp of complex patterns in data, making LoT a valuable addition to the current machine learning frameworks."],"url":"http://arxiv.org/abs/2402.02769v1"}
{"created":"2024-02-05 05:46:31","title":"DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models","abstract":"In the exciting generative AI era, the diffusion model has emerged as a very powerful and widely adopted content generation and editing tool for various data modalities, making the study of their potential security risks very necessary and critical. Very recently, some pioneering works have shown the vulnerability of the diffusion model against backdoor attacks, calling for in-depth analysis and investigation of the security challenges of this popular and fundamental AI technique.   In this paper, for the first time, we systematically explore the detectability of the poisoned noise input for the backdoored diffusion models, an important performance metric yet little explored in the existing works. Starting from the perspective of a defender, we first analyze the properties of the trigger pattern in the existing diffusion backdoor attacks, discovering the important role of distribution discrepancy in Trojan detection. Based on this finding, we propose a low-cost trigger detection mechanism that can effectively identify the poisoned input noise. We then take a further step to study the same problem from the attack side, proposing a backdoor attack strategy that can learn the unnoticeable trigger to evade our proposed detection scheme.   Empirical evaluations across various diffusion models and datasets demonstrate the effectiveness of the proposed trigger detection and detection-evading attack strategy. For trigger detection, our distribution discrepancy-based solution can achieve a 100\\% detection rate for the Trojan triggers used in the existing works. For evading trigger detection, our proposed stealthy trigger design approach performs end-to-end learning to make the distribution of poisoned noise input approach that of benign noise, enabling nearly 100\\% detection pass rate with very high attack and benign performance for the backdoored diffusion models.","sentences":["In the exciting generative AI era, the diffusion model has emerged as a very powerful and widely adopted content generation and editing tool for various data modalities, making the study of their potential security risks very necessary and critical.","Very recently, some pioneering works have shown the vulnerability of the diffusion model against backdoor attacks, calling for in-depth analysis and investigation of the security challenges of this popular and fundamental AI technique.   ","In this paper, for the first time, we systematically explore the detectability of the poisoned noise input for the backdoored diffusion models, an important performance metric yet little explored in the existing works.","Starting from the perspective of a defender, we first analyze the properties of the trigger pattern in the existing diffusion backdoor attacks, discovering the important role of distribution discrepancy in Trojan detection.","Based on this finding, we propose a low-cost trigger detection mechanism that can effectively identify the poisoned input noise.","We then take a further step to study the same problem from the attack side, proposing a backdoor attack strategy that can learn the unnoticeable trigger to evade our proposed detection scheme.   ","Empirical evaluations across various diffusion models and datasets demonstrate the effectiveness of the proposed trigger detection and detection-evading attack strategy.","For trigger detection, our distribution discrepancy-based solution can achieve a 100\\% detection rate for the Trojan triggers used in the existing works.","For evading trigger detection, our proposed stealthy trigger design approach performs end-to-end learning to make the distribution of poisoned noise input approach that of benign noise, enabling nearly 100\\% detection pass rate with very high attack and benign performance for the backdoored diffusion models."],"url":"http://arxiv.org/abs/2402.02739v1"}
{"created":"2024-02-05 05:37:48","title":"Using Motion Cues to Supervise Single-Frame Body Pose and Shape Estimation in Low Data Regimes","abstract":"When enough annotated training data is available, supervised deep-learning algorithms excel at estimating human body pose and shape using a single camera. The effects of too little such data being available can be mitigated by using other information sources, such as databases of body shapes, to learn priors. Unfortunately, such sources are not always available either. We show that, in such cases, easy-to-obtain unannotated videos can be used instead to provide the required supervisory signals. Given a trained model using too little annotated data, we compute poses in consecutive frames along with the optical flow between them. We then enforce consistency between the image optical flow and the one that can be inferred from the change in pose from one frame to the next. This provides enough additional supervision to effectively refine the network weights and to perform on par with methods trained using far more annotated data.","sentences":["When enough annotated training data is available, supervised deep-learning algorithms excel at estimating human body pose and shape using a single camera.","The effects of too little such data being available can be mitigated by using other information sources, such as databases of body shapes, to learn priors.","Unfortunately, such sources are not always available either.","We show that, in such cases, easy-to-obtain unannotated videos can be used instead to provide the required supervisory signals.","Given a trained model using too little annotated data, we compute poses in consecutive frames along with the optical flow between them.","We then enforce consistency between the image optical flow and the one that can be inferred from the change in pose from one frame to the next.","This provides enough additional supervision to effectively refine the network weights and to perform on par with methods trained using far more annotated data."],"url":"http://arxiv.org/abs/2402.02736v1"}
{"created":"2024-02-05 05:26:17","title":"InVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data","abstract":"There is a significant interest in exploring non-linear associations among multiple images derived from diverse imaging modalities. While there is a growing literature on image-on-image regression to delineate predictive inference of an image based on multiple images, existing approaches have limitations in efficiently borrowing information between multiple imaging modalities in the prediction of an image. Building on the literature of Variational Auto Encoders (VAEs), this article proposes a novel approach, referred to as Integrative Variational Autoencoder (\\texttt{InVA}) method, which borrows information from multiple images obtained from different sources to draw predictive inference of an image. The proposed approach captures complex non-linear association between the outcome image and input images, while allowing rapid computation. Numerical results demonstrate substantial advantages of \\texttt{InVA} over VAEs, which typically do not allow borrowing information between input images. The proposed framework offers highly accurate predictive inferences for costly positron emission topography (PET) from multiple measures of cortical structure in human brain scans readily available from magnetic resonance imaging (MRI).","sentences":["There is a significant interest in exploring non-linear associations among multiple images derived from diverse imaging modalities.","While there is a growing literature on image-on-image regression to delineate predictive inference of an image based on multiple images, existing approaches have limitations in efficiently borrowing information between multiple imaging modalities in the prediction of an image.","Building on the literature of Variational Auto Encoders (VAEs), this article proposes a novel approach, referred to as Integrative Variational Autoencoder (\\texttt{InVA}) method, which borrows information from multiple images obtained from different sources to draw predictive inference of an image.","The proposed approach captures complex non-linear association between the outcome image and input images, while allowing rapid computation.","Numerical results demonstrate substantial advantages of \\texttt{InVA} over VAEs, which typically do not allow borrowing information between input images.","The proposed framework offers highly accurate predictive inferences for costly positron emission topography (PET) from multiple measures of cortical structure in human brain scans readily available from magnetic resonance imaging (MRI)."],"url":"http://arxiv.org/abs/2402.02734v1"}
{"created":"2024-02-05 05:25:33","title":"ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer","abstract":"Face re-aging is a prominent field in computer vision and graphics, with significant applications in photorealistic domains such as movies, advertising, and live streaming. Recently, the need to apply face re-aging to non-photorealistic images, like comics, illustrations, and animations, has emerged as an extension in various entertainment sectors. However, the absence of a network capable of seamlessly editing the apparent age on NPR images means that these tasks have been confined to a naive approach, applying each task sequentially. This often results in unpleasant artifacts and a loss of facial attributes due to domain discrepancies. In this paper, we introduce a novel one-stage method for face re-aging combined with portrait style transfer, executed in a single generative step. We leverage existing face re-aging and style transfer networks, both trained within the same PR domain. Our method uniquely fuses distinct latent vectors, each responsible for managing aging-related attributes and NPR appearance. Adopting an exemplar-based approach, our method offers greater flexibility than domain-level fine-tuning approaches, which typically require separate training or fine-tuning for each domain. This effectively addresses the limitation of requiring paired datasets for re-aging and domain-level, data-driven approaches for stylization. Our experiments show that our model can effortlessly generate re-aged images while simultaneously transferring the style of examples, maintaining both natural appearance and controllability.","sentences":["Face re-aging is a prominent field in computer vision and graphics, with significant applications in photorealistic domains such as movies, advertising, and live streaming.","Recently, the need to apply face re-aging to non-photorealistic images, like comics, illustrations, and animations, has emerged as an extension in various entertainment sectors.","However, the absence of a network capable of seamlessly editing the apparent age on NPR images means that these tasks have been confined to a naive approach, applying each task sequentially.","This often results in unpleasant artifacts and a loss of facial attributes due to domain discrepancies.","In this paper, we introduce a novel one-stage method for face re-aging combined with portrait style transfer, executed in a single generative step.","We leverage existing face re-aging and style transfer networks, both trained within the same PR domain.","Our method uniquely fuses distinct latent vectors, each responsible for managing aging-related attributes and NPR appearance.","Adopting an exemplar-based approach, our method offers greater flexibility than domain-level fine-tuning approaches, which typically require separate training or fine-tuning for each domain.","This effectively addresses the limitation of requiring paired datasets for re-aging and domain-level, data-driven approaches for stylization.","Our experiments show that our model can effortlessly generate re-aged images while simultaneously transferring the style of examples, maintaining both natural appearance and controllability."],"url":"http://arxiv.org/abs/2402.02733v1"}
{"created":"2024-02-05 05:01:28","title":"Fast and Accurate Cooperative Radio Map Estimation Enabled by GAN","abstract":"In the 6G era, real-time radio resource monitoring and management are urged to support diverse wireless-empowered applications. This calls for fast and accurate estimation on the distribution of the radio resources, which is usually represented by the spatial signal power strength over the geographical environment, known as a radio map. In this paper, we present a cooperative radio map estimation (CRME) approach enabled by the generative adversarial network (GAN), called as GAN-CRME, which features fast and accurate radio map estimation without the transmitters' information. The radio map is inferred by exploiting the interaction between distributed received signal strength (RSS) measurements at mobile users and the geographical map using a deep neural network estimator, resulting in low data-acquisition cost and computational complexity. Moreover, a GAN-based learning algorithm is proposed to boost the inference capability of the deep neural network estimator by exploiting the power of generative AI. Simulation results showcase that the proposed GAN-CRME is even capable of coarse error-correction when the geographical map information is inaccurate.","sentences":["In the 6G era, real-time radio resource monitoring and management are urged to support diverse wireless-empowered applications.","This calls for fast and accurate estimation on the distribution of the radio resources, which is usually represented by the spatial signal power strength over the geographical environment, known as a radio map.","In this paper, we present a cooperative radio map estimation (CRME) approach enabled by the generative adversarial network (GAN), called as GAN-CRME, which features fast and accurate radio map estimation without the transmitters' information.","The radio map is inferred by exploiting the interaction between distributed received signal strength (RSS) measurements at mobile users and the geographical map using a deep neural network estimator, resulting in low data-acquisition cost and computational complexity.","Moreover, a GAN-based learning algorithm is proposed to boost the inference capability of the deep neural network estimator by exploiting the power of generative AI.","Simulation results showcase that the proposed GAN-CRME is even capable of coarse error-correction when the geographical map information is inaccurate."],"url":"http://arxiv.org/abs/2402.02729v1"}
{"created":"2024-02-05 04:49:59","title":"Innovative Cybersickness Detection: Exploring Head Movement Patterns in Virtual Reality","abstract":"Despite the widespread adoption of Virtual Reality (VR) technology, cybersickness remains a barrier for some users. This research investigates head movement patterns as a novel physiological marker for cybersickness detection. Unlike traditional markers, head movements provide a continuous, non-invasive measure that can be easily captured through the sensors embedded in all commercial VR headsets. We used a publicly available dataset from a VR experiment involving 75 participants and analyzed head movements across six axes. An extensive feature extraction process was then performed on the head movement dataset and its derivatives, including velocity, acceleration, and jerk. Three categories of features were extracted, encompassing statistical, temporal, and spectral features. Subsequently, we employed the Recursive Feature Elimination method to select the most important and effective features. In a series of experiments, we trained a variety of machine learning algorithms. The results demonstrate a 76% accuracy and 83% precision in predicting cybersickness in the subjects based on the head movements. This study contribution to the cybersickness literature lies in offering a preliminary analysis of a new source of data and providing insight into the relationship of head movements and cybersickness.","sentences":["Despite the widespread adoption of Virtual Reality (VR) technology, cybersickness remains a barrier for some users.","This research investigates head movement patterns as a novel physiological marker for cybersickness detection.","Unlike traditional markers, head movements provide a continuous, non-invasive measure that can be easily captured through the sensors embedded in all commercial VR headsets.","We used a publicly available dataset from a VR experiment involving 75 participants and analyzed head movements across six axes.","An extensive feature extraction process was then performed on the head movement dataset and its derivatives, including velocity, acceleration, and jerk.","Three categories of features were extracted, encompassing statistical, temporal, and spectral features.","Subsequently, we employed the Recursive Feature Elimination method to select the most important and effective features.","In a series of experiments, we trained a variety of machine learning algorithms.","The results demonstrate a 76% accuracy and 83% precision in predicting cybersickness in the subjects based on the head movements.","This study contribution to the cybersickness literature lies in offering a preliminary analysis of a new source of data and providing insight into the relationship of head movements and cybersickness."],"url":"http://arxiv.org/abs/2402.02725v1"}
{"created":"2024-02-05 04:29:39","title":"Discounted Adaptive Online Prediction","abstract":"Online learning is not always about memorizing everything. Since the future can be statistically very different from the past, a critical challenge is to gracefully forget the history while new data comes in. To formalize this intuition, we revisit the classical notion of discounted regret using recently developed techniques in adaptive online learning. Our main result is a new algorithm that adapts to the complexity of both the loss sequence and the comparator, improving the widespread non-adaptive algorithm - gradient descent with a constant learning rate. In particular, our theoretical guarantee does not require any structural assumption beyond convexity, and the algorithm is provably robust to suboptimal hyperparameter tuning. We further demonstrate such benefits through online conformal prediction, a downstream online learning task with set-membership decisions.","sentences":["Online learning is not always about memorizing everything.","Since the future can be statistically very different from the past, a critical challenge is to gracefully forget the history while new data comes in.","To formalize this intuition, we revisit the classical notion of discounted regret using recently developed techniques in adaptive online learning.","Our main result is a new algorithm that adapts to the complexity of both the loss sequence and the comparator, improving the widespread non-adaptive algorithm - gradient descent with a constant learning rate.","In particular, our theoretical guarantee does not require any structural assumption beyond convexity, and the algorithm is provably robust to suboptimal hyperparameter tuning.","We further demonstrate such benefits through online conformal prediction, a downstream online learning task with set-membership decisions."],"url":"http://arxiv.org/abs/2402.02720v1"}
{"created":"2024-02-05 04:28:38","title":"Budget-feasible Egalitarian Allocation of Conflicting Jobs","abstract":"Allocating conflicting jobs among individuals while respecting a budget constraint for each individual is an optimization problem that arises in various real-world scenarios. In this paper, we consider the situation where each individual derives some satisfaction from each job. We focus on finding a feasible allocation of conflicting jobs that maximize egalitarian cost, i.e. the satisfaction of the \\nc{individual who is worst-off}. To the best of our knowledge, this is the first paper to combine egalitarianism, budget-feasibility, and conflict-freeness in allocations. We provide a systematic study of the computational complexity of finding budget-feasible conflict-free egalitarian allocation and show that our problem generalizes a large number of classical optimization problems. Therefore, unsurprisingly, our problem is \\NPH even for two individuals and when there is no conflict between any jobs. We show that the problem admits algorithms when studied in the realm of approximation algorithms and parameterized algorithms with a host of natural parameters that match and in some cases improve upon the running time of known algorithms.","sentences":["Allocating conflicting jobs among individuals while respecting a budget constraint for each individual is an optimization problem that arises in various real-world scenarios.","In this paper, we consider the situation where each individual derives some satisfaction from each job.","We focus on finding a feasible allocation of conflicting jobs that maximize egalitarian cost, i.e. the satisfaction of the \\nc{individual who is worst-off}.","To the best of our knowledge, this is the first paper to combine egalitarianism, budget-feasibility, and conflict-freeness in allocations.","We provide a systematic study of the computational complexity of finding budget-feasible conflict-free egalitarian allocation and show that our problem generalizes a large number of classical optimization problems.","Therefore, unsurprisingly, our problem is \\NPH even for two individuals and when there is no conflict between any jobs.","We show that the problem admits algorithms when studied in the realm of approximation algorithms and parameterized algorithms with a host of natural parameters that match and in some cases improve upon the running time of known algorithms."],"url":"http://arxiv.org/abs/2402.02719v1"}
{"created":"2024-02-05 03:39:39","title":"Representation Surgery for Multi-Task Model Merging","abstract":"Multi-task learning (MTL) compresses the information from multiple tasks into a unified backbone to improve computational efficiency and generalization. Recent work directly merges multiple independently trained models to perform MTL instead of collecting their raw data for joint training, greatly expanding the application scenarios of MTL. However, by visualizing the representation distribution of existing model merging schemes, we find that the merged model often suffers from the dilemma of representation bias. That is, there is a significant discrepancy in the representation distribution between the merged and individual models, resulting in poor performance of merged MTL. In this paper, we propose a representation surgery solution called \"Surgery\" to reduce representation bias in the merged model. Specifically, Surgery is a lightweight task-specific module that takes the representation of the merged model as input and attempts to output the biases contained in the representation from the merged model. We then designed an unsupervised optimization objective that updates the Surgery module by minimizing the distance between the merged model's representation and the individual model's representation. Extensive experiments demonstrate significant MTL performance improvements when our Surgery module is applied to state-of-the-art (SOTA) model merging schemes.","sentences":["Multi-task learning (MTL) compresses the information from multiple tasks into a unified backbone to improve computational efficiency and generalization.","Recent work directly merges multiple independently trained models to perform MTL instead of collecting their raw data for joint training, greatly expanding the application scenarios of MTL.","However, by visualizing the representation distribution of existing model merging schemes, we find that the merged model often suffers from the dilemma of representation bias.","That is, there is a significant discrepancy in the representation distribution between the merged and individual models, resulting in poor performance of merged MTL.","In this paper, we propose a representation surgery solution called \"Surgery\" to reduce representation bias in the merged model.","Specifically, Surgery is a lightweight task-specific module that takes the representation of the merged model as input and attempts to output the biases contained in the representation from the merged model.","We then designed an unsupervised optimization objective that updates the Surgery module by minimizing the distance between the merged model's representation and the individual model's representation.","Extensive experiments demonstrate significant MTL performance improvements when our Surgery module is applied to state-of-the-art (SOTA) model merging schemes."],"url":"http://arxiv.org/abs/2402.02705v1"}
{"created":"2024-02-05 03:23:34","title":"Adversarial Data Augmentation for Robust Speaker Verification","abstract":"Data augmentation (DA) has gained widespread popularity in deep speaker models due to its ease of implementation and significant effectiveness. It enriches training data by simulating real-life acoustic variations, enabling deep neural networks to learn speaker-related representations while disregarding irrelevant acoustic variations, thereby improving robustness and generalization. However, a potential issue with the vanilla DA is augmentation residual, i.e., unwanted distortion caused by different types of augmentation. To address this problem, this paper proposes a novel approach called adversarial data augmentation (A-DA) which combines DA with adversarial learning. Specifically, it involves an additional augmentation classifier to categorize various augmentation types used in data augmentation. This adversarial learning empowers the network to generate speaker embeddings that can deceive the augmentation classifier, making the learned speaker embeddings more robust in the face of augmentation variations. Experiments conducted on VoxCeleb and CN-Celeb datasets demonstrate that our proposed A-DA outperforms standard DA in both augmentation matched and mismatched test conditions, showcasing its superior robustness and generalization against acoustic variations.","sentences":["Data augmentation (DA) has gained widespread popularity in deep speaker models due to its ease of implementation and significant effectiveness.","It enriches training data by simulating real-life acoustic variations, enabling deep neural networks to learn speaker-related representations while disregarding irrelevant acoustic variations, thereby improving robustness and generalization.","However, a potential issue with the vanilla DA is augmentation residual, i.e., unwanted distortion caused by different types of augmentation.","To address this problem, this paper proposes a novel approach called adversarial data augmentation (A-DA) which combines DA with adversarial learning.","Specifically, it involves an additional augmentation classifier to categorize various augmentation types used in data augmentation.","This adversarial learning empowers the network to generate speaker embeddings that can deceive the augmentation classifier, making the learned speaker embeddings more robust in the face of augmentation variations.","Experiments conducted on VoxCeleb and CN-Celeb datasets demonstrate that our proposed A-DA outperforms standard DA in both augmentation matched and mismatched test conditions, showcasing its superior robustness and generalization against acoustic variations."],"url":"http://arxiv.org/abs/2402.02699v1"}
{"created":"2024-02-05 03:20:33","title":"Deep Equilibrium Models are Almost Equivalent to Not-so-deep Explicit Models for High-dimensional Gaussian Mixtures","abstract":"Deep equilibrium models (DEQs), as a typical implicit neural network, have demonstrated remarkable success on various tasks. There is, however, a lack of theoretical understanding of the connections and differences between implicit DEQs and explicit neural network models. In this paper, leveraging recent advances in random matrix theory (RMT), we perform an in-depth analysis on the eigenspectra of the conjugate kernel (CK) and neural tangent kernel (NTK) matrices for implicit DEQs, when the input data are drawn from a high-dimensional Gaussian mixture. We prove, in this setting, that the spectral behavior of these Implicit-CKs and NTKs depend on the DEQ activation function and initial weight variances, but only via a system of four nonlinear equations. As a direct consequence of this theoretical result, we demonstrate that a shallow explicit network can be carefully designed to produce the same CK or NTK as a given DEQ. Despite derived here for Gaussian mixture data, empirical results show the proposed theory and design principle also apply to popular real-world datasets.","sentences":["Deep equilibrium models (DEQs), as a typical implicit neural network, have demonstrated remarkable success on various tasks.","There is, however, a lack of theoretical understanding of the connections and differences between implicit DEQs and explicit neural network models.","In this paper, leveraging recent advances in random matrix theory (RMT), we perform an in-depth analysis on the eigenspectra of the conjugate kernel (CK) and neural tangent kernel (NTK) matrices for implicit DEQs, when the input data are drawn from a high-dimensional Gaussian mixture.","We prove, in this setting, that the spectral behavior of these Implicit-CKs and NTKs depend on the DEQ activation function and initial weight variances, but only via a system of four nonlinear equations.","As a direct consequence of this theoretical result, we demonstrate that a shallow explicit network can be carefully designed to produce the same CK or NTK as a given DEQ.","Despite derived here for Gaussian mixture data, empirical results show the proposed theory and design principle also apply to popular real-world datasets."],"url":"http://arxiv.org/abs/2402.02697v1"}
{"created":"2024-02-05 02:35:11","title":"Equivariant Symmetry Breaking Sets","abstract":"Equivariant neural networks (ENNs) have been shown to be extremely effective in applications involving underlying symmetries. By construction ENNs cannot produce lower symmetry outputs given a higher symmetry input. However, spontaneous symmetry breaking occurs in many physical systems and we may obtain a less symmetric stable state from an initial highly symmetric one. Hence, it is imperative that we understand how to systematically break symmetry in ENNs. In this work, we propose a novel symmetry breaking framework that is fully equivariant. We emphasize that our approach is general and applicable to equivariance under any group. To achieve this, we introduce the idea of symmetry breaking sets (SBS). Rather than redesign existing networks, we design sets of symmetry breaking objects which we feed into our network based on the symmetry of our inputs and outputs. We show there is a natural way to define equivariance on these sets, which gives an additional constraint. Minimizing the size of these sets equates to data efficiency. We prove that minimizing these sets translates to a well studied group theory problem, and tabulate solutions to this problem for the point groups. Finally, we provide some examples of symmetry breaking to demonstrate how our approach works in practice.","sentences":["Equivariant neural networks (ENNs) have been shown to be extremely effective in applications involving underlying symmetries.","By construction ENNs cannot produce lower symmetry outputs given a higher symmetry input.","However, spontaneous symmetry breaking occurs in many physical systems and we may obtain a less symmetric stable state from an initial highly symmetric one.","Hence, it is imperative that we understand how to systematically break symmetry in ENNs.","In this work, we propose a novel symmetry breaking framework that is fully equivariant.","We emphasize that our approach is general and applicable to equivariance under any group.","To achieve this, we introduce the idea of symmetry breaking sets (SBS).","Rather than redesign existing networks, we design sets of symmetry breaking objects which we feed into our network based on the symmetry of our inputs and outputs.","We show there is a natural way to define equivariance on these sets, which gives an additional constraint.","Minimizing the size of these sets equates to data efficiency.","We prove that minimizing these sets translates to a well studied group theory problem, and tabulate solutions to this problem for the point groups.","Finally, we provide some examples of symmetry breaking to demonstrate how our approach works in practice."],"url":"http://arxiv.org/abs/2402.02681v1"}
{"created":"2024-02-05 02:26:24","title":"Counterfactual Explanations of Black-box Machine Learning Models using Causal Discovery with Applications to Credit Rating","abstract":"Explainable artificial intelligence (XAI) has helped elucidate the internal mechanisms of machine learning algorithms, bolstering their reliability by demonstrating the basis of their predictions. Several XAI models consider causal relationships to explain models by examining the input-output relationships of prediction models and the dependencies between features. The majority of these models have been based their explanations on counterfactual probabilities, assuming that the causal graph is known. However, this assumption complicates the application of such models to real data, given that the causal relationships between features are unknown in most cases. Thus, this study proposed a novel XAI framework that relaxed the constraint that the causal graph is known. This framework leveraged counterfactual probabilities and additional prior information on causal structure, facilitating the integration of a causal graph estimated through causal discovery methods and a black-box classification model. Furthermore, explanatory scores were estimated based on counterfactual probabilities. Numerical experiments conducted employing artificial data confirmed the possibility of estimating the explanatory score more accurately than in the absence of a causal graph. Finally, as an application to real data, we constructed a classification model of credit ratings assigned by Shiga Bank, Shiga prefecture, Japan. We demonstrated the effectiveness of the proposed method in cases where the causal graph is unknown.","sentences":["Explainable artificial intelligence (XAI) has helped elucidate the internal mechanisms of machine learning algorithms, bolstering their reliability by demonstrating the basis of their predictions.","Several XAI models consider causal relationships to explain models by examining the input-output relationships of prediction models and the dependencies between features.","The majority of these models have been based their explanations on counterfactual probabilities, assuming that the causal graph is known.","However, this assumption complicates the application of such models to real data, given that the causal relationships between features are unknown in most cases.","Thus, this study proposed a novel XAI framework that relaxed the constraint that the causal graph is known.","This framework leveraged counterfactual probabilities and additional prior information on causal structure, facilitating the integration of a causal graph estimated through causal discovery methods and a black-box classification model.","Furthermore, explanatory scores were estimated based on counterfactual probabilities.","Numerical experiments conducted employing artificial data confirmed the possibility of estimating the explanatory score more accurately than in the absence of a causal graph.","Finally, as an application to real data, we constructed a classification model of credit ratings assigned by Shiga Bank, Shiga prefecture, Japan.","We demonstrated the effectiveness of the proposed method in cases where the causal graph is unknown."],"url":"http://arxiv.org/abs/2402.02678v1"}
{"created":"2024-02-05 02:26:00","title":"The Gig's Up: How ChatGPT Stacks Up Against Quora on Gig Economy Insights","abstract":"Generative AI is changing the way in which humans seek to find answers to questions in different fields including on the gig economy and labour markets, but there is limited information available about closely ChatGPT simulated output matches that obtainable from existing question and answer platforms. This paper uses ChatGPT as a research assistant to explore how far ChatGPT can replicate Quora question and answers, using data from the gig economy as an indicative case study. The results from content analysis suggest that Quora is likely to be asked questions from users looking to make money and answers are likely to include personal experiences and examples. ChatGPT simulated versions are less personal and more concept-based, including considerations on employment implications and labour rights. It appears therefore that generative AI simulates only part of what a human would want in their answers relating to the gig economy. The paper proposes that a similar comparative methodology would also be useful across other research fields to help in establishing the best real world uses of generative AI.","sentences":["Generative AI is changing the way in which humans seek to find answers to questions in different fields including on the gig economy and labour markets, but there is limited information available about closely ChatGPT simulated output matches that obtainable from existing question and answer platforms.","This paper uses ChatGPT as a research assistant to explore how far ChatGPT can replicate Quora question and answers, using data from the gig economy as an indicative case study.","The results from content analysis suggest that Quora is likely to be asked questions from users looking to make money and answers are likely to include personal experiences and examples.","ChatGPT simulated versions are less personal and more concept-based, including considerations on employment implications and labour rights.","It appears therefore that generative AI simulates only part of what a human would want in their answers relating to the gig economy.","The paper proposes that a similar comparative methodology would also be useful across other research fields to help in establishing the best real world uses of generative AI."],"url":"http://arxiv.org/abs/2402.02676v1"}
{"created":"2024-02-05 01:14:07","title":"Image-Caption Encoding for Improving Zero-Shot Generalization","abstract":"Recent advances in vision-language models have combined contrastive approaches with generative methods to achieve state-of-the-art (SOTA) on downstream inference tasks like zero-shot image classification. However, a persistent issue of these models for image classification is their out-of-distribution (OOD) generalization capabilities. We first show that when an OOD data point is misclassified, the correct class can be typically found in the Top-K predicted classes. In order to steer the model prediction toward the correct class within the top predicted classes, we propose the Image-Caption Encoding (ICE) method, a straightforward approach that directly enforces consistency between the image-conditioned and caption-conditioned predictions at evaluation time only. Intuitively, we take advantage of unique properties of the generated captions to guide our local search for the correct class label within the Top-K predicted classes. We show that our method can be easily combined with other SOTA methods to enhance Top-1 OOD accuracies by 0.5% on average and up to 3% on challenging datasets. Our code: https://github.com/Chris210634/ice","sentences":["Recent advances in vision-language models have combined contrastive approaches with generative methods to achieve state-of-the-art (SOTA) on downstream inference tasks like zero-shot image classification.","However, a persistent issue of these models for image classification is their out-of-distribution (OOD) generalization capabilities.","We first show that when an OOD data point is misclassified, the correct class can be typically found in the Top-K predicted classes.","In order to steer the model prediction toward the correct class within the top predicted classes, we propose the Image-Caption Encoding (ICE) method, a straightforward approach that directly enforces consistency between the image-conditioned and caption-conditioned predictions at evaluation time only.","Intuitively, we take advantage of unique properties of the generated captions to guide our local search for the correct class label within the Top-K predicted classes.","We show that our method can be easily combined with other SOTA methods to enhance Top-1 OOD accuracies by 0.5% on average and up to 3% on challenging datasets.","Our code: https://github.com/Chris210634/ice"],"url":"http://arxiv.org/abs/2402.02662v1"}
{"created":"2024-02-05 00:57:51","title":"Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision","abstract":"Process supervision, using a trained verifier to evaluate the intermediate steps generated by reasoner, has demonstrated significant improvements in multi-step problem solving. In this paper, to avoid expensive human annotation effort on the verifier training data, we introduce Model-induced Process Supervision (MiPS), a novel method for automating data curation. MiPS annotates an intermediate step by sampling completions of this solution through the reasoning model, and obtaining an accuracy defined as the proportion of correct completions. Errors in the reasoner would cause MiPS to underestimate the accuracy of intermediate steps, therefore, we suggest and empirically show that verification focusing on high predicted scores of the verifier shall be preferred over that of low predicted scores, contrary to prior work. Our approach significantly improves the performance of PaLM 2 on math and coding tasks (accuracy +0.67% on GSM8K, +4.16% on MATH, +0.92% on MBPP compared with an output supervision trained verifier). Additionally, our study demonstrates that the verifier exhibits strong generalization ability across different reasoning models.","sentences":["Process supervision, using a trained verifier to evaluate the intermediate steps generated by reasoner, has demonstrated significant improvements in multi-step problem solving.","In this paper, to avoid expensive human annotation effort on the verifier training data, we introduce Model-induced Process Supervision (MiPS), a novel method for automating data curation.","MiPS annotates an intermediate step by sampling completions of this solution through the reasoning model, and obtaining an accuracy defined as the proportion of correct completions.","Errors in the reasoner would cause MiPS to underestimate the accuracy of intermediate steps, therefore, we suggest and empirically show that verification focusing on high predicted scores of the verifier shall be preferred over that of low predicted scores, contrary to prior work.","Our approach significantly improves the performance of PaLM 2 on math and coding tasks (accuracy +0.67% on GSM8K, +4.16% on MATH, +0.92% on MBPP compared with an output supervision trained verifier).","Additionally, our study demonstrates that the verifier exhibits strong generalization ability across different reasoning models."],"url":"http://arxiv.org/abs/2402.02658v1"}
{"created":"2024-02-05 00:56:30","title":"RACER: An LLM-powered Methodology for Scalable Analysis of Semi-structured Mental Health Interviews","abstract":"Semi-structured interviews (SSIs) are a commonly employed data-collection method in healthcare research, offering in-depth qualitative insights into subject experiences. Despite their value, the manual analysis of SSIs is notoriously time-consuming and labor-intensive, in part due to the difficulty of extracting and categorizing emotional responses, and challenges in scaling human evaluation for large populations. In this study, we develop RACER, a Large Language Model (LLM) based expert-guided automated pipeline that efficiently converts raw interview transcripts into insightful domain-relevant themes and sub-themes. We used RACER to analyze SSIs conducted with 93 healthcare professionals and trainees to assess the broad personal and professional mental health impacts of the COVID-19 crisis. RACER achieves moderately high agreement with two human evaluators (72%), which approaches the human inter-rater agreement (77%). Interestingly, LLMs and humans struggle with similar content involving nuanced emotional, ambivalent/dialectical, and psychological statements. Our study highlights the opportunities and challenges in using LLMs to improve research efficiency and opens new avenues for scalable analysis of SSIs in healthcare research.","sentences":["Semi-structured interviews (SSIs) are a commonly employed data-collection method in healthcare research, offering in-depth qualitative insights into subject experiences.","Despite their value, the manual analysis of SSIs is notoriously time-consuming and labor-intensive, in part due to the difficulty of extracting and categorizing emotional responses, and challenges in scaling human evaluation for large populations.","In this study, we develop RACER, a Large Language Model (LLM) based expert-guided automated pipeline that efficiently converts raw interview transcripts into insightful domain-relevant themes and sub-themes.","We used RACER to analyze SSIs conducted with 93 healthcare professionals and trainees to assess the broad personal and professional mental health impacts of the COVID-19 crisis.","RACER achieves moderately high agreement with two human evaluators (72%), which approaches the human inter-rater agreement (77%).","Interestingly, LLMs and humans struggle with similar content involving nuanced emotional, ambivalent/dialectical, and psychological statements.","Our study highlights the opportunities and challenges in using LLMs to improve research efficiency and opens new avenues for scalable analysis of SSIs in healthcare research."],"url":"http://arxiv.org/abs/2402.02656v1"}
{"created":"2024-02-05 00:54:40","title":"VlogQA: Task, Dataset, and Baseline Models for Vietnamese Spoken-Based Machine Reading Comprehension","abstract":"This paper presents the development process of a Vietnamese spoken language corpus for machine reading comprehension (MRC) tasks and provides insights into the challenges and opportunities associated with using real-world data for machine reading comprehension tasks. The existing MRC corpora in Vietnamese mainly focus on formal written documents such as Wikipedia articles, online newspapers, or textbooks. In contrast, the VlogQA consists of 10,076 question-answer pairs based on 1,230 transcript documents sourced from YouTube -- an extensive source of user-uploaded content, covering the topics of food and travel. By capturing the spoken language of native Vietnamese speakers in natural settings, an obscure corner overlooked in Vietnamese research, the corpus provides a valuable resource for future research in reading comprehension tasks for the Vietnamese language. Regarding performance evaluation, our deep-learning models achieved the highest F1 score of 75.34% on the test set, indicating significant progress in machine reading comprehension for Vietnamese spoken language data. In terms of EM, the highest score we accomplished is 53.97%, which reflects the challenge in processing spoken-based content and highlights the need for further improvement.","sentences":["This paper presents the development process of a Vietnamese spoken language corpus for machine reading comprehension (MRC) tasks and provides insights into the challenges and opportunities associated with using real-world data for machine reading comprehension tasks.","The existing MRC corpora in Vietnamese mainly focus on formal written documents such as Wikipedia articles, online newspapers, or textbooks.","In contrast, the VlogQA consists of 10,076 question-answer pairs based on 1,230 transcript documents sourced from YouTube -- an extensive source of user-uploaded content, covering the topics of food and travel.","By capturing the spoken language of native Vietnamese speakers in natural settings, an obscure corner overlooked in Vietnamese research, the corpus provides a valuable resource for future research in reading comprehension tasks for the Vietnamese language.","Regarding performance evaluation, our deep-learning models achieved the highest F1 score of 75.34% on the test set, indicating significant progress in machine reading comprehension for Vietnamese spoken language data.","In terms of EM, the highest score we accomplished is 53.97%, which reflects the challenge in processing spoken-based content and highlights the need for further improvement."],"url":"http://arxiv.org/abs/2402.02655v1"}
{"created":"2024-02-05 00:52:50","title":"Learning with Mixture of Prototypes for Out-of-Distribution Detection","abstract":"Out-of-distribution (OOD) detection aims to detect testing samples far away from the in-distribution (ID) training data, which is crucial for the safe deployment of machine learning models in the real world. Distance-based OOD detection methods have emerged with enhanced deep representation learning. They identify unseen OOD samples by measuring their distances from ID class centroids or prototypes. However, existing approaches learn the representation relying on oversimplified data assumptions, e.g, modeling ID data of each class with one centroid class prototype or using loss functions not designed for OOD detection, which overlook the natural diversities within the data. Naively enforcing data samples of each class to be compact around only one prototype leads to inadequate modeling of realistic data and limited performance. To tackle these issues, we propose PrototypicAl Learning with a Mixture of prototypes (PALM) which models each class with multiple prototypes to capture the sample diversities, and learns more faithful and compact samples embeddings to enhance OOD detection. Our method automatically identifies and dynamically updates prototypes, assigning each sample to a subset of prototypes via reciprocal neighbor soft assignment weights. PALM optimizes a maximum likelihood estimation (MLE) loss to encourage the sample embeddings to be compact around the associated prototypes, as well as a contrastive loss on all prototypes to enhance intra-class compactness and inter-class discrimination at the prototype level. Moreover, the automatic estimation of prototypes enables our approach to be extended to the challenging OOD detection task with unlabelled ID data. Extensive experiments demonstrate the superiority of PALM, achieving state-of-the-art average AUROC performance of 93.82 on the challenging CIFAR-100 benchmark. Code is available at https://github.com/jeff024/PALM.","sentences":["Out-of-distribution (OOD) detection aims to detect testing samples far away from the in-distribution (ID) training data, which is crucial for the safe deployment of machine learning models in the real world.","Distance-based OOD detection methods have emerged with enhanced deep representation learning.","They identify unseen OOD samples by measuring their distances from ID class centroids or prototypes.","However, existing approaches learn the representation relying on oversimplified data assumptions, e.g, modeling ID data of each class with one centroid class prototype or using loss functions not designed for OOD detection, which overlook the natural diversities within the data.","Naively enforcing data samples of each class to be compact around only one prototype leads to inadequate modeling of realistic data and limited performance.","To tackle these issues, we propose PrototypicAl Learning with a Mixture of prototypes (PALM) which models each class with multiple prototypes to capture the sample diversities, and learns more faithful and compact samples embeddings to enhance OOD detection.","Our method automatically identifies and dynamically updates prototypes, assigning each sample to a subset of prototypes via reciprocal neighbor soft assignment weights.","PALM optimizes a maximum likelihood estimation (MLE) loss to encourage the sample embeddings to be compact around the associated prototypes, as well as a contrastive loss on all prototypes to enhance intra-class compactness and inter-class discrimination at the prototype level.","Moreover, the automatic estimation of prototypes enables our approach to be extended to the challenging OOD detection task with unlabelled ID data.","Extensive experiments demonstrate the superiority of PALM, achieving state-of-the-art average AUROC performance of 93.82 on the challenging CIFAR-100 benchmark.","Code is available at https://github.com/jeff024/PALM."],"url":"http://arxiv.org/abs/2402.02653v1"}
{"created":"2024-02-05 00:48:56","title":"Vision-Language Models Provide Promptable Representations for Reinforcement Learning","abstract":"Humans can quickly learn new behaviors by leveraging background world knowledge. In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch. We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize policies with VLMs by using them as promptable representations: embeddings that are grounded in visual observations and encode semantic features based on the VLM's internal knowledge, as elicited through prompts that provide task context and auxiliary information. We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat. We find that our policies trained on embeddings extracted from general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings. We also find our approach outperforms instruction-following methods and performs comparably to domain-specific embeddings.","sentences":["Humans can quickly learn new behaviors by leveraging background world knowledge.","In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch.","We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL.","We initialize policies with VLMs by using them as promptable representations: embeddings that are grounded in visual observations and encode semantic features based on the VLM's internal knowledge, as elicited through prompts that provide task context and auxiliary information.","We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat.","We find that our policies trained on embeddings extracted from general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings.","We also find our approach outperforms instruction-following methods and performs comparably to domain-specific embeddings."],"url":"http://arxiv.org/abs/2402.02651v1"}
{"created":"2024-02-04 23:51:04","title":"Variational DAG Estimation via State Augmentation With Stochastic Permutations","abstract":"Estimating the structure of a Bayesian network, in the form of a directed acyclic graph (DAG), from observational data is a statistically and computationally hard problem with essential applications in areas such as causal discovery. Bayesian approaches are a promising direction for solving this task, as they allow for uncertainty quantification and deal with well-known identifiability issues. From a probabilistic inference perspective, the main challenges are (i) representing distributions over graphs that satisfy the DAG constraint and (ii) estimating a posterior over the underlying combinatorial space. We propose an approach that addresses these challenges by formulating a joint distribution on an augmented space of DAGs and permutations. We carry out posterior estimation via variational inference, where we exploit continuous relaxations of discrete distributions. We show that our approach can outperform competitive Bayesian and non-Bayesian benchmarks on a range of synthetic and real datasets.","sentences":["Estimating the structure of a Bayesian network, in the form of a directed acyclic graph (DAG), from observational data is a statistically and computationally hard problem with essential applications in areas such as causal discovery.","Bayesian approaches are a promising direction for solving this task, as they allow for uncertainty quantification and deal with well-known identifiability issues.","From a probabilistic inference perspective, the main challenges are (i) representing distributions over graphs that satisfy the DAG constraint and (ii) estimating a posterior over the underlying combinatorial space.","We propose an approach that addresses these challenges by formulating a joint distribution on an augmented space of DAGs and permutations.","We carry out posterior estimation via variational inference, where we exploit continuous relaxations of discrete distributions.","We show that our approach can outperform competitive Bayesian and non-Bayesian benchmarks on a range of synthetic and real datasets."],"url":"http://arxiv.org/abs/2402.02644v1"}
{"created":"2024-02-04 23:42:02","title":"LLM-Enhanced Data Management","abstract":"Machine learning (ML) techniques for optimizing data management problems have been extensively studied and widely deployed in recent five years. However traditional ML methods have limitations on generalizability (adapting to different scenarios) and inference ability (understanding the context). Fortunately, large language models (LLMs) have shown high generalizability and human-competitive abilities in understanding context, which are promising for data management tasks (e.g., database diagnosis, database tuning). However, existing LLMs have several limitations: hallucination, high cost, and low accuracy for complicated tasks. To address these challenges, we design LLMDB, an LLM-enhanced data management paradigm which has generalizability and high inference ability while avoiding hallucination, reducing LLM cost, and achieving high accuracy. LLMDB embeds domain-specific knowledge to avoid hallucination by LLM fine-tuning and prompt engineering. LLMDB reduces the high cost of LLMs by vector databases which provide semantic search and caching abilities. LLMDB improves the task accuracy by LLM agent which provides multiple-round inference and pipeline executions. We showcase three real-world scenarios that LLMDB can well support, including query rewrite, database diagnosis and data analytics. We also summarize the open research challenges of LLMDB.","sentences":["Machine learning (ML) techniques for optimizing data management problems have been extensively studied and widely deployed in recent five years.","However traditional ML methods have limitations on generalizability (adapting to different scenarios) and inference ability (understanding the context).","Fortunately, large language models (LLMs) have shown high generalizability and human-competitive abilities in understanding context, which are promising for data management tasks (e.g., database diagnosis, database tuning).","However, existing LLMs have several limitations: hallucination, high cost, and low accuracy for complicated tasks.","To address these challenges, we design LLMDB, an LLM-enhanced data management paradigm which has generalizability and high inference ability while avoiding hallucination, reducing LLM cost, and achieving high accuracy.","LLMDB embeds domain-specific knowledge to avoid hallucination by LLM fine-tuning and prompt engineering.","LLMDB reduces the high cost of LLMs by vector databases which provide semantic search and caching abilities.","LLMDB improves the task accuracy by LLM agent which provides multiple-round inference and pipeline executions.","We showcase three real-world scenarios that LLMDB can well support, including query rewrite, database diagnosis and data analytics.","We also summarize the open research challenges of LLMDB."],"url":"http://arxiv.org/abs/2402.02643v1"}
{"created":"2024-02-04 23:39:03","title":"Object Graph Programming","abstract":"We introduce Object Graph Programming (OGO), which enables reading and modifying an object graph (i.e., the entire state of the object heap) via declarative queries. OGO models the objects and their relations in the heap as an object graph thereby treating the heap as a graph database: each node in the graph is an object (e.g., an instance of a class or an instance of a metadata class) and each edge is a relation between objects (e.g., a field of one object references another object). We leverage Cypher, the most popular query language for graph databases, as OGO's query language. Unlike LINQ, which uses collections (e.g., List) as a source of data, OGO views the entire object graph as a single \"collection\". OGO is ideal for querying collections (just like LINQ), introspecting the runtime system state (e.g., finding all instances of a given class or accessing fields via reflection), and writing assertions that have access to the entire program state. We prototyped OGO for Java in two ways: (a) by translating an object graph into a Neo4j database on which we run Cypher queries, and (b) by implementing our own in-memory graph query engine that directly queries the object heap. We used OGO to rewrite hundreds of statements in large open-source projects into OGO queries. We report our experience and performance of our prototypes.","sentences":["We introduce Object Graph Programming (OGO), which enables reading and modifying an object graph (i.e., the entire state of the object heap) via declarative queries.","OGO models the objects and their relations in the heap as an object graph thereby treating the heap as a graph database: each node in the graph is an object (e.g., an instance of a class or an instance of a metadata class) and each edge is a relation between objects (e.g., a field of one object references another object).","We leverage Cypher, the most popular query language for graph databases, as OGO's query language.","Unlike LINQ, which uses collections (e.g., List) as a source of data, OGO views the entire object graph as a single \"collection\".","OGO is ideal for querying collections (just like LINQ), introspecting the runtime system state (e.g., finding all instances of a given class or accessing fields via reflection), and writing assertions that have access to the entire program state.","We prototyped OGO for Java in two ways: (a) by translating an object graph into a Neo4j database on which we run Cypher queries, and (b) by implementing our own in-memory graph query engine that directly queries the object heap.","We used OGO to rewrite hundreds of statements in large open-source projects into OGO queries.","We report our experience and performance of our prototypes."],"url":"http://arxiv.org/abs/2402.02642v1"}
{"created":"2024-02-04 23:11:19","title":"$C^*$-Algebraic Machine Learning: Moving in a New Direction","abstract":"Machine learning has a long collaborative tradition with several fields of mathematics, such as statistics, probability and linear algebra. We propose a new direction for machine learning research: $C^*$-algebraic ML $-$ a cross-fertilization between $C^*$-algebra and machine learning. The mathematical concept of $C^*$-algebra is a natural generalization of the space of complex numbers. It enables us to unify existing learning strategies, and construct a new framework for more diverse and information-rich data models. We explain why and how to use $C^*$-algebras in machine learning, and provide technical considerations that go into the design of $C^*$-algebraic learning models in the contexts of kernel methods and neural networks. Furthermore, we discuss open questions and challenges in $C^*$-algebraic ML and give our thoughts for future development and applications.","sentences":["Machine learning has a long collaborative tradition with several fields of mathematics, such as statistics, probability and linear algebra.","We propose a new direction for machine learning research: $C^*$-algebraic ML $-$ a cross-fertilization between $C^*$-algebra and machine learning.","The mathematical concept of $C^*$-algebra is a natural generalization of the space of complex numbers.","It enables us to unify existing learning strategies, and construct a new framework for more diverse and information-rich data models.","We explain why and how to use $C^*$-algebras in machine learning, and provide technical considerations that go into the design of $C^*$-algebraic learning models in the contexts of kernel methods and neural networks.","Furthermore, we discuss open questions and challenges in $C^*$-algebraic ML and give our thoughts for future development and applications."],"url":"http://arxiv.org/abs/2402.02637v1"}
{"created":"2024-02-04 23:04:02","title":"Can Large Language Models Learn Independent Causal Mechanisms?","abstract":"Despite impressive performance on language modelling and complex reasoning tasks, Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting some lack of generalisation ability. This issue has usually been alleviated by feeding more training data into the LLM. However, this method is brittle, as the scope of tasks may not be readily predictable or may evolve, and updating the model with new data generally requires extensive additional training. By contrast, systems, such as causal models, that learn abstract variables and causal relationships can demonstrate increased robustness against changes in the distribution. One reason for this success is the existence and use of Independent Causal Mechanisms (ICMs) representing high-level concepts that only sparsely interact. In this work, we apply two concepts from causality to learn ICMs within LLMs. We develop a new LLM architecture composed of multiple sparsely interacting language modelling modules. We introduce a routing scheme to induce specialisation of the network into domain-specific modules. We also present a Mutual Information minimisation objective that trains a separate module to learn abstraction and domain-invariant mechanisms. We show that such causal constraints can improve out-of-distribution performance on abstract and causal reasoning tasks.","sentences":["Despite impressive performance on language modelling and complex reasoning tasks, Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting some lack of generalisation ability.","This issue has usually been alleviated by feeding more training data into the LLM.","However, this method is brittle, as the scope of tasks may not be readily predictable or may evolve, and updating the model with new data generally requires extensive additional training.","By contrast, systems, such as causal models, that learn abstract variables and causal relationships can demonstrate increased robustness against changes in the distribution.","One reason for this success is the existence and use of Independent Causal Mechanisms (ICMs) representing high-level concepts that only sparsely interact.","In this work, we apply two concepts from causality to learn ICMs within LLMs.","We develop a new LLM architecture composed of multiple sparsely interacting language modelling modules.","We introduce a routing scheme to induce specialisation of the network into domain-specific modules.","We also present a Mutual Information minimisation objective that trains a separate module to learn abstraction and domain-invariant mechanisms.","We show that such causal constraints can improve out-of-distribution performance on abstract and causal reasoning tasks."],"url":"http://arxiv.org/abs/2402.02636v1"}
{"created":"2024-02-04 22:12:29","title":"Enhancing Transformer RNNs with Multiple Temporal Perspectives","abstract":"We introduce the concept of multiple temporal perspectives, a novel approach applicable to Recurrent Neural Network (RNN) architectures for enhancing their understanding of sequential data. This method involves maintaining diverse temporal views of previously encountered text, significantly enriching the language models' capacity to interpret context. To show the efficacy of this approach, we incorporate it into the Receptance Weighted Key Value (RWKV) architecture, addressing its inherent challenge of retaining all historical information within a single hidden state. Notably, this improvement is achieved with a minimal increase in the number of parameters --even as little as $0.04\\%$ of the original number of parameters. Further, the additional parameters necessary for the multiple temporal perspectives are fine-tuned with minimal computational overhead, avoiding the need for a full pre-training. The resulting model maintains linear computational complexity during prompt inference, ensuring consistent efficiency across various sequence lengths. The empirical results and ablation studies included in our research validate the effectiveness of our approach, showcasing improved performance across multiple benchmarks. The code, model weights and datasets are open-sourced at: https://github.com/RazvanDu/TemporalRNNs.","sentences":["We introduce the concept of multiple temporal perspectives, a novel approach applicable to Recurrent Neural Network (RNN) architectures for enhancing their understanding of sequential data.","This method involves maintaining diverse temporal views of previously encountered text, significantly enriching the language models' capacity to interpret context.","To show the efficacy of this approach, we incorporate it into the Receptance Weighted Key Value (RWKV) architecture, addressing its inherent challenge of retaining all historical information within a single hidden state.","Notably, this improvement is achieved with a minimal increase in the number of parameters --even as little as $0.04\\%$ of the original number of parameters.","Further, the additional parameters necessary for the multiple temporal perspectives are fine-tuned with minimal computational overhead, avoiding the need for a full pre-training.","The resulting model maintains linear computational complexity during prompt inference, ensuring consistent efficiency across various sequence lengths.","The empirical results and ablation studies included in our research validate the effectiveness of our approach, showcasing improved performance across multiple benchmarks.","The code, model weights and datasets are open-sourced at: https://github.com/RazvanDu/TemporalRNNs."],"url":"http://arxiv.org/abs/2402.02625v1"}
{"created":"2024-02-04 21:54:25","title":"Efficient Market Dynamics: Unraveling Informational Efficiency in UK Horse Racing Betting Markets Through Betfair's Time Series Analysis","abstract":"Using Betfair's time series data, an analysis of the United Kingdom (UK) horse racing market reveals an interesting paradox: a market with short tails, rapidly decaying autocorrelations, and no long-term memory. There seems to be a remarkably high level of informational efficiency in betting exchange returns, in contrast to financial assets that are characterized by heavy tails and volatility clustering. The generalized Gaussian unconditional distribution with a light tail point to a market where knowledge is quickly assimilated and reflected in prices. This is further supported by the extremely quick fading of autocorrelations and the absence of gain-loss asymmetry. Therefore, in addition to measuring long-range memory, the Hurst exponent also shows mean reversion, a sign that markets respond quickly to fresh information.","sentences":["Using Betfair's time series data, an analysis of the United Kingdom (UK) horse racing market reveals an interesting paradox: a market with short tails, rapidly decaying autocorrelations, and no long-term memory.","There seems to be a remarkably high level of informational efficiency in betting exchange returns, in contrast to financial assets that are characterized by heavy tails and volatility clustering.","The generalized Gaussian unconditional distribution with a light tail point to a market where knowledge is quickly assimilated and reflected in prices.","This is further supported by the extremely quick fading of autocorrelations and the absence of gain-loss asymmetry.","Therefore, in addition to measuring long-range memory, the Hurst exponent also shows mean reversion, a sign that markets respond quickly to fresh information."],"url":"http://arxiv.org/abs/2402.02623v1"}
{"created":"2024-02-04 21:44:09","title":"DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging","abstract":"The transformer architecture from Vaswani et al. (2017) is now ubiquitous across application domains, from natural language processing to speech processing and image understanding. We propose DenseFormer, a simple modification to the standard architecture that improves the perplexity of the model without increasing its size -- adding a few thousand parameters for large-scale models in the 100B parameters range. Our approach relies on an additional averaging step after each transformer block, which computes a weighted average of current and past representations -- we refer to this operation as Depth-Weighted-Average (DWA). The learned DWA weights exhibit coherent patterns of information flow, revealing the strong and structured reuse of activations from distant layers. Experiments demonstrate that DenseFormer is more data efficient, reaching the same perplexity of much deeper transformer models, and that for the same perplexity, these new models outperform transformer baselines in terms of memory efficiency and inference time.","sentences":["The transformer architecture from Vaswani et al. (2017) is now ubiquitous across application domains, from natural language processing to speech processing and image understanding.","We propose DenseFormer, a simple modification to the standard architecture that improves the perplexity of the model without increasing its size -- adding a few thousand parameters for large-scale models in the 100B parameters range.","Our approach relies on an additional averaging step after each transformer block, which computes a weighted average of current and past representations -- we refer to this operation as Depth-Weighted-Average (DWA).","The learned DWA weights exhibit coherent patterns of information flow, revealing the strong and structured reuse of activations from distant layers.","Experiments demonstrate that DenseFormer is more data efficient, reaching the same perplexity of much deeper transformer models, and that for the same perplexity, these new models outperform transformer baselines in terms of memory efficiency and inference time."],"url":"http://arxiv.org/abs/2402.02622v1"}
{"created":"2024-02-04 21:40:23","title":"Perfect Multi-User Distributed Computing","abstract":"In this paper, we investigate the problem of multi-user linearly decomposable function computation, where $N$ servers help compute functions for $K$ users, and where each such function can be expressed as a linear combination of $L$ basis subfunctions. The process begins with each server computing some of the subfunctions, then broadcasting a linear combination of its computed outputs to a selected group of users, and finally having each user linearly combine its received data to recover its function. As it has become recently known, this problem can be translated into a matrix decomposition problem $\\mathbf{F}=\\mathbf{D}\\mathbf{E}$, where $\\mathbf{F} \\in \\mathbf{GF}(q)^{K \\times L}$ describes the coefficients that define the users' demands, where $\\mathbf{E} \\in \\mathbf{GF}(q)^{N \\times L}$ describes which subfunction each server computes and how it combines the computed outputs, and where $\\mathbf{D} \\in \\mathbf{GF}(q)^{K \\times N}$ describes which servers each user receives data from and how it combines this data. Our interest here is in reducing the total number of subfunction computations across the servers (cumulative computational cost), as well as the worst-case load which can be a measure of computational delay. Our contribution consists of novel bounds on the two computing costs, where these bounds are linked here to the covering and packing radius of classical codes. One of our findings is that in certain cases, our distributed computing problem -- and by extension our matrix decomposition problem -- is treated optimally when $\\mathbf{F}$ is decomposed into a parity check matrix $\\mathbf{D}$ of a perfect code, and a matrix $\\mathbf{E}$ which has as columns the coset leaders of this same code.","sentences":["In this paper, we investigate the problem of multi-user linearly decomposable function computation, where $N$ servers help compute functions for $K$ users, and where each such function can be expressed as a linear combination of $L$ basis subfunctions.","The process begins with each server computing some of the subfunctions, then broadcasting a linear combination of its computed outputs to a selected group of users, and finally having each user linearly combine its received data to recover its function.","As it has become recently known, this problem can be translated into a matrix decomposition problem $\\mathbf{F}=\\mathbf{D}\\mathbf{E}$, where $\\mathbf{F} \\in \\mathbf{GF}(q)^{K \\times L}$ describes the coefficients that define the users' demands, where $\\mathbf{E} \\in \\mathbf{GF}(q)^{N \\times L}$ describes which subfunction each server computes and how it combines the computed outputs, and where $\\mathbf{D} \\in \\mathbf{GF}(q)^{K \\times N}$ describes which servers each user receives data from and how it combines this data.","Our interest here is in reducing the total number of subfunction computations across the servers (cumulative computational cost), as well as the worst-case load which can be a measure of computational delay.","Our contribution consists of novel bounds on the two computing costs, where these bounds are linked here to the covering and packing radius of classical codes.","One of our findings is that in certain cases, our distributed computing problem -- and by extension our matrix decomposition problem -- is treated optimally when $\\mathbf{F}$ is decomposed into a parity check matrix $\\mathbf{D}$ of a perfect code, and a matrix $\\mathbf{E}$ which has as columns the coset leaders of this same code."],"url":"http://arxiv.org/abs/2402.02621v1"}
{"created":"2024-02-04 21:22:29","title":"The Virtues of Pessimism in Inverse Reinforcement Learning","abstract":"Inverse Reinforcement Learning (IRL) is a powerful framework for learning complex behaviors from expert demonstrations. However, it traditionally requires repeatedly solving a computationally expensive reinforcement learning (RL) problem in its inner loop. It is desirable to reduce the exploration burden by leveraging expert demonstrations in the inner-loop RL. As an example, recent work resets the learner to expert states in order to inform the learner of high-reward expert states. However, such an approach is infeasible in the real world. In this work, we consider an alternative approach to speeding up the RL subroutine in IRL: \\emph{pessimism}, i.e., staying close to the expert's data distribution, instantiated via the use of offline RL algorithms. We formalize a connection between offline RL and IRL, enabling us to use an arbitrary offline RL algorithm to improve the sample efficiency of IRL. We validate our theory experimentally by demonstrating a strong correlation between the efficacy of an offline RL algorithm and how well it works as part of an IRL procedure. By using a strong offline RL algorithm as part of an IRL procedure, we are able to find policies that match expert performance significantly more efficiently than the prior art.","sentences":["Inverse Reinforcement Learning (IRL) is a powerful framework for learning complex behaviors from expert demonstrations.","However, it traditionally requires repeatedly solving a computationally expensive reinforcement learning (RL) problem in its inner loop.","It is desirable to reduce the exploration burden by leveraging expert demonstrations in the inner-loop RL.","As an example, recent work resets the learner to expert states in order to inform the learner of high-reward expert states.","However, such an approach is infeasible in the real world.","In this work, we consider an alternative approach to speeding up the RL subroutine in IRL: \\emph{pessimism}, i.e., staying close to the expert's data distribution, instantiated via the use of offline RL algorithms.","We formalize a connection between offline RL and IRL, enabling us to use an arbitrary offline RL algorithm to improve the sample efficiency of IRL.","We validate our theory experimentally by demonstrating a strong correlation between the efficacy of an offline RL algorithm and how well it works as part of an IRL procedure.","By using a strong offline RL algorithm as part of an IRL procedure, we are able to find policies that match expert performance significantly more efficiently than the prior art."],"url":"http://arxiv.org/abs/2402.02616v1"}
{"created":"2024-02-04 20:18:23","title":"Synthesizing Follow-Up Drive Data for Enhanced Road Safety in Intelligent Driving Function Systems","abstract":"This study underscores the vital importance of intelligent driving functions in enhancing road safety and driving comfort. Central to our research is the challenge of obtaining sufficient test data for evaluating these functions, especially in high-risk, safety-critical driving scenarios. Such scenarios often suffer from a dearth of available data, primarily due to their inherent complexity and the risks involved.   Addressing this gap, our research introduces a novel methodology designed to create a wide array of diverse and realistic safety-critical driving scenarios. This approach significantly broadens the testing spectrum for driver assistance systems and autonomous vehicle functions. We particularly focus on the follow-up drive scenario due to its high relevance in practical applications. Here, vehicle movements are intricately modeled using kinematic equations, incorporating factors like driver reaction times. We vary parameters to generate a spectrum of plausible driving scenarios.   The utilization of the Difference Space Stopping (DSS) metric is a pivotal element in our research. This metric plays a crucial role in the safety evaluation of follow-up drives, facilitating a more thorough and comprehensive validation process. By doing so, our methodology enhances the reliability and safety assessment of driver assistance and autonomous driving systems, specifically tailored for the most challenging and safety-critical scenarios.","sentences":["This study underscores the vital importance of intelligent driving functions in enhancing road safety and driving comfort.","Central to our research is the challenge of obtaining sufficient test data for evaluating these functions, especially in high-risk, safety-critical driving scenarios.","Such scenarios often suffer from a dearth of available data, primarily due to their inherent complexity and the risks involved.   ","Addressing this gap, our research introduces a novel methodology designed to create a wide array of diverse and realistic safety-critical driving scenarios.","This approach significantly broadens the testing spectrum for driver assistance systems and autonomous vehicle functions.","We particularly focus on the follow-up drive scenario due to its high relevance in practical applications.","Here, vehicle movements are intricately modeled using kinematic equations, incorporating factors like driver reaction times.","We vary parameters to generate a spectrum of plausible driving scenarios.   ","The utilization of the Difference Space Stopping (DSS) metric is a pivotal element in our research.","This metric plays a crucial role in the safety evaluation of follow-up drives, facilitating a more thorough and comprehensive validation process.","By doing so, our methodology enhances the reliability and safety assessment of driver assistance and autonomous driving systems, specifically tailored for the most challenging and safety-critical scenarios."],"url":"http://arxiv.org/abs/2402.02598v1"}
{"created":"2024-02-04 20:00:45","title":"Unified Training of Universal Time Series Forecasting Transformers","abstract":"Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) featuring over 27B observations across nine domains, Moirai achieves competitive or superior performance as a zero-shot forecaster when compared to full-shot models. Code, model weights, and data will be released.","sentences":["Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models.","The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks.","However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data.","To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai).","Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) featuring over 27B observations across nine domains, Moirai achieves competitive or superior performance as a zero-shot forecaster when compared to full-shot models.","Code, model weights, and data will be released."],"url":"http://arxiv.org/abs/2402.02592v1"}
{"created":"2024-02-04 18:45:33","title":"On the development of an application for the compilation of global sea level changes","abstract":"There is a lot of data about mean sea level variation from studies conducted around the globe. This data is dispersed, lacks organization along with standardization, and in most cases, it is not available online. In some instances, when it is available, it is often in unpractical ways and different formats. Analyzing it would be inefficient and very time-consuming. In addition to all of that, to successfully process spatial-temporal data, the user has to be equipped with particular skills and tools used for geographic data like PostGIS, PostgreSQL and GeoAlchemy. The presented solution is to develop a web application that solves some of the issues faced by researchers. The web application allows the user to add data, be it through forms in a browser or automated with the help of an API. The application also assists with data querying, processing and visualization by making tables, showing maps and drawing graphs. Comparing data points from different areas and publications is also made possible. The implemented web application permits the query and storage of spatial-temporal data about mean sea level variation in a simplified, easily accessible and user-friendly manner. It will also allow the realization of more global studies.","sentences":["There is a lot of data about mean sea level variation from studies conducted around the globe.","This data is dispersed, lacks organization along with standardization, and in most cases, it is not available online.","In some instances, when it is available, it is often in unpractical ways and different formats.","Analyzing it would be inefficient and very time-consuming.","In addition to all of that, to successfully process spatial-temporal data, the user has to be equipped with particular skills and tools used for geographic data like PostGIS, PostgreSQL and GeoAlchemy.","The presented solution is to develop a web application that solves some of the issues faced by researchers.","The web application allows the user to add data, be it through forms in a browser or automated with the help of an API.","The application also assists with data querying, processing and visualization by making tables, showing maps and drawing graphs.","Comparing data points from different areas and publications is also made possible.","The implemented web application permits the query and storage of spatial-temporal data about mean sea level variation in a simplified, easily accessible and user-friendly manner.","It will also allow the realization of more global studies."],"url":"http://arxiv.org/abs/2402.02582v1"}
{"created":"2024-02-04 18:05:02","title":"AM-CCA: A Memory-Driven System for Fine-Grain and Dynamic Computations","abstract":"Techniques of computer systems that have been successfully deployed for dense regular workloads fall short of achieving their goals of scalability and efficiency when applied to irregular and dynamic applications. This is primarily due to the discontent between the multiple layers of the system design from hardware architecture, execution model, programming model, to data-structure and application code. The paper approaches this issue by addressing all layers of the system design. It presents and argues key design principles needed for scalable and efficient dynamic graph processing, and from which it builds: 1) a fine-grain memory driven architecture that supports asynchronous active messages, 2) a programming and execution model that allows spawning tasks from within the data-parallelism, 3) and a data-structure that parallelizes vertex object across many compute cells and yet provides a single programming abstraction to the data object.   Simulated experimental results show performance gain of geomean $2.38 \\times$ against an state-of-the-art similar system for graph traversals and yet being able to natively support dynamic graph processing. It uses programming abstractions of actions, introduces new dynamic graph storage scheme, and message delivery mechanisms with continuations that contain post-completion actions. Continuations seamlessly adjusts, prior or running, execution to mutations in the input graph and enable dynamic graph processing.","sentences":["Techniques of computer systems that have been successfully deployed for dense regular workloads fall short of achieving their goals of scalability and efficiency when applied to irregular and dynamic applications.","This is primarily due to the discontent between the multiple layers of the system design from hardware architecture, execution model, programming model, to data-structure and application code.","The paper approaches this issue by addressing all layers of the system design.","It presents and argues key design principles needed for scalable and efficient dynamic graph processing, and from which it builds: 1) a fine-grain memory driven architecture that supports asynchronous active messages, 2) a programming and execution model that allows spawning tasks from within the data-parallelism, 3) and a data-structure that parallelizes vertex object across many compute cells and yet provides a single programming abstraction to the data object.   ","Simulated experimental results show performance gain of geomean $2.38 \\times$ against an state-of-the-art similar system for graph traversals and yet being able to natively support dynamic graph processing.","It uses programming abstractions of actions, introduces new dynamic graph storage scheme, and message delivery mechanisms with continuations that contain post-completion actions.","Continuations seamlessly adjusts, prior or running, execution to mutations in the input graph and enable dynamic graph processing."],"url":"http://arxiv.org/abs/2402.02576v1"}
{"created":"2024-02-04 17:19:46","title":"Gazebo Plants: Simulating Plant-Robot Interaction with Cosserat Rods","abstract":"Robotic harvesting has the potential to positively impact agricultural productivity, reduce costs, improve food quality, enhance sustainability, and to address labor shortage. In the rapidly advancing field of agricultural robotics, the necessity of training robots in a virtual environment has become essential. Generating training data to automatize the underlying computer vision tasks such as image segmentation, object detection and classification, also heavily relies on such virtual environments as synthetic data is often required to overcome the shortage and lack of variety of real data sets. However, physics engines commonly employed within the robotics community, such as ODE, Simbody, Bullet, and DART, primarily support motion and collision interaction of rigid bodies. This inherent limitation hinders experimentation and progress in handling non-rigid objects such as plants and crops. In this contribution, we present a plugin for the Gazebo simulation platform based on Cosserat rods to model plant motion. It enables the simulation of plants and their interaction with the environment. We demonstrate that, using our plugin, users can conduct harvesting simulations in Gazebo by simulating a robotic arm picking fruits and achieve results comparable to real-world experiments.","sentences":["Robotic harvesting has the potential to positively impact agricultural productivity, reduce costs, improve food quality, enhance sustainability, and to address labor shortage.","In the rapidly advancing field of agricultural robotics, the necessity of training robots in a virtual environment has become essential.","Generating training data to automatize the underlying computer vision tasks such as image segmentation, object detection and classification, also heavily relies on such virtual environments as synthetic data is often required to overcome the shortage and lack of variety of real data sets.","However, physics engines commonly employed within the robotics community, such as ODE, Simbody, Bullet, and DART, primarily support motion and collision interaction of rigid bodies.","This inherent limitation hinders experimentation and progress in handling non-rigid objects such as plants and crops.","In this contribution, we present a plugin for the Gazebo simulation platform based on Cosserat rods to model plant motion.","It enables the simulation of plants and their interaction with the environment.","We demonstrate that, using our plugin, users can conduct harvesting simulations in Gazebo by simulating a robotic arm picking fruits and achieve results comparable to real-world experiments."],"url":"http://arxiv.org/abs/2402.02570v1"}
{"created":"2024-02-04 16:27:37","title":"Foundation Model Makes Clustering a Better Initialization for Active Learning","abstract":"Active learning selects the most informative samples from the unlabeled dataset to annotate in the context of a limited annotation budget. While numerous methods have been proposed for subsequent sample selection based on an initialized model, scant attention has been paid to the indispensable phase of active learning: selecting samples for model initialization. Most of the previous studies resort to random sampling or naive clustering. However, random sampling is prone to fluctuation, and naive clustering suffers from convergence speed, particularly when dealing with high-dimensional data such as imaging data. In this work, we propose to integrate foundation models with clustering methods to select samples for active learning initialization. Foundation models refer to those trained on massive datasets by the self-supervised paradigm and capable of generating informative and compacted embeddings for various downstream tasks. Leveraging these embeddings to replace raw features such as pixel values, clustering quickly converges and identifies better initial samples. For a comprehensive comparison, we included a classic ImageNet-supervised model to acquire embeddings. Experiments on two clinical tasks of image classification and segmentation demonstrated that foundation model-based clustering efficiently pinpointed informative initial samples, leading to models showcasing enhanced performance than the baseline methods. We envisage that this study provides an effective paradigm for future active learning.","sentences":["Active learning selects the most informative samples from the unlabeled dataset to annotate in the context of a limited annotation budget.","While numerous methods have been proposed for subsequent sample selection based on an initialized model, scant attention has been paid to the indispensable phase of active learning: selecting samples for model initialization.","Most of the previous studies resort to random sampling or naive clustering.","However, random sampling is prone to fluctuation, and naive clustering suffers from convergence speed, particularly when dealing with high-dimensional data such as imaging data.","In this work, we propose to integrate foundation models with clustering methods to select samples for active learning initialization.","Foundation models refer to those trained on massive datasets by the self-supervised paradigm and capable of generating informative and compacted embeddings for various downstream tasks.","Leveraging these embeddings to replace raw features such as pixel values, clustering quickly converges and identifies better initial samples.","For a comprehensive comparison, we included a classic ImageNet-supervised model to acquire embeddings.","Experiments on two clinical tasks of image classification and segmentation demonstrated that foundation model-based clustering efficiently pinpointed informative initial samples, leading to models showcasing enhanced performance than the baseline methods.","We envisage that this study provides an effective paradigm for future active learning."],"url":"http://arxiv.org/abs/2402.02561v1"}
