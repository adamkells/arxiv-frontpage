{"created":"2024-06-06 17:59:58","title":"Stereo-Depth Fusion through Virtual Pattern Projection","abstract":"This paper presents a novel general-purpose stereo and depth data fusion paradigm that mimics the active stereo principle by replacing the unreliable physical pattern projector with a depth sensor. It works by projecting virtual patterns consistent with the scene geometry onto the left and right images acquired by a conventional stereo camera, using the sparse hints obtained from a depth sensor, to facilitate the visual correspondence. Purposely, any depth sensing device can be seamlessly plugged into our framework, enabling the deployment of a virtual active stereo setup in any possible environment and overcoming the severe limitations of physical pattern projection, such as the limited working range and environmental conditions. Exhaustive experiments on indoor and outdoor datasets featuring both long and close range, including those providing raw, unfiltered depth hints from off-the-shelf depth sensors, highlight the effectiveness of our approach in notably boosting the robustness and accuracy of algorithms and deep stereo without any code modification and even without re-training. Additionally, we assess the performance of our strategy on active stereo evaluation datasets with conventional pattern projection. Indeed, in all these scenarios, our virtual pattern projection paradigm achieves state-of-the-art performance. The source code is available at: https://github.com/bartn8/vppstereo.","sentences":["This paper presents a novel general-purpose stereo and depth data fusion paradigm that mimics the active stereo principle by replacing the unreliable physical pattern projector with a depth sensor.","It works by projecting virtual patterns consistent with the scene geometry onto the left and right images acquired by a conventional stereo camera, using the sparse hints obtained from a depth sensor, to facilitate the visual correspondence.","Purposely, any depth sensing device can be seamlessly plugged into our framework, enabling the deployment of a virtual active stereo setup in any possible environment and overcoming the severe limitations of physical pattern projection, such as the limited working range and environmental conditions.","Exhaustive experiments on indoor and outdoor datasets featuring both long and close range, including those providing raw, unfiltered depth hints from off-the-shelf depth sensors, highlight the effectiveness of our approach in notably boosting the robustness and accuracy of algorithms and deep stereo without any code modification and even without re-training.","Additionally, we assess the performance of our strategy on active stereo evaluation datasets with conventional pattern projection.","Indeed, in all these scenarios, our virtual pattern projection paradigm achieves state-of-the-art performance.","The source code is available at: https://github.com/bartn8/vppstereo."],"url":"http://arxiv.org/abs/2406.04345v1"}
{"created":"2024-06-06 17:59:56","title":"Verbalized Machine Learning: Revisiting Machine Learning with Language Models","abstract":"Motivated by the large progress made by large language models (LLMs), we introduce the framework of verbalized machine learning (VML). In contrast to conventional machine learning models that are typically optimized over a continuous parameter space, VML constrains the parameter space to be human-interpretable natural language. Such a constraint leads to a new perspective of function approximation, where an LLM with a text prompt can be viewed as a function parameterized by the text prompt. Guided by this perspective, we revisit classical machine learning problems, such as regression and classification, and find that these problems can be solved by an LLM-parameterized learner and optimizer. The major advantages of VML include (1) easy encoding of inductive bias: prior knowledge about the problem and hypothesis class can be encoded in natural language and fed into the LLM-parameterized learner; (2) automatic model class selection: the optimizer can automatically select a concrete model class based on data and verbalized prior knowledge, and it can update the model class during training; and (3) interpretable learner updates: the LLM-parameterized optimizer can provide explanations for why each learner update is performed. We conduct several studies to empirically evaluate the effectiveness of VML, and hope that VML can serve as a stepping stone to stronger interpretability and trustworthiness in ML.","sentences":["Motivated by the large progress made by large language models (LLMs), we introduce the framework of verbalized machine learning (VML).","In contrast to conventional machine learning models that are typically optimized over a continuous parameter space, VML constrains the parameter space to be human-interpretable natural language.","Such a constraint leads to a new perspective of function approximation, where an LLM with a text prompt can be viewed as a function parameterized by the text prompt.","Guided by this perspective, we revisit classical machine learning problems, such as regression and classification, and find that these problems can be solved by an LLM-parameterized learner and optimizer.","The major advantages of VML include (1) easy encoding of inductive bias: prior knowledge about the problem and hypothesis class can be encoded in natural language and fed into the LLM-parameterized learner; (2) automatic model class selection: the optimizer can automatically select a concrete model class based on data and verbalized prior knowledge, and it can update the model class during training; and (3) interpretable learner updates: the LLM-parameterized optimizer can provide explanations for why each learner update is performed.","We conduct several studies to empirically evaluate the effectiveness of VML, and hope that VML can serve as a stepping stone to stronger interpretability and trustworthiness in ML."],"url":"http://arxiv.org/abs/2406.04344v1"}
{"created":"2024-06-06 17:59:47","title":"RoboMamba: Multimodal State Space Model for Efficient Robot Reasoning and Manipulation","abstract":"A fundamental objective in robot manipulation is to enable models to comprehend visual scenes and execute actions. Although existing robot Multimodal Large Language Models (MLLMs) can handle a range of basic tasks, they still face challenges in two areas: 1) inadequate reasoning ability to tackle complex tasks, and 2) high computational costs for MLLM fine-tuning and inference. The recently proposed state space model (SSM) known as Mamba demonstrates promising capabilities in non-trivial sequence modeling with linear inference complexity. Inspired by this, we introduce RoboMamba, an end-to-end robotic MLLM that leverages the Mamba model to deliver both robotic reasoning and action capabilities, while maintaining efficient fine-tuning and inference. Specifically, we first integrate the vision encoder with Mamba, aligning visual data with language embedding through co-training, empowering our model with visual common sense and robot-related reasoning. To further equip RoboMamba with action pose prediction abilities, we explore an efficient fine-tuning strategy with a simple policy head. We find that once RoboMamba possesses sufficient reasoning capability, it can acquire manipulation skills with minimal fine-tuning parameters (0.1\\% of the model) and time (20 minutes). In experiments, RoboMamba demonstrates outstanding reasoning capabilities on general and robotic evaluation benchmarks. Meanwhile, our model showcases impressive pose prediction results in both simulation and real-world experiments, achieving inference speeds 7 times faster than existing robot MLLMs. Our project web page: https://sites.google.com/view/robomamba-web","sentences":["A fundamental objective in robot manipulation is to enable models to comprehend visual scenes and execute actions.","Although existing robot Multimodal Large Language Models (MLLMs) can handle a range of basic tasks, they still face challenges in two areas: 1) inadequate reasoning ability to tackle complex tasks, and 2) high computational costs for MLLM fine-tuning and inference.","The recently proposed state space model (SSM) known as Mamba demonstrates promising capabilities in non-trivial sequence modeling with linear inference complexity.","Inspired by this, we introduce RoboMamba, an end-to-end robotic MLLM that leverages the Mamba model to deliver both robotic reasoning and action capabilities, while maintaining efficient fine-tuning and inference.","Specifically, we first integrate the vision encoder with Mamba, aligning visual data with language embedding through co-training, empowering our model with visual common sense and robot-related reasoning.","To further equip RoboMamba with action pose prediction abilities, we explore an efficient fine-tuning strategy with a simple policy head.","We find that once RoboMamba possesses sufficient reasoning capability, it can acquire manipulation skills with minimal fine-tuning parameters (0.1\\% of the model) and time (20 minutes).","In experiments, RoboMamba demonstrates outstanding reasoning capabilities on general and robotic evaluation benchmarks.","Meanwhile, our model showcases impressive pose prediction results in both simulation and real-world experiments, achieving inference speeds 7 times faster than existing robot MLLMs.","Our project web page: https://sites.google.com/view/robomamba-web"],"url":"http://arxiv.org/abs/2406.04339v1"}
{"created":"2024-06-06 17:59:41","title":"On the Expressive Power of Spectral Invariant Graph Neural Networks","abstract":"Incorporating spectral information to enhance Graph Neural Networks (GNNs) has shown promising results but raises a fundamental challenge due to the inherent ambiguity of eigenvectors. Various architectures have been proposed to address this ambiguity, referred to as spectral invariant architectures. Notable examples include GNNs and Graph Transformers that use spectral distances, spectral projection matrices, or other invariant spectral features. However, the potential expressive power of these spectral invariant architectures remains largely unclear. The goal of this work is to gain a deep theoretical understanding of the expressive power obtainable when using spectral features. We first introduce a unified message-passing framework for designing spectral invariant GNNs, called Eigenspace Projection GNN (EPNN). A comprehensive analysis shows that EPNN essentially unifies all prior spectral invariant architectures, in that they are either strictly less expressive or equivalent to EPNN. A fine-grained expressiveness hierarchy among different architectures is also established. On the other hand, we prove that EPNN itself is bounded by a recently proposed class of Subgraph GNNs, implying that all these spectral invariant architectures are strictly less expressive than 3-WL. Finally, we discuss whether using spectral features can gain additional expressiveness when combined with more expressive GNNs.","sentences":["Incorporating spectral information to enhance Graph Neural Networks (GNNs) has shown promising results but raises a fundamental challenge due to the inherent ambiguity of eigenvectors.","Various architectures have been proposed to address this ambiguity, referred to as spectral invariant architectures.","Notable examples include GNNs and Graph Transformers that use spectral distances, spectral projection matrices, or other invariant spectral features.","However, the potential expressive power of these spectral invariant architectures remains largely unclear.","The goal of this work is to gain a deep theoretical understanding of the expressive power obtainable when using spectral features.","We first introduce a unified message-passing framework for designing spectral invariant GNNs, called Eigenspace Projection GNN (EPNN).","A comprehensive analysis shows that EPNN essentially unifies all prior spectral invariant architectures, in that they are either strictly less expressive or equivalent to EPNN.","A fine-grained expressiveness hierarchy among different architectures is also established.","On the other hand, we prove that EPNN itself is bounded by a recently proposed class of Subgraph GNNs, implying that all these spectral invariant architectures are strictly less expressive than 3-WL.","Finally, we discuss whether using spectral features can gain additional expressiveness when combined with more expressive GNNs."],"url":"http://arxiv.org/abs/2406.04336v1"}
{"created":"2024-06-06 17:59:23","title":"Coarse-To-Fine Tensor Trains for Compact Visual Representations","abstract":"The ability to learn compact, high-quality, and easy-to-optimize representations for visual data is paramount to many applications such as novel view synthesis and 3D reconstruction. Recent work has shown substantial success in using tensor networks to design such compact and high-quality representations. However, the ability to optimize tensor-based representations, and in particular, the highly compact tensor train representation, is still lacking. This has prevented practitioners from deploying the full potential of tensor networks for visual data. To this end, we propose 'Prolongation Upsampling Tensor Train (PuTT)', a novel method for learning tensor train representations in a coarse-to-fine manner. Our method involves the prolonging or `upsampling' of a learned tensor train representation, creating a sequence of 'coarse-to-fine' tensor trains that are incrementally refined. We evaluate our representation along three axes: (1). compression, (2). denoising capability, and (3). image completion capability. To assess these axes, we consider the tasks of image fitting, 3D fitting, and novel view synthesis, where our method shows an improved performance compared to state-of-the-art tensor-based methods. For full results see our project webpage: https://sebulo.github.io/PuTT_website/","sentences":["The ability to learn compact, high-quality, and easy-to-optimize representations for visual data is paramount to many applications such as novel view synthesis and 3D reconstruction.","Recent work has shown substantial success in using tensor networks to design such compact and high-quality representations.","However, the ability to optimize tensor-based representations, and in particular, the highly compact tensor train representation, is still lacking.","This has prevented practitioners from deploying the full potential of tensor networks for visual data.","To this end, we propose 'Prolongation Upsampling Tensor Train (PuTT)', a novel method for learning tensor train representations in a coarse-to-fine manner.","Our method involves the prolonging or `upsampling' of a learned tensor train representation, creating a sequence of 'coarse-to-fine' tensor trains that are incrementally refined.","We evaluate our representation along three axes: (1).","compression, (2).","denoising capability, and (3).","image completion capability.","To assess these axes, we consider the tasks of image fitting, 3D fitting, and novel view synthesis, where our method shows an improved performance compared to state-of-the-art tensor-based methods.","For full results see our project webpage: https://sebulo.github.io/PuTT_website/"],"url":"http://arxiv.org/abs/2406.04332v1"}
{"created":"2024-06-06 17:59:10","title":"Simplified and Generalized Masked Diffusion for Discrete Data","abstract":"Masked (or absorbing) diffusion is actively explored as an alternative to autoregressive models for generative modeling of discrete data. However, existing work in this area has been hindered by unnecessarily complex model formulations and unclear relationships between different perspectives, leading to suboptimal parameterization, training objectives, and ad hoc adjustments to counteract these issues. In this work, we aim to provide a simple and general framework that unlocks the full potential of masked diffusion models. We show that the continuous-time variational objective of masked diffusion models is a simple weighted integral of cross-entropy losses. Our framework also enables training generalized masked diffusion models with state-dependent masking schedules. When evaluated by perplexity, our models trained on OpenWebText surpass prior diffusion language models at GPT-2 scale and demonstrate superior performance on 4 out of 5 zero-shot language modeling tasks. Furthermore, our models vastly outperform previous discrete diffusion models on pixel-level image modeling, achieving 2.78~(CIFAR-10) and 3.42 (ImageNet 64$\\times$64) bits per dimension that are comparable or better than autoregressive models of similar sizes.","sentences":["Masked (or absorbing) diffusion is actively explored as an alternative to autoregressive models for generative modeling of discrete data.","However, existing work in this area has been hindered by unnecessarily complex model formulations and unclear relationships between different perspectives, leading to suboptimal parameterization, training objectives, and ad hoc adjustments to counteract these issues.","In this work, we aim to provide a simple and general framework that unlocks the full potential of masked diffusion models.","We show that the continuous-time variational objective of masked diffusion models is a simple weighted integral of cross-entropy losses.","Our framework also enables training generalized masked diffusion models with state-dependent masking schedules.","When evaluated by perplexity, our models trained on OpenWebText surpass prior diffusion language models at GPT-2 scale and demonstrate superior performance on 4 out of 5 zero-shot language modeling tasks.","Furthermore, our models vastly outperform previous discrete diffusion models on pixel-level image modeling, achieving 2.78~(CIFAR-10) and 3.42 (ImageNet 64$\\times$64) bits per dimension that are comparable or better than autoregressive models of similar sizes."],"url":"http://arxiv.org/abs/2406.04329v1"}
{"created":"2024-06-06 17:59:09","title":"Causal Estimation of Memorisation Profiles","abstract":"Understanding memorisation in language models has practical and societal implications, e.g., studying models' training dynamics or preventing copyright infringements. Prior work defines memorisation as the causal effect of training with an instance on the model's ability to predict that instance. This definition relies on a counterfactual: the ability to observe what would have happened had the model not seen that instance. Existing methods struggle to provide computationally efficient and accurate estimates of this counterfactual. Further, they often estimate memorisation for a model architecture rather than for a specific model instance. This paper fills an important gap in the literature, proposing a new, principled, and efficient method to estimate memorisation based on the difference-in-differences design from econometrics. Using this method, we characterise a model's memorisation profile--its memorisation trends across training--by only observing its behaviour on a small set of instances throughout training. In experiments with the Pythia model suite, we find that memorisation (i) is stronger and more persistent in larger models, (ii) is determined by data order and learning rate, and (iii) has stable trends across model sizes, thus making memorisation in larger models predictable from smaller ones.","sentences":["Understanding memorisation in language models has practical and societal implications, e.g., studying models' training dynamics or preventing copyright infringements.","Prior work defines memorisation as the causal effect of training with an instance on the model's ability to predict that instance.","This definition relies on a counterfactual: the ability to observe what would have happened had the model not seen that instance.","Existing methods struggle to provide computationally efficient and accurate estimates of this counterfactual.","Further, they often estimate memorisation for a model architecture rather than for a specific model instance.","This paper fills an important gap in the literature, proposing a new, principled, and efficient method to estimate memorisation based on the difference-in-differences design from econometrics.","Using this method, we characterise a model's memorisation profile--its memorisation trends across training--by only observing its behaviour on a small set of instances throughout training.","In experiments with the Pythia model suite, we find that memorisation (i) is stronger and more persistent in larger models, (ii) is determined by data order and learning rate, and (iii) has stable trends across model sizes, thus making memorisation in larger models predictable from smaller ones."],"url":"http://arxiv.org/abs/2406.04327v1"}
{"created":"2024-06-06 17:59:09","title":"The Brain's Bitter Lesson: Scaling Speech Decoding With Self-Supervised Learning","abstract":"The past few years have produced a series of spectacular advances in the decoding of speech from brain activity. The engine of these advances has been the acquisition of labelled data, with increasingly large datasets acquired from single subjects. However, participants exhibit anatomical and other individual differences, and datasets use varied scanners and task designs. As a result, prior work has struggled to leverage data from multiple subjects, multiple datasets, multiple tasks, and unlabelled datasets. In turn, the field has not benefited from the rapidly growing number of open neural data repositories to exploit large-scale data and deep learning. To address this, we develop an initial set of neuroscience-inspired self-supervised objectives, together with a neural architecture, for representation learning from heterogeneous and unlabelled neural recordings. Experimental results show that representations learned with these objectives generalise across subjects, datasets, and tasks, and are also learned faster than using only labelled data. In addition, we set new benchmarks for two foundational speech decoding tasks. Taken together, these methods now unlock the potential for training speech decoding models with orders of magnitude more existing data.","sentences":["The past few years have produced a series of spectacular advances in the decoding of speech from brain activity.","The engine of these advances has been the acquisition of labelled data, with increasingly large datasets acquired from single subjects.","However, participants exhibit anatomical and other individual differences, and datasets use varied scanners and task designs.","As a result, prior work has struggled to leverage data from multiple subjects, multiple datasets, multiple tasks, and unlabelled datasets.","In turn, the field has not benefited from the rapidly growing number of open neural data repositories to exploit large-scale data and deep learning.","To address this, we develop an initial set of neuroscience-inspired self-supervised objectives, together with a neural architecture, for representation learning from heterogeneous and unlabelled neural recordings.","Experimental results show that representations learned with these objectives generalise across subjects, datasets, and tasks, and are also learned faster than using only labelled data.","In addition, we set new benchmarks for two foundational speech decoding tasks.","Taken together, these methods now unlock the potential for training speech decoding models with orders of magnitude more existing data."],"url":"http://arxiv.org/abs/2406.04328v1"}
{"created":"2024-06-06 17:58:54","title":"ShareGPT4Video: Improving Video Understanding and Generation with Better Captions","abstract":"We present the ShareGPT4Video series, aiming to facilitate the video understanding of large video-language models (LVLMs) and the video generation of text-to-video models (T2VMs) via dense and precise captions. The series comprises: 1) ShareGPT4Video, 40K GPT4V annotated dense captions of videos with various lengths and sources, developed through carefully designed data filtering and annotating strategy. 2) ShareCaptioner-Video, an efficient and capable captioning model for arbitrary videos, with 4.8M high-quality aesthetic videos annotated by it. 3) ShareGPT4Video-8B, a simple yet superb LVLM that reached SOTA performance on three advancing video benchmarks. To achieve this, taking aside the non-scalable costly human annotators, we find using GPT4V to caption video with a naive multi-frame or frame-concatenation input strategy leads to less detailed and sometimes temporal-confused results. We argue the challenge of designing a high-quality video captioning strategy lies in three aspects: 1) Inter-frame precise temporal change understanding. 2) Intra-frame detailed content description. 3) Frame-number scalability for arbitrary-length videos. To this end, we meticulously designed a differential video captioning strategy, which is stable, scalable, and efficient for generating captions for videos with arbitrary resolution, aspect ratios, and length. Based on it, we construct ShareGPT4Video, which contains 40K high-quality videos spanning a wide range of categories, and the resulting captions encompass rich world knowledge, object attributes, camera movements, and crucially, detailed and precise temporal descriptions of events. Based on ShareGPT4Video, we further develop ShareCaptioner-Video, a superior captioner capable of efficiently generating high-quality captions for arbitrary videos...","sentences":["We present the ShareGPT4Video series, aiming to facilitate the video understanding of large video-language models (LVLMs) and the video generation of text-to-video models (T2VMs) via dense and precise captions.","The series comprises: 1) ShareGPT4Video, 40K GPT4V annotated dense captions of videos with various lengths and sources, developed through carefully designed data filtering and annotating strategy.","2) ShareCaptioner-Video, an efficient and capable captioning model for arbitrary videos, with 4.8M high-quality aesthetic videos annotated by it.","3) ShareGPT4Video-8B, a simple yet superb LVLM that reached SOTA performance on three advancing video benchmarks.","To achieve this, taking aside the non-scalable costly human annotators, we find using GPT4V to caption video with a naive multi-frame or frame-concatenation input strategy leads to less detailed and sometimes temporal-confused results.","We argue the challenge of designing a high-quality video captioning strategy lies in three aspects: 1) Inter-frame precise temporal change understanding.","2) Intra-frame detailed content description.","3) Frame-number scalability for arbitrary-length videos.","To this end, we meticulously designed a differential video captioning strategy, which is stable, scalable, and efficient for generating captions for videos with arbitrary resolution, aspect ratios, and length.","Based on it, we construct ShareGPT4Video, which contains 40K high-quality videos spanning a wide range of categories, and the resulting captions encompass rich world knowledge, object attributes, camera movements, and crucially, detailed and precise temporal descriptions of events.","Based on ShareGPT4Video, we further develop ShareCaptioner-Video, a superior captioner capable of efficiently generating high-quality captions for arbitrary videos..."],"url":"http://arxiv.org/abs/2406.04325v1"}
{"created":"2024-06-06 17:58:27","title":"SF-V: Single Forward Video Generation Model","abstract":"Diffusion-based video generation models have demonstrated remarkable success in obtaining high-fidelity videos through the iterative denoising process. However, these models require multiple denoising steps during sampling, resulting in high computational costs. In this work, we propose a novel approach to obtain single-step video generation models by leveraging adversarial training to fine-tune pre-trained video diffusion models. We show that, through the adversarial training, the multi-steps video diffusion model, i.e., Stable Video Diffusion (SVD), can be trained to perform single forward pass to synthesize high-quality videos, capturing both temporal and spatial dependencies in the video data. Extensive experiments demonstrate that our method achieves competitive generation quality of synthesized videos with significantly reduced computational overhead for the denoising process (i.e., around $23\\times$ speedup compared with SVD and $6\\times$ speedup compared with existing works, with even better generation quality), paving the way for real-time video synthesis and editing. More visualization results are made publicly available at https://snap-research.github.io/SF-V.","sentences":["Diffusion-based video generation models have demonstrated remarkable success in obtaining high-fidelity videos through the iterative denoising process.","However, these models require multiple denoising steps during sampling, resulting in high computational costs.","In this work, we propose a novel approach to obtain single-step video generation models by leveraging adversarial training to fine-tune pre-trained video diffusion models.","We show that, through the adversarial training, the multi-steps video diffusion model, i.e., Stable Video Diffusion (SVD), can be trained to perform single forward pass to synthesize high-quality videos, capturing both temporal and spatial dependencies in the video data.","Extensive experiments demonstrate that our method achieves competitive generation quality of synthesized videos with significantly reduced computational overhead for the denoising process (i.e., around $23\\times$ speedup compared with SVD and $6\\times$ speedup compared with existing works, with even better generation quality), paving the way for real-time video synthesis and editing.","More visualization results are made publicly available at https://snap-research.github.io/SF-V."],"url":"http://arxiv.org/abs/2406.04324v1"}
{"created":"2024-06-06 17:58:15","title":"DIRECT-3D: Learning Direct Text-to-3D Generation on Massive Noisy 3D Data","abstract":"We present DIRECT-3D, a diffusion-based 3D generative model for creating high-quality 3D assets (represented by Neural Radiance Fields) from text prompts. Unlike recent 3D generative models that rely on clean and well-aligned 3D data, limiting them to single or few-class generation, our model is directly trained on extensive noisy and unaligned `in-the-wild' 3D assets, mitigating the key challenge (i.e., data scarcity) in large-scale 3D generation. In particular, DIRECT-3D is a tri-plane diffusion model that integrates two innovations: 1) A novel learning framework where noisy data are filtered and aligned automatically during the training process. Specifically, after an initial warm-up phase using a small set of clean data, an iterative optimization is introduced in the diffusion process to explicitly estimate the 3D pose of objects and select beneficial data based on conditional density. 2) An efficient 3D representation that is achieved by disentangling object geometry and color features with two separate conditional diffusion models that are optimized hierarchically. Given a prompt input, our model generates high-quality, high-resolution, realistic, and complex 3D objects with accurate geometric details in seconds. We achieve state-of-the-art performance in both single-class generation and text-to-3D generation. We also demonstrate that DIRECT-3D can serve as a useful 3D geometric prior of objects, for example to alleviate the well-known Janus problem in 2D-lifting methods such as DreamFusion. The code and models are available for research purposes at: https://github.com/qihao067/direct3d.","sentences":["We present DIRECT-3D, a diffusion-based 3D generative model for creating high-quality 3D assets (represented by Neural Radiance Fields) from text prompts.","Unlike recent 3D generative models that rely on clean and well-aligned 3D data, limiting them to single or few-class generation, our model is directly trained on extensive noisy and unaligned `in-the-wild' 3D assets, mitigating the key challenge (i.e., data scarcity) in large-scale 3D generation.","In particular, DIRECT-3D is a tri-plane diffusion model that integrates two innovations: 1) A novel learning framework where noisy data are filtered and aligned automatically during the training process.","Specifically, after an initial warm-up phase using a small set of clean data, an iterative optimization is introduced in the diffusion process to explicitly estimate the 3D pose of objects and select beneficial data based on conditional density.","2) An efficient 3D representation that is achieved by disentangling object geometry and color features with two separate conditional diffusion models that are optimized hierarchically.","Given a prompt input, our model generates high-quality, high-resolution, realistic, and complex 3D objects with accurate geometric details in seconds.","We achieve state-of-the-art performance in both single-class generation and text-to-3D generation.","We also demonstrate that DIRECT-3D can serve as a useful 3D geometric prior of objects, for example to alleviate the well-known Janus problem in 2D-lifting methods such as DreamFusion.","The code and models are available for research purposes at: https://github.com/qihao067/direct3d."],"url":"http://arxiv.org/abs/2406.04322v1"}
{"created":"2024-06-06 17:58:15","title":"ATraDiff: Accelerating Online Reinforcement Learning with Imaginary Trajectories","abstract":"Training autonomous agents with sparse rewards is a long-standing problem in online reinforcement learning (RL), due to low data efficiency. Prior work overcomes this challenge by extracting useful knowledge from offline data, often accomplished through the learning of action distribution from offline data and utilizing the learned distribution to facilitate online RL. However, since the offline data are given and fixed, the extracted knowledge is inherently limited, making it difficult to generalize to new tasks. We propose a novel approach that leverages offline data to learn a generative diffusion model, coined as Adaptive Trajectory Diffuser (ATraDiff). This model generates synthetic trajectories, serving as a form of data augmentation and consequently enhancing the performance of online RL methods. The key strength of our diffuser lies in its adaptability, allowing it to effectively handle varying trajectory lengths and mitigate distribution shifts between online and offline data. Because of its simplicity, ATraDiff seamlessly integrates with a wide spectrum of RL methods. Empirical evaluation shows that ATraDiff consistently achieves state-of-the-art performance across a variety of environments, with particularly pronounced improvements in complicated settings. Our code and demo video are available at https://atradiff.github.io .","sentences":["Training autonomous agents with sparse rewards is a long-standing problem in online reinforcement learning (RL), due to low data efficiency.","Prior work overcomes this challenge by extracting useful knowledge from offline data, often accomplished through the learning of action distribution from offline data and utilizing the learned distribution to facilitate online RL.","However, since the offline data are given and fixed, the extracted knowledge is inherently limited, making it difficult to generalize to new tasks.","We propose a novel approach that leverages offline data to learn a generative diffusion model, coined as Adaptive Trajectory Diffuser (ATraDiff).","This model generates synthetic trajectories, serving as a form of data augmentation and consequently enhancing the performance of online RL methods.","The key strength of our diffuser lies in its adaptability, allowing it to effectively handle varying trajectory lengths and mitigate distribution shifts between online and offline data.","Because of its simplicity, ATraDiff seamlessly integrates with a wide spectrum of RL methods.","Empirical evaluation shows that ATraDiff consistently achieves state-of-the-art performance across a variety of environments, with particularly pronounced improvements in complicated settings.","Our code and demo video are available at https://atradiff.github.io ."],"url":"http://arxiv.org/abs/2406.04323v1"}
{"created":"2024-06-06 17:58:00","title":"Adaptive Sampling of k-Space in Magnetic Resonance for Rapid Pathology Prediction","abstract":"Magnetic Resonance (MR) imaging, despite its proven diagnostic utility, remains an inaccessible imaging modality for disease surveillance at the population level. A major factor rendering MR inaccessible is lengthy scan times. An MR scanner collects measurements associated with the underlying anatomy in the Fourier space, also known as the k-space. Creating a high-fidelity image requires collecting large quantities of such measurements, increasing the scan time. Traditionally to accelerate an MR scan, image reconstruction from under-sampled k-space data is the method of choice. However, recent works show the feasibility of bypassing image reconstruction and directly learning to detect disease directly from a sparser learned subset of the k-space measurements. In this work, we propose Adaptive Sampling for MR (ASMR), a sampling method that learns an adaptive policy to sequentially select k-space samples to optimize for target disease detection. On 6 out of 8 pathology classification tasks spanning the Knee, Brain, and Prostate MR scans, ASMR reaches within 2% of the performance of a fully sampled classifier while using only 8% of the k-space, as well as outperforming prior state-of-the-art work in k-space sampling such as EMRT, LOUPE, and DPS.","sentences":["Magnetic Resonance (MR) imaging, despite its proven diagnostic utility, remains an inaccessible imaging modality for disease surveillance at the population level.","A major factor rendering MR inaccessible is lengthy scan times.","An MR scanner collects measurements associated with the underlying anatomy in the Fourier space, also known as the k-space.","Creating a high-fidelity image requires collecting large quantities of such measurements, increasing the scan time.","Traditionally to accelerate an MR scan, image reconstruction from under-sampled k-space data is the method of choice.","However, recent works show the feasibility of bypassing image reconstruction and directly learning to detect disease directly from a sparser learned subset of the k-space measurements.","In this work, we propose Adaptive Sampling for MR (ASMR), a sampling method that learns an adaptive policy to sequentially select k-space samples to optimize for target disease detection.","On 6 out of 8 pathology classification tasks spanning the Knee, Brain, and Prostate MR scans, ASMR reaches within 2% of the performance of a fully sampled classifier while using only 8% of the k-space, as well as outperforming prior state-of-the-art work in k-space sampling such as EMRT, LOUPE, and DPS."],"url":"http://arxiv.org/abs/2406.04318v1"}
{"created":"2024-06-06 17:57:49","title":"Regularized KL-Divergence for Well-Defined Function-Space Variational Inference in Bayesian neural networks","abstract":"Bayesian neural networks (BNN) promise to combine the predictive performance of neural networks with principled uncertainty modeling important for safety-critical systems and decision making. However, posterior uncertainty estimates depend on the choice of prior, and finding informative priors in weight-space has proven difficult. This has motivated variational inference (VI) methods that pose priors directly on the function generated by the BNN rather than on weights. In this paper, we address a fundamental issue with such function-space VI approaches pointed out by Burt et al. (2020), who showed that the objective function (ELBO) is negative infinite for most priors of interest. Our solution builds on generalized VI (Knoblauch et al., 2019) with the regularized KL divergence (Quang, 2019) and is, to the best of our knowledge, the first well-defined variational objective for function-space inference in BNNs with Gaussian process (GP) priors. Experiments show that our method incorporates the properties specified by the GP prior on synthetic and small real-world data sets, and provides competitive uncertainty estimates for regression, classification and out-of-distribution detection compared to BNN baselines with both function and weight-space priors.","sentences":["Bayesian neural networks (BNN) promise to combine the predictive performance of neural networks with principled uncertainty modeling important for safety-critical systems and decision making.","However, posterior uncertainty estimates depend on the choice of prior, and finding informative priors in weight-space has proven difficult.","This has motivated variational inference (VI) methods that pose priors directly on the function generated by the BNN rather than on weights.","In this paper, we address a fundamental issue with such function-space VI approaches pointed out by Burt et al. (2020), who showed that the objective function (ELBO) is negative infinite for most priors of interest.","Our solution builds on generalized VI (Knoblauch et al., 2019) with the regularized KL divergence (Quang, 2019) and is, to the best of our knowledge, the first well-defined variational objective for function-space inference in BNNs with Gaussian process (GP) priors.","Experiments show that our method incorporates the properties specified by the GP prior on synthetic and small real-world data sets, and provides competitive uncertainty estimates for regression, classification and out-of-distribution detection compared to BNN baselines with both function and weight-space priors."],"url":"http://arxiv.org/abs/2406.04317v1"}
{"created":"2024-06-06 17:55:34","title":"ReFiNe: Recursive Field Networks for Cross-modal Multi-scene Representation","abstract":"The common trade-offs of state-of-the-art methods for multi-shape representation (a single model \"packing\" multiple objects) involve trading modeling accuracy against memory and storage. We show how to encode multiple shapes represented as continuous neural fields with a higher degree of precision than previously possible and with low memory usage. Key to our approach is a recursive hierarchical formulation that exploits object self-similarity, leading to a highly compressed and efficient shape latent space. Thanks to the recursive formulation, our method supports spatial and global-to-local latent feature fusion without needing to initialize and maintain auxiliary data structures, while still allowing for continuous field queries to enable applications such as raytracing. In experiments on a set of diverse datasets, we provide compelling qualitative results and demonstrate state-of-the-art multi-scene reconstruction and compression results with a single network per dataset.","sentences":["The common trade-offs of state-of-the-art methods for multi-shape representation (a single model \"packing\" multiple objects) involve trading modeling accuracy against memory and storage.","We show how to encode multiple shapes represented as continuous neural fields with a higher degree of precision than previously possible and with low memory usage.","Key to our approach is a recursive hierarchical formulation that exploits object self-similarity, leading to a highly compressed and efficient shape latent space.","Thanks to the recursive formulation, our method supports spatial and global-to-local latent feature fusion without needing to initialize and maintain auxiliary data structures, while still allowing for continuous field queries to enable applications such as raytracing.","In experiments on a set of diverse datasets, we provide compelling qualitative results and demonstrate state-of-the-art multi-scene reconstruction and compression results with a single network per dataset."],"url":"http://arxiv.org/abs/2406.04309v1"}
{"created":"2024-06-06 17:55:02","title":"Approximation-Aware Bayesian Optimization","abstract":"High-dimensional Bayesian optimization (BO) tasks such as molecular design often require 10,000 function evaluations before obtaining meaningful results. While methods like sparse variational Gaussian processes (SVGPs) reduce computational requirements in these settings, the underlying approximations result in suboptimal data acquisitions that slow the progress of optimization. In this paper we modify SVGPs to better align with the goals of BO: targeting informed data acquisition rather than global posterior fidelity. Using the framework of utility-calibrated variational inference, we unify GP approximation and data acquisition into a joint optimization problem, thereby ensuring optimal decisions under a limited computational budget. Our approach can be used with any decision-theoretic acquisition function and is compatible with trust region methods like TuRBO. We derive efficient joint objectives for the expected improvement and knowledge gradient acquisition functions in both the standard and batch BO settings. Our approach outperforms standard SVGPs on high-dimensional benchmark tasks in control and molecular design.","sentences":["High-dimensional Bayesian optimization (BO) tasks such as molecular design often require 10,000 function evaluations before obtaining meaningful results.","While methods like sparse variational Gaussian processes (SVGPs) reduce computational requirements in these settings, the underlying approximations result in suboptimal data acquisitions that slow the progress of optimization.","In this paper we modify SVGPs to better align with the goals of BO: targeting informed data acquisition rather than global posterior fidelity.","Using the framework of utility-calibrated variational inference, we unify GP approximation and data acquisition into a joint optimization problem, thereby ensuring optimal decisions under a limited computational budget.","Our approach can be used with any decision-theoretic acquisition function and is compatible with trust region methods like TuRBO.","We derive efficient joint objectives for the expected improvement and knowledge gradient acquisition functions in both the standard and batch BO settings.","Our approach outperforms standard SVGPs on high-dimensional benchmark tasks in control and molecular design."],"url":"http://arxiv.org/abs/2406.04308v1"}
{"created":"2024-06-06 17:47:41","title":"Text-to-Drive: Diverse Driving Behavior Synthesis via Large Language Models","abstract":"Generating varied scenarios through simulation is crucial for training and evaluating safety-critical systems, such as autonomous vehicles. Yet, the task of modeling the trajectories of other vehicles to simulate diverse and meaningful close interactions remains prohibitively costly. Adopting language descriptions to generate driving behaviors emerges as a promising strategy, offering a scalable and intuitive method for human operators to simulate a wide range of driving interactions. However, the scarcity of large-scale annotated language-trajectory data makes this approach challenging.   To address this gap, we propose Text-to-Drive (T2D) to synthesize diverse driving behaviors via Large Language Models (LLMs). We introduce a knowledge-driven approach that operates in two stages. In the first stage, we employ the embedded knowledge of LLMs to generate diverse language descriptions of driving behaviors for a scene. Then, we leverage LLM's reasoning capabilities to synthesize these behaviors in simulation. At its core, T2D employs an LLM to construct a state chart that maps low-level states to high-level abstractions. This strategy aids in downstream tasks such as summarizing low-level observations, assessing policy alignment with behavior description, and shaping the auxiliary reward, all without needing human supervision. With our knowledge-driven approach, we demonstrate that T2D generates more diverse trajectories compared to other baselines and offers a natural language interface that allows for interactive incorporation of human preference. Please check our website for more examples: https://text-to-drive.github.io/","sentences":["Generating varied scenarios through simulation is crucial for training and evaluating safety-critical systems, such as autonomous vehicles.","Yet, the task of modeling the trajectories of other vehicles to simulate diverse and meaningful close interactions remains prohibitively costly.","Adopting language descriptions to generate driving behaviors emerges as a promising strategy, offering a scalable and intuitive method for human operators to simulate a wide range of driving interactions.","However, the scarcity of large-scale annotated language-trajectory data makes this approach challenging.   ","To address this gap, we propose Text-to-Drive (T2D) to synthesize diverse driving behaviors via Large Language Models (LLMs).","We introduce a knowledge-driven approach that operates in two stages.","In the first stage, we employ the embedded knowledge of LLMs to generate diverse language descriptions of driving behaviors for a scene.","Then, we leverage LLM's reasoning capabilities to synthesize these behaviors in simulation.","At its core, T2D employs an LLM to construct a state chart that maps low-level states to high-level abstractions.","This strategy aids in downstream tasks such as summarizing low-level observations, assessing policy alignment with behavior description, and shaping the auxiliary reward, all without needing human supervision.","With our knowledge-driven approach, we demonstrate that T2D generates more diverse trajectories compared to other baselines and offers a natural language interface that allows for interactive incorporation of human preference.","Please check our website for more examples: https://text-to-drive.github.io/"],"url":"http://arxiv.org/abs/2406.04300v1"}
{"created":"2024-06-06 17:45:00","title":"NoisyGL: A Comprehensive Benchmark for Graph Neural Networks under Label Noise","abstract":"Graph Neural Networks (GNNs) exhibit strong potential in node classification task through a message-passing mechanism. However, their performance often hinges on high-quality node labels, which are challenging to obtain in real-world scenarios due to unreliable sources or adversarial attacks. Consequently, label noise is common in real-world graph data, negatively impacting GNNs by propagating incorrect information during training. To address this issue, the study of Graph Neural Networks under Label Noise (GLN) has recently gained traction. However, due to variations in dataset selection, data splitting, and preprocessing techniques, the community currently lacks a comprehensive benchmark, which impedes deeper understanding and further development of GLN. To fill this gap, we introduce NoisyGL in this paper, the first comprehensive benchmark for graph neural networks under label noise. NoisyGL enables fair comparisons and detailed analyses of GLN methods on noisy labeled graph data across various datasets, with unified experimental settings and interface. Our benchmark has uncovered several important insights that were missed in previous research, and we believe these findings will be highly beneficial for future studies. We hope our open-source benchmark library will foster further advancements in this field. The code of the benchmark can be found in https://github.com/eaglelab-zju/NoisyGL.","sentences":["Graph Neural Networks (GNNs) exhibit strong potential in node classification task through a message-passing mechanism.","However, their performance often hinges on high-quality node labels, which are challenging to obtain in real-world scenarios due to unreliable sources or adversarial attacks.","Consequently, label noise is common in real-world graph data, negatively impacting GNNs by propagating incorrect information during training.","To address this issue, the study of Graph Neural Networks under Label Noise (GLN) has recently gained traction.","However, due to variations in dataset selection, data splitting, and preprocessing techniques, the community currently lacks a comprehensive benchmark, which impedes deeper understanding and further development of GLN.","To fill this gap, we introduce NoisyGL in this paper, the first comprehensive benchmark for graph neural networks under label noise.","NoisyGL enables fair comparisons and detailed analyses of GLN methods on noisy labeled graph data across various datasets, with unified experimental settings and interface.","Our benchmark has uncovered several important insights that were missed in previous research, and we believe these findings will be highly beneficial for future studies.","We hope our open-source benchmark library will foster further advancements in this field.","The code of the benchmark can be found in https://github.com/eaglelab-zju/NoisyGL."],"url":"http://arxiv.org/abs/2406.04299v1"}
{"created":"2024-06-06 17:39:09","title":"Everything to the Synthetic: Diffusion-driven Test-time Adaptation via Synthetic-Domain Alignment","abstract":"Test-time adaptation (TTA) aims to enhance the performance of source-domain pretrained models when tested on unknown shifted target domains. Traditional TTA methods primarily adapt model weights based on target data streams, making model performance sensitive to the amount and order of target data. Recently, diffusion-driven TTA methods have demonstrated strong performance by using an unconditional diffusion model, which is also trained on the source domain to transform target data into synthetic data as a source domain projection. This allows the source model to make predictions without weight adaptation. In this paper, we argue that the domains of the source model and the synthetic data in diffusion-driven TTA methods are not aligned. To adapt the source model to the synthetic domain of the unconditional diffusion model, we introduce a Synthetic-Domain Alignment (SDA) framework to fine-tune the source model with synthetic data. Specifically, we first employ a conditional diffusion model to generate labeled samples, creating a synthetic dataset. Subsequently, we use the aforementioned unconditional diffusion model to add noise to and denoise each sample before fine-tuning. This process mitigates the potential domain gap between the conditional and unconditional models. Extensive experiments across various models and benchmarks demonstrate that SDA achieves superior domain alignment and consistently outperforms existing diffusion-driven TTA methods. Our code is available at https://github.com/SHI-Labs/Diffusion-Driven-Test-Time-Adaptation-via-Synthetic-Domain-Alignment.","sentences":["Test-time adaptation (TTA) aims to enhance the performance of source-domain pretrained models when tested on unknown shifted target domains.","Traditional TTA methods primarily adapt model weights based on target data streams, making model performance sensitive to the amount and order of target data.","Recently, diffusion-driven TTA methods have demonstrated strong performance by using an unconditional diffusion model, which is also trained on the source domain to transform target data into synthetic data as a source domain projection.","This allows the source model to make predictions without weight adaptation.","In this paper, we argue that the domains of the source model and the synthetic data in diffusion-driven TTA methods are not aligned.","To adapt the source model to the synthetic domain of the unconditional diffusion model, we introduce a Synthetic-Domain Alignment (SDA) framework to fine-tune the source model with synthetic data.","Specifically, we first employ a conditional diffusion model to generate labeled samples, creating a synthetic dataset.","Subsequently, we use the aforementioned unconditional diffusion model to add noise to and denoise each sample before fine-tuning.","This process mitigates the potential domain gap between the conditional and unconditional models.","Extensive experiments across various models and benchmarks demonstrate that SDA achieves superior domain alignment and consistently outperforms existing diffusion-driven TTA methods.","Our code is available at https://github.com/SHI-Labs/Diffusion-Driven-Test-Time-Adaptation-via-Synthetic-Domain-Alignment."],"url":"http://arxiv.org/abs/2406.04295v1"}
{"created":"2024-06-06 17:37:47","title":"VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval","abstract":"Multi-modal retrieval becomes increasingly popular in practice. However, the existing retrievers are mostly text-oriented, which lack the capability to process visual information. Despite the presence of vision-language models like CLIP, the current methods are severely limited in representing the text-only and image-only data. In this work, we present a new embedding model VISTA for universal multi-modal retrieval. Our work brings forth threefold technical contributions. Firstly, we introduce a flexible architecture which extends a powerful text encoder with the image understanding capability by introducing visual token embeddings. Secondly, we develop two data generation strategies, which bring high-quality composed image-text to facilitate the training of the embedding model. Thirdly, we introduce a multi-stage training algorithm, which first aligns the visual token embedding with the text encoder using massive weakly labeled data, and then develops multi-modal representation capability using the generated composed image-text data. In our experiments, VISTA achieves superior performances across a variety of multi-modal retrieval tasks in both zero-shot and supervised settings. Our model, data, and source code are available at https://github.com/FlagOpen/FlagEmbedding.","sentences":["Multi-modal retrieval becomes increasingly popular in practice.","However, the existing retrievers are mostly text-oriented, which lack the capability to process visual information.","Despite the presence of vision-language models like CLIP, the current methods are severely limited in representing the text-only and image-only data.","In this work, we present a new embedding model VISTA for universal multi-modal retrieval.","Our work brings forth threefold technical contributions.","Firstly, we introduce a flexible architecture which extends a powerful text encoder with the image understanding capability by introducing visual token embeddings.","Secondly, we develop two data generation strategies, which bring high-quality composed image-text to facilitate the training of the embedding model.","Thirdly, we introduce a multi-stage training algorithm, which first aligns the visual token embedding with the text encoder using massive weakly labeled data, and then develops multi-modal representation capability using the generated composed image-text data.","In our experiments, VISTA achieves superior performances across a variety of multi-modal retrieval tasks in both zero-shot and supervised settings.","Our model, data, and source code are available at https://github.com/FlagOpen/FlagEmbedding."],"url":"http://arxiv.org/abs/2406.04292v1"}
{"created":"2024-06-06 17:37:39","title":"Stratified Prediction-Powered Inference for Hybrid Language Model Evaluation","abstract":"Prediction-powered inference (PPI) is a method that improves statistical estimates based on limited human-labeled data. PPI achieves this by combining small amounts of human-labeled data with larger amounts of data labeled by a reasonably accurate -- but potentially biased -- automatic system, in a way that results in tighter confidence intervals for certain parameters of interest (e.g., the mean performance of a language model). In this paper, we propose a method called Stratified Prediction-Powered Inference (StratPPI), in which we show that the basic PPI estimates can be considerably improved by employing simple data stratification strategies. Without making any assumptions on the underlying automatic labeling system or data distribution, we derive an algorithm for computing provably valid confidence intervals for population parameters (such as averages) that is based on stratified sampling. In particular, we show both theoretically and empirically that, with appropriate choices of stratification and sample allocation, our approach can provide substantially tighter confidence intervals than unstratified approaches. Specifically, StratPPI is expected to improve in cases where the performance of the autorater varies across different conditional distributions of the target data.","sentences":["Prediction-powered inference (PPI) is a method that improves statistical estimates based on limited human-labeled data.","PPI achieves this by combining small amounts of human-labeled data with larger amounts of data labeled by a reasonably accurate -- but potentially biased -- automatic system, in a way that results in tighter confidence intervals for certain parameters of interest (e.g., the mean performance of a language model).","In this paper, we propose a method called Stratified Prediction-Powered Inference (StratPPI), in which we show that the basic PPI estimates can be considerably improved by employing simple data stratification strategies.","Without making any assumptions on the underlying automatic labeling system or data distribution, we derive an algorithm for computing provably valid confidence intervals for population parameters (such as averages) that is based on stratified sampling.","In particular, we show both theoretically and empirically that, with appropriate choices of stratification and sample allocation, our approach can provide substantially tighter confidence intervals than unstratified approaches.","Specifically, StratPPI is expected to improve in cases where the performance of the autorater varies across different conditional distributions of the target data."],"url":"http://arxiv.org/abs/2406.04291v1"}
{"created":"2024-06-06 17:33:23","title":"SpectralZoom: Efficient Segmentation with an Adaptive Hyperspectral Camera","abstract":"Hyperspectral image segmentation is crucial for many fields such as agriculture, remote sensing, biomedical imaging, battlefield sensing and astronomy. However, the challenge of hyper and multi spectral imaging is its large data footprint. We propose both a novel camera design and a vision transformer-based (ViT) algorithm that alleviate both the captured data footprint and the computational load for hyperspectral segmentation. Our camera is able to adaptively sample image regions or patches at different resolutions, instead of capturing the entire hyperspectral cube at one high resolution. Our segmentation algorithm works in concert with the camera, applying ViT-based segmentation only to adaptively selected patches. We show results both in simulation and on a real hardware platform demonstrating both accurate segmentation results and reduced computational burden.","sentences":["Hyperspectral image segmentation is crucial for many fields such as agriculture, remote sensing, biomedical imaging, battlefield sensing and astronomy.","However, the challenge of hyper and multi spectral imaging is its large data footprint.","We propose both a novel camera design and a vision transformer-based (ViT) algorithm that alleviate both the captured data footprint and the computational load for hyperspectral segmentation.","Our camera is able to adaptively sample image regions or patches at different resolutions, instead of capturing the entire hyperspectral cube at one high resolution.","Our segmentation algorithm works in concert with the camera, applying ViT-based segmentation only to adaptively selected patches.","We show results both in simulation and on a real hardware platform demonstrating both accurate segmentation results and reduced computational burden."],"url":"http://arxiv.org/abs/2406.04287v1"}
{"created":"2024-06-06 17:29:57","title":"ABEX: Data Augmentation for Low-Resource NLU via Expanding Abstract Descriptions","abstract":"We present ABEX, a novel and effective generative data augmentation methodology for low-resource Natural Language Understanding (NLU) tasks. ABEX is based on ABstract-and-EXpand, a novel paradigm for generating diverse forms of an input document -- we first convert a document into its concise, abstract description and then generate new documents based on expanding the resultant abstraction. To learn the task of expanding abstract descriptions, we first train BART on a large-scale synthetic dataset with abstract-document pairs. Next, to generate abstract descriptions for a document, we propose a simple, controllable, and training-free method based on editing AMR graphs. ABEX brings the best of both worlds: by expanding from abstract representations, it preserves the original semantic properties of the documents, like style and meaning, thereby maintaining alignment with the original label and data distribution. At the same time, the fundamental process of elaborating on abstract descriptions facilitates diverse generations. We demonstrate the effectiveness of ABEX on 4 NLU tasks spanning 12 datasets and 4 low-resource settings. ABEX outperforms all our baselines qualitatively with improvements of 0.04% - 38.8%. Qualitatively, ABEX outperforms all prior methods from literature in terms of context and length diversity.","sentences":["We present ABEX, a novel and effective generative data augmentation methodology for low-resource Natural Language Understanding (NLU) tasks.","ABEX is based on ABstract-and-EXpand, a novel paradigm for generating diverse forms of an input document -- we first convert a document into its concise, abstract description and then generate new documents based on expanding the resultant abstraction.","To learn the task of expanding abstract descriptions, we first train BART on a large-scale synthetic dataset with abstract-document pairs.","Next, to generate abstract descriptions for a document, we propose a simple, controllable, and training-free method based on editing AMR graphs.","ABEX brings the best of both worlds: by expanding from abstract representations, it preserves the original semantic properties of the documents, like style and meaning, thereby maintaining alignment with the original label and data distribution.","At the same time, the fundamental process of elaborating on abstract descriptions facilitates diverse generations.","We demonstrate the effectiveness of ABEX on 4 NLU tasks spanning 12 datasets and 4 low-resource settings.","ABEX outperforms all our baselines qualitatively with improvements of 0.04% - 38.8%.","Qualitatively, ABEX outperforms all prior methods from literature in terms of context and length diversity."],"url":"http://arxiv.org/abs/2406.04286v1"}
{"created":"2024-06-06 17:28:56","title":"What is Dataset Distillation Learning?","abstract":"Dataset distillation has emerged as a strategy to overcome the hurdles associated with large datasets by learning a compact set of synthetic data that retains essential information from the original dataset. While distilled data can be used to train high performing models, little is understood about how the information is stored. In this study, we posit and answer three questions about the behavior, representativeness, and point-wise information content of distilled data. We reveal distilled data cannot serve as a substitute for real data during training outside the standard evaluation setting for dataset distillation. Additionally, the distillation process retains high task performance by compressing information related to the early training dynamics of real models. Finally, we provide an framework for interpreting distilled data and reveal that individual distilled data points contain meaningful semantic information. This investigation sheds light on the intricate nature of distilled data, providing a better understanding on how they can be effectively utilized.","sentences":["Dataset distillation has emerged as a strategy to overcome the hurdles associated with large datasets by learning a compact set of synthetic data that retains essential information from the original dataset.","While distilled data can be used to train high performing models, little is understood about how the information is stored.","In this study, we posit and answer three questions about the behavior, representativeness, and point-wise information content of distilled data.","We reveal distilled data cannot serve as a substitute for real data during training outside the standard evaluation setting for dataset distillation.","Additionally, the distillation process retains high task performance by compressing information related to the early training dynamics of real models.","Finally, we provide an framework for interpreting distilled data and reveal that individual distilled data points contain meaningful semantic information.","This investigation sheds light on the intricate nature of distilled data, providing a better understanding on how they can be effectively utilized."],"url":"http://arxiv.org/abs/2406.04284v1"}
{"created":"2024-06-06 17:26:00","title":"Characterizing Similarities and Divergences in Conversational Tones in Humans and LLMs by Sampling with People","abstract":"Conversational tones -- the manners and attitudes in which speakers communicate -- are essential to effective communication. Amidst the increasing popularization of Large Language Models (LLMs) over recent years, it becomes necessary to characterize the divergences in their conversational tones relative to humans. However, existing investigations of conversational modalities rely on pre-existing taxonomies or text corpora, which suffer from experimenter bias and may not be representative of real-world distributions for the studies' psycholinguistic domains. Inspired by methods from cognitive science, we propose an iterative method for simultaneously eliciting conversational tones and sentences, where participants alternate between two tasks: (1) one participant identifies the tone of a given sentence and (2) a different participant generates a sentence based on that tone. We run 100 iterations of this process with human participants and GPT-4, then obtain a dataset of sentences and frequent conversational tones. In an additional experiment, humans and GPT-4 annotated all sentences with all tones. With data from 1,339 human participants, 33,370 human judgments, and 29,900 GPT-4 queries, we show how our approach can be used to create an interpretable geometric representation of relations between conversational tones in humans and GPT-4. This work demonstrates how combining ideas from machine learning and cognitive science can address challenges in human-computer interactions.","sentences":["Conversational tones -- the manners and attitudes in which speakers communicate -- are essential to effective communication.","Amidst the increasing popularization of Large Language Models (LLMs) over recent years, it becomes necessary to characterize the divergences in their conversational tones relative to humans.","However, existing investigations of conversational modalities rely on pre-existing taxonomies or text corpora, which suffer from experimenter bias and may not be representative of real-world distributions for the studies' psycholinguistic domains.","Inspired by methods from cognitive science, we propose an iterative method for simultaneously eliciting conversational tones and sentences, where participants alternate between two tasks: (1) one participant identifies the tone of a given sentence and (2) a different participant generates a sentence based on that tone.","We run 100 iterations of this process with human participants and GPT-4, then obtain a dataset of sentences and frequent conversational tones.","In an additional experiment, humans and GPT-4 annotated all sentences with all tones.","With data from 1,339 human participants, 33,370 human judgments, and 29,900 GPT-4 queries, we show how our approach can be used to create an interpretable geometric representation of relations between conversational tones in humans and GPT-4.","This work demonstrates how combining ideas from machine learning and cognitive science can address challenges in human-computer interactions."],"url":"http://arxiv.org/abs/2406.04278v1"}
{"created":"2024-06-06 17:25:33","title":"VideoTetris: Towards Compositional Text-to-Video Generation","abstract":"Diffusion models have demonstrated great success in text-to-video (T2V) generation. However, existing methods may face challenges when handling complex (long) video generation scenarios that involve multiple objects or dynamic changes in object numbers. To address these limitations, we propose VideoTetris, a novel framework that enables compositional T2V generation. Specifically, we propose spatio-temporal compositional diffusion to precisely follow complex textual semantics by manipulating and composing the attention maps of denoising networks spatially and temporally. Moreover, we propose an enhanced video data preprocessing to enhance the training data regarding motion dynamics and prompt understanding, equipped with a new reference frame attention mechanism to improve the consistency of auto-regressive video generation. Extensive experiments demonstrate that our VideoTetris achieves impressive qualitative and quantitative results in compositional T2V generation. Code is available at: https://github.com/YangLing0818/VideoTetris","sentences":["Diffusion models have demonstrated great success in text-to-video (T2V) generation.","However, existing methods may face challenges when handling complex (long) video generation scenarios that involve multiple objects or dynamic changes in object numbers.","To address these limitations, we propose VideoTetris, a novel framework that enables compositional T2V generation.","Specifically, we propose spatio-temporal compositional diffusion to precisely follow complex textual semantics by manipulating and composing the attention maps of denoising networks spatially and temporally.","Moreover, we propose an enhanced video data preprocessing to enhance the training data regarding motion dynamics and prompt understanding, equipped with a new reference frame attention mechanism to improve the consistency of auto-regressive video generation.","Extensive experiments demonstrate that our VideoTetris achieves impressive qualitative and quantitative results in compositional T2V generation.","Code is available at: https://github.com/YangLing0818/VideoTetris"],"url":"http://arxiv.org/abs/2406.04277v1"}
{"created":"2024-06-06 17:25:07","title":"Generative AI-in-the-loop: Integrating LLMs and GPTs into the Next Generation Networks","abstract":"In recent years, machine learning (ML) techniques have created numerous opportunities for intelligent mobile networks and have accelerated the automation of network operations. However, complex network tasks may involve variables and considerations even beyond the capacity of traditional ML algorithms. On the other hand, large language models (LLMs) have recently emerged, demonstrating near-human-level performance in cognitive tasks across various fields. However, they remain prone to hallucinations and often lack common sense in basic tasks. Therefore, they are regarded as assistive tools for humans. In this work, we propose the concept of \"generative AI-in-the-loop\" and utilize the semantic understanding, context awareness, and reasoning abilities of LLMs to assist humans in handling complex or unforeseen situations in mobile communication networks. We believe that combining LLMs and ML models allows both to leverage their respective capabilities and achieve better results than either model alone. To support this idea, we begin by analyzing the capabilities of LLMs and compare them with traditional ML algorithms. We then explore potential LLM-based applications in line with the requirements of next-generation networks. We further examine the integration of ML and LLMs, discussing how they can be used together in mobile networks. Unlike existing studies, our research emphasizes the fusion of LLMs with traditional ML-driven next-generation networks and serves as a comprehensive refinement of existing surveys. Finally, we provide a case study to enhance ML-based network intrusion detection with synthesized data generated by LLMs. Our case study further demonstrates the advantages of our proposed idea.","sentences":["In recent years, machine learning (ML) techniques have created numerous opportunities for intelligent mobile networks and have accelerated the automation of network operations.","However, complex network tasks may involve variables and considerations even beyond the capacity of traditional ML algorithms.","On the other hand, large language models (LLMs) have recently emerged, demonstrating near-human-level performance in cognitive tasks across various fields.","However, they remain prone to hallucinations and often lack common sense in basic tasks.","Therefore, they are regarded as assistive tools for humans.","In this work, we propose the concept of \"generative AI-in-the-loop\" and utilize the semantic understanding, context awareness, and reasoning abilities of LLMs to assist humans in handling complex or unforeseen situations in mobile communication networks.","We believe that combining LLMs and ML models allows both to leverage their respective capabilities and achieve better results than either model alone.","To support this idea, we begin by analyzing the capabilities of LLMs and compare them with traditional ML algorithms.","We then explore potential LLM-based applications in line with the requirements of next-generation networks.","We further examine the integration of ML and LLMs, discussing how they can be used together in mobile networks.","Unlike existing studies, our research emphasizes the fusion of LLMs with traditional ML-driven next-generation networks and serves as a comprehensive refinement of existing surveys.","Finally, we provide a case study to enhance ML-based network intrusion detection with synthesized data generated by LLMs.","Our case study further demonstrates the advantages of our proposed idea."],"url":"http://arxiv.org/abs/2406.04276v1"}
{"created":"2024-06-06 17:23:49","title":"Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models","abstract":"This work studies the challenge of aligning large language models (LLMs) with offline preference data. We focus on alignment by Reinforcement Learning from Human Feedback (RLHF) in particular. While popular preference optimization methods exhibit good empirical performance in practice, they are not theoretically guaranteed to converge to the optimal policy and can provably fail when the data coverage is sparse by classical offline reinforcement learning (RL) results. On the other hand, a recent line of work has focused on theoretically motivated preference optimization methods with provable guarantees, but these are not computationally efficient for large-scale applications like LLM alignment. To bridge this gap, we propose SPAC, a new offline preference optimization method with self-play, inspired by the on-average pessimism technique from the offline RL literature, to be the first provable and scalable approach to LLM alignment. We both provide theoretical analysis for its convergence under single-policy concentrability for the general function approximation setting and demonstrate its competitive empirical performance for LLM alignment on a 7B Mistral model with Open LLM Leaderboard evaluations.","sentences":["This work studies the challenge of aligning large language models (LLMs) with offline preference data.","We focus on alignment by Reinforcement Learning from Human Feedback (RLHF) in particular.","While popular preference optimization methods exhibit good empirical performance in practice, they are not theoretically guaranteed to converge to the optimal policy and can provably fail when the data coverage is sparse by classical offline reinforcement learning (RL) results.","On the other hand, a recent line of work has focused on theoretically motivated preference optimization methods with provable guarantees, but these are not computationally efficient for large-scale applications like LLM alignment.","To bridge this gap, we propose SPAC, a new offline preference optimization method with self-play, inspired by the on-average pessimism technique from the offline RL literature, to be the first provable and scalable approach to LLM alignment.","We both provide theoretical analysis for its convergence under single-policy concentrability for the general function approximation setting and demonstrate its competitive empirical performance for LLM alignment on a 7B Mistral model with Open LLM Leaderboard evaluations."],"url":"http://arxiv.org/abs/2406.04274v1"}
{"created":"2024-06-06 17:23:05","title":"ELFS: Enhancing Label-Free Coreset Selection via Clustering-based Pseudo-Labeling","abstract":"High-quality human-annotated data is crucial for modern deep learning pipelines, yet the human annotation process is both costly and time-consuming. Given a constrained human labeling budget, selecting an informative and representative data subset for labeling can significantly reduce human annotation effort. Well-performing state-of-the-art (SOTA) coreset selection methods require ground-truth labels over the whole dataset, failing to reduce the human labeling burden. Meanwhile, SOTA label-free coreset selection methods deliver inferior performance due to poor geometry-based scores. In this paper, we introduce ELFS, a novel label-free coreset selection method. ELFS employs deep clustering to estimate data difficulty scores without ground-truth labels. Furthermore, ELFS uses a simple but effective double-end pruning method to mitigate bias on calculated scores, which further improves the performance on selected coresets. We evaluate ELFS on five vision benchmarks and show that ELFS consistently outperforms SOTA label-free baselines. For instance, at a 90% pruning rate, ELFS surpasses the best-performing baseline by 5.3% on CIFAR10 and 7.1% on CIFAR100. Moreover, ELFS even achieves comparable performance to supervised coreset selection at low pruning rates (e.g., 30% and 50%) on CIFAR10 and ImageNet-1K.","sentences":["High-quality human-annotated data is crucial for modern deep learning pipelines, yet the human annotation process is both costly and time-consuming.","Given a constrained human labeling budget, selecting an informative and representative data subset for labeling can significantly reduce human annotation effort.","Well-performing state-of-the-art (SOTA) coreset selection methods require ground-truth labels over the whole dataset, failing to reduce the human labeling burden.","Meanwhile, SOTA label-free coreset selection methods deliver inferior performance due to poor geometry-based scores.","In this paper, we introduce ELFS, a novel label-free coreset selection method.","ELFS employs deep clustering to estimate data difficulty scores without ground-truth labels.","Furthermore, ELFS uses a simple but effective double-end pruning method to mitigate bias on calculated scores, which further improves the performance on selected coresets.","We evaluate ELFS on five vision benchmarks and show that ELFS consistently outperforms SOTA label-free baselines.","For instance, at a 90% pruning rate, ELFS surpasses the best-performing baseline by 5.3% on CIFAR10 and 7.1% on CIFAR100.","Moreover, ELFS even achieves comparable performance to supervised coreset selection at low pruning rates (e.g., 30% and 50%) on CIFAR10 and ImageNet-1K."],"url":"http://arxiv.org/abs/2406.04273v1"}
{"created":"2024-06-06 17:15:02","title":"Open-Endedness is Essential for Artificial Superhuman Intelligence","abstract":"In recent years there has been a tremendous surge in the general capabilities of AI systems, mainly fuelled by training foundation models on internetscale data. Nevertheless, the creation of openended, ever self-improving AI remains elusive. In this position paper, we argue that the ingredients are now in place to achieve openendedness in AI systems with respect to a human observer. Furthermore, we claim that such open-endedness is an essential property of any artificial superhuman intelligence (ASI). We begin by providing a concrete formal definition of open-endedness through the lens of novelty and learnability. We then illustrate a path towards ASI via open-ended systems built on top of foundation models, capable of making novel, humanrelevant discoveries. We conclude by examining the safety implications of generally-capable openended AI. We expect that open-ended foundation models will prove to be an increasingly fertile and safety-critical area of research in the near future.","sentences":["In recent years there has been a tremendous surge in the general capabilities of AI systems, mainly fuelled by training foundation models on internetscale data.","Nevertheless, the creation of openended, ever self-improving AI remains elusive.","In this position paper, we argue that the ingredients are now in place to achieve openendedness in AI systems with respect to a human observer.","Furthermore, we claim that such open-endedness is an essential property of any artificial superhuman intelligence (ASI).","We begin by providing a concrete formal definition of open-endedness through the lens of novelty and learnability.","We then illustrate a path towards ASI via open-ended systems built on top of foundation models, capable of making novel, humanrelevant discoveries.","We conclude by examining the safety implications of generally-capable openended AI.","We expect that open-ended foundation models will prove to be an increasingly fertile and safety-critical area of research in the near future."],"url":"http://arxiv.org/abs/2406.04268v1"}
{"created":"2024-06-06 17:05:09","title":"Simulating, Fast and Slow: Learning Policies for Black-Box Optimization","abstract":"In recent years, solving optimization problems involving black-box simulators has become a point of focus for the machine learning community due to their ubiquity in science and engineering. The simulators describe a forward process $f_{\\mathrm{sim}}: (\\psi, x) \\rightarrow y$ from simulation parameters $\\psi$ and input data $x$ to observations $y$, and the goal of the optimization problem is to find parameters $\\psi$ that minimize a desired loss function. Sophisticated optimization algorithms typically require gradient information regarding the forward process, $f_{\\mathrm{sim}}$, with respect to the parameters $\\psi$. However, obtaining gradients from black-box simulators can often be prohibitively expensive or, in some cases, impossible. Furthermore, in many applications, practitioners aim to solve a set of related problems. Thus, starting the optimization ``ab initio\", i.e. from scratch, each time might be inefficient if the forward model is expensive to evaluate. To address those challenges, this paper introduces a novel method for solving classes of similar black-box optimization problems by learning an active learning policy that guides a differentiable surrogate's training and uses the surrogate's gradients to optimize the simulation parameters with gradient descent. After training the policy, downstream optimization of problems involving black-box simulators requires up to $\\sim$90\\% fewer expensive simulator calls compared to baselines such as local surrogate-based approaches, numerical optimization, and Bayesian methods.","sentences":["In recent years, solving optimization problems involving black-box simulators has become a point of focus for the machine learning community due to their ubiquity in science and engineering.","The simulators describe a forward process $f_{\\mathrm{sim}}: (\\psi, x) \\rightarrow y$ from simulation parameters $\\psi$ and input data $x$ to observations $y$, and the goal of the optimization problem is to find parameters $\\psi$ that minimize a desired loss function.","Sophisticated optimization algorithms typically require gradient information regarding the forward process, $f_{\\mathrm{sim}}$, with respect to the parameters $\\psi$. However, obtaining gradients from black-box simulators can often be prohibitively expensive or, in some cases, impossible.","Furthermore, in many applications, practitioners aim to solve a set of related problems.","Thus, starting the optimization ``ab initio\", i.e. from scratch, each time might be inefficient if the forward model is expensive to evaluate.","To address those challenges, this paper introduces a novel method for solving classes of similar black-box optimization problems by learning an active learning policy that guides a differentiable surrogate's training and uses the surrogate's gradients to optimize the simulation parameters with gradient descent.","After training the policy, downstream optimization of problems involving black-box simulators requires up to $\\sim$90\\% fewer expensive simulator calls compared to baselines such as local surrogate-based approaches, numerical optimization, and Bayesian methods."],"url":"http://arxiv.org/abs/2406.04261v1"}
{"created":"2024-06-06 17:03:51","title":"Data Measurements for Decentralized Data Markets","abstract":"Decentralized data markets can provide more equitable forms of data acquisition for machine learning. However, to realize practical marketplaces, efficient techniques for seller selection need to be developed. We propose and benchmark federated data measurements to allow a data buyer to find sellers with relevant and diverse datasets. Diversity and relevance measures enable a buyer to make relative comparisons between sellers without requiring intermediate brokers and training task-dependent models.","sentences":["Decentralized data markets can provide more equitable forms of data acquisition for machine learning.","However, to realize practical marketplaces, efficient techniques for seller selection need to be developed.","We propose and benchmark federated data measurements to allow a data buyer to find sellers with relevant and diverse datasets.","Diversity and relevance measures enable a buyer to make relative comparisons between sellers without requiring intermediate brokers and training task-dependent models."],"url":"http://arxiv.org/abs/2406.04257v1"}
{"created":"2024-06-06 16:41:39","title":"Benchmark Data Contamination of Large Language Models: A Survey","abstract":"The rapid development of Large Language Models (LLMs) like GPT-4, Claude-3, and Gemini has transformed the field of natural language processing. However, it has also resulted in a significant issue known as Benchmark Data Contamination (BDC). This occurs when language models inadvertently incorporate evaluation benchmark information from their training data, leading to inaccurate or unreliable performance during the evaluation phase of the process. This paper reviews the complex challenge of BDC in LLM evaluation and explores alternative assessment methods to mitigate the risks associated with traditional benchmarks. The paper also examines challenges and future directions in mitigating BDC risks, highlighting the complexity of the issue and the need for innovative solutions to ensure the reliability of LLM evaluation in real-world applications.","sentences":["The rapid development of Large Language Models (LLMs) like GPT-4, Claude-3, and Gemini has transformed the field of natural language processing.","However, it has also resulted in a significant issue known as Benchmark Data Contamination (BDC).","This occurs when language models inadvertently incorporate evaluation benchmark information from their training data, leading to inaccurate or unreliable performance during the evaluation phase of the process.","This paper reviews the complex challenge of BDC in LLM evaluation and explores alternative assessment methods to mitigate the risks associated with traditional benchmarks.","The paper also examines challenges and future directions in mitigating BDC risks, highlighting the complexity of the issue and the need for innovative solutions to ensure the reliability of LLM evaluation in real-world applications."],"url":"http://arxiv.org/abs/2406.04244v1"}
{"created":"2024-06-06 16:39:00","title":"Hypernetworks for Personalizing ASR to Atypical Speech","abstract":"Parameter-efficient fine-tuning (PEFT) for personalizing automatic speech recognition (ASR) has recently shown promise for adapting general population models to atypical speech. However, these approaches assume a priori knowledge of the atypical speech disorder being adapted for -- the diagnosis of which requires expert knowledge that is not always available. Even given this knowledge, data scarcity and high inter/intra-speaker variability further limit the effectiveness of traditional fine-tuning. To circumvent these challenges, we first identify the minimal set of model parameters required for ASR adaptation. Our analysis of each individual parameter's effect on adaptation performance allows us to reduce Word Error Rate (WER) by half while adapting 0.03\\% of all weights. Alleviating the need for cohort-specific models, we next propose the novel use of a meta-learned hypernetwork to generate highly individualized, utterance-level adaptations on-the-fly for a diverse set of atypical speech characteristics. Evaluating adaptation at the global, cohort and individual-level, we show that hypernetworks generalize better to out-of-distribution speakers, while maintaining an overall relative WER reduction of 75.2% using 0.1% of the full parameter budget.","sentences":["Parameter-efficient fine-tuning (PEFT) for personalizing automatic speech recognition (ASR) has recently shown promise for adapting general population models to atypical speech.","However, these approaches assume a priori knowledge of the atypical speech disorder being adapted for -- the diagnosis of which requires expert knowledge that is not always available.","Even given this knowledge, data scarcity and high inter/intra-speaker variability further limit the effectiveness of traditional fine-tuning.","To circumvent these challenges, we first identify the minimal set of model parameters required for ASR adaptation.","Our analysis of each individual parameter's effect on adaptation performance allows us to reduce Word Error Rate (WER) by half while adapting 0.03\\% of all weights.","Alleviating the need for cohort-specific models, we next propose the novel use of a meta-learned hypernetwork to generate highly individualized, utterance-level adaptations on-the-fly for a diverse set of atypical speech characteristics.","Evaluating adaptation at the global, cohort and individual-level, we show that hypernetworks generalize better to out-of-distribution speakers, while maintaining an overall relative WER reduction of 75.2% using 0.1% of the full parameter budget."],"url":"http://arxiv.org/abs/2406.04240v1"}
{"created":"2024-06-06 16:38:53","title":"Solving Inverse Problems in Protein Space Using Diffusion-Based Priors","abstract":"The interaction of a protein with its environment can be understood and controlled via its 3D structure. Experimental methods for protein structure determination, such as X-ray crystallography or cryogenic electron microscopy, shed light on biological processes but introduce challenging inverse problems. Learning-based approaches have emerged as accurate and efficient methods to solve these inverse problems for 3D structure determination, but are specialized for a predefined type of measurement. Here, we introduce a versatile framework to turn raw biophysical measurements of varying types into 3D atomic models. Our method combines a physics-based forward model of the measurement process with a pretrained generative model providing a task-agnostic, data-driven prior. Our method outperforms posterior sampling baselines on both linear and non-linear inverse problems. In particular, it is the first diffusion-based method for refining atomic models from cryo-EM density maps.","sentences":["The interaction of a protein with its environment can be understood and controlled via its 3D structure.","Experimental methods for protein structure determination, such as X-ray crystallography or cryogenic electron microscopy, shed light on biological processes but introduce challenging inverse problems.","Learning-based approaches have emerged as accurate and efficient methods to solve these inverse problems for 3D structure determination, but are specialized for a predefined type of measurement.","Here, we introduce a versatile framework to turn raw biophysical measurements of varying types into 3D atomic models.","Our method combines a physics-based forward model of the measurement process with a pretrained generative model providing a task-agnostic, data-driven prior.","Our method outperforms posterior sampling baselines on both linear and non-linear inverse problems.","In particular, it is the first diffusion-based method for refining atomic models from cryo-EM density maps."],"url":"http://arxiv.org/abs/2406.04239v1"}
{"created":"2024-06-06 16:31:47","title":"FairytaleQA Translated: Enabling Educational Question and Answer Generation in Less-Resourced Languages","abstract":"Question Answering (QA) datasets are crucial in assessing reading comprehension skills for both machines and humans. While numerous datasets have been developed in English for this purpose, a noticeable void exists in less-resourced languages. To alleviate this gap, our paper introduces machine-translated versions of FairytaleQA, a renowned QA dataset designed to assess and enhance narrative comprehension skills in young children. By employing fine-tuned, modest-scale models, we establish benchmarks for both Question Generation (QG) and QA tasks within the translated datasets. In addition, we present a case study proposing a model for generating question-answer pairs, with an evaluation incorporating quality metrics such as question well-formedness, answerability, relevance, and children suitability. Our evaluation prioritizes quantifying and describing error cases, along with providing directions for future work. This paper contributes to the advancement of QA and QG research in less-resourced languages, promoting accessibility and inclusivity in the development of these models for reading comprehension. The code and data is publicly available at github.com/bernardoleite/fairytaleqa-translated.","sentences":["Question Answering (QA) datasets are crucial in assessing reading comprehension skills for both machines and humans.","While numerous datasets have been developed in English for this purpose, a noticeable void exists in less-resourced languages.","To alleviate this gap, our paper introduces machine-translated versions of FairytaleQA, a renowned QA dataset designed to assess and enhance narrative comprehension skills in young children.","By employing fine-tuned, modest-scale models, we establish benchmarks for both Question Generation (QG) and QA tasks within the translated datasets.","In addition, we present a case study proposing a model for generating question-answer pairs, with an evaluation incorporating quality metrics such as question well-formedness, answerability, relevance, and children suitability.","Our evaluation prioritizes quantifying and describing error cases, along with providing directions for future work.","This paper contributes to the advancement of QA and QG research in less-resourced languages, promoting accessibility and inclusivity in the development of these models for reading comprehension.","The code and data is publicly available at github.com/bernardoleite/fairytaleqa-translated."],"url":"http://arxiv.org/abs/2406.04233v1"}
{"created":"2024-06-06 16:31:22","title":"Quantifying Misalignment Between Agents","abstract":"Growing concerns about the AI alignment problem have emerged in recent years, with previous work focusing mainly on (1) qualitative descriptions of the alignment problem; (2) attempting to align AI actions with human interests by focusing on value specification and learning; and/or (3) focusing on a single agent or on humanity as a singular unit. Recent work in sociotechnical AI alignment has made some progress in defining alignment inclusively, but the field as a whole still lacks a systematic understanding of how to specify, describe, and analyze misalignment among entities, which may include individual humans, AI agents, and complex compositional entities such as corporations, nation-states, and so forth. Previous work on controversy in computational social science offers a mathematical model of contention among populations (of humans). In this paper, we adapt this contention model to the alignment problem, and show how misalignment can vary depending on the population of agents (human or otherwise) being observed, the domain in question, and the agents' probability-weighted preferences between possible outcomes. Our model departs from value specification approaches and focuses instead on the morass of complex, interlocking, sometimes contradictory goals that agents may have in practice. We apply our model by analyzing several case studies ranging from social media moderation to autonomous vehicle behavior. By applying our model with appropriately representative value data, AI engineers can ensure that their systems learn values maximally aligned with diverse human interests.","sentences":["Growing concerns about the AI alignment problem have emerged in recent years, with previous work focusing mainly on (1) qualitative descriptions of the alignment problem; (2) attempting to align AI actions with human interests by focusing on value specification and learning; and/or (3) focusing on a single agent or on humanity as a singular unit.","Recent work in sociotechnical AI alignment has made some progress in defining alignment inclusively, but the field as a whole still lacks a systematic understanding of how to specify, describe, and analyze misalignment among entities, which may include individual humans, AI agents, and complex compositional entities such as corporations, nation-states, and so forth.","Previous work on controversy in computational social science offers a mathematical model of contention among populations (of humans).","In this paper, we adapt this contention model to the alignment problem, and show how misalignment can vary depending on the population of agents (human or otherwise) being observed, the domain in question, and the agents' probability-weighted preferences between possible outcomes.","Our model departs from value specification approaches and focuses instead on the morass of complex, interlocking, sometimes contradictory goals that agents may have in practice.","We apply our model by analyzing several case studies ranging from social media moderation to autonomous vehicle behavior.","By applying our model with appropriately representative value data, AI engineers can ensure that their systems learn values maximally aligned with diverse human interests."],"url":"http://arxiv.org/abs/2406.04231v1"}
{"created":"2024-06-06 16:30:41","title":"M3LEO: A Multi-Modal, Multi-Label Earth Observation Dataset Integrating Interferometric SAR and RGB Data","abstract":"Satellite-based remote sensing has revolutionised the way we address global challenges in a rapidly evolving world. Huge quantities of Earth Observation (EO) data are generated by satellite sensors daily, but processing these large datasets for use in ML pipelines is technically and computationally challenging. Specifically, different types of EO data are often hosted on a variety of platforms, with differing availability for Python preprocessing tools. In addition, spatial alignment across data sources and data tiling can present significant technical hurdles for novice users. While some preprocessed EO datasets exist, their content is often limited to optical or near-optical wavelength data, which is ineffective at night or in adverse weather conditions. Synthetic Aperture Radar (SAR), an active sensing technique based on microwave length radiation, offers a viable alternative. However, the application of machine learning to SAR has been limited due to a lack of ML-ready data and pipelines, particularly for the full diversity of SAR data, including polarimetry, coherence and interferometry. We introduce M3LEO, a multi-modal, multi-label EO dataset that includes polarimetric, interferometric, and coherence SAR data derived from Sentinel-1, alongside Sentinel-2 RGB imagery and a suite of labelled tasks for model evaluation. M3LEO spans 17.5TB and contains approximately 10M data chips across six geographic regions. The dataset is complemented by a flexible PyTorch Lightning framework, with configuration management using Hydra. We provide tools to process any dataset available on popular platforms such as Google Earth Engine for integration with our framework. Initial experiments validate the utility of our data and framework, showing that SAR imagery contains information additional to that extractable from RGB data. Data at huggingface.co/M3LEO, and code at github.com/spaceml-org/M3LEO.","sentences":["Satellite-based remote sensing has revolutionised the way we address global challenges in a rapidly evolving world.","Huge quantities of Earth Observation (EO) data are generated by satellite sensors daily, but processing these large datasets for use in ML pipelines is technically and computationally challenging.","Specifically, different types of EO data are often hosted on a variety of platforms, with differing availability for Python preprocessing tools.","In addition, spatial alignment across data sources and data tiling can present significant technical hurdles for novice users.","While some preprocessed EO datasets exist, their content is often limited to optical or near-optical wavelength data, which is ineffective at night or in adverse weather conditions.","Synthetic Aperture Radar (SAR), an active sensing technique based on microwave length radiation, offers a viable alternative.","However, the application of machine learning to SAR has been limited due to a lack of ML-ready data and pipelines, particularly for the full diversity of SAR data, including polarimetry, coherence and interferometry.","We introduce M3LEO, a multi-modal, multi-label EO dataset that includes polarimetric, interferometric, and coherence SAR data derived from Sentinel-1, alongside Sentinel-2 RGB imagery and a suite of labelled tasks for model evaluation.","M3LEO spans 17.5TB and contains approximately 10M data chips across six geographic regions.","The dataset is complemented by a flexible PyTorch Lightning framework, with configuration management using Hydra.","We provide tools to process any dataset available on popular platforms such as Google Earth Engine for integration with our framework.","Initial experiments validate the utility of our data and framework, showing that SAR imagery contains information additional to that extractable from RGB data.","Data at huggingface.co/M3LEO, and code at github.com/spaceml-org/M3LEO."],"url":"http://arxiv.org/abs/2406.04230v1"}
{"created":"2024-06-06 16:29:25","title":"The CLRS-Text Algorithmic Reasoning Language Benchmark","abstract":"Eliciting reasoning capabilities from language models (LMs) is a critical direction on the path towards building intelligent systems. Most recent studies dedicated to reasoning focus on out-of-distribution performance on procedurally-generated synthetic benchmarks, bespoke-built to evaluate specific skills only. This trend makes results hard to transfer across publications, slowing down progress. Three years ago, a similar issue was identified and rectified in the field of neural algorithmic reasoning, with the advent of the CLRS benchmark. CLRS is a dataset generator comprising graph execution traces of classical algorithms from the Introduction to Algorithms textbook. Inspired by this, we propose CLRS-Text -- a textual version of these algorithmic traces. Out of the box, CLRS-Text is capable of procedurally generating trace data for thirty diverse, challenging algorithmic tasks across any desirable input distribution, while offering a standard pipeline in which any additional algorithmic tasks may be created in the benchmark. We fine-tune and evaluate various LMs as generalist executors on this benchmark, validating prior work and revealing a novel, interesting challenge for the LM reasoning community. Our code is available at https://github.com/google-deepmind/clrs/tree/master/clrs/_src/clrs_text.","sentences":["Eliciting reasoning capabilities from language models (LMs) is a critical direction on the path towards building intelligent systems.","Most recent studies dedicated to reasoning focus on out-of-distribution performance on procedurally-generated synthetic benchmarks, bespoke-built to evaluate specific skills only.","This trend makes results hard to transfer across publications, slowing down progress.","Three years ago, a similar issue was identified and rectified in the field of neural algorithmic reasoning, with the advent of the CLRS benchmark.","CLRS is a dataset generator comprising graph execution traces of classical algorithms from the Introduction to Algorithms textbook.","Inspired by this, we propose CLRS-Text -- a textual version of these algorithmic traces.","Out of the box, CLRS-Text is capable of procedurally generating trace data for thirty diverse, challenging algorithmic tasks across any desirable input distribution, while offering a standard pipeline in which any additional algorithmic tasks may be created in the benchmark.","We fine-tune and evaluate various LMs as generalist executors on this benchmark, validating prior work and revealing a novel, interesting challenge for the LM reasoning community.","Our code is available at https://github.com/google-deepmind/clrs/tree/master/clrs/_src/clrs_text."],"url":"http://arxiv.org/abs/2406.04229v1"}
{"created":"2024-06-06 16:28:04","title":"R-CONV: An Analytical Approach for Efficient Data Reconstruction via Convolutional Gradients","abstract":"In the effort to learn from extensive collections of distributed data, federated learning has emerged as a promising approach for preserving privacy by using a gradient-sharing mechanism instead of exchanging raw data. However, recent studies show that private training data can be leaked through many gradient attacks. While previous analytical-based attacks have successfully reconstructed input data from fully connected layers, their effectiveness diminishes when applied to convolutional layers. This paper introduces an advanced data leakage method to efficiently exploit convolutional layers' gradients. We present a surprising finding: even with non-fully invertible activation functions, such as ReLU, we can analytically reconstruct training samples from the gradients. To the best of our knowledge, this is the first analytical approach that successfully reconstructs convolutional layer inputs directly from the gradients, bypassing the need to reconstruct layers' outputs. Prior research has mainly concentrated on the weight constraints of convolution layers, overlooking the significance of gradient constraints. Our findings demonstrate that existing analytical methods used to estimate the risk of gradient attacks lack accuracy. In some layers, attacks can be launched with less than 5% of the reported constraints.","sentences":["In the effort to learn from extensive collections of distributed data, federated learning has emerged as a promising approach for preserving privacy by using a gradient-sharing mechanism instead of exchanging raw data.","However, recent studies show that private training data can be leaked through many gradient attacks.","While previous analytical-based attacks have successfully reconstructed input data from fully connected layers, their effectiveness diminishes when applied to convolutional layers.","This paper introduces an advanced data leakage method to efficiently exploit convolutional layers' gradients.","We present a surprising finding: even with non-fully invertible activation functions, such as ReLU, we can analytically reconstruct training samples from the gradients.","To the best of our knowledge, this is the first analytical approach that successfully reconstructs convolutional layer inputs directly from the gradients, bypassing the need to reconstruct layers' outputs.","Prior research has mainly concentrated on the weight constraints of convolution layers, overlooking the significance of gradient constraints.","Our findings demonstrate that existing analytical methods used to estimate the risk of gradient attacks lack accuracy.","In some layers, attacks can be launched with less than 5% of the reported constraints."],"url":"http://arxiv.org/abs/2406.04227v1"}
{"created":"2024-06-06 16:20:07","title":"Matching Anything by Segmenting Anything","abstract":"The robust association of the same objects across video frames in complex scenes is crucial for many applications, especially Multiple Object Tracking (MOT). Current methods predominantly rely on labeled domain-specific video datasets, which limits the cross-domain generalization of learned similarity embeddings. We propose MASA, a novel method for robust instance association learning, capable of matching any objects within videos across diverse domains without tracking labels. Leveraging the rich object segmentation from the Segment Anything Model (SAM), MASA learns instance-level correspondence through exhaustive data transformations. We treat the SAM outputs as dense object region proposals and learn to match those regions from a vast image collection. We further design a universal MASA adapter which can work in tandem with foundational segmentation or detection models and enable them to track any detected objects. Those combinations present strong zero-shot tracking ability in complex domains. Extensive tests on multiple challenging MOT and MOTS benchmarks indicate that the proposed method, using only unlabeled static images, achieves even better performance than state-of-the-art methods trained with fully annotated in-domain video sequences, in zero-shot association. Project Page: https://matchinganything.github.io/","sentences":["The robust association of the same objects across video frames in complex scenes is crucial for many applications, especially Multiple Object Tracking (MOT).","Current methods predominantly rely on labeled domain-specific video datasets, which limits the cross-domain generalization of learned similarity embeddings.","We propose MASA, a novel method for robust instance association learning, capable of matching any objects within videos across diverse domains without tracking labels.","Leveraging the rich object segmentation from the Segment Anything Model (SAM), MASA learns instance-level correspondence through exhaustive data transformations.","We treat the SAM outputs as dense object region proposals and learn to match those regions from a vast image collection.","We further design a universal MASA adapter which can work in tandem with foundational segmentation or detection models and enable them to track any detected objects.","Those combinations present strong zero-shot tracking ability in complex domains.","Extensive tests on multiple challenging MOT and MOTS benchmarks indicate that the proposed method, using only unlabeled static images, achieves even better performance than state-of-the-art methods trained with fully annotated in-domain video sequences, in zero-shot association.","Project Page: https://matchinganything.github.io/"],"url":"http://arxiv.org/abs/2406.04221v1"}
{"created":"2024-06-06 16:18:30","title":"BEADs: Bias Evaluation Across Domains","abstract":"Recent improvements in large language models (LLMs) have significantly enhanced natural language processing (NLP) applications. However, these models can also inherit and perpetuate biases from their training data. Addressing this issue is crucial, yet many existing datasets do not offer evaluation across diverse NLP tasks. To tackle this, we introduce the Bias Evaluations Across Domains (BEADs) dataset, designed to support a wide range of NLP tasks, including text classification, bias entity recognition, bias quantification, and benign language generation. BEADs uses AI-driven annotation combined with experts' verification to provide reliable labels. This method overcomes the limitations of existing datasets that typically depend on crowd-sourcing, expert-only annotations with limited bias evaluations, or unverified AI labeling. Our empirical analysis shows that BEADs is effective in detecting and reducing biases across different language models, with smaller models fine-tuned on BEADs often outperforming LLMs in bias classification tasks. However, these models may still exhibit biases towards certain demographics. Fine-tuning LLMs with our benign language data also reduces biases while preserving the models' knowledge. Our findings highlight the importance of comprehensive bias evaluation and the potential of targeted fine-tuning for reducing the bias of LLMs. We are making BEADs publicly available at https://huggingface.co/datasets/shainar/BEAD   Warning: This paper contains examples that may be considered offensive.","sentences":["Recent improvements in large language models (LLMs) have significantly enhanced natural language processing (NLP) applications.","However, these models can also inherit and perpetuate biases from their training data.","Addressing this issue is crucial, yet many existing datasets do not offer evaluation across diverse NLP tasks.","To tackle this, we introduce the Bias Evaluations Across Domains (BEADs) dataset, designed to support a wide range of NLP tasks, including text classification, bias entity recognition, bias quantification, and benign language generation.","BEADs uses AI-driven annotation combined with experts' verification to provide reliable labels.","This method overcomes the limitations of existing datasets that typically depend on crowd-sourcing, expert-only annotations with limited bias evaluations, or unverified AI labeling.","Our empirical analysis shows that BEADs is effective in detecting and reducing biases across different language models, with smaller models fine-tuned on BEADs often outperforming LLMs in bias classification tasks.","However, these models may still exhibit biases towards certain demographics.","Fine-tuning LLMs with our benign language data also reduces biases while preserving the models' knowledge.","Our findings highlight the importance of comprehensive bias evaluation and the potential of targeted fine-tuning for reducing the bias of LLMs.","We are making BEADs publicly available at https://huggingface.co/datasets/shainar/BEAD   Warning: This paper contains examples that may be considered offensive."],"url":"http://arxiv.org/abs/2406.04220v1"}
{"created":"2024-06-06 16:14:16","title":"ValueBench: Towards Comprehensively Evaluating Value Orientations and Understanding of Large Language Models","abstract":"Large Language Models (LLMs) are transforming diverse fields and gaining increasing influence as human proxies. This development underscores the urgent need for evaluating value orientations and understanding of LLMs to ensure their responsible integration into public-facing applications. This work introduces ValueBench, the first comprehensive psychometric benchmark for evaluating value orientations and value understanding in LLMs. ValueBench collects data from 44 established psychometric inventories, encompassing 453 multifaceted value dimensions. We propose an evaluation pipeline grounded in realistic human-AI interactions to probe value orientations, along with novel tasks for evaluating value understanding in an open-ended value space. With extensive experiments conducted on six representative LLMs, we unveil their shared and distinctive value orientations and exhibit their ability to approximate expert conclusions in value-related extraction and generation tasks. ValueBench is openly accessible at https://github.com/Value4AI/ValueBench.","sentences":["Large Language Models (LLMs) are transforming diverse fields and gaining increasing influence as human proxies.","This development underscores the urgent need for evaluating value orientations and understanding of LLMs to ensure their responsible integration into public-facing applications.","This work introduces ValueBench, the first comprehensive psychometric benchmark for evaluating value orientations and value understanding in LLMs.","ValueBench collects data from 44 established psychometric inventories, encompassing 453 multifaceted value dimensions.","We propose an evaluation pipeline grounded in realistic human-AI interactions to probe value orientations, along with novel tasks for evaluating value understanding in an open-ended value space.","With extensive experiments conducted on six representative LLMs, we unveil their shared and distinctive value orientations and exhibit their ability to approximate expert conclusions in value-related extraction and generation tasks.","ValueBench is openly accessible at https://github.com/Value4AI/ValueBench."],"url":"http://arxiv.org/abs/2406.04214v1"}
{"created":"2024-06-06 16:00:20","title":"Legal Documents Drafting with Fine-Tuned Pre-Trained Large Language Model","abstract":"With the development of large-scale Language Models (LLM), fine-tuning pre-trained LLM has become a mainstream paradigm for solving downstream tasks of natural language processing. However, training a language model in the legal field requires a large number of legal documents so that the language model can learn legal terminology and the particularity of the format of legal documents. The typical NLP approaches usually rely on many manually annotated data sets for training. However, in the legal field application, it is difficult to obtain a large number of manually annotated data sets, which restricts the typical method applied to the task of drafting legal documents. The experimental results of this paper show that not only can we leverage a large number of annotation-free legal documents without Chinese word segmentation to fine-tune a large-scale language model, but more importantly, it can fine-tune a pre-trained LLM on the local computer to achieve the generating legal document drafts task, and at the same time achieve the protection of information privacy and to improve information security issues.","sentences":["With the development of large-scale Language Models (LLM), fine-tuning pre-trained LLM has become a mainstream paradigm for solving downstream tasks of natural language processing.","However, training a language model in the legal field requires a large number of legal documents so that the language model can learn legal terminology and the particularity of the format of legal documents.","The typical NLP approaches usually rely on many manually annotated data sets for training.","However, in the legal field application, it is difficult to obtain a large number of manually annotated data sets, which restricts the typical method applied to the task of drafting legal documents.","The experimental results of this paper show that not only can we leverage a large number of annotation-free legal documents without Chinese word segmentation to fine-tune a large-scale language model, but more importantly, it can fine-tune a pre-trained LLM on the local computer to achieve the generating legal document drafts task, and at the same time achieve the protection of information privacy and to improve information security issues."],"url":"http://arxiv.org/abs/2406.04202v1"}
{"created":"2024-06-06 15:55:53","title":"DICE: Detecting In-distribution Contamination in LLM's Fine-tuning Phase for Math Reasoning","abstract":"The advancement of large language models (LLMs) relies on evaluation using public benchmarks, but data contamination can lead to overestimated performance. Previous researches focus on detecting contamination by determining whether the model has seen the exact same data during training. In this work, we argue that even training on data similar to benchmark data inflates performance on in-distribution tasks without improving overall capacity, which we called In-distribution contamination. To effectively detect in-distribution contamination, we propose DICE, a novel method that leverages the internal states of LLMs to locate-then-detect the contamination. DICE first identifies the most sensitive layer to contamination, then trains a classifier based on the internal states of that layer. Experiments reveal DICE's high accuracy in detecting in-distribution contamination across various LLMs and math reasoning datasets. We also show the generalization capability of the trained DICE detector, which is able to detect contamination across multiple benchmarks with similar distributions. Additionally, we find that the DICE detection scores are positively correlated with the performance of ten LLMs fine-tuned by either us or other organizations on four math reasoning datasets (with $R^2$ values between 0.6 and 0.75). This indicates that the in-distribution contamination problem potentially lead to an overestimation of the true capabilities of many existing models. The code and data are available at https://github.com/THU-KEG/DICE.","sentences":["The advancement of large language models (LLMs) relies on evaluation using public benchmarks, but data contamination can lead to overestimated performance.","Previous researches focus on detecting contamination by determining whether the model has seen the exact same data during training.","In this work, we argue that even training on data similar to benchmark data inflates performance on in-distribution tasks without improving overall capacity, which we called In-distribution contamination.","To effectively detect in-distribution contamination, we propose DICE, a novel method that leverages the internal states of LLMs to locate-then-detect the contamination.","DICE first identifies the most sensitive layer to contamination, then trains a classifier based on the internal states of that layer.","Experiments reveal DICE's high accuracy in detecting in-distribution contamination across various LLMs and math reasoning datasets.","We also show the generalization capability of the trained DICE detector, which is able to detect contamination across multiple benchmarks with similar distributions.","Additionally, we find that the DICE detection scores are positively correlated with the performance of ten LLMs fine-tuned by either us or other organizations on four math reasoning datasets (with $R^2$ values between 0.6 and 0.75).","This indicates that the in-distribution contamination problem potentially lead to an overestimation of the true capabilities of many existing models.","The code and data are available at https://github.com/THU-KEG/DICE."],"url":"http://arxiv.org/abs/2406.04197v1"}
{"created":"2024-06-06 15:40:29","title":"Shield Synthesis for LTL Modulo Theories","abstract":"In recent years, Machine Learning (ML) models have achieved remarkable success in various domains. However, these models also tend to demonstrate unsafe behaviors, precluding their deployment in safety-critical systems. To cope with this issue, ample research focuses on developing methods that guarantee the safe behaviour of a given ML model. A prominent example is shielding which incorporates an external component (a \"shield\") that blocks unwanted behavior. Despite significant progress, shielding suffers from a main setback: it is currently geared towards properties encoded solely in propositional logics (e.g., LTL) and is unsuitable for richer logics. This, in turn, limits the widespread applicability of shielding in many real-world systems. In this work, we address this gap, and extend shielding to LTL modulo theories, by building upon recent advances in reactive synthesis modulo theories. This allowed us to develop a novel approach for generating shields conforming to complex safety specifications in these more expressive, logics. We evaluated our shields and demonstrate their ability to handle rich data with temporal dynamics. To the best of our knowledge, this is the first approach for synthesizing shields for such expressivity.","sentences":["In recent years, Machine Learning (ML) models have achieved remarkable success in various domains.","However, these models also tend to demonstrate unsafe behaviors, precluding their deployment in safety-critical systems.","To cope with this issue, ample research focuses on developing methods that guarantee the safe behaviour of a given ML model.","A prominent example is shielding which incorporates an external component (a \"shield\") that blocks unwanted behavior.","Despite significant progress, shielding suffers from a main setback: it is currently geared towards properties encoded solely in propositional logics (e.g., LTL) and is unsuitable for richer logics.","This, in turn, limits the widespread applicability of shielding in many real-world systems.","In this work, we address this gap, and extend shielding to LTL modulo theories, by building upon recent advances in reactive synthesis modulo theories.","This allowed us to develop a novel approach for generating shields conforming to complex safety specifications in these more expressive, logics.","We evaluated our shields and demonstrate their ability to handle rich data with temporal dynamics.","To the best of our knowledge, this is the first approach for synthesizing shields for such expressivity."],"url":"http://arxiv.org/abs/2406.04184v1"}
{"created":"2024-06-06 15:35:25","title":"A Voxel-based Approach for Simulating Microbial Decomposition in Soil: Comparison with LBM and Improvement of Morphological Models","abstract":"This study presents a new computational approach for simulating the microbial decomposition of organic matter, from 3D micro-computed tomography (micro-CT) images of soil. The method employs a valuated graph of connected voxels to simulate transformation and diffusion processes involved in microbial decomposition within the complex soil matrix. The resulting model can be adapted to simulate any diffusion-transformation processes in porous media. We implemented parallelization strategies and explored different numerical methods, including implicit, explicit, synchronous, and asynchronous schemes. To validate our method, we compared simulation outputs with those provided by LBioS and by Mosaic models. LBioS uses a lattice-Boltzmann method for diffusion and Mosaic takes benefit of Pore Network Geometrical Modelling (PNGM) by means of geometrical primitives such as spheres and ellipsoids. This approach achieved comparable results to traditional LBM-based simulations, but required only one-fourth of the computing time. Compared to Mosaic simulation, the proposed method is slower but more accurate and does not require any calibration. Furthermore, we present a theoretical framework and an application example to enhance PNGM-based simulations. This is accomplished by approximating the diffusional conductance coefficients using stochastic gradient descent and data generated by the current approach.","sentences":["This study presents a new computational approach for simulating the microbial decomposition of organic matter, from 3D micro-computed tomography (micro-CT) images of soil.","The method employs a valuated graph of connected voxels to simulate transformation and diffusion processes involved in microbial decomposition within the complex soil matrix.","The resulting model can be adapted to simulate any diffusion-transformation processes in porous media.","We implemented parallelization strategies and explored different numerical methods, including implicit, explicit, synchronous, and asynchronous schemes.","To validate our method, we compared simulation outputs with those provided by LBioS and by Mosaic models.","LBioS uses a lattice-Boltzmann method for diffusion and Mosaic takes benefit of Pore Network Geometrical Modelling (PNGM) by means of geometrical primitives such as spheres and ellipsoids.","This approach achieved comparable results to traditional LBM-based simulations, but required only one-fourth of the computing time.","Compared to Mosaic simulation, the proposed method is slower but more accurate and does not require any calibration.","Furthermore, we present a theoretical framework and an application example to enhance PNGM-based simulations.","This is accomplished by approximating the diffusional conductance coefficients using stochastic gradient descent and data generated by the current approach."],"url":"http://arxiv.org/abs/2406.04177v1"}
{"created":"2024-06-06 15:22:33","title":"Repurposing Language Models into Embedding Models: Finding the Compute-Optimal Recipe","abstract":"Text embeddings are essential for many tasks, such as document retrieval, clustering, and semantic similarity assessment. In this paper, we study how to contrastively train text embedding models in a compute-optimal fashion, given a suite of pre-trained decoder-only language models. Our innovation is an algorithm that produces optimal configurations of model sizes, data quantities, and fine-tuning methods for text-embedding models at different computational budget levels. The resulting recipe, which we obtain through extensive experiments, can be used by practitioners to make informed design choices for their embedding models. Specifically, our findings suggest that full fine-tuning and low-rank adaptation fine-tuning produce optimal models at lower and higher computational budgets respectively.","sentences":["Text embeddings are essential for many tasks, such as document retrieval, clustering, and semantic similarity assessment.","In this paper, we study how to contrastively train text embedding models in a compute-optimal fashion, given a suite of pre-trained decoder-only language models.","Our innovation is an algorithm that produces optimal configurations of model sizes, data quantities, and fine-tuning methods for text-embedding models at different computational budget levels.","The resulting recipe, which we obtain through extensive experiments, can be used by practitioners to make informed design choices for their embedding models.","Specifically, our findings suggest that full fine-tuning and low-rank adaptation fine-tuning produce optimal models at lower and higher computational budgets respectively."],"url":"http://arxiv.org/abs/2406.04165v1"}
{"created":"2024-06-06 15:18:59","title":"Sparse Multi-baseline SAR Cross-modal 3D Reconstruction of Vehicle Targets","abstract":"Multi-baseline SAR 3D imaging faces significant challenges due to data sparsity. In recent years, deep learning techniques have achieved notable success in enhancing the quality of sparse SAR 3D imaging. However, previous work typically rely on full-aperture high-resolution radar images to supervise the training of deep neural networks (DNNs), utilizing only single-modal information from radar data. Consequently, imaging performance is limited, and acquiring full-aperture data for multi-baseline SAR is costly and sometimes impractical in real-world applications. In this paper, we propose a Cross-Modal Reconstruction Network (CMR-Net), which integrates differentiable render and cross-modal supervision with optical images to reconstruct highly sparse multi-baseline SAR 3D images of vehicle targets into visually structured and high-resolution images. We meticulously designed the network architecture and training strategies to enhance network generalization capability. Remarkably, CMR-Net, trained solely on simulated data, demonstrates high-resolution reconstruction capabilities on both publicly available simulation datasets and real measured datasets, outperforming traditional sparse reconstruction algorithms based on compressed sensing and other learning-based methods. Additionally, using optical images as supervision provides a cost-effective way to build training datasets, reducing the difficulty of method dissemination. Our work showcases the broad prospects of deep learning in multi-baseline SAR 3D imaging and offers a novel path for researching radar imaging based on cross-modal learning theory.","sentences":["Multi-baseline SAR 3D imaging faces significant challenges due to data sparsity.","In recent years, deep learning techniques have achieved notable success in enhancing the quality of sparse SAR 3D imaging.","However, previous work typically rely on full-aperture high-resolution radar images to supervise the training of deep neural networks (DNNs), utilizing only single-modal information from radar data.","Consequently, imaging performance is limited, and acquiring full-aperture data for multi-baseline SAR is costly and sometimes impractical in real-world applications.","In this paper, we propose a Cross-Modal Reconstruction Network (CMR-Net), which integrates differentiable render and cross-modal supervision with optical images to reconstruct highly sparse multi-baseline SAR 3D images of vehicle targets into visually structured and high-resolution images.","We meticulously designed the network architecture and training strategies to enhance network generalization capability.","Remarkably, CMR-Net, trained solely on simulated data, demonstrates high-resolution reconstruction capabilities on both publicly available simulation datasets and real measured datasets, outperforming traditional sparse reconstruction algorithms based on compressed sensing and other learning-based methods.","Additionally, using optical images as supervision provides a cost-effective way to build training datasets, reducing the difficulty of method dissemination.","Our work showcases the broad prospects of deep learning in multi-baseline SAR 3D imaging and offers a novel path for researching radar imaging based on cross-modal learning theory."],"url":"http://arxiv.org/abs/2406.04158v1"}
{"created":"2024-06-06 15:17:00","title":"Learned Feature Importance Scores for Automated Feature Engineering","abstract":"Feature engineering has demonstrated substantial utility for many machine learning workflows, such as in the small data regime or when distribution shifts are severe. Thus automating this capability can relieve much manual effort and improve model performance. Towards this, we propose AutoMAN, or Automated Mask-based Feature Engineering, an automated feature engineering framework that achieves high accuracy, low latency, and can be extended to heterogeneous and time-varying data. AutoMAN is based on effectively exploring the candidate transforms space, without explicitly manifesting transformed features. This is achieved by learning feature importance masks, which can be extended to support other modalities such as time series. AutoMAN learns feature transform importance end-to-end, incorporating a dataset's task target directly into feature engineering, resulting in state-of-the-art performance with significantly lower latency compared to alternatives.","sentences":["Feature engineering has demonstrated substantial utility for many machine learning workflows, such as in the small data regime or when distribution shifts are severe.","Thus automating this capability can relieve much manual effort and improve model performance.","Towards this, we propose AutoMAN, or Automated Mask-based Feature Engineering, an automated feature engineering framework that achieves high accuracy, low latency, and can be extended to heterogeneous and time-varying data.","AutoMAN is based on effectively exploring the candidate transforms space, without explicitly manifesting transformed features.","This is achieved by learning feature importance masks, which can be extended to support other modalities such as time series.","AutoMAN learns feature transform importance end-to-end, incorporating a dataset's task target directly into feature engineering, resulting in state-of-the-art performance with significantly lower latency compared to alternatives."],"url":"http://arxiv.org/abs/2406.04153v1"}
{"created":"2024-06-06 15:15:41","title":"AgentGym: Evolving Large Language Model-based Agents across Diverse Environments","abstract":"Building generalist agents that can handle diverse tasks and evolve themselves across different environments is a long-term goal in the AI community. Large language models (LLMs) are considered a promising foundation to build such agents due to their generalized capabilities. Current approaches either have LLM-based agents imitate expert-provided trajectories step-by-step, requiring human supervision, which is hard to scale and limits environmental exploration; or they let agents explore and learn in isolated environments, resulting in specialist agents with limited generalization. In this paper, we take the first step towards building generally-capable LLM-based agents with self-evolution ability. We identify a trinity of ingredients: 1) diverse environments for agent exploration and learning, 2) a trajectory set to equip agents with basic capabilities and prior knowledge, and 3) an effective and scalable evolution method. We propose AgentGym, a new framework featuring a variety of environments and tasks for broad, real-time, uni-format, and concurrent agent exploration. AgentGym also includes a database with expanded instructions, a benchmark suite, and high-quality trajectories across environments. Next, we propose a novel method, AgentEvol, to investigate the potential of agent self-evolution beyond previously seen data across tasks and environments. Experimental results show that the evolved agents can achieve results comparable to SOTA models. We release the AgentGym suite, including the platform, dataset, benchmark, checkpoints, and algorithm implementations. The AgentGym suite is available on https://github.com/WooooDyy/AgentGym.","sentences":["Building generalist agents that can handle diverse tasks and evolve themselves across different environments is a long-term goal in the AI community.","Large language models (LLMs) are considered a promising foundation to build such agents due to their generalized capabilities.","Current approaches either have LLM-based agents imitate expert-provided trajectories step-by-step, requiring human supervision, which is hard to scale and limits environmental exploration; or they let agents explore and learn in isolated environments, resulting in specialist agents with limited generalization.","In this paper, we take the first step towards building generally-capable LLM-based agents with self-evolution ability.","We identify a trinity of ingredients: 1) diverse environments for agent exploration and learning, 2) a trajectory set to equip agents with basic capabilities and prior knowledge, and 3) an effective and scalable evolution method.","We propose AgentGym, a new framework featuring a variety of environments and tasks for broad, real-time, uni-format, and concurrent agent exploration.","AgentGym also includes a database with expanded instructions, a benchmark suite, and high-quality trajectories across environments.","Next, we propose a novel method, AgentEvol, to investigate the potential of agent self-evolution beyond previously seen data across tasks and environments.","Experimental results show that the evolved agents can achieve results comparable to SOTA models.","We release the AgentGym suite, including the platform, dataset, benchmark, checkpoints, and algorithm implementations.","The AgentGym suite is available on https://github.com/WooooDyy/AgentGym."],"url":"http://arxiv.org/abs/2406.04151v1"}
{"created":"2024-06-06 15:13:48","title":"Fast Redescription Mining Using Locality-Sensitive Hashing","abstract":"Redescription mining is a data analysis technique that has found applications in diverse fields. The most used redescription mining approaches involve two phases: finding matching pairs among data attributes and extending the pairs. This process is relatively efficient when the number of attributes remains limited and when the attributes are Boolean, but becomes almost intractable when the data consist of many numerical attributes. In this paper, we present new algorithms that perform the matching and extension orders of magnitude faster than the existing approaches. Our algorithms are based on locality-sensitive hashing with a tailored approach to handle the discretisation of numerical attributes as used in redescription mining.","sentences":["Redescription mining is a data analysis technique that has found applications in diverse fields.","The most used redescription mining approaches involve two phases: finding matching pairs among data attributes and extending the pairs.","This process is relatively efficient when the number of attributes remains limited and when the attributes are Boolean, but becomes almost intractable when the data consist of many numerical attributes.","In this paper, we present new algorithms that perform the matching and extension orders of magnitude faster than the existing approaches.","Our algorithms are based on locality-sensitive hashing with a tailored approach to handle the discretisation of numerical attributes as used in redescription mining."],"url":"http://arxiv.org/abs/2406.04148v1"}
{"created":"2024-06-06 15:11:11","title":"Towards Understanding Task-agnostic Debiasing Through the Lenses of Intrinsic Bias and Forgetfulness","abstract":"While task-agnostic debiasing provides notable generalizability and reduced reliance on downstream data, its impact on language modeling ability and the risk of relearning social biases from downstream task-specific data remain as the two most significant challenges when debiasing Pretrained Language Models (PLMs). The impact on language modeling ability can be alleviated given a high-quality and long-contextualized debiasing corpus, but there remains a deficiency in understanding the specifics of relearning biases. We empirically ascertain that the effectiveness of task-agnostic debiasing hinges on the quantitative bias level of both the task-specific data used for downstream applications and the debiased model. We empirically show that the lower bound of the bias level of the downstream fine-tuned model can be approximated by the bias level of the debiased model, in most practical cases. To gain more in-depth understanding about how the parameters of PLMs change during fine-tuning due to the forgetting issue of PLMs, we propose a novel framework which can Propagate Socially-fair Debiasing to Downstream Fine-tuning, ProSocialTuning. Our proposed framework can push the fine-tuned model to approach the bias lower bound during downstream fine-tuning, indicating that the ineffectiveness of debiasing can be alleviated by overcoming the forgetting issue through regularizing successfully debiased attention heads based on the PLMs' bias levels from stages of pretraining and debiasing.","sentences":["While task-agnostic debiasing provides notable generalizability and reduced reliance on downstream data, its impact on language modeling ability and the risk of relearning social biases from downstream task-specific data remain as the two most significant challenges when debiasing Pretrained Language Models (PLMs).","The impact on language modeling ability can be alleviated given a high-quality and long-contextualized debiasing corpus, but there remains a deficiency in understanding the specifics of relearning biases.","We empirically ascertain that the effectiveness of task-agnostic debiasing hinges on the quantitative bias level of both the task-specific data used for downstream applications and the debiased model.","We empirically show that the lower bound of the bias level of the downstream fine-tuned model can be approximated by the bias level of the debiased model, in most practical cases.","To gain more in-depth understanding about how the parameters of PLMs change during fine-tuning due to the forgetting issue of PLMs, we propose a novel framework which can Propagate Socially-fair Debiasing to Downstream Fine-tuning, ProSocialTuning.","Our proposed framework can push the fine-tuned model to approach the bias lower bound during downstream fine-tuning, indicating that the ineffectiveness of debiasing can be alleviated by overcoming the forgetting issue through regularizing successfully debiased attention heads based on the PLMs' bias levels from stages of pretraining and debiasing."],"url":"http://arxiv.org/abs/2406.04146v1"}
{"created":"2024-06-06 15:08:41","title":"Redundancy-aware Action Spaces for Robot Learning","abstract":"Joint space and task space control are the two dominant action modes for controlling robot arms within the robot learning literature. Actions in joint space provide precise control over the robot's pose, but tend to suffer from inefficient training; actions in task space boast data-efficient training but sacrifice the ability to perform tasks in confined spaces due to limited control over the full joint configuration. This work analyses the criteria for designing action spaces for robot manipulation and introduces ER (End-effector Redundancy), a novel action space formulation that, by addressing the redundancies present in the manipulator, aims to combine the advantages of both joint and task spaces, offering fine-grained comprehensive control with overactuated robot arms whilst achieving highly efficient robot learning. We present two implementations of ER, ERAngle (ERA) and ERJoint (ERJ), and we show that ERJ in particular demonstrates superior performance across multiple settings, especially when precise control over the robot configuration is required. We validate our results both in simulated and real robotic environments.","sentences":["Joint space and task space control are the two dominant action modes for controlling robot arms within the robot learning literature.","Actions in joint space provide precise control over the robot's pose, but tend to suffer from inefficient training; actions in task space boast data-efficient training but sacrifice the ability to perform tasks in confined spaces due to limited control over the full joint configuration.","This work analyses the criteria for designing action spaces for robot manipulation and introduces ER (End-effector Redundancy), a novel action space formulation that, by addressing the redundancies present in the manipulator, aims to combine the advantages of both joint and task spaces, offering fine-grained comprehensive control with overactuated robot arms whilst achieving highly efficient robot learning.","We present two implementations of ER, ERAngle (ERA) and ERJoint (ERJ), and we show that ERJ in particular demonstrates superior performance across multiple settings, especially when precise control over the robot configuration is required.","We validate our results both in simulated and real robotic environments."],"url":"http://arxiv.org/abs/2406.04144v1"}
{"created":"2024-06-06 15:08:16","title":"Do Language Models Understand Morality? Towards a Robust Detection of Moral Content","abstract":"The task of detecting moral values in text has significant implications in various fields, including natural language processing, social sciences, and ethical decision-making. Previously proposed supervised models often suffer from overfitting, leading to hyper-specialized moral classifiers that struggle to perform well on data from different domains. To address this issue, we introduce novel systems that leverage abstract concepts and common-sense knowledge acquired from Large Language Models and Natural Language Inference models during previous stages of training on multiple data sources. By doing so, we aim to develop versatile and robust methods for detecting moral values in real-world scenarios. Our approach uses the GPT 3.5 model as a zero-shot ready-made unsupervised multi-label classifier for moral values detection, eliminating the need for explicit training on labeled data. We compare it with a smaller NLI-based zero-shot model. The results show that the NLI approach achieves competitive results compared to the Davinci model. Furthermore, we conduct an in-depth investigation of the performance of supervised systems in the context of cross-domain multi-label moral value detection. This involves training supervised models on different domains to explore their effectiveness in handling data from different sources and comparing their performance with the unsupervised methods. Our contributions encompass a thorough analysis of both supervised and unsupervised methodologies for cross-domain value detection. We introduce the Davinci model as a state-of-the-art zero-shot unsupervised moral values classifier, pushing the boundaries of moral value detection without the need for explicit training on labeled data. Additionally, we perform a comparative evaluation of our approach with the supervised models, shedding light on their respective strengths and weaknesses.","sentences":["The task of detecting moral values in text has significant implications in various fields, including natural language processing, social sciences, and ethical decision-making.","Previously proposed supervised models often suffer from overfitting, leading to hyper-specialized moral classifiers that struggle to perform well on data from different domains.","To address this issue, we introduce novel systems that leverage abstract concepts and common-sense knowledge acquired from Large Language Models and Natural Language Inference models during previous stages of training on multiple data sources.","By doing so, we aim to develop versatile and robust methods for detecting moral values in real-world scenarios.","Our approach uses the GPT 3.5 model as a zero-shot ready-made unsupervised multi-label classifier for moral values detection, eliminating the need for explicit training on labeled data.","We compare it with a smaller NLI-based zero-shot model.","The results show that the NLI approach achieves competitive results compared to the Davinci model.","Furthermore, we conduct an in-depth investigation of the performance of supervised systems in the context of cross-domain multi-label moral value detection.","This involves training supervised models on different domains to explore their effectiveness in handling data from different sources and comparing their performance with the unsupervised methods.","Our contributions encompass a thorough analysis of both supervised and unsupervised methodologies for cross-domain value detection.","We introduce the Davinci model as a state-of-the-art zero-shot unsupervised moral values classifier, pushing the boundaries of moral value detection without the need for explicit training on labeled data.","Additionally, we perform a comparative evaluation of our approach with the supervised models, shedding light on their respective strengths and weaknesses."],"url":"http://arxiv.org/abs/2406.04143v1"}
{"created":"2024-06-06 15:04:36","title":"Coding Over Coupon Collector Channels for Combinatorial Motif-Based DNA Storage","abstract":"Encoding information in combinations of pre-synthesised deoxyribonucleic acid (DNA) strands (referred to as motifs) is an interesting approach to DNA storage that could potentially circumvent the prohibitive costs of nucleotide-by-nucleotide DNA synthesis. Based on our analysis of an empirical data set from HelixWorks, we propose two channel models for this setup (with and without interference) and analyse their fundamental limits. We propose a coding scheme that approaches those limits by leveraging all information available at the output of the channel, in contrast to earlier schemes developed for a similar setup by Preuss et al. We highlight an important connection between channel capacity curves and the fundamental trade-off between synthesis (writing) and sequencing (reading), and offer a way to mitigate an exponential growth in decoding complexity with the size of the motif library.","sentences":["Encoding information in combinations of pre-synthesised deoxyribonucleic acid (DNA) strands (referred to as motifs) is an interesting approach to DNA storage that could potentially circumvent the prohibitive costs of nucleotide-by-nucleotide DNA synthesis.","Based on our analysis of an empirical data set from HelixWorks, we propose two channel models for this setup (with and without interference) and analyse their fundamental limits.","We propose a coding scheme that approaches those limits by leveraging all information available at the output of the channel, in contrast to earlier schemes developed for a similar setup by Preuss et al.","We highlight an important connection between channel capacity curves and the fundamental trade-off between synthesis (writing) and sequencing (reading), and offer a way to mitigate an exponential growth in decoding complexity with the size of the motif library."],"url":"http://arxiv.org/abs/2406.04141v1"}
{"created":"2024-06-06 14:36:07","title":"Promoting Fairness and Diversity in Speech Datasets for Mental Health and Neurological Disorders Research","abstract":"Current research in machine learning and artificial intelligence is largely centered on modeling and performance evaluation, less so on data collection. However, recent research demonstrated that limitations and biases in data may negatively impact trustworthiness and reliability. These aspects are particularly impactful on sensitive domains such as mental health and neurological disorders, where speech data are used to develop AI applications aimed at improving the health of patients and supporting healthcare providers. In this paper, we chart the landscape of available speech datasets for this domain, to highlight possible pitfalls and opportunities for improvement and promote fairness and diversity. We present a comprehensive list of desiderata for building speech datasets for mental health and neurological disorders and distill it into a checklist focused on ethical concerns to foster more responsible research.","sentences":["Current research in machine learning and artificial intelligence is largely centered on modeling and performance evaluation, less so on data collection.","However, recent research demonstrated that limitations and biases in data may negatively impact trustworthiness and reliability.","These aspects are particularly impactful on sensitive domains such as mental health and neurological disorders, where speech data are used to develop AI applications aimed at improving the health of patients and supporting healthcare providers.","In this paper, we chart the landscape of available speech datasets for this domain, to highlight possible pitfalls and opportunities for improvement and promote fairness and diversity.","We present a comprehensive list of desiderata for building speech datasets for mental health and neurological disorders and distill it into a checklist focused on ethical concerns to foster more responsible research."],"url":"http://arxiv.org/abs/2406.04116v1"}
{"created":"2024-06-06 14:36:02","title":"Global Parameterization-based Texture Space Optimization","abstract":"Texture mapping is a common technology in the area of computer graphics, it maps the 3D surface space onto the 2D texture space. However, the loose texture space will reduce the efficiency of data storage and GPU memory addressing in the rendering process. Many of the existing methods focus on repacking given textures, but they still suffer from high computational cost and hardly produce a wholly tight texture space. In this paper, we propose a method to optimize the texture space and produce a new texture mapping which is compact based on global parameterization. The proposed method is computationally robust and efficient. Experiments show the effectiveness of the proposed method and the potency in improving the storage and rendering efficiency.","sentences":["Texture mapping is a common technology in the area of computer graphics, it maps the 3D surface space onto the 2D texture space.","However, the loose texture space will reduce the efficiency of data storage and GPU memory addressing in the rendering process.","Many of the existing methods focus on repacking given textures, but they still suffer from high computational cost and hardly produce a wholly tight texture space.","In this paper, we propose a method to optimize the texture space and produce a new texture mapping which is compact based on global parameterization.","The proposed method is computationally robust and efficient.","Experiments show the effectiveness of the proposed method and the potency in improving the storage and rendering efficiency."],"url":"http://arxiv.org/abs/2406.04115v1"}
{"created":"2024-06-06 14:30:59","title":"Uncovering Limitations of Large Language Models in Information Seeking from Tables","abstract":"Tables are recognized for their high information density and widespread usage, serving as essential sources of information. Seeking information from tables (TIS) is a crucial capability for Large Language Models (LLMs), serving as the foundation of knowledge-based Q&A systems. However, this field presently suffers from an absence of thorough and reliable evaluation. This paper introduces a more reliable benchmark for Table Information Seeking (TabIS). To avoid the unreliable evaluation caused by text similarity-based metrics, TabIS adopts a single-choice question format (with two options per question) instead of a text generation format. We establish an effective pipeline for generating options, ensuring their difficulty and quality. Experiments conducted on 12 LLMs reveal that while the performance of GPT-4-turbo is marginally satisfactory, both other proprietary and open-source models perform inadequately. Further analysis shows that LLMs exhibit a poor understanding of table structures, and struggle to balance between TIS performance and robustness against pseudo-relevant tables (common in retrieval-augmented systems). These findings uncover the limitations and potential challenges of LLMs in seeking information from tables. We release our data and code to facilitate further research in this field.","sentences":["Tables are recognized for their high information density and widespread usage, serving as essential sources of information.","Seeking information from tables (TIS) is a crucial capability for Large Language Models (LLMs), serving as the foundation of knowledge-based Q&A systems.","However, this field presently suffers from an absence of thorough and reliable evaluation.","This paper introduces a more reliable benchmark for Table Information Seeking (TabIS).","To avoid the unreliable evaluation caused by text similarity-based metrics, TabIS adopts a single-choice question format (with two options per question) instead of a text generation format.","We establish an effective pipeline for generating options, ensuring their difficulty and quality.","Experiments conducted on 12 LLMs reveal that while the performance of GPT-4-turbo is marginally satisfactory, both other proprietary and open-source models perform inadequately.","Further analysis shows that LLMs exhibit a poor understanding of table structures, and struggle to balance between TIS performance and robustness against pseudo-relevant tables (common in retrieval-augmented systems).","These findings uncover the limitations and potential challenges of LLMs in seeking information from tables.","We release our data and code to facilitate further research in this field."],"url":"http://arxiv.org/abs/2406.04113v1"}
{"created":"2024-06-06 14:29:49","title":"Compressible Dynamics in Deep Overparameterized Low-Rank Learning & Adaptation","abstract":"While overparameterization in machine learning models offers great benefits in terms of optimization and generalization, it also leads to increased computational requirements as model sizes grow. In this work, we show that by leveraging the inherent low-dimensional structures of data and compressible dynamics within the model parameters, we can reap the benefits of overparameterization without the computational burdens. In practice, we demonstrate the effectiveness of this approach for deep low-rank matrix completion as well as fine-tuning language models. Our approach is grounded in theoretical findings for deep overparameterized low-rank matrix recovery, where we show that the learning dynamics of each weight matrix are confined to an invariant low-dimensional subspace. Consequently, we can construct and train compact, highly compressed factorizations possessing the same benefits as their overparameterized counterparts. In the context of deep matrix completion, our technique substantially improves training efficiency while retaining the advantages of overparameterization. For language model fine-tuning, we propose a method called \"Deep LoRA\", which improves the existing low-rank adaptation (LoRA) technique, leading to reduced overfitting and a simplified hyperparameter setup, while maintaining comparable efficiency. We validate the effectiveness of Deep LoRA on natural language tasks, particularly when fine-tuning with limited data.","sentences":["While overparameterization in machine learning models offers great benefits in terms of optimization and generalization, it also leads to increased computational requirements as model sizes grow.","In this work, we show that by leveraging the inherent low-dimensional structures of data and compressible dynamics within the model parameters, we can reap the benefits of overparameterization without the computational burdens.","In practice, we demonstrate the effectiveness of this approach for deep low-rank matrix completion as well as fine-tuning language models.","Our approach is grounded in theoretical findings for deep overparameterized low-rank matrix recovery, where we show that the learning dynamics of each weight matrix are confined to an invariant low-dimensional subspace.","Consequently, we can construct and train compact, highly compressed factorizations possessing the same benefits as their overparameterized counterparts.","In the context of deep matrix completion, our technique substantially improves training efficiency while retaining the advantages of overparameterization.","For language model fine-tuning, we propose a method called \"Deep LoRA\", which improves the existing low-rank adaptation (LoRA) technique, leading to reduced overfitting and a simplified hyperparameter setup, while maintaining comparable efficiency.","We validate the effectiveness of Deep LoRA on natural language tasks, particularly when fine-tuning with limited data."],"url":"http://arxiv.org/abs/2406.04112v1"}
{"created":"2024-06-06 14:28:43","title":"UrbanSARFloods: Sentinel-1 SLC-Based Benchmark Dataset for Urban and Open-Area Flood Mapping","abstract":"Due to its cloud-penetrating capability and independence from solar illumination, satellite Synthetic Aperture Radar (SAR) is the preferred data source for large-scale flood mapping, providing global coverage and including various land cover classes. However, most studies on large-scale SAR-derived flood mapping using deep learning algorithms have primarily focused on flooded open areas, utilizing available open-access datasets (e.g., Sen1Floods11) and with limited attention to urban floods. To address this gap, we introduce \\textbf{UrbanSARFloods}, a floodwater dataset featuring pre-processed Sentinel-1 intensity data and interferometric coherence imagery acquired before and during flood events. It contains 8,879 $512\\times 512$ chips covering 807,500 $km^2$ across 20 land cover classes and 5 continents, spanning 18 flood events. We used UrbanSARFloods to benchmark existing state-of-the-art convolutional neural networks (CNNs) for segmenting open and urban flood areas. Our findings indicate that prevalent approaches, including the Weighted Cross-Entropy (WCE) loss and the application of transfer learning with pretrained models, fall short in overcoming the obstacles posed by imbalanced data and the constraints of a small training dataset. Urban flood detection remains challenging. Future research should explore strategies for addressing imbalanced data challenges and investigate transfer learning's potential for SAR-based large-scale flood mapping. Besides, expanding this dataset to include additional flood events holds promise for enhancing its utility and contributing to advancements in flood mapping techniques.","sentences":["Due to its cloud-penetrating capability and independence from solar illumination, satellite Synthetic Aperture Radar (SAR) is the preferred data source for large-scale flood mapping, providing global coverage and including various land cover classes.","However, most studies on large-scale SAR-derived flood mapping using deep learning algorithms have primarily focused on flooded open areas, utilizing available open-access datasets (e.g., Sen1Floods11) and with limited attention to urban floods.","To address this gap, we introduce \\textbf{UrbanSARFloods}, a floodwater dataset featuring pre-processed Sentinel-1 intensity data and interferometric coherence imagery acquired before and during flood events.","It contains 8,879 $512\\times 512$ chips covering 807,500 $km^2$ across 20 land cover classes and 5 continents, spanning 18 flood events.","We used UrbanSARFloods to benchmark existing state-of-the-art convolutional neural networks (CNNs) for segmenting open and urban flood areas.","Our findings indicate that prevalent approaches, including the Weighted Cross-Entropy (WCE) loss and the application of transfer learning with pretrained models, fall short in overcoming the obstacles posed by imbalanced data and the constraints of a small training dataset.","Urban flood detection remains challenging.","Future research should explore strategies for addressing imbalanced data challenges and investigate transfer learning's potential for SAR-based large-scale flood mapping.","Besides, expanding this dataset to include additional flood events holds promise for enhancing its utility and contributing to advancements in flood mapping techniques."],"url":"http://arxiv.org/abs/2406.04111v1"}
{"created":"2024-06-06 14:20:21","title":"Multistep Distillation of Diffusion Models via Moment Matching","abstract":"We present a new method for making diffusion models faster to sample. The method distills many-step diffusion models into few-step models by matching conditional expectations of the clean data given noisy data along the sampling trajectory. Our approach extends recently proposed one-step methods to the multi-step case, and provides a new perspective by interpreting these approaches in terms of moment matching. By using up to 8 sampling steps, we obtain distilled models that outperform not only their one-step versions but also their original many-step teacher models, obtaining new state-of-the-art results on the Imagenet dataset. We also show promising results on a large text-to-image model where we achieve fast generation of high resolution images directly in image space, without needing autoencoders or upsamplers.","sentences":["We present a new method for making diffusion models faster to sample.","The method distills many-step diffusion models into few-step models by matching conditional expectations of the clean data given noisy data along the sampling trajectory.","Our approach extends recently proposed one-step methods to the multi-step case, and provides a new perspective by interpreting these approaches in terms of moment matching.","By using up to 8 sampling steps, we obtain distilled models that outperform not only their one-step versions but also their original many-step teacher models, obtaining new state-of-the-art results on the Imagenet dataset.","We also show promising results on a large text-to-image model where we achieve fast generation of high resolution images directly in image space, without needing autoencoders or upsamplers."],"url":"http://arxiv.org/abs/2406.04103v1"}
{"created":"2024-06-06 14:17:50","title":"Chromatic Topological Data Analysis","abstract":"Exploring the shape of point configurations has been a key driver in the evolution of TDA (short for topological data analysis) since its infancy. This survey illustrates the recent efforts to broaden these ideas to model spatial interactions among multiple configurations, each distinguished by a color. It describes advances in this area and prepares the ground for further exploration by mentioning unresolved questions and promising research avenues while focusing on the overlap with discrete geometry.","sentences":["Exploring the shape of point configurations has been a key driver in the evolution of TDA (short for topological data analysis) since its infancy.","This survey illustrates the recent efforts to broaden these ideas to model spatial interactions among multiple configurations, each distinguished by a color.","It describes advances in this area and prepares the ground for further exploration by mentioning unresolved questions and promising research avenues while focusing on the overlap with discrete geometry."],"url":"http://arxiv.org/abs/2406.04102v1"}
{"created":"2024-06-06 14:15:15","title":"Class-Aware Cartilage Segmentation for Autonomous US-CT Registration in Robotic Intercostal Ultrasound Imaging","abstract":"Ultrasound imaging has been widely used in clinical examinations owing to the advantages of being portable, real-time, and radiation-free. Considering the potential of extensive deployment of autonomous examination systems in hospitals, robotic US imaging has attracted increased attention. However, due to the inter-patient variations, it is still challenging to have an optimal path for each patient, particularly for thoracic applications with limited acoustic windows, e.g., intercostal liver imaging. To address this problem, a class-aware cartilage bone segmentation network with geometry-constraint post-processing is presented to capture patient-specific rib skeletons. Then, a dense skeleton graph-based non-rigid registration is presented to map the intercostal scanning path from a generic template to individual patients. By explicitly considering the high-acoustic impedance bone structures, the transferred scanning path can be precisely located in the intercostal space, enhancing the visibility of internal organs by reducing the acoustic shadow. To evaluate the proposed approach, the final path mapping performance is validated on five distinct CTs and two volunteer US data, resulting in ten pairs of CT-US combinations. Results demonstrate that the proposed graph-based registration method can robustly and precisely map the path from CT template to individual patients (Euclidean error: $2.21\\pm1.11~mm$).","sentences":["Ultrasound imaging has been widely used in clinical examinations owing to the advantages of being portable, real-time, and radiation-free.","Considering the potential of extensive deployment of autonomous examination systems in hospitals, robotic US imaging has attracted increased attention.","However, due to the inter-patient variations, it is still challenging to have an optimal path for each patient, particularly for thoracic applications with limited acoustic windows, e.g., intercostal liver imaging.","To address this problem, a class-aware cartilage bone segmentation network with geometry-constraint post-processing is presented to capture patient-specific rib skeletons.","Then, a dense skeleton graph-based non-rigid registration is presented to map the intercostal scanning path from a generic template to individual patients.","By explicitly considering the high-acoustic impedance bone structures, the transferred scanning path can be precisely located in the intercostal space, enhancing the visibility of internal organs by reducing the acoustic shadow.","To evaluate the proposed approach, the final path mapping performance is validated on five distinct CTs and two volunteer US data, resulting in ten pairs of CT-US combinations.","Results demonstrate that the proposed graph-based registration method can robustly and precisely map the path from CT template to individual patients (Euclidean error: $2.21\\pm1.11~mm$)."],"url":"http://arxiv.org/abs/2406.04100v1"}
{"created":"2024-06-06 14:15:12","title":"Enhancing Weather Predictions: Super-Resolution via Deep Diffusion Models","abstract":"This study investigates the application of deep-learning diffusion models for the super-resolution of weather data, a novel approach aimed at enhancing the spatial resolution and detail of meteorological variables. Leveraging the capabilities of diffusion models, specifically the SR3 and ResDiff architectures, we present a methodology for transforming low-resolution weather data into high-resolution outputs. Our experiments, conducted using the WeatherBench dataset, focus on the super-resolution of the two-meter temperature variable, demonstrating the models' ability to generate detailed and accurate weather maps. The results indicate that the ResDiff model, further improved by incorporating physics-based modifications, significantly outperforms traditional SR3 methods in terms of Mean Squared Error (MSE), Structural Similarity Index (SSIM), and Peak Signal-to-Noise Ratio (PSNR). This research highlights the potential of diffusion models in meteorological applications, offering insights into their effectiveness, challenges, and prospects for future advancements in weather prediction and climate analysis.","sentences":["This study investigates the application of deep-learning diffusion models for the super-resolution of weather data, a novel approach aimed at enhancing the spatial resolution and detail of meteorological variables.","Leveraging the capabilities of diffusion models, specifically the SR3 and ResDiff architectures, we present a methodology for transforming low-resolution weather data into high-resolution outputs.","Our experiments, conducted using the WeatherBench dataset, focus on the super-resolution of the two-meter temperature variable, demonstrating the models' ability to generate detailed and accurate weather maps.","The results indicate that the ResDiff model, further improved by incorporating physics-based modifications, significantly outperforms traditional SR3 methods in terms of Mean Squared Error (MSE), Structural Similarity Index (SSIM), and Peak Signal-to-Noise Ratio (PSNR).","This research highlights the potential of diffusion models in meteorological applications, offering insights into their effectiveness, challenges, and prospects for future advancements in weather prediction and climate analysis."],"url":"http://arxiv.org/abs/2406.04099v1"}
{"created":"2024-06-06 14:11:09","title":"Data-driven Explainable Controller for Soft Robots based on Recurrent Neural Networks","abstract":"The nonlinearity and hysteresis of soft robot motions have posed challenges in accurate soft robot control. Neural networks, especially recurrent neural networks (RNNs), have been widely leveraged for this issue due to their nonlinear activation functions and recurrent structures. Although they have shown satisfying accuracy in most tasks, these black-box approaches are not explainable, and hence, they are unsuitable for areas with high safety requirements, like robot-assisted surgery. Based on the RNN controllers, we propose a data-driven explainable controller (DDEC) whose parameters can be updated online. We discuss the Jacobian controller and kinematics controller in theory and demonstrate that they are only special cases of DDEC. Moreover, we utilize RNN, the Jacobian controller, the kinematics controller, and DDECs for trajectory following tasks. Experimental results have shown that our approach outperforms the other controllers considering trajectory following errors while being explainable. We also conduct a study to explore and explain the functions of each DDEC component. This is the first interpretable soft robot controller that overcomes the shortcomings of both NN controllers and interpretable controllers. Future work may involve proposing different DDECs based on different RNN controllers and exploiting them for high-safety-required applications.","sentences":["The nonlinearity and hysteresis of soft robot motions have posed challenges in accurate soft robot control.","Neural networks, especially recurrent neural networks (RNNs), have been widely leveraged for this issue due to their nonlinear activation functions and recurrent structures.","Although they have shown satisfying accuracy in most tasks, these black-box approaches are not explainable, and hence, they are unsuitable for areas with high safety requirements, like robot-assisted surgery.","Based on the RNN controllers, we propose a data-driven explainable controller (DDEC) whose parameters can be updated online.","We discuss the Jacobian controller and kinematics controller in theory and demonstrate that they are only special cases of DDEC.","Moreover, we utilize RNN, the Jacobian controller, the kinematics controller, and DDECs for trajectory following tasks.","Experimental results have shown that our approach outperforms the other controllers considering trajectory following errors while being explainable.","We also conduct a study to explore and explain the functions of each DDEC component.","This is the first interpretable soft robot controller that overcomes the shortcomings of both NN controllers and interpretable controllers.","Future work may involve proposing different DDECs based on different RNN controllers and exploiting them for high-safety-required applications."],"url":"http://arxiv.org/abs/2406.04094v1"}
{"created":"2024-06-06 13:57:05","title":"A Survey of Language-Based Communication in Robotics","abstract":"Embodied robots which can interact with their environment and neighbours are increasingly being used as a test case to develop Artificial Intelligence. This creates a need for multimodal robot controllers which can operate across different types of information including text. Large Language Models are able to process and generate textual as well as audiovisual data and, more recently, robot actions. Language Models are increasingly being applied to robotic systems; these Language-Based robots leverage the power of language models in a variety of ways. Additionally, the use of language opens up multiple forms of information exchange between members of a human-robot team. This survey motivates the use of language models in robotics, and then delineates works based on the part of the overall control flow in which language is incorporated. Language can be used by human to task a robot, by a robot to inform a human, between robots as a human-like communication medium, and internally for a robot's planning and control. Applications of language-based robots are explored, and finally numerous limitations and challenges are discussed to provide a summary of the development needed for language-based robotics moving forward. Links to each paper and, if available, source code are made available in the accompanying site at https://uos-haris.online/sooratilab/papers/WillSurvey/LangRobotSurvey.php","sentences":["Embodied robots which can interact with their environment and neighbours are increasingly being used as a test case to develop Artificial Intelligence.","This creates a need for multimodal robot controllers which can operate across different types of information including text.","Large Language Models are able to process and generate textual as well as audiovisual data and, more recently, robot actions.","Language Models are increasingly being applied to robotic systems; these Language-Based robots leverage the power of language models in a variety of ways.","Additionally, the use of language opens up multiple forms of information exchange between members of a human-robot team.","This survey motivates the use of language models in robotics, and then delineates works based on the part of the overall control flow in which language is incorporated.","Language can be used by human to task a robot, by a robot to inform a human, between robots as a human-like communication medium, and internally for a robot's planning and control.","Applications of language-based robots are explored, and finally numerous limitations and challenges are discussed to provide a summary of the development needed for language-based robotics moving forward.","Links to each paper and, if available, source code are made available in the accompanying site at https://uos-haris.online/sooratilab/papers/WillSurvey/LangRobotSurvey.php"],"url":"http://arxiv.org/abs/2406.04086v1"}
{"created":"2024-06-06 13:44:44","title":"Federated TrustChain: Blockchain-Enhanced LLM Training and Unlearning","abstract":"The development of Large Language Models (LLMs) faces a significant challenge: the exhausting of publicly available fresh data. This is because training a LLM needs a large demanding of new data. Federated learning emerges as a promising solution, enabling collaborative model to contribute their private data to LLM global model. However, integrating federated learning with LLMs introduces new challenges, including the lack of transparency and the need for effective unlearning mechanisms. Transparency is essential to ensuring trust and fairness among participants, while accountability is crucial for deterring malicious behaviour and enabling corrective actions when necessary. To address these challenges, we propose a novel blockchain-based federated learning framework for LLMs that enhances transparency, accountability, and unlearning capabilities. Our framework leverages blockchain technology to create a tamper-proof record of each model's contributions and introduces an innovative unlearning function that seamlessly integrates with the federated learning mechanism. We investigate the impact of Low-Rank Adaptation (LoRA) hyperparameters on unlearning performance and integrate Hyperledger Fabric to ensure the security, transparency, and verifiability of the unlearning process. Through comprehensive experiments and analysis, we showcase the effectiveness of our proposed framework in achieving highly effective unlearning in LLMs trained using federated learning. Our findings highlight the feasibility of integrating blockchain technology into federated learning frameworks for LLMs.","sentences":["The development of Large Language Models (LLMs) faces a significant challenge: the exhausting of publicly available fresh data.","This is because training a LLM needs a large demanding of new data.","Federated learning emerges as a promising solution, enabling collaborative model to contribute their private data to LLM global model.","However, integrating federated learning with LLMs introduces new challenges, including the lack of transparency and the need for effective unlearning mechanisms.","Transparency is essential to ensuring trust and fairness among participants, while accountability is crucial for deterring malicious behaviour and enabling corrective actions when necessary.","To address these challenges, we propose a novel blockchain-based federated learning framework for LLMs that enhances transparency, accountability, and unlearning capabilities.","Our framework leverages blockchain technology to create a tamper-proof record of each model's contributions and introduces an innovative unlearning function that seamlessly integrates with the federated learning mechanism.","We investigate the impact of Low-Rank Adaptation (LoRA) hyperparameters on unlearning performance and integrate Hyperledger Fabric to ensure the security, transparency, and verifiability of the unlearning process.","Through comprehensive experiments and analysis, we showcase the effectiveness of our proposed framework in achieving highly effective unlearning in LLMs trained using federated learning.","Our findings highlight the feasibility of integrating blockchain technology into federated learning frameworks for LLMs."],"url":"http://arxiv.org/abs/2406.04076v1"}
{"created":"2024-06-06 13:25:42","title":"Watching Popular Musicians Learn by Ear: A Hypothesis-Generating Study of Human-Recording Interactions in YouTube Videos","abstract":"Popular musicians often learn music by ear. It is unclear what role technology plays for those with experience at this task. In search of opportunities for the development of novel human-recording interactions, we analyze 18 YouTube videos depicting real-world examples of by-ear learning, and discuss why, during this preliminary phase of research, online videos are appropriate data. From our observations we generate hypotheses that can inform future work. For example, a musician's scope of learning may influence what technological interactions would help them, they could benefit from tools that accommodate their working memory, and transcription does not appear to play a key role in ear learning. Based on these findings, we pose a number of research questions, and discuss their methodological considerations to guide future study.","sentences":["Popular musicians often learn music by ear.","It is unclear what role technology plays for those with experience at this task.","In search of opportunities for the development of novel human-recording interactions, we analyze 18 YouTube videos depicting real-world examples of by-ear learning, and discuss why, during this preliminary phase of research, online videos are appropriate data.","From our observations we generate hypotheses that can inform future work.","For example, a musician's scope of learning may influence what technological interactions would help them, they could benefit from tools that accommodate their working memory, and transcription does not appear to play a key role in ear learning.","Based on these findings, we pose a number of research questions, and discuss their methodological considerations to guide future study."],"url":"http://arxiv.org/abs/2406.04058v1"}
{"created":"2024-06-06 13:21:28","title":"Leveraging SPD Matrices on Riemannian Manifolds in Quantum Classical Hybrid Models for Structural Health Monitoring","abstract":"Realtime finite element modeling of bridges assists modern structural health monitoring systems by providing comprehensive insights into structural integrity. This capability is essential for ensuring the safe operation of bridges and preventing sudden catastrophic failures. However, FEM computational cost and the need for realtime analysis pose significant challenges. Additionally, the input data is a 7 dimensional vector, while the output is a 1017 dimensional vector, making accurate and efficient analysis particularly difficult. In this study, we propose a novel hybrid quantum classical Multilayer Perceptron pipeline leveraging Symmetric Positive Definite matrices and Riemannian manifolds for effective data representation. To maintain the integrity of the qubit structure, we utilize SPD matrices, ensuring data representation is well aligned with the quantum computational framework. Additionally, the method leverages polynomial feature expansion to capture nonlinear relationships within the data. The proposed pipeline combines classical fully connected neural network layers with quantum circuit layers to enhance model performance and efficiency. Our experiments focused on various configurations of such hybrid models to identify the optimal structure for accurate and efficient realtime analysis. The best performing model achieved a Mean Squared Error of 0.00031, significantly outperforming traditional methods.","sentences":["Realtime finite element modeling of bridges assists modern structural health monitoring systems by providing comprehensive insights into structural integrity.","This capability is essential for ensuring the safe operation of bridges and preventing sudden catastrophic failures.","However, FEM computational cost and the need for realtime analysis pose significant challenges.","Additionally, the input data is a 7 dimensional vector, while the output is a 1017 dimensional vector, making accurate and efficient analysis particularly difficult.","In this study, we propose a novel hybrid quantum classical Multilayer Perceptron pipeline leveraging Symmetric Positive Definite matrices and Riemannian manifolds for effective data representation.","To maintain the integrity of the qubit structure, we utilize SPD matrices, ensuring data representation is well aligned with the quantum computational framework.","Additionally, the method leverages polynomial feature expansion to capture nonlinear relationships within the data.","The proposed pipeline combines classical fully connected neural network layers with quantum circuit layers to enhance model performance and efficiency.","Our experiments focused on various configurations of such hybrid models to identify the optimal structure for accurate and efficient realtime analysis.","The best performing model achieved a Mean Squared Error of 0.00031, significantly outperforming traditional methods."],"url":"http://arxiv.org/abs/2406.04055v1"}
{"created":"2024-06-06 13:17:24","title":"Semmeldetector: Application of Machine Learning in Commercial Bakeries","abstract":"The Semmeldetector, is a machine learning application that utilizes object detection models to detect, classify and count baked goods in images. Our application allows commercial bakers to track unsold baked goods, which allows them to optimize production and increase resource efficiency. We compiled a dataset comprising 1151 images that distinguishes between 18 different types of baked goods to train our detection models. To facilitate model training, we used a Copy-Paste augmentation pipeline to expand our dataset. We trained the state-of-the-art object detection model YOLOv8 on our detection task. We tested the impact of different training data, model scale, and online image augmentation pipelines on model performance. Our overall best performing model, achieved an AP@0.5 of 89.1% on our test set. Based on our results, we conclude that machine learning can be a valuable tool even for unforeseen industries like bakeries, even with very limited datasets.","sentences":["The Semmeldetector, is a machine learning application that utilizes object detection models to detect, classify and count baked goods in images.","Our application allows commercial bakers to track unsold baked goods, which allows them to optimize production and increase resource efficiency.","We compiled a dataset comprising 1151 images that distinguishes between 18 different types of baked goods to train our detection models.","To facilitate model training, we used a Copy-Paste augmentation pipeline to expand our dataset.","We trained the state-of-the-art object detection model YOLOv8 on our detection task.","We tested the impact of different training data, model scale, and online image augmentation pipelines on model performance.","Our overall best performing model, achieved an AP@0.5 of 89.1% on our test set.","Based on our results, we conclude that machine learning can be a valuable tool even for unforeseen industries like bakeries, even with very limited datasets."],"url":"http://arxiv.org/abs/2406.04050v1"}
{"created":"2024-06-06 13:13:29","title":"Energy-based Epistemic Uncertainty for Graph Neural Networks","abstract":"In domains with interdependent data, such as graphs, quantifying the epistemic uncertainty of a Graph Neural Network (GNN) is challenging as uncertainty can arise at different structural scales. Existing techniques neglect this issue or only distinguish between structure-aware and structure-agnostic uncertainty without combining them into a single measure. We propose GEBM, an energy-based model (EBM) that provides high-quality uncertainty estimates by aggregating energy at different structural levels that naturally arise from graph diffusion. In contrast to logit-based EBMs, we provably induce an integrable density in the data space by regularizing the energy function. We introduce an evidential interpretation of our EBM that significantly improves the predictive robustness of the GNN. Our framework is a simple and effective post hoc method applicable to any pre-trained GNN that is sensitive to various distribution shifts. It consistently achieves the best separation of in-distribution and out-of-distribution data on 6 out of 7 anomaly types while having the best average rank over shifts on \\emph{all} datasets.","sentences":["In domains with interdependent data, such as graphs, quantifying the epistemic uncertainty of a Graph Neural Network (GNN) is challenging as uncertainty can arise at different structural scales.","Existing techniques neglect this issue or only distinguish between structure-aware and structure-agnostic uncertainty without combining them into a single measure.","We propose GEBM, an energy-based model (EBM) that provides high-quality uncertainty estimates by aggregating energy at different structural levels that naturally arise from graph diffusion.","In contrast to logit-based EBMs, we provably induce an integrable density in the data space by regularizing the energy function.","We introduce an evidential interpretation of our EBM that significantly improves the predictive robustness of the GNN.","Our framework is a simple and effective post hoc method applicable to any pre-trained GNN that is sensitive to various distribution shifts.","It consistently achieves the best separation of in-distribution and out-of-distribution data on 6 out of 7 anomaly types while having the best average rank over shifts on \\emph{all} datasets."],"url":"http://arxiv.org/abs/2406.04043v1"}
{"created":"2024-06-06 13:10:37","title":"Linear Opinion Pooling for Uncertainty Quantification on Graphs","abstract":"We address the problem of uncertainty quantification for graph-structured data, or, more specifically, the problem to quantify the predictive uncertainty in (semi-supervised) node classification. Key questions in this regard concern the distinction between two different types of uncertainty, aleatoric and epistemic, and how to support uncertainty quantification by leveraging the structural information provided by the graph topology. Challenging assumptions and postulates of state-of-the-art methods, we propose a novel approach that represents (epistemic) uncertainty in terms of mixtures of Dirichlet distributions and refers to the established principle of linear opinion pooling for propagating information between neighbored nodes in the graph. The effectiveness of this approach is demonstrated in a series of experiments on a variety of graph-structured datasets.","sentences":["We address the problem of uncertainty quantification for graph-structured data, or, more specifically, the problem to quantify the predictive uncertainty in (semi-supervised) node classification.","Key questions in this regard concern the distinction between two different types of uncertainty, aleatoric and epistemic, and how to support uncertainty quantification by leveraging the structural information provided by the graph topology.","Challenging assumptions and postulates of state-of-the-art methods, we propose a novel approach that represents (epistemic) uncertainty in terms of mixtures of Dirichlet distributions and refers to the established principle of linear opinion pooling for propagating information between neighbored nodes in the graph.","The effectiveness of this approach is demonstrated in a series of experiments on a variety of graph-structured datasets."],"url":"http://arxiv.org/abs/2406.04041v1"}
{"created":"2024-06-06 13:05:32","title":"Shaping History: Advanced Machine Learning Techniques for the Analysis and Dating of Cuneiform Tablets over Three Millennia","abstract":"Cuneiform tablets, emerging in ancient Mesopotamia around the late fourth millennium BCE, represent one of humanity's earliest writing systems. Characterized by wedge-shaped marks on clay tablets, these artifacts provided insight into Mesopotamian civilization across various domains. Traditionally, the analysis and dating of these tablets rely on subjective assessment of shape and writing style, leading to uncertainties in pinpointing their exact temporal origins. Recent advances in digitization have revolutionized the study of cuneiform by enhancing accessibility and analytical capabilities. Our research uniquely focuses on the silhouette of tablets as significant indicators of their historical periods, diverging from most studies that concentrate on textual content. Utilizing an unprecedented dataset of over 94,000 images from the Cuneiform Digital Library Initiative collection, we apply deep learning methods to classify cuneiform tablets, covering over 3,000 years of history. By leveraging statistical, computational techniques, and generative modeling through Variational Auto-Encoders (VAEs), we achieve substantial advancements in the automatic classification of these ancient documents, focusing on the tablets' silhouettes as key predictors. Our classification approach begins with a Decision Tree using height-to-width ratios and culminates with a ResNet50 model, achieving a 61% macro F1-score for tablet silhouettes. Moreover, we introduce novel VAE-powered tools to enhance explainability and enable researchers to explore changes in tablet shapes across different eras and genres. This research contributes to document analysis and diplomatics by demonstrating the value of large-scale data analysis combined with statistical methods. These insights offer valuable tools for historians and epigraphists, enriching our understanding of cuneiform tablets and the cultures that produced them.","sentences":["Cuneiform tablets, emerging in ancient Mesopotamia around the late fourth millennium BCE, represent one of humanity's earliest writing systems.","Characterized by wedge-shaped marks on clay tablets, these artifacts provided insight into Mesopotamian civilization across various domains.","Traditionally, the analysis and dating of these tablets rely on subjective assessment of shape and writing style, leading to uncertainties in pinpointing their exact temporal origins.","Recent advances in digitization have revolutionized the study of cuneiform by enhancing accessibility and analytical capabilities.","Our research uniquely focuses on the silhouette of tablets as significant indicators of their historical periods, diverging from most studies that concentrate on textual content.","Utilizing an unprecedented dataset of over 94,000 images from the Cuneiform Digital Library Initiative collection, we apply deep learning methods to classify cuneiform tablets, covering over 3,000 years of history.","By leveraging statistical, computational techniques, and generative modeling through Variational Auto-Encoders (VAEs), we achieve substantial advancements in the automatic classification of these ancient documents, focusing on the tablets' silhouettes as key predictors.","Our classification approach begins with a Decision Tree using height-to-width ratios and culminates with a ResNet50 model, achieving a 61% macro F1-score for tablet silhouettes.","Moreover, we introduce novel VAE-powered tools to enhance explainability and enable researchers to explore changes in tablet shapes across different eras and genres.","This research contributes to document analysis and diplomatics by demonstrating the value of large-scale data analysis combined with statistical methods.","These insights offer valuable tools for historians and epigraphists, enriching our understanding of cuneiform tablets and the cultures that produced them."],"url":"http://arxiv.org/abs/2406.04039v1"}
{"created":"2024-06-06 12:59:46","title":"Pre-trained Transformer Uncovers Meaningful Patterns in Human Mobility Data","abstract":"We empirically demonstrate that a transformer pre-trained on country-scale unlabeled human mobility data learns embeddings capable, through fine-tuning, of developing a deep understanding of the target geography and its corresponding mobility patterns. Utilizing an adaptation framework, we evaluate the performance of our pre-trained embeddings in encapsulating a broad spectrum of concepts directly and indirectly related to human mobility. This includes basic notions, such as geographic location and distance, and extends to more complex constructs, such as administrative divisions and land cover. Our extensive empirical analysis reveals a substantial performance boost gained from pre-training, reaching up to 38% in tasks such as tree-cover regression. We attribute this result to the ability of the pre-training to uncover meaningful patterns hidden in the raw data, beneficial for modeling relevant high-level concepts. The pre-trained embeddings emerge as robust representations of regions and trajectories, potentially valuable for a wide range of downstream applications.","sentences":["We empirically demonstrate that a transformer pre-trained on country-scale unlabeled human mobility data learns embeddings capable, through fine-tuning, of developing a deep understanding of the target geography and its corresponding mobility patterns.","Utilizing an adaptation framework, we evaluate the performance of our pre-trained embeddings in encapsulating a broad spectrum of concepts directly and indirectly related to human mobility.","This includes basic notions, such as geographic location and distance, and extends to more complex constructs, such as administrative divisions and land cover.","Our extensive empirical analysis reveals a substantial performance boost gained from pre-training, reaching up to 38% in tasks such as tree-cover regression.","We attribute this result to the ability of the pre-training to uncover meaningful patterns hidden in the raw data, beneficial for modeling relevant high-level concepts.","The pre-trained embeddings emerge as robust representations of regions and trajectories, potentially valuable for a wide range of downstream applications."],"url":"http://arxiv.org/abs/2406.04029v1"}
{"created":"2024-06-06 12:55:50","title":"PowerPeeler: A Precise and General Dynamic Deobfuscation Method for PowerShell Scripts","abstract":"PowerShell is a powerful and versatile task automation tool. Unfortunately, it is also widely abused by cyber attackers. To bypass malware detection and hinder threat analysis, attackers often employ diverse techniques to obfuscate malicious PowerShell scripts. Existing deobfuscation tools suffer from the limitation of static analysis, which fails to simulate the real deobfuscation process accurately.   In this paper, we propose PowerPeeler. To the best of our knowledge, it is the first dynamic PowerShell script deobfuscation approach at the instruction level. It utilizes expression-related Abstract Syntax Tree (AST) nodes to identify potential obfuscated script pieces. Then, PowerPeeler correlates the AST nodes with their corresponding instructions and monitors the script's entire execution process. Subsequently, PowerPeeler dynamically tracks the execution of these instructions and records their execution results. Finally, PowerPeeler stringifies these results to replace the corresponding obfuscated script pieces and reconstruct the deobfuscated script.   To evaluate the effectiveness of PowerPeeler, we collect 1,736,669 real-world malicious PowerShell samples with diversity obfuscation methods. We compare PowerPeeler with five state-of-the-art deobfuscation tools and GPT-4. The evaluation results demonstrate that PowerPeeler can effectively handle all well-known obfuscation methods. Additionally, the deobfuscation correctness rate of PowerPeeler reaches 95%, significantly surpassing that of other tools. PowerPeeler not only recovers the highest amount of sensitive data but also maintains a semantic consistency over 97%, which is also the best. Moreover, PowerPeeler effectively obtains the largest quantity of valid deobfuscated results within a limited time frame. Furthermore, PowerPeeler is extendable and can be used as a helpful tool for other cyber security solutions.","sentences":["PowerShell is a powerful and versatile task automation tool.","Unfortunately, it is also widely abused by cyber attackers.","To bypass malware detection and hinder threat analysis, attackers often employ diverse techniques to obfuscate malicious PowerShell scripts.","Existing deobfuscation tools suffer from the limitation of static analysis, which fails to simulate the real deobfuscation process accurately.   ","In this paper, we propose PowerPeeler.","To the best of our knowledge, it is the first dynamic PowerShell script deobfuscation approach at the instruction level.","It utilizes expression-related Abstract Syntax Tree (AST) nodes to identify potential obfuscated script pieces.","Then, PowerPeeler correlates the AST nodes with their corresponding instructions and monitors the script's entire execution process.","Subsequently, PowerPeeler dynamically tracks the execution of these instructions and records their execution results.","Finally, PowerPeeler stringifies these results to replace the corresponding obfuscated script pieces and reconstruct the deobfuscated script.   ","To evaluate the effectiveness of PowerPeeler, we collect 1,736,669 real-world malicious PowerShell samples with diversity obfuscation methods.","We compare PowerPeeler with five state-of-the-art deobfuscation tools and GPT-4.","The evaluation results demonstrate that PowerPeeler can effectively handle all well-known obfuscation methods.","Additionally, the deobfuscation correctness rate of PowerPeeler reaches 95%, significantly surpassing that of other tools.","PowerPeeler not only recovers the highest amount of sensitive data but also maintains a semantic consistency over 97%, which is also the best.","Moreover, PowerPeeler effectively obtains the largest quantity of valid deobfuscated results within a limited time frame.","Furthermore, PowerPeeler is extendable and can be used as a helpful tool for other cyber security solutions."],"url":"http://arxiv.org/abs/2406.04027v1"}
{"created":"2024-06-06 12:29:14","title":"The Failed Migration of Academic Twitter","abstract":"Following change in Twitter's ownership and subsequent changes to content moderation policies, many in academia looked to move their discourse elsewhere and migration to Mastodon was pursued by some. Our study looks at the dynamics of this migration. Utilizing publicly available user account data, we track the posting activity of academics on Mastodon over a one year period. Our analyses reveal significant challenges sustaining user engagement on Mastodon due to its decentralized structure as well as competition from other platforms such as Bluesky and Threads. The movement lost momentum after an initial surge of enthusiasm as most users did not maintain their activity levels, and those who did faced lower levels of engagement compared to Twitter. Our findings highlight the challenges involved in transitioning professional communities to decentralized platforms, emphasizing the need for focusing on migrating social connections for long-term user engagement.","sentences":["Following change in Twitter's ownership and subsequent changes to content moderation policies, many in academia looked to move their discourse elsewhere and migration to Mastodon was pursued by some.","Our study looks at the dynamics of this migration.","Utilizing publicly available user account data, we track the posting activity of academics on Mastodon over a one year period.","Our analyses reveal significant challenges sustaining user engagement on Mastodon due to its decentralized structure as well as competition from other platforms such as Bluesky and Threads.","The movement lost momentum after an initial surge of enthusiasm as most users did not maintain their activity levels, and those who did faced lower levels of engagement compared to Twitter.","Our findings highlight the challenges involved in transitioning professional communities to decentralized platforms, emphasizing the need for focusing on migrating social connections for long-term user engagement."],"url":"http://arxiv.org/abs/2406.04005v1"}
{"created":"2024-06-06 12:17:57","title":"Unveiling the Dynamics of Information Interplay in Supervised Learning","abstract":"In this paper, we use matrix information theory as an analytical tool to analyze the dynamics of the information interplay between data representations and classification head vectors in the supervised learning process. Specifically, inspired by the theory of Neural Collapse, we introduce matrix mutual information ratio (MIR) and matrix entropy difference ratio (HDR) to assess the interactions of data representation and class classification heads in supervised learning, and we determine the theoretical optimal values for MIR and HDR when Neural Collapse happens. Our experiments show that MIR and HDR can effectively explain many phenomena occurring in neural networks, for example, the standard supervised training dynamics, linear mode connectivity, and the performance of label smoothing and pruning. Additionally, we use MIR and HDR to gain insights into the dynamics of grokking, which is an intriguing phenomenon observed in supervised training, where the model demonstrates generalization capabilities long after it has learned to fit the training data. Furthermore, we introduce MIR and HDR as loss terms in supervised and semi-supervised learning to optimize the information interactions among samples and classification heads. The empirical results provide evidence of the method's effectiveness, demonstrating that the utilization of MIR and HDR not only aids in comprehending the dynamics throughout the training process but can also enhances the training procedure itself.","sentences":["In this paper, we use matrix information theory as an analytical tool to analyze the dynamics of the information interplay between data representations and classification head vectors in the supervised learning process.","Specifically, inspired by the theory of Neural Collapse, we introduce matrix mutual information ratio (MIR) and matrix entropy difference ratio (HDR) to assess the interactions of data representation and class classification heads in supervised learning, and we determine the theoretical optimal values for MIR and HDR when Neural Collapse happens.","Our experiments show that MIR and HDR can effectively explain many phenomena occurring in neural networks, for example, the standard supervised training dynamics, linear mode connectivity, and the performance of label smoothing and pruning.","Additionally, we use MIR and HDR to gain insights into the dynamics of grokking, which is an intriguing phenomenon observed in supervised training, where the model demonstrates generalization capabilities long after it has learned to fit the training data.","Furthermore, we introduce MIR and HDR as loss terms in supervised and semi-supervised learning to optimize the information interactions among samples and classification heads.","The empirical results provide evidence of the method's effectiveness, demonstrating that the utilization of MIR and HDR not only aids in comprehending the dynamics throughout the training process but can also enhances the training procedure itself."],"url":"http://arxiv.org/abs/2406.03999v1"}
{"created":"2024-06-06 12:15:39","title":"Exploring Topic Modelling of User Reviews as a Monitoring Mechanism for Emergent Issues Within Social VR Communities","abstract":"Users of social virtual reality (VR) platforms often use user reviews to document incidents of witnessed and/or experienced user harassment. However, at present, research has yet to be explore utilising this data as a monitoring mechanism to identify emergent issues within social VR communities. Such a system would be of much benefit to developers and researchers as it would enable the automatic identification of emergent issues as they occur, provide a means of longitudinally analysing harassment, and reduce the reliance on alternative, high cost, monitoring methodologies, e.g. observation or interview studies. To contribute towards the development of such a system, we collected approximately 40,000 Rec Room user reviews from the Steam storefront. We then analysed our dataset's sentiment, word/term frequencies, and conducted a topic modelling analysis of the negative reviews detected in our dataset. We report our approach was capable of longitudinally monitoring changes in review sentiment and identifying high level themes related to types of harassment known to occur in social VR platforms.","sentences":["Users of social virtual reality (VR) platforms often use user reviews to document incidents of witnessed and/or experienced user harassment.","However, at present, research has yet to be explore utilising this data as a monitoring mechanism to identify emergent issues within social VR communities.","Such a system would be of much benefit to developers and researchers as it would enable the automatic identification of emergent issues as they occur, provide a means of longitudinally analysing harassment, and reduce the reliance on alternative, high cost, monitoring methodologies, e.g. observation or interview studies.","To contribute towards the development of such a system, we collected approximately 40,000 Rec Room user reviews from the Steam storefront.","We then analysed our dataset's sentiment, word/term frequencies, and conducted a topic modelling analysis of the negative reviews detected in our dataset.","We report our approach was capable of longitudinally monitoring changes in review sentiment and identifying high level themes related to types of harassment known to occur in social VR platforms."],"url":"http://arxiv.org/abs/2406.03994v1"}
{"created":"2024-06-06 12:00:41","title":"On The Persona-based Summarization of Domain-Specific Documents","abstract":"In an ever-expanding world of domain-specific knowledge, the increasing complexity of consuming, and storing information necessitates the generation of summaries from large information repositories. However, every persona of a domain has different requirements of information and hence their summarization. For example, in the healthcare domain, a persona-based (such as Doctor, Nurse, Patient etc.) approach is imperative to deliver targeted medical information efficiently. Persona-based summarization of domain-specific information by humans is a high cognitive load task and is generally not preferred. The summaries generated by two different humans have high variability and do not scale in cost and subject matter expertise as domains and personas grow. Further, AI-generated summaries using generic Large Language Models (LLMs) may not necessarily offer satisfactory accuracy for different domains unless they have been specifically trained on domain-specific data and can also be very expensive to use in day-to-day operations. Our contribution in this paper is two-fold: 1) We present an approach to efficiently fine-tune a domain-specific small foundation LLM using a healthcare corpus and also show that we can effectively evaluate the summarization quality using AI-based critiquing. 2) We further show that AI-based critiquing has good concordance with Human-based critiquing of the summaries. Hence, such AI-based pipelines to generate domain-specific persona-based summaries can be easily scaled to other domains such as legal, enterprise documents, education etc. in a very efficient and cost-effective manner.","sentences":["In an ever-expanding world of domain-specific knowledge, the increasing complexity of consuming, and storing information necessitates the generation of summaries from large information repositories.","However, every persona of a domain has different requirements of information and hence their summarization.","For example, in the healthcare domain, a persona-based (such as Doctor, Nurse, Patient etc.)","approach is imperative to deliver targeted medical information efficiently.","Persona-based summarization of domain-specific information by humans is a high cognitive load task and is generally not preferred.","The summaries generated by two different humans have high variability and do not scale in cost and subject matter expertise as domains and personas grow.","Further, AI-generated summaries using generic Large Language Models (LLMs) may not necessarily offer satisfactory accuracy for different domains unless they have been specifically trained on domain-specific data and can also be very expensive to use in day-to-day operations.","Our contribution in this paper is two-fold: 1) We present an approach to efficiently fine-tune a domain-specific small foundation LLM using a healthcare corpus and also show that we can effectively evaluate the summarization quality using AI-based critiquing.","2) We further show that AI-based critiquing has good concordance with Human-based critiquing of the summaries.","Hence, such AI-based pipelines to generate domain-specific persona-based summaries can be easily scaled to other domains such as legal, enterprise documents, education etc.","in a very efficient and cost-effective manner."],"url":"http://arxiv.org/abs/2406.03986v1"}
{"created":"2024-06-06 11:57:25","title":"LNQ Challenge 2023: Learning Mediastinal Lymph Node Segmentation with a Probabilistic Lymph Node Atlas","abstract":"The evaluation of lymph node metastases plays a crucial role in achieving precise cancer staging, influencing subsequent decisions regarding treatment options. Lymph node detection poses challenges due to the presence of unclear boundaries and the diverse range of sizes and morphological characteristics, making it a resource-intensive process. As part of the LNQ 2023 MICCAI challenge, we propose the use of anatomical priors as a tool to address the challenges that persist in mediastinal lymph node segmentation in combination with the partial annotation of the challenge training data. The model ensemble using all suggested modifications yields a Dice score of 0.6033 and segments 57% of the ground truth lymph nodes, compared to 27% when training on CT only. Segmentation accuracy is improved significantly by incorporating a probabilistic lymph node atlas in loss weighting and post-processing. The largest performance gains are achieved by oversampling fully annotated data to account for the partial annotation of the challenge training data, as well as adding additional data augmentation to address the high heterogeneity of the CT images and lymph node appearance. Our code is available at https://github.com/MICAI-IMI-UzL/LNQ2023.","sentences":["The evaluation of lymph node metastases plays a crucial role in achieving precise cancer staging, influencing subsequent decisions regarding treatment options.","Lymph node detection poses challenges due to the presence of unclear boundaries and the diverse range of sizes and morphological characteristics, making it a resource-intensive process.","As part of the LNQ 2023 MICCAI challenge, we propose the use of anatomical priors as a tool to address the challenges that persist in mediastinal lymph node segmentation in combination with the partial annotation of the challenge training data.","The model ensemble using all suggested modifications yields a Dice score of 0.6033 and segments 57% of the ground truth lymph nodes, compared to 27% when training on CT only.","Segmentation accuracy is improved significantly by incorporating a probabilistic lymph node atlas in loss weighting and post-processing.","The largest performance gains are achieved by oversampling fully annotated data to account for the partial annotation of the challenge training data, as well as adding additional data augmentation to address the high heterogeneity of the CT images and lymph node appearance.","Our code is available at https://github.com/MICAI-IMI-UzL/LNQ2023."],"url":"http://arxiv.org/abs/2406.03984v1"}
{"created":"2024-06-06 11:05:49","title":"Haptic in-sensor computing device made of carbon nanotube-polydimethylsiloxane nanocomposites","abstract":"The importance of haptic in-sensor computing devices has been increasing. In this study, we successfully fabricated a haptic sensor with a hierarchical structure via the sacrificial template method, using carbon nanotubes-polydimethylsiloxane (CNTs-PDMS) nanocomposites for in-sensor computing applications. The CNTs-PDMS nanocomposite sensors, with different sensitivities, were obtained by varying the amount of CNTs. We transformed the input stimuli into higher-dimensional information, enabling a new path for the CNTs-PDMS nanocomposite application, which was implemented on a robotic hand as an in-sensor computing device by applying a reservoir computing paradigm. The nonlinear output data obtained from the sensors were trained using linear regression and used to classify nine different objects used in everyday life with an object recognition accuracy of >80 % for each object. This approach could enable tactile sensation in robots while reducing the computational cost.","sentences":["The importance of haptic in-sensor computing devices has been increasing.","In this study, we successfully fabricated a haptic sensor with a hierarchical structure via the sacrificial template method, using carbon nanotubes-polydimethylsiloxane (CNTs-PDMS) nanocomposites for in-sensor computing applications.","The CNTs-PDMS nanocomposite sensors, with different sensitivities, were obtained by varying the amount of CNTs.","We transformed the input stimuli into higher-dimensional information, enabling a new path for the CNTs-PDMS nanocomposite application, which was implemented on a robotic hand as an in-sensor computing device by applying a reservoir computing paradigm.","The nonlinear output data obtained from the sensors were trained using linear regression and used to classify nine different objects used in everyday life with an object recognition accuracy of >80 % for each object.","This approach could enable tactile sensation in robots while reducing the computational cost."],"url":"http://arxiv.org/abs/2406.03958v1"}
{"created":"2024-06-06 10:50:26","title":"UltraMedical: Building Specialized Generalists in Biomedicine","abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains and are moving towards more specialized areas. Recent advanced proprietary models such as GPT-4 and Gemini have achieved significant advancements in biomedicine, which have also raised privacy and security challenges. The construction of specialized generalists hinges largely on high-quality datasets, enhanced by techniques like supervised fine-tuning and reinforcement learning from human or AI feedback, and direct preference optimization. However, these leading technologies (e.g., preference learning) are still significantly limited in the open source community due to the scarcity of specialized data. In this paper, we present the UltraMedical collections, which consist of high-quality manual and synthetic datasets in the biomedicine domain, featuring preference annotations across multiple advanced LLMs. By utilizing these datasets, we fine-tune a suite of specialized medical models based on Llama-3 series, demonstrating breathtaking capabilities across various medical benchmarks. Moreover, we develop powerful reward models skilled in biomedical and general reward benchmark, enhancing further online preference learning within the biomedical LLM community.","sentences":["Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains and are moving towards more specialized areas.","Recent advanced proprietary models such as GPT-4 and Gemini have achieved significant advancements in biomedicine, which have also raised privacy and security challenges.","The construction of specialized generalists hinges largely on high-quality datasets, enhanced by techniques like supervised fine-tuning and reinforcement learning from human or AI feedback, and direct preference optimization.","However, these leading technologies (e.g., preference learning) are still significantly limited in the open source community due to the scarcity of specialized data.","In this paper, we present the UltraMedical collections, which consist of high-quality manual and synthetic datasets in the biomedicine domain, featuring preference annotations across multiple advanced LLMs.","By utilizing these datasets, we fine-tune a suite of specialized medical models based on Llama-3 series, demonstrating breathtaking capabilities across various medical benchmarks.","Moreover, we develop powerful reward models skilled in biomedical and general reward benchmark, enhancing further online preference learning within the biomedical LLM community."],"url":"http://arxiv.org/abs/2406.03949v1"}
{"created":"2024-06-06 10:38:01","title":"Provably Neural Active Learning Succeeds via Prioritizing Perplexing Samples","abstract":"Neural Network-based active learning (NAL) is a cost-effective data selection technique that utilizes neural networks to select and train on a small subset of samples. While existing work successfully develops various effective or theory-justified NAL algorithms, the understanding of the two commonly used query criteria of NAL: uncertainty-based and diversity-based, remains in its infancy. In this work, we try to move one step forward by offering a unified explanation for the success of both query criteria-based NAL from a feature learning view. Specifically, we consider a feature-noise data model comprising easy-to-learn or hard-to-learn features disrupted by noise, and conduct analysis over 2-layer NN-based NALs in the pool-based scenario. We provably show that both uncertainty-based and diversity-based NAL are inherently amenable to one and the same principle, i.e., striving to prioritize samples that contain yet-to-be-learned features. We further prove that this shared principle is the key to their success-achieve small test error within a small labeled set. Contrastingly, the strategy-free passive learning exhibits a large test error due to the inadequate learning of yet-to-be-learned features, necessitating resort to a significantly larger label complexity for a sufficient test error reduction. Experimental results validate our findings.","sentences":["Neural Network-based active learning (NAL) is a cost-effective data selection technique that utilizes neural networks to select and train on a small subset of samples.","While existing work successfully develops various effective or theory-justified NAL algorithms, the understanding of the two commonly used query criteria of NAL: uncertainty-based and diversity-based, remains in its infancy.","In this work, we try to move one step forward by offering a unified explanation for the success of both query criteria-based NAL from a feature learning view.","Specifically, we consider a feature-noise data model comprising easy-to-learn or hard-to-learn features disrupted by noise, and conduct analysis over 2-layer NN-based NALs in the pool-based scenario.","We provably show that both uncertainty-based and diversity-based NAL are inherently amenable to one and the same principle, i.e., striving to prioritize samples that contain yet-to-be-learned features.","We further prove that this shared principle is the key to their success-achieve small test error within a small labeled set.","Contrastingly, the strategy-free passive learning exhibits a large test error due to the inadequate learning of yet-to-be-learned features, necessitating resort to a significantly larger label complexity for a sufficient test error reduction.","Experimental results validate our findings."],"url":"http://arxiv.org/abs/2406.03944v1"}
{"created":"2024-06-06 10:17:51","title":"Breeding Programs Optimization with Reinforcement Learning","abstract":"Crop breeding is crucial in improving agricultural productivity while potentially decreasing land usage, greenhouse gas emissions, and water consumption. However, breeding programs are challenging due to long turnover times, high-dimensional decision spaces, long-term objectives, and the need to adapt to rapid climate change. This paper introduces the use of Reinforcement Learning (RL) to optimize simulated crop breeding programs. RL agents are trained to make optimal crop selection and cross-breeding decisions based on genetic information. To benchmark RL-based breeding algorithms, we introduce a suite of Gym environments. The study demonstrates the superiority of RL techniques over standard practices in terms of genetic gain when simulated in silico using real-world genomic maize data.","sentences":["Crop breeding is crucial in improving agricultural productivity while potentially decreasing land usage, greenhouse gas emissions, and water consumption.","However, breeding programs are challenging due to long turnover times, high-dimensional decision spaces, long-term objectives, and the need to adapt to rapid climate change.","This paper introduces the use of Reinforcement Learning (RL) to optimize simulated crop breeding programs.","RL agents are trained to make optimal crop selection and cross-breeding decisions based on genetic information.","To benchmark RL-based breeding algorithms, we introduce a suite of Gym environments.","The study demonstrates the superiority of RL techniques over standard practices in terms of genetic gain when simulated in silico using real-world genomic maize data."],"url":"http://arxiv.org/abs/2406.03932v1"}
{"created":"2024-06-06 10:04:53","title":"Latent Neural Operator for Solving Forward and Inverse PDE Problems","abstract":"Neural operators effectively solve PDE problems from data without knowing the explicit equations, which learn the map from the input sequences of observed samples to the predicted values. Most existed works build the model in the original geometric space, leading to high computational costs when the number of sample points is large. We present the Latent Neural Operator (LNO) solving PDEs in the latent space. In particular, we first propose Physics-Cross-Attention (PhCA) transforming representation from the geometric space to the latent space, then learn the operator in the latent space, and finally recover the real-world geometric space via the inverse PhCA map. Our model retains flexibility that can decode values in any position not limited to locations defined in training set, and therefore can naturally perform interpolation and extrapolation tasks particularly useful for inverse problems. Moreover, the proposed LNO improves in both prediction accuracy and computational efficiency. Experiments show that LNO reduces the GPU memory by 50%, speeds up training 1.8 times, and reaches state-of-the-art accuracy on four out of six benchmarks for forward problems and a benchmark for inverse problem.","sentences":["Neural operators effectively solve PDE problems from data without knowing the explicit equations, which learn the map from the input sequences of observed samples to the predicted values.","Most existed works build the model in the original geometric space, leading to high computational costs when the number of sample points is large.","We present the Latent Neural Operator (LNO) solving PDEs in the latent space.","In particular, we first propose Physics-Cross-Attention (PhCA) transforming representation from the geometric space to the latent space, then learn the operator in the latent space, and finally recover the real-world geometric space via the inverse PhCA map.","Our model retains flexibility that can decode values in any position not limited to locations defined in training set, and therefore can naturally perform interpolation and extrapolation tasks particularly useful for inverse problems.","Moreover, the proposed LNO improves in both prediction accuracy and computational efficiency.","Experiments show that LNO reduces the GPU memory by 50%, speeds up training 1.8 times, and reaches state-of-the-art accuracy on four out of six benchmarks for forward problems and a benchmark for inverse problem."],"url":"http://arxiv.org/abs/2406.03923v1"}
{"created":"2024-06-06 10:04:10","title":"Engineering Semi-streaming DFS algorithms","abstract":"Depth first search is a fundamental graph problem having a wide range of applications. For a graph $G=(V,E)$ having $n$ vertices and $m$ edges, the DFS tree can be computed in $O(m+n)$ using $O(m)$ space where $m=O(n^2)$. In the streaming environment, most graph problems are studied in the semi-streaming model where several passes (preferably one) are allowed over the input, allowing $O(nk)$ local space for some $k=o(n)$. Trivially, using $O(m)$ space, DFS can be computed in one pass, and using $O(n)$ space, it can be computed in $O(n)$ passes.   Khan and Mehta [STACS19] presented several algorithms allowing trade-offs between space and passes, where $O(nk)$ space results in $O(n/k)$ passes. They also empirically analyzed their algorithm to require only a few passes in practice for even $O(n)$ space. Chang et al. [STACS20] presented an alternate proof for the same and also presented $O(\\sqrt{n})$ pass algorithm requiring $O(n~poly\\log n)$ space with a finer trade-off between space and passes. However, their algorithm uses complex black box algorithms, making it impractical.   We perform an experimental analysis of the practical semi-streaming DFS algorithms. Our analysis ranges from real graphs to random graphs (uniform and power-law). We also present several heuristics to improve the state-of-the-art algorithms and study their impact. Our heuristics improve state of the art by $40-90\\%$, achieving optimal one pass in almost $40-50\\%$ cases (improved from zero). In random graphs, they improve from $30-90\\%$, again requiring optimal one pass for even very small values of $k$. Overall, our heuristics improved the relatively complex state-of-the-art algorithm significantly, requiring merely two passes in the worst case for random graphs. Additionally, our heuristics made the relatively simpler algorithm practically usable even for very small space bounds, which was impractical earlier.","sentences":["Depth first search is a fundamental graph problem having a wide range of applications.","For a graph $G=(V,E)$ having $n$ vertices and $m$ edges, the DFS tree can be computed in $O(m+n)$ using $O(m)$ space where $m=O(n^2)$.","In the streaming environment, most graph problems are studied in the semi-streaming model where several passes (preferably one) are allowed over the input, allowing $O(nk)$ local space for some $k=o(n)$. Trivially, using $O(m)$ space, DFS can be computed in one pass, and using $O(n)$ space, it can be computed in $O(n)$ passes.   ","Khan and Mehta [STACS19] presented several algorithms allowing trade-offs between space and passes, where $O(nk)$ space results in $O(n/k)$ passes.","They also empirically analyzed their algorithm to require only a few passes in practice for even $O(n)$ space.","Chang et al.","[STACS20] presented an alternate proof for the same and also presented $O(\\sqrt{n})$ pass algorithm requiring $O(n~poly\\log n)$ space with a finer trade-off between space and passes.","However, their algorithm uses complex black box algorithms, making it impractical.   ","We perform an experimental analysis of the practical semi-streaming DFS algorithms.","Our analysis ranges from real graphs to random graphs (uniform and power-law).","We also present several heuristics to improve the state-of-the-art algorithms and study their impact.","Our heuristics improve state of the art by $40-90\\%$, achieving optimal one pass in almost $40-50\\%$ cases (improved from zero).","In random graphs, they improve from $30-90\\%$, again requiring optimal one pass for even very small values of $k$. Overall, our heuristics improved the relatively complex state-of-the-art algorithm significantly, requiring merely two passes in the worst case for random graphs.","Additionally, our heuristics made the relatively simpler algorithm practically usable even for very small space bounds, which was impractical earlier."],"url":"http://arxiv.org/abs/2406.03922v1"}
{"created":"2024-06-06 10:03:42","title":"Knowledge Transfer, Knowledge Gaps, and Knowledge Silos in Citation Networks","abstract":"The advancement of science relies on the exchange of ideas across disciplines and the integration of diverse knowledge domains. However, tracking knowledge flows and interdisciplinary integration in rapidly evolving, multidisciplinary fields remains a significant challenge. This work introduces a novel network analysis framework to study the dynamics of knowledge transfer directly from citation data. By applying dynamic community detection to cumulative, time-evolving citation networks, we can identify research areas as groups of papers sharing knowledge sources and outputs. Our analysis characterises the life-cycles and knowledge transfer patterns of these dynamic communities over time. We demonstrate our approach through a case study of eXplainable Artificial Intelligence (XAI) research, an emerging interdisciplinary field at the intersection of machine learning, statistics, and psychology. Key findings include: (i) knowledge transfer between these important foundational topics and the contemporary topics in XAI research is limited, and the extent of knowledge transfer varies across different contemporary research topics; (ii) certain application domains exist as isolated \"knowledge silos\"; (iii) significant \"knowledge gaps\" are identified between related XAI research areas, suggesting opportunities for cross-pollination and improved knowledge integration. By mapping interdisciplinary integration and bridging knowledge gaps, this work can inform strategies to synthesise ideas from disparate sources and drive innovation. More broadly, our proposed framework enables new insights into the evolution of knowledge ecosystems directly from citation data, with applications spanning literature review, research planning, and science policy.","sentences":["The advancement of science relies on the exchange of ideas across disciplines and the integration of diverse knowledge domains.","However, tracking knowledge flows and interdisciplinary integration in rapidly evolving, multidisciplinary fields remains a significant challenge.","This work introduces a novel network analysis framework to study the dynamics of knowledge transfer directly from citation data.","By applying dynamic community detection to cumulative, time-evolving citation networks, we can identify research areas as groups of papers sharing knowledge sources and outputs.","Our analysis characterises the life-cycles and knowledge transfer patterns of these dynamic communities over time.","We demonstrate our approach through a case study of eXplainable Artificial Intelligence (XAI) research, an emerging interdisciplinary field at the intersection of machine learning, statistics, and psychology.","Key findings include: (i) knowledge transfer between these important foundational topics and the contemporary topics in XAI research is limited, and the extent of knowledge transfer varies across different contemporary research topics; (ii) certain application domains exist as isolated \"knowledge silos\"; (iii) significant \"knowledge gaps\" are identified between related XAI research areas, suggesting opportunities for cross-pollination and improved knowledge integration.","By mapping interdisciplinary integration and bridging knowledge gaps, this work can inform strategies to synthesise ideas from disparate sources and drive innovation.","More broadly, our proposed framework enables new insights into the evolution of knowledge ecosystems directly from citation data, with applications spanning literature review, research planning, and science policy."],"url":"http://arxiv.org/abs/2406.03921v1"}
{"created":"2024-06-06 10:02:49","title":"Towards Physically Consistent Deep Learning For Climate Model Parameterizations","abstract":"Climate models play a critical role in understanding and projecting climate change. Due to their complexity, their horizontal resolution of ~40-100 km remains too coarse to resolve processes such as clouds and convection, which need to be approximated via parameterizations. These parameterizations are a major source of systematic errors and large uncertainties in climate projections. Deep learning (DL)-based parameterizations, trained on computationally expensive, short high-resolution simulations, have shown great promise for improving climate models in that regard. However, their lack of interpretability and tendency to learn spurious non-physical correlations result in reduced trust in the climate simulation. We propose an efficient supervised learning framework for DL-based parameterizations that leads to physically consistent models with improved interpretability and negligible computational overhead compared to standard supervised training. First, key features determining the target physical processes are uncovered. Subsequently, the neural network is fine-tuned using only those relevant features. We show empirically that our method robustly identifies a small subset of the inputs as actual physical drivers, therefore, removing spurious non-physical relationships. This results in by design physically consistent and interpretable neural networks while maintaining the predictive performance of standard black-box DL-based parameterizations. Our framework represents a crucial step in addressing a major challenge in data-driven climate model parameterizations by respecting the underlying physical processes, and may also benefit physically consistent deep learning in other research fields.","sentences":["Climate models play a critical role in understanding and projecting climate change.","Due to their complexity, their horizontal resolution of ~40-100 km remains too coarse to resolve processes such as clouds and convection, which need to be approximated via parameterizations.","These parameterizations are a major source of systematic errors and large uncertainties in climate projections.","Deep learning (DL)-based parameterizations, trained on computationally expensive, short high-resolution simulations, have shown great promise for improving climate models in that regard.","However, their lack of interpretability and tendency to learn spurious non-physical correlations result in reduced trust in the climate simulation.","We propose an efficient supervised learning framework for DL-based parameterizations that leads to physically consistent models with improved interpretability and negligible computational overhead compared to standard supervised training.","First, key features determining the target physical processes are uncovered.","Subsequently, the neural network is fine-tuned using only those relevant features.","We show empirically that our method robustly identifies a small subset of the inputs as actual physical drivers, therefore, removing spurious non-physical relationships.","This results in by design physically consistent and interpretable neural networks while maintaining the predictive performance of standard black-box DL-based parameterizations.","Our framework represents a crucial step in addressing a major challenge in data-driven climate model parameterizations by respecting the underlying physical processes, and may also benefit physically consistent deep learning in other research fields."],"url":"http://arxiv.org/abs/2406.03920v1"}
{"created":"2024-06-06 09:58:35","title":"The \u03b1-Lomax Distribution: A Compound Channel Model","abstract":"In this paper, we propose the {\\alpha}-Lomax distribution as a new compound fading channel model. This new distribution generalizes the recently introduced Lomax fading channel model. It is worth noting that the Lomax distribution is a decreasing function, while the {\\alpha}-Lomax is a unimodal function, offering greater flexibility in modeling wireless fading channels. In particular, we derive closed-form expressions for the probability density function and cumulative distribution function for the instantaneous signal-to-noise ratio (SNR). Additionally, we provide closed-form expressions for several fundamental performance metrics, including outage probability, average bit error rate, and channel capacity. Furthermore, we derive closed-form expression for the average block-length error rate in short-packet communications. Moreover, we fit the PDF of the proposed channel model to empirical data obtained from a device-to-device communication system. We also offer simple and accurate approximations for these expressions in the high SNR regime.","sentences":["In this paper, we propose the {\\alpha}-Lomax distribution as a new compound fading channel model.","This new distribution generalizes the recently introduced Lomax fading channel model.","It is worth noting that the Lomax distribution is a decreasing function, while the {\\alpha}-Lomax is a unimodal function, offering greater flexibility in modeling wireless fading channels.","In particular, we derive closed-form expressions for the probability density function and cumulative distribution function for the instantaneous signal-to-noise ratio (SNR).","Additionally, we provide closed-form expressions for several fundamental performance metrics, including outage probability, average bit error rate, and channel capacity.","Furthermore, we derive closed-form expression for the average block-length error rate in short-packet communications.","Moreover, we fit the PDF of the proposed channel model to empirical data obtained from a device-to-device communication system.","We also offer simple and accurate approximations for these expressions in the high SNR regime."],"url":"http://arxiv.org/abs/2406.03918v1"}
{"created":"2024-06-06 09:51:30","title":"GenSafe: A Generalizable Safety Enhancer for Safe Reinforcement Learning Algorithms Based on Reduced Order Markov Decision Process Model","abstract":"Although deep reinforcement learning has demonstrated impressive achievements in controlling various autonomous systems, e.g., autonomous vehicles or humanoid robots, its inherent reliance on random exploration raises safety concerns in their real-world applications. To improve system safety during the learning process, a variety of Safe Reinforcement Learning (SRL) algorithms have been proposed, which usually incorporate safety constraints within the Constrained Markov Decision Process (CMDP) framework. However, the efficacy of these SRL algorithms often relies on accurate function approximations, a task that is notably challenging to accomplish in the early learning stages due to data insufficiency. To address this problem, we introduce a Genralizable Safety enhancer (GenSafe) in this work. Leveraging model order reduction techniques, we first construct a Reduced Order Markov Decision Process (ROMDP) as a low-dimensional proxy for the original cost function in CMDP. Then, by solving ROMDP-based constraints that are reformulated from the original cost constraints, the proposed GenSafe refines the actions taken by the agent to enhance the possibility of constraint satisfaction. Essentially, GenSafe acts as an additional safety layer for SRL algorithms, offering broad compatibility across diverse SRL approaches. The performance of GenSafe is examined on multiple SRL benchmark problems. The results show that, it is not only able to improve the safety performance, especially in the early learning phases, but also to maintain the task performance at a satisfactory level.","sentences":["Although deep reinforcement learning has demonstrated impressive achievements in controlling various autonomous systems, e.g., autonomous vehicles or humanoid robots, its inherent reliance on random exploration raises safety concerns in their real-world applications.","To improve system safety during the learning process, a variety of Safe Reinforcement Learning (SRL) algorithms have been proposed, which usually incorporate safety constraints within the Constrained Markov Decision Process (CMDP) framework.","However, the efficacy of these SRL algorithms often relies on accurate function approximations, a task that is notably challenging to accomplish in the early learning stages due to data insufficiency.","To address this problem, we introduce a Genralizable Safety enhancer (GenSafe) in this work.","Leveraging model order reduction techniques, we first construct a Reduced Order Markov Decision Process (ROMDP) as a low-dimensional proxy for the original cost function in CMDP.","Then, by solving ROMDP-based constraints that are reformulated from the original cost constraints, the proposed GenSafe refines the actions taken by the agent to enhance the possibility of constraint satisfaction.","Essentially, GenSafe acts as an additional safety layer for SRL algorithms, offering broad compatibility across diverse SRL approaches.","The performance of GenSafe is examined on multiple SRL benchmark problems.","The results show that, it is not only able to improve the safety performance, especially in the early learning phases, but also to maintain the task performance at a satisfactory level."],"url":"http://arxiv.org/abs/2406.03912v1"}
{"created":"2024-06-06 09:29:40","title":"Transductive Off-policy Proximal Policy Optimization","abstract":"Proximal Policy Optimization (PPO) is a popular model-free reinforcement learning algorithm, esteemed for its simplicity and efficacy. However, due to its inherent on-policy nature, its proficiency in harnessing data from disparate policies is constrained. This paper introduces a novel off-policy extension to the original PPO method, christened Transductive Off-policy PPO (ToPPO). Herein, we provide theoretical justification for incorporating off-policy data in PPO training and prudent guidelines for its safe application. Our contribution includes a novel formulation of the policy improvement lower bound for prospective policies derived from off-policy data, accompanied by a computationally efficient mechanism to optimize this bound, underpinned by assurances of monotonic improvement. Comprehensive experimental results across six representative tasks underscore ToPPO's promising performance.","sentences":["Proximal Policy Optimization (PPO) is a popular model-free reinforcement learning algorithm, esteemed for its simplicity and efficacy.","However, due to its inherent on-policy nature, its proficiency in harnessing data from disparate policies is constrained.","This paper introduces a novel off-policy extension to the original PPO method, christened Transductive Off-policy PPO (ToPPO).","Herein, we provide theoretical justification for incorporating off-policy data in PPO training and prudent guidelines for its safe application.","Our contribution includes a novel formulation of the policy improvement lower bound for prospective policies derived from off-policy data, accompanied by a computationally efficient mechanism to optimize this bound, underpinned by assurances of monotonic improvement.","Comprehensive experimental results across six representative tasks underscore ToPPO's promising performance."],"url":"http://arxiv.org/abs/2406.03894v1"}
{"created":"2024-06-06 09:28:08","title":"How Good is Zero-Shot MT Evaluation for Low Resource Indian Languages?","abstract":"While machine translation evaluation has been studied primarily for high-resource languages, there has been a recent interest in evaluation for low-resource languages due to the increasing availability of data and models. In this paper, we focus on a zero-shot evaluation setting focusing on low-resource Indian languages, namely Assamese, Kannada, Maithili, and Punjabi. We collect sufficient Multi-Dimensional Quality Metrics (MQM) and Direct Assessment (DA) annotations to create test sets and meta-evaluate a plethora of automatic evaluation metrics. We observe that even for learned metrics, which are known to exhibit zero-shot performance, the Kendall Tau and Pearson correlations with human annotations are only as high as 0.32 and 0.45. Synthetic data approaches show mixed results and overall do not help close the gap by much for these languages. This indicates that there is still a long way to go for low-resource evaluation.","sentences":["While machine translation evaluation has been studied primarily for high-resource languages, there has been a recent interest in evaluation for low-resource languages due to the increasing availability of data and models.","In this paper, we focus on a zero-shot evaluation setting focusing on low-resource Indian languages, namely Assamese, Kannada, Maithili, and Punjabi.","We collect sufficient Multi-Dimensional Quality Metrics (MQM) and Direct Assessment (DA) annotations to create test sets and meta-evaluate a plethora of automatic evaluation metrics.","We observe that even for learned metrics, which are known to exhibit zero-shot performance, the Kendall Tau and Pearson correlations with human annotations are only as high as 0.32 and 0.45.","Synthetic data approaches show mixed results and overall do not help close the gap by much for these languages.","This indicates that there is still a long way to go for low-resource evaluation."],"url":"http://arxiv.org/abs/2406.03893v1"}
{"created":"2024-06-06 09:24:42","title":"MSE-Based Training and Transmission Optimization for MIMO ISAC Systems","abstract":"In this paper, we investigate a multiple-input multiple-output (MIMO) integrated sensing and communication (ISAC) system under typical block-fading channels. As a non-trivial extension to most existing works on ISAC, both the training and transmission signals sent by the ISAC transmitter are exploited for sensing. Specifically, we develop two training and transmission design schemes to minimize a weighted sum of the mean-squared errors (MSEs) of data transmission and radar target response matrix (TRM) estimation. For the former, we first optimize the training signal for simultaneous communication channel and radar TRM estimation. Then, based on the estimated instantaneous channel state information (CSI), we propose an efficient majorization-minimization (MM)-based robust ISAC transmission design, where a semi-closed form solution is obtained in each iteration. For the second scheme, the ISAC transmitter is assumed to have statistical CSI only for reducing the feedback overhead. With CSI statistics available, we integrate the training and transmission design into one single problem and propose an MM-based alternating algorithm to find a high-quality solution. In addition, we provide alternative structured and low-complexity solutions for both schemes under certain special cases. Finally, simulation results demonstrate that the radar performance is significantly improved compared to the existing scheme that integrates sensing into the transmission stage only. Moreover, it is verified that the investigated two schemes have advantages in terms of communication and sensing performances, respectively.","sentences":["In this paper, we investigate a multiple-input multiple-output (MIMO) integrated sensing and communication (ISAC) system under typical block-fading channels.","As a non-trivial extension to most existing works on ISAC, both the training and transmission signals sent by the ISAC transmitter are exploited for sensing.","Specifically, we develop two training and transmission design schemes to minimize a weighted sum of the mean-squared errors (MSEs) of data transmission and radar target response matrix (TRM) estimation.","For the former, we first optimize the training signal for simultaneous communication channel and radar TRM estimation.","Then, based on the estimated instantaneous channel state information (CSI), we propose an efficient majorization-minimization (MM)-based robust ISAC transmission design, where a semi-closed form solution is obtained in each iteration.","For the second scheme, the ISAC transmitter is assumed to have statistical CSI only for reducing the feedback overhead.","With CSI statistics available, we integrate the training and transmission design into one single problem and propose an MM-based alternating algorithm to find a high-quality solution.","In addition, we provide alternative structured and low-complexity solutions for both schemes under certain special cases.","Finally, simulation results demonstrate that the radar performance is significantly improved compared to the existing scheme that integrates sensing into the transmission stage only.","Moreover, it is verified that the investigated two schemes have advantages in terms of communication and sensing performances, respectively."],"url":"http://arxiv.org/abs/2406.03888v1"}
{"created":"2024-06-06 09:18:42","title":"Evaluating the IWSLT2023 Speech Translation Tasks: Human Annotations, Automatic Metrics, and Segmentation","abstract":"Human evaluation is a critical component in machine translation system development and has received much attention in text translation research. However, little prior work exists on the topic of human evaluation for speech translation, which adds additional challenges such as noisy data and segmentation mismatches. We take first steps to fill this gap by conducting a comprehensive human evaluation of the results of several shared tasks from the last International Workshop on Spoken Language Translation (IWSLT 2023). We propose an effective evaluation strategy based on automatic resegmentation and direct assessment with segment context. Our analysis revealed that: 1) the proposed evaluation strategy is robust and scores well-correlated with other types of human judgements; 2) automatic metrics are usually, but not always, well-correlated with direct assessment scores; and 3) COMET as a slightly stronger automatic metric than chrF, despite the segmentation noise introduced by the resegmentation step systems. We release the collected human-annotated data in order to encourage further investigation.","sentences":["Human evaluation is a critical component in machine translation system development and has received much attention in text translation research.","However, little prior work exists on the topic of human evaluation for speech translation, which adds additional challenges such as noisy data and segmentation mismatches.","We take first steps to fill this gap by conducting a comprehensive human evaluation of the results of several shared tasks from the last International Workshop on Spoken Language Translation (IWSLT 2023).","We propose an effective evaluation strategy based on automatic resegmentation and direct assessment with segment context.","Our analysis revealed that: 1) the proposed evaluation strategy is robust and scores well-correlated with other types of human judgements; 2) automatic metrics are usually, but not always, well-correlated with direct assessment scores; and 3) COMET as a slightly stronger automatic metric than chrF, despite the segmentation noise introduced by the resegmentation step systems.","We release the collected human-annotated data in order to encourage further investigation."],"url":"http://arxiv.org/abs/2406.03881v1"}
{"created":"2024-06-06 09:12:30","title":"Bench2Drive: Towards Multi-Ability Benchmarking of Closed-Loop End-To-End Autonomous Driving","abstract":"In an era marked by the rapid scaling of foundation models, autonomous driving technologies are approaching a transformative threshold where end-to-end autonomous driving (E2E-AD) emerges due to its potential of scaling up in the data-driven manner. However, existing E2E-AD methods are mostly evaluated under the open-loop log-replay manner with L2 errors and collision rate as metrics (e.g., in nuScenes), which could not fully reflect the driving performance of algorithms as recently acknowledged in the community. For those E2E-AD methods evaluated under the closed-loop protocol, they are tested in fixed routes (e.g., Town05Long and Longest6 in CARLA) with the driving score as metrics, which is known for high variance due to the unsmoothed metric function and large randomness in the long route. Besides, these methods usually collect their own data for training, which makes algorithm-level fair comparison infeasible.   To fulfill the paramount need of comprehensive, realistic, and fair testing environments for Full Self-Driving (FSD), we present Bench2Drive, the first benchmark for evaluating E2E-AD systems' multiple abilities in a closed-loop manner. Bench2Drive's official training data consists of 2 million fully annotated frames, collected from 10000 short clips uniformly distributed under 44 interactive scenarios (cut-in, overtaking, detour, etc), 23 weathers (sunny, foggy, rainy, etc), and 12 towns (urban, village, university, etc) in CARLA v2. Its evaluation protocol requires E2E-AD models to pass 44 interactive scenarios under different locations and weathers which sums up to 220 routes and thus provides a comprehensive and disentangled assessment about their driving capability under different situations. We implement state-of-the-art E2E-AD models and evaluate them in Bench2Drive, providing insights regarding current status and future directions.","sentences":["In an era marked by the rapid scaling of foundation models, autonomous driving technologies are approaching a transformative threshold where end-to-end autonomous driving (E2E-AD) emerges due to its potential of scaling up in the data-driven manner.","However, existing E2E-AD methods are mostly evaluated under the open-loop log-replay manner with L2 errors and collision rate as metrics (e.g., in nuScenes), which could not fully reflect the driving performance of algorithms as recently acknowledged in the community.","For those E2E-AD methods evaluated under the closed-loop protocol, they are tested in fixed routes (e.g., Town05Long and Longest6 in CARLA) with the driving score as metrics, which is known for high variance due to the unsmoothed metric function and large randomness in the long route.","Besides, these methods usually collect their own data for training, which makes algorithm-level fair comparison infeasible.   ","To fulfill the paramount need of comprehensive, realistic, and fair testing environments for Full Self-Driving (FSD), we present Bench2Drive, the first benchmark for evaluating E2E-AD systems' multiple abilities in a closed-loop manner.","Bench2Drive's official training data consists of 2 million fully annotated frames, collected from 10000 short clips uniformly distributed under 44 interactive scenarios (cut-in, overtaking, detour, etc), 23 weathers (sunny, foggy, rainy, etc), and 12 towns (urban, village, university, etc) in CARLA v2.","Its evaluation protocol requires E2E-AD models to pass 44 interactive scenarios under different locations and weathers which sums up to 220 routes and thus provides a comprehensive and disentangled assessment about their driving capability under different situations.","We implement state-of-the-art E2E-AD models and evaluate them in Bench2Drive, providing insights regarding current status and future directions."],"url":"http://arxiv.org/abs/2406.03877v1"}
{"created":"2024-06-06 09:02:31","title":"BLSP-Emo: Towards Empathetic Large Speech-Language Models","abstract":"The recent release of GPT-4o showcased the potential of end-to-end multimodal models, not just in terms of low latency but also in their ability to understand and generate expressive speech with rich emotions. While the details are unknown to the open research community, it likely involves significant amounts of curated data and compute, neither of which is readily accessible. In this paper, we present BLSP-Emo (Bootstrapped Language-Speech Pretraining with Emotion support), a novel approach to developing an end-to-end speech-language model capable of understanding both semantics and emotions in speech and generate empathetic responses. BLSP-Emo utilizes existing speech recognition (ASR) and speech emotion recognition (SER) datasets through a two-stage process. The first stage focuses on semantic alignment, following recent work on pretraining speech-language models using ASR data. The second stage performs emotion alignment with the pretrained speech-language model on an emotion-aware continuation task constructed from SER data. Our experiments demonstrate that the BLSP-Emo model excels in comprehending speech and delivering empathetic responses, both in instruction-following tasks and conversations.","sentences":["The recent release of GPT-4o showcased the potential of end-to-end multimodal models, not just in terms of low latency but also in their ability to understand and generate expressive speech with rich emotions.","While the details are unknown to the open research community, it likely involves significant amounts of curated data and compute, neither of which is readily accessible.","In this paper, we present BLSP-Emo (Bootstrapped Language-Speech Pretraining with Emotion support), a novel approach to developing an end-to-end speech-language model capable of understanding both semantics and emotions in speech and generate empathetic responses.","BLSP-Emo utilizes existing speech recognition (ASR) and speech emotion recognition (SER) datasets through a two-stage process.","The first stage focuses on semantic alignment, following recent work on pretraining speech-language models using ASR data.","The second stage performs emotion alignment with the pretrained speech-language model on an emotion-aware continuation task constructed from SER data.","Our experiments demonstrate that the BLSP-Emo model excels in comprehending speech and delivering empathetic responses, both in instruction-following tasks and conversations."],"url":"http://arxiv.org/abs/2406.03872v1"}
{"created":"2024-06-06 08:58:14","title":"Recovering document annotations for sentence-level bitext","abstract":"Data availability limits the scope of any given task. In machine translation, historical models were incapable of handling longer contexts, so the lack of document-level datasets was less noticeable. Now, despite the emergence of long-sequence methods, we remain within a sentence-level paradigm and without data to adequately approach context-aware machine translation. Most large-scale datasets have been processed through a pipeline that discards document-level metadata. In this work, we reconstruct document-level information for three (ParaCrawl, News Commentary, and Europarl) large datasets in German, French, Spanish, Italian, Polish, and Portuguese (paired with English). We then introduce a document-level filtering technique as an alternative to traditional bitext filtering. We present this filtering with analysis to show that this method prefers context-consistent translations rather than those that may have been sentence-level machine translated. Last we train models on these longer contexts and demonstrate improvement in document-level translation without degradation of sentence-level translation. We release our dataset, ParaDocs, and resulting models as a resource to the community.","sentences":["Data availability limits the scope of any given task.","In machine translation, historical models were incapable of handling longer contexts, so the lack of document-level datasets was less noticeable.","Now, despite the emergence of long-sequence methods, we remain within a sentence-level paradigm and without data to adequately approach context-aware machine translation.","Most large-scale datasets have been processed through a pipeline that discards document-level metadata.","In this work, we reconstruct document-level information for three (ParaCrawl, News Commentary, and Europarl) large datasets in German, French, Spanish, Italian, Polish, and Portuguese (paired with English).","We then introduce a document-level filtering technique as an alternative to traditional bitext filtering.","We present this filtering with analysis to show that this method prefers context-consistent translations rather than those that may have been sentence-level machine translated.","Last we train models on these longer contexts and demonstrate improvement in document-level translation without degradation of sentence-level translation.","We release our dataset, ParaDocs, and resulting models as a resource to the community."],"url":"http://arxiv.org/abs/2406.03869v1"}
{"created":"2024-06-06 08:55:01","title":"PALM: A Efficient Performance Simulator for Tiled Accelerators with Large-scale Model Training","abstract":"Deep learning (DL) models are piquing high interest and scaling at an unprecedented rate. To this end, a handful of tiled accelerators have been proposed to support such large-scale training tasks. However, these accelerators often incorporate numerous cores or tiles even extending to wafer-scale, substantial on-chip bandwidth, and distributed memory systems. This results in an exceedingly complex design space. Moreover, conducting actual training experiments to find optimal configurations is impractical due to time constraints. Hence, predicting the optimal mapping of various parallelisms to such tiled system architectures becomes crucial. In this study, leveraging an analysis of existing mainstream DL model training strategies, we introduce a performance simulator named PALM. PALM targets both the training and inference processes for tiled accelerators, aiming to inspire the design of current and future accelerators. Specifically, (i) we establish a scheduling mechanism among tiled accelerators based on an event-driven framework; (ii) we support user-configurable pipeline, tensor, and data parallelism on tiled accelerators, determining the absolute performance throughput under these parallelism strategies; (iii) we model the interaction of on-chip SRAM, NoC, and off-chip DRAM during operator execution. This work is available here: https://github.com/fangjh21/PALM.","sentences":["Deep learning (DL) models are piquing high interest and scaling at an unprecedented rate.","To this end, a handful of tiled accelerators have been proposed to support such large-scale training tasks.","However, these accelerators often incorporate numerous cores or tiles even extending to wafer-scale, substantial on-chip bandwidth, and distributed memory systems.","This results in an exceedingly complex design space.","Moreover, conducting actual training experiments to find optimal configurations is impractical due to time constraints.","Hence, predicting the optimal mapping of various parallelisms to such tiled system architectures becomes crucial.","In this study, leveraging an analysis of existing mainstream DL model training strategies, we introduce a performance simulator named PALM.","PALM targets both the training and inference processes for tiled accelerators, aiming to inspire the design of current and future accelerators.","Specifically, (i) we establish a scheduling mechanism among tiled accelerators based on an event-driven framework; (ii) we support user-configurable pipeline, tensor, and data parallelism on tiled accelerators, determining the absolute performance throughput under these parallelism strategies; (iii) we model the interaction of on-chip SRAM, NoC, and off-chip DRAM during operator execution.","This work is available here: https://github.com/fangjh21/PALM."],"url":"http://arxiv.org/abs/2406.03868v1"}
{"created":"2024-06-06 08:53:01","title":"LLplace: The 3D Indoor Scene Layout Generation and Editing via Large Language Model","abstract":"Designing 3D indoor layouts is a crucial task with significant applications in virtual reality, interior design, and automated space planning. Existing methods for 3D layout design either rely on diffusion models, which utilize spatial relationship priors, or heavily leverage the inferential capabilities of proprietary Large Language Models (LLMs), which require extensive prompt engineering and in-context exemplars via black-box trials. These methods often face limitations in generalization and dynamic scene editing. In this paper, we introduce LLplace, a novel 3D indoor scene layout designer based on lightweight fine-tuned open-source LLM Llama3. LLplace circumvents the need for spatial relationship priors and in-context exemplars, enabling efficient and credible room layout generation based solely on user inputs specifying the room type and desired objects. We curated a new dialogue dataset based on the 3D-Front dataset, expanding the original data volume and incorporating dialogue data for adding and removing objects. This dataset can enhance the LLM's spatial understanding. Furthermore, through dialogue, LLplace activates the LLM's capability to understand 3D layouts and perform dynamic scene editing, enabling the addition and removal of objects. Our approach demonstrates that LLplace can effectively generate and edit 3D indoor layouts interactively and outperform existing methods in delivering high-quality 3D design solutions. Code and dataset will be released.","sentences":["Designing 3D indoor layouts is a crucial task with significant applications in virtual reality, interior design, and automated space planning.","Existing methods for 3D layout design either rely on diffusion models, which utilize spatial relationship priors, or heavily leverage the inferential capabilities of proprietary Large Language Models (LLMs), which require extensive prompt engineering and in-context exemplars via black-box trials.","These methods often face limitations in generalization and dynamic scene editing.","In this paper, we introduce LLplace, a novel 3D indoor scene layout designer based on lightweight fine-tuned open-source LLM Llama3.","LLplace circumvents the need for spatial relationship priors and in-context exemplars, enabling efficient and credible room layout generation based solely on user inputs specifying the room type and desired objects.","We curated a new dialogue dataset based on the 3D-Front dataset, expanding the original data volume and incorporating dialogue data for adding and removing objects.","This dataset can enhance the LLM's spatial understanding.","Furthermore, through dialogue, LLplace activates the LLM's capability to understand 3D layouts and perform dynamic scene editing, enabling the addition and removal of objects.","Our approach demonstrates that LLplace can effectively generate and edit 3D indoor layouts interactively and outperform existing methods in delivering high-quality 3D design solutions.","Code and dataset will be released."],"url":"http://arxiv.org/abs/2406.03866v1"}
{"created":"2024-06-06 08:50:16","title":"PairNet: Training with Observed Pairs to Estimate Individual Treatment Effect","abstract":"Given a dataset of individuals each described by a covariate vector, a treatment, and an observed outcome on the treatment, the goal of the individual treatment effect (ITE) estimation task is to predict outcome changes resulting from a change in treatment. A fundamental challenge is that in the observational data, a covariate's outcome is observed only under one treatment, whereas we need to infer the difference in outcomes under two different treatments. Several existing approaches address this issue through training with inferred pseudo-outcomes, but their success relies on the quality of these pseudo-outcomes. We propose PairNet, a novel ITE estimation training strategy that minimizes losses over pairs of examples based on their factual observed outcomes. Theoretical analysis for binary treatments reveals that PairNet is a consistent estimator of ITE risk, and achieves smaller generalization error than baseline models. Empirical comparison with thirteen existing methods across eight benchmarks, covering both discrete and continuous treatments, shows that PairNet achieves significantly lower ITE error compared to the baselines. Also, it is model-agnostic and easy to implement.","sentences":["Given a dataset of individuals each described by a covariate vector, a treatment, and an observed outcome on the treatment, the goal of the individual treatment effect (ITE) estimation task is to predict outcome changes resulting from a change in treatment.","A fundamental challenge is that in the observational data, a covariate's outcome is observed only under one treatment, whereas we need to infer the difference in outcomes under two different treatments.","Several existing approaches address this issue through training with inferred pseudo-outcomes, but their success relies on the quality of these pseudo-outcomes.","We propose PairNet, a novel ITE estimation training strategy that minimizes losses over pairs of examples based on their factual observed outcomes.","Theoretical analysis for binary treatments reveals that PairNet is a consistent estimator of ITE risk, and achieves smaller generalization error than baseline models.","Empirical comparison with thirteen existing methods across eight benchmarks, covering both discrete and continuous treatments, shows that PairNet achieves significantly lower ITE error compared to the baselines.","Also, it is model-agnostic and easy to implement."],"url":"http://arxiv.org/abs/2406.03864v1"}
{"created":"2024-06-06 08:46:00","title":"From operculum and body tail movements to different coupling of physical activity and respiratory frequency in farmed gilthead sea bream and European sea bass. Insights on aquaculture biosensing","abstract":"The AEFishBIT tri-axial accelerometer was externally attached to the operculum to assess the divergent activity and respiratory patterns of two marine farmed fish, the gilthead sea bream (Sparus aurata) and European sea bass (Dicentrarchus labrax). Analysis of raw data from exercised fish highlighted the large amplitude of operculum aperture and body tail movements in European sea bass, which were overall more stable at low-medium exercise intensity levels. Cosinor analysis in free-swimming fish (on-board data processing) highlighted a pronounced daily rhythmicity of locomotor activity and respiratory frequency in both gilthead sea bream and European sea bass. Acrophases of activity and respiration were coupled in gilthead sea bream, acting feeding time (once daily at 11:00 h) as a main synchronizing factor. By contrast, locomotor activity and respiratory frequency were out of phase in European sea bass with activity acrophase on early morning and respiration acrophase on the afternoon. The daily range of activity and respiration variation was also higher in European sea bass, probably as part of the adaptation of this fish species to act as a fast swimming predator. In any case, lower locomotor activity and enhanced respiration were associated with larger body weight in both fish species. This agrees with the notion that selection for fast growth in farming conditions is accompanied by a lower activity profile, which may favor an efficient feed conversion for growth purposes. Therefore, the use of behavioral monitoring is becoming a reliable and large-scale promising tool for selecting more efficient farmed fish, allowing researchers and farmers to establish stricter criteria of welfare for more sustainable and ethical fish production.","sentences":["The AEFishBIT tri-axial accelerometer was externally attached to the operculum to assess the divergent activity and respiratory patterns of two marine farmed fish, the gilthead sea bream (Sparus aurata) and European sea bass (Dicentrarchus labrax).","Analysis of raw data from exercised fish highlighted the large amplitude of operculum aperture and body tail movements in European sea bass, which were overall more stable at low-medium exercise intensity levels.","Cosinor analysis in free-swimming fish (on-board data processing) highlighted a pronounced daily rhythmicity of locomotor activity and respiratory frequency in both gilthead sea bream and European sea bass.","Acrophases of activity and respiration were coupled in gilthead sea bream, acting feeding time (once daily at 11:00 h) as a main synchronizing factor.","By contrast, locomotor activity and respiratory frequency were out of phase in European sea bass with activity acrophase on early morning and respiration acrophase on the afternoon.","The daily range of activity and respiration variation was also higher in European sea bass, probably as part of the adaptation of this fish species to act as a fast swimming predator.","In any case, lower locomotor activity and enhanced respiration were associated with larger body weight in both fish species.","This agrees with the notion that selection for fast growth in farming conditions is accompanied by a lower activity profile, which may favor an efficient feed conversion for growth purposes.","Therefore, the use of behavioral monitoring is becoming a reliable and large-scale promising tool for selecting more efficient farmed fish, allowing researchers and farmers to establish stricter criteria of welfare for more sustainable and ethical fish production."],"url":"http://arxiv.org/abs/2406.03859v1"}
{"created":"2024-06-06 08:45:36","title":"Reducing the climate impact of data portals: a case study","abstract":"The carbon footprint share of the information and communication technology (ICT) sector has steadily increased in the past decade and is predicted to make up as much as 23 \\% of global emissions in 2030. This shows a pressing need for developers, including the information retrieval community, to make their code more energy-efficient. In this project proposal, we discuss techniques to reduce the energy footprint of the MaRDI (Mathematical Research Data Initiative) Portal, a MediaWiki-based knowledge base. In future work, we plan to implement these changes and provide concrete measurements on the gain in energy efficiency. Researchers developing similar knowledge bases can adapt our measures to reduce their environmental footprint. In this way, we are working on mitigating the climate impact of Information Retrieval research.","sentences":["The carbon footprint share of the information and communication technology (ICT) sector has steadily increased in the past decade and is predicted to make up as much as 23 \\% of global emissions in 2030.","This shows a pressing need for developers, including the information retrieval community, to make their code more energy-efficient.","In this project proposal, we discuss techniques to reduce the energy footprint of the MaRDI (Mathematical Research Data Initiative) Portal, a MediaWiki-based knowledge base.","In future work, we plan to implement these changes and provide concrete measurements on the gain in energy efficiency.","Researchers developing similar knowledge bases can adapt our measures to reduce their environmental footprint.","In this way, we are working on mitigating the climate impact of Information Retrieval research."],"url":"http://arxiv.org/abs/2406.03858v1"}
{"created":"2024-06-06 08:42:36","title":"MuJo: Multimodal Joint Feature Space Learning for Human Activity Recognition","abstract":"Human Activity Recognition is a longstanding problem in AI with applications in a broad range of areas: from healthcare, sports and fitness, security, and human computer interaction to robotics. The performance of HAR in real-world settings is strongly dependent on the type and quality of the input signal that can be acquired. Given an unobstructed, high-quality camera view of a scene, computer vision systems, in particular in conjunction with foundational models (e.g., CLIP), can today fairly reliably distinguish complex activities. On the other hand, recognition using modalities such as wearable sensors (which are often more broadly available, e.g, in mobile phones and smartwatches) is a more difficult problem, as the signals often contain less information and labeled training data is more difficult to acquire. In this work, we show how we can improve HAR performance across different modalities using multimodal contrastive pretraining. Our approach MuJo (Multimodal Joint Feature Space Learning), learns a multimodal joint feature space with video, language, pose, and IMU sensor data. The proposed approach combines contrastive and multitask learning methods and analyzes different multitasking strategies for learning a compact shared representation. A large dataset with parallel video, language, pose, and sensor data points is also introduced to support the research, along with an analysis of the robustness of the multimodal joint space for modal-incomplete and low-resource data. On the MM-Fit dataset, our model achieves an impressive Macro F1-Score of up to 0.992 with only 2% of the train data and 0.999 when using all available training data for classification tasks. Moreover, in the scenario where the MM-Fit dataset is unseen, we demonstrate a generalization performance of up to 0.638.","sentences":["Human Activity Recognition is a longstanding problem in AI with applications in a broad range of areas: from healthcare, sports and fitness, security, and human computer interaction to robotics.","The performance of HAR in real-world settings is strongly dependent on the type and quality of the input signal that can be acquired.","Given an unobstructed, high-quality camera view of a scene, computer vision systems, in particular in conjunction with foundational models (e.g., CLIP), can today fairly reliably distinguish complex activities.","On the other hand, recognition using modalities such as wearable sensors (which are often more broadly available, e.g, in mobile phones and smartwatches) is a more difficult problem, as the signals often contain less information and labeled training data is more difficult to acquire.","In this work, we show how we can improve HAR performance across different modalities using multimodal contrastive pretraining.","Our approach MuJo (Multimodal Joint Feature Space Learning), learns a multimodal joint feature space with video, language, pose, and IMU sensor data.","The proposed approach combines contrastive and multitask learning methods and analyzes different multitasking strategies for learning a compact shared representation.","A large dataset with parallel video, language, pose, and sensor data points is also introduced to support the research, along with an analysis of the robustness of the multimodal joint space for modal-incomplete and low-resource data.","On the MM-Fit dataset, our model achieves an impressive Macro F1-Score of up to 0.992 with only 2% of the train data and 0.999 when using all available training data for classification tasks.","Moreover, in the scenario where the MM-Fit dataset is unseen, we demonstrate a generalization performance of up to 0.638."],"url":"http://arxiv.org/abs/2406.03857v1"}
{"created":"2024-06-06 08:41:46","title":"Performance of large language models in numerical vs. semantic medical knowledge: Benchmarking on evidence-based Q&As","abstract":"Clinical problem-solving requires processing of semantic medical knowledge such as illness scripts and numerical medical knowledge of diagnostic tests for evidence-based decision-making. As large language models (LLMs) show promising results in many aspects of language-based clinical practice, their ability to generate non-language evidence-based answers to clinical questions is inherently limited by tokenization. Therefore, we evaluated LLMs' performance on two question types: numeric (correlating findings) and semantic (differentiating entities) while examining differences within and between LLMs in medical aspects and comparing their performance to humans. To generate straightforward multi-choice questions and answers (QAs) based on evidence-based medicine (EBM), we used a comprehensive medical knowledge graph (encompassed data from more than 50,00 peer-reviewed articles) and created the \"EBMQA\". EBMQA contains 105,000 QAs labeled with medical and non-medical topics and classified into numerical or semantic questions. We benchmarked this dataset using more than 24,500 QAs on two state-of-the-art LLMs: Chat-GPT4 and Claude3-Opus. We evaluated the LLMs accuracy on semantic and numerical question types and according to sub-labeled topics. For validation, six medical experts were tested on 100 numerical EBMQA questions. We found that both LLMs excelled more in semantic than numerical QAs, with Claude3 surpassing GPT4 in numerical QAs. However, both LLMs showed inter and intra gaps in different medical aspects and remained inferior to humans. Thus, their medical advice should be addressed carefully.","sentences":["Clinical problem-solving requires processing of semantic medical knowledge such as illness scripts and numerical medical knowledge of diagnostic tests for evidence-based decision-making.","As large language models (LLMs) show promising results in many aspects of language-based clinical practice, their ability to generate non-language evidence-based answers to clinical questions is inherently limited by tokenization.","Therefore, we evaluated LLMs' performance on two question types: numeric (correlating findings) and semantic (differentiating entities) while examining differences within and between LLMs in medical aspects and comparing their performance to humans.","To generate straightforward multi-choice questions and answers (QAs) based on evidence-based medicine (EBM), we used a comprehensive medical knowledge graph (encompassed data from more than 50,00 peer-reviewed articles) and created the \"EBMQA\".","EBMQA contains 105,000 QAs labeled with medical and non-medical topics and classified into numerical or semantic questions.","We benchmarked this dataset using more than 24,500 QAs on two state-of-the-art LLMs: Chat-GPT4 and Claude3-Opus.","We evaluated the LLMs accuracy on semantic and numerical question types and according to sub-labeled topics.","For validation, six medical experts were tested on 100 numerical EBMQA questions.","We found that both LLMs excelled more in semantic than numerical QAs, with Claude3 surpassing GPT4 in numerical QAs.","However, both LLMs showed inter and intra gaps in different medical aspects and remained inferior to humans.","Thus, their medical advice should be addressed carefully."],"url":"http://arxiv.org/abs/2406.03855v1"}
{"created":"2024-06-06 08:31:52","title":"A Noise-robust Multi-head Attention Mechanism for Formation Resistivity Prediction: Frequency Aware LSTM","abstract":"The prediction of formation resistivity plays a crucial role in the evaluation of oil and gas reservoirs, identification and assessment of geothermal energy resources, groundwater detection and monitoring, and carbon capture and storage. However, traditional well logging techniques fail to measure accurate resistivity in cased boreholes, and the transient electromagnetic method for cased borehole resistivity logging encounters challenges of high-frequency disaster (the problem of inadequate learning by neural networks in high-frequency features) and noise interference, badly affecting accuracy. To address these challenges, frequency-aware framework and temporal anti-noise block are proposed to build frequency aware LSTM (FAL). The frequency-aware framework implements a dual-stream structure through wavelet transformation, allowing the neural network to simultaneously handle high-frequency and low-frequency flows of time-series data, thus avoiding high-frequency disaster. The temporal anti-noise block integrates multiple attention mechanisms and soft-threshold attention mechanisms, enabling the model to better distinguish noise from redundant features. Ablation experiments demonstrate that the frequency-aware framework and temporal anti-noise block contribute significantly to performance improvement. FAL achieves a 24.3% improvement in R2 over LSTM, reaching the highest value of 0.91 among all models. In robustness experiments, the impact of noise on FAL is approximately 1/8 of the baseline, confirming the noise resistance of FAL. The proposed FAL effectively reduces noise interference in predicting formation resistivity from cased transient electromagnetic well logging curves, better learns high-frequency features, and thereby enhances the prediction accuracy and noise resistance of the neural network model.","sentences":["The prediction of formation resistivity plays a crucial role in the evaluation of oil and gas reservoirs, identification and assessment of geothermal energy resources, groundwater detection and monitoring, and carbon capture and storage.","However, traditional well logging techniques fail to measure accurate resistivity in cased boreholes, and the transient electromagnetic method for cased borehole resistivity logging encounters challenges of high-frequency disaster (the problem of inadequate learning by neural networks in high-frequency features) and noise interference, badly affecting accuracy.","To address these challenges, frequency-aware framework and temporal anti-noise block are proposed to build frequency aware LSTM (FAL).","The frequency-aware framework implements a dual-stream structure through wavelet transformation, allowing the neural network to simultaneously handle high-frequency and low-frequency flows of time-series data, thus avoiding high-frequency disaster.","The temporal anti-noise block integrates multiple attention mechanisms and soft-threshold attention mechanisms, enabling the model to better distinguish noise from redundant features.","Ablation experiments demonstrate that the frequency-aware framework and temporal anti-noise block contribute significantly to performance improvement.","FAL achieves a 24.3% improvement in R2 over LSTM, reaching the highest value of 0.91 among all models.","In robustness experiments, the impact of noise on FAL is approximately 1/8 of the baseline, confirming the noise resistance of FAL.","The proposed FAL effectively reduces noise interference in predicting formation resistivity from cased transient electromagnetic well logging curves, better learns high-frequency features, and thereby enhances the prediction accuracy and noise resistance of the neural network model."],"url":"http://arxiv.org/abs/2406.03849v1"}
{"created":"2024-06-06 08:25:43","title":"Lean Workbook: A large-scale Lean problem set formalized from natural language math problems","abstract":"Large language models have demonstrated impressive capabilities across various natural language processing tasks, especially in solving mathematical problems. However, large language models are not good at math theorem proving using formal languages like Lean. A significant challenge in this area is the scarcity of training data available in these formal languages. To address this issue, we propose a novel pipeline that iteratively generates and filters synthetic data to translate natural language mathematical problems into Lean 4 statements, and vice versa. Our results indicate that the synthetic data pipeline can provide useful training data and improve the performance of LLMs in translating and understanding complex mathematical problems and proofs. Our final dataset contains about 57K formal-informal question pairs along with searched proof from the math contest forum and 21 new IMO questions. We open-source our code at https://github.com/InternLM/InternLM-Math and our data at https://huggingface.co/datasets/InternLM/Lean-Workbook.","sentences":["Large language models have demonstrated impressive capabilities across various natural language processing tasks, especially in solving mathematical problems.","However, large language models are not good at math theorem proving using formal languages like Lean.","A significant challenge in this area is the scarcity of training data available in these formal languages.","To address this issue, we propose a novel pipeline that iteratively generates and filters synthetic data to translate natural language mathematical problems into Lean 4 statements, and vice versa.","Our results indicate that the synthetic data pipeline can provide useful training data and improve the performance of LLMs in translating and understanding complex mathematical problems and proofs.","Our final dataset contains about 57K formal-informal question pairs along with searched proof from the math contest forum and 21 new IMO questions.","We open-source our code at https://github.com/InternLM/InternLM-Math and our data at https://huggingface.co/datasets/InternLM/Lean-Workbook."],"url":"http://arxiv.org/abs/2406.03847v1"}
{"created":"2024-06-06 08:23:22","title":"Open Problem: Active Representation Learning","abstract":"In this work, we introduce the concept of Active Representation Learning, a novel class of problems that intertwines exploration and representation learning within partially observable environments. We extend ideas from Active Simultaneous Localization and Mapping (active SLAM), and translate them to scientific discovery problems, exemplified by adaptive microscopy. We explore the need for a framework that derives exploration skills from representations that are in some sense actionable, aiming to enhance the efficiency and effectiveness of data collection and model building in the natural sciences.","sentences":["In this work, we introduce the concept of Active Representation Learning, a novel class of problems that intertwines exploration and representation learning within partially observable environments.","We extend ideas from Active Simultaneous Localization and Mapping (active SLAM), and translate them to scientific discovery problems, exemplified by adaptive microscopy.","We explore the need for a framework that derives exploration skills from representations that are in some sense actionable, aiming to enhance the efficiency and effectiveness of data collection and model building in the natural sciences."],"url":"http://arxiv.org/abs/2406.03845v1"}
{"created":"2024-06-06 08:12:38","title":"Monocular Localization with Semantics Map for Autonomous Vehicles","abstract":"Accurate and robust localization remains a significant challenge for autonomous vehicles. The cost of sensors and limitations in local computational efficiency make it difficult to scale to large commercial applications. Traditional vision-based approaches focus on texture features that are susceptible to changes in lighting, season, perspective, and appearance. Additionally, the large storage size of maps with descriptors and complex optimization processes hinder system performance. To balance efficiency and accuracy, we propose a novel lightweight visual semantic localization algorithm that employs stable semantic features instead of low-level texture features. First, semantic maps are constructed offline by detecting semantic objects, such as ground markers, lane lines, and poles, using cameras or LiDAR sensors. Then, online visual localization is performed through data association of semantic features and map objects. We evaluated our proposed localization framework in the publicly available KAIST Urban dataset and in scenarios recorded by ourselves. The experimental results demonstrate that our method is a reliable and practical localization solution in various autonomous driving localization tasks.","sentences":["Accurate and robust localization remains a significant challenge for autonomous vehicles.","The cost of sensors and limitations in local computational efficiency make it difficult to scale to large commercial applications.","Traditional vision-based approaches focus on texture features that are susceptible to changes in lighting, season, perspective, and appearance.","Additionally, the large storage size of maps with descriptors and complex optimization processes hinder system performance.","To balance efficiency and accuracy, we propose a novel lightweight visual semantic localization algorithm that employs stable semantic features instead of low-level texture features.","First, semantic maps are constructed offline by detecting semantic objects, such as ground markers, lane lines, and poles, using cameras or LiDAR sensors.","Then, online visual localization is performed through data association of semantic features and map objects.","We evaluated our proposed localization framework in the publicly available KAIST Urban dataset and in scenarios recorded by ourselves.","The experimental results demonstrate that our method is a reliable and practical localization solution in various autonomous driving localization tasks."],"url":"http://arxiv.org/abs/2406.03835v1"}
{"created":"2024-06-06 08:08:01","title":"Exploiting Global Graph Homophily for Generalized Defense in Graph Neural Networks","abstract":"Graph neural network (GNN) models play a pivotal role in numerous tasks involving graph-related data analysis. Despite their efficacy, similar to other deep learning models, GNNs are susceptible to adversarial attacks. Even minor perturbations in graph data can induce substantial alterations in model predictions. While existing research has explored various adversarial defense techniques for GNNs, the challenge of defending against adversarial attacks on real-world scale graph data remains largely unresolved. On one hand, methods reliant on graph purification and preprocessing tend to excessively emphasize local graph information, leading to sub-optimal defensive outcomes. On the other hand, approaches rooted in graph structure learning entail significant time overheads, rendering them impractical for large-scale graphs. In this paper, we propose a new defense method named Talos, which enhances the global, rather than local, homophily of graphs as a defense. Experiments show that the proposed approach notably outperforms state-of-the-art defense approaches, while imposing little computational overhead.","sentences":["Graph neural network (GNN) models play a pivotal role in numerous tasks involving graph-related data analysis.","Despite their efficacy, similar to other deep learning models, GNNs are susceptible to adversarial attacks.","Even minor perturbations in graph data can induce substantial alterations in model predictions.","While existing research has explored various adversarial defense techniques for GNNs, the challenge of defending against adversarial attacks on real-world scale graph data remains largely unresolved.","On one hand, methods reliant on graph purification and preprocessing tend to excessively emphasize local graph information, leading to sub-optimal defensive outcomes.","On the other hand, approaches rooted in graph structure learning entail significant time overheads, rendering them impractical for large-scale graphs.","In this paper, we propose a new defense method named Talos, which enhances the global, rather than local, homophily of graphs as a defense.","Experiments show that the proposed approach notably outperforms state-of-the-art defense approaches, while imposing little computational overhead."],"url":"http://arxiv.org/abs/2406.03833v1"}
{"created":"2024-06-06 07:55:30","title":"A Survey on Intelligent Internet of Things: Applications, Security, Privacy, and Future Directions","abstract":"The rapid advances in the Internet of Things (IoT) have promoted a revolution in communication technology and offered various customer services. Artificial intelligence (AI) techniques have been exploited to facilitate IoT operations and maximize their potential in modern application scenarios. In particular, the convergence of IoT and AI has led to a new networking paradigm called Intelligent IoT (IIoT), which has the potential to significantly transform businesses and industrial domains. This paper presents a comprehensive survey of IIoT by investigating its significant applications in mobile networks, as well as its associated security and privacy issues. Specifically, we explore and discuss the roles of IIoT in a wide range of key application domains, from smart healthcare and smart cities to smart transportation and smart industries. Through such extensive discussions, we investigate important security issues in IIoT networks, where network attacks, confidentiality, integrity, and intrusion are analyzed, along with a discussion of potential countermeasures. Privacy issues in IIoT networks were also surveyed and discussed, including data, location, and model privacy leakage. Finally, we outline several key challenges and highlight potential research directions in this important area.","sentences":["The rapid advances in the Internet of Things (IoT) have promoted a revolution in communication technology and offered various customer services.","Artificial intelligence (AI) techniques have been exploited to facilitate IoT operations and maximize their potential in modern application scenarios.","In particular, the convergence of IoT and AI has led to a new networking paradigm called Intelligent IoT (IIoT), which has the potential to significantly transform businesses and industrial domains.","This paper presents a comprehensive survey of IIoT by investigating its significant applications in mobile networks, as well as its associated security and privacy issues.","Specifically, we explore and discuss the roles of IIoT in a wide range of key application domains, from smart healthcare and smart cities to smart transportation and smart industries.","Through such extensive discussions, we investigate important security issues in IIoT networks, where network attacks, confidentiality, integrity, and intrusion are analyzed, along with a discussion of potential countermeasures.","Privacy issues in IIoT networks were also surveyed and discussed, including data, location, and model privacy leakage.","Finally, we outline several key challenges and highlight potential research directions in this important area."],"url":"http://arxiv.org/abs/2406.03820v1"}
{"created":"2024-06-06 07:49:11","title":"Subspace Clustering in Wavelet Packets Domain","abstract":"Subspace clustering (SC) algorithms utilize the union of subspaces model to cluster data points according to the subspaces from which they are drawn. To better address separability of subspaces and robustness to noise we propose a wavelet packet (WP) based transform domain subspace clustering. Depending on the number of resolution levels, WP yields several representations instantiated in terms of subbands. The first approach combines original and subband data into one complementary multi-view representation. Afterward, we formulate joint representation learning as a low-rank MERA tensor network approximation problem. That is motivated by the strong representation power of the MERA network to capture complex intra/inter-view dependencies in corresponding self-representation tensor. In the second approach, we use a self-stopping computationally efficient method to select the subband with the smallest clustering error on the validation set. When existing SC algorithms are applied to the chosen subband, their performance is expected to improve. Consequently, both approaches enable the re-use of SC algorithms developed so far. Improved clustering performance is due to the dual nature of subbands as representations and filters, which is essential for noise suppression. We exemplify the proposed WP domain approach to SC on the MERA tensor network and eight other well-known linear SC algorithms using six well-known image datasets representing faces, digits, and objects. Although WP domain-based SC is a linear method, it achieved clustering performance comparable with some best deep SC algorithms and outperformed many other deep SC algorithms by a significant margin. That is in particular case for the WP MERA SC algorithm. On the COIL100 dataset, it achieves an accuracy of 87.45% and outperforms the best deep SC competitor in the amount of 14.75%.","sentences":["Subspace clustering (SC) algorithms utilize the union of subspaces model to cluster data points according to the subspaces from which they are drawn.","To better address separability of subspaces and robustness to noise we propose a wavelet packet (WP) based transform domain subspace clustering.","Depending on the number of resolution levels, WP yields several representations instantiated in terms of subbands.","The first approach combines original and subband data into one complementary multi-view representation.","Afterward, we formulate joint representation learning as a low-rank MERA tensor network approximation problem.","That is motivated by the strong representation power of the MERA network to capture complex intra/inter-view dependencies in corresponding self-representation tensor.","In the second approach, we use a self-stopping computationally efficient method to select the subband with the smallest clustering error on the validation set.","When existing SC algorithms are applied to the chosen subband, their performance is expected to improve.","Consequently, both approaches enable the re-use of SC algorithms developed so far.","Improved clustering performance is due to the dual nature of subbands as representations and filters, which is essential for noise suppression.","We exemplify the proposed WP domain approach to SC on the MERA tensor network and eight other well-known linear SC algorithms using six well-known image datasets representing faces, digits, and objects.","Although WP domain-based SC is a linear method, it achieved clustering performance comparable with some best deep SC algorithms and outperformed many other deep SC algorithms by a significant margin.","That is in particular case for the WP MERA SC algorithm.","On the COIL100 dataset, it achieves an accuracy of 87.45% and outperforms the best deep SC competitor in the amount of 14.75%."],"url":"http://arxiv.org/abs/2406.03819v1"}
{"created":"2024-06-06 07:40:00","title":"ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search","abstract":"Recent methodologies in LLM self-training mostly rely on LLM generating responses and filtering those with correct output answers as training data. This approach often yields a low-quality fine-tuning training set (e.g., incorrect plans or intermediate reasoning). In this paper, we develop a reinforced self-training approach, called ReST-MCTS*, based on integrating process reward guidance with tree search MCTS* for collecting higher-quality reasoning traces as well as per-step value to train policy and reward models. ReST-MCTS* circumvents the per-step manual annotation typically used to train process rewards by tree-search-based reinforcement learning: Given oracle final correct answers, ReST-MCTS* is able to infer the correct process rewards by estimating the probability this step can help lead to the correct answer. These inferred rewards serve dual purposes: they act as value targets for further refining the process reward model and also facilitate the selection of high-quality traces for policy model self-training. We first show that the tree-search policy in ReST-MCTS* achieves higher accuracy compared with prior LLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same search budget. We then show that by using traces searched by this tree-search policy as training data, we can continuously enhance the three language models for multiple iterations, and outperform other self-training algorithms such as ReST$^\\text{EM}$ and Self-Rewarding LM.","sentences":["Recent methodologies in LLM self-training mostly rely on LLM generating responses and filtering those with correct output answers as training data.","This approach often yields a low-quality fine-tuning training set (e.g., incorrect plans or intermediate reasoning).","In this paper, we develop a reinforced self-training approach, called ReST-MCTS*, based on integrating process reward guidance with tree search MCTS* for collecting higher-quality reasoning traces as well as per-step value to train policy and reward models.","ReST-MCTS* circumvents the per-step manual annotation typically used to train process rewards by tree-search-based reinforcement learning:","Given oracle final correct answers, ReST-MCTS* is able to infer the correct process rewards by estimating the probability this step can help lead to the correct answer.","These inferred rewards serve dual purposes: they act as value targets for further refining the process reward model and also facilitate the selection of high-quality traces for policy model self-training.","We first show that the tree-search policy in ReST-MCTS* achieves higher accuracy compared with prior LLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same search budget.","We then show that by using traces searched by this tree-search policy as training data, we can continuously enhance the three language models for multiple iterations, and outperform other self-training algorithms such as ReST$^\\text{EM}$ and Self-Rewarding LM."],"url":"http://arxiv.org/abs/2406.03816v1"}
{"created":"2024-06-06 07:20:16","title":"Continual Counting with Gradual Privacy Expiration","abstract":"Differential privacy with gradual expiration models the setting where data items arrive in a stream and at a given time $t$ the privacy loss guaranteed for a data item seen at time $(t-d)$ is $\\epsilon g(d)$, where $g$ is a monotonically non-decreasing function. We study the fundamental $\\textit{continual (binary) counting}$ problem where each data item consists of a bit, and the algorithm needs to output at each time step the sum of all the bits streamed so far. For a stream of length $T$ and privacy $\\textit{without}$ expiration continual counting is possible with maximum (over all time steps) additive error $O(\\log^2(T)/\\varepsilon)$ and the best known lower bound is $\\Omega(\\log(T)/\\varepsilon)$; closing this gap is a challenging open problem.   We show that the situation is very different for privacy with gradual expiration by giving upper and lower bounds for a large set of expiration functions $g$. Specifically, our algorithm achieves an additive error of $ O(\\log(T)/\\epsilon)$ for a large set of privacy expiration functions. We also give a lower bound that shows that if $C$ is the additive error of any $\\epsilon$-DP algorithm for this problem, then the product of $C$ and the privacy expiration function after $2C$ steps must be $\\Omega(\\log(T)/\\epsilon)$. Our algorithm matches this lower bound as its additive error is $O(\\log(T)/\\epsilon)$, even when $g(2C) = O(1)$.   Our empirical evaluation shows that we achieve a slowly growing privacy loss with significantly smaller empirical privacy loss for large values of $d$ than a natural baseline algorithm.","sentences":["Differential privacy with gradual expiration models the setting where data items arrive in a stream and at a given time $t$ the privacy loss guaranteed for a data item seen at time $(t-d)$ is $\\epsilon g(d)$, where $g$ is a monotonically non-decreasing function.","We study the fundamental $\\textit{continual (binary) counting}$ problem where each data item consists of a bit, and the algorithm needs to output at each time step the sum of all the bits streamed so far.","For a stream of length $T$ and privacy $\\textit{without}$ expiration continual counting is possible with maximum (over all time steps) additive error $O(\\log^2(T)/\\varepsilon)$ and the best known lower bound is $\\Omega(\\log(T)/\\varepsilon)$; closing this gap is a challenging open problem.   ","We show that the situation is very different for privacy with gradual expiration by giving upper and lower bounds for a large set of expiration functions $g$. Specifically, our algorithm achieves an additive error of $ O(\\log(T)/\\epsilon)$ for a large set of privacy expiration functions.","We also give a lower bound that shows that if $C$ is the additive error of any $\\epsilon$-DP algorithm for this problem, then the product of $C$ and the privacy expiration function after $2C$ steps must be $\\Omega(\\log(T)/\\epsilon)$. Our algorithm matches this lower bound as its additive error is $O(\\log(T)/\\epsilon)$, even when $g(2C) = O(1)$.   Our empirical evaluation shows that we achieve a slowly growing privacy loss with significantly smaller empirical privacy loss for large values of $d$ than a natural baseline algorithm."],"url":"http://arxiv.org/abs/2406.03802v1"}
{"created":"2024-06-06 07:12:50","title":"Enhanced Semantic Segmentation Pipeline for WeatherProof Dataset Challenge","abstract":"This report describes the winning solution to the WeatherProof Dataset Challenge (CVPR 2024 UG2+ Track 3). Details regarding the challenge are available at https://cvpr2024ug2challenge.github.io/track3.html. We propose an enhanced semantic segmentation pipeline for this challenge. Firstly, we improve semantic segmentation models, using backbone pretrained with Depth Anything to improve UperNet model and SETRMLA model, and adding language guidance based on both weather and category information to InternImage model. Secondly, we introduce a new dataset WeatherProofExtra with wider viewing angle and employ data augmentation methods, including adverse weather and super-resolution. Finally, effective training strategies and ensemble method are applied to improve final performance further. Our solution is ranked 1st on the final leaderboard. Code will be available at https://github.com/KaneiGi/WeatherProofChallenge.","sentences":["This report describes the winning solution to the WeatherProof Dataset Challenge (CVPR 2024 UG2+ Track 3).","Details regarding the challenge are available at https://cvpr2024ug2challenge.github.io/track3.html.","We propose an enhanced semantic segmentation pipeline for this challenge.","Firstly, we improve semantic segmentation models, using backbone pretrained with Depth Anything to improve UperNet model and SETRMLA model, and adding language guidance based on both weather and category information to InternImage model.","Secondly, we introduce a new dataset WeatherProofExtra with wider viewing angle and employ data augmentation methods, including adverse weather and super-resolution.","Finally, effective training strategies and ensemble method are applied to improve final performance further.","Our solution is ranked 1st on the final leaderboard.","Code will be available at https://github.com/KaneiGi/WeatherProofChallenge."],"url":"http://arxiv.org/abs/2406.03799v1"}
{"created":"2024-06-06 07:05:20","title":"Low-Rank Similarity Mining for Multimodal Dataset Distillation","abstract":"Though dataset distillation has witnessed rapid development in recent years, the distillation of multimodal data, e.g., image-text pairs, poses unique and under-explored challenges. Unlike unimodal data, image-text contrastive learning (ITC) data lack inherent categorization and should instead place greater emphasis on modality correspondence. In this work, we propose Low-Rank Similarity Mining (LoRS) for multimodal dataset distillation, that concurrently distills a ground truth similarity matrix with image-text pairs, and leverages low-rank factorization for efficiency and scalability. The proposed approach brings significant improvement to the existing algorithms, marking a significant contribution to the field of visual-language dataset distillation. We advocate adopting LoRS as a foundational synthetic data setup for image-text dataset distillation. Our code is available at https://github.com/silicx/LoRS_Distill.","sentences":["Though dataset distillation has witnessed rapid development in recent years, the distillation of multimodal data, e.g., image-text pairs, poses unique and under-explored challenges.","Unlike unimodal data, image-text contrastive learning (ITC) data lack inherent categorization and should instead place greater emphasis on modality correspondence.","In this work, we propose Low-Rank Similarity Mining (LoRS) for multimodal dataset distillation, that concurrently distills a ground truth similarity matrix with image-text pairs, and leverages low-rank factorization for efficiency and scalability.","The proposed approach brings significant improvement to the existing algorithms, marking a significant contribution to the field of visual-language dataset distillation.","We advocate adopting LoRS as a foundational synthetic data setup for image-text dataset distillation.","Our code is available at https://github.com/silicx/LoRS_Distill."],"url":"http://arxiv.org/abs/2406.03793v1"}
{"created":"2024-06-06 07:01:50","title":"End-to-End Trainable Soft Retriever for Low-resource Relation Extraction","abstract":"This study addresses a crucial challenge in instance-based relation extraction using text generation models: end-to-end training in target relation extraction task is not applicable to retrievers due to the non-differentiable nature of instance selection. We propose a novel End-to-end TRAinable Soft K-nearest neighbor retriever (ETRASK) by the neural prompting method that utilizes a soft, differentiable selection of the $k$ nearest instances. This approach enables the end-to-end training of retrievers in target tasks. On the TACRED benchmark dataset with a low-resource setting where the training data was reduced to 10\\%, our method achieved a state-of-the-art F1 score of 71.5\\%. Moreover, ETRASK consistently improved the baseline model by adding instances for all settings. These results highlight the efficacy of our approach in enhancing relation extraction performance, especially in resource-constrained environments. Our findings offer a promising direction for future research with extraction and the broader application of text generation in natural language processing.","sentences":["This study addresses a crucial challenge in instance-based relation extraction using text generation models: end-to-end training in target relation extraction task is not applicable to retrievers due to the non-differentiable nature of instance selection.","We propose a novel End-to-end TRAinable Soft K-nearest neighbor retriever (ETRASK) by the neural prompting method that utilizes a soft, differentiable selection of the $k$ nearest instances.","This approach enables the end-to-end training of retrievers in target tasks.","On the TACRED benchmark dataset with a low-resource setting where the training data was reduced to 10\\%, our method achieved a state-of-the-art F1 score of 71.5\\%.","Moreover, ETRASK consistently improved the baseline model by adding instances for all settings.","These results highlight the efficacy of our approach in enhancing relation extraction performance, especially in resource-constrained environments.","Our findings offer a promising direction for future research with extraction and the broader application of text generation in natural language processing."],"url":"http://arxiv.org/abs/2406.03790v1"}
{"created":"2024-06-06 07:01:36","title":"Enhancing Graph U-Nets for Mesh-Agnostic Spatio-Temporal Flow Prediction","abstract":"This study aims to overcome the conventional deep-learning approaches based on convolutional neural networks, whose applicability to complex geometries and unstructured meshes is limited due to their inherent mesh dependency. We propose novel approaches to improve mesh-agnostic spatio-temporal prediction of transient flow fields using graph U-Nets, enabling accurate prediction on diverse mesh configurations. Key enhancements to the graph U-Net architecture, including the Gaussian mixture model convolutional operator and noise injection approaches, provide increased flexibility in modeling node dynamics: the former reduces prediction error by 95\\% compared to conventional convolutional operators, while the latter improves long-term prediction robustness, resulting in an error reduction of 86\\%. We also investigate transductive and inductive-learning perspectives of graph U-Nets with proposed improvements. In the transductive setting, they effectively predict quantities for unseen nodes within the trained graph. In the inductive setting, they successfully perform in mesh scenarios with different vortex-shedding periods, showing 98\\% improvement in predicting the future flow fields compared to a model trained without the inductive settings. It is found that graph U-Nets without pooling operations, i.e. without reducing and restoring the node dimensionality of the graph data, perform better in inductive settings due to their ability to learn from the detailed structure of each graph. Meanwhile, we also discover that the choice of normalization technique significantly impacts graph U-Net performance.","sentences":["This study aims to overcome the conventional deep-learning approaches based on convolutional neural networks, whose applicability to complex geometries and unstructured meshes is limited due to their inherent mesh dependency.","We propose novel approaches to improve mesh-agnostic spatio-temporal prediction of transient flow fields using graph U-Nets, enabling accurate prediction on diverse mesh configurations.","Key enhancements to the graph U-Net architecture, including the Gaussian mixture model convolutional operator and noise injection approaches, provide increased flexibility in modeling node dynamics: the former reduces prediction error by 95\\% compared to conventional convolutional operators, while the latter improves long-term prediction robustness, resulting in an error reduction of 86\\%.","We also investigate transductive and inductive-learning perspectives of graph U-Nets with proposed improvements.","In the transductive setting, they effectively predict quantities for unseen nodes within the trained graph.","In the inductive setting, they successfully perform in mesh scenarios with different vortex-shedding periods, showing 98\\% improvement in predicting the future flow fields compared to a model trained without the inductive settings.","It is found that graph U-Nets without pooling operations, i.e. without reducing and restoring the node dimensionality of the graph data, perform better in inductive settings due to their ability to learn from the detailed structure of each graph.","Meanwhile, we also discover that the choice of normalization technique significantly impacts graph U-Net performance."],"url":"http://arxiv.org/abs/2406.03789v1"}
{"created":"2024-06-06 06:43:41","title":"A Nearly Optimal Deterministic Algorithm for Online Transportation Problem","abstract":"We propose a new deterministic algorithm called Subtree-Decomposition for the online transportation problem and show that the algorithm is $(8m-5)$-competitive, where $m$ is the number of server sites.   It has long been known that the competitive ratio of any deterministic algorithm is lower bounded by $2m-1$ for this problem. On the other hand, the conjecture proposed by Kalyanasundaram and Pruhs in 1998 asking whether a deterministic $(2m-1)$-competitive algorithm exists for the online transportation problem has remained open for over two decades.   The upper bound on the competitive ratio, $8m-5$, which is the result of this paper, is the first to come close to this conjecture, and is the best possible within a constant factor.","sentences":["We propose a new deterministic algorithm called Subtree-Decomposition for the online transportation problem and show that the algorithm is $(8m-5)$-competitive, where $m$ is the number of server sites.   ","It has long been known that the competitive ratio of any deterministic algorithm is lower bounded by $2m-1$ for this problem.","On the other hand, the conjecture proposed by Kalyanasundaram and Pruhs in 1998 asking whether a deterministic $(2m-1)$-competitive algorithm exists for the online transportation problem has remained open for over two decades.   ","The upper bound on the competitive ratio, $8m-5$, which is the result of this paper, is the first to come close to this conjecture, and is the best possible within a constant factor."],"url":"http://arxiv.org/abs/2406.03778v1"}
{"created":"2024-06-06 06:41:53","title":"Empirical Guidelines for Deploying LLMs onto Resource-constrained Edge Devices","abstract":"The scaling laws have become the de facto guidelines for designing large language models (LLMs), but they were studied under the assumption of unlimited computing resources for both training and inference. As LLMs are increasingly used as personalized intelligent assistants, their customization (i.e., learning through fine-tuning) and deployment onto resource-constrained edge devices will become more and more prevalent. An urging but open question is how a resource-constrained computing environment would affect the design choices for a personalized LLM. We study this problem empirically in this work. In particular, we consider the tradeoffs among a number of key design factors and their intertwined impacts on learning efficiency and accuracy. The factors include the learning methods for LLM customization, the amount of personalized data used for learning customization, the types and sizes of LLMs, the compression methods of LLMs, the amount of time afforded to learn, and the difficulty levels of the target use cases. Through extensive experimentation and benchmarking, we draw a number of surprisingly insightful guidelines for deploying LLMs onto resource-constrained devices. For example, an optimal choice between parameter learning and RAG may vary depending on the difficulty of the downstream task, the longer fine-tuning time does not necessarily help the model, and a compressed LLM may be a better choice than an uncompressed LLM to learn from limited personalized data.","sentences":["The scaling laws have become the de facto guidelines for designing large language models (LLMs), but they were studied under the assumption of unlimited computing resources for both training and inference.","As LLMs are increasingly used as personalized intelligent assistants, their customization (i.e., learning through fine-tuning) and deployment onto resource-constrained edge devices will become more and more prevalent.","An urging but open question is how a resource-constrained computing environment would affect the design choices for a personalized LLM.","We study this problem empirically in this work.","In particular, we consider the tradeoffs among a number of key design factors and their intertwined impacts on learning efficiency and accuracy.","The factors include the learning methods for LLM customization, the amount of personalized data used for learning customization, the types and sizes of LLMs, the compression methods of LLMs, the amount of time afforded to learn, and the difficulty levels of the target use cases.","Through extensive experimentation and benchmarking, we draw a number of surprisingly insightful guidelines for deploying LLMs onto resource-constrained devices.","For example, an optimal choice between parameter learning and RAG may vary depending on the difficulty of the downstream task, the longer fine-tuning time does not necessarily help the model, and a compressed LLM may be a better choice than an uncompressed LLM to learn from limited personalized data."],"url":"http://arxiv.org/abs/2406.03777v1"}
{"created":"2024-06-06 06:23:42","title":"Optimizing Multi-User Semantic Communication via Transfer Learning and Knowledge Distillation","abstract":"Semantic communication, notable for ensuring quality of service by jointly optimizing source and channel coding, effectively extracts data semantics, reduces transmission length, and mitigates channel noise. However, most studies overlook multi-user scenarios and resource availability, limiting real-world application. This paper addresses this gap by focusing on downlink communication from a base station to multiple users with varying computing capacities. Users employ variants of Swin transformer models for source decoding and a simple architecture for channel decoding. We propose a novel training regimen, incorporating transfer learning and knowledge distillation to improve low-computing users' performance. Extensive simulations validate the proposed methods.","sentences":["Semantic communication, notable for ensuring quality of service by jointly optimizing source and channel coding, effectively extracts data semantics, reduces transmission length, and mitigates channel noise.","However, most studies overlook multi-user scenarios and resource availability, limiting real-world application.","This paper addresses this gap by focusing on downlink communication from a base station to multiple users with varying computing capacities.","Users employ variants of Swin transformer models for source decoding and a simple architecture for channel decoding.","We propose a novel training regimen, incorporating transfer learning and knowledge distillation to improve low-computing users' performance.","Extensive simulations validate the proposed methods."],"url":"http://arxiv.org/abs/2406.03773v1"}
{"created":"2024-06-06 05:57:38","title":"CORTEX: Large-Scale Brain Simulator Utilizing Indegree Sub-Graph Decomposition on Fugaku Supercomputer","abstract":"We introduce CORTEX, an algorithmic framework designed for large-scale brain simulation. Leveraging the computational capacity of the Fugaku Supercomputer, CORTEX maximizes available problem size and processing performance. Our primary innovation, Indegree Sub-Graph Decomposition, along with a suite of parallel algorithms, facilitates efficient domain decomposition by segmenting the global graph structure into smaller, identically structured sub-graphs. This segmentation allows for parallel processing of synaptic interactions without inter-process dependencies, effectively eliminating data racing at the thread level without necessitating mutexes or atomic operations. Additionally, this strategy enhances the overlap of communication and computation. Benchmark tests conducted on spiking neural networks, characterized by biological parameters, have demonstrated significant enhancements in both problem size and simulation performance, surpassing the capabilities of the current leading open-source solution, the NEST Simulator. Our work offers a powerful new tool for the field of neuromorphic computing and understanding brain function.","sentences":["We introduce CORTEX, an algorithmic framework designed for large-scale brain simulation.","Leveraging the computational capacity of the Fugaku Supercomputer, CORTEX maximizes available problem size and processing performance.","Our primary innovation, Indegree Sub-Graph Decomposition, along with a suite of parallel algorithms, facilitates efficient domain decomposition by segmenting the global graph structure into smaller, identically structured sub-graphs.","This segmentation allows for parallel processing of synaptic interactions without inter-process dependencies, effectively eliminating data racing at the thread level without necessitating mutexes or atomic operations.","Additionally, this strategy enhances the overlap of communication and computation.","Benchmark tests conducted on spiking neural networks, characterized by biological parameters, have demonstrated significant enhancements in both problem size and simulation performance, surpassing the capabilities of the current leading open-source solution, the NEST Simulator.","Our work offers a powerful new tool for the field of neuromorphic computing and understanding brain function."],"url":"http://arxiv.org/abs/2406.03762v1"}
{"created":"2024-06-06 05:30:42","title":"VisLTR: Visualization-in-the-Loop Table Reasoning","abstract":"Table reasoning transforms user requirements into corresponding answers according to the provided table, which is often integrated with natural language interfaces for lay users to explore tabular data effortlessly. Recent research exploits large language models to facilitate table reasoning, by transforming vague user requirements into structured query languages (SQLs). However, these SQL-based approaches often overlook changes in data patterns, suffer from LLM drift, and limit exploration to only text queries. To this end, VisLTR is designed as a visualization-in-the-loop table reasoning framework that leverages visualizations as a proxy to provide concise data representations, capture interesting data patterns, and support cross-modal analysis. We describe VisLTR as a process consisting of four major modules: 1) visualization alignment that utilizes large vision-language models to align visualizations across various modalities, including chart, text, and sketch; 2) visualization referencing that decomposes a table into multifaceted visualization references that comprehensively represent the table; 3) visualization pruning that incorporates data and retrieval pruning to excise visualization references with poor information and enhance retrieval efficiency; and 4) visualization interaction that offers an interactive visual interface with multi-modal interactions for user-friendly table reasoning. Quantitative evaluation demonstrates the effectiveness of the alignment model in cross-modal visualization pairings. We further demonstrate applications of the framework on various table reasoning tasks such as table summarization and pattern detection.","sentences":["Table reasoning transforms user requirements into corresponding answers according to the provided table, which is often integrated with natural language interfaces for lay users to explore tabular data effortlessly.","Recent research exploits large language models to facilitate table reasoning, by transforming vague user requirements into structured query languages (SQLs).","However, these SQL-based approaches often overlook changes in data patterns, suffer from LLM drift, and limit exploration to only text queries.","To this end, VisLTR is designed as a visualization-in-the-loop table reasoning framework that leverages visualizations as a proxy to provide concise data representations, capture interesting data patterns, and support cross-modal analysis.","We describe VisLTR as a process consisting of four major modules: 1) visualization alignment that utilizes large vision-language models to align visualizations across various modalities, including chart, text, and sketch; 2) visualization referencing that decomposes a table into multifaceted visualization references that comprehensively represent the table; 3) visualization pruning that incorporates data and retrieval pruning to excise visualization references with poor information and enhance retrieval efficiency; and 4) visualization interaction that offers an interactive visual interface with multi-modal interactions for user-friendly table reasoning.","Quantitative evaluation demonstrates the effectiveness of the alignment model in cross-modal visualization pairings.","We further demonstrate applications of the framework on various table reasoning tasks such as table summarization and pattern detection."],"url":"http://arxiv.org/abs/2406.03753v1"}
{"created":"2024-06-06 05:27:33","title":"Adaptive Multi-Scale Decomposition Framework for Time Series Forecasting","abstract":"Transformer-based and MLP-based methods have emerged as leading approaches in time series forecasting (TSF). While Transformer-based methods excel in capturing long-range dependencies, they suffer from high computational complexities and tend to overfit. Conversely, MLP-based methods offer computational efficiency and adeptness in modeling temporal dynamics, but they struggle with capturing complex temporal patterns effectively. To address these challenges, we propose a novel MLP-based Adaptive Multi-Scale Decomposition (AMD) framework for TSF. Our framework decomposes time series into distinct temporal patterns at multiple scales, leveraging the Multi-Scale Decomposable Mixing (MDM) block to dissect and aggregate these patterns in a residual manner. Complemented by the Dual Dependency Interaction (DDI) block and the Adaptive Multi-predictor Synthesis (AMS) block, our approach effectively models both temporal and channel dependencies and utilizes autocorrelation to refine multi-scale data integration. Comprehensive experiments demonstrate that our AMD framework not only overcomes the limitations of existing methods but also consistently achieves state-of-the-art performance in both long-term and short-term forecasting tasks across various datasets, showcasing superior efficiency. Code is available at \\url{https://github.com/TROUBADOUR000/AMD}","sentences":["Transformer-based and MLP-based methods have emerged as leading approaches in time series forecasting (TSF).","While Transformer-based methods excel in capturing long-range dependencies, they suffer from high computational complexities and tend to overfit.","Conversely, MLP-based methods offer computational efficiency and adeptness in modeling temporal dynamics, but they struggle with capturing complex temporal patterns effectively.","To address these challenges, we propose a novel MLP-based Adaptive Multi-Scale Decomposition (AMD) framework for TSF.","Our framework decomposes time series into distinct temporal patterns at multiple scales, leveraging the Multi-Scale Decomposable Mixing (MDM) block to dissect and aggregate these patterns in a residual manner.","Complemented by the Dual Dependency Interaction (DDI) block and the Adaptive Multi-predictor Synthesis (AMS) block, our approach effectively models both temporal and channel dependencies and utilizes autocorrelation to refine multi-scale data integration.","Comprehensive experiments demonstrate that our AMD framework not only overcomes the limitations of existing methods but also consistently achieves state-of-the-art performance in both long-term and short-term forecasting tasks across various datasets, showcasing superior efficiency.","Code is available at \\url{https://github.com/TROUBADOUR000/AMD}"],"url":"http://arxiv.org/abs/2406.03751v1"}
{"created":"2024-06-06 05:07:44","title":"NAP^2: A Benchmark for Naturalness and Privacy-Preserving Text Rewriting by Learning from Human","abstract":"Increasing concerns about privacy leakage issues in academia and industry arise when employing NLP models from third-party providers to process sensitive texts. To protect privacy before sending sensitive data to those models, we suggest sanitizing sensitive text using two common strategies used by humans: i) deleting sensitive expressions, and ii) obscuring sensitive details by abstracting them. To explore the issues and develop a tool for text rewriting, we curate the first corpus, coined NAP^2, through both crowdsourcing and the use of large language models (LLMs). Compared to the prior works based on differential privacy, which lead to a sharp drop in information utility and unnatural texts, the human-inspired approaches result in more natural rewrites and offer an improved balance between privacy protection and data utility, as demonstrated by our extensive experiments.","sentences":["Increasing concerns about privacy leakage issues in academia and industry arise when employing NLP models from third-party providers to process sensitive texts.","To protect privacy before sending sensitive data to those models, we suggest sanitizing sensitive text using two common strategies used by humans: i) deleting sensitive expressions, and ii) obscuring sensitive details by abstracting them.","To explore the issues and develop a tool for text rewriting, we curate the first corpus, coined NAP^2, through both crowdsourcing and the use of large language models (LLMs).","Compared to the prior works based on differential privacy, which lead to a sharp drop in information utility and unnatural texts, the human-inspired approaches result in more natural rewrites and offer an improved balance between privacy protection and data utility, as demonstrated by our extensive experiments."],"url":"http://arxiv.org/abs/2406.03749v1"}
{"created":"2024-06-06 04:22:11","title":"Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data","abstract":"Discrete diffusion models with absorbing processes have shown promise in language modeling. The key quantities to be estimated are the ratios between the marginal probabilities of two transitive states at all timesteps, called the concrete score. In this paper, we reveal that the concrete score in absorbing diffusion can be expressed as conditional probabilities of clean data, multiplied by a time-dependent scalar in an analytic form. Motivated by the finding, we propose reparameterized absorbing discrete diffusion (RADD), a dedicated diffusion model that characterizes the time-independent conditional probabilities. Besides its simplicity, RADD can reduce the number of function evaluations (NFEs) by caching the output of the time-independent network when the noisy sample remains unchanged in a sampling interval. Empirically, RADD is up to 3.5 times faster while consistently achieving a better performance than the strongest baseline. Built upon the new factorization of the concrete score, we further prove a surprising result that the exact likelihood of absorbing diffusion can be rewritten to a simple form (named denoising cross-entropy) and then estimated efficiently by the Monte Carlo method. The resulting approach also applies to the original parameterization of the concrete score. It significantly advances the state-of-the-art discrete diffusion on 5 zero-shot language modeling benchmarks (measured by perplexity) at the GPT-2 scale.","sentences":["Discrete diffusion models with absorbing processes have shown promise in language modeling.","The key quantities to be estimated are the ratios between the marginal probabilities of two transitive states at all timesteps, called the concrete score.","In this paper, we reveal that the concrete score in absorbing diffusion can be expressed as conditional probabilities of clean data, multiplied by a time-dependent scalar in an analytic form.","Motivated by the finding, we propose reparameterized absorbing discrete diffusion (RADD), a dedicated diffusion model that characterizes the time-independent conditional probabilities.","Besides its simplicity, RADD can reduce the number of function evaluations (NFEs) by caching the output of the time-independent network when the noisy sample remains unchanged in a sampling interval.","Empirically, RADD is up to 3.5 times faster while consistently achieving a better performance than the strongest baseline.","Built upon the new factorization of the concrete score, we further prove a surprising result that the exact likelihood of absorbing diffusion can be rewritten to a simple form (named denoising cross-entropy) and then estimated efficiently by the Monte Carlo method.","The resulting approach also applies to the original parameterization of the concrete score.","It significantly advances the state-of-the-art discrete diffusion on 5 zero-shot language modeling benchmarks (measured by perplexity) at the GPT-2 scale."],"url":"http://arxiv.org/abs/2406.03736v1"}
{"created":"2024-06-06 04:12:57","title":"Credit Card Fraud Detection Using Advanced Transformer Model","abstract":"With the proliferation of various online and mobile payment systems, credit card fraud has emerged as a significant threat to financial security. This study focuses on innovative applications of the latest Transformer models for more robust and precise fraud detection. To ensure the reliability of the data, we meticulously processed the data sources, balancing the dataset to address the issue of data sparsity significantly. We also selected highly correlated vectors to strengthen the training process.To guarantee the reliability and practicality of the new Transformer model, we conducted performance comparisons with several widely adopted models, including Support Vector Machine (SVM), Random Forest, Neural Network, and Logistic Regression. We rigorously compared these models using metrics such as Precision, Recall, and F1 Score. Through these detailed analyses and comparisons, we present to the readers a highly efficient and powerful anti-fraud mechanism with promising prospects. The results demonstrate that the Transformer model not only excels in traditional applications but also shows great potential in niche areas like fraud detection, offering a substantial advancement in the field.","sentences":["With the proliferation of various online and mobile payment systems, credit card fraud has emerged as a significant threat to financial security.","This study focuses on innovative applications of the latest Transformer models for more robust and precise fraud detection.","To ensure the reliability of the data, we meticulously processed the data sources, balancing the dataset to address the issue of data sparsity significantly.","We also selected highly correlated vectors to strengthen the training process.","To guarantee the reliability and practicality of the new Transformer model, we conducted performance comparisons with several widely adopted models, including Support Vector Machine (SVM), Random Forest, Neural Network, and Logistic Regression.","We rigorously compared these models using metrics such as Precision, Recall, and F1 Score.","Through these detailed analyses and comparisons, we present to the readers a highly efficient and powerful anti-fraud mechanism with promising prospects.","The results demonstrate that the Transformer model not only excels in traditional applications but also shows great potential in niche areas like fraud detection, offering a substantial advancement in the field."],"url":"http://arxiv.org/abs/2406.03733v1"}
{"created":"2024-06-06 04:05:54","title":"FastGAS: Fast Graph-based Annotation Selection for In-Context Learning","abstract":"In-context learning (ICL) empowers large language models (LLMs) to tackle new tasks by using a series of training instances as prompts. Since generating the prompts needs to sample from a vast pool of instances and annotate them (e.g., add labels in classification task), existing methods have proposed to select a subset of unlabeled examples for annotation, thus enhancing the quality of prompts and concurrently mitigating annotation costs. However, these methods often require a long time to select instances due to their complexity, hindering their practical viability. To address this limitation, we propose a graph-based selection method, FastGAS, designed to efficiently identify high-quality instances while minimizing computational overhead. Initially, we construct a data similarity graph based on instance similarities. Subsequently, employing a graph partitioning algorithm, we partition the graph into pieces. Within each piece (i.e., subgraph), we adopt a greedy approach to pick the most representative nodes. By aggregating nodes from diverse pieces and annotating the corresponding instances, we identify a set of diverse and representative instances for ICL. Compared to prior approaches, our method not only exhibits superior performance on different tasks but also significantly reduces selection time. In addition, we demonstrate the efficacy of our approach in LLMs of larger sizes.","sentences":["In-context learning (ICL) empowers large language models (LLMs) to tackle new tasks by using a series of training instances as prompts.","Since generating the prompts needs to sample from a vast pool of instances and annotate them (e.g., add labels in classification task), existing methods have proposed to select a subset of unlabeled examples for annotation, thus enhancing the quality of prompts and concurrently mitigating annotation costs.","However, these methods often require a long time to select instances due to their complexity, hindering their practical viability.","To address this limitation, we propose a graph-based selection method, FastGAS, designed to efficiently identify high-quality instances while minimizing computational overhead.","Initially, we construct a data similarity graph based on instance similarities.","Subsequently, employing a graph partitioning algorithm, we partition the graph into pieces.","Within each piece (i.e., subgraph), we adopt a greedy approach to pick the most representative nodes.","By aggregating nodes from diverse pieces and annotating the corresponding instances, we identify a set of diverse and representative instances for ICL.","Compared to prior approaches, our method not only exhibits superior performance on different tasks but also significantly reduces selection time.","In addition, we demonstrate the efficacy of our approach in LLMs of larger sizes."],"url":"http://arxiv.org/abs/2406.03730v1"}
{"created":"2024-06-06 03:49:34","title":"Efficient Graph Encoder Embedding for Large Sparse Graphs in Python","abstract":"Graph is a ubiquitous representation of data in various research fields, and graph embedding is a prevalent machine learning technique for capturing key features and generating fixed-sized attributes. However, most state-of-the-art graph embedding methods are computationally and spatially expensive. Recently, the Graph Encoder Embedding (GEE) has been shown as the fastest graph embedding technique and is suitable for a variety of network data applications. As real-world data often involves large and sparse graphs, the huge sparsity usually results in redundant computations and storage. To address this issue, we propose an improved version of GEE, sparse GEE, which optimizes the calculation and storage of zero entries in sparse matrices to enhance the running time further. Our experiments demonstrate that the sparse version achieves significant speedup compared to the original GEE with Python implementation for large sparse graphs, and sparse GEE is capable of processing millions of edges within minutes on a standard laptop.","sentences":["Graph is a ubiquitous representation of data in various research fields, and graph embedding is a prevalent machine learning technique for capturing key features and generating fixed-sized attributes.","However, most state-of-the-art graph embedding methods are computationally and spatially expensive.","Recently, the Graph Encoder Embedding (GEE) has been shown as the fastest graph embedding technique and is suitable for a variety of network data applications.","As real-world data often involves large and sparse graphs, the huge sparsity usually results in redundant computations and storage.","To address this issue, we propose an improved version of GEE, sparse GEE, which optimizes the calculation and storage of zero entries in sparse matrices to enhance the running time further.","Our experiments demonstrate that the sparse version achieves significant speedup compared to the original GEE with Python implementation for large sparse graphs, and sparse GEE is capable of processing millions of edges within minutes on a standard laptop."],"url":"http://arxiv.org/abs/2406.03726v1"}
