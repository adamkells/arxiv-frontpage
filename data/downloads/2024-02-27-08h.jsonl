{"created":"2024-02-26 11:31:48","title":"LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments","abstract":"Recent advancements in large language models (LLMs) have revealed their potential for achieving autonomous agents possessing human-level intelligence. However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions. There is a lack of a benchmark that evaluates the diverse capabilities of LLM agents in multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel and easily extensible framework for evaluating the diverse capabilities of LLM in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming environments, employing Trueskill scoring to assess crucial abilities in LLM agents, including spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. We conduct an extensive experiment and human evaluation among different sizes and types of LLMs, showing that LLMs still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration. We hope LLMArena could guide future research towards enhancing these capabilities in LLMs, ultimately leading to more sophisticated and practical applications in dynamic, multi-agent settings. The code and data will be available.","sentences":["Recent advancements in large language models (LLMs) have revealed their potential for achieving autonomous agents possessing human-level intelligence.","However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions.","There is a lack of a benchmark that evaluates the diverse capabilities of LLM agents in multi-agent, dynamic environments.","To this end, we introduce LLMArena, a novel and easily extensible framework for evaluating the diverse capabilities of LLM in multi-agent dynamic environments.","LLMArena encompasses seven distinct gaming environments, employing Trueskill scoring to assess crucial abilities in LLM agents, including spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration.","We conduct an extensive experiment and human evaluation among different sizes and types of LLMs, showing that LLMs still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration.","We hope LLMArena could guide future research towards enhancing these capabilities in LLMs, ultimately leading to more sophisticated and practical applications in dynamic, multi-agent settings.","The code and data will be available."],"url":"http://arxiv.org/abs/2402.16499v1"}
{"created":"2024-02-26 11:08:26","title":"Intelligent Known and Novel Aircraft Recognition -- A Shift from Classification to Similarity Learning for Combat Identification","abstract":"Precise aircraft recognition in low-resolution remote sensing imagery is a challenging yet crucial task in aviation, especially combat identification. This research addresses this problem with a novel, scalable, and AI-driven solution. The primary hurdle in combat identification in remote sensing imagery is the accurate recognition of Novel/Unknown types of aircraft in addition to Known types. Traditional methods, human expert-driven combat identification and image classification, fall short in identifying Novel classes. Our methodology employs similarity learning to discern features of a broad spectrum of military and civilian aircraft. It discerns both Known and Novel aircraft types, leveraging metric learning for the identification and supervised few-shot learning for aircraft type classification. To counter the challenge of limited low-resolution remote sensing data, we propose an end-to-end framework that adapts to the diverse and versatile process of military aircraft recognition by training a generalized embedder in fully supervised manner. Comparative analysis with earlier aircraft image classification methods shows that our approach is effective for aircraft image classification (F1-score Aircraft Type of 0.861) and pioneering for quantifying the identification of Novel types (F1-score Bipartitioning of 0.936). The proposed methodology effectively addresses inherent challenges in remote sensing data, thereby setting new standards in dataset quality. The research opens new avenues for domain experts and demonstrates unique capabilities in distinguishing various aircraft types, contributing to a more robust, domain-adapted potential for real-time aircraft recognition.","sentences":["Precise aircraft recognition in low-resolution remote sensing imagery is a challenging yet crucial task in aviation, especially combat identification.","This research addresses this problem with a novel, scalable, and AI-driven solution.","The primary hurdle in combat identification in remote sensing imagery is the accurate recognition of Novel/Unknown types of aircraft in addition to Known types.","Traditional methods, human expert-driven combat identification and image classification, fall short in identifying Novel classes.","Our methodology employs similarity learning to discern features of a broad spectrum of military and civilian aircraft.","It discerns both Known and Novel aircraft types, leveraging metric learning for the identification and supervised few-shot learning for aircraft type classification.","To counter the challenge of limited low-resolution remote sensing data, we propose an end-to-end framework that adapts to the diverse and versatile process of military aircraft recognition by training a generalized embedder in fully supervised manner.","Comparative analysis with earlier aircraft image classification methods shows that our approach is effective for aircraft image classification (F1-score Aircraft Type of 0.861) and pioneering for quantifying the identification of Novel types (F1-score Bipartitioning of 0.936).","The proposed methodology effectively addresses inherent challenges in remote sensing data, thereby setting new standards in dataset quality.","The research opens new avenues for domain experts and demonstrates unique capabilities in distinguishing various aircraft types, contributing to a more robust, domain-adapted potential for real-time aircraft recognition."],"url":"http://arxiv.org/abs/2402.16486v1"}
{"created":"2024-02-26 10:33:36","title":"mEdIT: Multilingual Text Editing via Instruction Tuning","abstract":"We introduce mEdIT, a multi-lingual extension to CoEdIT -- the recent state-of-the-art text editing models for writing assistance. mEdIT models are trained by fine-tuning multi-lingual large, pre-trained language models (LLMs) via instruction tuning. They are designed to take instructions from the user specifying the attributes of the desired text in the form of natural language instructions, such as Grammatik korrigieren (German) or Parafrasee la oraci\\'on (Spanish). We build mEdIT by curating data from multiple publicly available human-annotated text editing datasets for three text editing tasks (Grammatical Error Correction (GEC), Text Simplification, and Paraphrasing) across diverse languages belonging to six different language families. We detail the design and training of mEdIT models and demonstrate their strong performance on many multi-lingual text editing benchmarks against other multilingual LLMs. We also find that mEdIT generalizes effectively to new languages over multilingual baselines. We publicly release our data, code, and trained models at https://github.com/vipulraheja/medit.","sentences":["We introduce mEdIT, a multi-lingual extension to CoEdIT -- the recent state-of-the-art text editing models for writing assistance.","mEdIT models are trained by fine-tuning multi-lingual large, pre-trained language models (LLMs) via instruction tuning.","They are designed to take instructions from the user specifying the attributes of the desired text in the form of natural language instructions, such as Grammatik korrigieren (German) or Parafrasee la oraci\\'on (Spanish).","We build mEdIT by curating data from multiple publicly available human-annotated text editing datasets for three text editing tasks (Grammatical Error Correction (GEC), Text Simplification, and Paraphrasing) across diverse languages belonging to six different language families.","We detail the design and training of mEdIT models and demonstrate their strong performance on many multi-lingual text editing benchmarks against other multilingual LLMs.","We also find that mEdIT generalizes effectively to new languages over multilingual baselines.","We publicly release our data, code, and trained models at https://github.com/vipulraheja/medit."],"url":"http://arxiv.org/abs/2402.16472v1"}
{"created":"2024-02-26 10:29:35","title":"A fast implementation of the good-suffix array for the Boyer-Moore string matching algorithm","abstract":"String matching is the problem of finding all the occurrences of a pattern in a text. It has been intensively studied and the Boyer-Moore string matching algorithm is probably one of the most famous solution to this problem. This algorithm uses two precomputed shift tables called the good-suffix table and the bad-character table. The good-suffix table is tricky to compute in linear time. Text book solutions perform redundant operations. Here we present a fast implementation for this good-suffix table based on a tight analysis of the pattern. Experimental results show two versions of this new implementation are the fastest in almost all tested situations.","sentences":["String matching is the problem of finding all the occurrences of a pattern in a text.","It has been intensively studied and the Boyer-Moore string matching algorithm is probably one of the most famous solution to this problem.","This algorithm uses two precomputed shift tables called the good-suffix table and the bad-character table.","The good-suffix table is tricky to compute in linear time.","Text book solutions perform redundant operations.","Here we present a fast implementation for this good-suffix table based on a tight analysis of the pattern.","Experimental results show two versions of this new implementation are the fastest in almost all tested situations."],"url":"http://arxiv.org/abs/2402.16469v1"}
{"created":"2024-02-26 10:17:28","title":"Parameterized and approximation algorithms for coverings points with segments in the plane","abstract":"We study parameterized and approximation algorithms for a variant of Set Cover, where the universe of elements to be covered consists of points in the plane and the sets with which the points should be covered are segments. We call this problem Segment Set Cover. We also consider a relaxation of the problem called $\\delta$-extension, where we need to cover the points by segments that are extended by a tiny fraction, but we compare the solution's quality to the optimum without extension.   For the unparameterized variant, we prove that Segment Set Cover does not admit a PTAS unless $\\mathsf{P}=\\mathsf{NP}$, even if we restrict segments to be axis-parallel and allow $\\frac{1}{2}$-extension. On the other hand, we show that parameterization helps for the tractability of Segment Set Cover: we give an FPT algorithm for unweighted Segment Set Cover parameterized by the solution size $k$, a parameterized approximation scheme for Weighted Segment Set Cover with $k$ being the parameter, and an FPT algorithm for Weighted Segment Set Cover with $\\delta$-extension parameterized by $k$ and $\\delta$. In the last two results, relaxing the problem is probably necessary: we prove that Weighted Segment Set Cover without any relaxation is $\\mathsf{W}[1]$-hard and, assuming ETH, there does not exist an algorithm running in time $f(k)\\cdot n^{o(k / \\log k)}$. This holds even if one restricts attention to axis-parallel segments.","sentences":["We study parameterized and approximation algorithms for a variant of Set Cover, where the universe of elements to be covered consists of points in the plane and the sets with which the points should be covered are segments.","We call this problem Segment Set Cover.","We also consider a relaxation of the problem called $\\delta$-extension, where we need to cover the points by segments that are extended by a tiny fraction, but we compare the solution's quality to the optimum without extension.   ","For the unparameterized variant, we prove that Segment Set Cover does not admit a PTAS unless $\\mathsf{P}=\\mathsf{NP}$, even if we restrict segments to be axis-parallel and allow $\\frac{1}{2}$-extension.","On the other hand, we show that parameterization helps for the tractability of Segment Set Cover: we give an FPT algorithm for unweighted Segment Set Cover parameterized by the solution size $k$, a parameterized approximation scheme for Weighted Segment Set Cover with $k$ being the parameter, and an FPT algorithm for Weighted Segment Set Cover with $\\delta$-extension parameterized by $k$ and $\\delta$.","In the last two results, relaxing the problem is probably necessary: we prove that Weighted Segment Set Cover without any relaxation is $\\mathsf{W}[1]$-hard and, assuming ETH, there does not exist an algorithm running in time $f(k)\\cdot n^{o(k / \\log k)}$.","This holds even if one restricts attention to axis-parallel segments."],"url":"http://arxiv.org/abs/2402.16466v1"}
{"created":"2024-02-26 10:02:29","title":"D-XCB: Data-independent Debiasing for Fair and Accurate Transformer-based Cyberbullying Detection","abstract":"Swear words are a common proxy to collect datasets with cyberbullying incidents. Our focus is on measuring and mitigating biases derived from spurious associations between swear words and incidents occurring as a result of such data collection strategies. After demonstrating and quantifying these biases, we introduce ID-XCB, the first data-independent debiasing technique that combines adversarial training, bias constraints and debias fine-tuning approach aimed at alleviating model attention to bias-inducing words without impacting overall model performance. We explore ID-XCB on two popular session-based cyberbullying datasets along with comprehensive ablation and generalisation studies. We show that ID-XCB learns robust cyberbullying detection capabilities while mitigating biases, outperforming state-of-the-art debiasing methods in both performance and bias mitigation. Our quantitative and qualitative analyses demonstrate its generalisability to unseen data.","sentences":["Swear words are a common proxy to collect datasets with cyberbullying incidents.","Our focus is on measuring and mitigating biases derived from spurious associations between swear words and incidents occurring as a result of such data collection strategies.","After demonstrating and quantifying these biases, we introduce ID-XCB, the first data-independent debiasing technique that combines adversarial training, bias constraints and debias fine-tuning approach aimed at alleviating model attention to bias-inducing words without impacting overall model performance.","We explore ID-XCB on two popular session-based cyberbullying datasets along with comprehensive ablation and generalisation studies.","We show that ID-XCB learns robust cyberbullying detection capabilities while mitigating biases, outperforming state-of-the-art debiasing methods in both performance and bias mitigation.","Our quantitative and qualitative analyses demonstrate its generalisability to unseen data."],"url":"http://arxiv.org/abs/2402.16458v1"}
{"created":"2024-02-26 09:57:25","title":"Autonomous Integration of TSN-unaware Applications with QoS Requirements in TSN Networks","abstract":"Modern industrial networks transport both best-effort and real-time traffic. Time-Sensitive Networking (TSN) was introduced by the IEEE TSN Task Group as an enhancement to Ethernet to provide high quality of service (QoS) for real-time traffic. In a TSN network, applications signal their QoS requirements to the network before transmitting data. The network then allocates resources to meet these requirements. However, TSN-unaware applications can neither perform this registration process nor profit from TSN's QoS benefits. The contributions of this paper are twofold. First, we introduce a novel network architecture in which an additional device autonomously signals the QoS requirements of TSN-unaware applications to the network. Second, we propose a processing method to detect real-time streams in a network and extract the necessary information for the TSN stream signaling. It leverages a Deep Recurrent Neural Network (DRNN) to detect periodic traffic, extracts an accurate traffic description, and uses traffic classification to determine the source application. As a result, our proposal allows TSN-unaware applications to benefit from TSNs QoS guarantees. Our evaluations underline the effectiveness of the proposed architecture and processing method.","sentences":["Modern industrial networks transport both best-effort and real-time traffic.","Time-Sensitive Networking (TSN) was introduced by the IEEE TSN Task Group as an enhancement to Ethernet to provide high quality of service (QoS) for real-time traffic.","In a TSN network, applications signal their QoS requirements to the network before transmitting data.","The network then allocates resources to meet these requirements.","However, TSN-unaware applications can neither perform this registration process nor profit from TSN's QoS benefits.","The contributions of this paper are twofold.","First, we introduce a novel network architecture in which an additional device autonomously signals the QoS requirements of TSN-unaware applications to the network.","Second, we propose a processing method to detect real-time streams in a network and extract the necessary information for the TSN stream signaling.","It leverages a Deep Recurrent Neural Network (DRNN) to detect periodic traffic, extracts an accurate traffic description, and uses traffic classification to determine the source application.","As a result, our proposal allows TSN-unaware applications to benefit from TSNs QoS guarantees.","Our evaluations underline the effectiveness of the proposed architecture and processing method."],"url":"http://arxiv.org/abs/2402.16454v1"}
{"created":"2024-02-26 09:53:20","title":"WetLinks: a Large-Scale Longitudinal Starlink Dataset with Contiguous Weather Data","abstract":"Low Orbit Satellite (LEO) networks such as Starlink promise Internet access everywhere around the world. In this paper, we present WetLinks - a large and publicly available trace-based dataset of Starlink measurements. The measurements were concurrently collected from two European vantage points over a span of six months. Consisting of approximately 140,000 measurements, the dataset comprises all relevant network parameters such as the upload and download throughputs, the RTT, packet loss, and traceroutes. We further augment the dataset with concurrent data from professional weather stations placed next to both Starlink terminals. Based on our dataset, we analyse Starlink performance, including its susceptibility to weather conditions. We use this to validate our dataset by replicating the results of earlier smaller-scale studies. We release our datasets and all accompanying tooling as open data. To the best of our knowledge, ours is the largest Starlink dataset to date.","sentences":["Low Orbit Satellite (LEO) networks such as Starlink promise Internet access everywhere around the world.","In this paper, we present WetLinks - a large and publicly available trace-based dataset of Starlink measurements.","The measurements were concurrently collected from two European vantage points over a span of six months.","Consisting of approximately 140,000 measurements, the dataset comprises all relevant network parameters such as the upload and download throughputs, the RTT, packet loss, and traceroutes.","We further augment the dataset with concurrent data from professional weather stations placed next to both Starlink terminals.","Based on our dataset, we analyse Starlink performance, including its susceptibility to weather conditions.","We use this to validate our dataset by replicating the results of earlier smaller-scale studies.","We release our datasets and all accompanying tooling as open data.","To the best of our knowledge, ours is the largest Starlink dataset to date."],"url":"http://arxiv.org/abs/2402.16448v1"}
{"created":"2024-02-26 09:37:24","title":"Retrouver l'inventeur-auteur : la lev{\u00e9}e d'homonymies d'autorat entre les brevets et les publications scientifiques","abstract":"Patents and scientific papers provide an essential source for measuring science and technology output, to be used as a basis for the most varied scientometric analyzes. Authors' and inventors' names are the key identifiers to carry out these analyses, which however, run up against the issue of disambiguation. By extension identifying inventors who are also academic authors is a non-trivial challenge. We propose a method using the International Patent Classification (IPC) and the IPCCAT API to assess the degree of similarity of patents and papers abstracts of a given inventor, in order to match both types of documents. The method is developed and manually qualified based on three corpora of patents extracted from the international EPO database Espacenet. Among a set of 4679 patents and 7720 inventors, we obtain 2501 authors. The proposed algorithm solves the general problem of disambiguation with an error rate lower than 5%.","sentences":["Patents and scientific papers provide an essential source for measuring science and technology output, to be used as a basis for the most varied scientometric analyzes.","Authors' and inventors' names are the key identifiers to carry out these analyses, which however, run up against the issue of disambiguation.","By extension identifying inventors who are also academic authors is a non-trivial challenge.","We propose a method using the International Patent Classification (IPC) and the IPCCAT API to assess the degree of similarity of patents and papers abstracts of a given inventor, in order to match both types of documents.","The method is developed and manually qualified based on three corpora of patents extracted from the international EPO database Espacenet.","Among a set of 4679 patents and 7720 inventors, we obtain 2501 authors.","The proposed algorithm solves the general problem of disambiguation with an error rate lower than 5%."],"url":"http://arxiv.org/abs/2402.16440v1"}
{"created":"2024-02-26 09:32:28","title":"Training Implicit Generative Models via an Invariant Statistical Loss","abstract":"Implicit generative models have the capability to learn arbitrary complex data distributions. On the downside, training requires telling apart real data from artificially-generated ones using adversarial discriminators, leading to unstable training and mode-dropping issues. As reported by Zahee et al. (2017), even in the one-dimensional (1D) case, training a generative adversarial network (GAN) is challenging and often suboptimal. In this work, we develop a discriminator-free method for training one-dimensional (1D) generative implicit models and subsequently expand this method to accommodate multivariate cases. Our loss function is a discrepancy measure between a suitably chosen transformation of the model samples and a uniform distribution; hence, it is invariant with respect to the true distribution of the data. We first formulate our method for 1D random variables, providing an effective solution for approximate reparameterization of arbitrary complex distributions. Then, we consider the temporal setting (both univariate and multivariate), in which we model the conditional distribution of each sample given the history of the process. We demonstrate through numerical simulations that this new method yields promising results, successfully learning true distributions in a variety of scenarios and mitigating some of the well-known problems that state-of-the-art implicit methods present.","sentences":["Implicit generative models have the capability to learn arbitrary complex data distributions.","On the downside, training requires telling apart real data from artificially-generated ones using adversarial discriminators, leading to unstable training and mode-dropping issues.","As reported by Zahee et al. (2017), even in the one-dimensional (1D) case, training a generative adversarial network (GAN) is challenging and often suboptimal.","In this work, we develop a discriminator-free method for training one-dimensional (1D) generative implicit models and subsequently expand this method to accommodate multivariate cases.","Our loss function is a discrepancy measure between a suitably chosen transformation of the model samples and a uniform distribution; hence, it is invariant with respect to the true distribution of the data.","We first formulate our method for 1D random variables, providing an effective solution for approximate reparameterization of arbitrary complex distributions.","Then, we consider the temporal setting (both univariate and multivariate), in which we model the conditional distribution of each sample given the history of the process.","We demonstrate through numerical simulations that this new method yields promising results, successfully learning true distributions in a variety of scenarios and mitigating some of the well-known problems that state-of-the-art implicit methods present."],"url":"http://arxiv.org/abs/2402.16435v1"}
{"created":"2024-02-26 09:31:46","title":"Optimization of the Downlink Spectral- and Energy-Efficiency of RIS-aided Multi-user URLLC MIMO Systems","abstract":"Modern wireless communication systems are expected to provide improved latency and reliability. To meet these expectations, a short packet length is needed, which makes the first-order Shannon rate an inaccurate performance metric for such communication systems. A more accurate approximation of the achievable rates of finite-block-length (FBL) coding regimes is known as the normal approximation (NA). It is therefore of substantial interest to study the optimization of the FBL rate in multi-user multiple-input multiple-output (MIMO) systems, in which each user may transmit and/or receive multiple data streams. Hence, we formulate a general optimization problem for improving the spectral and energy efficiency of multi-user MIMO-aided ultra-reliable low-latency communication (URLLC) systems, which are assisted by reconfigurable intelligent surfaces (RISs). We show that a RIS is capable of substantially improving the performance of multi-user MIMO-aided URLLC systems. Moreover, the benefits of RIS increase as the packet length and/or the tolerable bit error rate are reduced. This reveals that RISs can be even more beneficial in URLLC systems for improving the FBL rates than in conventional systems approaching Shannon rates.","sentences":["Modern wireless communication systems are expected to provide improved latency and reliability.","To meet these expectations, a short packet length is needed, which makes the first-order Shannon rate an inaccurate performance metric for such communication systems.","A more accurate approximation of the achievable rates of finite-block-length (FBL) coding regimes is known as the normal approximation (NA).","It is therefore of substantial interest to study the optimization of the FBL rate in multi-user multiple-input multiple-output (MIMO) systems, in which each user may transmit and/or receive multiple data streams.","Hence, we formulate a general optimization problem for improving the spectral and energy efficiency of multi-user MIMO-aided ultra-reliable low-latency communication (URLLC) systems, which are assisted by reconfigurable intelligent surfaces (RISs).","We show that a RIS is capable of substantially improving the performance of multi-user MIMO-aided URLLC systems.","Moreover, the benefits of RIS increase as the packet length and/or the tolerable bit error rate are reduced.","This reveals that RISs can be even more beneficial in URLLC systems for improving the FBL rates than in conventional systems approaching Shannon rates."],"url":"http://arxiv.org/abs/2402.16434v1"}
{"created":"2024-02-26 09:19:46","title":"Predicting Sustainable Development Goals Using Course Descriptions -- from LLMs to Conventional Foundation Models","abstract":"We present our work on predicting United Nations sustainable development goals (SDG) for university courses. We use an LLM named PaLM 2 to generate training data given a noisy human-authored course description input as input. We use this data to train several different smaller language models to predict SDGs for university courses. This work contributes to better university level adaptation of SDGs. The best performing model in our experiments was BART with an F1-score of 0.786.","sentences":["We present our work on predicting United Nations sustainable development goals (SDG) for university courses.","We use an LLM named PaLM 2 to generate training data given a noisy human-authored course description input as input.","We use this data to train several different smaller language models to predict SDGs for university courses.","This work contributes to better university level adaptation of SDGs.","The best performing model in our experiments was BART with an F1-score of 0.786."],"url":"http://arxiv.org/abs/2402.16420v1"}
{"created":"2024-02-26 09:11:12","title":"TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis","abstract":"The field of general time series analysis has recently begun to explore unified modeling, where a common architectural backbone can be retrained on a specific task for a specific dataset. In this work, we approach unification from a complementary vantage point: unification across tasks and domains. To this end, we explore the impact of discrete, learnt, time series data representations that enable generalist, cross-domain training. Our method, TOTEM, or TOkenized Time Series EMbeddings, proposes a simple tokenizer architecture that embeds time series data from varying domains using a discrete vectorized representation learned in a self-supervised manner. TOTEM works across multiple tasks and domains with minimal to no tuning. We study the efficacy of TOTEM with an extensive evaluation on 17 real world time series datasets across 3 tasks. We evaluate both the specialist (i.e., training a model on each domain) and generalist (i.e., training a single model on many domains) settings, and show that TOTEM matches or outperforms previous best methods on several popular benchmarks. The code can be found at: https://github.com/SaberaTalukder/TOTEM.","sentences":["The field of general time series analysis has recently begun to explore unified modeling, where a common architectural backbone can be retrained on a specific task for a specific dataset.","In this work, we approach unification from a complementary vantage point: unification across tasks and domains.","To this end, we explore the impact of discrete, learnt, time series data representations that enable generalist, cross-domain training.","Our method, TOTEM, or TOkenized Time Series EMbeddings, proposes a simple tokenizer architecture that embeds time series data from varying domains using a discrete vectorized representation learned in a self-supervised manner.","TOTEM works across multiple tasks and domains with minimal to no tuning.","We study the efficacy of TOTEM with an extensive evaluation on 17 real world time series datasets across 3 tasks.","We evaluate both the specialist (i.e., training a model on each domain) and generalist (i.e., training a single model on many domains) settings, and show that TOTEM matches or outperforms previous best methods on several popular benchmarks.","The code can be found at: https://github.com/SaberaTalukder/TOTEM."],"url":"http://arxiv.org/abs/2402.16412v1"}
{"created":"2024-02-26 08:55:10","title":"Graph Learning with Distributional Edge Layouts","abstract":"Graph Neural Networks (GNNs) learn from graph-structured data by passing local messages between neighboring nodes along edges on certain topological layouts. Typically, these topological layouts in modern GNNs are deterministically computed (e.g., attention-based GNNs) or locally sampled (e.g., GraphSage) under heuristic assumptions. In this paper, we for the first time pose that these layouts can be globally sampled via Langevin dynamics following Boltzmann distribution equipped with explicit physical energy, leading to higher feasibility in the physical world. We argue that such a collection of sampled/optimized layouts can capture the wide energy distribution and bring extra expressivity on top of WL-test, therefore easing downstream tasks. As such, we propose Distributional Edge Layouts (DELs) to serve as a complement to a variety of GNNs. DEL is a pre-processing strategy independent of subsequent GNN variants, thus being highly flexible. Experimental results demonstrate that DELs consistently and substantially improve a series of GNN baselines, achieving state-of-the-art performance on multiple datasets.","sentences":["Graph Neural Networks (GNNs) learn from graph-structured data by passing local messages between neighboring nodes along edges on certain topological layouts.","Typically, these topological layouts in modern GNNs are deterministically computed (e.g., attention-based GNNs) or locally sampled (e.g., GraphSage) under heuristic assumptions.","In this paper, we for the first time pose that these layouts can be globally sampled via Langevin dynamics following Boltzmann distribution equipped with explicit physical energy, leading to higher feasibility in the physical world.","We argue that such a collection of sampled/optimized layouts can capture the wide energy distribution and bring extra expressivity on top of WL-test, therefore easing downstream tasks.","As such, we propose Distributional Edge Layouts (DELs) to serve as a complement to a variety of GNNs.","DEL is a pre-processing strategy independent of subsequent GNN variants, thus being highly flexible.","Experimental results demonstrate that DELs consistently and substantially improve a series of GNN baselines, achieving state-of-the-art performance on multiple datasets."],"url":"http://arxiv.org/abs/2402.16402v1"}
{"created":"2024-02-26 08:49:17","title":"Analysis of Embeddings Learned by End-to-End Machine Learning Eye Movement-driven Biometrics Pipeline","abstract":"This paper expands on the foundational concept of temporal persistence in biometric systems, specifically focusing on the domain of eye movement biometrics facilitated by machine learning. Unlike previous studies that primarily focused on developing biometric authentication systems, our research delves into the embeddings learned by these systems, particularly examining their temporal persistence, reliability, and biometric efficacy in response to varying input data. Utilizing two publicly available eye-movement datasets, we employed the state-of-the-art Eye Know You Too machine learning pipeline for our analysis. We aim to validate whether the machine learning-derived embeddings in eye movement biometrics mirror the temporal persistence observed in traditional biometrics. Our methodology involved conducting extensive experiments to assess how different lengths and qualities of input data influence the performance of eye movement biometrics more specifically how it impacts the learned embeddings. We also explored the reliability and consistency of the embeddings under varying data conditions. Three key metrics (kendall's coefficient of concordance, intercorrelations, and equal error rate) were employed to quantitatively evaluate our findings. The results reveal while data length significantly impacts the stability of the learned embeddings, however, the intercorrelations among embeddings show minimal effect.","sentences":["This paper expands on the foundational concept of temporal persistence in biometric systems, specifically focusing on the domain of eye movement biometrics facilitated by machine learning.","Unlike previous studies that primarily focused on developing biometric authentication systems, our research delves into the embeddings learned by these systems, particularly examining their temporal persistence, reliability, and biometric efficacy in response to varying input data.","Utilizing two publicly available eye-movement datasets, we employed the state-of-the-art Eye Know You Too machine learning pipeline for our analysis.","We aim to validate whether the machine learning-derived embeddings in eye movement biometrics mirror the temporal persistence observed in traditional biometrics.","Our methodology involved conducting extensive experiments to assess how different lengths and qualities of input data influence the performance of eye movement biometrics more specifically how it impacts the learned embeddings.","We also explored the reliability and consistency of the embeddings under varying data conditions.","Three key metrics (kendall's coefficient of concordance, intercorrelations, and equal error rate) were employed to quantitatively evaluate our findings.","The results reveal while data length significantly impacts the stability of the learned embeddings, however, the intercorrelations among embeddings show minimal effect."],"url":"http://arxiv.org/abs/2402.16399v1"}
{"created":"2024-02-26 08:47:35","title":"Efficient Continuous-Time Ego-Motion Estimation for Asynchronous Event-based Data Associations","abstract":"Event cameras are bio-inspired vision sensors that asynchronously measure per-pixel brightness changes. The high temporal resolution and asynchronicity of event cameras offer great potential for estimating the robot motion state. Recent works have adopted the continuous-time ego-motion estimation methods to exploit the inherent nature of event cameras. However, most of the adopted methods have poor real-time performance. To alleviate it, a lightweight Gaussian Process (GP)-based estimation framework is proposed to efficiently estimate motion trajectory from asynchronous event-driven data associations. Concretely, an asynchronous front-end pipeline is designed to adapt event-driven feature trackers and generate feature trajectories from event streams; a parallel dynamic sliding-window back-end is presented within the framework of sparse GP regression on SE(3). Notably, a specially designed state marginalization strategy is employed to ensure the consistency and sparsity of this GP regression. Experiments conducted on synthetic and real-world datasets demonstrate that the proposed method achieves competitive precision and superior robustness compared to the state-of-the-art. Furthermore, the evaluations on three 60 s trajectories show that the proposal outperforms the ISAM2-based method in terms of computational efficiency by 2.64, 4.22, and 11.70 times, respectively.","sentences":["Event cameras are bio-inspired vision sensors that asynchronously measure per-pixel brightness changes.","The high temporal resolution and asynchronicity of event cameras offer great potential for estimating the robot motion state.","Recent works have adopted the continuous-time ego-motion estimation methods to exploit the inherent nature of event cameras.","However, most of the adopted methods have poor real-time performance.","To alleviate it, a lightweight Gaussian Process (GP)-based estimation framework is proposed to efficiently estimate motion trajectory from asynchronous event-driven data associations.","Concretely, an asynchronous front-end pipeline is designed to adapt event-driven feature trackers and generate feature trajectories from event streams; a parallel dynamic sliding-window back-end is presented within the framework of sparse GP regression on SE(3).","Notably, a specially designed state marginalization strategy is employed to ensure the consistency and sparsity of this GP regression.","Experiments conducted on synthetic and real-world datasets demonstrate that the proposed method achieves competitive precision and superior robustness compared to the state-of-the-art.","Furthermore, the evaluations on three 60 s trajectories show that the proposal outperforms the ISAM2-based method in terms of computational efficiency by 2.64, 4.22, and 11.70 times, respectively."],"url":"http://arxiv.org/abs/2402.16398v1"}
{"created":"2024-02-26 08:36:11","title":"Communication Optimal Unbalanced Private Set Union","abstract":"We consider the private set union (PSU) problem, where two parties each hold a private set of elements, and they want one of the parties (the receiver) to learn the union of the two sets and nothing else. Our protocols are targeted for the unbalanced case where the receiver's set size is larger than the sender's set size, with the goal of minimizing the costs for the sender both in terms of communication volume and local computation time. This setting is motivated by applications where the receiver has significantly more data (input set size) and computational resources than the sender which might be realized on a small, low-power device. Asymptotically, we achieve communication cost linear in the sender's (smaller) set size, and computation costs for sender and receiver which are nearly-linear in their respective set sizes. To our knowledge, ours is the first algorithm to achieve nearly-linear communication and computation for PSU in this unbalanced setting. Our protocols utilize fully homomorphic encryption (FHE) and, optionally, linearly homomorphic encryption (LHE) to perform the necessary computations while preserving privacy. The underlying computations are based on univariate polynomial arithmetic realized within homomorphic encryption, namely fast multiplication, modular reduction, and multi-point evaluation. These asymptotically fast HE polynomial arithmetic algorithms may be of independent interest.","sentences":["We consider the private set union (PSU) problem, where two parties each hold a private set of elements, and they want one of the parties (the receiver) to learn the union of the two sets and nothing else.","Our protocols are targeted for the unbalanced case where the receiver's set size is larger than the sender's set size, with the goal of minimizing the costs for the sender both in terms of communication volume and local computation time.","This setting is motivated by applications where the receiver has significantly more data (input set size) and computational resources than the sender which might be realized on a small, low-power device.","Asymptotically, we achieve communication cost linear in the sender's (smaller) set size, and computation costs for sender and receiver which are nearly-linear in their respective set sizes.","To our knowledge, ours is the first algorithm to achieve nearly-linear communication and computation for PSU in this unbalanced setting.","Our protocols utilize fully homomorphic encryption (FHE) and, optionally, linearly homomorphic encryption (LHE) to perform the necessary computations while preserving privacy.","The underlying computations are based on univariate polynomial arithmetic realized within homomorphic encryption, namely fast multiplication, modular reduction, and multi-point evaluation.","These asymptotically fast HE polynomial arithmetic algorithms may be of independent interest."],"url":"http://arxiv.org/abs/2402.16393v1"}
{"created":"2024-02-26 08:32:41","title":"Placing Objects in Context via Inpainting for Out-of-distribution Segmentation","abstract":"When deploying a semantic segmentation model into the real world, it will inevitably be confronted with semantic classes unseen during training. Thus, to safely deploy such systems, it is crucial to accurately evaluate and improve their anomaly segmentation capabilities. However, acquiring and labelling semantic segmentation data is expensive and unanticipated conditions are long-tail and potentially hazardous. Indeed, existing anomaly segmentation datasets capture a limited number of anomalies, lack realism or have strong domain shifts. In this paper, we propose the Placing Objects in Context (POC) pipeline to realistically add any object into any image via diffusion models. POC can be used to easily extend any dataset with an arbitrary number of objects. In our experiments, we present different anomaly segmentation datasets based on POC-generated data and show that POC can improve the performance of recent state-of-the-art anomaly fine-tuning methods in several standardized benchmarks. POC is also effective to learn new classes. For example, we use it to edit Cityscapes samples by adding a subset of Pascal classes and show that models trained on such data achieve comparable performance to the Pascal-trained baseline. This corroborates the low sim-to-real gap of models trained on POC-generated images.","sentences":["When deploying a semantic segmentation model into the real world, it will inevitably be confronted with semantic classes unseen during training.","Thus, to safely deploy such systems, it is crucial to accurately evaluate and improve their anomaly segmentation capabilities.","However, acquiring and labelling semantic segmentation data is expensive and unanticipated conditions are long-tail and potentially hazardous.","Indeed, existing anomaly segmentation datasets capture a limited number of anomalies, lack realism or have strong domain shifts.","In this paper, we propose the Placing Objects in Context (POC) pipeline to realistically add any object into any image via diffusion models.","POC can be used to easily extend any dataset with an arbitrary number of objects.","In our experiments, we present different anomaly segmentation datasets based on POC-generated data and show that POC can improve the performance of recent state-of-the-art anomaly fine-tuning methods in several standardized benchmarks.","POC is also effective to learn new classes.","For example, we use it to edit Cityscapes samples by adding a subset of Pascal classes and show that models trained on such data achieve comparable performance to the Pascal-trained baseline.","This corroborates the low sim-to-real gap of models trained on POC-generated images."],"url":"http://arxiv.org/abs/2402.16392v1"}
{"created":"2024-02-26 08:31:45","title":"Quality Assurance for Artificial Intelligence: A Study of Industrial Concerns, Challenges and Best Practices","abstract":"Quality Assurance (QA) aims to prevent mistakes and defects in manufactured products and avoid problems when delivering products or services to customers. QA for AI systems, however, poses particular challenges, given their data-driven and non-deterministic nature as well as more complex architectures and algorithms. While there is growing empirical evidence about practices of machine learning in industrial contexts, little is known about the challenges and best practices of quality assurance for AI systems (QA4AI). In this paper, we report on a mixed-method study of QA4AI in industry practice from various countries and companies. Through interviews with fifteen industry practitioners and a validation survey with 50 practitioner responses, we studied the concerns as well as challenges and best practices in ensuring the QA4AI properties reported in the literature, such as correctness, fairness, interpretability and others. Our findings suggest correctness as the most important property, followed by model relevance, efficiency and deployability. In contrast, transferability (applying knowledge learned in one task to another task), security and fairness are not paid much attention by practitioners compared to other properties. Challenges and solutions are identified for each QA4AI property. For example, interviewees highlighted the trade-off challenge among latency, cost and accuracy for efficiency (latency and cost are parts of efficiency concern). Solutions like model compression are proposed. We identified 21 QA4AI practices across each stage of AI development, with 10 practices being well recognized and another 8 practices being marginally agreed by the survey practitioners.","sentences":["Quality Assurance (QA) aims to prevent mistakes and defects in manufactured products and avoid problems when delivering products or services to customers.","QA for AI systems, however, poses particular challenges, given their data-driven and non-deterministic nature as well as more complex architectures and algorithms.","While there is growing empirical evidence about practices of machine learning in industrial contexts, little is known about the challenges and best practices of quality assurance for AI systems (QA4AI).","In this paper, we report on a mixed-method study of QA4AI in industry practice from various countries and companies.","Through interviews with fifteen industry practitioners and a validation survey with 50 practitioner responses, we studied the concerns as well as challenges and best practices in ensuring the QA4AI properties reported in the literature, such as correctness, fairness, interpretability and others.","Our findings suggest correctness as the most important property, followed by model relevance, efficiency and deployability.","In contrast, transferability (applying knowledge learned in one task to another task), security and fairness are not paid much attention by practitioners compared to other properties.","Challenges and solutions are identified for each QA4AI property.","For example, interviewees highlighted the trade-off challenge among latency, cost and accuracy for efficiency (latency and cost are parts of efficiency concern).","Solutions like model compression are proposed.","We identified 21 QA4AI practices across each stage of AI development, with 10 practices being well recognized and another 8 practices being marginally agreed by the survey practitioners."],"url":"http://arxiv.org/abs/2402.16391v1"}
{"created":"2024-02-26 08:27:50","title":"MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in Intellectual Property","abstract":"Large language models (LLMs) have demonstrated impressive performance in various natural language processing (NLP) tasks. However, there is limited understanding of how well LLMs perform in specific domains (e.g, the intellectual property (IP) domain). In this paper, we contribute a new benchmark, the first Multilingual-oriented quiZ on Intellectual Property (MoZIP), for the evaluation of LLMs in the IP domain. The MoZIP benchmark includes three challenging tasks: IP multiple-choice quiz (IPQuiz), IP question answering (IPQA), and patent matching (PatentMatch). In addition, we also develop a new IP-oriented multilingual large language model (called MoZi), which is a BLOOMZ-based model that has been supervised fine-tuned with multilingual IP-related text data. We evaluate our proposed MoZi model and four well-known LLMs (i.e., BLOOMZ, BELLE, ChatGLM and ChatGPT) on the MoZIP benchmark. Experimental results demonstrate that MoZi outperforms BLOOMZ, BELLE and ChatGLM by a noticeable margin, while it had lower scores compared with ChatGPT. Notably, the performance of current LLMs on the MoZIP benchmark has much room for improvement, and even the most powerful ChatGPT does not reach the passing level. Our source code, data, and models are available at \\url{https://github.com/AI-for-Science/MoZi}.","sentences":["Large language models (LLMs) have demonstrated impressive performance in various natural language processing (NLP) tasks.","However, there is limited understanding of how well LLMs perform in specific domains (e.g, the intellectual property (IP) domain).","In this paper, we contribute a new benchmark, the first Multilingual-oriented quiZ on Intellectual Property (MoZIP), for the evaluation of LLMs in the IP domain.","The MoZIP benchmark includes three challenging tasks: IP multiple-choice quiz (IPQuiz), IP question answering (IPQA), and patent matching (PatentMatch).","In addition, we also develop a new IP-oriented multilingual large language model (called MoZi), which is a BLOOMZ-based model that has been supervised fine-tuned with multilingual IP-related text data.","We evaluate our proposed MoZi model and four well-known LLMs (i.e., BLOOMZ, BELLE, ChatGLM and ChatGPT) on the MoZIP benchmark.","Experimental results demonstrate that MoZi outperforms BLOOMZ, BELLE and ChatGLM by a noticeable margin, while it had lower scores compared with ChatGPT.","Notably, the performance of current LLMs on the MoZIP benchmark has much room for improvement, and even the most powerful ChatGPT does not reach the passing level.","Our source code, data, and models are available at \\url{https://github.com/AI-for-Science/MoZi}."],"url":"http://arxiv.org/abs/2402.16389v1"}
{"created":"2024-02-26 08:22:22","title":"On the Generalization Capability of Temporal Graph Learning Algorithms: Theoretical Insights and a Simpler Method","abstract":"Temporal Graph Learning (TGL) has become a prevalent technique across diverse real-world applications, especially in domains where data can be represented as a graph and evolves over time. Although TGL has recently seen notable progress in algorithmic solutions, its theoretical foundations remain largely unexplored. This paper aims at bridging this gap by investigating the generalization ability of different TGL algorithms (e.g., GNN-based, RNN-based, and memory-based methods) under the finite-wide over-parameterized regime. We establish the connection between the generalization error of TGL algorithms and \"the number of layers/steps\" in the GNN-/RNN-based TGL methods and \"the feature-label alignment (FLA) score\", where FLA can be used as a proxy for the expressive power and explains the performance of memory-based methods. Guided by our theoretical analysis, we propose Simplified-Temporal-Graph-Network, which enjoys a small generalization error, improved overall performance, and lower model complexity. Extensive experiments on real-world datasets demonstrate the effectiveness of our method. Our theoretical findings and proposed algorithm offer essential insights into TGL from a theoretical standpoint, laying the groundwork for the designing practical TGL algorithms in future studies.","sentences":["Temporal Graph Learning (TGL) has become a prevalent technique across diverse real-world applications, especially in domains where data can be represented as a graph and evolves over time.","Although TGL has recently seen notable progress in algorithmic solutions, its theoretical foundations remain largely unexplored.","This paper aims at bridging this gap by investigating the generalization ability of different TGL algorithms (e.g., GNN-based, RNN-based, and memory-based methods) under the finite-wide over-parameterized regime.","We establish the connection between the generalization error of TGL algorithms and \"the number of layers/steps\" in the GNN-/RNN-based TGL methods and \"the feature-label alignment (FLA) score\", where FLA can be used as a proxy for the expressive power and explains the performance of memory-based methods.","Guided by our theoretical analysis, we propose Simplified-Temporal-Graph-Network, which enjoys a small generalization error, improved overall performance, and lower model complexity.","Extensive experiments on real-world datasets demonstrate the effectiveness of our method.","Our theoretical findings and proposed algorithm offer essential insights into TGL from a theoretical standpoint, laying the groundwork for the designing practical TGL algorithms in future studies."],"url":"http://arxiv.org/abs/2402.16387v1"}
{"created":"2024-02-26 08:08:30","title":"Self Supervised Correlation-based Permutations for Multi-View Clustering","abstract":"Fusing information from different modalities can enhance data analysis tasks, including clustering. However, existing multi-view clustering (MVC) solutions are limited to specific domains or rely on a suboptimal and computationally demanding two-stage procedure of representation and clustering. We propose an end-to-end deep learning-based MVC framework for general data (image, tabular, etc.). Our approach involves learning meaningful fused data representations with a novel permutation-based canonical correlation objective. Concurrently, we learn cluster assignments by identifying consistent pseudo-labels across multiple views. We demonstrate the effectiveness of our model using ten MVC benchmark datasets. Theoretically, we show that our model approximates the supervised linear discrimination analysis (LDA) representation. Additionally, we provide an error bound induced by false-pseudo label annotations.","sentences":["Fusing information from different modalities can enhance data analysis tasks, including clustering.","However, existing multi-view clustering (MVC) solutions are limited to specific domains or rely on a suboptimal and computationally demanding two-stage procedure of representation and clustering.","We propose an end-to-end deep learning-based MVC framework for general data (image, tabular, etc.).","Our approach involves learning meaningful fused data representations with a novel permutation-based canonical correlation objective.","Concurrently, we learn cluster assignments by identifying consistent pseudo-labels across multiple views.","We demonstrate the effectiveness of our model using ten MVC benchmark datasets.","Theoretically, we show that our model approximates the supervised linear discrimination analysis (LDA) representation.","Additionally, we provide an error bound induced by false-pseudo label annotations."],"url":"http://arxiv.org/abs/2402.16383v1"}
{"created":"2024-02-26 07:52:40","title":"Graph Learning under Distribution Shifts: A Comprehensive Survey on Domain Adaptation, Out-of-distribution, and Continual Learning","abstract":"Graph learning plays a pivotal role and has gained significant attention in various application scenarios, from social network analysis to recommendation systems, for its effectiveness in modeling complex data relations represented by graph structural data. In reality, the real-world graph data typically show dynamics over time, with changing node attributes and edge structure, leading to the severe graph data distribution shift issue. This issue is compounded by the diverse and complex nature of distribution shifts, which can significantly impact the performance of graph learning methods in degraded generalization and adaptation capabilities, posing a substantial challenge to their effectiveness. In this survey, we provide a comprehensive review and summary of the latest approaches, strategies, and insights that address distribution shifts within the context of graph learning. Concretely, according to the observability of distributions in the inference stage and the availability of sufficient supervision information in the training stage, we categorize existing graph learning methods into several essential scenarios, including graph domain adaptation learning, graph out-of-distribution learning, and graph continual learning. For each scenario, a detailed taxonomy is proposed, with specific descriptions and discussions of existing progress made in distribution-shifted graph learning. Additionally, we discuss the potential applications and future directions for graph learning under distribution shifts with a systematic analysis of the current state in this field. The survey is positioned to provide general guidance for the development of effective graph learning algorithms in handling graph distribution shifts, and to stimulate future research and advancements in this area.","sentences":["Graph learning plays a pivotal role and has gained significant attention in various application scenarios, from social network analysis to recommendation systems, for its effectiveness in modeling complex data relations represented by graph structural data.","In reality, the real-world graph data typically show dynamics over time, with changing node attributes and edge structure, leading to the severe graph data distribution shift issue.","This issue is compounded by the diverse and complex nature of distribution shifts, which can significantly impact the performance of graph learning methods in degraded generalization and adaptation capabilities, posing a substantial challenge to their effectiveness.","In this survey, we provide a comprehensive review and summary of the latest approaches, strategies, and insights that address distribution shifts within the context of graph learning.","Concretely, according to the observability of distributions in the inference stage and the availability of sufficient supervision information in the training stage, we categorize existing graph learning methods into several essential scenarios, including graph domain adaptation learning, graph out-of-distribution learning, and graph continual learning.","For each scenario, a detailed taxonomy is proposed, with specific descriptions and discussions of existing progress made in distribution-shifted graph learning.","Additionally, we discuss the potential applications and future directions for graph learning under distribution shifts with a systematic analysis of the current state in this field.","The survey is positioned to provide general guidance for the development of effective graph learning algorithms in handling graph distribution shifts, and to stimulate future research and advancements in this area."],"url":"http://arxiv.org/abs/2402.16374v1"}
{"created":"2024-02-26 07:48:19","title":"DEYO: DETR with YOLO for End-to-End Object Detection","abstract":"The training paradigm of DETRs is heavily contingent upon pre-training their backbone on the ImageNet dataset. However, the limited supervisory signals provided by the image classification task and one-to-one matching strategy result in an inadequately pre-trained neck for DETRs. Additionally, the instability of matching in the early stages of training engenders inconsistencies in the optimization objectives of DETRs. To address these issues, we have devised an innovative training methodology termed step-by-step training. Specifically, in the first stage of training, we employ a classic detector, pre-trained with a one-to-many matching strategy, to initialize the backbone and neck of the end-to-end detector. In the second stage of training, we froze the backbone and neck of the end-to-end detector, necessitating the training of the decoder from scratch. Through the application of step-by-step training, we have introduced the first real-time end-to-end object detection model that utilizes a purely convolutional structure encoder, DETR with YOLO (DEYO). Without reliance on any supplementary training data, DEYO surpasses all existing real-time object detectors in both speed and accuracy. Moreover, the comprehensive DEYO series can complete its second-phase training on the COCO dataset using a single 8GB RTX 4060 GPU, significantly reducing the training expenditure. Source code and pre-trained models are available at https://github.com/ouyanghaodong/DEYO.","sentences":["The training paradigm of DETRs is heavily contingent upon pre-training their backbone on the ImageNet dataset.","However, the limited supervisory signals provided by the image classification task and one-to-one matching strategy result in an inadequately pre-trained neck for DETRs.","Additionally, the instability of matching in the early stages of training engenders inconsistencies in the optimization objectives of DETRs.","To address these issues, we have devised an innovative training methodology termed step-by-step training.","Specifically, in the first stage of training, we employ a classic detector, pre-trained with a one-to-many matching strategy, to initialize the backbone and neck of the end-to-end detector.","In the second stage of training, we froze the backbone and neck of the end-to-end detector, necessitating the training of the decoder from scratch.","Through the application of step-by-step training, we have introduced the first real-time end-to-end object detection model that utilizes a purely convolutional structure encoder, DETR with YOLO (DEYO).","Without reliance on any supplementary training data, DEYO surpasses all existing real-time object detectors in both speed and accuracy.","Moreover, the comprehensive DEYO series can complete its second-phase training on the COCO dataset using a single 8GB RTX 4060 GPU, significantly reducing the training expenditure.","Source code and pre-trained models are available at https://github.com/ouyanghaodong/DEYO."],"url":"http://arxiv.org/abs/2402.16370v1"}
{"created":"2024-02-26 07:47:12","title":"Generative AI in Vision: A Survey on Models, Metrics and Applications","abstract":"Generative AI models have revolutionized various fields by enabling the creation of realistic and diverse data samples. Among these models, diffusion models have emerged as a powerful approach for generating high-quality images, text, and audio. This survey paper provides a comprehensive overview of generative AI diffusion and legacy models, focusing on their underlying techniques, applications across different domains, and their challenges. We delve into the theoretical foundations of diffusion models, including concepts such as denoising diffusion probabilistic models (DDPM) and score-based generative modeling. Furthermore, we explore the diverse applications of these models in text-to-image, image inpainting, and image super-resolution, along with others, showcasing their potential in creative tasks and data augmentation. By synthesizing existing research and highlighting critical advancements in this field, this survey aims to provide researchers and practitioners with a comprehensive understanding of generative AI diffusion and legacy models and inspire future innovations in this exciting area of artificial intelligence.","sentences":["Generative AI models have revolutionized various fields by enabling the creation of realistic and diverse data samples.","Among these models, diffusion models have emerged as a powerful approach for generating high-quality images, text, and audio.","This survey paper provides a comprehensive overview of generative AI diffusion and legacy models, focusing on their underlying techniques, applications across different domains, and their challenges.","We delve into the theoretical foundations of diffusion models, including concepts such as denoising diffusion probabilistic models (DDPM) and score-based generative modeling.","Furthermore, we explore the diverse applications of these models in text-to-image, image inpainting, and image super-resolution, along with others, showcasing their potential in creative tasks and data augmentation.","By synthesizing existing research and highlighting critical advancements in this field, this survey aims to provide researchers and practitioners with a comprehensive understanding of generative AI diffusion and legacy models and inspire future innovations in this exciting area of artificial intelligence."],"url":"http://arxiv.org/abs/2402.16369v1"}
{"created":"2024-02-26 07:24:32","title":"Feedback Efficient Online Fine-Tuning of Diffusion Models","abstract":"Diffusion models excel at modeling complex data distributions, including those of images, proteins, and small molecules. However, in many cases, our goal is to model parts of the distribution that maximize certain properties: for example, we may want to generate images with high aesthetic quality, or molecules with high bioactivity. It is natural to frame this as a reinforcement learning (RL) problem, in which the objective is to fine-tune a diffusion model to maximize a reward function that corresponds to some property. Even with access to online queries of the ground-truth reward function, efficiently discovering high-reward samples can be challenging: they might have a low probability in the initial distribution, and there might be many infeasible samples that do not even have a well-defined reward (e.g., unnatural images or physically impossible molecules). In this work, we propose a novel reinforcement learning procedure that efficiently explores on the manifold of feasible samples. We present a theoretical analysis providing a regret guarantee, as well as empirical validation across three domains: images, biological sequences, and molecules.","sentences":["Diffusion models excel at modeling complex data distributions, including those of images, proteins, and small molecules.","However, in many cases, our goal is to model parts of the distribution that maximize certain properties: for example, we may want to generate images with high aesthetic quality, or molecules with high bioactivity.","It is natural to frame this as a reinforcement learning (RL) problem, in which the objective is to fine-tune a diffusion model to maximize a reward function that corresponds to some property.","Even with access to online queries of the ground-truth reward function, efficiently discovering high-reward samples can be challenging: they might have a low probability in the initial distribution, and there might be many infeasible samples that do not even have a well-defined reward (e.g., unnatural images or physically impossible molecules).","In this work, we propose a novel reinforcement learning procedure that efficiently explores on the manifold of feasible samples.","We present a theoretical analysis providing a regret guarantee, as well as empirical validation across three domains: images, biological sequences, and molecules."],"url":"http://arxiv.org/abs/2402.16359v1"}
{"created":"2024-02-26 07:22:51","title":"An Integrated Data Processing Framework for Pretraining Foundation Models","abstract":"The ability of the foundation models heavily relies on large-scale, diverse, and high-quality pretraining data. In order to improve data quality, researchers and practitioners often have to manually curate datasets from difference sources and develop dedicated data cleansing pipeline for each data repository. Lacking a unified data processing framework, this process is repetitive and cumbersome. To mitigate this issue, we propose a data processing framework that integrates a Processing Module which consists of a series of operators at different granularity levels, and an Analyzing Module which supports probing and evaluation of the refined data. The proposed framework is easy to use and highly flexible. In this demo paper, we first introduce how to use this framework with some example use cases and then demonstrate its effectiveness in improving the data quality with an automated evaluation with ChatGPT and an end-to-end evaluation in pretraining the GPT-2 model. The code and demonstration videos are accessible on GitHub.","sentences":["The ability of the foundation models heavily relies on large-scale, diverse, and high-quality pretraining data.","In order to improve data quality, researchers and practitioners often have to manually curate datasets from difference sources and develop dedicated data cleansing pipeline for each data repository.","Lacking a unified data processing framework, this process is repetitive and cumbersome.","To mitigate this issue, we propose a data processing framework that integrates a Processing Module which consists of a series of operators at different granularity levels, and an Analyzing Module which supports probing and evaluation of the refined data.","The proposed framework is easy to use and highly flexible.","In this demo paper, we first introduce how to use this framework with some example use cases and then demonstrate its effectiveness in improving the data quality with an automated evaluation with ChatGPT and an end-to-end evaluation in pretraining the GPT-2 model.","The code and demonstration videos are accessible on GitHub."],"url":"http://arxiv.org/abs/2402.16358v1"}
{"created":"2024-02-26 07:17:25","title":"MathGenie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of LLMs","abstract":"Large language models (LLMs) have exhibited great potential in mathematical reasoning. However, there remains a performance gap in this area between existing open-source models and closed-source models such as GPT-4. In this paper, we introduce MathGenie, a novel method for generating diverse and reliable math problems from a small-scale problem-solution dataset (denoted as seed data). We augment the ground-truth solutions of our seed data and train a back-translation model to translate the augmented solutions back into new questions. Subsequently, we generate code-integrated solutions for the new questions. To ensure the correctness of the code-integrated solutions, we employ rationale-based strategy for solution verification. Various pretrained models, ranging from 7B to 70B, are trained on the newly curated data to test the effectiveness of the proposed augmentation technique, resulting in a family of models known as MathGenieLM. These models consistently outperform previous open-source models across five representative mathematical reasoning datasets, achieving state-of-the-art performance. In particular, MathGenieLM-InternLM2 achieves an accuracy of 87.7% on GSM8K and 55.7% on MATH, securing the best overall score among open-source language models.","sentences":["Large language models (LLMs) have exhibited great potential in mathematical reasoning.","However, there remains a performance gap in this area between existing open-source models and closed-source models such as GPT-4.","In this paper, we introduce MathGenie, a novel method for generating diverse and reliable math problems from a small-scale problem-solution dataset (denoted as seed data).","We augment the ground-truth solutions of our seed data and train a back-translation model to translate the augmented solutions back into new questions.","Subsequently, we generate code-integrated solutions for the new questions.","To ensure the correctness of the code-integrated solutions, we employ rationale-based strategy for solution verification.","Various pretrained models, ranging from 7B to 70B, are trained on the newly curated data to test the effectiveness of the proposed augmentation technique, resulting in a family of models known as MathGenieLM.","These models consistently outperform previous open-source models across five representative mathematical reasoning datasets, achieving state-of-the-art performance.","In particular, MathGenieLM-InternLM2 achieves an accuracy of 87.7% on GSM8K and 55.7% on MATH, securing the best overall score among open-source language models."],"url":"http://arxiv.org/abs/2402.16352v1"}
{"created":"2024-02-26 07:00:58","title":"CodeS: Towards Building Open-source Language Models for Text-to-SQL","abstract":"Language models have shown promising performance on the task of translating natural language questions into SQL queries (Text-to-SQL). However, most of the state-of-the-art (SOTA) approaches rely on powerful yet closed-source large language models (LLMs), such as ChatGPT and GPT-4, which may have the limitations of unclear model architectures, data privacy risks, and expensive inference overheads. To address the limitations, we introduce CodeS, a series of pre-trained language models with parameters ranging from 1B to 15B, specifically designed for the text-to-SQL task. CodeS is a fully open-source language model, which achieves superior accuracy with much smaller parameter sizes. This paper studies the research challenges in building CodeS. To enhance the SQL generation abilities of CodeS, we adopt an incremental pre-training approach using a specifically curated SQL-centric corpus. Based on this, we address the challenges of schema linking and rapid domain adaptation through strategic prompt construction and a bi-directional data augmentation technique. We conduct comprehensive evaluations on multiple datasets, including the widely used Spider benchmark, the newly released BIRD benchmark, robustness-diagnostic benchmarks such as Spider-DK, Spider-Syn, Spider-Realistic, and Dr.Spider, as well as two real-world datasets created for financial and academic applications. The experimental results show that our CodeS achieves new SOTA accuracy and robustness on nearly all challenging text-to-SQL benchmarks.","sentences":["Language models have shown promising performance on the task of translating natural language questions into SQL queries (Text-to-SQL).","However, most of the state-of-the-art (SOTA) approaches rely on powerful yet closed-source large language models (LLMs), such as ChatGPT and GPT-4, which may have the limitations of unclear model architectures, data privacy risks, and expensive inference overheads.","To address the limitations, we introduce CodeS, a series of pre-trained language models with parameters ranging from 1B to 15B, specifically designed for the text-to-SQL task.","CodeS is a fully open-source language model, which achieves superior accuracy with much smaller parameter sizes.","This paper studies the research challenges in building CodeS.","To enhance the SQL generation abilities of CodeS, we adopt an incremental pre-training approach using a specifically curated SQL-centric corpus.","Based on this, we address the challenges of schema linking and rapid domain adaptation through strategic prompt construction and a bi-directional data augmentation technique.","We conduct comprehensive evaluations on multiple datasets, including the widely used Spider benchmark, the newly released BIRD benchmark, robustness-diagnostic benchmarks such as Spider-DK, Spider-Syn, Spider-Realistic, and Dr.Spider, as well as two real-world datasets created for financial and academic applications.","The experimental results show that our CodeS achieves new SOTA accuracy and robustness on nearly all challenging text-to-SQL benchmarks."],"url":"http://arxiv.org/abs/2402.16347v1"}
{"created":"2024-02-26 06:55:36","title":"Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems","abstract":"Hybrid main memory systems combine both performance and capacity advantages from heterogeneous memory technologies. With larger capacities, higher associativities, and finer granularities, hybrid memory systems currently exhibit significant metadata storage and lookup overheads for flexibly remapping data blocks between the two memory tiers. To alleviate the inefficiencies of existing designs, we propose Trimma, the combination of a multi-level metadata structure and an efficient metadata cache design. Trimma uses a multi-level metadata table to only track truly necessary address remap entries. The saved memory space is effectively utilized as extra DRAM cache capacity to improve performance. Trimma also uses separate formats to store the entries with non-identity and identity mappings. This improves the overall remap cache hit rate, further boosting the performance. Trimma is transparent to software and compatible with various types of hybrid memory systems. When evaluated on a representative DDR4 + NVM hybrid memory system, Trimma achieves up to 2.4$\\times$ and on average 58.1\\% speedup benefits, compared with a state-of-the-art design that only leverages the unallocated fast memory space for caching. Trimma addresses metadata management overheads and targets future scalable large-scale hybrid memory architectures.","sentences":["Hybrid main memory systems combine both performance and capacity advantages from heterogeneous memory technologies.","With larger capacities, higher associativities, and finer granularities, hybrid memory systems currently exhibit significant metadata storage and lookup overheads for flexibly remapping data blocks between the two memory tiers.","To alleviate the inefficiencies of existing designs, we propose Trimma, the combination of a multi-level metadata structure and an efficient metadata cache design.","Trimma uses a multi-level metadata table to only track truly necessary address remap entries.","The saved memory space is effectively utilized as extra DRAM cache capacity to improve performance.","Trimma also uses separate formats to store the entries with non-identity and identity mappings.","This improves the overall remap cache hit rate, further boosting the performance.","Trimma is transparent to software and compatible with various types of hybrid memory systems.","When evaluated on a representative DDR4 + NVM hybrid memory system, Trimma achieves up to 2.4$\\times$ and on average 58.1\\% speedup benefits, compared with a state-of-the-art design that only leverages the unallocated fast memory space for caching.","Trimma addresses metadata management overheads and targets future scalable large-scale hybrid memory architectures."],"url":"http://arxiv.org/abs/2402.16343v1"}
{"created":"2024-02-26 06:36:32","title":"BLO-SAM: Bi-level Optimization Based Overfitting-Preventing Finetuning of SAM","abstract":"The Segment Anything Model (SAM), a foundation model pretrained on millions of images and segmentation masks, has significantly advanced semantic segmentation, a fundamental task in computer vision. Despite its strengths, SAM encounters two major challenges. Firstly, it struggles with segmenting specific objects autonomously, as it relies on users to manually input prompts like points or bounding boxes to identify targeted objects. Secondly, SAM faces challenges in excelling at specific downstream tasks, like medical imaging, due to a disparity between the distribution of its pretraining data, which predominantly consists of general-domain images, and the data used in downstream tasks. Current solutions to these problems, which involve finetuning SAM, often lead to overfitting, a notable issue in scenarios with very limited data, like in medical imaging. To overcome these limitations, we introduce BLO-SAM, which finetunes SAM based on bi-level optimization (BLO). Our approach allows for automatic image segmentation without the need for manual prompts, by optimizing a learnable prompt embedding. Furthermore, it significantly reduces the risk of overfitting by training the model's weight parameters and the prompt embedding on two separate subsets of the training dataset, each at a different level of optimization. We apply BLO-SAM to diverse semantic segmentation tasks in general and medical domains. The results demonstrate BLO-SAM's superior performance over various state-of-the-art image semantic segmentation methods.","sentences":["The Segment Anything Model (SAM), a foundation model pretrained on millions of images and segmentation masks, has significantly advanced semantic segmentation, a fundamental task in computer vision.","Despite its strengths, SAM encounters two major challenges.","Firstly, it struggles with segmenting specific objects autonomously, as it relies on users to manually input prompts like points or bounding boxes to identify targeted objects.","Secondly, SAM faces challenges in excelling at specific downstream tasks, like medical imaging, due to a disparity between the distribution of its pretraining data, which predominantly consists of general-domain images, and the data used in downstream tasks.","Current solutions to these problems, which involve finetuning SAM, often lead to overfitting, a notable issue in scenarios with very limited data, like in medical imaging.","To overcome these limitations, we introduce BLO-SAM, which finetunes SAM based on bi-level optimization (BLO).","Our approach allows for automatic image segmentation without the need for manual prompts, by optimizing a learnable prompt embedding.","Furthermore, it significantly reduces the risk of overfitting by training the model's weight parameters and the prompt embedding on two separate subsets of the training dataset, each at a different level of optimization.","We apply BLO-SAM to diverse semantic segmentation tasks in general and medical domains.","The results demonstrate BLO-SAM's superior performance over various state-of-the-art image semantic segmentation methods."],"url":"http://arxiv.org/abs/2402.16338v1"}
{"created":"2024-02-26 06:22:41","title":"A Joint Communication and Computation Design for Probabilistic Semantic Communications","abstract":"In this paper, the problem of joint transmission and computation resource allocation for a multi-user probabilistic semantic communication (PSC) network is investigated. In the considered model, users employ semantic information extraction techniques to compress their large-sized data before transmitting them to a multi-antenna base station (BS). Our model represents large-sized data through substantial knowledge graphs, utilizing shared probability graphs between the users and the BS for efficient semantic compression. The resource allocation problem is formulated as an optimization problem with the objective of maximizing the sum of equivalent rate of all users, considering total power budget and semantic resource limit constraints. The computation load considered in the PSC network is formulated as a non-smooth piecewise function with respect to the semantic compression ratio. To tackle this non-convex non-smooth optimization challenge, a three-stage algorithm is proposed where the solutions for the receive beamforming matrix of the BS, transmit power of each user, and semantic compression ratio of each user are obtained stage by stage. Numerical results validate the effectiveness of our proposed scheme.","sentences":["In this paper, the problem of joint transmission and computation resource allocation for a multi-user probabilistic semantic communication (PSC) network is investigated.","In the considered model, users employ semantic information extraction techniques to compress their large-sized data before transmitting them to a multi-antenna base station (BS).","Our model represents large-sized data through substantial knowledge graphs, utilizing shared probability graphs between the users and the BS for efficient semantic compression.","The resource allocation problem is formulated as an optimization problem with the objective of maximizing the sum of equivalent rate of all users, considering total power budget and semantic resource limit constraints.","The computation load considered in the PSC network is formulated as a non-smooth piecewise function with respect to the semantic compression ratio.","To tackle this non-convex non-smooth optimization challenge, a three-stage algorithm is proposed where the solutions for the receive beamforming matrix of the BS, transmit power of each user, and semantic compression ratio of each user are obtained stage by stage.","Numerical results validate the effectiveness of our proposed scheme."],"url":"http://arxiv.org/abs/2402.16328v1"}
{"created":"2024-02-26 06:08:11","title":"Algorithms for Halfplane Coverage and Related Problems","abstract":"Given in the plane a set of points and a set of halfplanes, we consider the problem of computing a smallest subset of halfplanes whose union covers all points. In this paper, we present an $O(n^{4/3}\\log^{5/3}n\\log^{O(1)}\\log n)$-time algorithm for the problem, where $n$ is the total number of all points and halfplanes. This improves the previously best algorithm of $n^{10/3}2^{O(\\log^*n)}$ time by roughly a quadratic factor. For the special case where all halfplanes are lower ones, our algorithm runs in $O(n\\log n)$ time, which improves the previously best algorithm of $n^{4/3}2^{O(\\log^*n)}$ time and matches an $\\Omega(n\\log n)$ lower bound. Further, our techniques can be extended to solve a star-shaped polygon coverage problem in $O(n\\log n)$ time, which in turn leads to an $O(n\\log n)$-time algorithm for computing an instance-optimal $\\epsilon$-kernel of a set of $n$ points in the plane. Agarwal and Har-Peled presented an $O(nk\\log n)$-time algorithm for this problem in SoCG 2023, where $k$ is the size of the $\\epsilon$-kernel; they also raised an open question whether the problem can be solved in $O(n\\log n)$ time. Our result thus answers the open question affirmatively.","sentences":["Given in the plane a set of points and a set of halfplanes, we consider the problem of computing a smallest subset of halfplanes whose union covers all points.","In this paper, we present an $O(n^{4/3}\\log^{5/3}n\\log^{O(1)}\\log n)$-time algorithm for the problem, where $n$ is the total number of all points and halfplanes.","This improves the previously best algorithm of $n^{10/3}2^{O(\\log^*n)}$ time by roughly a quadratic factor.","For the special case where all halfplanes are lower ones, our algorithm runs in $O(n\\log n)$ time, which improves the previously best algorithm of $n^{4/3}2^{O(\\log^*n)}$ time and matches an $\\Omega(n\\log n)$ lower bound.","Further, our techniques can be extended to solve a star-shaped polygon coverage problem in $O(n\\log n)$ time, which in turn leads to an $O(n\\log n)$-time algorithm for computing an instance-optimal $\\epsilon$-kernel of a set of $n$ points in the plane.","Agarwal and Har-Peled presented an $O(nk\\log n)$-time algorithm for this problem in SoCG 2023, where $k$ is the size of the $\\epsilon$-kernel; they also raised an open question whether the problem can be solved in $O(n\\log n)$ time.","Our result thus answers the open question affirmatively."],"url":"http://arxiv.org/abs/2402.16323v1"}
{"created":"2024-02-26 05:51:47","title":"Data-freeWeight Compress and Denoise for Large Language Models","abstract":"Large Language Models (LLMs) are reshaping the research landscape in artificial intelligence, particularly as model parameters scale up significantly, unlocking remarkable capabilities across various domains. Nevertheless, the scalability of model parameters faces constraints due to limitations in GPU memory and computational speed. To address these constraints, various weight compression methods have emerged, such as Pruning and Quantization. Given the low-rank nature of weight matrices in language models, the reduction of weights through matrix decomposition undoubtedly holds significant potential and promise. In this paper, drawing upon the intrinsic structure of LLMs, we propose a novel approach termed Data-free Joint Rank-k Approximation for compressing the parameter matrices. Significantly, our method is characterized by without necessitating additional involvement of any corpus, while simultaneously preserving orthogonality in conjunction with pruning and quantization methods. We achieve a model pruning of 80% parameters while retaining 93.43% of the original performance without any calibration data. Additionally, we explore the fundamental properties of the weight matrix of LLMs undergone Rank-k Approximation and conduct comprehensive experiments to elucidate our hypothesis.","sentences":["Large Language Models (LLMs) are reshaping the research landscape in artificial intelligence, particularly as model parameters scale up significantly, unlocking remarkable capabilities across various domains.","Nevertheless, the scalability of model parameters faces constraints due to limitations in GPU memory and computational speed.","To address these constraints, various weight compression methods have emerged, such as Pruning and Quantization.","Given the low-rank nature of weight matrices in language models, the reduction of weights through matrix decomposition undoubtedly holds significant potential and promise.","In this paper, drawing upon the intrinsic structure of LLMs, we propose a novel approach termed Data-free Joint Rank-k Approximation for compressing the parameter matrices.","Significantly, our method is characterized by without necessitating additional involvement of any corpus, while simultaneously preserving orthogonality in conjunction with pruning and quantization methods.","We achieve a model pruning of 80% parameters while retaining 93.43% of the original performance without any calibration data.","Additionally, we explore the fundamental properties of the weight matrix of LLMs undergone Rank-k Approximation and conduct comprehensive experiments to elucidate our hypothesis."],"url":"http://arxiv.org/abs/2402.16319v1"}
{"created":"2024-02-26 05:50:43","title":"Gradient-Guided Modality Decoupling for Missing-Modality Robustness","abstract":"Multimodal learning with incomplete input data (missing modality) is practical and challenging. In this work, we conduct an in-depth analysis of this challenge and find that modality dominance has a significant negative impact on the model training, greatly degrading the missing modality performance. Motivated by Grad-CAM, we introduce a novel indicator, gradients, to monitor and reduce modality dominance which widely exists in the missing-modality scenario. In aid of this indicator, we present a novel Gradient-guided Modality Decoupling (GMD) method to decouple the dependency on dominating modalities. Specifically, GMD removes the conflicted gradient components from different modalities to achieve this decoupling, significantly improving the performance. In addition, to flexibly handle modal-incomplete data, we design a parameter-efficient Dynamic Sharing (DS) framework which can adaptively switch on/off the network parameters based on whether one modality is available. We conduct extensive experiments on three popular multimodal benchmarks, including BraTS 2018 for medical segmentation, CMU-MOSI, and CMU-MOSEI for sentiment analysis. The results show that our method can significantly outperform the competitors, showing the effectiveness of the proposed solutions. Our code is released here: https://github.com/HaoWang420/Gradient-guided-Modality-Decoupling.","sentences":["Multimodal learning with incomplete input data (missing modality) is practical and challenging.","In this work, we conduct an in-depth analysis of this challenge and find that modality dominance has a significant negative impact on the model training, greatly degrading the missing modality performance.","Motivated by Grad-CAM, we introduce a novel indicator, gradients, to monitor and reduce modality dominance which widely exists in the missing-modality scenario.","In aid of this indicator, we present a novel Gradient-guided Modality Decoupling (GMD) method to decouple the dependency on dominating modalities.","Specifically, GMD removes the conflicted gradient components from different modalities to achieve this decoupling, significantly improving the performance.","In addition, to flexibly handle modal-incomplete data, we design a parameter-efficient Dynamic Sharing (DS) framework which can adaptively switch on/off the network parameters based on whether one modality is available.","We conduct extensive experiments on three popular multimodal benchmarks, including BraTS 2018 for medical segmentation, CMU-MOSI, and CMU-MOSEI for sentiment analysis.","The results show that our method can significantly outperform the competitors, showing the effectiveness of the proposed solutions.","Our code is released here: https://github.com/HaoWang420/Gradient-guided-Modality-Decoupling."],"url":"http://arxiv.org/abs/2402.16318v1"}
{"created":"2024-02-26 05:31:34","title":"Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering","abstract":"Open-ended question answering requires models to find appropriate evidence to form well-reasoned, comprehensive and helpful answers. In practical applications, models also need to engage in extended discussions on potential scenarios closely relevant to the question. With augmentation of retrieval module, open-source Large Language Models (LLMs) can produce coherent answers often with different focuses, but are still sub-optimal in terms of reliable evidence selection and in-depth question analysis. In this paper, we propose a novel Chain-of-Discussion framework to leverage the synergy among multiple open-source LLMs aiming to provide \\textbf{more correct} and \\textbf{more comprehensive} answers for open-ended QA, although they are not strong enough individually. Our experiments show that discussions among multiple LLMs play a vital role in enhancing the quality of answers. We release our data and code at \\url{https://github.com/kobayashikanna01/Chain-of-Discussion}.","sentences":["Open-ended question answering requires models to find appropriate evidence to form well-reasoned, comprehensive and helpful answers.","In practical applications, models also need to engage in extended discussions on potential scenarios closely relevant to the question.","With augmentation of retrieval module, open-source Large Language Models (LLMs) can produce coherent answers often with different focuses, but are still sub-optimal in terms of reliable evidence selection and in-depth question analysis.","In this paper, we propose a novel Chain-of-Discussion framework to leverage the synergy among multiple open-source LLMs aiming to provide \\textbf{more correct} and \\textbf{more comprehensive} answers for open-ended QA, although they are not strong enough individually.","Our experiments show that discussions among multiple LLMs play a vital role in enhancing the quality of answers.","We release our data and code at \\url{https://github.com/kobayashikanna01/Chain-of-Discussion}."],"url":"http://arxiv.org/abs/2402.16313v1"}
{"created":"2024-02-26 05:30:48","title":"Cross-domain Chinese Sentence Pattern Parsing","abstract":"Sentence Pattern Structure (SPS) parsing is a syntactic analysis method primarily employed in language teaching.Existing SPS parsers rely heavily on textbook corpora for training, lacking cross-domain capability.To overcome this constraint, this paper proposes an innovative approach leveraging large language models (LLMs) within a self-training framework. Partial syntactic rules from a source domain are combined with target domain sentences to dynamically generate training data, enhancing the adaptability of the parser to diverse domains.Experiments conducted on textbook and news domains demonstrate the effectiveness of the proposed method, outperforming rule-based baselines by 1.68 points on F1 metrics.","sentences":["Sentence Pattern Structure (SPS) parsing is a syntactic analysis method primarily employed in language teaching.","Existing SPS parsers rely heavily on textbook corpora for training, lacking cross-domain capability.","To overcome this constraint, this paper proposes an innovative approach leveraging large language models (LLMs) within a self-training framework.","Partial syntactic rules from a source domain are combined with target domain sentences to dynamically generate training data, enhancing the adaptability of the parser to diverse domains.","Experiments conducted on textbook and news domains demonstrate the effectiveness of the proposed method, outperforming rule-based baselines by 1.68 points on F1 metrics."],"url":"http://arxiv.org/abs/2402.16311v1"}
{"created":"2024-02-26 04:43:44","title":"Against Filter Bubbles: Diversified Music Recommendation via Weighted Hypergraph Embedding Learning","abstract":"Recommender systems serve a dual purpose for users: sifting out inappropriate or mismatched information while accurately identifying items that align with their preferences. Numerous recommendation algorithms are designed to provide users with a personalized array of information tailored to their preferences. Nevertheless, excessive personalization can confine users within a \"filter bubble\". Consequently, achieving the right balance between accuracy and diversity in recommendations is a pressing concern. To address this challenge, exemplified by music recommendation, we introduce the Diversified Weighted Hypergraph music Recommendation algorithm (DWHRec). In the DWHRec algorithm, the initial connections between users and listened tracks are represented by a weighted hypergraph. Simultaneously, associations between artists, albums and tags with tracks are also appended to the hypergraph. To explore users' latent preferences, a hypergraph-based random walk embedding method is applied to the constructed hypergraph. In our investigation, accuracy is gauged by the alignment between the user and the track, whereas the array of recommended track types measures diversity. We rigorously compared DWHRec against seven state-of-the-art recommendation algorithms using two real-world music datasets. The experimental results validate DWHRec as a solution that adeptly harmonizes accuracy and diversity, delivering a more enriched musical experience. Beyond music recommendation, DWHRec can be extended to cater to other scenarios with similar data structures.","sentences":["Recommender systems serve a dual purpose for users: sifting out inappropriate or mismatched information while accurately identifying items that align with their preferences.","Numerous recommendation algorithms are designed to provide users with a personalized array of information tailored to their preferences.","Nevertheless, excessive personalization can confine users within a \"filter bubble\".","Consequently, achieving the right balance between accuracy and diversity in recommendations is a pressing concern.","To address this challenge, exemplified by music recommendation, we introduce the Diversified Weighted Hypergraph music Recommendation algorithm (DWHRec).","In the DWHRec algorithm, the initial connections between users and listened tracks are represented by a weighted hypergraph.","Simultaneously, associations between artists, albums and tags with tracks are also appended to the hypergraph.","To explore users' latent preferences, a hypergraph-based random walk embedding method is applied to the constructed hypergraph.","In our investigation, accuracy is gauged by the alignment between the user and the track, whereas the array of recommended track types measures diversity.","We rigorously compared DWHRec against seven state-of-the-art recommendation algorithms using two real-world music datasets.","The experimental results validate DWHRec as a solution that adeptly harmonizes accuracy and diversity, delivering a more enriched musical experience.","Beyond music recommendation, DWHRec can be extended to cater to other scenarios with similar data structures."],"url":"http://arxiv.org/abs/2402.16299v1"}
{"created":"2024-02-26 04:39:01","title":"Poisson-Gamma Dynamical Systems with Non-Stationary Transition Dynamics","abstract":"Bayesian methodologies for handling count-valued time series have gained prominence due to their ability to infer interpretable latent structures and to estimate uncertainties, and thus are especially suitable for dealing with noisy and incomplete count data. Among these Bayesian models, Poisson-Gamma Dynamical Systems (PGDSs) are proven to be effective in capturing the evolving dynamics underlying observed count sequences. However, the state-of-the-art PGDS still falls short in capturing the time-varying transition dynamics that are commonly observed in real-world count time series. To mitigate this limitation, a non-stationary PGDS is proposed to allow the underlying transition matrices to evolve over time, and the evolving transition matrices are modeled by sophisticatedly-designed Dirichlet Markov chains. Leveraging Dirichlet-Multinomial-Beta data augmentation techniques, a fully-conjugate and efficient Gibbs sampler is developed to perform posterior simulation. Experiments show that, in comparison with related models, the proposed non-stationary PGDS achieves improved predictive performance due to its capacity to learn non-stationary dependency structure captured by the time-evolving transition matrices.","sentences":["Bayesian methodologies for handling count-valued time series have gained prominence due to their ability to infer interpretable latent structures and to estimate uncertainties, and thus are especially suitable for dealing with noisy and incomplete count data.","Among these Bayesian models, Poisson-Gamma Dynamical Systems (PGDSs) are proven to be effective in capturing the evolving dynamics underlying observed count sequences.","However, the state-of-the-art PGDS still falls short in capturing the time-varying transition dynamics that are commonly observed in real-world count time series.","To mitigate this limitation, a non-stationary PGDS is proposed to allow the underlying transition matrices to evolve over time, and the evolving transition matrices are modeled by sophisticatedly-designed Dirichlet Markov chains.","Leveraging Dirichlet-Multinomial-Beta data augmentation techniques, a fully-conjugate and efficient Gibbs sampler is developed to perform posterior simulation.","Experiments show that, in comparison with related models, the proposed non-stationary PGDS achieves improved predictive performance due to its capacity to learn non-stationary dependency structure captured by the time-evolving transition matrices."],"url":"http://arxiv.org/abs/2402.16297v1"}
{"created":"2024-02-26 04:31:53","title":"Decentralized Federated Unlearning on Blockchain","abstract":"Blockchained Federated Learning (FL) has been gaining traction for ensuring the integrity and traceability of FL processes. Blockchained FL involves participants training models locally with their data and subsequently publishing the models on the blockchain, forming a Directed Acyclic Graph (DAG)-like inheritance structure that represents the model relationship. However, this particular DAG-based structure presents challenges in updating models with sensitive data, due to the complexity and overhead involved. To address this, we propose Blockchained Federated Unlearning (BlockFUL), a generic framework that redesigns the blockchain structure using Chameleon Hash (CH) technology to mitigate the complexity of model updating, thereby reducing the computational and consensus costs of unlearning tasks.Furthermore, BlockFUL supports various federated unlearning methods, ensuring the integrity and traceability of model updates, whether conducted in parallel or serial. We conduct a comprehensive study of two typical unlearning methods, gradient ascent and re-training, demonstrating the efficient unlearning workflow in these two categories with minimal CH and block update operations. Additionally, we compare the computation and communication costs of these methods.","sentences":["Blockchained Federated Learning (FL) has been gaining traction for ensuring the integrity and traceability of FL processes.","Blockchained FL involves participants training models locally with their data and subsequently publishing the models on the blockchain, forming a Directed Acyclic Graph (DAG)-like inheritance structure that represents the model relationship.","However, this particular DAG-based structure presents challenges in updating models with sensitive data, due to the complexity and overhead involved.","To address this, we propose Blockchained Federated Unlearning (BlockFUL), a generic framework that redesigns the blockchain structure using Chameleon Hash (CH) technology to mitigate the complexity of model updating, thereby reducing the computational and consensus costs of unlearning tasks.","Furthermore, BlockFUL supports various federated unlearning methods, ensuring the integrity and traceability of model updates, whether conducted in parallel or serial.","We conduct a comprehensive study of two typical unlearning methods, gradient ascent and re-training, demonstrating the efficient unlearning workflow in these two categories with minimal CH and block update operations.","Additionally, we compare the computation and communication costs of these methods."],"url":"http://arxiv.org/abs/2402.16294v1"}
{"created":"2024-02-26 03:54:32","title":"Towards Agile Robots: Intuitive Robot Position Speculation with Neural Networks","abstract":"The robot position speculation, which determines where the chassis should move, is one key step to control the mobile manipulators. The target position must ensure the feasibility of chassis movement and manipulability, which is guaranteed by randomized sampling and kinematic checking in traditional methods. Addressing the demands of agile robotics, this paper proposes a robot position speculation network(RPSN), a learning-based approach to enhance the agility of mobile manipulators. The RPSN incorporates a differentiable inverse kinematic algorithm and a neural network. Through end-to-end training, the RPSN can speculate positions with a high success rate. We apply the RPSN to mobile manipulators disassembling end-of-life electric vehicle batteries (EOL-EVBs). Extensive experiments on various simulated environments and physical mobile manipulators demonstrate that the probability of the initial position provided by RPSN being the ideal position is 96.67%. From the kinematic constraint perspective, it achieves 100% generation of the ideal position on average within 1.28 attempts. Much lower than that of random sampling, 31.04. Moreover, the proposed method demonstrates superior data efficiency over pure neural network approaches. The proposed RPSN enables the robot to quickly infer feasible target positions by intuition. This work moves towards building agile robots that can act swiftly like humans.","sentences":["The robot position speculation, which determines where the chassis should move, is one key step to control the mobile manipulators.","The target position must ensure the feasibility of chassis movement and manipulability, which is guaranteed by randomized sampling and kinematic checking in traditional methods.","Addressing the demands of agile robotics, this paper proposes a robot position speculation network(RPSN), a learning-based approach to enhance the agility of mobile manipulators.","The RPSN incorporates a differentiable inverse kinematic algorithm and a neural network.","Through end-to-end training, the RPSN can speculate positions with a high success rate.","We apply the RPSN to mobile manipulators disassembling end-of-life electric vehicle batteries (EOL-EVBs).","Extensive experiments on various simulated environments and physical mobile manipulators demonstrate that the probability of the initial position provided by RPSN being the ideal position is 96.67%.","From the kinematic constraint perspective, it achieves 100% generation of the ideal position on average within 1.28 attempts.","Much lower than that of random sampling, 31.04.","Moreover, the proposed method demonstrates superior data efficiency over pure neural network approaches.","The proposed RPSN enables the robot to quickly infer feasible target positions by intuition.","This work moves towards building agile robots that can act swiftly like humans."],"url":"http://arxiv.org/abs/2402.16281v1"}
{"created":"2024-02-26 02:54:04","title":"CoGenT: A Content-oriented Generative-hit Framework for Content Delivery Networks","abstract":"The service provided by content delivery networks (CDNs) may overlook content locality, leaving room for potential performance improvement. In this study, we explore the feasibility of leveraging generated data as a replacement for fetching data in missing scenarios based on content locality. Due to sufficient local computing resources and reliable generation efficiency, we propose a content-oriented generative-hit framework (CoGenT) for CDNs. CoGenT utilizes idle computing resources on edge nodes to generate requested data based on similar or related cached data to achieve hits. Our implementation in a real-world system demonstrates that CoGenT reduces the average access latency by half. Additionally, experiments conducted on a simulator also confirm that CoGenT can enhance existing caching algorithms, resulting in reduced latency and bandwidth usage.","sentences":["The service provided by content delivery networks (CDNs) may overlook content locality, leaving room for potential performance improvement.","In this study, we explore the feasibility of leveraging generated data as a replacement for fetching data in missing scenarios based on content locality.","Due to sufficient local computing resources and reliable generation efficiency, we propose a content-oriented generative-hit framework (CoGenT) for CDNs.","CoGenT utilizes idle computing resources on edge nodes to generate requested data based on similar or related cached data to achieve hits.","Our implementation in a real-world system demonstrates that CoGenT reduces the average access latency by half.","Additionally, experiments conducted on a simulator also confirm that CoGenT can enhance existing caching algorithms, resulting in reduced latency and bandwidth usage."],"url":"http://arxiv.org/abs/2402.16262v1"}
{"created":"2024-02-26 02:44:14","title":"Problems on Group-labeled Matroid Bases","abstract":"Consider a matroid equipped with a labeling of its ground set to an abelian group. We define the label of a subset of the ground set as the sum of the labels of its elements. We study a collection of problems on finding bases and common bases of matroids with restrictions on their labels. For zero bases and zero common bases, the results are mostly negative. While finding a non-zero basis of a matroid is not difficult, it turns out that the complexity of finding a non-zero common basis depends on the group. Namely, we show that the problem is hard for a fixed group if it contains an element of order two, otherwise it is polynomially solvable.   As a generalization of both zero and non-zero constraints, we further study $F$-avoiding constraints where we seek a basis or common basis whose label is not in a given set $F$ of forbidden labels. Using algebraic techniques, we give a randomized algorithm for finding an $F$-avoiding common basis of two matroids represented over the same field for finite groups given as operation tables. The study of $F$-avoiding bases with groups given as oracles leads to a conjecture stating that whenever an $F$-avoiding basis exists, an $F$-avoiding basis can be obtained from an arbitrary basis by exchanging at most $|F|$ elements. We prove the conjecture for the special cases when $|F|\\le 2$ or the group is ordered. By relying on structural observations on matroids representable over fixed, finite fields, we verify a relaxed version of the conjecture for these matroids. As a consequence, we obtain a polynomial-time algorithm in these special cases for finding an $F$-avoiding basis when $|F|$ is fixed.","sentences":["Consider a matroid equipped with a labeling of its ground set to an abelian group.","We define the label of a subset of the ground set as the sum of the labels of its elements.","We study a collection of problems on finding bases and common bases of matroids with restrictions on their labels.","For zero bases and zero common bases, the results are mostly negative.","While finding a non-zero basis of a matroid is not difficult, it turns out that the complexity of finding a non-zero common basis depends on the group.","Namely, we show that the problem is hard for a fixed group if it contains an element of order two, otherwise it is polynomially solvable.   ","As a generalization of both zero and non-zero constraints, we further study $F$-avoiding constraints where we seek a basis or common basis whose label is not in a given set $F$ of forbidden labels.","Using algebraic techniques, we give a randomized algorithm for finding an $F$-avoiding common basis of two matroids represented over the same field for finite groups given as operation tables.","The study of $F$-avoiding bases with groups given as oracles leads to a conjecture stating that whenever an $F$-avoiding basis exists, an $F$-avoiding basis can be obtained from an arbitrary basis by exchanging at most $|F|$ elements.","We prove the conjecture for the special cases when $|F|\\le 2$ or the group is ordered.","By relying on structural observations on matroids representable over fixed, finite fields, we verify a relaxed version of the conjecture for these matroids.","As a consequence, we obtain a polynomial-time algorithm in these special cases for finding an $F$-avoiding basis when $|F|$ is fixed."],"url":"http://arxiv.org/abs/2402.16259v1"}
{"created":"2024-02-26 02:37:39","title":"Watch Your Head: Assembling Projection Heads to Save the Reliability of Federated Models","abstract":"Federated learning encounters substantial challenges with heterogeneous data, leading to performance degradation and convergence issues. While considerable progress has been achieved in mitigating such an impact, the reliability aspect of federated models has been largely disregarded. In this study, we conduct extensive experiments to investigate the reliability of both generic and personalized federated models. Our exploration uncovers a significant finding: \\textbf{federated models exhibit unreliability when faced with heterogeneous data}, demonstrating poor calibration on in-distribution test data and low uncertainty levels on out-of-distribution data. This unreliability is primarily attributed to the presence of biased projection heads, which introduce miscalibration into the federated models. Inspired by this observation, we propose the \"Assembled Projection Heads\" (APH) method for enhancing the reliability of federated models. By treating the existing projection head parameters as priors, APH randomly samples multiple initialized parameters of projection heads from the prior and further performs targeted fine-tuning on locally available data under varying learning rates. Such a head ensemble introduces parameter diversity into the deterministic model, eliminating the bias and producing reliable predictions via head averaging. We evaluate the effectiveness of the proposed APH method across three prominent federated benchmarks. Experimental results validate the efficacy of APH in model calibration and uncertainty estimation. Notably, APH can be seamlessly integrated into various federated approaches but only requires less than 30\\% additional computation cost for 100$\\times$ inferences within large models.","sentences":["Federated learning encounters substantial challenges with heterogeneous data, leading to performance degradation and convergence issues.","While considerable progress has been achieved in mitigating such an impact, the reliability aspect of federated models has been largely disregarded.","In this study, we conduct extensive experiments to investigate the reliability of both generic and personalized federated models.","Our exploration uncovers a significant finding: \\textbf{federated models exhibit unreliability when faced with heterogeneous data}, demonstrating poor calibration on in-distribution test data and low uncertainty levels on out-of-distribution data.","This unreliability is primarily attributed to the presence of biased projection heads, which introduce miscalibration into the federated models.","Inspired by this observation, we propose the \"Assembled Projection Heads\" (APH) method for enhancing the reliability of federated models.","By treating the existing projection head parameters as priors, APH randomly samples multiple initialized parameters of projection heads from the prior and further performs targeted fine-tuning on locally available data under varying learning rates.","Such a head ensemble introduces parameter diversity into the deterministic model, eliminating the bias and producing reliable predictions via head averaging.","We evaluate the effectiveness of the proposed APH method across three prominent federated benchmarks.","Experimental results validate the efficacy of APH in model calibration and uncertainty estimation.","Notably, APH can be seamlessly integrated into various federated approaches but only requires less than 30\\% additional computation cost for 100$\\times$ inferences within large models."],"url":"http://arxiv.org/abs/2402.16255v1"}
{"created":"2024-02-26 02:13:36","title":"Learning Translations: Emergent Communication Pretraining for Cooperative Language Acquisition","abstract":"In Emergent Communication (EC) agents learn to communicate with one another, but the protocols that they develop are specialised to their training community. This observation led to research into Zero-Shot Coordination (ZSC) for learning communication strategies that are robust to agents not encountered during training. However, ZSC typically assumes that no prior data is available about the agents that will be encountered in the zero-shot setting. In many cases, this presents an unnecessarily hard problem and rules out communication via preestablished conventions. We propose a novel AI challenge called a Cooperative Language Acquisition Problem (CLAP) in which the ZSC assumptions are relaxed by allowing a 'joiner' agent to learn from a dataset of interactions between agents in a target community. We propose and compare two methods for solving CLAPs: Imitation Learning (IL), and Emergent Communication pretraining and Translation Learning (ECTL), in which an agent is trained in self-play with EC and then learns from the data to translate between the emergent protocol and the target community's protocol.","sentences":["In Emergent Communication (EC) agents learn to communicate with one another, but the protocols that they develop are specialised to their training community.","This observation led to research into Zero-Shot Coordination (ZSC) for learning communication strategies that are robust to agents not encountered during training.","However, ZSC typically assumes that no prior data is available about the agents that will be encountered in the zero-shot setting.","In many cases, this presents an unnecessarily hard problem and rules out communication via preestablished conventions.","We propose a novel AI challenge called a Cooperative Language Acquisition Problem (CLAP) in which the ZSC assumptions are relaxed by allowing a 'joiner' agent to learn from a dataset of interactions between agents in a target community.","We propose and compare two methods for solving CLAPs: Imitation Learning (IL), and Emergent Communication pretraining and Translation Learning (ECTL), in which an agent is trained in self-play with EC and then learns from the data to translate between the emergent protocol and the target community's protocol."],"url":"http://arxiv.org/abs/2402.16247v1"}
{"created":"2024-02-26 02:09:36","title":"Real-Time Vehicle Detection and Urban Traffic Behavior Analysis Based on UAV Traffic Videos on Mobile Devices","abstract":"This paper focuses on a real-time vehicle detection and urban traffic behavior analysis system based on Unmanned Aerial Vehicle (UAV) traffic video. By using UAV to collect traffic data and combining the YOLOv8 model and SORT tracking algorithm, the object detection and tracking functions are implemented on the iOS mobile platform. For the problem of traffic data acquisition and analysis, the dynamic computing method is used to process the performance in real time and calculate the micro and macro traffic parameters of the vehicles, and real-time traffic behavior analysis is conducted and visualized. The experiment results reveals that the vehicle object detection can reach 98.27% precision rate and 87.93% recall rate, and the real-time processing capacity is stable at 30 frames per seconds. This work integrates drone technology, iOS development, and deep learning techniques to integrate traffic video acquisition, object detection, object tracking, and traffic behavior analysis functions on mobile devices. It provides new possibilities for lightweight traffic information collection and data analysis, and offers innovative solutions to improve the efficiency of analyzing road traffic conditions and addressing transportation issues for transportation authorities.","sentences":["This paper focuses on a real-time vehicle detection and urban traffic behavior analysis system based on Unmanned Aerial Vehicle (UAV) traffic video.","By using UAV to collect traffic data and combining the YOLOv8 model and SORT tracking algorithm, the object detection and tracking functions are implemented on the iOS mobile platform.","For the problem of traffic data acquisition and analysis, the dynamic computing method is used to process the performance in real time and calculate the micro and macro traffic parameters of the vehicles, and real-time traffic behavior analysis is conducted and visualized.","The experiment results reveals that the vehicle object detection can reach 98.27% precision rate and 87.93% recall rate, and the real-time processing capacity is stable at 30 frames per seconds.","This work integrates drone technology, iOS development, and deep learning techniques to integrate traffic video acquisition, object detection, object tracking, and traffic behavior analysis functions on mobile devices.","It provides new possibilities for lightweight traffic information collection and data analysis, and offers innovative solutions to improve the efficiency of analyzing road traffic conditions and addressing transportation issues for transportation authorities."],"url":"http://arxiv.org/abs/2402.16246v1"}
{"created":"2024-02-26 01:46:56","title":"Active Level Set Estimation for Continuous Search Space with Theoretical Guarantee","abstract":"A common problem encountered in many real-world applications is level set estimation where the goal is to determine the region in the function domain where the function is above or below a given threshold. When the function is black-box and expensive to evaluate, the level sets need to be found in a minimum set of function evaluations. Existing methods often assume a discrete search space with a finite set of data points for function evaluations and estimating the level sets. When applied to a continuous search space, these methods often need to first discretize the space which leads to poor results while needing high computational time. While some methods cater for the continuous setting, they still lack a proper guarantee for theoretical convergence. To address this problem, we propose a novel algorithm that does not need any discretization and can directly work in continuous search spaces. Our method suggests points by constructing an acquisition function that is defined as a measure of confidence of the function being higher or lower than the given threshold. A theoretical analysis for the convergence of the algorithm to an accurate solution is provided. On multiple synthetic and real-world datasets, our algorithm successfully outperforms state-of-the-art methods.","sentences":["A common problem encountered in many real-world applications is level set estimation where the goal is to determine the region in the function domain where the function is above or below a given threshold.","When the function is black-box and expensive to evaluate, the level sets need to be found in a minimum set of function evaluations.","Existing methods often assume a discrete search space with a finite set of data points for function evaluations and estimating the level sets.","When applied to a continuous search space, these methods often need to first discretize the space which leads to poor results while needing high computational time.","While some methods cater for the continuous setting, they still lack a proper guarantee for theoretical convergence.","To address this problem, we propose a novel algorithm that does not need any discretization and can directly work in continuous search spaces.","Our method suggests points by constructing an acquisition function that is defined as a measure of confidence of the function being higher or lower than the given threshold.","A theoretical analysis for the convergence of the algorithm to an accurate solution is provided.","On multiple synthetic and real-world datasets, our algorithm successfully outperforms state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.16237v1"}
{"created":"2024-02-26 01:18:53","title":"GARNN: An Interpretable Graph Attentive Recurrent Neural Network for Predicting Blood Glucose Levels via Multivariate Time Series","abstract":"Accurate prediction of future blood glucose (BG) levels can effectively improve BG management for people living with diabetes, thereby reducing complications and improving quality of life. The state of the art of BG prediction has been achieved by leveraging advanced deep learning methods to model multi-modal data, i.e., sensor data and self-reported event data, organised as multi-variate time series (MTS). However, these methods are mostly regarded as ``black boxes'' and not entirely trusted by clinicians and patients. In this paper, we propose interpretable graph attentive recurrent neural networks (GARNNs) to model MTS, explaining variable contributions via summarizing variable importance and generating feature maps by graph attention mechanisms instead of post-hoc analysis. We evaluate GARNNs on four datasets, representing diverse clinical scenarios. Upon comparison with twelve well-established baseline methods, GARNNs not only achieve the best prediction accuracy but also provide high-quality temporal interpretability, in particular for postprandial glucose levels as a result of corresponding meal intake and insulin injection. These findings underline the potential of GARNN as a robust tool for improving diabetes care, bridging the gap between deep learning technology and real-world healthcare solutions.","sentences":["Accurate prediction of future blood glucose (BG) levels can effectively improve BG management for people living with diabetes, thereby reducing complications and improving quality of life.","The state of the art of BG prediction has been achieved by leveraging advanced deep learning methods to model multi-modal data, i.e., sensor data and self-reported event data, organised as multi-variate time series (MTS).","However, these methods are mostly regarded as ``black boxes'' and not entirely trusted by clinicians and patients.","In this paper, we propose interpretable graph attentive recurrent neural networks (GARNNs) to model MTS, explaining variable contributions via summarizing variable importance and generating feature maps by graph attention mechanisms instead of post-hoc analysis.","We evaluate GARNNs on four datasets, representing diverse clinical scenarios.","Upon comparison with twelve well-established baseline methods, GARNNs not only achieve the best prediction accuracy but also provide high-quality temporal interpretability, in particular for postprandial glucose levels as a result of corresponding meal intake and insulin injection.","These findings underline the potential of GARNN as a robust tool for improving diabetes care, bridging the gap between deep learning technology and real-world healthcare solutions."],"url":"http://arxiv.org/abs/2402.16230v1"}
{"created":"2024-02-25 21:58:52","title":"Enhanced Graph Pattern Matching","abstract":"Pattern matching queries on strings can be solved in linear time by Knuth-Morris-Pratt (KMP) algorithm. In 1973, Weiner introduced the suffix tree of a string [FOCS 1973] and showed that the seemingly more difficult problem of computing matching statistics can also be solved in liner time. Pattern matching queries on graphs are inherently more difficult: under the Orthogonal Vector hypothesis, the graph pattern matching problem cannot be solved in subquadratic time [TALG 2023]. The complexity of graph pattern matching can be parameterized by the topological complexity of the considered graph, which is captured by a parameter $ p $ [JACM 2023].   In this paper, we show that, as in the string setting, computing matching statistics on graph is as difficult as solving standard pattern matching queries. To this end, we introduce a notion of longest common prefix (LCP) array for arbitrary graphs.","sentences":["Pattern matching queries on strings can be solved in linear time by Knuth-Morris-Pratt (KMP) algorithm.","In 1973, Weiner introduced the suffix tree of a string [FOCS 1973] and showed that the seemingly more difficult problem of computing matching statistics can also be solved in liner time.","Pattern matching queries on graphs are inherently more difficult: under the Orthogonal Vector hypothesis, the graph pattern matching problem cannot be solved in subquadratic time [TALG 2023].","The complexity of graph pattern matching can be parameterized by the topological complexity of the considered graph, which is captured by a parameter $ p $","[JACM 2023].   ","In this paper, we show that, as in the string setting, computing matching statistics on graph is as difficult as solving standard pattern matching queries.","To this end, we introduce a notion of longest common prefix (LCP) array for arbitrary graphs."],"url":"http://arxiv.org/abs/2402.16205v1"}
{"created":"2024-02-25 21:29:44","title":"Honeybee: Decentralized Peer Sampling with Verifiable Random Walks for Blockchain Data Sharding","abstract":"Data sharding - in which block data is sharded without sharding compute - is at the present the favored approach for scaling Ethereum. A key challenge toward implementing data sharding is verifying whether the entirety of a block's data is available in the network (across its shards). A central technique proposed to conduct this verification uses erasure coded blocks and is called data availability sampling (DAS). While the high-level protocol details of DAS has been well discussed in the community, discussions around how such a protocol will be implemented at the peer-to-peer layer are lacking. We identify random sampling of nodes as a fundamental primitive necessary to carry out DAS and present Honeybee, a decentralized algorithm for sampling node that uses verifiable random walks. Honeybee is secure against attacks even in the presence of a large number of Byzantine nodes (e.g., 50% of the network). We evaluate Honeybee through experiments and show that the quality of sampling achieved by Honeybee is significantly better compared to the state-of-the-art. Our proposed algorithm has implications for DAS functions in both full nodes and light nodes.","sentences":["Data sharding - in which block data is sharded without sharding compute - is at the present the favored approach for scaling Ethereum.","A key challenge toward implementing data sharding is verifying whether the entirety of a block's data is available in the network (across its shards).","A central technique proposed to conduct this verification uses erasure coded blocks and is called data availability sampling (DAS).","While the high-level protocol details of DAS has been well discussed in the community, discussions around how such a protocol will be implemented at the peer-to-peer layer are lacking.","We identify random sampling of nodes as a fundamental primitive necessary to carry out DAS and present Honeybee, a decentralized algorithm for sampling node that uses verifiable random walks.","Honeybee is secure against attacks even in the presence of a large number of Byzantine nodes (e.g., 50% of the network).","We evaluate Honeybee through experiments and show that the quality of sampling achieved by Honeybee is significantly better compared to the state-of-the-art.","Our proposed algorithm has implications for DAS functions in both full nodes and light nodes."],"url":"http://arxiv.org/abs/2402.16201v1"}
{"created":"2024-02-25 21:25:06","title":"IR2: Information Regularization for Information Retrieval","abstract":"Effective information retrieval (IR) in settings with limited training data, particularly for complex queries, remains a challenging task. This paper introduces IR2, Information Regularization for Information Retrieval, a technique for reducing overfitting during synthetic data generation. This approach, representing a novel application of regularization techniques in synthetic data creation for IR, is tested on three recent IR tasks characterized by complex queries: DORIS-MAE, ArguAna, and WhatsThatBook. Experimental results indicate that our regularization techniques not only outperform previous synthetic query generation methods on the tasks considered but also reduce cost by up to 50%. Furthermore, this paper categorizes and explores three regularization methods at different stages of the query synthesis pipeline-input, prompt, and output-each offering varying degrees of performance improvement compared to models where no regularization is applied. This provides a systematic approach for optimizing synthetic data generation in data-limited, complex-query IR scenarios. All code, prompts and synthetic data are available at https://github.com/Info-Regularization/Information-Regularization.","sentences":["Effective information retrieval (IR) in settings with limited training data, particularly for complex queries, remains a challenging task.","This paper introduces IR2, Information Regularization for Information Retrieval, a technique for reducing overfitting during synthetic data generation.","This approach, representing a novel application of regularization techniques in synthetic data creation for IR, is tested on three recent IR tasks characterized by complex queries: DORIS-MAE, ArguAna, and WhatsThatBook.","Experimental results indicate that our regularization techniques not only outperform previous synthetic query generation methods on the tasks considered but also reduce cost by up to 50%.","Furthermore, this paper categorizes and explores three regularization methods at different stages of the query synthesis pipeline-input, prompt, and output-each offering varying degrees of performance improvement compared to models where no regularization is applied.","This provides a systematic approach for optimizing synthetic data generation in data-limited, complex-query IR scenarios.","All code, prompts and synthetic data are available at https://github.com/Info-Regularization/Information-Regularization."],"url":"http://arxiv.org/abs/2402.16200v1"}
{"created":"2024-02-25 20:43:55","title":"Language Models for Code Completion: A Practical Evaluation","abstract":"Transformer-based language models for automatic code completion have shown great promise so far, yet the evaluation of these models rarely uses real data. This study provides both quantitative and qualitative assessments of three public code language models when completing real-world code. We first developed an open-source IDE extension, Code4Me, for the online evaluation of the models. We collected real auto-completion usage data for over a year from more than 1200 users, resulting in over 600K valid completions. These models were then evaluated using six standard metrics across twelve programming languages. Next, we conducted a qualitative study of 1690 real-world completion requests to identify the reasons behind the poor model performance. A comparative analysis of the models' performance in online and offline settings was also performed, using benchmark synthetic datasets and two masking strategies. Our findings suggest that while developers utilize code completion across various languages, the best results are achieved for mainstream languages such as Python and Java. InCoder outperformed the other models across all programming languages, highlighting the significance of training data and objectives. Our study also revealed that offline evaluations do not accurately reflect real-world scenarios. Upon qualitative analysis of the model's predictions, we found that 66.3% of failures were due to the models' limitations, 24.4% occurred due to inappropriate model usage in a development context, and 9.3% were valid requests that developers overwrote. Given these findings, we propose several strategies to overcome the current limitations. These include refining training objectives, improving resilience to typographical errors, adopting hybrid approaches, and enhancing implementations and usability.","sentences":["Transformer-based language models for automatic code completion have shown great promise so far, yet the evaluation of these models rarely uses real data.","This study provides both quantitative and qualitative assessments of three public code language models when completing real-world code.","We first developed an open-source IDE extension, Code4Me, for the online evaluation of the models.","We collected real auto-completion usage data for over a year from more than 1200 users, resulting in over 600K valid completions.","These models were then evaluated using six standard metrics across twelve programming languages.","Next, we conducted a qualitative study of 1690 real-world completion requests to identify the reasons behind the poor model performance.","A comparative analysis of the models' performance in online and offline settings was also performed, using benchmark synthetic datasets and two masking strategies.","Our findings suggest that while developers utilize code completion across various languages, the best results are achieved for mainstream languages such as Python and Java.","InCoder outperformed the other models across all programming languages, highlighting the significance of training data and objectives.","Our study also revealed that offline evaluations do not accurately reflect real-world scenarios.","Upon qualitative analysis of the model's predictions, we found that 66.3% of failures were due to the models' limitations, 24.4% occurred due to inappropriate model usage in a development context, and 9.3% were valid requests that developers overwrote.","Given these findings, we propose several strategies to overcome the current limitations.","These include refining training objectives, improving resilience to typographical errors, adopting hybrid approaches, and enhancing implementations and usability."],"url":"http://arxiv.org/abs/2402.16197v1"}
{"created":"2024-02-25 20:39:44","title":"Combining Machine Learning with Computational Fluid Dynamics using OpenFOAM and SmartSim","abstract":"Combining machine learning (ML) with computational fluid dynamics (CFD) opens many possibilities for improving simulations of technical and natural systems. However, CFD+ML algorithms require exchange of data, synchronization, and calculation on heterogeneous hardware, making their implementation for large-scale problems exceptionally challenging.   We provide an effective and scalable solution to developing CFD+ML algorithms using open source software OpenFOAM and SmartSim. SmartSim provides an Orchestrator that significantly simplifies the programming of CFD+ML algorithms and a Redis database that ensures highly scalable data exchange between ML and CFD clients. We show how to leverage SmartSim to effectively couple different segments of OpenFOAM with ML, including pre/post-processing applications, solvers, function objects, and mesh motion solvers. We additionally provide an OpenFOAM sub-module with examples that can be used as starting points for real-world applications in CFD+ML.","sentences":["Combining machine learning (ML) with computational fluid dynamics (CFD) opens many possibilities for improving simulations of technical and natural systems.","However, CFD+ML algorithms require exchange of data, synchronization, and calculation on heterogeneous hardware, making their implementation for large-scale problems exceptionally challenging.   ","We provide an effective and scalable solution to developing CFD+ML algorithms using open source software OpenFOAM and SmartSim.","SmartSim provides an Orchestrator that significantly simplifies the programming of CFD+ML algorithms and a Redis database that ensures highly scalable data exchange between ML and CFD clients.","We show how to leverage SmartSim to effectively couple different segments of OpenFOAM with ML, including pre/post-processing applications, solvers, function objects, and mesh motion solvers.","We additionally provide an OpenFOAM sub-module with examples that can be used as starting points for real-world applications in CFD+ML."],"url":"http://arxiv.org/abs/2402.16196v1"}
{"created":"2024-02-25 20:30:18","title":"Accurate predictions of keyhole depths using machine learning-aided simulations","abstract":"The keyhole phenomenon is widely observed in laser materials processing, including laser welding, remelting, cladding, drilling, and additive manufacturing. Keyhole-induced defects, primarily pores, dramatically affect the performance of final products, impeding the broad use of these laser-based technologies. The formation of these pores is typically associated with the dynamic behavior of the keyhole. So far, the accurate characterization and prediction of keyhole features, particularly keyhole depth, as a function of time has been a challenging task. In situ characterization of keyhole dynamic behavior using a synchrotron X-ray is complicated and expensive. Current simulations are hindered by their poor accuracies in predicting keyhole depths due to the lack of real-time laser absorptance data. Here, we develop a machine learning-aided simulation method that allows us to accurately predict keyhole depth over a wide range of processing parameters. Based on titanium and aluminum alloys, two commonly used engineering materials as examples, we achieve an accuracy with an error margin of 10 %, surpassing those simulated using other existing models (with an error margin in a range of 50-200 %). Our machine learning-aided simulation method is affordable and readily deployable for a large variety of materials, opening new doors to eliminate or reduce defects for a wide range of laser materials processing techniques.","sentences":["The keyhole phenomenon is widely observed in laser materials processing, including laser welding, remelting, cladding, drilling, and additive manufacturing.","Keyhole-induced defects, primarily pores, dramatically affect the performance of final products, impeding the broad use of these laser-based technologies.","The formation of these pores is typically associated with the dynamic behavior of the keyhole.","So far, the accurate characterization and prediction of keyhole features, particularly keyhole depth, as a function of time has been a challenging task.","In situ characterization of keyhole dynamic behavior using a synchrotron X-ray is complicated and expensive.","Current simulations are hindered by their poor accuracies in predicting keyhole depths due to the lack of real-time laser absorptance data.","Here, we develop a machine learning-aided simulation method that allows us to accurately predict keyhole depth over a wide range of processing parameters.","Based on titanium and aluminum alloys, two commonly used engineering materials as examples, we achieve an accuracy with an error margin of 10 %, surpassing those simulated using other existing models (with an error margin in a range of 50-200 %).","Our machine learning-aided simulation method is affordable and readily deployable for a large variety of materials, opening new doors to eliminate or reduce defects for a wide range of laser materials processing techniques."],"url":"http://arxiv.org/abs/2402.16190v1"}
{"created":"2024-02-25 20:07:13","title":"How Can LLM Guide RL? A Value-Based Approach","abstract":"Reinforcement learning (RL) has become the de facto standard practice for sequential decision-making problems by improving future acting policies with feedback. However, RL algorithms may require extensive trial-and-error interactions to collect useful feedback for improvement. On the other hand, recent developments in large language models (LLMs) have showcased impressive capabilities in language understanding and generation, yet they fall short in exploration and self-improvement capabilities for planning tasks, lacking the ability to autonomously refine their responses based on feedback. Therefore, in this paper, we study how the policy prior provided by the LLM can enhance the sample efficiency of RL algorithms. Specifically, we develop an algorithm named LINVIT that incorporates LLM guidance as a regularization factor in value-based RL, leading to significant reductions in the amount of data needed for learning, particularly when the difference between the ideal policy and the LLM-informed policy is small, which suggests that the initial policy is close to optimal, reducing the need for further exploration. Additionally, we present a practical algorithm SLINVIT that simplifies the construction of the value function and employs subgoals to reduce the search complexity. Our experiments across three interactive environments ALFWorld, InterCode, and BlocksWorld demonstrate that our method achieves state-of-the-art success rates and also surpasses previous RL and LLM approaches in terms of sample efficiency. Our code is available at https://github.com/agentification/Language-Integrated-VI.","sentences":["Reinforcement learning (RL) has become the de facto standard practice for sequential decision-making problems by improving future acting policies with feedback.","However, RL algorithms may require extensive trial-and-error interactions to collect useful feedback for improvement.","On the other hand, recent developments in large language models (LLMs) have showcased impressive capabilities in language understanding and generation, yet they fall short in exploration and self-improvement capabilities for planning tasks, lacking the ability to autonomously refine their responses based on feedback.","Therefore, in this paper, we study how the policy prior provided by the LLM can enhance the sample efficiency of RL algorithms.","Specifically, we develop an algorithm named LINVIT that incorporates LLM guidance as a regularization factor in value-based RL, leading to significant reductions in the amount of data needed for learning, particularly when the difference between the ideal policy and the LLM-informed policy is small, which suggests that the initial policy is close to optimal, reducing the need for further exploration.","Additionally, we present a practical algorithm SLINVIT that simplifies the construction of the value function and employs subgoals to reduce the search complexity.","Our experiments across three interactive environments ALFWorld, InterCode, and BlocksWorld demonstrate that our method achieves state-of-the-art success rates and also surpasses previous RL and LLM approaches in terms of sample efficiency.","Our code is available at https://github.com/agentification/Language-Integrated-VI."],"url":"http://arxiv.org/abs/2402.16181v1"}
{"created":"2024-02-25 17:40:49","title":"DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem","abstract":"This paper proposes a novel named entity recognition (NER) technique specifically tailored for the open-source software systems. Our approach aims to address the scarcity of annotated software data by employing a comprehensive two-step distantly supervised annotation process. This process strategically leverages language heuristics, unique lookup tables, external knowledge sources, and an active learning approach. By harnessing these powerful techniques, we not only enhance model performance but also effectively mitigate the limitations associated with cost and the scarcity of expert annotators. It is noteworthy that our framework significantly outperforms the state-of-the-art LLMs by a substantial margin. We also show the effectiveness of NER in the downstream task of relation extraction.","sentences":["This paper proposes a novel named entity recognition (NER) technique specifically tailored for the open-source software systems.","Our approach aims to address the scarcity of annotated software data by employing a comprehensive two-step distantly supervised annotation process.","This process strategically leverages language heuristics, unique lookup tables, external knowledge sources, and an active learning approach.","By harnessing these powerful techniques, we not only enhance model performance but also effectively mitigate the limitations associated with cost and the scarcity of expert annotators.","It is noteworthy that our framework significantly outperforms the state-of-the-art LLMs by a substantial margin.","We also show the effectiveness of NER in the downstream task of relation extraction."],"url":"http://arxiv.org/abs/2402.16159v1"}
{"created":"2024-02-25 17:35:31","title":"Consensus learning: A novel decentralised ensemble learning paradigm","abstract":"The widespread adoption of large-scale machine learning models in recent years highlights the need for distributed computing for efficiency and scalability. This work introduces a novel distributed machine learning paradigm -- \\emph{consensus learning} -- which combines classical ensemble methods with consensus protocols deployed in peer-to-peer systems. These algorithms consist of two phases: first, participants develop their models and submit predictions for any new data inputs; second, the individual predictions are used as inputs for a communication phase, which is governed by a consensus protocol. Consensus learning ensures user data privacy, while also inheriting the safety measures against Byzantine attacks from the underlying consensus mechanism. We provide a detailed theoretical analysis for a particular consensus protocol and compare the performance of the consensus learning ensemble with centralised ensemble learning algorithms. The discussion is supplemented by various numerical simulations, which describe the robustness of the algorithms against Byzantine participants.","sentences":["The widespread adoption of large-scale machine learning models in recent years highlights the need for distributed computing for efficiency and scalability.","This work introduces a novel distributed machine learning paradigm -- \\emph{consensus learning} -- which combines classical ensemble methods with consensus protocols deployed in peer-to-peer systems.","These algorithms consist of two phases: first, participants develop their models and submit predictions for any new data inputs; second, the individual predictions are used as inputs for a communication phase, which is governed by a consensus protocol.","Consensus learning ensures user data privacy, while also inheriting the safety measures against Byzantine attacks from the underlying consensus mechanism.","We provide a detailed theoretical analysis for a particular consensus protocol and compare the performance of the consensus learning ensemble with centralised ensemble learning algorithms.","The discussion is supplemented by various numerical simulations, which describe the robustness of the algorithms against Byzantine participants."],"url":"http://arxiv.org/abs/2402.16157v1"}
{"created":"2024-02-25 16:48:25","title":"Cinematographic Camera Diffusion Model","abstract":"Designing effective camera trajectories in virtual 3D environments is a challenging task even for experienced animators. Despite an elaborate film grammar, forged through years of experience, that enables the specification of camera motions through cinematographic properties (framing, shots sizes, angles, motions), there are endless possibilities in deciding how to place and move cameras with characters. Dealing with these possibilities is part of the complexity of the problem. While numerous techniques have been proposed in the literature (optimization-based solving, encoding of empirical rules, learning from real examples,...), the results either lack variety or ease of control.   In this paper, we propose a cinematographic camera diffusion model using a transformer-based architecture to handle temporality and exploit the stochasticity of diffusion models to generate diverse and qualitative trajectories conditioned by high-level textual descriptions. We extend the work by integrating keyframing constraints and the ability to blend naturally between motions using latent interpolation, in a way to augment the degree of control of the designers. We demonstrate the strengths of this text-to-camera motion approach through qualitative and quantitative experiments and gather feedback from professional artists. The code and data are available at \\URL{https://github.com/jianghd1996/Camera-control}.","sentences":["Designing effective camera trajectories in virtual 3D environments is a challenging task even for experienced animators.","Despite an elaborate film grammar, forged through years of experience, that enables the specification of camera motions through cinematographic properties (framing, shots sizes, angles, motions), there are endless possibilities in deciding how to place and move cameras with characters.","Dealing with these possibilities is part of the complexity of the problem.","While numerous techniques have been proposed in the literature (optimization-based solving, encoding of empirical rules, learning from real examples,...), the results either lack variety or ease of control.   ","In this paper, we propose a cinematographic camera diffusion model using a transformer-based architecture to handle temporality and exploit the stochasticity of diffusion models to generate diverse and qualitative trajectories conditioned by high-level textual descriptions.","We extend the work by integrating keyframing constraints and the ability to blend naturally between motions using latent interpolation, in a way to augment the degree of control of the designers.","We demonstrate the strengths of this text-to-camera motion approach through qualitative and quantitative experiments and gather feedback from professional artists.","The code and data are available at \\URL{https://github.com/jianghd1996/Camera-control}."],"url":"http://arxiv.org/abs/2402.16143v1"}
{"created":"2024-02-25 16:11:32","title":"A VAE-based Framework for Learning Multi-Level Neural Granger-Causal Connectivity","abstract":"Granger causality has been widely used in various application domains to capture lead-lag relationships amongst the components of complex dynamical systems, and the focus in extant literature has been on a single dynamical system. In certain applications in macroeconomics and neuroscience, one has access to data from a collection of related such systems, wherein the modeling task of interest is to extract the shared common structure that is embedded across them, as well as to identify the idiosyncrasies within individual ones. This paper introduces a Variational Autoencoder (VAE) based framework that jointly learns Granger-causal relationships amongst components in a collection of related-yet-heterogeneous dynamical systems, and handles the aforementioned task in a principled way. The performance of the proposed framework is evaluated on several synthetic data settings and benchmarked against existing approaches designed for individual system learning. The method is further illustrated on a real dataset involving time series data from a neurophysiological experiment and produces interpretable results.","sentences":["Granger causality has been widely used in various application domains to capture lead-lag relationships amongst the components of complex dynamical systems, and the focus in extant literature has been on a single dynamical system.","In certain applications in macroeconomics and neuroscience, one has access to data from a collection of related such systems, wherein the modeling task of interest is to extract the shared common structure that is embedded across them, as well as to identify the idiosyncrasies within individual ones.","This paper introduces a Variational Autoencoder (VAE) based framework that jointly learns Granger-causal relationships amongst components in a collection of related-yet-heterogeneous dynamical systems, and handles the aforementioned task in a principled way.","The performance of the proposed framework is evaluated on several synthetic data settings and benchmarked against existing approaches designed for individual system learning.","The method is further illustrated on a real dataset involving time series data from a neurophysiological experiment and produces interpretable results."],"url":"http://arxiv.org/abs/2402.16131v1"}
{"created":"2024-02-25 15:37:14","title":"DeepForge: Leveraging AI for Microstructural Control in Metal Forming via Model Predictive Control","abstract":"This study presents a novel method for microstructure control in closed die hot forging that combines Model Predictive Control (MPC) with a developed machine learning model called DeepForge. DeepForge uses an architecture that combines 1D convolutional neural networks and gated recurrent units. It uses surface temperature measurements of a workpiece as input to predict microstructure changes during forging. The paper also details DeepForge's architecture and the finite element simulation model used to generate the data set, using a three-stroke forging process. The results demonstrate DeepForge's ability to predict microstructure with a mean absolute error of 0.4$\\pm$0.3%. In addition, the study explores the use of MPC to adjust inter-stroke wait times, effectively counteracting temperature disturbances to achieve a target grain size of less than 35 microns within a specific 2D region of the workpiece. These results are then verified experimentally, demonstrating a significant step towards improved control and quality in forging processes where temperature can be used as an additional degree of freedom in the process.","sentences":["This study presents a novel method for microstructure control in closed die hot forging that combines Model Predictive Control (MPC) with a developed machine learning model called DeepForge.","DeepForge uses an architecture that combines 1D convolutional neural networks and gated recurrent units.","It uses surface temperature measurements of a workpiece as input to predict microstructure changes during forging.","The paper also details DeepForge's architecture and the finite element simulation model used to generate the data set, using a three-stroke forging process.","The results demonstrate DeepForge's ability to predict microstructure with a mean absolute error of 0.4$\\pm$0.3%.","In addition, the study explores the use of MPC to adjust inter-stroke wait times, effectively counteracting temperature disturbances to achieve a target grain size of less than 35 microns within a specific 2D region of the workpiece.","These results are then verified experimentally, demonstrating a significant step towards improved control and quality in forging processes where temperature can be used as an additional degree of freedom in the process."],"url":"http://arxiv.org/abs/2402.16119v1"}
{"created":"2024-02-25 15:11:58","title":"FuseChat: Knowledge Fusion of Chat Models","abstract":"While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, this approach incurs substantial costs and may lead to potential redundancy in competencies. An alternative strategy is to combine existing LLMs into a more robust LLM, thereby diminishing the necessity for expensive pre-training. However, due to the diverse architectures of LLMs, direct parameter blending proves to be unfeasible. Recently, \\textsc{FuseLLM} introduced the concept of knowledge fusion to transfer the collective knowledge of multiple structurally varied LLMs into a target LLM through lightweight continual training. In this report, we extend the scalability and flexibility of the \\textsc{FuseLLM} framework to realize the fusion of chat LLMs, resulting in \\textsc{FuseChat}. \\textsc{FuseChat} comprises two main stages. Firstly, we undertake knowledge fusion for structurally and scale-varied source LLMs to derive multiple target LLMs of identical structure and size via lightweight fine-tuning. Then, these target LLMs are merged within the parameter space, wherein we propose a novel method for determining the merging weights based on the variation ratio of parameter matrices before and after fine-tuning. We validate our approach using three prominent chat LLMs with diverse architectures and scales, namely \\texttt{NH2-Mixtral-8x7B}, \\texttt{NH2-Solar-10.7B}, and \\texttt{OpenChat-3.5-7B}. Experimental results spanning various chat domains demonstrate the superiority of \\texttt{\\textsc{FuseChat}-7B} across a broad spectrum of chat LLMs at 7B and 34B scales, even surpassing \\texttt{GPT-3.5 (March)} and approaching \\texttt{Mixtral-8x7B-Instruct}. Our code, model weights, and data are openly accessible at \\url{https://github.com/fanqiwan/FuseLLM}.","sentences":["While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, this approach incurs substantial costs and may lead to potential redundancy in competencies.","An alternative strategy is to combine existing LLMs into a more robust LLM, thereby diminishing the necessity for expensive pre-training.","However, due to the diverse architectures of LLMs, direct parameter blending proves to be unfeasible.","Recently, \\textsc{FuseLLM} introduced the concept of knowledge fusion to transfer the collective knowledge of multiple structurally varied LLMs into a target LLM through lightweight continual training.","In this report, we extend the scalability and flexibility of the \\textsc{FuseLLM} framework to realize the fusion of chat LLMs, resulting in \\textsc{FuseChat}.","\\textsc{FuseChat} comprises two main stages.","Firstly, we undertake knowledge fusion for structurally and scale-varied source LLMs to derive multiple target LLMs of identical structure and size via lightweight fine-tuning.","Then, these target LLMs are merged within the parameter space, wherein we propose a novel method for determining the merging weights based on the variation ratio of parameter matrices before and after fine-tuning.","We validate our approach using three prominent chat LLMs with diverse architectures and scales, namely \\texttt{NH2-Mixtral-8x7B}, \\texttt{NH2-Solar-10.7B}, and \\texttt{OpenChat-3.5-7B}.","Experimental results spanning various chat domains demonstrate the superiority of \\texttt{\\textsc{FuseChat}-7B} across a broad spectrum of chat LLMs at 7B and 34B scales, even surpassing \\texttt{GPT-3.5 (March)} and approaching \\texttt{Mixtral-8x7B-Instruct}.","Our code, model weights, and data are openly accessible at \\url{https://github.com/fanqiwan/FuseLLM}."],"url":"http://arxiv.org/abs/2402.16107v1"}
{"created":"2024-02-25 15:08:37","title":"Informed Meta-Learning","abstract":"In noisy and low-data regimes prevalent in real-world applications, an outstanding challenge of machine learning lies in effectively incorporating inductive biases that promote data efficiency and robustness. Meta-learning and informed ML stand out as two approaches for incorporating prior knowledge into the ML pipeline. While the former relies on a purely data-driven source of priors, the latter is guided by a formal representation of expert knowledge. This paper introduces a novel hybrid paradigm, informed meta-learning, seeking complementarity in cross-task knowledge sharing of humans and machines. We establish the foundational components of informed meta-learning and present a concrete instantiation of this framework--the Informed Neural Process. Through a series of illustrative and larger-scale experiments, we demonstrate the potential benefits of informed meta-learning in improving data efficiency and robustness to observational noise, task distribution shifts, and heterogeneity.","sentences":["In noisy and low-data regimes prevalent in real-world applications, an outstanding challenge of machine learning lies in effectively incorporating inductive biases that promote data efficiency and robustness.","Meta-learning and informed ML stand out as two approaches for incorporating prior knowledge into the ML pipeline.","While the former relies on a purely data-driven source of priors, the latter is guided by a formal representation of expert knowledge.","This paper introduces a novel hybrid paradigm, informed meta-learning, seeking complementarity in cross-task knowledge sharing of humans and machines.","We establish the foundational components of informed meta-learning and present a concrete instantiation of this framework--the Informed Neural Process.","Through a series of illustrative and larger-scale experiments, we demonstrate the potential benefits of informed meta-learning in improving data efficiency and robustness to observational noise, task distribution shifts, and heterogeneity."],"url":"http://arxiv.org/abs/2402.16105v1"}
{"created":"2024-02-25 15:00:06","title":"Optimizing Base Placement of Surgical Robot: Kinematics Data-Driven Approach by Analyzing Working Pattern","abstract":"In robot-assisted minimally invasive surgery (RAMIS), optimal placement of the surgical robot's base is crucial for successful surgery. Improper placement can hinder performance due to manipulator limitations and inaccessible workspaces. Traditionally, trained medical staff rely on experience for base placement, but this approach lacks objectivity. This paper proposes a novel method to determine the optimal base pose based on the individual surgeon's working pattern. The proposed method analyzes recorded end-effector poses using machine-learning based clustering technique to identify key positions and orientations preferred by the surgeon. To address joint limits and singularities problems, we introduce two scoring metrics: joint margin score and manipulability score. We then train a multi-layer perceptron (MLP) regressor to predict the optimal base pose based on these scores. Evaluation in a simulated environment using the da Vinci Research Kit (dVRK) showed unique base pose-score maps for four volunteers, highlighting the individuality of working patterns. After conducting tests on the base poses identified using the proposed method, we confirmed that they have a score approximately 28.2\\% higher than when the robots were placed randomly, with respect to the score we defined. This emphasizes the need for operator-specific optimization in RAMIS base placement.","sentences":["In robot-assisted minimally invasive surgery (RAMIS), optimal placement of the surgical robot's base is crucial for successful surgery.","Improper placement can hinder performance due to manipulator limitations and inaccessible workspaces.","Traditionally, trained medical staff rely on experience for base placement, but this approach lacks objectivity.","This paper proposes a novel method to determine the optimal base pose based on the individual surgeon's working pattern.","The proposed method analyzes recorded end-effector poses using machine-learning based clustering technique to identify key positions and orientations preferred by the surgeon.","To address joint limits and singularities problems, we introduce two scoring metrics: joint margin score and manipulability score.","We then train a multi-layer perceptron (MLP) regressor to predict the optimal base pose based on these scores.","Evaluation in a simulated environment using the da Vinci Research Kit (dVRK) showed unique base pose-score maps for four volunteers, highlighting the individuality of working patterns.","After conducting tests on the base poses identified using the proposed method, we confirmed that they have a score approximately 28.2\\% higher than when the robots were placed randomly, with respect to the score we defined.","This emphasizes the need for operator-specific optimization in RAMIS base placement."],"url":"http://arxiv.org/abs/2402.16101v1"}
{"created":"2024-02-25 14:19:41","title":"chainBoost: A Secure Performance Booster for Blockchain-based Resource Markets","abstract":"Cryptocurrencies and blockchain technology provide an innovative model for reshaping digital services. Driven by the movement toward Web 3.0, recent systems started to provide distributed services, such as computation outsourcing or file storage, on top of the currency exchange medium. By allowing anyone to join and collect cryptocurrency payments for serving others, these systems create decentralized markets for trading digital resources. Yet, there is still a big gap between the promise of these markets and their practical viability. Existing initiatives are still early-stage and have already encountered security and efficiency obstacles. At the same time, existing work around promising ideas, specifically sidechains, fall short in exploiting their full potential in addressing these problems.   To bridge this gap, we propose chainBoost, a secure performance booster for decentralized resource markets. It expedites service related operations, reduces the blockchain size, and supports flexible service-payment exchange modalities at low overhead. At its core, chainBoost employs a sidechain, that has a (security and semantic) mutual-dependence with the mainchain, to which the system offloads heavy/frequent operations. To enable it, we develop a novel sidechain architecture composed of temporary and permanent blocks, a block suppression mechanism to prune the sidechain, a syncing protocol to permit arbitrary data exchange between the two chains, and an autorecovery protocol to support robustness and resilience. We analyze the security of chainBoost, and implement a proof-of-concept prototype for a distributed file storage market as a use case. For a market handling around 2000 transactions per round, our experiments show up to 11x improvement in throughput and 94\\% reduction in confirmation time. They also show that chainBoost can reduce the main blockchain size by around 90%.","sentences":["Cryptocurrencies and blockchain technology provide an innovative model for reshaping digital services.","Driven by the movement toward Web 3.0, recent systems started to provide distributed services, such as computation outsourcing or file storage, on top of the currency exchange medium.","By allowing anyone to join and collect cryptocurrency payments for serving others, these systems create decentralized markets for trading digital resources.","Yet, there is still a big gap between the promise of these markets and their practical viability.","Existing initiatives are still early-stage and have already encountered security and efficiency obstacles.","At the same time, existing work around promising ideas, specifically sidechains, fall short in exploiting their full potential in addressing these problems.   ","To bridge this gap, we propose chainBoost, a secure performance booster for decentralized resource markets.","It expedites service related operations, reduces the blockchain size, and supports flexible service-payment exchange modalities at low overhead.","At its core, chainBoost employs a sidechain, that has a (security and semantic) mutual-dependence with the mainchain, to which the system offloads heavy/frequent operations.","To enable it, we develop a novel sidechain architecture composed of temporary and permanent blocks, a block suppression mechanism to prune the sidechain, a syncing protocol to permit arbitrary data exchange between the two chains, and an autorecovery protocol to support robustness and resilience.","We analyze the security of chainBoost, and implement a proof-of-concept prototype for a distributed file storage market as a use case.","For a market handling around 2000 transactions per round, our experiments show up to 11x improvement in throughput and 94\\% reduction in confirmation time.","They also show that chainBoost can reduce the main blockchain size by around 90%."],"url":"http://arxiv.org/abs/2402.16095v1"}
{"created":"2024-02-25 14:18:14","title":"Bistochastically private release of data streams with zero delay","abstract":"Although the bulk of the research in privacy and statistical disclosure control is designed for static data, more and more data are often collected as continuous streams, and extensions of popular privacy tools and models have been proposed for this scenario. However, most of these proposals require buffers, where incoming individuals are momentarily stored, anonymized, and then released following a delay, thus considering a data stream as a succession of batches while it is by nature continuous. Having a delay unavoidably alters data freshness but also, more critically, inordinately exerts constraints on what can be achieved in terms of protection and information preservation. By considering randomized response, and specifically its recent bistochastic extension, in the context of dynamic data, this paper proposes a protocol for the anonymization of data streams that achieves zero delay while exhibiting formal privacy guarantees. Using a new tool in the privacy literature that introduces the concept of elementary plausible deniability, we show that it is feasible to achieve an atomic processing of individuals entering a stream, in-stead of proceeding by batches. We illustrate the application of the proposed approach by an empirical example.","sentences":["Although the bulk of the research in privacy and statistical disclosure control is designed for static data, more and more data are often collected as continuous streams, and extensions of popular privacy tools and models have been proposed for this scenario.","However, most of these proposals require buffers, where incoming individuals are momentarily stored, anonymized, and then released following a delay, thus considering a data stream as a succession of batches while it is by nature continuous.","Having a delay unavoidably alters data freshness but also, more critically, inordinately exerts constraints on what can be achieved in terms of protection and information preservation.","By considering randomized response, and specifically its recent bistochastic extension, in the context of dynamic data, this paper proposes a protocol for the anonymization of data streams that achieves zero delay while exhibiting formal privacy guarantees.","Using a new tool in the privacy literature that introduces the concept of elementary plausible deniability, we show that it is feasible to achieve an atomic processing of individuals entering a stream, in-stead of proceeding by batches.","We illustrate the application of the proposed approach by an empirical example."],"url":"http://arxiv.org/abs/2402.16094v1"}
{"created":"2024-02-25 13:37:53","title":"Bayesian Neural Network For Personalized Federated Learning Parameter Selection","abstract":"Federated learning's poor performance in the presence of heterogeneous data remains one of the most pressing issues in the field. Personalized federated learning departs from the conventional paradigm in which all clients employ the same model, instead striving to discover an individualized model for each client to address the heterogeneity in the data. One of such approach involves personalizing specific layers of neural networks. However, prior endeavors have not provided a dependable rationale, and some have selected personalized layers that are entirely distinct and conflicting. In this work, we take a step further by proposing personalization at the elemental level, rather than the traditional layer-level personalization. To select personalized parameters, we introduce Bayesian neural networks and rely on the uncertainty they offer to guide our selection of personalized parameters. Finally, we validate our algorithm's efficacy on several real-world datasets, demonstrating that our proposed approach outperforms existing baselines.","sentences":["Federated learning's poor performance in the presence of heterogeneous data remains one of the most pressing issues in the field.","Personalized federated learning departs from the conventional paradigm in which all clients employ the same model, instead striving to discover an individualized model for each client to address the heterogeneity in the data.","One of such approach involves personalizing specific layers of neural networks.","However, prior endeavors have not provided a dependable rationale, and some have selected personalized layers that are entirely distinct and conflicting.","In this work, we take a step further by proposing personalization at the elemental level, rather than the traditional layer-level personalization.","To select personalized parameters, we introduce Bayesian neural networks and rely on the uncertainty they offer to guide our selection of personalized parameters.","Finally, we validate our algorithm's efficacy on several real-world datasets, demonstrating that our proposed approach outperforms existing baselines."],"url":"http://arxiv.org/abs/2402.16091v1"}
{"created":"2024-02-25 13:25:51","title":"How to Privately Tune Hyperparameters in Federated Learning? Insights from a Benchmark Study","abstract":"In this paper, we address the problem of privacy-preserving hyperparameter (HP) tuning for cross-silo federated learning (FL). We first perform a comprehensive measurement study that benchmarks various HP strategies suitable for FL. Our benchmarks show that the optimal parameters of the FL server, e.g., the learning rate, can be accurately and efficiently tuned based on the HPs found by each client on its local data. We demonstrate that HP averaging is suitable for iid settings, while density-based clustering can uncover the optimal set of parameters in non-iid ones. Then, to prevent information leakage from the exchange of the clients' local HPs, we design and implement PrivTuna, a novel framework for privacy-preserving HP tuning using multiparty homomorphic encryption. We use PrivTuna to implement privacy-preserving federated averaging and density-based clustering, and we experimentally evaluate its performance demonstrating its computation/communication efficiency and its precision in tuning hyperparameters.","sentences":["In this paper, we address the problem of privacy-preserving hyperparameter (HP) tuning for cross-silo federated learning (FL).","We first perform a comprehensive measurement study that benchmarks various HP strategies suitable for FL.","Our benchmarks show that the optimal parameters of the FL server, e.g., the learning rate, can be accurately and efficiently tuned based on the HPs found by each client on its local data.","We demonstrate that HP averaging is suitable for iid settings, while density-based clustering can uncover the optimal set of parameters in non-iid ones.","Then, to prevent information leakage from the exchange of the clients' local HPs, we design and implement PrivTuna, a novel framework for privacy-preserving HP tuning using multiparty homomorphic encryption.","We use PrivTuna to implement privacy-preserving federated averaging and density-based clustering, and we experimentally evaluate its performance demonstrating its computation/communication efficiency and its precision in tuning hyperparameters."],"url":"http://arxiv.org/abs/2402.16087v1"}
{"created":"2024-02-25 13:20:28","title":"Online Drone Scheduling for Last-mile Delivery","abstract":"Delivering a parcel from the distribution hub to the customer's doorstep is called the \\textit{last-mile delivery} step in delivery logistics. In this paper, we study a hybrid {\\it truck-drones} model for the last-mile delivery step, in which a truck moves on a predefined path carrying parcels and drones deliver the parcels. We define the \\textsc{online drone scheduling} problem, where the truck moves in a predefined path, and the customer's requests appear online during the truck's movement. The objective is to schedule a drone associated with every request to minimize the number of drones used subject to the battery budget of the drones and compatibility of the schedules. We propose a 3-competitive deterministic algorithm using the next-fit strategy and 2.7-competitive algorithms using the first-fit strategy for the problem with $O(\\log n)$ worst-case time complexity per request, where $n$ is the maximum number of active requests at any time. We also introduce \\textsc{online variable-size drone scheduling} problem (OVDS). Here, we know all the customer's requests in advance; however, the drones with different battery capacities appear online. The objective is to schedule customers' requests for drones to minimize the number of drones used. We propose a $(2\\alpha + 1)$-competitive algorithm for the OVDS problem with total running time $O(n \\log n)$ for $n$ customer requests, where $\\alpha$ is the ratio of the maximum battery capacity to the minimum battery capacity of the drones. Finally, we address how to generate intervals corresponding to each customer request when there are discrete stopping points on the truck's route, from where the drone can fly and meet with the truck.","sentences":["Delivering a parcel from the distribution hub to the customer's doorstep is called the \\textit{last-mile delivery} step in delivery logistics.","In this paper, we study a hybrid {\\it truck-drones} model for the last-mile delivery step, in which a truck moves on a predefined path carrying parcels and drones deliver the parcels.","We define the \\textsc{online drone scheduling} problem, where the truck moves in a predefined path, and the customer's requests appear online during the truck's movement.","The objective is to schedule a drone associated with every request to minimize the number of drones used subject to the battery budget of the drones and compatibility of the schedules.","We propose a 3-competitive deterministic algorithm using the next-fit strategy and 2.7-competitive algorithms using the first-fit strategy for the problem with $O(\\log n)$ worst-case time complexity per request, where $n$ is the maximum number of active requests at any time.","We also introduce \\textsc{online variable-size drone scheduling} problem (OVDS).","Here, we know all the customer's requests in advance; however, the drones with different battery capacities appear online.","The objective is to schedule customers' requests for drones to minimize the number of drones used.","We propose a $(2\\alpha + 1)$-competitive algorithm for the OVDS problem with total running time $O(n \\log n)$ for $n$ customer requests, where $\\alpha$ is the ratio of the maximum battery capacity to the minimum battery capacity of the drones.","Finally, we address how to generate intervals corresponding to each customer request when there are discrete stopping points on the truck's route, from where the drone can fly and meet with the truck."],"url":"http://arxiv.org/abs/2402.16085v1"}
{"created":"2024-02-25 13:15:57","title":"Modeling Point Uncertainty in Radar SLAM","abstract":"While visual and laser-based simultaneous localization and mapping (SLAM) techniques have gained significant attention, radar SLAM remains a robust option for challenging conditions. This paper aims to improve the performance of radar SLAM by modeling point uncertainty. The basic SLAM system is a radar-inertial odometry (RIO) system that leverages velocity-aided radar points and high-frequency inertial measurements. We first propose to model the uncertainty of radar points in polar coordinates by considering the nature of radar sensing. Then in the SLAM system, the uncertainty model is designed into the data association module and is incorporated to weight the motion estimation. Real-world experiments on public and self-collected datasets validate the effectiveness of the proposed models and approaches. The findings highlight the potential of incorporating radar point uncertainty modeling to improve the radar SLAM system in adverse environments.","sentences":["While visual and laser-based simultaneous localization and mapping (SLAM) techniques have gained significant attention, radar SLAM remains a robust option for challenging conditions.","This paper aims to improve the performance of radar SLAM by modeling point uncertainty.","The basic SLAM system is a radar-inertial odometry (RIO) system that leverages velocity-aided radar points and high-frequency inertial measurements.","We first propose to model the uncertainty of radar points in polar coordinates by considering the nature of radar sensing.","Then in the SLAM system, the uncertainty model is designed into the data association module and is incorporated to weight the motion estimation.","Real-world experiments on public and self-collected datasets validate the effectiveness of the proposed models and approaches.","The findings highlight the potential of incorporating radar point uncertainty modeling to improve the radar SLAM system in adverse environments."],"url":"http://arxiv.org/abs/2402.16082v1"}
{"created":"2024-02-25 12:19:21","title":"Behavioral Refinement via Interpolant-based Policy Diffusion","abstract":"Imitation learning empowers artificial agents to mimic behavior by learning from demonstrations. Recently, diffusion models, which have the ability to model high-dimensional and multimodal distributions, have shown impressive performance on imitation learning tasks. These models learn to shape a policy by diffusing actions (or states) from standard Gaussian noise. However, the target policy to be learned is often significantly different from Gaussian and this mismatch can result in poor performance when using a small number of diffusion steps (to improve inference speed) and under limited data. The key idea in this work is that initiating from a more informative source than Gaussian enables diffusion methods to overcome the above limitations. We contribute both theoretical results, a new method, and empirical findings that show the benefits of using an informative source policy. Our method, which we call BRIDGER, leverages the stochastic interpolants framework to bridge arbitrary policies, thus enabling a flexible approach towards imitation learning. It generalizes prior work in that standard Gaussians can still be applied, but other source policies can be used if available. In experiments on challenging benchmarks, BRIDGER outperforms state-of-the-art diffusion policies and we provide further analysis on design considerations when applying BRIDGER.","sentences":["Imitation learning empowers artificial agents to mimic behavior by learning from demonstrations.","Recently, diffusion models, which have the ability to model high-dimensional and multimodal distributions, have shown impressive performance on imitation learning tasks.","These models learn to shape a policy by diffusing actions (or states) from standard Gaussian noise.","However, the target policy to be learned is often significantly different from Gaussian and this mismatch can result in poor performance when using a small number of diffusion steps (to improve inference speed) and under limited data.","The key idea in this work is that initiating from a more informative source than Gaussian enables diffusion methods to overcome the above limitations.","We contribute both theoretical results, a new method, and empirical findings that show the benefits of using an informative source policy.","Our method, which we call BRIDGER, leverages the stochastic interpolants framework to bridge arbitrary policies, thus enabling a flexible approach towards imitation learning.","It generalizes prior work in that standard Gaussians can still be applied, but other source policies can be used if available.","In experiments on challenging benchmarks, BRIDGER outperforms state-of-the-art diffusion policies and we provide further analysis on design considerations when applying BRIDGER."],"url":"http://arxiv.org/abs/2402.16075v1"}
{"created":"2024-02-25 11:37:23","title":"ROS-Causal: A ROS-based Causal Analysis Framework for Human-Robot Interaction Applications","abstract":"Deploying robots in human-shared spaces requires understanding interactions among nearby agents and objects. Modelling cause-and-effect relations through causal inference aids in predicting human behaviours and anticipating robot interventions. However, a critical challenge arises as existing causal discovery methods currently lack an implementation inside the ROS ecosystem, the standard de facto in robotics, hindering effective utilisation in robotics. To address this gap, this paper introduces ROS-Causal, a ROS-based framework for onboard data collection and causal discovery in human-robot spatial interactions. An ad-hoc simulator, integrated with ROS, illustrates the approach's effectiveness, showcasing the robot onboard generation of causal models during data collection. ROS-Causal is available on GitHub: https://github.com/lcastri/roscausal.git.","sentences":["Deploying robots in human-shared spaces requires understanding interactions among nearby agents and objects.","Modelling cause-and-effect relations through causal inference aids in predicting human behaviours and anticipating robot interventions.","However, a critical challenge arises as existing causal discovery methods currently lack an implementation inside the ROS ecosystem, the standard de facto in robotics, hindering effective utilisation in robotics.","To address this gap, this paper introduces ROS-Causal, a ROS-based framework for onboard data collection and causal discovery in human-robot spatial interactions.","An ad-hoc simulator, integrated with ROS, illustrates the approach's effectiveness, showcasing the robot onboard generation of causal models during data collection.","ROS-Causal is available on GitHub: https://github.com/lcastri/roscausal.git."],"url":"http://arxiv.org/abs/2402.16068v1"}
{"created":"2024-02-25 11:24:41","title":"Citation-Enhanced Generation for LLM-based Chatbot","abstract":"Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots. However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability. Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation. In this paper, we propose a novel post-hoc \\textbf{C}itation-\\textbf{E}nhanced \\textbf{G}eneration (\\textbf{CEG}) approach combined with retrieval argumentation. Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way. It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-based citation generation module. Once the statements in the generated content lack of reference, our model can regenerate responses until all statements are supported by citations. Note that our method is a training-free plug-and-play plugin that is capable of various LLMs. Experiments on various hallucination-related datasets show our framework outperforms state-of-the-art methods in both hallucination detection and response regeneration on three benchmarks. Our codes and dataset will be publicly available.","sentences":["Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots.","However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability.","Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation.","In this paper, we propose a novel post-hoc \\textbf{C}itation-\\textbf{E}nhanced \\textbf{G}eneration (\\textbf{CEG}) approach combined with retrieval argumentation.","Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way.","It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-based citation generation module.","Once the statements in the generated content lack of reference, our model can regenerate responses until all statements are supported by citations.","Note that our method is a training-free plug-and-play plugin that is capable of various LLMs.","Experiments on various hallucination-related datasets show our framework outperforms state-of-the-art methods in both hallucination detection and response regeneration on three benchmarks.","Our codes and dataset will be publicly available."],"url":"http://arxiv.org/abs/2402.16063v1"}
{"created":"2024-02-25 11:07:08","title":"Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression","abstract":"Large language models (LLMs) require lengthy prompts as the input context to produce output aligned with user intentions, a process that incurs extra costs during inference. In this paper, we propose the Gist COnditioned deCOding (Gist-COCO) model, introducing a novel method for compressing prompts which also can assist the prompt interpretation and engineering. Gist-COCO employs an encoder-decoder based language model and then incorporates an additional encoder as a plugin module to compress prompts with inputs using gist tokens. It finetunes the compression plugin module and uses the representations of gist tokens to emulate the raw prompts in the vanilla language model. By verbalizing the representations of gist tokens into gist prompts, the compression ability of Gist-COCO can be generalized to different LLMs with high compression rates. Our experiments demonstrate that Gist-COCO outperforms previous prompt compression models in both passage and instruction compression tasks. Further analysis on gist verbalization results suggests that our gist prompts serve different functions in aiding language models. They may directly provide potential answers, generate the chain-of-thought, or simply repeat the inputs. All data and codes are available at https://github.com/OpenMatch/Gist-COCO .","sentences":["Large language models (LLMs) require lengthy prompts as the input context to produce output aligned with user intentions, a process that incurs extra costs during inference.","In this paper, we propose the Gist COnditioned deCOding (Gist-COCO) model, introducing a novel method for compressing prompts which also can assist the prompt interpretation and engineering.","Gist-COCO employs an encoder-decoder based language model and then incorporates an additional encoder as a plugin module to compress prompts with inputs using gist tokens.","It finetunes the compression plugin module and uses the representations of gist tokens to emulate the raw prompts in the vanilla language model.","By verbalizing the representations of gist tokens into gist prompts, the compression ability of Gist-COCO can be generalized to different LLMs with high compression rates.","Our experiments demonstrate that Gist-COCO outperforms previous prompt compression models in both passage and instruction compression tasks.","Further analysis on gist verbalization results suggests that our gist prompts serve different functions in aiding language models.","They may directly provide potential answers, generate the chain-of-thought, or simply repeat the inputs.","All data and codes are available at https://github.com/OpenMatch/Gist-COCO ."],"url":"http://arxiv.org/abs/2402.16058v1"}
{"created":"2024-02-25 10:27:46","title":"LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding","abstract":"Despite progress in video-language modeling, the computational challenge of interpreting long-form videos in response to task-specific linguistic queries persists, largely due to the complexity of high-dimensional video data and the misalignment between language and visual cues over space and time. To tackle this issue, we introduce a novel approach called Language-guided Spatial-Temporal Prompt Learning (LSTP). This approach features two key components: a Temporal Prompt Sampler (TPS) with optical flow prior that leverages temporal information to efficiently extract relevant video content, and a Spatial Prompt Solver (SPS) that adeptly captures the intricate spatial relationships between visual and textual elements. By harmonizing TPS and SPS with a cohesive training strategy, our framework significantly enhances computational efficiency, temporal understanding, and spatial-temporal alignment. Empirical evaluations across two challenging tasks--video question answering and temporal question grounding in videos--using a variety of video-language pretrainings (VLPs) and large language models (LLMs) demonstrate the superior performance, speed, and versatility of our proposed LSTP paradigm.","sentences":["Despite progress in video-language modeling, the computational challenge of interpreting long-form videos in response to task-specific linguistic queries persists, largely due to the complexity of high-dimensional video data and the misalignment between language and visual cues over space and time.","To tackle this issue, we introduce a novel approach called Language-guided Spatial-Temporal Prompt Learning (LSTP).","This approach features two key components: a Temporal Prompt Sampler (TPS) with optical flow prior that leverages temporal information to efficiently extract relevant video content, and a Spatial Prompt Solver (SPS) that adeptly captures the intricate spatial relationships between visual and textual elements.","By harmonizing TPS and SPS with a cohesive training strategy, our framework significantly enhances computational efficiency, temporal understanding, and spatial-temporal alignment.","Empirical evaluations across two challenging tasks--video question answering and temporal question grounding in videos--using a variety of video-language pretrainings (VLPs) and large language models (LLMs) demonstrate the superior performance, speed, and versatility of our proposed LSTP paradigm."],"url":"http://arxiv.org/abs/2402.16050v1"}
{"created":"2024-02-25 09:32:17","title":"Deep Learning Approaches for Improving Question Answering Systems in Hepatocellular Carcinoma Research","abstract":"In recent years, advancements in natural language processing (NLP) have been fueled by deep learning techniques, particularly through the utilization of powerful computing resources like GPUs and TPUs. Models such as BERT and GPT-3, trained on vast amounts of data, have revolutionized language understanding and generation. These pre-trained models serve as robust bases for various tasks including semantic understanding, intelligent writing, and reasoning, paving the way for a more generalized form of artificial intelligence. NLP, as a vital application of AI, aims to bridge the gap between humans and computers through natural language interaction. This paper delves into the current landscape and future prospects of large-scale model-based NLP, focusing on the question-answering systems within this domain. Practical cases and developments in artificial intelligence-driven question-answering systems are analyzed to foster further exploration and research in the realm of large-scale NLP.","sentences":["In recent years, advancements in natural language processing (NLP) have been fueled by deep learning techniques, particularly through the utilization of powerful computing resources like GPUs and TPUs.","Models such as BERT and GPT-3, trained on vast amounts of data, have revolutionized language understanding and generation.","These pre-trained models serve as robust bases for various tasks including semantic understanding, intelligent writing, and reasoning, paving the way for a more generalized form of artificial intelligence.","NLP, as a vital application of AI, aims to bridge the gap between humans and computers through natural language interaction.","This paper delves into the current landscape and future prospects of large-scale model-based NLP, focusing on the question-answering systems within this domain.","Practical cases and developments in artificial intelligence-driven question-answering systems are analyzed to foster further exploration and research in the realm of large-scale NLP."],"url":"http://arxiv.org/abs/2402.16038v1"}
{"created":"2024-02-25 09:19:11","title":"Text Understanding and Generation Using Transformer Models for Intelligent E-commerce Recommendations","abstract":"With the rapid development of artificial intelligence technology, Transformer structural pre-training model has become an important tool for large language model (LLM) tasks. In the field of e-commerce, these models are especially widely used, from text understanding to generating recommendation systems, which provide powerful technical support for improving user experience and optimizing service processes. This paper reviews the core application scenarios of Transformer pre-training model in e-commerce text understanding and recommendation generation, including but not limited to automatic generation of product descriptions, sentiment analysis of user comments, construction of personalized recommendation system and automated processing of customer service conversations. Through a detailed analysis of the model's working principle, implementation process, and application effects in specific cases, this paper emphasizes the unique advantages of pre-trained models in understanding complex user intentions and improving the quality of recommendations. In addition, the challenges and improvement directions for the future are also discussed, such as how to further improve the generalization ability of the model, the ability to handle large-scale data sets, and technical strategies to protect user privacy. Ultimately, the paper points out that the application of Transformer structural pre-training models in e-commerce has not only driven technological innovation, but also brought substantial benefits to merchants and consumers, and looking forward, these models will continue to play a key role in e-commerce and beyond.","sentences":["With the rapid development of artificial intelligence technology, Transformer structural pre-training model has become an important tool for large language model (LLM) tasks.","In the field of e-commerce, these models are especially widely used, from text understanding to generating recommendation systems, which provide powerful technical support for improving user experience and optimizing service processes.","This paper reviews the core application scenarios of Transformer pre-training model in e-commerce text understanding and recommendation generation, including but not limited to automatic generation of product descriptions, sentiment analysis of user comments, construction of personalized recommendation system and automated processing of customer service conversations.","Through a detailed analysis of the model's working principle, implementation process, and application effects in specific cases, this paper emphasizes the unique advantages of pre-trained models in understanding complex user intentions and improving the quality of recommendations.","In addition, the challenges and improvement directions for the future are also discussed, such as how to further improve the generalization ability of the model, the ability to handle large-scale data sets, and technical strategies to protect user privacy.","Ultimately, the paper points out that the application of Transformer structural pre-training models in e-commerce has not only driven technological innovation, but also brought substantial benefits to merchants and consumers, and looking forward, these models will continue to play a key role in e-commerce and beyond."],"url":"http://arxiv.org/abs/2402.16035v1"}
{"created":"2024-02-25 08:41:32","title":"GraphWiz: An Instruction-Following Language Model for Graph Problems","abstract":"Large language models (LLMs) have achieved impressive success across several fields, but their proficiency in understanding and resolving complex graph problems is less explored. To bridge this gap, we introduce GraphInstruct, a novel and comprehensive instruction-tuning dataset designed to equip language models with the ability to tackle a broad spectrum of graph problems using explicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an open-source language model capable of resolving various graph problem types while generating clear reasoning processes. To enhance the model's capability and reliability, we incorporate the Direct Preference Optimization (DPO) framework into the graph problem-solving context. The enhanced model, GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with different complexity levels, surpassing GPT-4 which has an average accuracy of 43.8%. Moreover, our research delves into the delicate balance between training data volume and model performance, highlighting the potential for overfitting with increased data. We also explore the transferability of the model's reasoning ability across different graph tasks, indicating the model's adaptability and practical application potential. Our investigation offers a new blueprint and valuable insights for developing LLMs specialized in graph reasoning and problem-solving.","sentences":["Large language models (LLMs) have achieved impressive success across several fields, but their proficiency in understanding and resolving complex graph problems is less explored.","To bridge this gap, we introduce GraphInstruct, a novel and comprehensive instruction-tuning dataset designed to equip language models with the ability to tackle a broad spectrum of graph problems using explicit reasoning paths.","Utilizing GraphInstruct, we build GraphWiz, an open-source language model capable of resolving various graph problem types while generating clear reasoning processes.","To enhance the model's capability and reliability, we incorporate the Direct Preference Optimization (DPO) framework into the graph problem-solving context.","The enhanced model, GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with different complexity levels, surpassing GPT-4 which has an average accuracy of 43.8%.","Moreover, our research delves into the delicate balance between training data volume and model performance, highlighting the potential for overfitting with increased data.","We also explore the transferability of the model's reasoning ability across different graph tasks, indicating the model's adaptability and practical application potential.","Our investigation offers a new blueprint and valuable insights for developing LLMs specialized in graph reasoning and problem-solving."],"url":"http://arxiv.org/abs/2402.16029v1"}
{"created":"2024-02-25 08:35:21","title":"FedFDP: Federated Learning with Fairness and Differential Privacy","abstract":"Federated learning (FL) is a new machine learning paradigm to overcome the challenge of data silos and has garnered significant attention. However, through our observations, a globally effective trained model may performance disparities in different clients. This implies that the jointly trained models by clients may lead to unfair outcomes. On the other hand, relevant studies indicate that the transmission of gradients or models in federated learning can also give rise to privacy leakage issues, such as membership inference attacks.   To address the first issue mentioned above, we propose a federated algorithm with fairness, termed FedFair. Building upon FedFair, we introduce privacy protection to form the FedFDP algorithm to address the second issue mentioned above. In FedFDP, we devise a fairness-aware clipping strategy to achieve differential privacy while adjusting fairness. Additionally, for the extra uploaded loss values, we present an adaptive clipping approach to maximize utility. Furthermore, we theoretically prove that our algorithm converges and ensures differential privacy. Lastly, Extensive experimental results demonstrate that FedFair and FedFDP significantly outperforms state-of-the-art solutions in terms of model performance and fairness. The code is accessible at https://anonymous.4open.science/r/FedFDP-E754.","sentences":["Federated learning (FL) is a new machine learning paradigm to overcome the challenge of data silos and has garnered significant attention.","However, through our observations, a globally effective trained model may performance disparities in different clients.","This implies that the jointly trained models by clients may lead to unfair outcomes.","On the other hand, relevant studies indicate that the transmission of gradients or models in federated learning can also give rise to privacy leakage issues, such as membership inference attacks.   ","To address the first issue mentioned above, we propose a federated algorithm with fairness, termed FedFair.","Building upon FedFair, we introduce privacy protection to form the FedFDP algorithm to address the second issue mentioned above.","In FedFDP, we devise a fairness-aware clipping strategy to achieve differential privacy while adjusting fairness.","Additionally, for the extra uploaded loss values, we present an adaptive clipping approach to maximize utility.","Furthermore, we theoretically prove that our algorithm converges and ensures differential privacy.","Lastly, Extensive experimental results demonstrate that FedFair and FedFDP significantly outperforms state-of-the-art solutions in terms of model performance and fairness.","The code is accessible at https://anonymous.4open.science/r/FedFDP-E754."],"url":"http://arxiv.org/abs/2402.16028v1"}
{"created":"2024-02-25 08:26:12","title":"Enhancing xURLLC with RSMA-Assisted Massive-MIMO Networks: Performance Analysis and Optimization","abstract":"Massive interconnection has sparked people's envisioning for next-generation ultra-reliable and low-latency communications (xURLLC), prompting the design of customized next-generation advanced transceivers (NGAT). Rate-splitting multiple access (RSMA) has emerged as a pivotal technology for NGAT design, given its robustness to imperfect channel state information (CSI) and resilience to quality of service (QoS). Additionally, xURLLC urgently appeals to large-scale access techniques, thus massive multiple-input multiple-output (mMIMO) is anticipated to integrate with RSMA to enhance xURLLC. In this paper, we develop an innovative RSMA-assisted massive-MIMO xURLLC (RSMA-mMIMO-xURLLC) network architecture tailored to accommodate xURLLC's critical QoS constraints in finite blocklength (FBL) regimes. Leveraging uplink pilot training under imperfect CSI at the transmitter, we estimate channel gains and customize linear precoders for efficient downlink short-packet data transmission. Subsequently, we formulate a joint rate-splitting, beamforming, and transmit antenna selection optimization problem to maximize the total effective transmission rate (ETR). Addressing this multi-variable coupled non-convex problem, we decompose it into three corresponding subproblems and propose a low-complexity joint iterative algorithm for efficient optimization. Extensive simulations substantiate that compared with non-orthogonal multiple access (NOMA) and space division multiple access (SDMA), the developed architecture improves the total ETR by 15.3% and 41.91%, respectively, as well as accommodates larger-scale access.","sentences":["Massive interconnection has sparked people's envisioning for next-generation ultra-reliable and low-latency communications (xURLLC), prompting the design of customized next-generation advanced transceivers (NGAT).","Rate-splitting multiple access (RSMA) has emerged as a pivotal technology for NGAT design, given its robustness to imperfect channel state information (CSI) and resilience to quality of service (QoS).","Additionally, xURLLC urgently appeals to large-scale access techniques, thus massive multiple-input multiple-output (mMIMO) is anticipated to integrate with RSMA to enhance xURLLC.","In this paper, we develop an innovative RSMA-assisted massive-MIMO xURLLC (RSMA-mMIMO-xURLLC) network architecture tailored to accommodate xURLLC's critical QoS constraints in finite blocklength (FBL) regimes.","Leveraging uplink pilot training under imperfect CSI at the transmitter, we estimate channel gains and customize linear precoders for efficient downlink short-packet data transmission.","Subsequently, we formulate a joint rate-splitting, beamforming, and transmit antenna selection optimization problem to maximize the total effective transmission rate (ETR).","Addressing this multi-variable coupled non-convex problem, we decompose it into three corresponding subproblems and propose a low-complexity joint iterative algorithm for efficient optimization.","Extensive simulations substantiate that compared with non-orthogonal multiple access (NOMA) and space division multiple access (SDMA), the developed architecture improves the total ETR by 15.3% and 41.91%, respectively, as well as accommodates larger-scale access."],"url":"http://arxiv.org/abs/2402.16027v1"}
{"created":"2024-02-25 08:12:19","title":"Learning with Semantics: Towards a Semantics-Aware Routing Anomaly Detection System","abstract":"BGP is the de facto inter-domain routing protocol to ensure global connectivity of the Internet. However, various reasons, such as deliberate attacks or misconfigurations, could cause BGP routing anomalies. Traditional methods for BGP routing anomaly detection require significant manual investigation of routes by network operators. Although machine learning has been applied to automate the process, prior arts typically impose significant training overhead (such as large-scale data labeling and feature crafting), and only produce uninterpretable results. To address these limitations, this paper presents a routing anomaly detection system centering around a novel network representation learning model named BEAM. The core design of BEAM is to accurately learn the unique properties (defined as \\emph{routing role}) of each Autonomous System (AS) in the Internet by incorporating BGP semantics. As a result, routing anomaly detection, given BEAM, is reduced to a matter of discovering unexpected routing role churns upon observing new route announcements. We implement a prototype of our routing anomaly detection system and extensively evaluate its performance. The experimental results, based on 18 real-world RouteViews datasets containing over 11 billion route announcement records, demonstrate that our system can detect all previously-confirmed routing anomalies, while only introducing at most five false alarms every 180 million route announcements. We also deploy our system at a large ISP to perform real-world detection for one month. During the course of deployment, our system detects 497 true anomalies in the wild with an average of only 1.65 false alarms per day.","sentences":["BGP is the de facto inter-domain routing protocol to ensure global connectivity of the Internet.","However, various reasons, such as deliberate attacks or misconfigurations, could cause BGP routing anomalies.","Traditional methods for BGP routing anomaly detection require significant manual investigation of routes by network operators.","Although machine learning has been applied to automate the process, prior arts typically impose significant training overhead (such as large-scale data labeling and feature crafting), and only produce uninterpretable results.","To address these limitations, this paper presents a routing anomaly detection system centering around a novel network representation learning model named BEAM.","The core design of BEAM is to accurately learn the unique properties (defined as \\emph{routing role}) of each Autonomous System (AS) in the Internet by incorporating BGP semantics.","As a result, routing anomaly detection, given BEAM, is reduced to a matter of discovering unexpected routing role churns upon observing new route announcements.","We implement a prototype of our routing anomaly detection system and extensively evaluate its performance.","The experimental results, based on 18 real-world RouteViews datasets containing over 11 billion route announcement records, demonstrate that our system can detect all previously-confirmed routing anomalies, while only introducing at most five false alarms every 180 million route announcements.","We also deploy our system at a large ISP to perform real-world detection for one month.","During the course of deployment, our system detects 497 true anomalies in the wild with an average of only 1.65 false alarms per day."],"url":"http://arxiv.org/abs/2402.16025v1"}
{"created":"2024-02-25 08:07:22","title":"HiGPT: Heterogeneous Graph Language Model","abstract":"Heterogeneous graph learning aims to capture complex relationships and diverse relational semantics among entities in a heterogeneous graph to obtain meaningful representations for nodes and edges. Recent advancements in heterogeneous graph neural networks (HGNNs) have achieved state-of-the-art performance by considering relation heterogeneity and using specialized message functions and aggregation rules. However, existing frameworks for heterogeneous graph learning have limitations in generalizing across diverse heterogeneous graph datasets. Most of these frameworks follow the \"pre-train\" and \"fine-tune\" paradigm on the same dataset, which restricts their capacity to adapt to new and unseen data. This raises the question: \"Can we generalize heterogeneous graph models to be well-adapted to diverse downstream learning tasks with distribution shifts in both node token sets and relation type heterogeneity?'' To tackle those challenges, we propose HiGPT, a general large graph model with Heterogeneous graph instruction-tuning paradigm. Our framework enables learning from arbitrary heterogeneous graphs without the need for any fine-tuning process from downstream datasets. To handle distribution shifts in heterogeneity, we introduce an in-context heterogeneous graph tokenizer that captures semantic relationships in different heterogeneous graphs, facilitating model adaptation. We incorporate a large corpus of heterogeneity-aware graph instructions into our HiGPT, enabling the model to effectively comprehend complex relation heterogeneity and distinguish between various types of graph tokens. Furthermore, we introduce the Mixture-of-Thought (MoT) instruction augmentation paradigm to mitigate data scarcity by generating diverse and informative instructions. Through comprehensive evaluations, our proposed framework demonstrates exceptional performance in terms of generalization performance.","sentences":["Heterogeneous graph learning aims to capture complex relationships and diverse relational semantics among entities in a heterogeneous graph to obtain meaningful representations for nodes and edges.","Recent advancements in heterogeneous graph neural networks (HGNNs) have achieved state-of-the-art performance by considering relation heterogeneity and using specialized message functions and aggregation rules.","However, existing frameworks for heterogeneous graph learning have limitations in generalizing across diverse heterogeneous graph datasets.","Most of these frameworks follow the \"pre-train\" and \"fine-tune\" paradigm on the same dataset, which restricts their capacity to adapt to new and unseen data.","This raises the question: \"Can we generalize heterogeneous graph models to be well-adapted to diverse downstream learning tasks with distribution shifts in both node token sets and relation type heterogeneity?''","To tackle those challenges, we propose HiGPT, a general large graph model with Heterogeneous graph instruction-tuning paradigm.","Our framework enables learning from arbitrary heterogeneous graphs without the need for any fine-tuning process from downstream datasets.","To handle distribution shifts in heterogeneity, we introduce an in-context heterogeneous graph tokenizer that captures semantic relationships in different heterogeneous graphs, facilitating model adaptation.","We incorporate a large corpus of heterogeneity-aware graph instructions into our HiGPT, enabling the model to effectively comprehend complex relation heterogeneity and distinguish between various types of graph tokens.","Furthermore, we introduce the Mixture-of-Thought (MoT) instruction augmentation paradigm to mitigate data scarcity by generating diverse and informative instructions.","Through comprehensive evaluations, our proposed framework demonstrates exceptional performance in terms of generalization performance."],"url":"http://arxiv.org/abs/2402.16024v1"}
{"created":"2024-02-25 07:46:57","title":"TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages","abstract":"The capability to jointly process multi-modal information is becoming an essential task. However, the limited number of paired multi-modal data and the large computational requirements in multi-modal learning hinder the development. We propose a novel Tri-Modal Translation (TMT) model that translates between arbitrary modalities spanning speech, image, and text. We introduce a novel viewpoint, where we interpret different modalities as different languages, and treat multi-modal translation as a well-established machine translation problem. To this end, we tokenize speech and image data into discrete tokens, which provide a unified interface across modalities and significantly decrease the computational cost. In the proposed TMT, a multi-modal encoder-decoder conducts the core translation, whereas modality-specific processing is conducted only within the tokenization and detokenization stages. We evaluate the proposed TMT on all six modality translation tasks. TMT outperforms single model counterparts consistently, demonstrating that unifying tasks is beneficial not only for practicality but also for performance.","sentences":["The capability to jointly process multi-modal information is becoming an essential task.","However, the limited number of paired multi-modal data and the large computational requirements in multi-modal learning hinder the development.","We propose a novel Tri-Modal Translation (TMT) model that translates between arbitrary modalities spanning speech, image, and text.","We introduce a novel viewpoint, where we interpret different modalities as different languages, and treat multi-modal translation as a well-established machine translation problem.","To this end, we tokenize speech and image data into discrete tokens, which provide a unified interface across modalities and significantly decrease the computational cost.","In the proposed TMT, a multi-modal encoder-decoder conducts the core translation, whereas modality-specific processing is conducted only within the tokenization and detokenization stages.","We evaluate the proposed TMT on all six modality translation tasks.","TMT outperforms single model counterparts consistently, demonstrating that unifying tasks is beneficial not only for practicality but also for performance."],"url":"http://arxiv.org/abs/2402.16021v1"}
{"created":"2024-02-25 07:19:01","title":"Building Flexible Machine Learning Models for Scientific Computing at Scale","abstract":"Foundation models have revolutionized knowledge acquisition across domains, and our study introduces OmniArch, a paradigm-shifting approach designed for building foundation models in multi-physics scientific computing. OmniArch's pre-training involves a versatile pipeline that processes multi-physics spatio-temporal data, casting forward problem learning into scalable auto-regressive tasks, while our novel Physics-Informed Reinforcement Learning (PIRL) technique during fine-tuning ensures alignment with physical laws. Pre-trained on the comprehensive PDEBench dataset, OmniArch not only sets new performance benchmarks for 1D, 2D and 3D PDEs but also demonstrates exceptional adaptability to new physics via few-shot and zero-shot learning approaches. The model's representations further extend to inverse problem-solving, highlighting the transformative potential of AI-enabled Scientific Computing(AI4SC) foundation models for engineering applications and physics discovery.","sentences":["Foundation models have revolutionized knowledge acquisition across domains, and our study introduces OmniArch, a paradigm-shifting approach designed for building foundation models in multi-physics scientific computing.","OmniArch's pre-training involves a versatile pipeline that processes multi-physics spatio-temporal data, casting forward problem learning into scalable auto-regressive tasks, while our novel Physics-Informed Reinforcement Learning (PIRL) technique during fine-tuning ensures alignment with physical laws.","Pre-trained on the comprehensive PDEBench dataset, OmniArch not only sets new performance benchmarks for 1D, 2D and 3D PDEs but also demonstrates exceptional adaptability to new physics via few-shot and zero-shot learning approaches.","The model's representations further extend to inverse problem-solving, highlighting the transformative potential of AI-enabled Scientific Computing(AI4SC) foundation models for engineering applications and physics discovery."],"url":"http://arxiv.org/abs/2402.16014v1"}
{"created":"2024-02-25 07:12:51","title":"Semi-supervised Open-World Object Detection","abstract":"Conventional open-world object detection (OWOD) problem setting first distinguishes known and unknown classes and then later incrementally learns the unknown objects when introduced with labels in the subsequent tasks. However, the current OWOD formulation heavily relies on the external human oracle for knowledge input during the incremental learning stages. Such reliance on run-time makes this formulation less realistic in a real-world deployment. To address this, we introduce a more realistic formulation, named semi-supervised open-world detection (SS-OWOD), that reduces the annotation cost by casting the incremental learning stages of OWOD in a semi-supervised manner. We demonstrate that the performance of the state-of-the-art OWOD detector dramatically deteriorates in the proposed SS-OWOD setting. Therefore, we introduce a novel SS-OWOD detector, named SS-OWFormer, that utilizes a feature-alignment scheme to better align the object query representations between the original and augmented images to leverage the large unlabeled and few labeled data. We further introduce a pseudo-labeling scheme for unknown detection that exploits the inherent capability of decoder object queries to capture object-specific information. We demonstrate the effectiveness of our SS-OWOD problem setting and approach for remote sensing object detection, proposing carefully curated splits and baseline performance evaluations. Our experiments on 4 datasets including MS COCO, PASCAL, Objects365 and DOTA demonstrate the effectiveness of our approach. Our source code, models and splits are available here - https://github.com/sahalshajim/SS-OWFormer","sentences":["Conventional open-world object detection (OWOD) problem setting first distinguishes known and unknown classes and then later incrementally learns the unknown objects when introduced with labels in the subsequent tasks.","However, the current OWOD formulation heavily relies on the external human oracle for knowledge input during the incremental learning stages.","Such reliance on run-time makes this formulation less realistic in a real-world deployment.","To address this, we introduce a more realistic formulation, named semi-supervised open-world detection (SS-OWOD), that reduces the annotation cost by casting the incremental learning stages of OWOD in a semi-supervised manner.","We demonstrate that the performance of the state-of-the-art OWOD detector dramatically deteriorates in the proposed SS-OWOD setting.","Therefore, we introduce a novel SS-OWOD detector, named SS-OWFormer, that utilizes a feature-alignment scheme to better align the object query representations between the original and augmented images to leverage the large unlabeled and few labeled data.","We further introduce a pseudo-labeling scheme for unknown detection that exploits the inherent capability of decoder object queries to capture object-specific information.","We demonstrate the effectiveness of our SS-OWOD problem setting and approach for remote sensing object detection, proposing carefully curated splits and baseline performance evaluations.","Our experiments on 4 datasets including MS COCO, PASCAL, Objects365 and DOTA demonstrate the effectiveness of our approach.","Our source code, models and splits are available here - https://github.com/sahalshajim/SS-OWFormer"],"url":"http://arxiv.org/abs/2402.16013v1"}
{"created":"2024-02-25 07:03:37","title":"Deep Contrastive Graph Learning with Clustering-Oriented Guidance","abstract":"Graph Convolutional Network (GCN) has exhibited remarkable potential in improving graph-based clustering. To handle the general clustering scenario without a prior graph, these models estimate an initial graph beforehand to apply GCN. Throughout the literature, we have witnessed that 1) most models focus on the initial graph while neglecting the original features. Therefore, the discriminability of the learned representation may be corrupted by a low-quality initial graph; 2) the training procedure lacks effective clustering guidance, which may lead to the incorporation of clustering-irrelevant information into the learned graph. To tackle these problems, the Deep Contrastive Graph Learning (DCGL) model is proposed for general data clustering. Specifically, we establish a pseudo-siamese network, which incorporates auto-encoder with GCN to emphasize both the graph structure and the original features. On this basis, feature-level contrastive learning is introduced to enhance the discriminative capacity, and the relationship between samples and centroids is employed as the clustering-oriented guidance. Afterward, a two-branch graph learning mechanism is designed to extract the local and global structural relationships, which are further embedded into a unified graph under the cluster-level contrastive guidance. Experimental results on several benchmark datasets demonstrate the superiority of DCGL against state-of-the-art algorithms.","sentences":["Graph Convolutional Network (GCN) has exhibited remarkable potential in improving graph-based clustering.","To handle the general clustering scenario without a prior graph, these models estimate an initial graph beforehand to apply GCN.","Throughout the literature, we have witnessed that 1) most models focus on the initial graph while neglecting the original features.","Therefore, the discriminability of the learned representation may be corrupted by a low-quality initial graph; 2) the training procedure lacks effective clustering guidance, which may lead to the incorporation of clustering-irrelevant information into the learned graph.","To tackle these problems, the Deep Contrastive Graph Learning (DCGL) model is proposed for general data clustering.","Specifically, we establish a pseudo-siamese network, which incorporates auto-encoder with GCN to emphasize both the graph structure and the original features.","On this basis, feature-level contrastive learning is introduced to enhance the discriminative capacity, and the relationship between samples and centroids is employed as the clustering-oriented guidance.","Afterward, a two-branch graph learning mechanism is designed to extract the local and global structural relationships, which are further embedded into a unified graph under the cluster-level contrastive guidance.","Experimental results on several benchmark datasets demonstrate the superiority of DCGL against state-of-the-art algorithms."],"url":"http://arxiv.org/abs/2402.16012v1"}
{"created":"2024-02-25 06:53:35","title":"Unmasking Dementia Detection by Masking Input Gradients: A JSM Approach to Model Interpretability and Precision","abstract":"The evolution of deep learning and artificial intelligence has significantly reshaped technological landscapes. However, their effective application in crucial sectors such as medicine demands more than just superior performance, but trustworthiness as well. While interpretability plays a pivotal role, existing explainable AI (XAI) approaches often do not reveal {\\em Clever Hans} behavior where a model makes (ungeneralizable) correct predictions using spurious correlations or biases in data. Likewise, current post-hoc XAI methods are susceptible to generating unjustified counterfactual examples. In this paper, we approach XAI with an innovative {\\em model debugging} methodology realized through Jacobian Saliency Map (JSM). To cast the problem into a concrete context, we employ Alzheimer's disease (AD) diagnosis as the use case, motivated by its significant impact on human lives and the formidable challenge in its early detection, stemming from the intricate nature of its progression. We introduce an interpretable, multimodal model for AD classification over its multi-stage progression, incorporating JSM as a modality-agnostic tool that provides insights into volumetric changes indicative of brain abnormalities. Our extensive evaluation including ablation study manifests the efficacy of using JSM for model debugging and interpretation, while significantly enhancing model accuracy as well.","sentences":["The evolution of deep learning and artificial intelligence has significantly reshaped technological landscapes.","However, their effective application in crucial sectors such as medicine demands more than just superior performance, but trustworthiness as well.","While interpretability plays a pivotal role, existing explainable AI (XAI) approaches often do not reveal {\\em Clever Hans} behavior where a model makes (ungeneralizable) correct predictions using spurious correlations or biases in data.","Likewise, current post-hoc XAI methods are susceptible to generating unjustified counterfactual examples.","In this paper, we approach XAI with an innovative {\\em model debugging} methodology realized through Jacobian Saliency Map (JSM).","To cast the problem into a concrete context, we employ Alzheimer's disease (AD) diagnosis as the use case, motivated by its significant impact on human lives and the formidable challenge in its early detection, stemming from the intricate nature of its progression.","We introduce an interpretable, multimodal model for AD classification over its multi-stage progression, incorporating JSM as a modality-agnostic tool that provides insights into volumetric changes indicative of brain abnormalities.","Our extensive evaluation including ablation study manifests the efficacy of using JSM for model debugging and interpretation, while significantly enhancing model accuracy as well."],"url":"http://arxiv.org/abs/2402.16008v1"}
{"created":"2024-02-25 06:39:15","title":"Adversarial-Robust Transfer Learning for Medical Imaging via Domain Assimilation","abstract":"In the field of Medical Imaging, extensive research has been dedicated to leveraging its potential in uncovering critical diagnostic features in patients. Artificial Intelligence (AI)-driven medical diagnosis relies on sophisticated machine learning and deep learning models to analyze, detect, and identify diseases from medical images. Despite the remarkable performance of these models, characterized by high accuracy, they grapple with trustworthiness issues. The introduction of a subtle perturbation to the original image empowers adversaries to manipulate the prediction output, redirecting it to other targeted or untargeted classes. Furthermore, the scarcity of publicly available medical images, constituting a bottleneck for reliable training, has led contemporary algorithms to depend on pretrained models grounded on a large set of natural images -- a practice referred to as transfer learning. However, a significant {\\em domain discrepancy} exists between natural and medical images, which causes AI models resulting from transfer learning to exhibit heightened {\\em vulnerability} to adversarial attacks. This paper proposes a {\\em domain assimilation} approach that introduces texture and color adaptation into transfer learning, followed by a texture preservation component to suppress undesired distortion. We systematically analyze the performance of transfer learning in the face of various adversarial attacks under different data modalities, with the overarching goal of fortifying the model's robustness and security in medical imaging tasks. The results demonstrate high effectiveness in reducing attack efficacy, contributing toward more trustworthy transfer learning in biomedical applications.","sentences":["In the field of Medical Imaging, extensive research has been dedicated to leveraging its potential in uncovering critical diagnostic features in patients.","Artificial Intelligence (AI)-driven medical diagnosis relies on sophisticated machine learning and deep learning models to analyze, detect, and identify diseases from medical images.","Despite the remarkable performance of these models, characterized by high accuracy, they grapple with trustworthiness issues.","The introduction of a subtle perturbation to the original image empowers adversaries to manipulate the prediction output, redirecting it to other targeted or untargeted classes.","Furthermore, the scarcity of publicly available medical images, constituting a bottleneck for reliable training, has led contemporary algorithms to depend on pretrained models grounded on a large set of natural images -- a practice referred to as transfer learning.","However, a significant {\\em domain discrepancy} exists between natural and medical images, which causes AI models resulting from transfer learning to exhibit heightened {\\em vulnerability} to adversarial attacks.","This paper proposes a {\\em domain assimilation} approach that introduces texture and color adaptation into transfer learning, followed by a texture preservation component to suppress undesired distortion.","We systematically analyze the performance of transfer learning in the face of various adversarial attacks under different data modalities, with the overarching goal of fortifying the model's robustness and security in medical imaging tasks.","The results demonstrate high effectiveness in reducing attack efficacy, contributing toward more trustworthy transfer learning in biomedical applications."],"url":"http://arxiv.org/abs/2402.16005v1"}
{"created":"2024-02-25 06:18:41","title":"Cross-Resolution Land Cover Classification Using Outdated Products and Transformers","abstract":"Large-scale high-resolution land cover classification is a prerequisite for constructing Earth system models and addressing ecological and resource issues. Advancements in satellite sensor technology have led to an improvement in spatial resolution and wider coverage areas. Nevertheless, the lack of high-resolution labeled data is still a challenge, hindering the largescale application of land cover classification methods. In this paper, we propose a Transformerbased weakly supervised method for cross-resolution land cover classification using outdated data. First, to capture long-range dependencies without missing the fine-grained details of objects, we propose a U-Net-like Transformer based on a reverse difference mechanism (RDM) using dynamic sparse attention. Second, we propose an anti-noise loss calculation (ANLC) module based on optimal transport (OT). Anti-noise loss calculation identifies confident areas (CA) and vague areas (VA) based on the OT matrix, which relieves the impact of noises in outdated land cover products. By introducing a weakly supervised loss with weights and employing unsupervised loss, the RDM-based U-Net-like Transformer was trained. Remote sensing images with 1 m resolution and the corresponding ground-truths of six states in the United States were employed to validate the performance of the proposed method. The experiments utilized outdated land cover products with 30 m resolution from 2013 as training labels, and produced land cover maps with 1 m resolution from 2017. The results show the superiority of the proposed method compared to state-of-the-art methods. The code is available at https://github.com/yu-ni1989/ANLC-Former.","sentences":["Large-scale high-resolution land cover classification is a prerequisite for constructing Earth system models and addressing ecological and resource issues.","Advancements in satellite sensor technology have led to an improvement in spatial resolution and wider coverage areas.","Nevertheless, the lack of high-resolution labeled data is still a challenge, hindering the largescale application of land cover classification methods.","In this paper, we propose a Transformerbased weakly supervised method for cross-resolution land cover classification using outdated data.","First, to capture long-range dependencies without missing the fine-grained details of objects, we propose a U-Net-like Transformer based on a reverse difference mechanism (RDM) using dynamic sparse attention.","Second, we propose an anti-noise loss calculation (ANLC) module based on optimal transport (OT).","Anti-noise loss calculation identifies confident areas (CA) and vague areas (VA) based on the OT matrix, which relieves the impact of noises in outdated land cover products.","By introducing a weakly supervised loss with weights and employing unsupervised loss, the RDM-based U-Net-like Transformer was trained.","Remote sensing images with 1 m resolution and the corresponding ground-truths of six states in the United States were employed to validate the performance of the proposed method.","The experiments utilized outdated land cover products with 30 m resolution from 2013 as training labels, and produced land cover maps with 1 m resolution from 2017.","The results show the superiority of the proposed method compared to state-of-the-art methods.","The code is available at https://github.com/yu-ni1989/ANLC-Former."],"url":"http://arxiv.org/abs/2402.16001v1"}
{"created":"2024-02-25 05:45:36","title":"Cieran: Designing Sequential Colormaps via In-Situ Active Preference Learning","abstract":"Quality colormaps can help communicate important data patterns. However, finding an aesthetically pleasing colormap that looks \"just right\" for a given scenario requires significant design and technical expertise. We introduce Cieran, a tool that allows any data analyst to rapidly find quality colormaps while designing charts within Jupyter Notebooks. Our system employs an active preference learning paradigm to rank expert-designed colormaps and create new ones from pairwise comparisons, allowing analysts who are novices in color design to tailor colormaps to their data context. We accomplish this by treating colormap design as a path planning problem through the CIELAB colorspace with a context-specific reward model. In an evaluation with twelve scientists, we found that Cieran effectively modeled user preferences to rank colormaps and leveraged this model to create new quality designs. Our work shows the potential of active preference learning for supporting efficient visualization design optimization.","sentences":["Quality colormaps can help communicate important data patterns.","However, finding an aesthetically pleasing colormap that looks \"just right\" for a given scenario requires significant design and technical expertise.","We introduce Cieran, a tool that allows any data analyst to rapidly find quality colormaps while designing charts within Jupyter Notebooks.","Our system employs an active preference learning paradigm to rank expert-designed colormaps and create new ones from pairwise comparisons, allowing analysts who are novices in color design to tailor colormaps to their data context.","We accomplish this by treating colormap design as a path planning problem through the CIELAB colorspace with a context-specific reward model.","In an evaluation with twelve scientists, we found that Cieran effectively modeled user preferences to rank colormaps and leveraged this model to create new quality designs.","Our work shows the potential of active preference learning for supporting efficient visualization design optimization."],"url":"http://arxiv.org/abs/2402.15997v1"}
{"created":"2024-02-25 05:22:45","title":"Learning method for S4 with Diagonal State Space Layers using Balanced Truncation","abstract":"We introduce a novel learning method for Structured State Space Sequence (S4) models incorporating Diagonal State Space (DSS) layers, tailored for processing long-sequence data in edge intelligence applications, including sensor data analysis and real-time analytics. This method utilizes the balanced truncation technique, prevalent in control theory, applied specifically to DSS layers to reduce computational costs during inference. By leveraging parameters from the reduced model, we refine the initialization process of S4 models, outperforming the widely used Skew-HiPPo initialization in terms of performance. Numerical experiments demonstrate that our trained S4 models with DSS layers surpass conventionally trained models in accuracy and efficiency metrics. Furthermore, our observations reveal a positive correlation: higher accuracy in the original model consistently leads to increased accuracy in models trained using our method, suggesting that our approach effectively leverages the strengths of the original model.","sentences":["We introduce a novel learning method for Structured State Space Sequence (S4) models incorporating Diagonal State Space (DSS) layers, tailored for processing long-sequence data in edge intelligence applications, including sensor data analysis and real-time analytics.","This method utilizes the balanced truncation technique, prevalent in control theory, applied specifically to DSS layers to reduce computational costs during inference.","By leveraging parameters from the reduced model, we refine the initialization process of S4 models, outperforming the widely used Skew-HiPPo initialization in terms of performance.","Numerical experiments demonstrate that our trained S4 models with DSS layers surpass conventionally trained models in accuracy and efficiency metrics.","Furthermore, our observations reveal a positive correlation: higher accuracy in the original model consistently leads to increased accuracy in models trained using our method, suggesting that our approach effectively leverages the strengths of the original model."],"url":"http://arxiv.org/abs/2402.15993v1"}
{"created":"2024-02-25 05:05:52","title":"An Empirical Study of Challenges in Machine Learning Asset Management","abstract":"In machine learning (ML), efficient asset management, including ML models, datasets, algorithms, and tools, is vital for resource optimization, consistent performance, and a streamlined development lifecycle. This enables quicker iterations, adaptability, reduced development-to-deployment time, and reliable outputs. Despite existing research, a significant knowledge gap remains in operational challenges like model versioning, data traceability, and collaboration, which are crucial for the success of ML projects. Our study aims to address this gap by analyzing 15,065 posts from developer forums and platforms, employing a mixed-method approach to classify inquiries, extract challenges using BERTopic, and identify solutions through open card sorting and BERTopic clustering. We uncover 133 topics related to asset management challenges, grouped into 16 macro-topics, with software dependency, model deployment, and model training being the most discussed. We also find 79 solution topics, categorized under 18 macro-topics, highlighting software dependency, feature development, and file management as key solutions. This research underscores the need for further exploration of identified pain points and the importance of collaborative efforts across academia, industry, and the research community.","sentences":["In machine learning (ML), efficient asset management, including ML models, datasets, algorithms, and tools, is vital for resource optimization, consistent performance, and a streamlined development lifecycle.","This enables quicker iterations, adaptability, reduced development-to-deployment time, and reliable outputs.","Despite existing research, a significant knowledge gap remains in operational challenges like model versioning, data traceability, and collaboration, which are crucial for the success of ML projects.","Our study aims to address this gap by analyzing 15,065 posts from developer forums and platforms, employing a mixed-method approach to classify inquiries, extract challenges using BERTopic, and identify solutions through open card sorting and BERTopic clustering.","We uncover 133 topics related to asset management challenges, grouped into 16 macro-topics, with software dependency, model deployment, and model training being the most discussed.","We also find 79 solution topics, categorized under 18 macro-topics, highlighting software dependency, feature development, and file management as key solutions.","This research underscores the need for further exploration of identified pain points and the importance of collaborative efforts across academia, industry, and the research community."],"url":"http://arxiv.org/abs/2402.15990v1"}
{"created":"2024-02-25 05:00:20","title":"Towards Fair Graph Anomaly Detection: Problem, New Datasets, and Evaluation","abstract":"The Fair Graph Anomaly Detection (FairGAD) problem aims to accurately detect anomalous nodes in an input graph while ensuring fairness and avoiding biased predictions against individuals from sensitive subgroups such as gender or political leanings. Fairness in graphs is particularly crucial in anomaly detection areas such as misinformation detection in search/ranking systems, where decision outcomes can significantly affect individuals. However, the current literature does not comprehensively discuss this problem, nor does it provide realistic datasets that encompass actual graph structures, anomaly labels, and sensitive attributes for research in FairGAD. To bridge this gap, we introduce a formal definition of the FairGAD problem and present two novel graph datasets constructed from the globally prominent social media platforms Reddit and Twitter. These datasets comprise 1.2 million and 400,000 edges associated with 9,000 and 47,000 nodes, respectively, and leverage political leanings as sensitive attributes and misinformation spreaders as anomaly labels. We demonstrate that our FairGAD datasets significantly differ from the synthetic datasets used currently by the research community. These new datasets offer significant values for FairGAD by providing realistic data that captures the intricacies of social networks. Using our datasets, we investigate the performance-fairness trade-off in eleven existing GAD and non-graph AD methods on five state-of-the-art fairness methods, which sheds light on their effectiveness and limitations in addressing the FairGAD problem.","sentences":["The Fair Graph Anomaly Detection (FairGAD) problem aims to accurately detect anomalous nodes in an input graph while ensuring fairness and avoiding biased predictions against individuals from sensitive subgroups such as gender or political leanings.","Fairness in graphs is particularly crucial in anomaly detection areas such as misinformation detection in search/ranking systems, where decision outcomes can significantly affect individuals.","However, the current literature does not comprehensively discuss this problem, nor does it provide realistic datasets that encompass actual graph structures, anomaly labels, and sensitive attributes for research in FairGAD.","To bridge this gap, we introduce a formal definition of the FairGAD problem and present two novel graph datasets constructed from the globally prominent social media platforms Reddit and Twitter.","These datasets comprise 1.2 million and 400,000 edges associated with 9,000 and 47,000 nodes, respectively, and leverage political leanings as sensitive attributes and misinformation spreaders as anomaly labels.","We demonstrate that our FairGAD datasets significantly differ from the synthetic datasets used currently by the research community.","These new datasets offer significant values for FairGAD by providing realistic data that captures the intricacies of social networks.","Using our datasets, we investigate the performance-fairness trade-off in eleven existing GAD and non-graph AD methods on five state-of-the-art fairness methods, which sheds light on their effectiveness and limitations in addressing the FairGAD problem."],"url":"http://arxiv.org/abs/2402.15988v1"}
{"created":"2024-02-25 04:52:02","title":"Likelihood-based Mitigation of Evaluation Bias in Large Language Models","abstract":"Large Language Models (LLMs) are widely used to evaluate natural language generation tasks as automated metrics. However, the likelihood, a measure of LLM's plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure. It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation: they might overrate sentences with higher likelihoods while underrating those with lower likelihoods. In this paper, we investigate the presence and impact of likelihood bias in LLM-based evaluators. We also propose a method to mitigate the likelihood bias. Our method utilizes highly biased instances as few-shot examples for in-context learning. Our experiments in evaluating the data-to-text and grammatical error correction tasks reveal that several LLMs we test display a likelihood bias. Furthermore, our proposed method successfully mitigates this bias, also improving evaluation performance (in terms of correlation of models with human scores) significantly.","sentences":["Large Language Models (LLMs) are widely used to evaluate natural language generation tasks as automated metrics.","However, the likelihood, a measure of LLM's plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure.","It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation: they might overrate sentences with higher likelihoods while underrating those with lower likelihoods.","In this paper, we investigate the presence and impact of likelihood bias in LLM-based evaluators.","We also propose a method to mitigate the likelihood bias.","Our method utilizes highly biased instances as few-shot examples for in-context learning.","Our experiments in evaluating the data-to-text and grammatical error correction tasks reveal that several LLMs we test display a likelihood bias.","Furthermore, our proposed method successfully mitigates this bias, also improving evaluation performance (in terms of correlation of models with human scores) significantly."],"url":"http://arxiv.org/abs/2402.15987v1"}
{"created":"2024-02-25 03:48:13","title":"Shaving Weights with Occam's Razor: Bayesian Sparsification for Neural Networks Using the Marginal Likelihood","abstract":"Neural network sparsification is a promising avenue to save computational time and memory costs, especially in an age where many successful AI models are becoming too large to na\\\"ively deploy on consumer hardware. While much work has focused on different weight pruning criteria, the overall sparsifiability of the network, i.e., its capacity to be pruned without quality loss, has often been overlooked. We present Sparsifiability via the Marginal likelihood (SpaM), a pruning framework that highlights the effectiveness of using the Bayesian marginal likelihood in conjunction with sparsity-inducing priors for making neural networks more sparsifiable. Our approach implements an automatic Occam's razor that selects the most sparsifiable model that still explains the data well, both for structured and unstructured sparsification. In addition, we demonstrate that the pre-computed posterior Hessian approximation used in the Laplace approximation can be re-used to define a cheap pruning criterion, which outperforms many existing (more expensive) approaches. We demonstrate the effectiveness of our framework, especially at high sparsity levels, across a range of different neural network architectures and datasets.","sentences":["Neural network sparsification is a promising avenue to save computational time and memory costs, especially in an age where many successful AI models are becoming too large to na\\\"ively deploy on consumer hardware.","While much work has focused on different weight pruning criteria, the overall sparsifiability of the network, i.e., its capacity to be pruned without quality loss, has often been overlooked.","We present Sparsifiability via the Marginal likelihood (SpaM), a pruning framework that highlights the effectiveness of using the Bayesian marginal likelihood in conjunction with sparsity-inducing priors for making neural networks more sparsifiable.","Our approach implements an automatic Occam's razor that selects the most sparsifiable model that still explains the data well, both for structured and unstructured sparsification.","In addition, we demonstrate that the pre-computed posterior Hessian approximation used in the Laplace approximation can be re-used to define a cheap pruning criterion, which outperforms many existing (more expensive) approaches.","We demonstrate the effectiveness of our framework, especially at high sparsity levels, across a range of different neural network architectures and datasets."],"url":"http://arxiv.org/abs/2402.15978v1"}
{"created":"2024-02-25 03:31:59","title":"Structural Knowledge-Driven Meta-Learning for Task Offloading in Vehicular Networks with Integrated Communications, Sensing and Computing","abstract":"Task offloading is a potential solution to satisfy the strict requirements of computation-intensive and latency-sensitive vehicular applications due to the limited onboard computing resources. However, the overwhelming upload traffic may lead to unacceptable uploading time. To tackle this issue, for tasks taking environmental data as input, the data perceived by roadside units (RSU) equipped with several sensors can be directly exploited for computation, resulting in a novel task offloading paradigm with integrated communications, sensing and computing (I-CSC). With this paradigm, vehicles can select to upload their sensed data to RSUs or transmit computing instructions to RSUs during the offloading. By optimizing the computation mode and network resources, in this paper, we investigate an I-CSC-based task offloading problem to reduce the cost caused by resource consumption while guaranteeing the latency of each task. Although this non-convex problem can be handled by the alternating minimization (AM) algorithm that alternatively minimizes the divided four sub-problems, it leads to high computational complexity and local optimal solution. To tackle this challenge, we propose a creative structural knowledge-driven meta-learning (SKDML) method, involving both the model-based AM algorithm and neural networks. Specifically, borrowing the iterative structure of the AM algorithm, also referred to as structural knowledge, the proposed SKDML adopts long short-term memory (LSTM) network-based meta-learning to learn an adaptive optimizer for updating variables in each sub-problem, instead of the handcrafted counterpart in the AM algorithm.","sentences":["Task offloading is a potential solution to satisfy the strict requirements of computation-intensive and latency-sensitive vehicular applications due to the limited onboard computing resources.","However, the overwhelming upload traffic may lead to unacceptable uploading time.","To tackle this issue, for tasks taking environmental data as input, the data perceived by roadside units (RSU) equipped with several sensors can be directly exploited for computation, resulting in a novel task offloading paradigm with integrated communications, sensing and computing (I-CSC).","With this paradigm, vehicles can select to upload their sensed data to RSUs or transmit computing instructions to RSUs during the offloading.","By optimizing the computation mode and network resources, in this paper, we investigate an I-CSC-based task offloading problem to reduce the cost caused by resource consumption while guaranteeing the latency of each task.","Although this non-convex problem can be handled by the alternating minimization (AM) algorithm that alternatively minimizes the divided four sub-problems, it leads to high computational complexity and local optimal solution.","To tackle this challenge, we propose a creative structural knowledge-driven meta-learning (SKDML) method, involving both the model-based AM algorithm and neural networks.","Specifically, borrowing the iterative structure of the AM algorithm, also referred to as structural knowledge, the proposed SKDML adopts long short-term memory (LSTM) network-based meta-learning to learn an adaptive optimizer for updating variables in each sub-problem, instead of the handcrafted counterpart in the AM algorithm."],"url":"http://arxiv.org/abs/2402.15972v1"}
{"created":"2024-02-25 03:07:32","title":"CoDream: Exchanging dreams instead of models for federated aggregation with heterogeneous models","abstract":"Federated Learning (FL) enables collaborative optimization of machine learning models across decentralized data by aggregating model parameters. Our approach extends this concept by aggregating \"knowledge\" derived from models, instead of model parameters. We present a novel framework called \\codream, where clients collaboratively optimize randomly initialized data using federated optimization in the input data space, similar to how randomly initialized model parameters are optimized in FL. Our key insight is that jointly optimizing this data can effectively capture the properties of the global data distribution. Sharing knowledge in data space offers numerous benefits: (1) model-agnostic collaborative learning, i.e., different clients can have different model architectures; (2) communication that is independent of the model size, eliminating scalability concerns with model parameters; (3) compatibility with secure aggregation, thus preserving the privacy benefits of federated learning; (4) allowing of adaptive optimization of knowledge shared for personalized learning. We empirically validate \\codream on standard FL tasks, demonstrating competitive performance despite not sharing model parameters. Our code: https://mitmedialab.github.io/codream.github.io/","sentences":["Federated Learning (FL) enables collaborative optimization of machine learning models across decentralized data by aggregating model parameters.","Our approach extends this concept by aggregating \"knowledge\" derived from models, instead of model parameters.","We present a novel framework called \\codream, where clients collaboratively optimize randomly initialized data using federated optimization in the input data space, similar to how randomly initialized model parameters are optimized in FL.","Our key insight is that jointly optimizing this data can effectively capture the properties of the global data distribution.","Sharing knowledge in data space offers numerous benefits: (1) model-agnostic collaborative learning, i.e., different clients can have different model architectures; (2) communication that is independent of the model size, eliminating scalability concerns with model parameters; (3) compatibility with secure aggregation, thus preserving the privacy benefits of federated learning; (4) allowing of adaptive optimization of knowledge shared for personalized learning.","We empirically validate \\codream on standard FL tasks, demonstrating competitive performance despite not sharing model parameters.","Our code: https://mitmedialab.github.io/codream.github.io/"],"url":"http://arxiv.org/abs/2402.15968v1"}
{"created":"2024-02-25 02:54:14","title":"Hierarchical energy signatures using machine learning for operational visibility and diagnostics in automotive manufacturing","abstract":"Manufacturing energy consumption data contains important process signatures required for operational visibility and diagnostics. These signatures may be of different temporal scales, ranging from monthly to sub-second resolutions. We introduce a hierarchical machine learning approach to identify automotive process signatures from paint shop electricity consumption data at varying temporal scales (weekly and daily). A Multi-Layer Perceptron (MLP), a Convolutional Neural Network (CNN), and Principal Component Analysis (PCA) combined with Logistic Regression (LR) are used for the analysis. We validate the utility of the developed algorithms with subject matter experts for (i) better operational visibility, and (ii) identifying energy saving opportunities.","sentences":["Manufacturing energy consumption data contains important process signatures required for operational visibility and diagnostics.","These signatures may be of different temporal scales, ranging from monthly to sub-second resolutions.","We introduce a hierarchical machine learning approach to identify automotive process signatures from paint shop electricity consumption data at varying temporal scales (weekly and daily).","A Multi-Layer Perceptron (MLP), a Convolutional Neural Network (CNN), and Principal Component Analysis (PCA) combined with Logistic Regression (LR) are used for the analysis.","We validate the utility of the developed algorithms with subject matter experts for (i) better operational visibility, and (ii) identifying energy saving opportunities."],"url":"http://arxiv.org/abs/2402.15962v1"}
{"created":"2024-02-25 02:07:50","title":"Convolution and Cross-Correlation of Count Sketches Enables Fast Cardinality Estimation of Multi-Join Queries","abstract":"With the increasing rate of data generated by critical systems, estimating functions on streaming data has become essential. This demand has driven numerous advancements in algorithms designed to efficiently query and analyze one or more data streams while operating under memory constraints. The primary challenge arises from the rapid influx of new items, requiring algorithms that enable efficient incremental processing of streams in order to keep up. A prominent algorithm in this domain is the AMS sketch. Originally developed to estimate the second frequency moment of a data stream, it can also estimate the cardinality of the equi-join between two relations. Since then, two important advancements are the Count sketch, a method which significantly improves upon the sketch update time, and secondly, an extension of the AMS sketch to accommodate multi-join queries. However, combining the strengths of these methods to maintain sketches for multi-join queries while ensuring fast update times is a non-trivial task, and has remained an open problem for decades as highlighted in the existing literature. In this work, we successfully address this problem by introducing a novel sketching method which has fast updates, even for sketches capable of accurately estimating the cardinality of complex multi-join queries. We prove that our estimator is unbiased and has the same error guarantees as the AMS-based method. Our experimental results confirm the significant improvement in update time complexity, resulting in orders of magnitude faster estimates, with equal or better estimation accuracy.","sentences":["With the increasing rate of data generated by critical systems, estimating functions on streaming data has become essential.","This demand has driven numerous advancements in algorithms designed to efficiently query and analyze one or more data streams while operating under memory constraints.","The primary challenge arises from the rapid influx of new items, requiring algorithms that enable efficient incremental processing of streams in order to keep up.","A prominent algorithm in this domain is the AMS sketch.","Originally developed to estimate the second frequency moment of a data stream, it can also estimate the cardinality of the equi-join between two relations.","Since then, two important advancements are the Count sketch, a method which significantly improves upon the sketch update time, and secondly, an extension of the AMS sketch to accommodate multi-join queries.","However, combining the strengths of these methods to maintain sketches for multi-join queries while ensuring fast update times is a non-trivial task, and has remained an open problem for decades as highlighted in the existing literature.","In this work, we successfully address this problem by introducing a novel sketching method which has fast updates, even for sketches capable of accurately estimating the cardinality of complex multi-join queries.","We prove that our estimator is unbiased and has the same error guarantees as the AMS-based method.","Our experimental results confirm the significant improvement in update time complexity, resulting in orders of magnitude faster estimates, with equal or better estimation accuracy."],"url":"http://arxiv.org/abs/2402.15953v1"}
{"created":"2024-02-25 02:04:56","title":"ViSTec: Video Modeling for Sports Technique Recognition and Tactical Analysis","abstract":"The immense popularity of racket sports has fueled substantial demand in tactical analysis with broadcast videos. However, existing manual methods require laborious annotation, and recent attempts leveraging video perception models are limited to low-level annotations like ball trajectories, overlooking tactics that necessitate an understanding of stroke techniques. State-of-the-art action segmentation models also struggle with technique recognition due to frequent occlusions and motion-induced blurring in racket sports videos. To address these challenges, We propose ViSTec, a Video-based Sports Technique recognition model inspired by human cognition that synergizes sparse visual data with rich contextual insights. Our approach integrates a graph to explicitly model strategic knowledge in stroke sequences and enhance technique recognition with contextual inductive bias. A two-stage action perception model is jointly trained to align with the contextual knowledge in the graph. Experiments demonstrate that our method outperforms existing models by a significant margin. Case studies with experts from the Chinese national table tennis team validate our model's capacity to automate analysis for technical actions and tactical strategies. More details are available at: https://ViSTec2024.github.io/.","sentences":["The immense popularity of racket sports has fueled substantial demand in tactical analysis with broadcast videos.","However, existing manual methods require laborious annotation, and recent attempts leveraging video perception models are limited to low-level annotations like ball trajectories, overlooking tactics that necessitate an understanding of stroke techniques.","State-of-the-art action segmentation models also struggle with technique recognition due to frequent occlusions and motion-induced blurring in racket sports videos.","To address these challenges, We propose ViSTec, a Video-based Sports Technique recognition model inspired by human cognition that synergizes sparse visual data with rich contextual insights.","Our approach integrates a graph to explicitly model strategic knowledge in stroke sequences and enhance technique recognition with contextual inductive bias.","A two-stage action perception model is jointly trained to align with the contextual knowledge in the graph.","Experiments demonstrate that our method outperforms existing models by a significant margin.","Case studies with experts from the Chinese national table tennis team validate our model's capacity to automate analysis for technical actions and tactical strategies.","More details are available at: https://ViSTec2024.github.io/."],"url":"http://arxiv.org/abs/2402.15952v1"}
{"created":"2024-02-25 01:56:47","title":"GreenLLaMA: A Framework for Detoxification with Explanations","abstract":"Prior works on detoxification are scattered in the sense that they do not cover all aspects of detoxification needed in a real-world scenario. Notably, prior works restrict the task of developing detoxification models to only a seen subset of platforms, leaving the question of how the models would perform on unseen platforms unexplored. Additionally, these works do not address non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified without altering the meaning. We propose GreenLLaMA, the first comprehensive end-to-end detoxification framework, which attempts to alleviate the aforementioned limitations. We first introduce a cross-platform pseudo-parallel corpus applying multi-step data processing and generation strategies leveraging ChatGPT. We then train a suite of detoxification models with our cross-platform corpus. We show that our detoxification models outperform the SoTA model trained with human-annotated parallel corpus. We further introduce explanation to promote transparency and trustworthiness. GreenLLaMA additionally offers a unique paraphrase detector especially dedicated for the detoxification task to tackle the non-detoxifiable cases. Through experimental analysis, we demonstrate the effectiveness of our cross-platform corpus and the robustness of GreenLLaMA against adversarial toxicity.","sentences":["Prior works on detoxification are scattered in the sense that they do not cover all aspects of detoxification needed in a real-world scenario.","Notably, prior works restrict the task of developing detoxification models to only a seen subset of platforms, leaving the question of how the models would perform on unseen platforms unexplored.","Additionally, these works do not address non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified without altering the meaning.","We propose GreenLLaMA, the first comprehensive end-to-end detoxification framework, which attempts to alleviate the aforementioned limitations.","We first introduce a cross-platform pseudo-parallel corpus applying multi-step data processing and generation strategies leveraging ChatGPT.","We then train a suite of detoxification models with our cross-platform corpus.","We show that our detoxification models outperform the SoTA model trained with human-annotated parallel corpus.","We further introduce explanation to promote transparency and trustworthiness.","GreenLLaMA additionally offers a unique paraphrase detector especially dedicated for the detoxification task to tackle the non-detoxifiable cases.","Through experimental analysis, we demonstrate the effectiveness of our cross-platform corpus and the robustness of GreenLLaMA against adversarial toxicity."],"url":"http://arxiv.org/abs/2402.15951v1"}
{"created":"2024-02-25 01:10:55","title":"Attention-GAN for Anomaly Detection: A Cutting-Edge Approach to Cybersecurity Threat Management","abstract":"This paper proposes an innovative Attention-GAN framework for enhancing cybersecurity, focusing on anomaly detection. In response to the challenges posed by the constantly evolving nature of cyber threats, the proposed approach aims to generate diverse and realistic synthetic attack scenarios, thereby enriching the dataset and improving threat identification. Integrating attention mechanisms with Generative Adversarial Networks (GANs) is a key feature of the proposed method. The attention mechanism enhances the model's ability to focus on relevant features, essential for detecting subtle and complex attack patterns. In addition, GANs address the issue of data scarcity by generating additional varied attack data, encompassing known and emerging threats. This dual approach ensures that the system remains relevant and effective against the continuously evolving cyberattacks. The KDD Cup and CICIDS2017 datasets were used to validate this model, which exhibited significant improvements in anomaly detection. It achieved an accuracy of 99.69% on the KDD dataset and 97.93% on the CICIDS2017 dataset, with precision, recall, and F1-scores above 97%, demonstrating its effectiveness in recognizing complex attack patterns. This study contributes significantly to cybersecurity by providing a scalable and adaptable solution for anomaly detection in the face of sophisticated and dynamic cyber threats. The exploration of GANs for data augmentation highlights a promising direction for future research, particularly in situations where data limitations restrict the development of cybersecurity systems. The attention-GAN framework has emerged as a pioneering approach, setting a new benchmark for advanced cyber-defense strategies.","sentences":["This paper proposes an innovative Attention-GAN framework for enhancing cybersecurity, focusing on anomaly detection.","In response to the challenges posed by the constantly evolving nature of cyber threats, the proposed approach aims to generate diverse and realistic synthetic attack scenarios, thereby enriching the dataset and improving threat identification.","Integrating attention mechanisms with Generative Adversarial Networks (GANs) is a key feature of the proposed method.","The attention mechanism enhances the model's ability to focus on relevant features, essential for detecting subtle and complex attack patterns.","In addition, GANs address the issue of data scarcity by generating additional varied attack data, encompassing known and emerging threats.","This dual approach ensures that the system remains relevant and effective against the continuously evolving cyberattacks.","The KDD Cup and CICIDS2017 datasets were used to validate this model, which exhibited significant improvements in anomaly detection.","It achieved an accuracy of 99.69% on the KDD dataset and 97.93% on the CICIDS2017 dataset, with precision, recall, and F1-scores above 97%, demonstrating its effectiveness in recognizing complex attack patterns.","This study contributes significantly to cybersecurity by providing a scalable and adaptable solution for anomaly detection in the face of sophisticated and dynamic cyber threats.","The exploration of GANs for data augmentation highlights a promising direction for future research, particularly in situations where data limitations restrict the development of cybersecurity systems.","The attention-GAN framework has emerged as a pioneering approach, setting a new benchmark for advanced cyber-defense strategies."],"url":"http://arxiv.org/abs/2402.15945v1"}
{"created":"2024-02-25 01:05:39","title":"On A Class of Greedy Sparse Recovery Algorithms -- A High Dimensional Approach","abstract":"Sparse signal recovery deals with finding the sparest solution of an under-determined linear system $x = Qs$. In this paper, we propose a novel greedy approach to addressing the challenges from such a problem. Such an approach is based on a characterization of solutions to the system, which allows us to work on the sparse recovery in the $s$-space directly with a given measure. With $l_2$-based measure, two OMP-type algorithms are proposed, which significantly outperform the classical OMP algorithm in terms of recovery accuracy while maintaining comparable computational complexity. An $l_1$-based algorithm, denoted as $\\text{Alg}_{GBP}$ (greedy basis pursuit) algorithm, is derived. Such an algorithm significantly outperforms the classical BP algorithm. A CoSaMP-type algorithm is also proposed to further enhance the performance of the two proposed OMP-type algorithms. The superior performance of our proposed algorithms is demonstrated through extensive numerical simulations using synthetic data as well as video signals, highlighting their potential for various applications in compressed sensing and signal processing.","sentences":["Sparse signal recovery deals with finding the sparest solution of an under-determined linear system $x = Qs$.","In this paper, we propose a novel greedy approach to addressing the challenges from such a problem.","Such an approach is based on a characterization of solutions to the system, which allows us to work on the sparse recovery in the $s$-space directly with a given measure.","With $l_2$-based measure, two OMP-type algorithms are proposed, which significantly outperform the classical OMP algorithm in terms of recovery accuracy while maintaining comparable computational complexity.","An $l_1$-based algorithm, denoted as $\\text{Alg}_{GBP}$ (greedy basis pursuit) algorithm, is derived.","Such an algorithm significantly outperforms the classical BP algorithm.","A CoSaMP-type algorithm is also proposed to further enhance the performance of the two proposed OMP-type algorithms.","The superior performance of our proposed algorithms is demonstrated through extensive numerical simulations using synthetic data as well as video signals, highlighting their potential for various applications in compressed sensing and signal processing."],"url":"http://arxiv.org/abs/2402.15944v1"}
{"created":"2024-02-24 23:54:41","title":"Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models","abstract":"Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's output distribution. To facilitate this study, we introduce two benchmarks, i.e., DetCon and ComiEval, for data contamination detection and contamination mitigation evaluation tasks. Extensive experimental results show that CDD achieves the average relative improvements of 21.8\\%-30.2\\% over other contamination detection approaches in terms of Accuracy, F1 Score, and AUC metrics, and can effectively detect contamination caused by the variants of test data. TED significantly mitigates performance improvements up to 66.9\\% attributed to data contamination across 24 settings and 21 contamination degrees. In real-world applications, we reveal that ChatGPT exhibits a high potential to suffer from data contamination on HumanEval benchmark.","sentences":["Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks.","Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination.","However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges.","In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs.","CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution.","To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's output distribution.","To facilitate this study, we introduce two benchmarks, i.e., DetCon and ComiEval, for data contamination detection and contamination mitigation evaluation tasks.","Extensive experimental results show that CDD achieves the average relative improvements of 21.8\\%-30.2\\% over other contamination detection approaches in terms of Accuracy, F1 Score, and AUC metrics, and can effectively detect contamination caused by the variants of test data.","TED significantly mitigates performance improvements up to 66.9\\% attributed to data contamination across 24 settings and 21 contamination degrees.","In real-world applications, we reveal that ChatGPT exhibits a high potential to suffer from data contamination on HumanEval benchmark."],"url":"http://arxiv.org/abs/2402.15938v1"}
{"created":"2024-02-24 23:31:34","title":"Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA","abstract":"In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated data and limited visual content diversity hampers the generalization to novel scenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and SQA dataset). Current approaches resort supplement 3D reasoning with 2D information. However, these methods face challenges: either they use top-down 2D views that introduce overly complex and sometimes question-irrelevant visual clues, or they rely on globally aggregated scene/image-level representations from 2D VLMs, losing the fine-grained vision-language correlations. To overcome these limitations, our approach utilizes question-conditional 2D view selection procedure, pinpointing semantically relevant 2D inputs for crucial visual clues. We then integrate this 2D knowledge into the 3D-VQA system via a two-branch Transformer structure. This structure, featuring a Twin-Transformer design, compactly combines 2D and 3D modalities and captures fine-grained correlations between modalities, allowing them mutually augmenting each other. Integrating proposed mechanisms above, we present BridgeQA, that offers a fresh perspective on multi-modal transformer-based architectures for 3D-VQA. Experiments validate that BridgeQA achieves state-of-the-art on 3D-VQA datasets and significantly outperforms existing solutions. Code is available at $\\href{https://github.com/matthewdm0816/BridgeQA}{\\text{this URL}}$.","sentences":["In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated data and limited visual content diversity hampers the generalization to novel scenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and SQA dataset).","Current approaches resort supplement 3D reasoning with 2D information.","However, these methods face challenges: either they use top-down 2D views that introduce overly complex and sometimes question-irrelevant visual clues, or they rely on globally aggregated scene/image-level representations from 2D VLMs, losing the fine-grained vision-language correlations.","To overcome these limitations, our approach utilizes question-conditional 2D view selection procedure, pinpointing semantically relevant 2D inputs for crucial visual clues.","We then integrate this 2D knowledge into the 3D-VQA system via a two-branch Transformer structure.","This structure, featuring a Twin-Transformer design, compactly combines 2D and 3D modalities and captures fine-grained correlations between modalities, allowing them mutually augmenting each other.","Integrating proposed mechanisms above, we present BridgeQA, that offers a fresh perspective on multi-modal transformer-based architectures for 3D-VQA.","Experiments validate that BridgeQA achieves state-of-the-art on 3D-VQA datasets and significantly outperforms existing solutions.","Code is available at $\\href{https://github.com/matthewdm0816/BridgeQA}{\\text{this URL}}$."],"url":"http://arxiv.org/abs/2402.15933v1"}
{"created":"2024-02-24 23:10:28","title":"Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of the Loss Improves Optimization Efficiency","abstract":"We consider gradient descent (GD) with a constant stepsize applied to logistic regression with linearly separable data, where the constant stepsize $\\eta$ is so large that the loss initially oscillates. We show that GD exits this initial oscillatory phase rapidly -- in $\\mathcal{O}(\\eta)$ steps -- and subsequently achieves an $\\tilde{\\mathcal{O}}(1 / (\\eta t) )$ convergence rate after $t$ additional steps. Our results imply that, given a budget of $T$ steps, GD can achieve an accelerated loss of $\\tilde{\\mathcal{O}}(1/T^2)$ with an aggressive stepsize $\\eta:= \\Theta( T)$, without any use of momentum or variable stepsize schedulers. Our proof technique is versatile and also handles general classification loss functions (where exponential tails are needed for the $\\tilde{\\mathcal{O}}(1/T^2)$ acceleration), nonlinear predictors in the neural tangent kernel regime, and online stochastic gradient descent (SGD) with a large stepsize, under suitable separability conditions.","sentences":["We consider gradient descent (GD) with a constant stepsize applied to logistic regression with linearly separable data, where the constant stepsize $\\eta$ is so large that the loss initially oscillates.","We show that GD exits this initial oscillatory phase rapidly -- in $\\mathcal{O}(\\eta)$ steps -- and subsequently achieves an $\\tilde{\\mathcal{O}}(1 / (\\eta t) )","$ convergence rate after $t$ additional steps.","Our results imply that, given a budget of $T$ steps, GD can achieve an accelerated loss of $\\tilde{\\mathcal{O}}(1/T^2)$ with an aggressive stepsize $\\eta:= \\Theta( T)$, without any use of momentum or variable stepsize schedulers.","Our proof technique is versatile and also handles general classification loss functions (where exponential tails are needed for the $\\tilde{\\mathcal{O}}(1/T^2)$ acceleration), nonlinear predictors in the neural tangent kernel regime, and online stochastic gradient descent (SGD) with a large stepsize, under suitable separability conditions."],"url":"http://arxiv.org/abs/2402.15926v1"}
{"created":"2024-02-24 23:01:21","title":"MultiContrievers: Analysis of Dense Retrieval Representations","abstract":"Dense retrievers compress source documents into (possibly lossy) vector representations, yet there is little analysis of what information is lost versus preserved, and how it affects downstream tasks. We conduct the first analysis of the information captured by dense retrievers compared to the language models they are based on (e.g., BERT versus Contriever). We use 25 MultiBert checkpoints as randomized initialisations to train MultiContrievers, a set of 25 contriever models. We test whether specific pieces of information -- such as gender and occupation -- can be extracted from contriever vectors of wikipedia-like documents. We measure this extractability via information theoretic probing. We then examine the relationship of extractability to performance and gender bias, as well as the sensitivity of these results to many random initialisations and data shuffles. We find that (1) contriever models have significantly increased extractability, but extractability usually correlates poorly with benchmark performance 2) gender bias is present, but is not caused by the contriever representations 3) there is high sensitivity to both random initialisation and to data shuffle, suggesting that future retrieval research should test across a wider spread of both.","sentences":["Dense retrievers compress source documents into (possibly lossy) vector representations, yet there is little analysis of what information is lost versus preserved, and how it affects downstream tasks.","We conduct the first analysis of the information captured by dense retrievers compared to the language models they are based on (e.g., BERT versus Contriever).","We use 25 MultiBert checkpoints as randomized initialisations to train MultiContrievers, a set of 25 contriever models.","We test whether specific pieces of information -- such as gender and occupation -- can be extracted from contriever vectors of wikipedia-like documents.","We measure this extractability via information theoretic probing.","We then examine the relationship of extractability to performance and gender bias, as well as the sensitivity of these results to many random initialisations and data shuffles.","We find that (1) contriever models have significantly increased extractability, but extractability usually correlates poorly with benchmark performance 2) gender bias is present, but is not caused by the contriever representations 3) there is high sensitivity to both random initialisation and to data shuffle, suggesting that future retrieval research should test across a wider spread of both."],"url":"http://arxiv.org/abs/2402.15925v1"}
{"created":"2024-02-24 22:36:23","title":"Predicting Outcomes in Video Games with Long Short Term Memory Networks","abstract":"Forecasting winners in E-sports with real-time analytics has the potential to further engage audiences watching major tournament events. However, making such real-time predictions is challenging due to unpredictable variables within the game involving diverse player strategies and decision-making. Our work attempts to enhance audience engagement within video game tournaments by introducing a real-time method of predicting wins. Our Long Short Term Memory Network (LSTMs) based approach enables efficient predictions of win-lose outcomes by only using the health indicator of each player as a time series. As a proof of concept, we evaluate our model's performance within a classic, two-player arcade game, Super Street Fighter II Turbo. We also benchmark our method against state of the art methods for time series forecasting; i.e. Transformer models found in large language models (LLMs). Finally, we open-source our data set and code in hopes of furthering work in predictive analysis for arcade games.","sentences":["Forecasting winners in E-sports with real-time analytics has the potential to further engage audiences watching major tournament events.","However, making such real-time predictions is challenging due to unpredictable variables within the game involving diverse player strategies and decision-making.","Our work attempts to enhance audience engagement within video game tournaments by introducing a real-time method of predicting wins.","Our Long Short Term Memory Network (LSTMs) based approach enables efficient predictions of win-lose outcomes by only using the health indicator of each player as a time series.","As a proof of concept, we evaluate our model's performance within a classic, two-player arcade game, Super Street Fighter II Turbo.","We also benchmark our method against state of the art methods for time series forecasting; i.e. Transformer models found in large language models (LLMs).","Finally, we open-source our data set and code in hopes of furthering work in predictive analysis for arcade games."],"url":"http://arxiv.org/abs/2402.15923v1"}
{"created":"2024-02-24 22:32:34","title":"Pretraining Strategy for Neural Potentials","abstract":"We propose a mask pretraining method for Graph Neural Networks (GNNs) to improve their performance on fitting potential energy surfaces, particularly in water systems. GNNs are pretrained by recovering spatial information related to masked-out atoms from molecules, then transferred and finetuned on atomic forcefields. Through such pretraining, GNNs learn meaningful prior about structural and underlying physical information of molecule systems that are useful for downstream tasks. From comprehensive experiments and ablation studies, we show that the proposed method improves the accuracy and convergence speed compared to GNNs trained from scratch or using other pretraining techniques such as denoising. On the other hand, our pretraining method is suitable for both energy-centric and force-centric GNNs. This approach showcases its potential to enhance the performance and data efficiency of GNNs in fitting molecular force fields.","sentences":["We propose a mask pretraining method for Graph Neural Networks (GNNs) to improve their performance on fitting potential energy surfaces, particularly in water systems.","GNNs are pretrained by recovering spatial information related to masked-out atoms from molecules, then transferred and finetuned on atomic forcefields.","Through such pretraining, GNNs learn meaningful prior about structural and underlying physical information of molecule systems that are useful for downstream tasks.","From comprehensive experiments and ablation studies, we show that the proposed method improves the accuracy and convergence speed compared to GNNs trained from scratch or using other pretraining techniques such as denoising.","On the other hand, our pretraining method is suitable for both energy-centric and force-centric GNNs.","This approach showcases its potential to enhance the performance and data efficiency of GNNs in fitting molecular force fields."],"url":"http://arxiv.org/abs/2402.15921v1"}
{"created":"2024-02-24 21:20:53","title":"Enhanced Droplet Analysis Using Generative Adversarial Networks","abstract":"Precision devices play an important role in enhancing production quality and productivity in agricultural systems. Therefore, the optimization of these devices is essential in precision agriculture. Recently, with the advancements of deep learning, there have been several studies aiming to harness its capabilities for improving spray system performance. However, the effectiveness of these methods heavily depends on the size of the training dataset, which is expensive and time-consuming to collect. To address the challenge of insufficient training samples, this paper proposes an alternative solution by generating artificial images of droplets using generative adversarial networks (GAN). The GAN model is trained by using a small dataset captured by a high-speed camera and capable of generating images with progressively increasing resolution. The results demonstrate that the model can generate high-quality images with the size of $1024\\times1024$. Furthermore, this research leverages recent advancements in computer vision and deep learning to develop a light droplet detector using the synthetic dataset. As a result, the detection model achieves a 16.06\\% increase in mean average precision (mAP) when utilizing the synthetic dataset. To the best of our knowledge, this work stands as the first to employ a generative model for augmenting droplet detection. Its significance lies not only in optimizing nozzle design for constructing efficient spray systems but also in addressing the common challenge of insufficient data in various precision agriculture tasks. This work offers a critical contribution to conserving resources while striving for optimal and sustainable agricultural practices.","sentences":["Precision devices play an important role in enhancing production quality and productivity in agricultural systems.","Therefore, the optimization of these devices is essential in precision agriculture.","Recently, with the advancements of deep learning, there have been several studies aiming to harness its capabilities for improving spray system performance.","However, the effectiveness of these methods heavily depends on the size of the training dataset, which is expensive and time-consuming to collect.","To address the challenge of insufficient training samples, this paper proposes an alternative solution by generating artificial images of droplets using generative adversarial networks (GAN).","The GAN model is trained by using a small dataset captured by a high-speed camera and capable of generating images with progressively increasing resolution.","The results demonstrate that the model can generate high-quality images with the size of $1024\\times1024$. Furthermore, this research leverages recent advancements in computer vision and deep learning to develop a light droplet detector using the synthetic dataset.","As a result, the detection model achieves a 16.06\\% increase in mean average precision (mAP) when utilizing the synthetic dataset.","To the best of our knowledge, this work stands as the first to employ a generative model for augmenting droplet detection.","Its significance lies not only in optimizing nozzle design for constructing efficient spray systems but also in addressing the common challenge of insufficient data in various precision agriculture tasks.","This work offers a critical contribution to conserving resources while striving for optimal and sustainable agricultural practices."],"url":"http://arxiv.org/abs/2402.15909v1"}
{"created":"2024-02-24 20:50:29","title":"ESFL: Efficient Split Federated Learning over Resource-Constrained Heterogeneous Wireless Devices","abstract":"Federated learning (FL) allows multiple parties (distributed devices) to train a machine learning model without sharing raw data. How to effectively and efficiently utilize the resources on devices and the central server is a highly interesting yet challenging problem. In this paper, we propose an efficient split federated learning algorithm (ESFL) to take full advantage of the powerful computing capabilities at a central server under a split federated learning framework with heterogeneous end devices (EDs). By splitting the model into different submodels between the server and EDs, our approach jointly optimizes user-side workload and server-side computing resource allocation by considering users' heterogeneity. We formulate the whole optimization problem as a mixed-integer non-linear program, which is an NP-hard problem, and develop an iterative approach to obtain an approximate solution efficiently. Extensive simulations have been conducted to validate the significantly increased efficiency of our ESFL approach compared with standard federated learning, split learning, and splitfed learning.","sentences":["Federated learning (FL) allows multiple parties (distributed devices) to train a machine learning model without sharing raw data.","How to effectively and efficiently utilize the resources on devices and the central server is a highly interesting yet challenging problem.","In this paper, we propose an efficient split federated learning algorithm (ESFL) to take full advantage of the powerful computing capabilities at a central server under a split federated learning framework with heterogeneous end devices (EDs).","By splitting the model into different submodels between the server and EDs, our approach jointly optimizes user-side workload and server-side computing resource allocation by considering users' heterogeneity.","We formulate the whole optimization problem as a mixed-integer non-linear program, which is an NP-hard problem, and develop an iterative approach to obtain an approximate solution efficiently.","Extensive simulations have been conducted to validate the significantly increased efficiency of our ESFL approach compared with standard federated learning, split learning, and splitfed learning."],"url":"http://arxiv.org/abs/2402.15903v1"}
{"created":"2024-02-24 20:42:09","title":"Dedicated Restricted Target Wake Time for Real-Time Applications in Wi-Fi 7","abstract":"Real-time applications (RTA) tend to play a crucial role in people's everyday life. Such applications are among the key use cases for the next generations of wireless technologies. RTA applications are characterized by strict guaranteed delay requirements (in the order of a few milliseconds). One of the pillars of enabling RTA in next-generation Wi-Fi standards is Restricted Target Wake Time (R-TWT), which provides Wi-Fi stations exclusive channel access within negotiated service periods (SPs). If each RTA data flow uses dedicated SPs for data transmission, they are completely isolated from each other and do not experience any contention. To ensure the satisfaction of RTA QoS requirements while minimizing the channel airtime consumption, it is important to properly select the R-TWT parameters, namely the duration of SPs and the period between SPs. In this paper, we develop a mathematical model that estimates the delay probability distribution and packet loss probability for a given set of network, traffic and R-TWT parameters. Using this model, the access point can select the optimal R-TWT parameters for the given QoS requirements. The high accuracy of the model is proven by means of simulation.","sentences":["Real-time applications (RTA) tend to play a crucial role in people's everyday life.","Such applications are among the key use cases for the next generations of wireless technologies.","RTA applications are characterized by strict guaranteed delay requirements (in the order of a few milliseconds).","One of the pillars of enabling RTA in next-generation Wi-Fi standards is Restricted Target Wake Time (R-TWT), which provides Wi-Fi stations exclusive channel access within negotiated service periods (SPs).","If each RTA data flow uses dedicated SPs for data transmission, they are completely isolated from each other and do not experience any contention.","To ensure the satisfaction of RTA QoS requirements while minimizing the channel airtime consumption, it is important to properly select the R-TWT parameters, namely the duration of SPs and the period between SPs.","In this paper, we develop a mathematical model that estimates the delay probability distribution and packet loss probability for a given set of network, traffic and R-TWT parameters.","Using this model, the access point can select the optimal R-TWT parameters for the given QoS requirements.","The high accuracy of the model is proven by means of simulation."],"url":"http://arxiv.org/abs/2402.15900v1"}
