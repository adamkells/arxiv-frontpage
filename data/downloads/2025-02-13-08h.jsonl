{"created":"2025-02-12 18:59:43","title":"Poly-Autoregressive Prediction for Modeling Interactions","abstract":"We introduce a simple framework for predicting the behavior of an agent in multi-agent settings. In contrast to autoregressive (AR) tasks, such as language processing, our focus is on scenarios with multiple agents whose interactions are shaped by physical constraints and internal motivations. To this end, we propose Poly-Autoregressive (PAR) modeling, which forecasts an ego agent's future behavior by reasoning about the ego agent's state history and the past and current states of other interacting agents. At its core, PAR represents the behavior of all agents as a sequence of tokens, each representing an agent's state at a specific timestep. With minimal data pre-processing changes, we show that PAR can be applied to three different problems: human action forecasting in social situations, trajectory prediction for autonomous vehicles, and object pose forecasting during hand-object interaction. Using a small proof-of-concept transformer backbone, PAR outperforms AR across these three scenarios. The project website can be found at https://neerja.me/PAR/.","sentences":["We introduce a simple framework for predicting the behavior of an agent in multi-agent settings.","In contrast to autoregressive (AR) tasks, such as language processing, our focus is on scenarios with multiple agents whose interactions are shaped by physical constraints and internal motivations.","To this end, we propose Poly-Autoregressive (PAR) modeling, which forecasts an ego agent's future behavior by reasoning about the ego agent's state history and the past and current states of other interacting agents.","At its core, PAR represents the behavior of all agents as a sequence of tokens, each representing an agent's state at a specific timestep.","With minimal data pre-processing changes, we show that PAR can be applied to three different problems: human action forecasting in social situations, trajectory prediction for autonomous vehicles, and object pose forecasting during hand-object interaction.","Using a small proof-of-concept transformer backbone, PAR outperforms AR across these three scenarios.","The project website can be found at https://neerja.me/PAR/."],"url":"http://arxiv.org/abs/2502.08646v1"}
{"created":"2025-02-12 18:59:04","title":"Re$^3$Sim: Generating High-Fidelity Simulation Data via 3D-Photorealistic Real-to-Sim for Robotic Manipulation","abstract":"Real-world data collection for robotics is costly and resource-intensive, requiring skilled operators and expensive hardware. Simulations offer a scalable alternative but often fail to achieve sim-to-real generalization due to geometric and visual gaps. To address these challenges, we propose a 3D-photorealistic real-to-sim system, namely, RE$^3$SIM, addressing geometric and visual sim-to-real gaps. RE$^3$SIM employs advanced 3D reconstruction and neural rendering techniques to faithfully recreate real-world scenarios, enabling real-time rendering of simulated cross-view cameras within a physics-based simulator. By utilizing privileged information to collect expert demonstrations efficiently in simulation, and train robot policies with imitation learning, we validate the effectiveness of the real-to-sim-to-real pipeline across various manipulation task scenarios. Notably, with only simulated data, we can achieve zero-shot sim-to-real transfer with an average success rate exceeding 58%. To push the limit of real-to-sim, we further generate a large-scale simulation dataset, demonstrating how a robust policy can be built from simulation data that generalizes across various objects. Codes and demos are available at: http://xshenhan.github.io/Re3Sim/.","sentences":["Real-world data collection for robotics is costly and resource-intensive, requiring skilled operators and expensive hardware.","Simulations offer a scalable alternative but often fail to achieve sim-to-real generalization due to geometric and visual gaps.","To address these challenges, we propose a 3D-photorealistic real-to-sim system, namely, RE$^3$SIM, addressing geometric and visual sim-to-real gaps.","RE$^3$SIM employs advanced 3D reconstruction and neural rendering techniques to faithfully recreate real-world scenarios, enabling real-time rendering of simulated cross-view cameras within a physics-based simulator.","By utilizing privileged information to collect expert demonstrations efficiently in simulation, and train robot policies with imitation learning, we validate the effectiveness of the real-to-sim-to-real pipeline across various manipulation task scenarios.","Notably, with only simulated data, we can achieve zero-shot sim-to-real transfer with an average success rate exceeding 58%.","To push the limit of real-to-sim, we further generate a large-scale simulation dataset, demonstrating how a robust policy can be built from simulation data that generalizes across various objects.","Codes and demos are available at: http://xshenhan.github.io/Re3Sim/."],"url":"http://arxiv.org/abs/2502.08645v1"}
{"created":"2025-02-12 18:58:34","title":"Rhythmic sharing: A bio-inspired paradigm for zero-shot adaptation and learning in neural networks","abstract":"The brain can rapidly adapt to new contexts and learn from limited data, a coveted characteristic that artificial intelligence algorithms have struggled to mimic. Inspired by oscillatory rhythms of the mechanical structures of neural cells, we developed a learning paradigm that is based on oscillations in link strengths and associates learning with the coordination of these oscillations. We find that this paradigm yields rapid adaptation and learning in artificial neural networks. Link oscillations can rapidly change coordination, endowing the network with the ability to sense subtle context changes in an unsupervised manner. In other words, the network generates the missing contextual tokens required to perform as a generalist AI architecture capable of predicting dynamics in multiple contexts. Oscillations also allow the network to extrapolate dynamics to never-seen-before contexts. These capabilities make our learning paradigm a powerful starting point for novel models of learning and cognition. Furthermore, learning through link coordination is agnostic to the specifics of the neural network architecture, hence our study opens the door for introducing rapid adaptation and learning capabilities into leading AI models.","sentences":["The brain can rapidly adapt to new contexts and learn from limited data, a coveted characteristic that artificial intelligence algorithms have struggled to mimic.","Inspired by oscillatory rhythms of the mechanical structures of neural cells, we developed a learning paradigm that is based on oscillations in link strengths and associates learning with the coordination of these oscillations.","We find that this paradigm yields rapid adaptation and learning in artificial neural networks.","Link oscillations can rapidly change coordination, endowing the network with the ability to sense subtle context changes in an unsupervised manner.","In other words, the network generates the missing contextual tokens required to perform as a generalist AI architecture capable of predicting dynamics in multiple contexts.","Oscillations also allow the network to extrapolate dynamics to never-seen-before contexts.","These capabilities make our learning paradigm a powerful starting point for novel models of learning and cognition.","Furthermore, learning through link coordination is agnostic to the specifics of the neural network architecture, hence our study opens the door for introducing rapid adaptation and learning capabilities into leading AI models."],"url":"http://arxiv.org/abs/2502.08644v1"}
{"created":"2025-02-12 18:55:36","title":"CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation","abstract":"In this work, we present CineMaster, a novel framework for 3D-aware and controllable text-to-video generation. Our goal is to empower users with comparable controllability as professional film directors: precise placement of objects within the scene, flexible manipulation of both objects and camera in 3D space, and intuitive layout control over the rendered frames. To achieve this, CineMaster operates in two stages. In the first stage, we design an interactive workflow that allows users to intuitively construct 3D-aware conditional signals by positioning object bounding boxes and defining camera movements within the 3D space. In the second stage, these control signals--comprising rendered depth maps, camera trajectories and object class labels--serve as the guidance for a text-to-video diffusion model, ensuring to generate the user-intended video content. Furthermore, to overcome the scarcity of in-the-wild datasets with 3D object motion and camera pose annotations, we carefully establish an automated data annotation pipeline that extracts 3D bounding boxes and camera trajectories from large-scale video data. Extensive qualitative and quantitative experiments demonstrate that CineMaster significantly outperforms existing methods and implements prominent 3D-aware text-to-video generation. Project page: https://cinemaster-dev.github.io/.","sentences":["In this work, we present CineMaster, a novel framework for 3D-aware and controllable text-to-video generation.","Our goal is to empower users with comparable controllability as professional film directors: precise placement of objects within the scene, flexible manipulation of both objects and camera in 3D space, and intuitive layout control over the rendered frames.","To achieve this, CineMaster operates in two stages.","In the first stage, we design an interactive workflow that allows users to intuitively construct 3D-aware conditional signals by positioning object bounding boxes and defining camera movements within the 3D space.","In the second stage, these control signals--comprising rendered depth maps, camera trajectories and object class labels--serve as the guidance for a text-to-video diffusion model, ensuring to generate the user-intended video content.","Furthermore, to overcome the scarcity of in-the-wild datasets with 3D object motion and camera pose annotations, we carefully establish an automated data annotation pipeline that extracts 3D bounding boxes and camera trajectories from large-scale video data.","Extensive qualitative and quantitative experiments demonstrate that CineMaster significantly outperforms existing methods and implements prominent 3D-aware text-to-video generation.","Project page: https://cinemaster-dev.github.io/."],"url":"http://arxiv.org/abs/2502.08639v1"}
{"created":"2025-02-12 18:23:23","title":"Robot Data Curation with Mutual Information Estimators","abstract":"The performance of imitation learning policies often hinges on the datasets with which they are trained. Consequently, investment in data collection for robotics has grown across both industrial and academic labs. However, despite the marked increase in the quantity of demonstrations collected, little work has sought to assess the quality of said data despite mounting evidence of its importance in other areas such as vision and language. In this work, we take a critical step towards addressing the data quality in robotics. Given a dataset of demonstrations, we aim to estimate the relative quality of individual demonstrations in terms of both state diversity and action predictability. To do so, we estimate the average contribution of a trajectory towards the mutual information between states and actions in the entire dataset, which precisely captures both the entropy of the state distribution and the state-conditioned entropy of actions. Though commonly used mutual information estimators require vast amounts of data often beyond the scale available in robotics, we introduce a novel technique based on k-nearest neighbor estimates of mutual information on top of simple VAE embeddings of states and actions. Empirically, we demonstrate that our approach is able to partition demonstration datasets by quality according to human expert scores across a diverse set of benchmarks spanning simulation and real world environments. Moreover, training policies based on data filtered by our method leads to a 5-10% improvement in RoboMimic and better performance on real ALOHA and Franka setups.","sentences":["The performance of imitation learning policies often hinges on the datasets with which they are trained.","Consequently, investment in data collection for robotics has grown across both industrial and academic labs.","However, despite the marked increase in the quantity of demonstrations collected, little work has sought to assess the quality of said data despite mounting evidence of its importance in other areas such as vision and language.","In this work, we take a critical step towards addressing the data quality in robotics.","Given a dataset of demonstrations, we aim to estimate the relative quality of individual demonstrations in terms of both state diversity and action predictability.","To do so, we estimate the average contribution of a trajectory towards the mutual information between states and actions in the entire dataset, which precisely captures both the entropy of the state distribution and the state-conditioned entropy of actions.","Though commonly used mutual information estimators require vast amounts of data often beyond the scale available in robotics, we introduce a novel technique based on k-nearest neighbor estimates of mutual information on top of simple VAE embeddings of states and actions.","Empirically, we demonstrate that our approach is able to partition demonstration datasets by quality according to human expert scores across a diverse set of benchmarks spanning simulation and real world environments.","Moreover, training policies based on data filtered by our method leads to a 5-10% improvement in RoboMimic and better performance on real ALOHA and Franka setups."],"url":"http://arxiv.org/abs/2502.08623v1"}
{"created":"2025-02-12 18:20:41","title":"Forecasting Drought Using Machine Learning in California","abstract":"Drought is a frequent and costly natural disaster in California, with major negative impacts on agricultural production and water resource availability, particularly groundwater. This study investigated the performance of applying different machine learning approaches to predicting the U.S. Drought Monitor classification in California. Four approaches were used: a convolutional neural network (CNN), random forest, XGBoost, and long short term memory (LSTM) recurrent neural network, and compared to a baseline persistence model. We evaluated the models' performance in predicting severe drought (USDM drought category D2 or higher) using a macro F1 binary classification metric. The LSTM model emerged as the top performer, followed by XGBoost, CNN, and random forest. Further evaluation of our results at the county level suggested that the LSTM model would perform best in counties with more consistent drought patterns and where severe drought was more common, and the LSTM model would perform worse where drought scores increased rapidly. Utilizing 30 weeks of historical data, the LSTM model successfully forecasted drought scores for a 12-week period with a Mean Absolute Error (MAE) of 0.33, equivalent to less than half a drought category on a scale of 0 to 5. Additionally, the LSTM achieved a macro F1 score of 0.9, indicating high accuracy in binary classification for severe drought conditions. Evaluation of different window and future horizon sizes in weeks suggested that at least 24 weeks of data would result in the best performance, with best performance for shorter horizon sizes, particularly less than eight weeks.","sentences":["Drought is a frequent and costly natural disaster in California, with major negative impacts on agricultural production and water resource availability, particularly groundwater.","This study investigated the performance of applying different machine learning approaches to predicting the U.S. Drought Monitor classification in California.","Four approaches were used: a convolutional neural network (CNN), random forest, XGBoost, and long short term memory (LSTM) recurrent neural network, and compared to a baseline persistence model.","We evaluated the models' performance in predicting severe drought (USDM drought category D2 or higher) using a macro F1 binary classification metric.","The LSTM model emerged as the top performer, followed by XGBoost, CNN, and random forest.","Further evaluation of our results at the county level suggested that the LSTM model would perform best in counties with more consistent drought patterns and where severe drought was more common, and the LSTM model would perform worse where drought scores increased rapidly.","Utilizing 30 weeks of historical data, the LSTM model successfully forecasted drought scores for a 12-week period with a Mean Absolute Error (MAE) of 0.33, equivalent to less than half a drought category on a scale of 0 to 5.","Additionally, the LSTM achieved a macro F1 score of 0.9, indicating high accuracy in binary classification for severe drought conditions.","Evaluation of different window and future horizon sizes in weeks suggested that at least 24 weeks of data would result in the best performance, with best performance for shorter horizon sizes, particularly less than eight weeks."],"url":"http://arxiv.org/abs/2502.08622v1"}
{"created":"2025-02-12 17:59:21","title":"Robustly Learning Monotone Generalized Linear Models via Data Augmentation","abstract":"We study the task of learning Generalized Linear models (GLMs) in the agnostic model under the Gaussian distribution. We give the first polynomial-time algorithm that achieves a constant-factor approximation for \\textit{any} monotone Lipschitz activation. Prior constant-factor GLM learners succeed for a substantially smaller class of activations. Our work resolves a well-known open problem, by developing a robust counterpart to the classical GLMtron algorithm (Kakade et al., 2011). Our robust learner applies more generally, encompassing all monotone activations with bounded $(2+\\zeta)$-moments, for any fixed $\\zeta>0$ -- a condition that is essentially necessary. To obtain our results, we leverage a novel data augmentation technique with decreasing Gaussian noise injection and prove a number of structural results that may be useful in other settings.","sentences":["We study the task of learning Generalized Linear models (GLMs) in the agnostic model under the Gaussian distribution.","We give the first polynomial-time algorithm that achieves a constant-factor approximation for \\textit{any} monotone Lipschitz activation.","Prior constant-factor GLM learners succeed for a substantially smaller class of activations.","Our work resolves a well-known open problem, by developing a robust counterpart to the classical GLMtron algorithm (Kakade et al., 2011).","Our robust learner applies more generally, encompassing all monotone activations with bounded $(2+\\zeta)$-moments, for any fixed $\\zeta>0$ -- a condition that is essentially necessary.","To obtain our results, we leverage a novel data augmentation technique with decreasing Gaussian noise injection and prove a number of structural results that may be useful in other settings."],"url":"http://arxiv.org/abs/2502.08611v1"}
{"created":"2025-02-12 17:57:54","title":"Quantifying Security Vulnerabilities: A Metric-Driven Security Analysis of Gaps in Current AI Standards","abstract":"As AI systems integrate into critical infrastructure, security gaps in AI compliance frameworks demand urgent attention. This paper audits and quantifies security risks in three major AI governance standards: NIST AI RMF 1.0, UK's AI and Data Protection Risk Toolkit, and the EU's ALTAI. Using a novel risk assessment methodology, we develop four key metrics: Risk Severity Index (RSI), Attack Potential Index (AVPI), Compliance-Security Gap Percentage (CSGP), and Root Cause Vulnerability Score (RCVS). Our analysis identifies 136 concerns across the frameworks, exposing significant gaps. NIST fails to address 69.23 percent of identified risks, ALTAI has the highest attack vector vulnerability (AVPI = 0.51) and the ICO Toolkit has the largest compliance-security gap, with 80.00 percent of high-risk concerns remaining unresolved. Root cause analysis highlights under-defined processes (ALTAI RCVS = 033) and weak implementation guidance (NIST and ICO RCVS = 0.25) as critical weaknesses. These findings emphasize the need for stronger, enforceable security controls in AI compliance. We offer targeted recommendations to enhance security posture and bridge the gap between compliance and real-world AI risks.","sentences":["As AI systems integrate into critical infrastructure, security gaps in AI compliance frameworks demand urgent attention.","This paper audits and quantifies security risks in three major AI governance standards: NIST AI RMF 1.0, UK's AI and Data Protection Risk Toolkit, and the EU's ALTAI.","Using a novel risk assessment methodology, we develop four key metrics: Risk Severity Index (RSI), Attack Potential Index (AVPI), Compliance-Security Gap Percentage (CSGP), and Root Cause Vulnerability Score (RCVS).","Our analysis identifies 136 concerns across the frameworks, exposing significant gaps.","NIST fails to address 69.23 percent of identified risks, ALTAI has the highest attack vector vulnerability (AVPI = 0.51) and the ICO Toolkit has the largest compliance-security gap, with 80.00 percent of high-risk concerns remaining unresolved.","Root cause analysis highlights under-defined processes (ALTAI RCVS = 033) and weak implementation guidance (NIST and ICO RCVS = 0.25) as critical weaknesses.","These findings emphasize the need for stronger, enforceable security controls in AI compliance.","We offer targeted recommendations to enhance security posture and bridge the gap between compliance and real-world AI risks."],"url":"http://arxiv.org/abs/2502.08610v1"}
{"created":"2025-02-12 17:39:02","title":"Two-stage hybrid models for enhancing forecasting accuracy on heterogeneous time series","abstract":"Compared to local models built in a series-by-series manner, global models leverage relevant information across time series, resulting in improved forecasting performance and generalization capacity. Constructing global models on a set of time series is becoming mainstream in the field of time series forecasting. However, the advantages of global models may not always be realized when dealing with heterogeneous data. While they can adapt to heterogeneous datasets by increasing the model complexity, the model cannot be infinitely complex due to the finite sample size, which poses challenges for the application of global models. Additionally, determining whether the time series data is homogeneous or heterogeneous can be ambiguous in practice. To address these research gaps, this paper argues that the heterogeneity of the data should be defined by the global model used, and for each series, the portion not modelled by the global model represents heterogeneity. It further proposes two-stage hybrid models, which include a second stage to identify and model heterogeneous patterns. In this second stage, we can estimate either all local models or sub-global models across different domains divided based on heterogeneity. Experiments on four open datasets reveal that the proposed methods significantly outperform five existing models, indicating they contribute to fully unleash the potential of global models on heterogeneous datasets.","sentences":["Compared to local models built in a series-by-series manner, global models leverage relevant information across time series, resulting in improved forecasting performance and generalization capacity.","Constructing global models on a set of time series is becoming mainstream in the field of time series forecasting.","However, the advantages of global models may not always be realized when dealing with heterogeneous data.","While they can adapt to heterogeneous datasets by increasing the model complexity, the model cannot be infinitely complex due to the finite sample size, which poses challenges for the application of global models.","Additionally, determining whether the time series data is homogeneous or heterogeneous can be ambiguous in practice.","To address these research gaps, this paper argues that the heterogeneity of the data should be defined by the global model used, and for each series, the portion not modelled by the global model represents heterogeneity.","It further proposes two-stage hybrid models, which include a second stage to identify and model heterogeneous patterns.","In this second stage, we can estimate either all local models or sub-global models across different domains divided based on heterogeneity.","Experiments on four open datasets reveal that the proposed methods significantly outperform five existing models, indicating they contribute to fully unleash the potential of global models on heterogeneous datasets."],"url":"http://arxiv.org/abs/2502.08600v1"}
{"created":"2025-02-12 17:14:07","title":"A method for classification of data with uncertainty using hypothesis testing","abstract":"Binary classification is a task that involves the classification of data into one of two distinct classes. It is widely utilized in various fields. However, conventional classifiers tend to make overconfident predictions for data that belong to overlapping regions of the two class distributions or for data outside the distributions (out-of-distribution data). Therefore, conventional classifiers should not be applied in high-risk fields where classification results can have significant consequences. In order to address this issue, it is necessary to quantify uncertainty and adopt decision-making approaches that take it into account. Many methods have been proposed for this purpose; however, implementing these methods often requires performing resampling, improving the structure or performance of models, and optimizing the thresholds of classifiers. We propose a new decision-making approach using two types of hypothesis testing. This method is capable of detecting ambiguous data that belong to the overlapping regions of two class distributions, as well as out-of-distribution data that are not included in the training data distribution. In addition, we quantify uncertainty using the empirical distribution of feature values derived from the training data obtained through the trained model. The classification threshold is determined by the $\\alpha$-quantile and ($1-\\alpha$)-quantile, where the significance level $\\alpha$ is set according to each specific situation.","sentences":["Binary classification is a task that involves the classification of data into one of two distinct classes.","It is widely utilized in various fields.","However, conventional classifiers tend to make overconfident predictions for data that belong to overlapping regions of the two class distributions or for data outside the distributions (out-of-distribution data).","Therefore, conventional classifiers should not be applied in high-risk fields where classification results can have significant consequences.","In order to address this issue, it is necessary to quantify uncertainty and adopt decision-making approaches that take it into account.","Many methods have been proposed for this purpose; however, implementing these methods often requires performing resampling, improving the structure or performance of models, and optimizing the thresholds of classifiers.","We propose a new decision-making approach using two types of hypothesis testing.","This method is capable of detecting ambiguous data that belong to the overlapping regions of two class distributions, as well as out-of-distribution data that are not included in the training data distribution.","In addition, we quantify uncertainty using the empirical distribution of feature values derived from the training data obtained through the trained model.","The classification threshold is determined by the $\\alpha$-quantile and ($1-\\alpha$)-quantile, where the significance level $\\alpha$ is set according to each specific situation."],"url":"http://arxiv.org/abs/2502.08582v1"}
{"created":"2025-02-12 17:10:53","title":"FBFL: A Field-Based Coordination Approach for Data Heterogeneity in Federated Learning","abstract":"In the last years, Federated learning (FL) has become a popular solution to train machine learning models in domains with high privacy concerns. However, FL scalability and performance face significant challenges in real-world deployments where data across devices are non-independently and identically distributed (non-IID). The heterogeneity in data distribution frequently arises from spatial distribution of devices, leading to degraded model performance in the absence of proper handling. Additionally, FL typical reliance on centralized architectures introduces bottlenecks and single-point-of-failure risks, particularly problematic at scale or in dynamic environments. To close this gap, we propose Field-Based Federated Learning (FBFL), a novel approach leveraging macroprogramming and field coordination to address these limitations through: (i) distributed spatial-based leader election for personalization to mitigate non-IID data challenges; and (ii) construction of a self-organizing, hierarchical architecture using advanced macroprogramming patterns. Moreover, FBFL not only overcomes the aforementioned limitations, but also enables the development of more specialized models tailored to the specific data distribution in each subregion. This paper formalizes FBFL and evaluates it extensively using MNIST, FashionMNIST, and Extended MNIST datasets. We demonstrate that, when operating under IID data conditions, FBFL performs comparably to the widely-used FedAvg algorithm. Furthermore, in challenging non-IID scenarios, FBFL not only outperforms FedAvg but also surpasses other state-of-the-art methods, namely FedProx and Scaffold, which have been specifically designed to address non-IID data distributions. Additionally, we showcase the resilience of FBFL's self-organizing hierarchical architecture against server failures.","sentences":["In the last years, Federated learning (FL) has become a popular solution to train machine learning models in domains with high privacy concerns.","However, FL scalability and performance face significant challenges in real-world deployments where data across devices are non-independently and identically distributed (non-IID).","The heterogeneity in data distribution frequently arises from spatial distribution of devices, leading to degraded model performance in the absence of proper handling.","Additionally, FL typical reliance on centralized architectures introduces bottlenecks and single-point-of-failure risks, particularly problematic at scale or in dynamic environments.","To close this gap, we propose Field-Based Federated Learning (FBFL), a novel approach leveraging macroprogramming and field coordination to address these limitations through: (i) distributed spatial-based leader election for personalization to mitigate non-IID data challenges; and (ii) construction of a self-organizing, hierarchical architecture using advanced macroprogramming patterns.","Moreover, FBFL not only overcomes the aforementioned limitations, but also enables the development of more specialized models tailored to the specific data distribution in each subregion.","This paper formalizes FBFL and evaluates it extensively using MNIST, FashionMNIST, and Extended MNIST datasets.","We demonstrate that, when operating under IID data conditions, FBFL performs comparably to the widely-used FedAvg algorithm.","Furthermore, in challenging non-IID scenarios, FBFL not only outperforms FedAvg but also surpasses other state-of-the-art methods, namely FedProx and Scaffold, which have been specifically designed to address non-IID data distributions.","Additionally, we showcase the resilience of FBFL's self-organizing hierarchical architecture against server failures."],"url":"http://arxiv.org/abs/2502.08577v1"}
{"created":"2025-02-12 17:07:43","title":"A Novel Approach to for Multimodal Emotion Recognition : Multimodal semantic information fusion","abstract":"With the advancement of artificial intelligence and computer vision technologies, multimodal emotion recognition has become a prominent research topic. However, existing methods face challenges such as heterogeneous data fusion and the effective utilization of modality correlations. This paper proposes a novel multimodal emotion recognition approach, DeepMSI-MER, based on the integration of contrastive learning and visual sequence compression. The proposed method enhances cross-modal feature fusion through contrastive learning and reduces redundancy in the visual modality by leveraging visual sequence compression. Experimental results on two public datasets, IEMOCAP and MELD, demonstrate that DeepMSI-MER significantly improves the accuracy and robustness of emotion recognition, validating the effectiveness of multimodal feature fusion and the proposed approach.","sentences":["With the advancement of artificial intelligence and computer vision technologies, multimodal emotion recognition has become a prominent research topic.","However, existing methods face challenges such as heterogeneous data fusion and the effective utilization of modality correlations.","This paper proposes a novel multimodal emotion recognition approach, DeepMSI-MER, based on the integration of contrastive learning and visual sequence compression.","The proposed method enhances cross-modal feature fusion through contrastive learning and reduces redundancy in the visual modality by leveraging visual sequence compression.","Experimental results on two public datasets, IEMOCAP and MELD, demonstrate that DeepMSI-MER significantly improves the accuracy and robustness of emotion recognition, validating the effectiveness of multimodal feature fusion and the proposed approach."],"url":"http://arxiv.org/abs/2502.08573v1"}
{"created":"2025-02-12 16:47:41","title":"Brain Latent Progression: Individual-based Spatiotemporal Disease Progression on 3D Brain MRIs via Latent Diffusion","abstract":"The growing availability of longitudinal Magnetic Resonance Imaging (MRI) datasets has facilitated Artificial Intelligence (AI)-driven modeling of disease progression, making it possible to predict future medical scans for individual patients. However, despite significant advancements in AI, current methods continue to face challenges including achieving patient-specific individualization, ensuring spatiotemporal consistency, efficiently utilizing longitudinal data, and managing the substantial memory demands of 3D scans. To address these challenges, we propose Brain Latent Progression (BrLP), a novel spatiotemporal model designed to predict individual-level disease progression in 3D brain MRIs. The key contributions in BrLP are fourfold: (i) it operates in a small latent space, mitigating the computational challenges posed by high-dimensional imaging data; (ii) it explicitly integrates subject metadata to enhance the individualization of predictions; (iii) it incorporates prior knowledge of disease dynamics through an auxiliary model, facilitating the integration of longitudinal data; and (iv) it introduces the Latent Average Stabilization (LAS) algorithm, which (a) enforces spatiotemporal consistency in the predicted progression at inference time and (b) allows us to derive a measure of the uncertainty for the prediction. We train and evaluate BrLP on 11,730 T1-weighted (T1w) brain MRIs from 2,805 subjects and validate its generalizability on an external test set comprising 2,257 MRIs from 962 subjects. Our experiments compare BrLP-generated MRI scans with real follow-up MRIs, demonstrating state-of-the-art accuracy compared to existing methods. The code is publicly available at: https://github.com/LemuelPuglisi/BrLP.","sentences":["The growing availability of longitudinal Magnetic Resonance Imaging (MRI) datasets has facilitated Artificial Intelligence (AI)-driven modeling of disease progression, making it possible to predict future medical scans for individual patients.","However, despite significant advancements in AI, current methods continue to face challenges including achieving patient-specific individualization, ensuring spatiotemporal consistency, efficiently utilizing longitudinal data, and managing the substantial memory demands of 3D scans.","To address these challenges, we propose Brain Latent Progression (BrLP), a novel spatiotemporal model designed to predict individual-level disease progression in 3D brain MRIs.","The key contributions in BrLP are fourfold: (i) it operates in a small latent space, mitigating the computational challenges posed by high-dimensional imaging data; (ii) it explicitly integrates subject metadata to enhance the individualization of predictions; (iii) it incorporates prior knowledge of disease dynamics through an auxiliary model, facilitating the integration of longitudinal data; and (iv) it introduces the Latent Average Stabilization (LAS) algorithm, which (a) enforces spatiotemporal consistency in the predicted progression at inference time and (b) allows us to derive a measure of the uncertainty for the prediction.","We train and evaluate BrLP on 11,730 T1-weighted (T1w) brain MRIs from 2,805 subjects and validate its generalizability on an external test set comprising 2,257 MRIs from 962 subjects.","Our experiments compare BrLP-generated MRI scans with real follow-up MRIs, demonstrating state-of-the-art accuracy compared to existing methods.","The code is publicly available at: https://github.com/LemuelPuglisi/BrLP."],"url":"http://arxiv.org/abs/2502.08560v1"}
{"created":"2025-02-12 16:30:39","title":"Copula-based mixture model identification for subgroup clustering with imaging applications","abstract":"Model-based clustering techniques have been widely applied to various application areas, while most studies focus on canonical mixtures with unique component distribution form. However, this strict assumption is often hard to satisfy. In this paper, we consider the more flexible Copula-Based Mixture Models (CBMMs) for clustering, which allow heterogeneous component distributions composed by flexible choices of marginal and copula forms. More specifically, we propose an adaptation of the Generalized Iterative Conditional Estimation (GICE) algorithm to identify the CBMMs in an unsupervised manner, where the marginal and copula forms and their parameters are estimated iteratively. GICE is adapted from its original version developed for switching Markov model identification with the choice of realization time. Our CBMM-GICE clustering method is then tested on synthetic two-cluster data (N=2000 samples) with discussion of the factors impacting its convergence. Finally, it is compared to the Expectation Maximization identified mixture models with unique component form on the entire MNIST database (N=70000), and on real cardiac magnetic resonance data (N=276) to illustrate its value for imaging applications.","sentences":["Model-based clustering techniques have been widely applied to various application areas, while most studies focus on canonical mixtures with unique component distribution form.","However, this strict assumption is often hard to satisfy.","In this paper, we consider the more flexible Copula-Based Mixture Models (CBMMs) for clustering, which allow heterogeneous component distributions composed by flexible choices of marginal and copula forms.","More specifically, we propose an adaptation of the Generalized Iterative Conditional Estimation (GICE) algorithm to identify the CBMMs in an unsupervised manner, where the marginal and copula forms and their parameters are estimated iteratively.","GICE is adapted from its original version developed for switching Markov model identification with the choice of realization time.","Our CBMM-GICE clustering method is then tested on synthetic two-cluster data (N=2000 samples) with discussion of the factors impacting its convergence.","Finally, it is compared to the Expectation Maximization identified mixture models with unique component form on the entire MNIST database (N=70000), and on real cardiac magnetic resonance data (N=276) to illustrate its value for imaging applications."],"url":"http://arxiv.org/abs/2502.08549v1"}
{"created":"2025-02-12 16:29:39","title":"Representation Learning to Advance Multi-institutional Studies with Electronic Health Record Data","abstract":"The adoption of EHRs has expanded opportunities to leverage data-driven algorithms in clinical care and research. A major bottleneck in effectively conducting multi-institutional EHR studies is the data heterogeneity across systems with numerous codes that either do not exist or represent different clinical concepts across institutions. The need for data privacy further limits the feasibility of including multi-institutional patient-level data required to study similarities and differences across patient subgroups. To address these challenges, we developed the GAME algorithm. Tested and validated across 7 institutions and 2 languages, GAME integrates data in several levels: (1) at the institutional level with knowledge graphs to establish relationships between codes and existing knowledge sources, providing the medical context for standard codes and their relationship to each other; (2) between institutions, leveraging language models to determine the relationships between institution-specific codes with established standard codes; and (3) quantifying the strength of the relationships between codes using a graph attention network. Jointly trained embeddings are created using transfer and federated learning to preserve data privacy. In this study, we demonstrate the applicability of GAME in selecting relevant features as inputs for AI-driven algorithms in a range of conditions, e.g., heart failure, rheumatoid arthritis. We then highlight the application of GAME harmonized multi-institutional EHR data in a study of Alzheimer's disease outcomes and suicide risk among patients with mental health disorders, without sharing patient-level data outside individual institutions.","sentences":["The adoption of EHRs has expanded opportunities to leverage data-driven algorithms in clinical care and research.","A major bottleneck in effectively conducting multi-institutional EHR studies is the data heterogeneity across systems with numerous codes that either do not exist or represent different clinical concepts across institutions.","The need for data privacy further limits the feasibility of including multi-institutional patient-level data required to study similarities and differences across patient subgroups.","To address these challenges, we developed the GAME algorithm.","Tested and validated across 7 institutions and 2 languages, GAME integrates data in several levels: (1) at the institutional level with knowledge graphs to establish relationships between codes and existing knowledge sources, providing the medical context for standard codes and their relationship to each other; (2) between institutions, leveraging language models to determine the relationships between institution-specific codes with established standard codes; and (3) quantifying the strength of the relationships between codes using a graph attention network.","Jointly trained embeddings are created using transfer and federated learning to preserve data privacy.","In this study, we demonstrate the applicability of GAME in selecting relevant features as inputs for AI-driven algorithms in a range of conditions, e.g., heart failure, rheumatoid arthritis.","We then highlight the application of GAME harmonized multi-institutional EHR data in a study of Alzheimer's disease outcomes and suicide risk among patients with mental health disorders, without sharing patient-level data outside individual institutions."],"url":"http://arxiv.org/abs/2502.08547v1"}
{"created":"2025-02-12 16:27:40","title":"Beyond Predictions: A Participatory Framework for Multi-Stakeholder Decision-Making","abstract":"Conventional decision-support systems, primarily based on supervised learning, focus on outcome prediction models to recommend actions. However, they often fail to account for the complexities of multi-actor environments, where diverse and potentially conflicting stakeholder preferences must be balanced. In this paper, we propose a novel participatory framework that redefines decision-making as a multi-stakeholder optimization problem, capturing each actor's preferences through context-dependent reward functions. Our framework leverages $k$-fold cross-validation to fine-tune user-provided outcome prediction models and evaluate decision strategies, including compromise functions mediating stakeholder trade-offs. We introduce a synthetic scoring mechanism that exploits user-defined preferences across multiple metrics to rank decision-making strategies and identify the optimal decision-maker. The selected decision-maker can then be used to generate actionable recommendations for new data. We validate our framework using two real-world use cases, demonstrating its ability to deliver recommendations that effectively balance multiple metrics, achieving results that are often beyond the scope of purely prediction-based methods. Ablation studies demonstrate that our framework, with its modular, model-agnostic, and inherently transparent design, integrates seamlessly with various predictive models, reward structures, evaluation metrics, and sample sizes, making it particularly suited for complex, high-stakes decision-making contexts.","sentences":["Conventional decision-support systems, primarily based on supervised learning, focus on outcome prediction models to recommend actions.","However, they often fail to account for the complexities of multi-actor environments, where diverse and potentially conflicting stakeholder preferences must be balanced.","In this paper, we propose a novel participatory framework that redefines decision-making as a multi-stakeholder optimization problem, capturing each actor's preferences through context-dependent reward functions.","Our framework leverages $k$-fold cross-validation to fine-tune user-provided outcome prediction models and evaluate decision strategies, including compromise functions mediating stakeholder trade-offs.","We introduce a synthetic scoring mechanism that exploits user-defined preferences across multiple metrics to rank decision-making strategies and identify the optimal decision-maker.","The selected decision-maker can then be used to generate actionable recommendations for new data.","We validate our framework using two real-world use cases, demonstrating its ability to deliver recommendations that effectively balance multiple metrics, achieving results that are often beyond the scope of purely prediction-based methods.","Ablation studies demonstrate that our framework, with its modular, model-agnostic, and inherently transparent design, integrates seamlessly with various predictive models, reward structures, evaluation metrics, and sample sizes, making it particularly suited for complex, high-stakes decision-making contexts."],"url":"http://arxiv.org/abs/2502.08542v1"}
{"created":"2025-02-12 16:21:01","title":"Matrix Completion with Graph Information: A Provable Nonconvex Optimization Approach","abstract":"We consider the problem of matrix completion with graphs as side information depicting the interrelations between variables. The key challenge lies in leveraging the similarity structure of the graph to enhance matrix recovery. Existing approaches, primarily based on graph Laplacian regularization, suffer from several limitations: (1) they focus only on the similarity between neighboring variables, while overlooking long-range correlations; (2) they are highly sensitive to false edges in the graphs and (3) they lack theoretical guarantees regarding statistical and computational complexities. To address these issues, we propose in this paper a novel graph regularized matrix completion algorithm called GSGD, based on preconditioned projected gradient descent approach. We demonstrate that GSGD effectively captures the higher-order correlation information behind the graphs, and achieves superior robustness and stability against the false edges. Theoretically, we prove that GSGD achieves linear convergence to the global optimum with near-optimal sample complexity, providing the first theoretical guarantees for both recovery accuracy and efficacy in the perspective of nonconvex optimization. Our numerical experiments on both synthetic and real-world data further validate that GSGD achieves superior recovery accuracy and scalability compared with several popular alternatives.","sentences":["We consider the problem of matrix completion with graphs as side information depicting the interrelations between variables.","The key challenge lies in leveraging the similarity structure of the graph to enhance matrix recovery.","Existing approaches, primarily based on graph Laplacian regularization, suffer from several limitations: (1) they focus only on the similarity between neighboring variables, while overlooking long-range correlations; (2) they are highly sensitive to false edges in the graphs and (3) they lack theoretical guarantees regarding statistical and computational complexities.","To address these issues, we propose in this paper a novel graph regularized matrix completion algorithm called GSGD, based on preconditioned projected gradient descent approach.","We demonstrate that GSGD effectively captures the higher-order correlation information behind the graphs, and achieves superior robustness and stability against the false edges.","Theoretically, we prove that GSGD achieves linear convergence to the global optimum with near-optimal sample complexity, providing the first theoretical guarantees for both recovery accuracy and efficacy in the perspective of nonconvex optimization.","Our numerical experiments on both synthetic and real-world data further validate that GSGD achieves superior recovery accuracy and scalability compared with several popular alternatives."],"url":"http://arxiv.org/abs/2502.08536v1"}
{"created":"2025-02-12 16:08:48","title":"On Different Notions of Redundancy in Conditional-Independence-Based Discovery of Graphical Models","abstract":"The goal of conditional-independence-based discovery of graphical models is to find a graph that represents the independence structure of variables in a given dataset. To learn such a representation, conditional-independence-based approaches conduct a set of statistical tests that suffices to identify the graphical representation under some assumptions on the underlying distribution of the data. In this work, we highlight that due to the conciseness of the graphical representation, there are often many tests that are not used in the construction of the graph. These redundant tests have the potential to detect or sometimes correct errors in the learned model. We show that not all tests contain this additional information and that such redundant tests have to be applied with care. Precisely, we argue that particularly those conditional (in)dependence statements are interesting that follow only from graphical assumptions but do not hold for every probability distribution.","sentences":["The goal of conditional-independence-based discovery of graphical models is to find a graph that represents the independence structure of variables in a given dataset.","To learn such a representation, conditional-independence-based approaches conduct a set of statistical tests that suffices to identify the graphical representation under some assumptions on the underlying distribution of the data.","In this work, we highlight that due to the conciseness of the graphical representation, there are often many tests that are not used in the construction of the graph.","These redundant tests have the potential to detect or sometimes correct errors in the learned model.","We show that not all tests contain this additional information and that such redundant tests have to be applied with care.","Precisely, we argue that particularly those conditional (in)dependence statements are interesting that follow only from graphical assumptions but do not hold for every probability distribution."],"url":"http://arxiv.org/abs/2502.08531v1"}
{"created":"2025-02-12 16:03:33","title":"Checkerboard Target Measurement in Unordered Point Clouds with Coloured ICP","abstract":"In this work, we investigate the problem of measuring a the centre checkerboard target in an 3D point cloud. This is an important problem which has applications in registration, long term monitoring and linking to other sensor systems. We use a 3D template matching approach based on the coloured ICP algorithm to solve the problem. We tackle the problem under the additional constraints that we assume no structure in the 3D data in order to be able to handle unordered point clouds. This gives us the capability to process data from the new generation of low-cost LIDAR sensors. This category of sensors also suffers from increased noise in range and reflectivity measurement. We provide extensive simulation results using synthetic data to capture the potential of the approach. We then give the detailed steps for handling real sensor data.","sentences":["In this work, we investigate the problem of measuring a the centre checkerboard target in an 3D point cloud.","This is an important problem which has applications in registration, long term monitoring and linking to other sensor systems.","We use a 3D template matching approach based on the coloured ICP algorithm to solve the problem.","We tackle the problem under the additional constraints that we assume no structure in the 3D data in order to be able to handle unordered point clouds.","This gives us the capability to process data from the new generation of low-cost LIDAR sensors.","This category of sensors also suffers from increased noise in range and reflectivity measurement.","We provide extensive simulation results using synthetic data to capture the potential of the approach.","We then give the detailed steps for handling real sensor data."],"url":"http://arxiv.org/abs/2502.08525v1"}
{"created":"2025-02-12 15:54:56","title":"FedMHO: Heterogeneous One-Shot Federated Learning Towards Resource-Constrained Edge Devices","abstract":"Federated Learning (FL) is increasingly adopted in edge computing scenarios, where a large number of heterogeneous clients operate under constrained or sufficient resources. The iterative training process in conventional FL introduces significant computation and communication overhead, which is unfriendly for resource-constrained edge devices. One-shot FL has emerged as a promising approach to mitigate communication overhead, and model-heterogeneous FL solves the problem of diverse computing resources across clients. However, existing methods face challenges in effectively managing model-heterogeneous one-shot FL, often leading to unsatisfactory global model performance or reliance on auxiliary datasets. To address these challenges, we propose a novel FL framework named FedMHO, which leverages deep classification models on resource-sufficient clients and lightweight generative models on resource-constrained devices. On the server side, FedMHO involves a two-stage process that includes data generation and knowledge fusion. Furthermore, we introduce FedMHO-MD and FedMHO-SD to mitigate the knowledge-forgetting problem during the knowledge fusion stage, and an unsupervised data optimization solution to improve the quality of synthetic samples. Comprehensive experiments demonstrate the effectiveness of our methods, as they outperform state-of-the-art baselines in various experimental setups.","sentences":["Federated Learning (FL) is increasingly adopted in edge computing scenarios, where a large number of heterogeneous clients operate under constrained or sufficient resources.","The iterative training process in conventional FL introduces significant computation and communication overhead, which is unfriendly for resource-constrained edge devices.","One-shot FL has emerged as a promising approach to mitigate communication overhead, and model-heterogeneous FL solves the problem of diverse computing resources across clients.","However, existing methods face challenges in effectively managing model-heterogeneous one-shot FL, often leading to unsatisfactory global model performance or reliance on auxiliary datasets.","To address these challenges, we propose a novel FL framework named FedMHO, which leverages deep classification models on resource-sufficient clients and lightweight generative models on resource-constrained devices.","On the server side, FedMHO involves a two-stage process that includes data generation and knowledge fusion.","Furthermore, we introduce FedMHO-MD and FedMHO-SD to mitigate the knowledge-forgetting problem during the knowledge fusion stage, and an unsupervised data optimization solution to improve the quality of synthetic samples.","Comprehensive experiments demonstrate the effectiveness of our methods, as they outperform state-of-the-art baselines in various experimental setups."],"url":"http://arxiv.org/abs/2502.08518v1"}
{"created":"2025-02-12 15:47:48","title":"The Paradox of Stochasticity: Limited Creativity and Computational Decoupling in Temperature-Varied LLM Outputs of Structured Fictional Data","abstract":"This study examines how temperature settings and model architectures affect the generation of structured fictional data (names, birthdates) across three large language models (LLMs): llama3.1:8b, deepseek-r1:8b, and mistral:latest. By systematically testing temperature values from 0.0 to 1.0 in increments of 0.1, we conducted 330 trials yielding 889 structured entities, validated for syntactic consistency. Key findings reveal that model architecture significantly influences computational efficiency, with mistral:latest and llama3.1:8b processing data 8x faster than deepseek-r1:8b. Contrary to expectations, temperature showed no correlation with processing time, challenging assumptions about stochastic sampling costs. Output diversity remained limited, as models consistently defaulted to common name archetypes (e.g., 'John Doe' and 'Jane Smith') across all temperatures, though rare names clustered at intermediate values (0.3-0.7). These results demonstrate that architectural optimizations, rather than temperature adjustments, dominate performance in structured generation tasks. The findings emphasize prioritizing model selection over hyperparameter tuning for efficiency and suggest explicit diversity constraints are necessary to mitigate default output biases in synthetic data pipelines.","sentences":["This study examines how temperature settings and model architectures affect the generation of structured fictional data (names, birthdates) across three large language models (LLMs): llama3.1:8b, deepseek-r1:8b, and mistral:latest.","By systematically testing temperature values from 0.0 to 1.0 in increments of 0.1, we conducted 330 trials yielding 889 structured entities, validated for syntactic consistency.","Key findings reveal that model architecture significantly influences computational efficiency, with mistral:latest and llama3.1:8b processing data 8x faster than deepseek-r1:8b.","Contrary to expectations, temperature showed no correlation with processing time, challenging assumptions about stochastic sampling costs.","Output diversity remained limited, as models consistently defaulted to common name archetypes (e.g., 'John Doe' and 'Jane Smith') across all temperatures, though rare names clustered at intermediate values (0.3-0.7).","These results demonstrate that architectural optimizations, rather than temperature adjustments, dominate performance in structured generation tasks.","The findings emphasize prioritizing model selection over hyperparameter tuning for efficiency and suggest explicit diversity constraints are necessary to mitigate default output biases in synthetic data pipelines."],"url":"http://arxiv.org/abs/2502.08515v1"}
{"created":"2025-02-12 15:36:38","title":"Bridging Domain Adaptation and Graph Neural Networks: A Tensor-Based Framework for Effective Label Propagation","abstract":"Graph Neural Networks (GNNs) have recently become the predominant tools for studying graph data. Despite state-of-the-art performance on graph classification tasks, GNNs are overwhelmingly trained in a single domain under supervision, thus necessitating a prohibitively high demand for labels and resulting in poorly transferable representations. To address this challenge, we propose the Label-Propagation Tensor Graph Neural Network (LP-TGNN) framework to bridge the gap between graph data and traditional domain adaptation methods. It extracts graph topological information holistically with a tensor architecture and then reduces domain discrepancy through label propagation. It is readily compatible with general GNNs and domain adaptation techniques with minimal adjustment through pseudo-labeling. Experiments on various real-world benchmarks show that our LP-TGNN outperforms baselines by a notable margin. We also validate and analyze each component of the proposed framework in the ablation study.","sentences":["Graph Neural Networks (GNNs) have recently become the predominant tools for studying graph data.","Despite state-of-the-art performance on graph classification tasks, GNNs are overwhelmingly trained in a single domain under supervision, thus necessitating a prohibitively high demand for labels and resulting in poorly transferable representations.","To address this challenge, we propose the Label-Propagation Tensor Graph Neural Network (LP-TGNN) framework to bridge the gap between graph data and traditional domain adaptation methods.","It extracts graph topological information holistically with a tensor architecture and then reduces domain discrepancy through label propagation.","It is readily compatible with general GNNs and domain adaptation techniques with minimal adjustment through pseudo-labeling.","Experiments on various real-world benchmarks show that our LP-TGNN outperforms baselines by a notable margin.","We also validate and analyze each component of the proposed framework in the ablation study."],"url":"http://arxiv.org/abs/2502.08505v1"}
{"created":"2025-02-12 15:26:08","title":"Salamandra Technical Report","abstract":"This work introduces Salamandra, a suite of open-source decoder-only large language models available in three different sizes: 2, 7, and 40 billion parameters. The models were trained from scratch on highly multilingual data that comprises text in 35 European languages and code. Our carefully curated corpus is made exclusively from open-access data compiled from a wide variety of sources. Along with the base models, supplementary checkpoints that were fine-tuned on public-domain instruction data are also released for chat applications. Additionally, we also share our preliminary experiments on multimodality, which serve as proof-of-concept to showcase potential applications for the Salamandra family. Our extensive evaluations on multilingual benchmarks reveal that Salamandra has strong capabilities, achieving competitive performance when compared to similarly sized open-source models. We provide comprehensive evaluation results both on standard downstream tasks as well as key aspects related to bias and safety.With this technical report, we intend to promote open science by sharing all the details behind our design choices, data curation strategy and evaluation methodology. In addition to that, we deviate from the usual practice by making our training and evaluation scripts publicly accessible. We release all models under a permissive Apache 2.0 license in order to foster future research and facilitate commercial use, thereby contributing to the open-source ecosystem of large language models.","sentences":["This work introduces Salamandra, a suite of open-source decoder-only large language models available in three different sizes: 2, 7, and 40 billion parameters.","The models were trained from scratch on highly multilingual data that comprises text in 35 European languages and code.","Our carefully curated corpus is made exclusively from open-access data compiled from a wide variety of sources.","Along with the base models, supplementary checkpoints that were fine-tuned on public-domain instruction data are also released for chat applications.","Additionally, we also share our preliminary experiments on multimodality, which serve as proof-of-concept to showcase potential applications for the Salamandra family.","Our extensive evaluations on multilingual benchmarks reveal that Salamandra has strong capabilities, achieving competitive performance when compared to similarly sized open-source models.","We provide comprehensive evaluation results both on standard downstream tasks as well as key aspects related to bias and safety.","With this technical report, we intend to promote open science by sharing all the details behind our design choices, data curation strategy and evaluation methodology.","In addition to that, we deviate from the usual practice by making our training and evaluation scripts publicly accessible.","We release all models under a permissive Apache 2.0 license in order to foster future research and facilitate commercial use, thereby contributing to the open-source ecosystem of large language models."],"url":"http://arxiv.org/abs/2502.08489v1"}
{"created":"2025-02-12 15:23:29","title":"One-Shot Federated Learning with Classifier-Free Diffusion Models","abstract":"Federated learning (FL) enables collaborative learning without data centralization but introduces significant communication costs due to multiple communication rounds between clients and the server. One-shot federated learning (OSFL) addresses this by forming a global model with a single communication round, often relying on the server's model distillation or auxiliary dataset generation - often through pre-trained diffusion models (DMs). Existing DM-assisted OSFL methods, however, typically employ classifier-guided DMs, which require training auxiliary classifier models at each client, introducing additional computation overhead. This work introduces OSCAR (One-Shot Federated Learning with Classifier-Free Diffusion Models), a novel OSFL approach that eliminates the need for auxiliary models. OSCAR uses foundation models to devise category-specific data representations at each client, seamlessly integrated into a classifier-free diffusion model pipeline for server-side data generation. OSCAR is a simple yet cost-effective OSFL approach that outperforms the state-of-the-art on four benchmarking datasets while reducing the communication load by at least 99%.","sentences":["Federated learning (FL) enables collaborative learning without data centralization but introduces significant communication costs due to multiple communication rounds between clients and the server.","One-shot federated learning (OSFL) addresses this by forming a global model with a single communication round, often relying on the server's model distillation or auxiliary dataset generation - often through pre-trained diffusion models (DMs).","Existing DM-assisted OSFL methods, however, typically employ classifier-guided DMs, which require training auxiliary classifier models at each client, introducing additional computation overhead.","This work introduces OSCAR (One-Shot Federated Learning with Classifier-Free Diffusion Models), a novel OSFL approach that eliminates the need for auxiliary models.","OSCAR uses foundation models to devise category-specific data representations at each client, seamlessly integrated into a classifier-free diffusion model pipeline for server-side data generation.","OSCAR is a simple yet cost-effective OSFL approach that outperforms the state-of-the-art on four benchmarking datasets while reducing the communication load by at least 99%."],"url":"http://arxiv.org/abs/2502.08488v1"}
{"created":"2025-02-12 15:17:04","title":"Enhancing Auto-regressive Chain-of-Thought through Loop-Aligned Reasoning","abstract":"Chain-of-Thought (CoT) prompting has emerged as a powerful technique for enhancing language model's reasoning capabilities. However, generating long and correct CoT trajectories is challenging. Recent studies have demonstrated that Looped Transformers possess remarkable length generalization capabilities, but their limited generality and adaptability prevent them from serving as an alternative to auto-regressive solutions. To better leverage the strengths of Looped Transformers, we propose RELAY (REasoning through Loop Alignment iterativelY). Specifically, we align the steps of Chain-of-Thought (CoT) reasoning with loop iterations and apply intermediate supervision during the training of Looped Transformers. This additional iteration-wise supervision not only preserves the Looped Transformer's ability for length generalization but also enables it to predict CoT reasoning steps for unseen data. Therefore, we leverage this Looped Transformer to generate accurate reasoning chains for complex problems that exceed the training length, which will then be used to fine-tune an auto-regressive model. We conduct extensive experiments, and the results demonstrate the effectiveness of our approach, with significant improvements in the performance of the auto-regressive model. Code will be released at https://github.com/qifanyu/RELAY.","sentences":["Chain-of-Thought (CoT) prompting has emerged as a powerful technique for enhancing language model's reasoning capabilities.","However, generating long and correct CoT trajectories is challenging.","Recent studies have demonstrated that Looped Transformers possess remarkable length generalization capabilities, but their limited generality and adaptability prevent them from serving as an alternative to auto-regressive solutions.","To better leverage the strengths of Looped Transformers, we propose RELAY (REasoning through Loop Alignment iterativelY).","Specifically, we align the steps of Chain-of-Thought (CoT) reasoning with loop iterations and apply intermediate supervision during the training of Looped Transformers.","This additional iteration-wise supervision not only preserves the Looped Transformer's ability for length generalization but also enables it to predict CoT reasoning steps for unseen data.","Therefore, we leverage this Looped Transformer to generate accurate reasoning chains for complex problems that exceed the training length, which will then be used to fine-tune an auto-regressive model.","We conduct extensive experiments, and the results demonstrate the effectiveness of our approach, with significant improvements in the performance of the auto-regressive model.","Code will be released at https://github.com/qifanyu/RELAY."],"url":"http://arxiv.org/abs/2502.08482v1"}
{"created":"2025-02-12 15:03:33","title":"mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data","abstract":"Multimodal embedding models have gained significant attention for their ability to map data from different modalities, such as text and images, into a unified representation space. However, the limited labeled multimodal data often hinders embedding performance. Recent approaches have leveraged data synthesis to address this problem, yet the quality of synthetic data remains a critical bottleneck. In this work, we identify three criteria for high-quality synthetic multimodal data. First, broad scope ensures that the generated data covers diverse tasks and modalities, making it applicable to various downstream scenarios. Second, robust cross-modal alignment makes different modalities semantically consistent. Third, high fidelity ensures that the synthetic data maintains realistic details to enhance its reliability. Guided by these principles, we synthesize datasets that: (1) cover a wide range of tasks, modality combinations, and languages, (2) are generated via a deep thinking process within a single pass of a multimodal large language model, and (3) incorporate real-world images with accurate and relevant texts, ensuring fidelity through self-evaluation and refinement. Leveraging these high-quality synthetic and labeled datasets, we train a multimodal multilingual E5 model mmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art performance on the MMEB Benchmark and superior multilingual performance on the XTD benchmark. Our codes, datasets and models are released in https://github.com/haon-chen/mmE5.","sentences":["Multimodal embedding models have gained significant attention for their ability to map data from different modalities, such as text and images, into a unified representation space.","However, the limited labeled multimodal data often hinders embedding performance.","Recent approaches have leveraged data synthesis to address this problem, yet the quality of synthetic data remains a critical bottleneck.","In this work, we identify three criteria for high-quality synthetic multimodal data.","First, broad scope ensures that the generated data covers diverse tasks and modalities, making it applicable to various downstream scenarios.","Second, robust cross-modal alignment makes different modalities semantically consistent.","Third, high fidelity ensures that the synthetic data maintains realistic details to enhance its reliability.","Guided by these principles, we synthesize datasets that: (1) cover a wide range of tasks, modality combinations, and languages, (2) are generated via a deep thinking process within a single pass of a multimodal large language model, and (3) incorporate real-world images with accurate and relevant texts, ensuring fidelity through self-evaluation and refinement.","Leveraging these high-quality synthetic and labeled datasets, we train a multimodal multilingual E5 model mmE5.","Extensive experiments demonstrate that mmE5 achieves state-of-the-art performance on the MMEB Benchmark and superior multilingual performance on the XTD benchmark.","Our codes, datasets and models are released in https://github.com/haon-chen/mmE5."],"url":"http://arxiv.org/abs/2502.08468v1"}
{"created":"2025-02-12 14:46:27","title":"Learning to Group and Grasp Multiple Objects","abstract":"Simultaneously grasping and transporting multiple objects can significantly enhance robotic work efficiency and has been a key research focus for decades. The primary challenge lies in determining how to push objects, group them, and execute simultaneous grasping for respective groups while considering object distribution and the hardware constraints of the robot. Traditional rule-based methods struggle to flexibly adapt to diverse scenarios. To address this challenge, this paper proposes an imitation learning-based approach. We collect a series of expert demonstrations through teleoperation and train a diffusion policy network, enabling the robot to dynamically generate action sequences for pushing, grouping, and grasping, thereby facilitating efficient multi-object grasping and transportation. We conducted experiments to evaluate the method under different training dataset sizes, varying object quantities, and real-world object scenarios. The results demonstrate that the proposed approach can effectively and adaptively generate multi-object grouping and grasping strategies. With the support of more training data, imitation learning is expected to be an effective approach for solving the multi-object grasping problem.","sentences":["Simultaneously grasping and transporting multiple objects can significantly enhance robotic work efficiency and has been a key research focus for decades.","The primary challenge lies in determining how to push objects, group them, and execute simultaneous grasping for respective groups while considering object distribution and the hardware constraints of the robot.","Traditional rule-based methods struggle to flexibly adapt to diverse scenarios.","To address this challenge, this paper proposes an imitation learning-based approach.","We collect a series of expert demonstrations through teleoperation and train a diffusion policy network, enabling the robot to dynamically generate action sequences for pushing, grouping, and grasping, thereby facilitating efficient multi-object grasping and transportation.","We conducted experiments to evaluate the method under different training dataset sizes, varying object quantities, and real-world object scenarios.","The results demonstrate that the proposed approach can effectively and adaptively generate multi-object grouping and grasping strategies.","With the support of more training data, imitation learning is expected to be an effective approach for solving the multi-object grasping problem."],"url":"http://arxiv.org/abs/2502.08452v1"}
{"created":"2025-02-12 14:36:25","title":"$\\texttt{LucidAtlas}$: Learning Uncertainty-Aware, Covariate-Disentangled, Individualized Atlas Representations","abstract":"The goal of this work is to develop principled techniques to extract information from high dimensional data sets with complex dependencies in areas such as medicine that can provide insight into individual as well as population level variation. We develop $\\texttt{LucidAtlas}$, an approach that can represent spatially varying information, and can capture the influence of covariates as well as population uncertainty. As a versatile atlas representation, $\\texttt{LucidAtlas}$ offers robust capabilities for covariate interpretation, individualized prediction, population trend analysis, and uncertainty estimation, with the flexibility to incorporate prior knowledge. Additionally, we discuss the trustworthiness and potential risks of neural additive models for analyzing dependent covariates and then introduce a marginalization approach to explain the dependence of an individual predictor on the models' response (the atlas). To validate our method, we demonstrate its generalizability on two medical datasets. Our findings underscore the critical role of by-construction interpretable models in advancing scientific discovery. Our code will be publicly available upon acceptance.","sentences":["The goal of this work is to develop principled techniques to extract information from high dimensional data sets with complex dependencies in areas such as medicine that can provide insight into individual as well as population level variation.","We develop $\\texttt{LucidAtlas}$, an approach that can represent spatially varying information, and can capture the influence of covariates as well as population uncertainty.","As a versatile atlas representation, $\\texttt{LucidAtlas}$ offers robust capabilities for covariate interpretation, individualized prediction, population trend analysis, and uncertainty estimation, with the flexibility to incorporate prior knowledge.","Additionally, we discuss the trustworthiness and potential risks of neural additive models for analyzing dependent covariates and then introduce a marginalization approach to explain the dependence of an individual predictor on the models' response (the atlas).","To validate our method, we demonstrate its generalizability on two medical datasets.","Our findings underscore the critical role of by-construction interpretable models in advancing scientific discovery.","Our code will be publicly available upon acceptance."],"url":"http://arxiv.org/abs/2502.08445v1"}
{"created":"2025-02-12 14:20:36","title":"From Haystack to Needle: Label Space Reduction for Zero-shot Classification","abstract":"We present Label Space Reduction (LSR), a novel method for improving zero-shot classification performance of Large Language Models (LLMs). LSR iteratively refines the classification label space by systematically ranking and reducing candidate classes, enabling the model to concentrate on the most relevant options. By leveraging unlabeled data with the statistical learning capabilities of data-driven models, LSR dynamically optimizes the label space representation at test time. Our experiments across seven benchmarks demonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to 14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet compared to standard zero-shot classification baselines. To reduce the computational overhead of LSR, which requires an additional LLM call at each iteration, we propose distilling the model into a probabilistic classifier, allowing for efficient inference.","sentences":["We present Label Space Reduction (LSR), a novel method for improving zero-shot classification performance of Large Language Models (LLMs).","LSR iteratively refines the classification label space by systematically ranking and reducing candidate classes, enabling the model to concentrate on the most relevant options.","By leveraging unlabeled data with the statistical learning capabilities of data-driven models, LSR dynamically optimizes the label space representation at test time.","Our experiments across seven benchmarks demonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to 14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet compared to standard zero-shot classification baselines.","To reduce the computational overhead of LSR, which requires an additional LLM call at each iteration, we propose distilling the model into a probabilistic classifier, allowing for efficient inference."],"url":"http://arxiv.org/abs/2502.08436v1"}
{"created":"2025-02-12 14:16:45","title":"Closer through commonality: Enhancing hypergraph contrastive learning with shared groups","abstract":"Hypergraphs provide a superior modeling framework for representing complex multidimensional relationships in the context of real-world interactions that often occur in groups, overcoming the limitations of traditional homogeneous graphs. However, there have been few studies on hypergraphbased contrastive learning, and existing graph-based contrastive learning methods have not been able to fully exploit the highorder correlation information in hypergraphs. Here, we propose a Hypergraph Fine-grained contrastive learning (HyFi) method designed to exploit the complex high-dimensional information inherent in hypergraphs. While avoiding traditional graph augmentation methods that corrupt the hypergraph topology, the proposed method provides a simple and efficient learning augmentation function by adding noise to node features. Furthermore, we expands beyond the traditional dichotomous relationship between positive and negative samples in contrastive learning by introducing a new relationship of weak positives. It demonstrates the importance of fine-graining positive samples in contrastive learning. Therefore, HyFi is able to produce highquality embeddings, and outperforms both supervised and unsupervised baselines in average rank on node classification across 10 datasets. Our approach effectively exploits high-dimensional hypergraph information, shows significant improvement over existing graph-based contrastive learning methods, and is efficient in terms of training speed and GPU memory cost. The source code is available at https://github.com/Noverse0/HyFi.git.","sentences":["Hypergraphs provide a superior modeling framework for representing complex multidimensional relationships in the context of real-world interactions that often occur in groups, overcoming the limitations of traditional homogeneous graphs.","However, there have been few studies on hypergraphbased contrastive learning, and existing graph-based contrastive learning methods have not been able to fully exploit the highorder correlation information in hypergraphs.","Here, we propose a Hypergraph Fine-grained contrastive learning (HyFi) method designed to exploit the complex high-dimensional information inherent in hypergraphs.","While avoiding traditional graph augmentation methods that corrupt the hypergraph topology, the proposed method provides a simple and efficient learning augmentation function by adding noise to node features.","Furthermore, we expands beyond the traditional dichotomous relationship between positive and negative samples in contrastive learning by introducing a new relationship of weak positives.","It demonstrates the importance of fine-graining positive samples in contrastive learning.","Therefore, HyFi is able to produce highquality embeddings, and outperforms both supervised and unsupervised baselines in average rank on node classification across 10 datasets.","Our approach effectively exploits high-dimensional hypergraph information, shows significant improvement over existing graph-based contrastive learning methods, and is efficient in terms of training speed and GPU memory cost.","The source code is available at https://github.com/Noverse0/HyFi.git."],"url":"http://arxiv.org/abs/2502.08432v1"}
{"created":"2025-02-12 13:59:37","title":"Handwritten Text Recognition: A Survey","abstract":"Handwritten Text Recognition (HTR) has become an essential field within pattern recognition and machine learning, with applications spanning historical document preservation to modern data entry and accessibility solutions. The complexity of HTR lies in the high variability of handwriting, which makes it challenging to develop robust recognition systems. This survey examines the evolution of HTR models, tracing their progression from early heuristic-based approaches to contemporary state-of-the-art neural models, which leverage deep learning techniques. The scope of the field has also expanded, with models initially capable of recognizing only word-level content progressing to recent end-to-end document-level approaches. Our paper categorizes existing work into two primary levels of recognition: (1) \\emph{up to line-level}, encompassing word and line recognition, and (2) \\emph{beyond line-level}, addressing paragraph- and document-level challenges. We provide a unified framework that examines research methodologies, recent advances in benchmarking, key datasets in the field, and a discussion of the results reported in the literature. Finally, we identify pressing research challenges and outline promising future directions, aiming to equip researchers and practitioners with a roadmap for advancing the field.","sentences":["Handwritten Text Recognition (HTR) has become an essential field within pattern recognition and machine learning, with applications spanning historical document preservation to modern data entry and accessibility solutions.","The complexity of HTR lies in the high variability of handwriting, which makes it challenging to develop robust recognition systems.","This survey examines the evolution of HTR models, tracing their progression from early heuristic-based approaches to contemporary state-of-the-art neural models, which leverage deep learning techniques.","The scope of the field has also expanded, with models initially capable of recognizing only word-level content progressing to recent end-to-end document-level approaches.","Our paper categorizes existing work into two primary levels of recognition: (1) \\emph{up to line-level}, encompassing word and line recognition, and (2) \\emph{beyond line-level}, addressing paragraph- and document-level challenges.","We provide a unified framework that examines research methodologies, recent advances in benchmarking, key datasets in the field, and a discussion of the results reported in the literature.","Finally, we identify pressing research challenges and outline promising future directions, aiming to equip researchers and practitioners with a roadmap for advancing the field."],"url":"http://arxiv.org/abs/2502.08417v1"}
{"created":"2025-02-12 13:47:16","title":"Quantifying Collective Emotions: Japan's Societal Trends Through Enhanced Sentiment Index Using POMS2 and SNS","abstract":"In this study, we constructed an emotion index that quantitatively represents the collective emotions present in the Japanese web space by utilizing Social Networking Service (SNS) post data. Building upon previous research that used blog data and the Profile of Mood States (POMS), we restructured the methodology using posts from X (formerly Twitter) and updated the model by adding the ``Friendliness\" indicator from the POMS2 metrics. Through periodic and trend analyses of the emotional indicators derived from X's post data, we found that the extension is consistent with results previously reported using blog data. This suggests that our methodology effectively captures typical emotional fluctuations in Japanese society, independent of specific SNS platforms, and is expected to serve as an index to visualize societal trends.","sentences":["In this study, we constructed an emotion index that quantitatively represents the collective emotions present in the Japanese web space by utilizing Social Networking Service (SNS) post data.","Building upon previous research that used blog data and the Profile of Mood States (POMS), we restructured the methodology using posts from X (formerly Twitter) and updated the model by adding the ``Friendliness\" indicator from the POMS2 metrics.","Through periodic and trend analyses of the emotional indicators derived from X's post data, we found that the extension is consistent with results previously reported using blog data.","This suggests that our methodology effectively captures typical emotional fluctuations in Japanese society, independent of specific SNS platforms, and is expected to serve as an index to visualize societal trends."],"url":"http://arxiv.org/abs/2502.08404v1"}
{"created":"2025-02-12 13:28:46","title":"ViLa-MIL: Dual-scale Vision-Language Multiple Instance Learning for Whole Slide Image Classification","abstract":"Multiple instance learning (MIL)-based framework has become the mainstream for processing the whole slide image (WSI) with giga-pixel size and hierarchical image context in digital pathology. However, these methods heavily depend on a substantial number of bag-level labels and solely learn from the original slides, which are easily affected by variations in data distribution. Recently, vision language model (VLM)-based methods introduced the language prior by pre-training on large-scale pathological image-text pairs. However, the previous text prompt lacks the consideration of pathological prior knowledge, therefore does not substantially boost the model's performance. Moreover, the collection of such pairs and the pre-training process are very time-consuming and source-intensive.To solve the above problems, we propose a dual-scale vision-language multiple instance learning (ViLa-MIL) framework for whole slide image classification. Specifically, we propose a dual-scale visual descriptive text prompt based on the frozen large language model (LLM) to boost the performance of VLM effectively. To transfer the VLM to process WSI efficiently, for the image branch, we propose a prototype-guided patch decoder to aggregate the patch features progressively by grouping similar patches into the same prototype; for the text branch, we introduce a context-guided text decoder to enhance the text features by incorporating the multi-granular image contexts. Extensive studies on three multi-cancer and multi-center subtyping datasets demonstrate the superiority of ViLa-MIL.","sentences":["Multiple instance learning (MIL)-based framework has become the mainstream for processing the whole slide image (WSI) with giga-pixel size and hierarchical image context in digital pathology.","However, these methods heavily depend on a substantial number of bag-level labels and solely learn from the original slides, which are easily affected by variations in data distribution.","Recently, vision language model (VLM)-based methods introduced the language prior by pre-training on large-scale pathological image-text pairs.","However, the previous text prompt lacks the consideration of pathological prior knowledge, therefore does not substantially boost the model's performance.","Moreover, the collection of such pairs and the pre-training process are very time-consuming and source-intensive.","To solve the above problems, we propose a dual-scale vision-language multiple instance learning (ViLa-MIL) framework for whole slide image classification.","Specifically, we propose a dual-scale visual descriptive text prompt based on the frozen large language model (LLM) to boost the performance of VLM effectively.","To transfer the VLM to process WSI efficiently, for the image branch, we propose a prototype-guided patch decoder to aggregate the patch features progressively by grouping similar patches into the same prototype; for the text branch, we introduce a context-guided text decoder to enhance the text features by incorporating the multi-granular image contexts.","Extensive studies on three multi-cancer and multi-center subtyping datasets demonstrate the superiority of ViLa-MIL."],"url":"http://arxiv.org/abs/2502.08391v1"}
{"created":"2025-02-12 13:19:12","title":"Accelerating Stable Matching between Workers and Time-Dependent Tasks for Dynamic MCS: A Stagewise Service Trading Approach","abstract":"Designing proper incentives in mobile crowdsensing (MCS) networks represents a critical mechanism in engaging distributed mobile users (workers) to contribute heterogeneous data for diverse applications (tasks). We develop a novel stagewise trading framework to reach efficient and stable matching between tasks and workers, upon considering the diversity of tasks and the dynamism of MCS networks. This framework integrates futures and spot trading stages, where in the former, we propose futures trading-driven stable matching and pre-path-planning (FT-SMP^3) for long-term task-worker assignment and pre-planning of workers' paths based on historical statistics and risk analysis. While in the latter, we investigate spot trading-driven DQN path planning and onsite worker recruitment (ST-DP^2WR) mechanism to enhance workers' and tasks' practical utilities by facilitating temporary worker recruitment. We prove that our proposed mechanisms support crucial properties such as stability, individual rationality, competitive equilibrium, and weak Pareto optimality theoretically. Also, comprehensive evaluations confirm the satisfaction of these properties in practical network settings, demonstrating our commendable performance in terms of service quality, running time, and decision-making overheads.","sentences":["Designing proper incentives in mobile crowdsensing (MCS) networks represents a critical mechanism in engaging distributed mobile users (workers) to contribute heterogeneous data for diverse applications (tasks).","We develop a novel stagewise trading framework to reach efficient and stable matching between tasks and workers, upon considering the diversity of tasks and the dynamism of MCS networks.","This framework integrates futures and spot trading stages, where in the former, we propose futures trading-driven stable matching and pre-path-planning (FT-SMP^3) for long-term task-worker assignment and pre-planning of workers' paths based on historical statistics and risk analysis.","While in the latter, we investigate spot trading-driven DQN path planning and onsite worker recruitment (ST-DP^2WR) mechanism to enhance workers' and tasks' practical utilities by facilitating temporary worker recruitment.","We prove that our proposed mechanisms support crucial properties such as stability, individual rationality, competitive equilibrium, and weak Pareto optimality theoretically.","Also, comprehensive evaluations confirm the satisfaction of these properties in practical network settings, demonstrating our commendable performance in terms of service quality, running time, and decision-making overheads."],"url":"http://arxiv.org/abs/2502.08386v1"}
{"created":"2025-02-12 13:16:07","title":"The MoE-Empowered Edge LLMs Deployment: Architecture, Challenges, and Opportunities","abstract":"The powerfulness of LLMs indicates that deploying various LLMs with different scales and architectures on end, edge, and cloud to satisfy different requirements and adaptive heterogeneous hardware is the critical way to achieve ubiquitous intelligence for 6G. However, the massive parameter scale of LLMs poses significant challenges in deploying them on edge devices due to high computational and storage demands. Considering that the sparse activation in Mixture of Experts (MoE) is effective on scalable and dynamic allocation of computational and communications resources at the edge, this paper proposes a novel MoE-empowered collaborative deployment framework for edge LLMs, denoted as CoEL. This framework fully leverages the properties of MoE architecture and encompasses four key aspects: Perception, Deployment, Compression, and Updating. Edge servers broadcast their resource status and the specific resource requirements of LLMs to their neighbors. Then, utilizing this data, two sophisticated deployment strategies are proposed for satisfying varying model scales, ensuring that each model is deployed effectively. One for deploying LLMs on a single edge device through intra-device resource collaboration, and another for a distributed deployment across multiple edge devices via inter-device resource collaboration. Furthermore, both the models and the intermediate data are compressed for reducing memory footprint by quantization and reducing the volume of intermediate data by token fusion and pruning. Finally, given the dynamic of network topology, resource status, and user requirements, the deployment strategies are regularly updated to maintain its relevance and effectiveness. This paper also delineates the challenges and potential research directions for the deployment of edge LLMs.","sentences":["The powerfulness of LLMs indicates that deploying various LLMs with different scales and architectures on end, edge, and cloud to satisfy different requirements and adaptive heterogeneous hardware is the critical way to achieve ubiquitous intelligence for 6G.","However, the massive parameter scale of LLMs poses significant challenges in deploying them on edge devices due to high computational and storage demands.","Considering that the sparse activation in Mixture of Experts (MoE) is effective on scalable and dynamic allocation of computational and communications resources at the edge, this paper proposes a novel MoE-empowered collaborative deployment framework for edge LLMs, denoted as CoEL.","This framework fully leverages the properties of MoE architecture and encompasses four key aspects: Perception, Deployment, Compression, and Updating.","Edge servers broadcast their resource status and the specific resource requirements of LLMs to their neighbors.","Then, utilizing this data, two sophisticated deployment strategies are proposed for satisfying varying model scales, ensuring that each model is deployed effectively.","One for deploying LLMs on a single edge device through intra-device resource collaboration, and another for a distributed deployment across multiple edge devices via inter-device resource collaboration.","Furthermore, both the models and the intermediate data are compressed for reducing memory footprint by quantization and reducing the volume of intermediate data by token fusion and pruning.","Finally, given the dynamic of network topology, resource status, and user requirements, the deployment strategies are regularly updated to maintain its relevance and effectiveness.","This paper also delineates the challenges and potential research directions for the deployment of edge LLMs."],"url":"http://arxiv.org/abs/2502.08381v1"}
{"created":"2025-02-12 13:05:35","title":"AdvSwap: Covert Adversarial Perturbation with High Frequency Info-swapping for Autonomous Driving Perception","abstract":"Perception module of Autonomous vehicles (AVs) are increasingly susceptible to be attacked, which exploit vulnerabilities in neural networks through adversarial inputs, thereby compromising the AI safety. Some researches focus on creating covert adversarial samples, but existing global noise techniques are detectable and difficult to deceive the human visual system. This paper introduces a novel adversarial attack method, AdvSwap, which creatively utilizes wavelet-based high-frequency information swapping to generate covert adversarial samples and fool the camera. AdvSwap employs invertible neural network for selective high-frequency information swapping, preserving both forward propagation and data integrity. The scheme effectively removes the original label data and incorporates the guidance image data, producing concealed and robust adversarial samples. Experimental evaluations and comparisons on the GTSRB and nuScenes datasets demonstrate that AdvSwap can make concealed attacks on common traffic targets. The generates adversarial samples are also difficult to perceive by humans and algorithms. Meanwhile, the method has strong attacking robustness and attacking transferability.","sentences":["Perception module of Autonomous vehicles (AVs) are increasingly susceptible to be attacked, which exploit vulnerabilities in neural networks through adversarial inputs, thereby compromising the AI safety.","Some researches focus on creating covert adversarial samples, but existing global noise techniques are detectable and difficult to deceive the human visual system.","This paper introduces a novel adversarial attack method, AdvSwap, which creatively utilizes wavelet-based high-frequency information swapping to generate covert adversarial samples and fool the camera.","AdvSwap employs invertible neural network for selective high-frequency information swapping, preserving both forward propagation and data integrity.","The scheme effectively removes the original label data and incorporates the guidance image data, producing concealed and robust adversarial samples.","Experimental evaluations and comparisons on the GTSRB and nuScenes datasets demonstrate that AdvSwap can make concealed attacks on common traffic targets.","The generates adversarial samples are also difficult to perceive by humans and algorithms.","Meanwhile, the method has strong attacking robustness and attacking transferability."],"url":"http://arxiv.org/abs/2502.08374v1"}
{"created":"2025-02-12 13:05:24","title":"Uncertainty Aware Human-machine Collaboration in Camouflaged Object Detection","abstract":"Camouflaged Object Detection (COD), the task of identifying objects concealed within their environments, has seen rapid growth due to its wide range of practical applications. A key step toward developing trustworthy COD systems is the estimation and effective utilization of uncertainty. In this work, we propose a human-machine collaboration framework for classifying the presence of camouflaged objects, leveraging the complementary strengths of computer vision (CV) models and noninvasive brain-computer interfaces (BCIs). Our approach introduces a multiview backbone to estimate uncertainty in CV model predictions, utilizes this uncertainty during training to improve efficiency, and defers low-confidence cases to human evaluation via RSVP-based BCIs during testing for more reliable decision-making. We evaluated the framework in the CAMO dataset, achieving state-of-the-art results with an average improvement of 4.56\\% in balanced accuracy (BA) and 3.66\\% in the F1 score compared to existing methods. For the best-performing participants, the improvements reached 7.6\\% in BA and 6.66\\% in the F1 score. Analysis of the training process revealed a strong correlation between our confidence measures and precision, while an ablation study confirmed the effectiveness of the proposed training policy and the human-machine collaboration strategy. In general, this work reduces human cognitive load, improves system reliability, and provides a strong foundation for advancements in real-world COD applications and human-computer interaction. Our code and data are available at: https://github.com/ziyuey/Uncertainty-aware-human-machine-collaboration-in-camouflaged-object-identification.","sentences":["Camouflaged Object Detection (COD), the task of identifying objects concealed within their environments, has seen rapid growth due to its wide range of practical applications.","A key step toward developing trustworthy COD systems is the estimation and effective utilization of uncertainty.","In this work, we propose a human-machine collaboration framework for classifying the presence of camouflaged objects, leveraging the complementary strengths of computer vision (CV) models and noninvasive brain-computer interfaces (BCIs).","Our approach introduces a multiview backbone to estimate uncertainty in CV model predictions, utilizes this uncertainty during training to improve efficiency, and defers low-confidence cases to human evaluation via RSVP-based BCIs during testing for more reliable decision-making.","We evaluated the framework in the CAMO dataset, achieving state-of-the-art results with an average improvement of 4.56\\% in balanced accuracy (BA) and 3.66\\% in the F1 score compared to existing methods.","For the best-performing participants, the improvements reached 7.6\\% in BA and 6.66\\% in the F1 score.","Analysis of the training process revealed a strong correlation between our confidence measures and precision, while an ablation study confirmed the effectiveness of the proposed training policy and the human-machine collaboration strategy.","In general, this work reduces human cognitive load, improves system reliability, and provides a strong foundation for advancements in real-world COD applications and human-computer interaction.","Our code and data are available at: https://github.com/ziyuey/Uncertainty-aware-human-machine-collaboration-in-camouflaged-object-identification."],"url":"http://arxiv.org/abs/2502.08373v1"}
{"created":"2025-02-12 12:39:51","title":"Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG","abstract":"Retrieval-Augmented Generation (RAG) has emerged as a prominent method for incorporating domain knowledge into Large Language Models (LLMs). While RAG enhances response relevance by incorporating retrieved domain knowledge in the context, retrieval errors can still lead to hallucinations and incorrect answers. To recover from retriever failures, domain knowledge is injected by fine-tuning the model to generate the correct response, even in the case of retrieval errors. However, we observe that without systematic knowledge augmentation, fine-tuned LLMs may memorize new information but still fail to extract relevant domain knowledge, leading to poor performance. In this work, we present a novel framework that significantly enhances the fine-tuning process by augmenting the training data in two ways -- context augmentation and knowledge paraphrasing. In context augmentation, we create multiple training samples for a given QA pair by varying the relevance of the retrieved information, teaching the model when to ignore and when to rely on retrieved content. In knowledge paraphrasing, we fine-tune with multiple answers to the same question, enabling LLMs to better internalize specialized knowledge. To mitigate catastrophic forgetting due to fine-tuning, we add a domain-specific identifier to a question and also utilize a replay buffer containing general QA pairs. Experimental results demonstrate the efficacy of our method over existing techniques, achieving up to 10\\% relative gain in token-level recall while preserving the LLM's generalization capabilities.","sentences":["Retrieval-Augmented Generation (RAG) has emerged as a prominent method for incorporating domain knowledge into Large Language Models (LLMs).","While RAG enhances response relevance by incorporating retrieved domain knowledge in the context, retrieval errors can still lead to hallucinations and incorrect answers.","To recover from retriever failures, domain knowledge is injected by fine-tuning the model to generate the correct response, even in the case of retrieval errors.","However, we observe that without systematic knowledge augmentation, fine-tuned LLMs may memorize new information but still fail to extract relevant domain knowledge, leading to poor performance.","In this work, we present a novel framework that significantly enhances the fine-tuning process by augmenting the training data in two ways -- context augmentation and knowledge paraphrasing.","In context augmentation, we create multiple training samples for a given QA pair by varying the relevance of the retrieved information, teaching the model when to ignore and when to rely on retrieved content.","In knowledge paraphrasing, we fine-tune with multiple answers to the same question, enabling LLMs to better internalize specialized knowledge.","To mitigate catastrophic forgetting due to fine-tuning, we add a domain-specific identifier to a question and also utilize a replay buffer containing general QA pairs.","Experimental results demonstrate the efficacy of our method over existing techniques, achieving up to 10\\% relative gain in token-level recall while preserving the LLM's generalization capabilities."],"url":"http://arxiv.org/abs/2502.08356v1"}
{"created":"2025-02-12 12:00:58","title":"Hierarchical Multi-Agent Framework for Carbon-Efficient Liquid-Cooled Data Center Clusters","abstract":"Reducing the environmental impact of cloud computing requires efficient workload distribution across geographically dispersed Data Center Clusters (DCCs) and simultaneously optimizing liquid and air (HVAC) cooling with time shift of workloads within individual data centers (DC). This paper introduces Green-DCC, which proposes a Reinforcement Learning (RL) based hierarchical controller to optimize both workload and liquid cooling dynamically in a DCC. By incorporating factors such as weather, carbon intensity, and resource availability, Green-DCC addresses realistic constraints and interdependencies. We demonstrate how the system optimizes multiple data centers synchronously, enabling the scope of digital twins, and compare the performance of various RL approaches based on carbon emissions and sustainability metrics while also offering a framework and benchmark simulation for broader ML research in sustainability.","sentences":["Reducing the environmental impact of cloud computing requires efficient workload distribution across geographically dispersed Data Center Clusters (DCCs) and simultaneously optimizing liquid and air (HVAC) cooling with time shift of workloads within individual data centers (DC).","This paper introduces Green-DCC, which proposes a Reinforcement Learning (RL) based hierarchical controller to optimize both workload and liquid cooling dynamically in a DCC.","By incorporating factors such as weather, carbon intensity, and resource availability, Green-DCC addresses realistic constraints and interdependencies.","We demonstrate how the system optimizes multiple data centers synchronously, enabling the scope of digital twins, and compare the performance of various RL approaches based on carbon emissions and sustainability metrics while also offering a framework and benchmark simulation for broader ML research in sustainability."],"url":"http://arxiv.org/abs/2502.08337v1"}
{"created":"2025-02-12 12:00:16","title":"Salience-Invariant Consistent Policy Learning for Generalization in Visual Reinforcement Learning","abstract":"Generalizing policies to unseen scenarios remains a critical challenge in visual reinforcement learning, where agents often overfit to the specific visual observations of the training environment. In unseen environments, distracting pixels may lead agents to extract representations containing task-irrelevant information. As a result, agents may deviate from the optimal behaviors learned during training, thereby hindering visual generalization.To address this issue, we propose the Salience-Invariant Consistent Policy Learning (SCPL) algorithm, an efficient framework for zero-shot generalization. Our approach introduces a novel value consistency module alongside a dynamics module to effectively capture task-relevant representations. The value consistency module, guided by saliency, ensures the agent focuses on task-relevant pixels in both original and perturbed observations, while the dynamics module uses augmented data to help the encoder capture dynamic- and reward-relevant representations. Additionally, our theoretical analysis highlights the importance of policy consistency for generalization. To strengthen this, we introduce a policy consistency module with a KL divergence constraint to maintain consistent policies across original and perturbed observations.Extensive experiments on the DMC-GB, Robotic Manipulation, and CARLA benchmarks demonstrate that SCPL significantly outperforms state-of-the-art methods in terms of generalization. Notably, SCPL achieves average performance improvements of 14\\%, 39\\%, and 69\\% in the challenging DMC video hard setting, the Robotic hard setting, and the CARLA benchmark, respectively.Project Page: https://sites.google.com/view/scpl-rl.","sentences":["Generalizing policies to unseen scenarios remains a critical challenge in visual reinforcement learning, where agents often overfit to the specific visual observations of the training environment.","In unseen environments, distracting pixels may lead agents to extract representations containing task-irrelevant information.","As a result, agents may deviate from the optimal behaviors learned during training, thereby hindering visual generalization.","To address this issue, we propose the Salience-Invariant Consistent Policy Learning (SCPL) algorithm, an efficient framework for zero-shot generalization.","Our approach introduces a novel value consistency module alongside a dynamics module to effectively capture task-relevant representations.","The value consistency module, guided by saliency, ensures the agent focuses on task-relevant pixels in both original and perturbed observations, while the dynamics module uses augmented data to help the encoder capture dynamic-","and reward-relevant representations.","Additionally, our theoretical analysis highlights the importance of policy consistency for generalization.","To strengthen this, we introduce a policy consistency module with a KL divergence constraint to maintain consistent policies across original and perturbed observations.","Extensive experiments on the DMC-GB, Robotic Manipulation, and CARLA benchmarks demonstrate that SCPL significantly outperforms state-of-the-art methods in terms of generalization.","Notably, SCPL achieves average performance improvements of 14\\%, 39\\%, and 69\\% in the challenging DMC video hard setting, the Robotic hard setting, and the CARLA benchmark, respectively.","Project Page: https://sites.google.com/view/scpl-rl."],"url":"http://arxiv.org/abs/2502.08336v1"}
{"created":"2025-02-12 11:57:11","title":"Foundation Models in Computational Pathology: A Review of Challenges, Opportunities, and Impact","abstract":"From self-supervised, vision-only models to contrastive visual-language frameworks, computational pathology has rapidly evolved in recent years. Generative AI \"co-pilots\" now demonstrate the ability to mine subtle, sub-visual tissue cues across the cellular-to-pathology spectrum, generate comprehensive reports, and respond to complex user queries. The scale of data has surged dramatically, growing from tens to millions of multi-gigapixel tissue images, while the number of trainable parameters in these models has risen to several billion. The critical question remains: how will this new wave of generative and multi-purpose AI transform clinical diagnostics? In this article, we explore the true potential of these innovations and their integration into clinical practice. We review the rapid progress of foundation models in pathology, clarify their applications and significance. More precisely, we examine the very definition of foundational models, identifying what makes them foundational, general, or multipurpose, and assess their impact on computational pathology. Additionally, we address the unique challenges associated with their development and evaluation. These models have demonstrated exceptional predictive and generative capabilities, but establishing global benchmarks is crucial to enhancing evaluation standards and fostering their widespread clinical adoption. In computational pathology, the broader impact of frontier AI ultimately depends on widespread adoption and societal acceptance. While direct public exposure is not strictly necessary, it remains a powerful tool for dispelling misconceptions, building trust, and securing regulatory support.","sentences":["From self-supervised, vision-only models to contrastive visual-language frameworks, computational pathology has rapidly evolved in recent years.","Generative AI \"co-pilots\" now demonstrate the ability to mine subtle, sub-visual tissue cues across the cellular-to-pathology spectrum, generate comprehensive reports, and respond to complex user queries.","The scale of data has surged dramatically, growing from tens to millions of multi-gigapixel tissue images, while the number of trainable parameters in these models has risen to several billion.","The critical question remains: how will this new wave of generative and multi-purpose AI transform clinical diagnostics?","In this article, we explore the true potential of these innovations and their integration into clinical practice.","We review the rapid progress of foundation models in pathology, clarify their applications and significance.","More precisely, we examine the very definition of foundational models, identifying what makes them foundational, general, or multipurpose, and assess their impact on computational pathology.","Additionally, we address the unique challenges associated with their development and evaluation.","These models have demonstrated exceptional predictive and generative capabilities, but establishing global benchmarks is crucial to enhancing evaluation standards and fostering their widespread clinical adoption.","In computational pathology, the broader impact of frontier AI ultimately depends on widespread adoption and societal acceptance.","While direct public exposure is not strictly necessary, it remains a powerful tool for dispelling misconceptions, building trust, and securing regulatory support."],"url":"http://arxiv.org/abs/2502.08333v1"}
{"created":"2025-02-12 11:54:15","title":"Brame: Hierarchical Data Management Framework for Cloud-Edge-Device Collaboration","abstract":"In the realm of big data, cloud-edge-device collaboration is prevalent in industrial scenarios. However, a systematic exploration of the theory and methodologies related to data management in this field is lacking. This paper delves into the sub-problem of data storage and scheduling within cloud-edge-device collaborative environments. Following extensive research and analysis of the characteristics and requirements of data management in cloud-edge collaboration, it is evident that existing studies on hierarchical data management primarily focus on the migration of hot and cold data. Additionally, these studies encounter challenges such as elevated operational and maintenance costs, difficulties in locating data within tiered storage, and intricate metadata management attributable to excessively fine-grained management granularity. These challenges impede the fulfillment of the storage needs in cloud-edge-device collaboration.   To overcome these challenges, we propose a \\underline{B}lock-based hie\\underline{R}archical d\\underline{A}ta \\underline{M}anagement fram\\underline{E}work, \\textbf{Brame}, which advocates for a workload-aware three-tier storage architecture and suggests a shift from using tuples to employing $Blocks$ as the fundamental unit for data management. \\textbf{Brame} owns an offline block generation method designed to facilitate efficient block generation and expeditious query routing. Extensive experiments substantiate the superior performance of \\textbf{Brame}.","sentences":["In the realm of big data, cloud-edge-device collaboration is prevalent in industrial scenarios.","However, a systematic exploration of the theory and methodologies related to data management in this field is lacking.","This paper delves into the sub-problem of data storage and scheduling within cloud-edge-device collaborative environments.","Following extensive research and analysis of the characteristics and requirements of data management in cloud-edge collaboration, it is evident that existing studies on hierarchical data management primarily focus on the migration of hot and cold data.","Additionally, these studies encounter challenges such as elevated operational and maintenance costs, difficulties in locating data within tiered storage, and intricate metadata management attributable to excessively fine-grained management granularity.","These challenges impede the fulfillment of the storage needs in cloud-edge-device collaboration.   ","To overcome these challenges, we propose a \\underline{B}lock-based hie\\underline{R}archical d\\underline{A}ta \\underline{M}anagement fram\\underline{E}work, \\textbf{Brame}, which advocates for a workload-aware three-tier storage architecture and suggests a shift from using tuples to employing $Blocks$ as the fundamental unit for data management.","\\textbf{Brame} owns an offline block generation method designed to facilitate efficient block generation and expeditious query routing.","Extensive experiments substantiate the superior performance of \\textbf{Brame}."],"url":"http://arxiv.org/abs/2502.08331v1"}
{"created":"2025-02-12 11:48:15","title":"Model-Free Counterfactual Subset Selection at Scale","abstract":"Ensuring transparency in AI decision-making requires interpretable explanations, particularly at the instance level. Counterfactual explanations are a powerful tool for this purpose, but existing techniques frequently depend on synthetic examples, introducing biases from unrealistic assumptions, flawed models, or skewed data. Many methods also assume full dataset availability, an impractical constraint in real-time environments where data flows continuously. In contrast, streaming explanations offer adaptive, real-time insights without requiring persistent storage of the entire dataset. This work introduces a scalable, model-free approach to selecting diverse and relevant counterfactual examples directly from observed data. Our algorithm operates efficiently in streaming settings, maintaining $O(\\log k)$ update complexity per item while ensuring high-quality counterfactual selection. Empirical evaluations on both real-world and synthetic datasets demonstrate superior performance over baseline methods, with robust behavior even under adversarial conditions.","sentences":["Ensuring transparency in AI decision-making requires interpretable explanations, particularly at the instance level.","Counterfactual explanations are a powerful tool for this purpose, but existing techniques frequently depend on synthetic examples, introducing biases from unrealistic assumptions, flawed models, or skewed data.","Many methods also assume full dataset availability, an impractical constraint in real-time environments where data flows continuously.","In contrast, streaming explanations offer adaptive, real-time insights without requiring persistent storage of the entire dataset.","This work introduces a scalable, model-free approach to selecting diverse and relevant counterfactual examples directly from observed data.","Our algorithm operates efficiently in streaming settings, maintaining $O(\\log k)$ update complexity per item while ensuring high-quality counterfactual selection.","Empirical evaluations on both real-world and synthetic datasets demonstrate superior performance over baseline methods, with robust behavior even under adversarial conditions."],"url":"http://arxiv.org/abs/2502.08326v1"}
{"created":"2025-02-12 10:42:04","title":"Data Pricing for Graph Neural Networks without Pre-purchased Inspection","abstract":"Machine learning (ML) models have become essential tools in various scenarios. Their effectiveness, however, hinges on a substantial volume of data for satisfactory performance. Model marketplaces have thus emerged as crucial platforms bridging model consumers seeking ML solutions and data owners possessing valuable data. These marketplaces leverage model trading mechanisms to properly incentive data owners to contribute their data, and return a well performing ML model to the model consumers. However, existing model trading mechanisms often assume the data owners are willing to share their data before being paid, which is not reasonable in real world. Given that, we propose a novel mechanism, named Structural Importance based Model Trading (SIMT) mechanism, that assesses the data importance and compensates data owners accordingly without disclosing the data. Specifically, SIMT procures feature and label data from data owners according to their structural importance, and then trains a graph neural network for model consumers. Theoretically, SIMT ensures incentive compatible, individual rational and budget feasible. The experiments on five popular datasets validate that SIMT consistently outperforms vanilla baselines by up to $40\\%$ in both MacroF1 and MicroF1.","sentences":["Machine learning (ML) models have become essential tools in various scenarios.","Their effectiveness, however, hinges on a substantial volume of data for satisfactory performance.","Model marketplaces have thus emerged as crucial platforms bridging model consumers seeking ML solutions and data owners possessing valuable data.","These marketplaces leverage model trading mechanisms to properly incentive data owners to contribute their data, and return a well performing ML model to the model consumers.","However, existing model trading mechanisms often assume the data owners are willing to share their data before being paid, which is not reasonable in real world.","Given that, we propose a novel mechanism, named Structural Importance based Model Trading (SIMT) mechanism, that assesses the data importance and compensates data owners accordingly without disclosing the data.","Specifically, SIMT procures feature and label data from data owners according to their structural importance, and then trains a graph neural network for model consumers.","Theoretically, SIMT ensures incentive compatible, individual rational and budget feasible.","The experiments on five popular datasets validate that SIMT consistently outperforms vanilla baselines by up to $40\\%$ in both MacroF1 and MicroF1."],"url":"http://arxiv.org/abs/2502.08284v1"}
{"created":"2025-02-12 10:41:21","title":"Individualised Treatment Effects Estimation with Composite Treatments and Composite Outcomes","abstract":"Estimating individualised treatment effect (ITE) -- that is the causal effect of a set of variables (also called exposures, treatments, actions, policies, or interventions), referred to as \\textit{composite treatments}, on a set of outcome variables of interest, referred to as \\textit{composite outcomes}, for a unit from observational data -- remains a fundamental problem in causal inference with applications across disciplines, such as healthcare, economics, education, social science, marketing, and computer science. Previous work in causal machine learning for ITE estimation is limited to simple settings, like single treatments and single outcomes. This hinders their use in complex real-world scenarios; for example, consider studying the effect of different ICU interventions, such as beta-blockers and statins for a patient admitted for heart surgery, on different outcomes of interest such as atrial fibrillation and in-hospital mortality. The limited research into composite treatments and outcomes is primarily due to data scarcity for all treatments and outcomes. To address the above challenges, we propose a novel and innovative hypernetwork-based approach, called \\emph{H-Learner}, to solve ITE estimation under composite treatments and composite outcomes, which tackles the data scarcity issue by dynamically sharing information across treatments and outcomes. Our empirical analysis with binary and arbitrary composite treatments and outcomes demonstrates the effectiveness of the proposed approach compared to existing methods.","sentences":["Estimating individualised treatment effect (ITE) -- that is the causal effect of a set of variables (also called exposures, treatments, actions, policies, or interventions), referred to as \\textit{composite treatments}, on a set of outcome variables of interest, referred to as \\textit{composite outcomes}, for a unit from observational data -- remains a fundamental problem in causal inference with applications across disciplines, such as healthcare, economics, education, social science, marketing, and computer science.","Previous work in causal machine learning for ITE estimation is limited to simple settings, like single treatments and single outcomes.","This hinders their use in complex real-world scenarios; for example, consider studying the effect of different ICU interventions, such as beta-blockers and statins for a patient admitted for heart surgery, on different outcomes of interest such as atrial fibrillation and in-hospital mortality.","The limited research into composite treatments and outcomes is primarily due to data scarcity for all treatments and outcomes.","To address the above challenges, we propose a novel and innovative hypernetwork-based approach, called \\emph{H-Learner}, to solve ITE estimation under composite treatments and composite outcomes, which tackles the data scarcity issue by dynamically sharing information across treatments and outcomes.","Our empirical analysis with binary and arbitrary composite treatments and outcomes demonstrates the effectiveness of the proposed approach compared to existing methods."],"url":"http://arxiv.org/abs/2502.08282v1"}
{"created":"2025-02-12 10:27:18","title":"Weighted Pseudorandom Generators for Read-Once Branching Programs via Weighted Pseudorandom Reductions","abstract":"We study weighted pseudorandom generators (WPRGs) and derandomizations for read-once branching programs (ROBPs), which are key problems towards answering the fundamental open question $\\mathbf{BPL} \\stackrel{?}{=} \\mathbf{L}$. Denote $n$ and $w$ as the length and the width of a ROBP. We have the following results.   For standard ROBPs, there exists an explicit $\\varepsilon$-WPRG with seed length $$ O\\left(\\frac{\\log n\\log (nw)}{\\max\\left\\{1,\\log\\log w-\\log\\log n\\right\\}}+\\log w \\left(\\log\\log\\log w-\\log\\log\\max\\left\\{2,\\frac{\\log w}{\\log n/\\varepsilon}\\right\\}\\right)+\\log(1/\\varepsilon)\\right).$$ When $n = w^{o(1)},$ this is better than the constructions in Hoza (RANDOM 2022), Cohen, Doron, Renard, Sberlo, and Ta-Shma (CCC 2021).   For permutation ROBPs with unbounded widths and single accept nodes, there exists an explicit $\\varepsilon$-WPRG with seed length $$ O\\left( \\log n\\left( \\log\\log n + \\sqrt{\\log(1/\\varepsilon)} \\right)+\\log(1/\\varepsilon)\\right). $$ This slightly improves the result of Chen, Hoza, Lyu, Tal, and Wu (FOCS 2023).   For regular ROBPs with $n \\leq 2^{O(\\sqrt{\\log w})}, \\varepsilon = 1/\\text{poly} w$, we give a derandomization within space $O(\\log w)$, i.e. in $\\mathbf{L}$ exactly.   This is better than previous results of Ahmadinejad, Kelner, Murtagh, Peebles, Sidford, and Vadhan (FOCS 2020) in this regime.   Our main method is based on a recursive application of weighted pseudorandom reductions, which is a natural notion that is used to simplify ROBPs.","sentences":["We study weighted pseudorandom generators (WPRGs) and derandomizations for read-once branching programs (ROBPs), which are key problems towards answering the fundamental open question $\\mathbf{BPL} \\stackrel{?}{=} \\mathbf{L}$. Denote $n$ and $w$ as the length and the width of a ROBP.","We have the following results.   ","For standard ROBPs, there exists an explicit $\\varepsilon$-WPRG with seed length $$ O\\left(\\frac{\\log n\\log (nw)}{\\max\\left\\{1,\\log\\log w-\\log\\log n\\right\\}}+\\log w \\left(\\log\\log\\log w-\\log\\log\\max\\left\\{2,\\frac{\\log w}{\\log n/\\varepsilon}\\right\\}\\right)+\\log(1/\\varepsilon)\\right).$$ When $n = w^{o(1)},$ this is better than the constructions in Hoza (RANDOM 2022), Cohen, Doron, Renard, Sberlo, and Ta-Shma (CCC 2021).   ","For permutation ROBPs with unbounded widths and single accept nodes, there exists an explicit $\\varepsilon$-WPRG with seed length $$ O\\left( \\log n\\left( \\log\\log n + \\sqrt{\\log(1/\\varepsilon)} \\right)+\\log(1/\\varepsilon)\\right).","$$ This slightly improves the result of Chen, Hoza, Lyu, Tal, and Wu (FOCS 2023).   ","For regular ROBPs with $n \\leq 2^{O(\\sqrt{\\log w})}, \\varepsilon = 1/\\text{poly} w$, we give a derandomization within space $O(\\log w)$, i.e. in $\\mathbf{L}$ exactly.   ","This is better than previous results of Ahmadinejad, Kelner, Murtagh, Peebles, Sidford, and Vadhan (FOCS 2020) in this regime.   ","Our main method is based on a recursive application of weighted pseudorandom reductions, which is a natural notion that is used to simplify ROBPs."],"url":"http://arxiv.org/abs/2502.08272v1"}
{"created":"2025-02-12 10:24:22","title":"MoLoRec: A Generalizable and Efficient Framework for LLM-Based Recommendation","abstract":"Large Language Models (LLMs) have achieved remarkable success in recent years, owing to their impressive generalization capabilities and rich world knowledge. To capitalize on the potential of using LLMs as recommender systems, mainstream approaches typically focus on two paradigms. The first paradigm designs multi-domain or multi-task instruction data for generalizable recommendation, so as to align LLMs with general recommendation areas and deal with cold-start recommendation. The second paradigm enhances domain-specific recommendation tasks with parameter-efficient fine-tuning techniques, in order to improve models under the warm recommendation scenarios. While most previous works treat these two paradigms separately, we argue that they have complementary advantages, and combining them together would be helpful.   To that end, in this paper, we propose a generalizable and efficient LLM-based recommendation framework MoLoRec. Our approach starts by parameter-efficient fine-tuning a domain-general module with general recommendation instruction data, to align LLM with recommendation knowledge. Then, given users' behavior of a specific domain, we construct a domain-specific instruction dataset and apply efficient fine-tuning to the pre-trained LLM. After that, we provide approaches to integrate the above domain-general part and domain-specific part with parameters mixture. Please note that, MoLoRec is efficient with plug and play, as the domain-general module is trained only once, and any domain-specific plug-in can be efficiently merged with only domain-specific fine-tuning. Extensive experiments on multiple datasets under both warm and cold-start recommendation scenarios validate the effectiveness and generality of the proposed MoLoRec.","sentences":["Large Language Models (LLMs) have achieved remarkable success in recent years, owing to their impressive generalization capabilities and rich world knowledge.","To capitalize on the potential of using LLMs as recommender systems, mainstream approaches typically focus on two paradigms.","The first paradigm designs multi-domain or multi-task instruction data for generalizable recommendation, so as to align LLMs with general recommendation areas and deal with cold-start recommendation.","The second paradigm enhances domain-specific recommendation tasks with parameter-efficient fine-tuning techniques, in order to improve models under the warm recommendation scenarios.","While most previous works treat these two paradigms separately, we argue that they have complementary advantages, and combining them together would be helpful.   ","To that end, in this paper, we propose a generalizable and efficient LLM-based recommendation framework MoLoRec.","Our approach starts by parameter-efficient fine-tuning a domain-general module with general recommendation instruction data, to align LLM with recommendation knowledge.","Then, given users' behavior of a specific domain, we construct a domain-specific instruction dataset and apply efficient fine-tuning to the pre-trained LLM.","After that, we provide approaches to integrate the above domain-general part and domain-specific part with parameters mixture.","Please note that, MoLoRec is efficient with plug and play, as the domain-general module is trained only once, and any domain-specific plug-in can be efficiently merged with only domain-specific fine-tuning.","Extensive experiments on multiple datasets under both warm and cold-start recommendation scenarios validate the effectiveness and generality of the proposed MoLoRec."],"url":"http://arxiv.org/abs/2502.08271v1"}
{"created":"2025-02-12 10:21:06","title":"Cost Preserving Dependent Rounding for Allocation Problems","abstract":"We present a dependent randomized rounding scheme, which rounds fractional solutions to integral solutions satisfying certain hard constraints on the output while preserving Chernoff-like concentration properties. In contrast to previous dependent rounding schemes, our algorithm guarantees that the cost of the rounded integral solution does not exceed that of the fractional solution. Our algorithm works for a class of assignment problems with restrictions similar to those of prior works.   In a non-trivial combination of our general result with a classical approach from Shmoys and Tardos [Math. Programm.'93] and more recent linear programming techniques developed for the restricted assignment variant by Bansal, Sviridenko [STOC'06] and Davies, Rothvoss, Zhang [SODA'20], we derive a O(log n)-approximation algorithm for the Budgeted Santa Claus Problem. In this new variant, the goal is to allocate resources with different values to players, maximizing the minimum value a player receives, and satisfying a budget constraint on player-resource allocation costs.","sentences":["We present a dependent randomized rounding scheme, which rounds fractional solutions to integral solutions satisfying certain hard constraints on the output while preserving Chernoff-like concentration properties.","In contrast to previous dependent rounding schemes, our algorithm guarantees that the cost of the rounded integral solution does not exceed that of the fractional solution.","Our algorithm works for a class of assignment problems with restrictions similar to those of prior works.   ","In a non-trivial combination of our general result with a classical approach from Shmoys and Tardos [Math.","Programm.'93] and more recent linear programming techniques developed for the restricted assignment variant by Bansal, Sviridenko [STOC'06] and Davies, Rothvoss, Zhang","[SODA'20], we derive a O(log n)-approximation algorithm for the Budgeted Santa Claus Problem.","In this new variant, the goal is to allocate resources with different values to players, maximizing the minimum value a player receives, and satisfying a budget constraint on player-resource allocation costs."],"url":"http://arxiv.org/abs/2502.08267v1"}
{"created":"2025-02-12 10:19:50","title":"Dealing with Annotator Disagreement in Hate Speech Classification","abstract":"Hate speech detection is a crucial task, especially on social media, where harmful content can spread quickly. Implementing machine learning models to automatically identify and address hate speech is essential for mitigating its impact and preventing its proliferation. The first step in developing an effective hate speech detection model is to acquire a high-quality dataset for training. Labeled data is foundational for most natural language processing tasks, but categorizing hate speech is difficult due to the diverse and often subjective nature of hate speech, which can lead to varying interpretations and disagreements among annotators. This paper examines strategies for addressing annotator disagreement, an issue that has been largely overlooked. In particular, we evaluate different approaches to deal with annotator disagreement regarding hate speech classification in Turkish tweets, based on a fine-tuned BERT model. Our work highlights the importance of the problem and provides state-of-art benchmark results for detection and understanding of hate speech in online discourse.","sentences":["Hate speech detection is a crucial task, especially on social media, where harmful content can spread quickly.","Implementing machine learning models to automatically identify and address hate speech is essential for mitigating its impact and preventing its proliferation.","The first step in developing an effective hate speech detection model is to acquire a high-quality dataset for training.","Labeled data is foundational for most natural language processing tasks, but categorizing hate speech is difficult due to the diverse and often subjective nature of hate speech, which can lead to varying interpretations and disagreements among annotators.","This paper examines strategies for addressing annotator disagreement, an issue that has been largely overlooked.","In particular, we evaluate different approaches to deal with annotator disagreement regarding hate speech classification in Turkish tweets, based on a fine-tuned BERT model.","Our work highlights the importance of the problem and provides state-of-art benchmark results for detection and understanding of hate speech in online discourse."],"url":"http://arxiv.org/abs/2502.08266v1"}
{"created":"2025-02-12 10:10:04","title":"GenIAS: Generator for Instantiating Anomalies in time Series","abstract":"A recent and promising approach for building time series anomaly detection (TSAD) models is to inject synthetic samples of anomalies within real data sets. The existing injection mechanisms have significant limitations - most of them rely on ad hoc, hand-crafted strategies which fail to capture the natural diversity of anomalous patterns, or are restricted to univariate time series settings. To address these challenges, we design a generative model for TSAD using a variational autoencoder, which is referred to as a Generator for Instantiating Anomalies in Time Series (GenIAS). GenIAS is designed to produce diverse and realistic synthetic anomalies for TSAD tasks. By employing a novel learned perturbation mechanism in the latent space and injecting the perturbed patterns in different segments of time series, GenIAS can generate anomalies with greater diversity and varying scales. Further, guided by a new triplet loss function, which uses a min-max margin and a new variance-scaling approach to further enforce the learning of compact normal patterns, GenIAS ensures that anomalies are distinct from normal samples while remaining realistic. The approach is effective for both univariate and multivariate time series. We demonstrate the diversity and realism of the generated anomalies. Our extensive experiments demonstrate that GenIAS - when integrated into a TSAD task - consistently outperforms seventeen traditional and deep anomaly detection models, thereby highlighting the potential of generative models for time series anomaly generation.","sentences":["A recent and promising approach for building time series anomaly detection (TSAD) models is to inject synthetic samples of anomalies within real data sets.","The existing injection mechanisms have significant limitations - most of them rely on ad hoc, hand-crafted strategies which fail to capture the natural diversity of anomalous patterns, or are restricted to univariate time series settings.","To address these challenges, we design a generative model for TSAD using a variational autoencoder, which is referred to as a Generator for Instantiating Anomalies in Time Series (GenIAS).","GenIAS is designed to produce diverse and realistic synthetic anomalies for TSAD tasks.","By employing a novel learned perturbation mechanism in the latent space and injecting the perturbed patterns in different segments of time series, GenIAS can generate anomalies with greater diversity and varying scales.","Further, guided by a new triplet loss function, which uses a min-max margin and a new variance-scaling approach to further enforce the learning of compact normal patterns, GenIAS ensures that anomalies are distinct from normal samples while remaining realistic.","The approach is effective for both univariate and multivariate time series.","We demonstrate the diversity and realism of the generated anomalies.","Our extensive experiments demonstrate that GenIAS - when integrated into a TSAD task - consistently outperforms seventeen traditional and deep anomaly detection models, thereby highlighting the potential of generative models for time series anomaly generation."],"url":"http://arxiv.org/abs/2502.08262v1"}
{"created":"2025-02-12 10:05:25","title":"Balancing optimism and pessimism in offline-to-online learning","abstract":"We consider what we call the offline-to-online learning setting, focusing on stochastic finite-armed bandit problems. In offline-to-online learning, a learner starts with offline data collected from interactions with an unknown environment in a way that is not under the learner's control. Given this data, the learner begins interacting with the environment, gradually improving its initial strategy as it collects more data to maximize its total reward. The learner in this setting faces a fundamental dilemma: if the policy is deployed for only a short period, a suitable strategy (in a number of senses) is the Lower Confidence Bound (LCB) algorithm, which is based on pessimism. LCB can effectively compete with any policy that is sufficiently \"covered\" by the offline data. However, for longer time horizons, a preferred strategy is the Upper Confidence Bound (UCB) algorithm, which is based on optimism. Over time, UCB converges to the performance of the optimal policy at a rate that is nearly the best possible among all online algorithms. In offline-to-online learning, however, UCB initially explores excessively, leading to worse short-term performance compared to LCB. This suggests that a learner not in control of how long its policy will be in use should start with LCB for short horizons and gradually transition to a UCB-like strategy as more rounds are played. This article explores how and why this transition should occur. Our main result shows that our new algorithm performs nearly as well as the better of LCB and UCB at any point in time. The core idea behind our algorithm is broadly applicable, and we anticipate that our results will extend beyond the multi-armed bandit setting.","sentences":["We consider what we call the offline-to-online learning setting, focusing on stochastic finite-armed bandit problems.","In offline-to-online learning, a learner starts with offline data collected from interactions with an unknown environment in a way that is not under the learner's control.","Given this data, the learner begins interacting with the environment, gradually improving its initial strategy as it collects more data to maximize its total reward.","The learner in this setting faces a fundamental dilemma: if the policy is deployed for only a short period, a suitable strategy (in a number of senses) is the Lower Confidence Bound (LCB) algorithm, which is based on pessimism.","LCB can effectively compete with any policy that is sufficiently \"covered\" by the offline data.","However, for longer time horizons, a preferred strategy is the Upper Confidence Bound (UCB) algorithm, which is based on optimism.","Over time, UCB converges to the performance of the optimal policy at a rate that is nearly the best possible among all online algorithms.","In offline-to-online learning, however, UCB initially explores excessively, leading to worse short-term performance compared to LCB.","This suggests that a learner not in control of how long its policy will be in use should start with LCB for short horizons and gradually transition to a UCB-like strategy as more rounds are played.","This article explores how and why this transition should occur.","Our main result shows that our new algorithm performs nearly as well as the better of LCB and UCB at any point in time.","The core idea behind our algorithm is broadly applicable, and we anticipate that our results will extend beyond the multi-armed bandit setting."],"url":"http://arxiv.org/abs/2502.08259v1"}
{"created":"2025-02-12 09:39:54","title":"Inference-time sparse attention with asymmetric indexing","abstract":"Self-attention in transformer models is an incremental associative memory that maps key vectors to value vectors. One way to speed up self-attention is to employ GPU-compliant vector search algorithms, yet the standard partitioning methods yield poor results in this context, because (1) keys and queries follow different distributions and (2) the effect of RoPE positional encoding.   In this paper, we introduce SAAP (Self-Attention with Asymmetric Partitions), which overcomes these problems. It is an asymmetrical indexing technique that employs distinct partitions for keys and queries, thereby approximating self-attention with a data-adaptive sparsity pattern.   It works on pretrained language models without finetuning, as it only requires to train (offline) a small query classifier. On a long context Llama 3.1-8b model, with sequences ranging from 100k to 500k tokens, our method typically reduces by a factor 20 the fraction of memory that needs to be looked-up, which translates to a time saving of 60\\% when compared to FlashAttention-v2.","sentences":["Self-attention in transformer models is an incremental associative memory that maps key vectors to value vectors.","One way to speed up self-attention is to employ GPU-compliant vector search algorithms, yet the standard partitioning methods yield poor results in this context, because (1) keys and queries follow different distributions and (2) the effect of RoPE positional encoding.   ","In this paper, we introduce SAAP (Self-Attention with Asymmetric Partitions), which overcomes these problems.","It is an asymmetrical indexing technique that employs distinct partitions for keys and queries, thereby approximating self-attention with a data-adaptive sparsity pattern.   ","It works on pretrained language models without finetuning, as it only requires to train (offline) a small query classifier.","On a long context Llama 3.1-8b model, with sequences ranging from 100k to 500k tokens, our method typically reduces by a factor 20 the fraction of memory that needs to be looked-up, which translates to a time saving of 60\\% when compared to FlashAttention-v2."],"url":"http://arxiv.org/abs/2502.08246v1"}
{"created":"2025-02-12 09:21:40","title":"Learning Human Skill Generators at Key-Step Levels","abstract":"We are committed to learning human skill generators at key-step levels. The generation of skills is a challenging endeavor, but its successful implementation could greatly facilitate human skill learning and provide more experience for embodied intelligence. Although current video generation models can synthesis simple and atomic human operations, they struggle with human skills due to their complex procedure process. Human skills involve multi-step, long-duration actions and complex scene transitions, so the existing naive auto-regressive methods for synthesizing long videos cannot generate human skills. To address this, we propose a novel task, the Key-step Skill Generation (KS-Gen), aimed at reducing the complexity of generating human skill videos. Given the initial state and a skill description, the task is to generate video clips of key steps to complete the skill, rather than a full-length video. To support this task, we introduce a carefully curated dataset and define multiple evaluation metrics to assess performance. Considering the complexity of KS-Gen, we propose a new framework for this task. First, a multimodal large language model (MLLM) generates descriptions for key steps using retrieval argument. Subsequently, we use a Key-step Image Generator (KIG) to address the discontinuity between key steps in skill videos. Finally, a video generation model uses these descriptions and key-step images to generate video clips of the key steps with high temporal consistency. We offer a detailed analysis of the results, hoping to provide more insights on human skill generation. All models and data are available at https://github.com/MCG-NJU/KS-Gen.","sentences":["We are committed to learning human skill generators at key-step levels.","The generation of skills is a challenging endeavor, but its successful implementation could greatly facilitate human skill learning and provide more experience for embodied intelligence.","Although current video generation models can synthesis simple and atomic human operations, they struggle with human skills due to their complex procedure process.","Human skills involve multi-step, long-duration actions and complex scene transitions, so the existing naive auto-regressive methods for synthesizing long videos cannot generate human skills.","To address this, we propose a novel task, the Key-step Skill Generation (KS-Gen), aimed at reducing the complexity of generating human skill videos.","Given the initial state and a skill description, the task is to generate video clips of key steps to complete the skill, rather than a full-length video.","To support this task, we introduce a carefully curated dataset and define multiple evaluation metrics to assess performance.","Considering the complexity of KS-Gen, we propose a new framework for this task.","First, a multimodal large language model (MLLM) generates descriptions for key steps using retrieval argument.","Subsequently, we use a Key-step Image Generator (KIG) to address the discontinuity between key steps in skill videos.","Finally, a video generation model uses these descriptions and key-step images to generate video clips of the key steps with high temporal consistency.","We offer a detailed analysis of the results, hoping to provide more insights on human skill generation.","All models and data are available at https://github.com/MCG-NJU/KS-Gen."],"url":"http://arxiv.org/abs/2502.08234v1"}
{"created":"2025-02-12 09:20:36","title":"ChemZIP: Accelerated Modeling of Complex Aerothermochemical Interactions in Novel Turbomachines for Sustainable High-Temperature Chemical Processes","abstract":"This paper introduces a new platform to accelerate the modeling of complex aerothermochemical interactions in new turbomachines, turbo-reactors, to decarbonise chemical processes. While previous work has aerothermally demonstrated the potential to decarbonize the heat input to the reaction, optimizing the reaction efficiency has been a challenge. This is because measuring reaction performance with aerochemical simulations is computationally prohibitive due to the uniquely complex aerodynamics and chemistry within turbomachines. To address this, we introduce a new multifidelity machine-learning-assisted methodology, called ChemZIP, to mitigate this bottleneck. Although data-driven methodologies exist for combustion, modeling reactive flows along the bladed path of a turbomachine poses new challenges. This has led to a novel training data generation process, which allows rich dynamic responses of the chemical system to be embedded into the training dataset at a fraction of the cost of reacting flow simulations. The resulting high-dimensional composition vector is compressed into a low-dimensional basis using an autoencoder-like neural network, inspired by but more universal than traditional flamelet-generated manifolds. Verification against 10,000 unseen one-dimensional test conditions shows an R2 score exceeding 95% across all quantities of interest. Following this, ChemZIP is coupled into a fully-fledged viscous computational fluid dynamics solver. For a set of process-relevant three-dimensional configurations entirely different from the training data, the predictive accuracy of the thermochemical state remains within 10% of an industry-standard solver while convergence is achieved 50 times faster, even for a small mechanism. Therefore, numerical computations are sufficiently fast that aerothermochemical optimization is now feasible for the first time in the design cycle","sentences":["This paper introduces a new platform to accelerate the modeling of complex aerothermochemical interactions in new turbomachines, turbo-reactors, to decarbonise chemical processes.","While previous work has aerothermally demonstrated the potential to decarbonize the heat input to the reaction, optimizing the reaction efficiency has been a challenge.","This is because measuring reaction performance with aerochemical simulations is computationally prohibitive due to the uniquely complex aerodynamics and chemistry within turbomachines.","To address this, we introduce a new multifidelity machine-learning-assisted methodology, called ChemZIP, to mitigate this bottleneck.","Although data-driven methodologies exist for combustion, modeling reactive flows along the bladed path of a turbomachine poses new challenges.","This has led to a novel training data generation process, which allows rich dynamic responses of the chemical system to be embedded into the training dataset at a fraction of the cost of reacting flow simulations.","The resulting high-dimensional composition vector is compressed into a low-dimensional basis using an autoencoder-like neural network, inspired by but more universal than traditional flamelet-generated manifolds.","Verification against 10,000 unseen one-dimensional test conditions shows an R2 score exceeding 95% across all quantities of interest.","Following this, ChemZIP is coupled into a fully-fledged viscous computational fluid dynamics solver.","For a set of process-relevant three-dimensional configurations entirely different from the training data, the predictive accuracy of the thermochemical state remains within 10% of an industry-standard solver while convergence is achieved 50 times faster, even for a small mechanism.","Therefore, numerical computations are sufficiently fast that aerothermochemical optimization is now feasible for the first time in the design cycle"],"url":"http://arxiv.org/abs/2502.08232v1"}
{"created":"2025-02-12 09:01:25","title":"Take What You Need: Flexible Multi-Task Semantic Communications with Channel Adaptation","abstract":"The growing demand for efficient semantic communication systems capable of managing diverse tasks and adapting to fluctuating channel conditions has driven the development of robust, resource-efficient frameworks. This article introduces a novel channel-adaptive and multi-task-aware semantic communication framework based on a masked auto-encoder architecture. Our framework optimizes the transmission of meaningful information by incorporating a multi-task-aware scoring mechanism that identifies and prioritizes semantically significant data across multiple concurrent tasks. A channel-aware extractor is employed to dynamically select relevant information in response to real-time channel conditions. By jointly optimizing semantic relevance and transmission efficiency, the framework ensures minimal performance degradation under resource constraints. Experimental results demonstrate the superior performance of our framework compared to conventional methods in tasks such as image reconstruction and object detection. These results underscore the framework's adaptability to heterogeneous channel environments and its scalability for multi-task applications, positioning it as a promising solution for next-generation semantic communication networks.","sentences":["The growing demand for efficient semantic communication systems capable of managing diverse tasks and adapting to fluctuating channel conditions has driven the development of robust, resource-efficient frameworks.","This article introduces a novel channel-adaptive and multi-task-aware semantic communication framework based on a masked auto-encoder architecture.","Our framework optimizes the transmission of meaningful information by incorporating a multi-task-aware scoring mechanism that identifies and prioritizes semantically significant data across multiple concurrent tasks.","A channel-aware extractor is employed to dynamically select relevant information in response to real-time channel conditions.","By jointly optimizing semantic relevance and transmission efficiency, the framework ensures minimal performance degradation under resource constraints.","Experimental results demonstrate the superior performance of our framework compared to conventional methods in tasks such as image reconstruction and object detection.","These results underscore the framework's adaptability to heterogeneous channel environments and its scalability for multi-task applications, positioning it as a promising solution for next-generation semantic communication networks."],"url":"http://arxiv.org/abs/2502.08221v1"}
{"created":"2025-02-12 08:54:49","title":"Investigating Vulnerabilities of GPS Trip Data to Trajectory-User Linking Attacks","abstract":"Open human mobility data is considered an essential basis for the profound research and analysis required for the transition to sustainable mobility and sustainable urban planning. Cycling data has especially been the focus of data collection endeavors in recent years. Although privacy risks regarding location data are widely known, practitioners often refrain from advanced privacy mechanisms to prevent utility losses. Removing user identifiers from trips is thereby deemed a major privacy gain, as it supposedly prevents linking single trips to obtain entire movement patterns. In this paper, we propose a novel attack to reconstruct user identifiers in GPS trip datasets consisting of single trips, unlike previous ones that are dedicated to evaluating trajectory-user linking in the context of check-in data. We evaluate the remaining privacy risk for users in such datasets and our empirical findings from two real-world datasets show that the risk of re-identification is significant even when personal identifiers have been removed, and that truncation as a simple additional privacy mechanism may not be effective in protecting user privacy. Further investigations indicate that users who frequently visit locations that are only visited by a small number of others, tend to be more vulnerable to re-identification.","sentences":["Open human mobility data is considered an essential basis for the profound research and analysis required for the transition to sustainable mobility and sustainable urban planning.","Cycling data has especially been the focus of data collection endeavors in recent years.","Although privacy risks regarding location data are widely known, practitioners often refrain from advanced privacy mechanisms to prevent utility losses.","Removing user identifiers from trips is thereby deemed a major privacy gain, as it supposedly prevents linking single trips to obtain entire movement patterns.","In this paper, we propose a novel attack to reconstruct user identifiers in GPS trip datasets consisting of single trips, unlike previous ones that are dedicated to evaluating trajectory-user linking in the context of check-in data.","We evaluate the remaining privacy risk for users in such datasets and our empirical findings from two real-world datasets show that the risk of re-identification is significant even when personal identifiers have been removed, and that truncation as a simple additional privacy mechanism may not be effective in protecting user privacy.","Further investigations indicate that users who frequently visit locations that are only visited by a small number of others, tend to be more vulnerable to re-identification."],"url":"http://arxiv.org/abs/2502.08217v1"}
{"created":"2025-02-12 08:51:33","title":"Deepfake Detection with Spatio-Temporal Consistency and Attention","abstract":"Deepfake videos are causing growing concerns among communities due to their ever-increasing realism. Naturally, automated detection of forged Deepfake videos is attracting a proportional amount of interest of researchers. Current methods for detecting forged videos mainly rely on global frame features and under-utilize the spatio-temporal inconsistencies found in the manipulated videos. Moreover, they fail to attend to manipulation-specific subtle and well-localized pattern variations along both spatial and temporal dimensions. Addressing these gaps, we propose a neural Deepfake detector that focuses on the localized manipulative signatures of the forged videos at individual frame level as well as frame sequence level. Using a ResNet backbone, it strengthens the shallow frame-level feature learning with a spatial attention mechanism. The spatial stream of the model is further helped by fusing texture enhanced shallow features with the deeper features. Simultaneously, the model processes frame sequences with a distance attention mechanism that further allows fusion of temporal attention maps with the learned features at the deeper layers. The overall model is trained to detect forged content as a classifier. We evaluate our method on two popular large data sets and achieve significant performance over the state-of-the-art methods.Moreover, our technique also provides memory and computational advantages over the competitive techniques.","sentences":["Deepfake videos are causing growing concerns among communities due to their ever-increasing realism.","Naturally, automated detection of forged Deepfake videos is attracting a proportional amount of interest of researchers.","Current methods for detecting forged videos mainly rely on global frame features and under-utilize the spatio-temporal inconsistencies found in the manipulated videos.","Moreover, they fail to attend to manipulation-specific subtle and well-localized pattern variations along both spatial and temporal dimensions.","Addressing these gaps, we propose a neural Deepfake detector that focuses on the localized manipulative signatures of the forged videos at individual frame level as well as frame sequence level.","Using a ResNet backbone, it strengthens the shallow frame-level feature learning with a spatial attention mechanism.","The spatial stream of the model is further helped by fusing texture enhanced shallow features with the deeper features.","Simultaneously, the model processes frame sequences with a distance attention mechanism that further allows fusion of temporal attention maps with the learned features at the deeper layers.","The overall model is trained to detect forged content as a classifier.","We evaluate our method on two popular large data sets and achieve significant performance over the state-of-the-art methods.","Moreover, our technique also provides memory and computational advantages over the competitive techniques."],"url":"http://arxiv.org/abs/2502.08216v1"}
{"created":"2025-02-12 08:40:57","title":"Quality over Quantity: Boosting Data Efficiency Through Ensembled Multimodal Data Curation","abstract":"In an era overwhelmed by vast amounts of data, the effective curation of web-crawl datasets is essential for optimizing model performance. This paper tackles the challenges associated with the unstructured and heterogeneous nature of such datasets. Traditional heuristic curation methods often inadequately capture complex features, resulting in biases and the exclusion of relevant data. We introduce an advanced, learning-driven approach, Ensemble Curation Of DAta ThroUgh Multimodal Operators (EcoDatum), incorporating a novel quality-guided deduplication method to ensure balanced feature distributions. EcoDatum strategically integrates various unimodal and multimodal data curation operators within a weak supervision ensemble framework, utilizing automated optimization to score each data point effectively. EcoDatum, which significantly improves the data curation quality and efficiency, outperforms existing state-of-the-art (SOTA) techniques, ranked 1st on the DataComp leaderboard, with an average performance score of 0.182 across 38 diverse evaluation datasets. This represents a 28% improvement over the DataComp baseline method, demonstrating its effectiveness in improving dataset curation and model training efficiency.","sentences":["In an era overwhelmed by vast amounts of data, the effective curation of web-crawl datasets is essential for optimizing model performance.","This paper tackles the challenges associated with the unstructured and heterogeneous nature of such datasets.","Traditional heuristic curation methods often inadequately capture complex features, resulting in biases and the exclusion of relevant data.","We introduce an advanced, learning-driven approach, Ensemble Curation Of DAta ThroUgh Multimodal Operators (EcoDatum), incorporating a novel quality-guided deduplication method to ensure balanced feature distributions.","EcoDatum strategically integrates various unimodal and multimodal data curation operators within a weak supervision ensemble framework, utilizing automated optimization to score each data point effectively.","EcoDatum, which significantly improves the data curation quality and efficiency, outperforms existing state-of-the-art (SOTA) techniques, ranked 1st on the DataComp leaderboard, with an average performance score of 0.182 across 38 diverse evaluation datasets.","This represents a 28% improvement over the DataComp baseline method, demonstrating its effectiveness in improving dataset curation and model training efficiency."],"url":"http://arxiv.org/abs/2502.08211v1"}
{"created":"2025-02-12 08:39:26","title":"Equivariant Masked Position Prediction for Efficient Molecular Representation","abstract":"Graph neural networks (GNNs) have shown considerable promise in computational chemistry. However, the limited availability of molecular data raises concerns regarding GNNs' ability to effectively capture the fundamental principles of physics and chemistry, which constrains their generalization capabilities. To address this challenge, we introduce a novel self-supervised approach termed Equivariant Masked Position Prediction (EMPP), grounded in intramolecular potential and force theory. Unlike conventional attribute masking techniques, EMPP formulates a nuanced position prediction task that is more well-defined and enhances the learning of quantum mechanical features. EMPP also bypasses the approximation of the Gaussian mixture distribution commonly used in denoising methods, allowing for more accurate acquisition of physical properties. Experimental results indicate that EMPP significantly enhances performance of advanced molecular architectures, surpassing state-of-the-art self-supervised approaches. Our code is released in https://github.com/ajy112/EMPP.","sentences":["Graph neural networks (GNNs) have shown considerable promise in computational chemistry.","However, the limited availability of molecular data raises concerns regarding GNNs' ability to effectively capture the fundamental principles of physics and chemistry, which constrains their generalization capabilities.","To address this challenge, we introduce a novel self-supervised approach termed Equivariant Masked Position Prediction (EMPP), grounded in intramolecular potential and force theory.","Unlike conventional attribute masking techniques, EMPP formulates a nuanced position prediction task that is more well-defined and enhances the learning of quantum mechanical features.","EMPP also bypasses the approximation of the Gaussian mixture distribution commonly used in denoising methods, allowing for more accurate acquisition of physical properties.","Experimental results indicate that EMPP significantly enhances performance of advanced molecular architectures, surpassing state-of-the-art self-supervised approaches.","Our code is released in https://github.com/ajy112/EMPP."],"url":"http://arxiv.org/abs/2502.08209v1"}
{"created":"2025-02-12 08:35:10","title":"Wisdom of the Crowds in Forecasting: Forecast Summarization for Supporting Future Event Prediction","abstract":"Future Event Prediction (FEP) is an essential activity whose demand and application range across multiple domains. While traditional methods like simulations, predictive and time-series forecasting have demonstrated promising outcomes, their application in forecasting complex events is not entirely reliable due to the inability of numerical data to accurately capture the semantic information related to events. One forecasting way is to gather and aggregate collective opinions on the future to make predictions as cumulative perspectives carry the potential to help estimating the likelihood of upcoming events. In this work, we organize the existing research and frameworks that aim to support future event prediction based on crowd wisdom through aggregating individual forecasts. We discuss the challenges involved, available datasets, as well as the scope of improvement and future research directions for this task. We also introduce a novel data model to represent individual forecast statements.","sentences":["Future Event Prediction (FEP) is an essential activity whose demand and application range across multiple domains.","While traditional methods like simulations, predictive and time-series forecasting have demonstrated promising outcomes, their application in forecasting complex events is not entirely reliable due to the inability of numerical data to accurately capture the semantic information related to events.","One forecasting way is to gather and aggregate collective opinions on the future to make predictions as cumulative perspectives carry the potential to help estimating the likelihood of upcoming events.","In this work, we organize the existing research and frameworks that aim to support future event prediction based on crowd wisdom through aggregating individual forecasts.","We discuss the challenges involved, available datasets, as well as the scope of improvement and future research directions for this task.","We also introduce a novel data model to represent individual forecast statements."],"url":"http://arxiv.org/abs/2502.08205v1"}
{"created":"2025-02-12 08:32:10","title":"Privacy amplification by random allocation","abstract":"We consider the privacy guarantees of an algorithm in which a user's data is used in $k$ steps randomly and uniformly chosen from a sequence (or set) of $t$ differentially private steps. We demonstrate that the privacy guarantees of this sampling scheme can be upper bound by the privacy guarantees of the well-studied independent (or Poisson) subsampling in which each step uses the user's data with probability $(1+ o(1))k/t $. Further, we provide two additional analysis techniques that lead to numerical improvements in some parameter regimes. The case of $k=1$ has been previously studied in the context of DP-SGD in Balle et al. (2020) and very recently in Chua et al. (2024). Privacy analysis of Balle et al. (2020) relies on privacy amplification by shuffling which leads to overly conservative bounds. Privacy analysis of Chua et al. (2024a) relies on Monte Carlo simulations that are computationally prohibitive in many practical scenarios and have additional inherent limitations.","sentences":["We consider the privacy guarantees of an algorithm in which a user's data is used in $k$ steps randomly and uniformly chosen from a sequence (or set) of $t$ differentially private steps.","We demonstrate that the privacy guarantees of this sampling scheme can be upper bound by the privacy guarantees of the well-studied independent (or Poisson) subsampling in which each step uses the user's data with probability $(1+ o(1))k/t $.","Further, we provide two additional analysis techniques that lead to numerical improvements in some parameter regimes.","The case of $k=1$ has been previously studied in the context of DP-SGD in Balle et al. (2020) and very recently in Chua et al. (2024).","Privacy analysis of Balle et al. (2020) relies on privacy amplification by shuffling which leads to overly conservative bounds.","Privacy analysis of Chua et al. (2024a) relies on Monte Carlo simulations that are computationally prohibitive in many practical scenarios and have additional inherent limitations."],"url":"http://arxiv.org/abs/2502.08202v1"}
{"created":"2025-02-12 08:24:36","title":"ActiveSSF: An Active-Learning-Guided Self-Supervised Framework for Long-Tailed Megakaryocyte Classification","abstract":"Precise classification of megakaryocytes is crucial for diagnosing myelodysplastic syndromes. Although self-supervised learning has shown promise in medical image analysis, its application to classifying megakaryocytes in stained slides faces three main challenges: (1) pervasive background noise that obscures cellular details, (2) a long-tailed distribution that limits data for rare subtypes, and (3) complex morphological variations leading to high intra-class variability. To address these issues, we propose the ActiveSSF framework, which integrates active learning with self-supervised pretraining. Specifically, our approach employs Gaussian filtering combined with K-means clustering and HSV analysis (augmented by clinical prior knowledge) for accurate region-of-interest extraction; an adaptive sample selection mechanism that dynamically adjusts similarity thresholds to mitigate class imbalance; and prototype clustering on labeled samples to overcome morphological complexity. Experimental results on clinical megakaryocyte datasets demonstrate that ActiveSSF not only achieves state-of-the-art performance but also significantly improves recognition accuracy for rare subtypes. Moreover, the integration of these advanced techniques further underscores the practical potential of ActiveSSF in clinical settings. To foster further research, the code and datasets will be publicly released in the future.","sentences":["Precise classification of megakaryocytes is crucial for diagnosing myelodysplastic syndromes.","Although self-supervised learning has shown promise in medical image analysis, its application to classifying megakaryocytes in stained slides faces three main challenges: (1) pervasive background noise that obscures cellular details, (2) a long-tailed distribution that limits data for rare subtypes, and (3) complex morphological variations leading to high intra-class variability.","To address these issues, we propose the ActiveSSF framework, which integrates active learning with self-supervised pretraining.","Specifically, our approach employs Gaussian filtering combined with K-means clustering and HSV analysis (augmented by clinical prior knowledge) for accurate region-of-interest extraction; an adaptive sample selection mechanism that dynamically adjusts similarity thresholds to mitigate class imbalance; and prototype clustering on labeled samples to overcome morphological complexity.","Experimental results on clinical megakaryocyte datasets demonstrate that ActiveSSF not only achieves state-of-the-art performance but also significantly improves recognition accuracy for rare subtypes.","Moreover, the integration of these advanced techniques further underscores the practical potential of ActiveSSF in clinical settings.","To foster further research, the code and datasets will be publicly released in the future."],"url":"http://arxiv.org/abs/2502.08200v1"}
{"created":"2025-02-12 07:39:44","title":"Latest Advancements Towards Catastrophic Forgetting under Data Scarcity: A Comprehensive Survey on Few-Shot Class Incremental Learning","abstract":"Data scarcity significantly complicates the continual learning problem, i.e., how a deep neural network learns in dynamic environments with very few samples. However, the latest progress of few-shot class incremental learning (FSCIL) methods and related studies show insightful knowledge on how to tackle the problem. This paper presents a comprehensive survey on FSCIL that highlights several important aspects i.e. comprehensive and formal objectives of FSCIL approaches, the importance of prototype rectifications, the new learning paradigms based on pre-trained model and language-guided mechanism, the deeper analysis of FSCIL performance metrics and evaluation, and the practical contexts of FSCIL in various areas. Our extensive discussion presents the open challenges, potential solutions, and future directions of FSCIL.","sentences":["Data scarcity significantly complicates the continual learning problem, i.e., how a deep neural network learns in dynamic environments with very few samples.","However, the latest progress of few-shot class incremental learning (FSCIL) methods and related studies show insightful knowledge on how to tackle the problem.","This paper presents a comprehensive survey on FSCIL that highlights several important aspects i.e. comprehensive and formal objectives of FSCIL approaches, the importance of prototype rectifications, the new learning paradigms based on pre-trained model and language-guided mechanism, the deeper analysis of FSCIL performance metrics and evaluation, and the practical contexts of FSCIL in various areas.","Our extensive discussion presents the open challenges, potential solutions, and future directions of FSCIL."],"url":"http://arxiv.org/abs/2502.08181v1"}
{"created":"2025-02-12 07:37:39","title":"Enhancing LLM Character-Level Manipulation via Divide and Conquer","abstract":"Large Language Models (LLMs) have demonstrated strong generalization capabilities across a wide range of natural language processing (NLP) tasks. However, they exhibit notable weaknesses in character-level string manipulation, struggling with fundamental operations such as character deletion, insertion, and substitution. These challenges stem primarily from tokenization constraints, despite the critical role of such operations in data preprocessing and code generation. Through systematic analysis, we derive two key insights: (1) LLMs face significant difficulties in leveraging intrinsic token knowledge for character-level reasoning, and (2) atomized word structures can substantially enhance LLMs' ability to process token-level structural information. Building on these insights, we propose Character-Level Manipulation via Divide and Conquer, a novel approach designed to bridge the gap between token-level processing and character-level manipulation. Our method decomposes complex operations into explicit character-level subtasks coupled with controlled token reconstruction phases, leading to significant improvements in accuracy. Without additional training, our method significantly improves accuracies on the $\\texttt{Deletion}$, $\\texttt{Insertion}$, and $\\texttt{Substitution}$ tasks. To support further research, we open-source our implementation and benchmarks.","sentences":["Large Language Models (LLMs) have demonstrated strong generalization capabilities across a wide range of natural language processing (NLP) tasks.","However, they exhibit notable weaknesses in character-level string manipulation, struggling with fundamental operations such as character deletion, insertion, and substitution.","These challenges stem primarily from tokenization constraints, despite the critical role of such operations in data preprocessing and code generation.","Through systematic analysis, we derive two key insights: (1) LLMs face significant difficulties in leveraging intrinsic token knowledge for character-level reasoning, and (2) atomized word structures can substantially enhance LLMs' ability to process token-level structural information.","Building on these insights, we propose Character-Level Manipulation via Divide and Conquer, a novel approach designed to bridge the gap between token-level processing and character-level manipulation.","Our method decomposes complex operations into explicit character-level subtasks coupled with controlled token reconstruction phases, leading to significant improvements in accuracy.","Without additional training, our method significantly improves accuracies on the $\\texttt{Deletion}$, $\\texttt{Insertion}$, and $\\texttt{Substitution}$ tasks.","To support further research, we open-source our implementation and benchmarks."],"url":"http://arxiv.org/abs/2502.08180v1"}
{"created":"2025-02-12 07:26:13","title":"Intention is All You Need: Refining Your Code from Your Intention","abstract":"Code refinement aims to enhance existing code by addressing issues, refactoring, and optimizing to improve quality and meet specific requirements. As software projects scale in size and complexity, the traditional iterative exchange between reviewers and developers becomes increasingly burdensome. While recent deep learning techniques have been explored to accelerate this process, their performance remains limited, primarily due to challenges in accurately understanding reviewers' intents.   This paper proposes an intention-based code refinement technique that enhances the conventional comment-to-code process by explicitly extracting reviewer intentions from the comments. Our approach consists of two key phases: Intention Extraction and Intention Guided Revision Generation. Intention Extraction categorizes comments using predefined templates, while Intention Guided Revision Generation employs large language models (LLMs) to generate revised code based on these defined intentions. Three categories with eight subcategories are designed for comment transformation, which is followed by a hybrid approach that combines rule-based and LLM-based classifiers for accurate classification. Extensive experiments with five LLMs (GPT4o, GPT3.5, DeepSeekV2, DeepSeek7B, CodeQwen7B) under different prompting settings demonstrate that our approach achieves 79% accuracy in intention extraction and up to 66% in code refinement generation. Our results highlight the potential of our approach in enhancing data quality and improving the efficiency of code refinement.","sentences":["Code refinement aims to enhance existing code by addressing issues, refactoring, and optimizing to improve quality and meet specific requirements.","As software projects scale in size and complexity, the traditional iterative exchange between reviewers and developers becomes increasingly burdensome.","While recent deep learning techniques have been explored to accelerate this process, their performance remains limited, primarily due to challenges in accurately understanding reviewers' intents.   ","This paper proposes an intention-based code refinement technique that enhances the conventional comment-to-code process by explicitly extracting reviewer intentions from the comments.","Our approach consists of two key phases: Intention Extraction and Intention Guided Revision Generation.","Intention Extraction categorizes comments using predefined templates, while Intention Guided Revision Generation employs large language models (LLMs) to generate revised code based on these defined intentions.","Three categories with eight subcategories are designed for comment transformation, which is followed by a hybrid approach that combines rule-based and LLM-based classifiers for accurate classification.","Extensive experiments with five LLMs (GPT4o, GPT3.5, DeepSeekV2, DeepSeek7B, CodeQwen7B) under different prompting settings demonstrate that our approach achieves 79% accuracy in intention extraction and up to 66% in code refinement generation.","Our results highlight the potential of our approach in enhancing data quality and improving the efficiency of code refinement."],"url":"http://arxiv.org/abs/2502.08172v1"}
{"created":"2025-02-12 07:11:33","title":"From Individual Experience to Collective Evidence: A Reporting-Based Framework for Identifying Systemic Harms","abstract":"When an individual reports a negative interaction with some system, how can their personal experience be contextualized within broader patterns of system behavior? We study the incident database problem, where individual reports of adverse events arrive sequentially, and are aggregated over time. In this work, our goal is to identify whether there are subgroups--defined by any combination of relevant features--that are disproportionately likely to experience harmful interactions with the system. We formalize this problem as a sequential hypothesis test, and identify conditions on reporting behavior that are sufficient for making inferences about disparities in true rates of harm across subgroups. We show that algorithms for sequential hypothesis tests can be applied to this problem with a standard multiple testing correction. We then demonstrate our method on real-world datasets, including mortgage decisions and vaccine side effects; on each, our method (re-)identifies subgroups known to experience disproportionate harm using only a fraction of the data that was initially used to discover them.","sentences":["When an individual reports a negative interaction with some system, how can their personal experience be contextualized within broader patterns of system behavior?","We study the incident database problem, where individual reports of adverse events arrive sequentially, and are aggregated over time.","In this work, our goal is to identify whether there are subgroups--defined by any combination of relevant features--that are disproportionately likely to experience harmful interactions with the system.","We formalize this problem as a sequential hypothesis test, and identify conditions on reporting behavior that are sufficient for making inferences about disparities in true rates of harm across subgroups.","We show that algorithms for sequential hypothesis tests can be applied to this problem with a standard multiple testing correction.","We then demonstrate our method on real-world datasets, including mortgage decisions and vaccine side effects; on each, our method (re-)identifies subgroups known to experience disproportionate harm using only a fraction of the data that was initially used to discover them."],"url":"http://arxiv.org/abs/2502.08166v1"}
{"created":"2025-02-12 07:03:32","title":"Vertical Federated Learning in Practice: The Good, the Bad, and the Ugly","abstract":"Vertical Federated Learning (VFL) is a privacy-preserving collaborative learning paradigm that enables multiple parties with distinct feature sets to jointly train machine learning models without sharing their raw data. Despite its potential to facilitate cross-organizational collaborations, the deployment of VFL systems in real-world applications remains limited. To investigate the gap between existing VFL research and practical deployment, this survey analyzes the real-world data distributions in potential VFL applications and identifies four key findings that highlight this gap. We propose a novel data-oriented taxonomy of VFL algorithms based on real VFL data distributions. Our comprehensive review of existing VFL algorithms reveals that some common practical VFL scenarios have few or no viable solutions. Based on these observations, we outline key research directions aimed at bridging the gap between current VFL research and real-world applications.","sentences":["Vertical Federated Learning (VFL) is a privacy-preserving collaborative learning paradigm that enables multiple parties with distinct feature sets to jointly train machine learning models without sharing their raw data.","Despite its potential to facilitate cross-organizational collaborations, the deployment of VFL systems in real-world applications remains limited.","To investigate the gap between existing VFL research and practical deployment, this survey analyzes the real-world data distributions in potential VFL applications and identifies four key findings that highlight this gap.","We propose a novel data-oriented taxonomy of VFL algorithms based on real VFL data distributions.","Our comprehensive review of existing VFL algorithms reveals that some common practical VFL scenarios have few or no viable solutions.","Based on these observations, we outline key research directions aimed at bridging the gap between current VFL research and real-world applications."],"url":"http://arxiv.org/abs/2502.08160v1"}
{"created":"2025-02-12 06:51:42","title":"Open-Source Factor Graph Optimization Package for GNSS: Examples and Applications","abstract":"State estimation methods using factor graph optimization (FGO) have garnered significant attention in global navigation satellite system (GNSS) research. FGO exhibits superior estimation accuracy compared with traditional state estimation methods that rely on least-squares or Kalman filters. However, only a few FGO libraries are specialized for GNSS observations. This paper introduces an open-source GNSS FGO package named gtsam\\_gnss, which has a simple structure and can be easily applied to GNSS research and development. This package separates the preprocessing of GNSS observations from factor optimization. Moreover, it describes the error function of the GNSS factor in a straightforward manner, allowing for general-purpose inputs. This design facilitates the transition from ordinary least-squares-based positioning to FGO and supports user-specific GNSS research. In addition, gtsam\\_gnss includes analytical examples involving various factors using GNSS data in real urban environments. This paper presents three application examples: the use of a robust error model, estimation of integer ambiguity in the carrier phase, and combination of GNSS and inertial measurements from smartphones. The proposed framework demonstrates excellent state estimation performance across all use cases.","sentences":["State estimation methods using factor graph optimization (FGO) have garnered significant attention in global navigation satellite system (GNSS) research.","FGO exhibits superior estimation accuracy compared with traditional state estimation methods that rely on least-squares or Kalman filters.","However, only a few FGO libraries are specialized for GNSS observations.","This paper introduces an open-source GNSS FGO package named gtsam\\_gnss, which has a simple structure and can be easily applied to GNSS research and development.","This package separates the preprocessing of GNSS observations from factor optimization.","Moreover, it describes the error function of the GNSS factor in a straightforward manner, allowing for general-purpose inputs.","This design facilitates the transition from ordinary least-squares-based positioning to FGO and supports user-specific GNSS research.","In addition, gtsam\\_gnss includes analytical examples involving various factors using GNSS data in real urban environments.","This paper presents three application examples: the use of a robust error model, estimation of integer ambiguity in the carrier phase, and combination of GNSS and inertial measurements from smartphones.","The proposed framework demonstrates excellent state estimation performance across all use cases."],"url":"http://arxiv.org/abs/2502.08158v1"}
{"created":"2025-02-12 06:47:25","title":"DGSense: A Domain Generalization Framework for Wireless Sensing","abstract":"Wireless sensing is of great benefits to our daily lives. However, wireless signals are sensitive to the surroundings. Various factors, e.g. environments, locations, and individuals, may induce extra impact on wireless propagation. Such a change can be regarded as a domain, in which the data distribution shifts. A vast majority of the sensing schemes are learning-based. They are dependent on the training domains, resulting in performance degradation in unseen domains. Researchers have proposed various solutions to address this issue. But these solutions leverage either semi-supervised or unsupervised domain adaptation techniques. They still require some data in the target domains and do not perform well in unseen domains. In this paper, we propose a domain generalization framework DGSense, to eliminate the domain dependence problem in wireless sensing. The framework is a general solution working across diverse sensing tasks and wireless technologies. Once the sensing model is built, it can generalize to unseen domains without any data from the target domain. To achieve the goal, we first increase the diversity of the training set by a virtual data generator, and then extract the domain independent features via episodic training between the main feature extractor and the domain feature extractors. The feature extractors employ a pre-trained Residual Network (ResNet) with an attention mechanism for spatial features, and a 1D Convolutional Neural Network (1DCNN) for temporal features. To demonstrate the effectiveness and generality of DGSense, we evaluated on WiFi gesture recognition, Millimeter Wave (mmWave) activity recognition, and acoustic fall detection. All the systems exhibited high generalization capability to unseen domains, including new users, locations, and environments, free of new data and retraining.","sentences":["Wireless sensing is of great benefits to our daily lives.","However, wireless signals are sensitive to the surroundings.","Various factors, e.g. environments, locations, and individuals, may induce extra impact on wireless propagation.","Such a change can be regarded as a domain, in which the data distribution shifts.","A vast majority of the sensing schemes are learning-based.","They are dependent on the training domains, resulting in performance degradation in unseen domains.","Researchers have proposed various solutions to address this issue.","But these solutions leverage either semi-supervised or unsupervised domain adaptation techniques.","They still require some data in the target domains and do not perform well in unseen domains.","In this paper, we propose a domain generalization framework DGSense, to eliminate the domain dependence problem in wireless sensing.","The framework is a general solution working across diverse sensing tasks and wireless technologies.","Once the sensing model is built, it can generalize to unseen domains without any data from the target domain.","To achieve the goal, we first increase the diversity of the training set by a virtual data generator, and then extract the domain independent features via episodic training between the main feature extractor and the domain feature extractors.","The feature extractors employ a pre-trained Residual Network (ResNet) with an attention mechanism for spatial features, and a 1D Convolutional Neural Network (1DCNN) for temporal features.","To demonstrate the effectiveness and generality of DGSense, we evaluated on WiFi gesture recognition, Millimeter Wave (mmWave) activity recognition, and acoustic fall detection.","All the systems exhibited high generalization capability to unseen domains, including new users, locations, and environments, free of new data and retraining."],"url":"http://arxiv.org/abs/2502.08155v1"}
{"created":"2025-02-12 06:26:05","title":"Generalized Class Discovery in Instance Segmentation","abstract":"This work addresses the task of generalized class discovery (GCD) in instance segmentation. The goal is to discover novel classes and obtain a model capable of segmenting instances of both known and novel categories, given labeled and unlabeled data. Since the real world contains numerous objects with long-tailed distributions, the instance distribution for each class is inherently imbalanced. To address the imbalanced distributions, we propose an instance-wise temperature assignment (ITA) method for contrastive learning and class-wise reliability criteria for pseudo-labels. The ITA method relaxes instance discrimination for samples belonging to head classes to enhance GCD. The reliability criteria are to avoid excluding most pseudo-labels for tail classes when training an instance segmentation network using pseudo-labels from GCD. Additionally, we propose dynamically adjusting the criteria to leverage diverse samples in the early stages while relying only on reliable pseudo-labels in the later stages. We also introduce an efficient soft attention module to encode object-specific representations for GCD. Finally, we evaluate our proposed method by conducting experiments on two settings: COCO$_{half}$ + LVIS and LVIS + Visual Genome. The experimental results demonstrate that the proposed method outperforms previous state-of-the-art methods.","sentences":["This work addresses the task of generalized class discovery (GCD) in instance segmentation.","The goal is to discover novel classes and obtain a model capable of segmenting instances of both known and novel categories, given labeled and unlabeled data.","Since the real world contains numerous objects with long-tailed distributions, the instance distribution for each class is inherently imbalanced.","To address the imbalanced distributions, we propose an instance-wise temperature assignment (ITA) method for contrastive learning and class-wise reliability criteria for pseudo-labels.","The ITA method relaxes instance discrimination for samples belonging to head classes to enhance GCD.","The reliability criteria are to avoid excluding most pseudo-labels for tail classes when training an instance segmentation network using pseudo-labels from GCD.","Additionally, we propose dynamically adjusting the criteria to leverage diverse samples in the early stages while relying only on reliable pseudo-labels in the later stages.","We also introduce an efficient soft attention module to encode object-specific representations for GCD.","Finally, we evaluate our proposed method by conducting experiments on two settings: COCO$_{half}$ + LVIS and LVIS + Visual Genome.","The experimental results demonstrate that the proposed method outperforms previous state-of-the-art methods."],"url":"http://arxiv.org/abs/2502.08149v1"}
{"created":"2025-02-12 06:05:52","title":"Democratizing AI: Open-source Scalable LLM Training on GPU-based Supercomputers","abstract":"Training and fine-tuning large language models (LLMs) with hundreds of billions to trillions of parameters requires tens of thousands of GPUs, and a highly scalable software stack. In this work, we present a novel four-dimensional hybrid parallel algorithm implemented in a highly scalable, portable, open-source framework called AxoNN. We describe several performance optimizations in AxoNN to improve matrix multiply kernel performance, overlap non-blocking collectives with computation, and performance modeling to choose performance optimal configurations. These have resulted in unprecedented scaling and peak flop/s (bf16) for training of GPT-style transformer models on Perlmutter (620.1 Petaflop/s), Frontier (1.381 Exaflop/s) and Alps (1.423 Exaflop/s).   While the abilities of LLMs improve with the number of trainable parameters, so do privacy and copyright risks caused by memorization of training data, which can cause disclosure of sensitive or private information at inference time. We highlight this side effect of scale through experiments that explore \"catastrophic memorization\", where models are sufficiently large to memorize training data in a single pass, and present an approach to prevent it. As part of this study, we demonstrate fine-tuning of a 405-billion parameter LLM using AxoNN on Frontier.","sentences":["Training and fine-tuning large language models (LLMs) with hundreds of billions to trillions of parameters requires tens of thousands of GPUs, and a highly scalable software stack.","In this work, we present a novel four-dimensional hybrid parallel algorithm implemented in a highly scalable, portable, open-source framework called AxoNN.","We describe several performance optimizations in AxoNN to improve matrix multiply kernel performance, overlap non-blocking collectives with computation, and performance modeling to choose performance optimal configurations.","These have resulted in unprecedented scaling and peak flop/s (bf16) for training of GPT-style transformer models on Perlmutter (620.1 Petaflop/s), Frontier (1.381 Exaflop/s) and Alps (1.423 Exaflop/s).   ","While the abilities of LLMs improve with the number of trainable parameters, so do privacy and copyright risks caused by memorization of training data, which can cause disclosure of sensitive or private information at inference time.","We highlight this side effect of scale through experiments that explore \"catastrophic memorization\", where models are sufficiently large to memorize training data in a single pass, and present an approach to prevent it.","As part of this study, we demonstrate fine-tuning of a 405-billion parameter LLM using AxoNN on Frontier."],"url":"http://arxiv.org/abs/2502.08145v1"}
{"created":"2025-02-12 05:48:57","title":"Data-dependent Bounds with $T$-Optimal Best-of-Both-Worlds Guarantees in Multi-Armed Bandits using Stability-Penalty Matching","abstract":"Existing data-dependent and best-of-both-worlds regret bounds for multi-armed bandits problems have limited adaptivity as they are either data-dependent but not best-of-both-worlds (BOBW), BOBW but not data-dependent or have sub-optimal $O(\\sqrt{T\\ln{T}})$ worst-case guarantee in the adversarial regime. To overcome these limitations, we propose real-time stability-penalty matching (SPM), a new method for obtaining regret bounds that are simultaneously data-dependent, best-of-both-worlds and $T$-optimal for multi-armed bandits problems. In particular, we show that real-time SPM obtains bounds with worst-case guarantees of order $O(\\sqrt{T})$ in the adversarial regime and $O(\\ln{T})$ in the stochastic regime while simultaneously being adaptive to data-dependent quantities such as sparsity, variations, and small losses. Our results are obtained by extending the SPM technique for tuning the learning rates in the follow-the-regularized-leader (FTRL) framework, which further indicates that the combination of SPM and FTRL is a promising approach for proving new adaptive bounds in online learning problems.","sentences":["Existing data-dependent and best-of-both-worlds regret bounds for multi-armed bandits problems have limited adaptivity as they are either data-dependent but not best-of-both-worlds (BOBW), BOBW but not data-dependent or have sub-optimal $O(\\sqrt{T\\ln{T}})$ worst-case guarantee in the adversarial regime.","To overcome these limitations, we propose real-time stability-penalty matching (SPM), a new method for obtaining regret bounds that are simultaneously data-dependent, best-of-both-worlds and $T$-optimal for multi-armed bandits problems.","In particular, we show that real-time SPM obtains bounds with worst-case guarantees of order $O(\\sqrt{T})$ in the adversarial regime and $O(\\ln{T})$ in the stochastic regime while simultaneously being adaptive to data-dependent quantities such as sparsity, variations, and small losses.","Our results are obtained by extending the SPM technique for tuning the learning rates in the follow-the-regularized-leader (FTRL) framework, which further indicates that the combination of SPM and FTRL is a promising approach for proving new adaptive bounds in online learning problems."],"url":"http://arxiv.org/abs/2502.08143v1"}
{"created":"2025-02-12 05:40:11","title":"In-Context Learning of Linear Dynamical Systems with Transformers: Error Bounds and Depth-Separation","abstract":"This paper investigates approximation-theoretic aspects of the in-context learning capability of the transformers in representing a family of noisy linear dynamical systems. Our first theoretical result establishes an upper bound on the approximation error of multi-layer transformers with respect to an $L^2$-testing loss uniformly defined across tasks. This result demonstrates that transformers with logarithmic depth can achieve error bounds comparable with those of the least-squares estimator. In contrast, our second result establishes a non-diminishing lower bound on the approximation error for a class of single-layer linear transformers, which suggests a depth-separation phenomenon for transformers in the in-context learning of dynamical systems. Moreover, this second result uncovers a critical distinction in the approximation power of single-layer linear transformers when learning from IID versus non-IID data.","sentences":["This paper investigates approximation-theoretic aspects of the in-context learning capability of the transformers in representing a family of noisy linear dynamical systems.","Our first theoretical result establishes an upper bound on the approximation error of multi-layer transformers with respect to an $L^2$-testing loss uniformly defined across tasks.","This result demonstrates that transformers with logarithmic depth can achieve error bounds comparable with those of the least-squares estimator.","In contrast, our second result establishes a non-diminishing lower bound on the approximation error for a class of single-layer linear transformers, which suggests a depth-separation phenomenon for transformers in the in-context learning of dynamical systems.","Moreover, this second result uncovers a critical distinction in the approximation power of single-layer linear transformers when learning from IID versus non-IID data."],"url":"http://arxiv.org/abs/2502.08136v1"}
{"created":"2025-02-12 05:34:48","title":"A Survey on Data Curation for Visual Contrastive Learning: Why Crafting Effective Positive and Negative Pairs Matters","abstract":"Visual contrastive learning aims to learn representations by contrasting similar (positive) and dissimilar (negative) pairs of data samples. The design of these pairs significantly impacts representation quality, training efficiency, and computational cost. A well-curated set of pairs leads to stronger representations and faster convergence. As contrastive pre-training sees wider adoption for solving downstream tasks, data curation becomes essential for optimizing its effectiveness. In this survey, we attempt to create a taxonomy of existing techniques for positive and negative pair curation in contrastive learning, and describe them in detail.","sentences":["Visual contrastive learning aims to learn representations by contrasting similar (positive) and dissimilar (negative) pairs of data samples.","The design of these pairs significantly impacts representation quality, training efficiency, and computational cost.","A well-curated set of pairs leads to stronger representations and faster convergence.","As contrastive pre-training sees wider adoption for solving downstream tasks, data curation becomes essential for optimizing its effectiveness.","In this survey, we attempt to create a taxonomy of existing techniques for positive and negative pair curation in contrastive learning, and describe them in detail."],"url":"http://arxiv.org/abs/2502.08134v1"}
{"created":"2025-02-12 05:28:08","title":"SS4Rec: Continuous-Time Sequential Recommendation with State Space Models","abstract":"Sequential recommendation is a key area in the field of recommendation systems aiming to model user interest based on historical interaction sequences with irregular intervals. While previous recurrent neural network-based and attention-based approaches have achieved significant results, they have limitations in capturing system continuity due to the discrete characteristics. In the context of continuous-time modeling, state space model (SSM) offers a potential solution, as it can effectively capture the dynamic evolution of user interest over time. However, existing SSM-based approaches ignore the impact of irregular time intervals within historical user interactions, making it difficult to model complexed user-item transitions in sequences. To address this issue, we propose a hybrid SSM-based model called SS4Rec for continuous-time sequential recommendation. SS4Rec integrates a time-aware SSM to handle irregular time intervals and a relation-aware SSM to model contextual dependencies, enabling it to infer user interest from both temporal and sequential perspectives. In the training process, the time-aware SSM and the relation-aware SSM are discretized by variable stepsizes according to user interaction time intervals and input data, respectively. This helps capture the continuous dependency from irregular time intervals and provides time-specific personalized recommendations. Experimental studies on five benchmark datasets demonstrate the superiority and effectiveness of SS4Rec.","sentences":["Sequential recommendation is a key area in the field of recommendation systems aiming to model user interest based on historical interaction sequences with irregular intervals.","While previous recurrent neural network-based and attention-based approaches have achieved significant results, they have limitations in capturing system continuity due to the discrete characteristics.","In the context of continuous-time modeling, state space model (SSM) offers a potential solution, as it can effectively capture the dynamic evolution of user interest over time.","However, existing SSM-based approaches ignore the impact of irregular time intervals within historical user interactions, making it difficult to model complexed user-item transitions in sequences.","To address this issue, we propose a hybrid SSM-based model called SS4Rec for continuous-time sequential recommendation.","SS4Rec integrates a time-aware SSM to handle irregular time intervals and a relation-aware SSM to model contextual dependencies, enabling it to infer user interest from both temporal and sequential perspectives.","In the training process, the time-aware SSM and the relation-aware SSM are discretized by variable stepsizes according to user interaction time intervals and input data, respectively.","This helps capture the continuous dependency from irregular time intervals and provides time-specific personalized recommendations.","Experimental studies on five benchmark datasets demonstrate the superiority and effectiveness of SS4Rec."],"url":"http://arxiv.org/abs/2502.08132v1"}
{"created":"2025-02-12 05:24:21","title":"Selective Self-to-Supervised Fine-Tuning for Generalization in Large Language Models","abstract":"Fine-tuning Large Language Models (LLMs) on specific datasets is a common practice to improve performance on target tasks. However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task or the characteristics of the training data, resulting in a loss of generalization. This paper introduces Selective Self-to-Supervised Fine-Tuning (S3FT), a fine-tuning approach that achieves better performance than the standard supervised fine-tuning (SFT) while improving generalization. S3FT leverages the existence of multiple valid responses to a query. By utilizing the model's correct responses, S3FT reduces model specialization during the fine-tuning stage. S3FT first identifies the correct model responses from the training set by deploying an appropriate judge. Then, it fine-tunes the model using the correct model responses and the gold response (or its paraphrase) for the remaining samples. The effectiveness of S3FT is demonstrated through experiments on mathematical reasoning, Python programming and reading comprehension tasks. The results show that standard SFT can lead to an average performance drop of up to $4.4$ on multiple benchmarks, such as MMLU and TruthfulQA. In contrast, S3FT reduces this drop by half, i.e. $2.5$, indicating better generalization capabilities than SFT while performing significantly better on the fine-tuning tasks.","sentences":["Fine-tuning Large Language Models (LLMs) on specific datasets is a common practice to improve performance on target tasks.","However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task or the characteristics of the training data, resulting in a loss of generalization.","This paper introduces Selective Self-to-Supervised Fine-Tuning (S3FT), a fine-tuning approach that achieves better performance than the standard supervised fine-tuning (SFT) while improving generalization.","S3FT leverages the existence of multiple valid responses to a query.","By utilizing the model's correct responses, S3FT reduces model specialization during the fine-tuning stage.","S3FT first identifies the correct model responses from the training set by deploying an appropriate judge.","Then, it fine-tunes the model using the correct model responses and the gold response (or its paraphrase) for the remaining samples.","The effectiveness of S3FT is demonstrated through experiments on mathematical reasoning, Python programming and reading comprehension tasks.","The results show that standard SFT can lead to an average performance drop of up to $4.4$ on multiple benchmarks, such as MMLU and TruthfulQA.","In contrast, S3FT reduces this drop by half, i.e. $2.5$, indicating better generalization capabilities than SFT while performing significantly better on the fine-tuning tasks."],"url":"http://arxiv.org/abs/2502.08130v1"}
{"created":"2025-02-12 05:13:04","title":"Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance","abstract":"Recent advancements in large language models (LLMs) have shown strong general reasoning abilities, yet their effectiveness in financial reasoning remains underexplored. In this study, we comprehensively evaluate 16 powerful reasoning and general LLMs on three complex financial tasks involving financial text, tabular data, and equations, assessing numerical reasoning, tabular interpretation, financial terminology comprehension, long-context processing, and equation-based problem solving. Our results show that while better datasets and pretraining improve financial reasoning, general enhancements like CoT fine-tuning do not always yield consistent gains. Moreover, all reasoning strategies face challenges in improving performance on long-context and multi-table tasks. To address these limitations, we develop a financial reasoning-enhanced model based on Llama-3.1-8B-Instruct, by CoT fine-tuning and reinforcement learning with domain-specific reasoning paths. Even with simple fine-tuning with one financial dataset, our model achieves a consistent 10% performance improvement across tasks, surpassing all 8B models and even Llama3-70B-Instruct and Llama3.1-70B-Instruct on average. Our results highlight the need for domain-specific adaptations in financial tasks, emphasizing future directions such as multi-table reasoning, long-context processing, and financial terminology comprehension. All our datasets, models, and codes are publicly available. Furthermore, we introduce a leaderboard for benchmarking future datasets and models.","sentences":["Recent advancements in large language models (LLMs) have shown strong general reasoning abilities, yet their effectiveness in financial reasoning remains underexplored.","In this study, we comprehensively evaluate 16 powerful reasoning and general LLMs on three complex financial tasks involving financial text, tabular data, and equations, assessing numerical reasoning, tabular interpretation, financial terminology comprehension, long-context processing, and equation-based problem solving.","Our results show that while better datasets and pretraining improve financial reasoning, general enhancements like CoT fine-tuning do not always yield consistent gains.","Moreover, all reasoning strategies face challenges in improving performance on long-context and multi-table tasks.","To address these limitations, we develop a financial reasoning-enhanced model based on Llama-3.1-8B-Instruct, by CoT fine-tuning and reinforcement learning with domain-specific reasoning paths.","Even with simple fine-tuning with one financial dataset, our model achieves a consistent 10% performance improvement across tasks, surpassing all 8B models and even Llama3-70B-Instruct and Llama3.1-70B-Instruct on average.","Our results highlight the need for domain-specific adaptations in financial tasks, emphasizing future directions such as multi-table reasoning, long-context processing, and financial terminology comprehension.","All our datasets, models, and codes are publicly available.","Furthermore, we introduce a leaderboard for benchmarking future datasets and models."],"url":"http://arxiv.org/abs/2502.08127v1"}
{"created":"2025-02-12 05:06:23","title":"Incremental Approximate Single-Source Shortest Paths with Predictions","abstract":"The algorithms-with-predictions framework has been used extensively to develop online algorithms with improved beyond-worst-case competitive ratios. Recently, there is growing interest in leveraging predictions for designing data structures with improved beyond-worst-case running times. In this paper, we study the fundamental data structure problem of maintaining approximate shortest paths in incremental graphs in the algorithms-with-predictions model. Given a sequence $\\sigma$ of edges that are inserted one at a time, the goal is to maintain approximate shortest paths from the source to each vertex in the graph at each time step. Before any edges arrive, the data structure is given a prediction of the online edge sequence $\\hat{\\sigma}$ which is used to ``warm start'' its state.   As our main result, we design a learned algorithm that maintains $(1+\\epsilon)$-approximate single-source shortest paths, which runs in $\\tilde{O}(m \\eta \\log W/\\epsilon)$ time, where $W$ is the weight of the heaviest edge and $\\eta$ is the prediction error. We show these techniques immediately extend to the all-pairs shortest-path setting as well. Our algorithms are consistent (performing nearly as fast as the offline algorithm) when predictions are nearly perfect, have a smooth degradation in performance with respect to the prediction error and, in the worst case, match the best offline algorithm up to logarithmic factors.   As a building block, we study the offline incremental approximate single-source shortest-paths problem. In this problem, the edge sequence $\\sigma$ is known a priori and the goal is to efficiently return the length of the shortest paths in the intermediate graph $G_t$ consisting of the first $t$ edges, for all $t$. Note that the offline incremental problem is defined in the worst-case setting (without predictions) and is of independent interest.","sentences":["The algorithms-with-predictions framework has been used extensively to develop online algorithms with improved beyond-worst-case competitive ratios.","Recently, there is growing interest in leveraging predictions for designing data structures with improved beyond-worst-case running times.","In this paper, we study the fundamental data structure problem of maintaining approximate shortest paths in incremental graphs in the algorithms-with-predictions model.","Given a sequence $\\sigma$ of edges that are inserted one at a time, the goal is to maintain approximate shortest paths from the source to each vertex in the graph at each time step.","Before any edges arrive, the data structure is given a prediction of the online edge sequence $\\hat{\\sigma}$ which is used to ``warm start'' its state.   ","As our main result, we design a learned algorithm that maintains $(1+\\epsilon)$-approximate single-source shortest paths, which runs in $\\tilde{O}(m \\eta \\log W/\\epsilon)$ time, where $W$ is the weight of the heaviest edge and $\\eta$ is the prediction error.","We show these techniques immediately extend to the all-pairs shortest-path setting as well.","Our algorithms are consistent (performing nearly as fast as the offline algorithm) when predictions are nearly perfect, have a smooth degradation in performance with respect to the prediction error and, in the worst case, match the best offline algorithm up to logarithmic factors.   ","As a building block, we study the offline incremental approximate single-source shortest-paths problem.","In this problem, the edge sequence $\\sigma$ is known a priori and the goal is to efficiently return the length of the shortest paths in the intermediate graph $G_t$ consisting of the first $t$ edges, for all $t$. Note that the offline incremental problem is defined in the worst-case setting (without predictions) and is of independent interest."],"url":"http://arxiv.org/abs/2502.08125v1"}
{"created":"2025-02-12 05:03:49","title":"Hookpad Aria: A Copilot for Songwriters","abstract":"We present Hookpad Aria, a generative AI system designed to assist musicians in writing Western pop songs. Our system is seamlessly integrated into Hookpad, a web-based editor designed for the composition of lead sheets: symbolic music scores that describe melody and harmony. Hookpad Aria has numerous generation capabilities designed to assist users in non-sequential composition workflows, including: (1) generating left-to-right continuations of existing material, (2) filling in missing spans in the middle of existing material, and (3) generating harmony from melody and vice versa. Hookpad Aria is also a scalable data flywheel for music co-creation -- since its release in March 2024, Aria has generated 318k suggestions for 3k users who have accepted 74k into their songs.   More information about Hookpad Aria is available at https://www.hooktheory.com/hookpad/aria","sentences":["We present Hookpad Aria, a generative AI system designed to assist musicians in writing Western pop songs.","Our system is seamlessly integrated into Hookpad, a web-based editor designed for the composition of lead sheets: symbolic music scores that describe melody and harmony.","Hookpad Aria has numerous generation capabilities designed to assist users in non-sequential composition workflows, including: (1) generating left-to-right continuations of existing material, (2) filling in missing spans in the middle of existing material, and (3) generating harmony from melody and vice versa.","Hookpad Aria is also a scalable data flywheel for music co-creation -- since its release in March 2024, Aria has generated 318k suggestions for 3k users who have accepted 74k into their songs.   ","More information about Hookpad Aria is available at https://www.hooktheory.com/hookpad/aria"],"url":"http://arxiv.org/abs/2502.08122v1"}
{"created":"2025-02-12 04:40:09","title":"Future Resource Bank for ISAC: Achieving Fast and Stable Win-Win Matching for Both Individuals and Coalitions","abstract":"Future wireless networks must support emerging applications where environmental awareness is as critical as data transmission. Integrated Sensing and Communication (ISAC) enables this vision by allowing base stations (BSs) to allocate bandwidth and power to mobile users (MUs) for communications and cooperative sensing. However, this resource allocation is highly challenging due to: (i) dynamic resource demands from MUs and resource supply from BSs, and (ii) the selfishness of MUs and BSs. To address these challenges, existing solutions rely on either real-time (online) resource trading, which incurs high overhead and failures, or static long-term (offline) resource contracts, which lack flexibility. To overcome these limitations, we propose the Future Resource Bank for ISAC, a hybrid trading framework that integrates offline and online resource allocation through a level-wise client model, where MUs and their coalitions negotiate with BSs. We introduce two mechanisms: (i) Role-Friendly Win-Win Matching (offRFW$^2$M), leveraging overbooking to establish risk-aware, stable contracts, and (ii) Effective Backup Win-Win Matching (onEBW$^2$M), which dynamically reallocates unmet demand and surplus supply. We theoretically prove stability, individual rationality, and weak Pareto optimality of these mechanisms. Through simulations, we show that our framework improves social welfare, latency, and energy efficiency compared to existing methods.","sentences":["Future wireless networks must support emerging applications where environmental awareness is as critical as data transmission.","Integrated Sensing and Communication (ISAC) enables this vision by allowing base stations (BSs) to allocate bandwidth and power to mobile users (MUs) for communications and cooperative sensing.","However, this resource allocation is highly challenging due to: (i) dynamic resource demands from MUs and resource supply from BSs, and (ii) the selfishness of MUs and BSs.","To address these challenges, existing solutions rely on either real-time (online) resource trading, which incurs high overhead and failures, or static long-term (offline) resource contracts, which lack flexibility.","To overcome these limitations, we propose the Future Resource Bank for ISAC, a hybrid trading framework that integrates offline and online resource allocation through a level-wise client model, where MUs and their coalitions negotiate with BSs.","We introduce two mechanisms: (i) Role-Friendly Win-Win Matching (offRFW$^2$M), leveraging overbooking to establish risk-aware, stable contracts, and (ii) Effective Backup Win-Win Matching (onEBW$^2$M), which dynamically reallocates unmet demand and surplus supply.","We theoretically prove stability, individual rationality, and weak Pareto optimality of these mechanisms.","Through simulations, we show that our framework improves social welfare, latency, and energy efficiency compared to existing methods."],"url":"http://arxiv.org/abs/2502.08118v1"}
{"created":"2025-02-12 04:35:23","title":"From Clicks to Conversations: Evaluating the Effectiveness of Conversational Agents in Statistical Analysis","abstract":"The rapid proliferation of data science forced different groups of individuals with different backgrounds to adapt to statistical analysis. We hypothesize that conversational agents are better suited for statistical analysis than traditional graphical user interfaces (GUI). In this work, we propose a novel conversational agent, StatZ, for statistical analysis. We evaluate the efficacy of StatZ relative to established statistical software:SPSS, SAS, Stata, and JMP in terms of accuracy, task completion time, user experience, and user satisfaction. We combined the proposed analysis question from state-of-the-art language models with suggestions from statistical analysis experts and tested with 51 participants from diverse backgrounds. Our experimental design assessed each participant's ability to perform statistical analysis tasks using traditional statistical analysis tools with GUI and our conversational agent. Results indicate that the proposed conversational agents significantly outperform GUI statistical software in all assessed metrics, including quantitative (task completion time, accuracy, and user experience), and qualitative (user satisfaction) metrics. Our findings underscore the potential of using conversational agents to enhance statistical analysis processes, reducing cognitive load and learning curves and thereby proliferating data analysis capabilities, to individuals with limited knowledge of statistics.","sentences":["The rapid proliferation of data science forced different groups of individuals with different backgrounds to adapt to statistical analysis.","We hypothesize that conversational agents are better suited for statistical analysis than traditional graphical user interfaces (GUI).","In this work, we propose a novel conversational agent, StatZ, for statistical analysis.","We evaluate the efficacy of StatZ relative to established statistical software:SPSS, SAS, Stata, and JMP in terms of accuracy, task completion time, user experience, and user satisfaction.","We combined the proposed analysis question from state-of-the-art language models with suggestions from statistical analysis experts and tested with 51 participants from diverse backgrounds.","Our experimental design assessed each participant's ability to perform statistical analysis tasks using traditional statistical analysis tools with GUI and our conversational agent.","Results indicate that the proposed conversational agents significantly outperform GUI statistical software in all assessed metrics, including quantitative (task completion time, accuracy, and user experience), and qualitative (user satisfaction) metrics.","Our findings underscore the potential of using conversational agents to enhance statistical analysis processes, reducing cognitive load and learning curves and thereby proliferating data analysis capabilities, to individuals with limited knowledge of statistics."],"url":"http://arxiv.org/abs/2502.08114v1"}
{"created":"2025-02-12 04:13:07","title":"Generative AI and Empirical Software Engineering: A Paradigm Shift","abstract":"The widespread adoption of generative AI in software engineering marks a paradigm shift, offering new opportunities to design and utilize software engineering tools while influencing both developers and the artifacts they create. Traditional empirical methods in software engineering, including quantitative, qualitative, and mixed-method approaches, are well established. However, this paradigm shift introduces novel data types and redefines many concepts in the software engineering process. The roles of developers, users, agents, and researchers increasingly overlap, blurring the distinctions between these social and technical actors within the field.   This paper examines how integrating AI into software engineering challenges traditional research paradigms. It focuses on the research phenomena that we investigate, the methods and theories that we employ, the data we analyze, and the threats to validity that emerge in this new context. Through this exploration, our goal is to understand how AI adoption disrupts established software development practices that creates new opportunities for empirical software engineering research.","sentences":["The widespread adoption of generative AI in software engineering marks a paradigm shift, offering new opportunities to design and utilize software engineering tools while influencing both developers and the artifacts they create.","Traditional empirical methods in software engineering, including quantitative, qualitative, and mixed-method approaches, are well established.","However, this paradigm shift introduces novel data types and redefines many concepts in the software engineering process.","The roles of developers, users, agents, and researchers increasingly overlap, blurring the distinctions between these social and technical actors within the field.   ","This paper examines how integrating AI into software engineering challenges traditional research paradigms.","It focuses on the research phenomena that we investigate, the methods and theories that we employ, the data we analyze, and the threats to validity that emerge in this new context.","Through this exploration, our goal is to understand how AI adoption disrupts established software development practices that creates new opportunities for empirical software engineering research."],"url":"http://arxiv.org/abs/2502.08108v1"}
{"created":"2025-02-12 04:07:14","title":"PoGDiff: Product-of-Gaussians Diffusion Models for Imbalanced Text-to-Image Generation","abstract":"Diffusion models have made significant advancements in recent years. However, their performance often deteriorates when trained or fine-tuned on imbalanced datasets. This degradation is largely due to the disproportionate representation of majority and minority data in image-text pairs. In this paper, we propose a general fine-tuning approach, dubbed PoGDiff, to address this challenge. Rather than directly minimizing the KL divergence between the predicted and ground-truth distributions, PoGDiff replaces the ground-truth distribution with a Product of Gaussians (PoG), which is constructed by combining the original ground-truth targets with the predicted distribution conditioned on a neighboring text embedding. Experiments on real-world datasets demonstrate that our method effectively addresses the imbalance problem in diffusion models, improving both generation accuracy and quality.","sentences":["Diffusion models have made significant advancements in recent years.","However, their performance often deteriorates when trained or fine-tuned on imbalanced datasets.","This degradation is largely due to the disproportionate representation of majority and minority data in image-text pairs.","In this paper, we propose a general fine-tuning approach, dubbed PoGDiff, to address this challenge.","Rather than directly minimizing the KL divergence between the predicted and ground-truth distributions, PoGDiff replaces the ground-truth distribution with a Product of Gaussians (PoG), which is constructed by combining the original ground-truth targets with the predicted distribution conditioned on a neighboring text embedding.","Experiments on real-world datasets demonstrate that our method effectively addresses the imbalance problem in diffusion models, improving both generation accuracy and quality."],"url":"http://arxiv.org/abs/2502.08106v1"}
{"created":"2025-02-12 04:07:12","title":"Out-of-Distribution Detection on Graphs: A Survey","abstract":"Graph machine learning has witnessed rapid growth, driving advancements across diverse domains. However, the in-distribution assumption, where training and testing data share the same distribution, often breaks in real-world scenarios, leading to degraded model performance under distribution shifts. This challenge has catalyzed interest in graph out-of-distribution (GOOD) detection, which focuses on identifying graph data that deviates from the distribution seen during training, thereby enhancing model robustness. In this paper, we provide a rigorous definition of GOOD detection and systematically categorize existing methods into four types: enhancement-based, reconstruction-based, information propagation-based, and classification-based approaches. We analyze the principles and mechanisms of each approach and clarify the distinctions between GOOD detection and related fields, such as graph anomaly detection, outlier detection, and GOOD generalization. Beyond methodology, we discuss practical applications and theoretical foundations, highlighting the unique challenges posed by graph data. Finally, we discuss the primary challenges and propose future directions to advance this emerging field. The repository of this survey is available at https://github.com/ca1man-2022/Awesome-GOOD-Detection.","sentences":["Graph machine learning has witnessed rapid growth, driving advancements across diverse domains.","However, the in-distribution assumption, where training and testing data share the same distribution, often breaks in real-world scenarios, leading to degraded model performance under distribution shifts.","This challenge has catalyzed interest in graph out-of-distribution (GOOD) detection, which focuses on identifying graph data that deviates from the distribution seen during training, thereby enhancing model robustness.","In this paper, we provide a rigorous definition of GOOD detection and systematically categorize existing methods into four types: enhancement-based, reconstruction-based, information propagation-based, and classification-based approaches.","We analyze the principles and mechanisms of each approach and clarify the distinctions between GOOD detection and related fields, such as graph anomaly detection, outlier detection, and GOOD generalization.","Beyond methodology, we discuss practical applications and theoretical foundations, highlighting the unique challenges posed by graph data.","Finally, we discuss the primary challenges and propose future directions to advance this emerging field.","The repository of this survey is available at https://github.com/ca1man-2022/Awesome-GOOD-Detection."],"url":"http://arxiv.org/abs/2502.08105v1"}
{"created":"2025-02-12 03:34:25","title":"Ground-Optimized 4D Radar-Inertial Odometry via Continuous Velocity Integration using Gaussian Process","abstract":"Radar ensures robust sensing capabilities in adverse weather conditions, yet challenges remain due to its high inherent noise level. Existing radar odometry has overcome these challenges with strategies such as filtering spurious points, exploiting Doppler velocity, or integrating with inertial measurements. This paper presents two novel improvements beyond the existing radar-inertial odometry: ground-optimized noise filtering and continuous velocity preintegration. Despite the widespread use of ground planes in LiDAR odometry, imprecise ground point distributions of radar measurements cause naive plane fitting to fail. Unlike plane fitting in LiDAR, we introduce a zone-based uncertainty-aware ground modeling specifically designed for radar. Secondly, we note that radar velocity measurements can be better combined with IMU for a more accurate preintegration in radar-inertial odometry. Existing methods often ignore temporal discrepancies between radar and IMU by simplifying the complexities of asynchronous data streams with discretized propagation models. Tackling this issue, we leverage GP and formulate a continuous preintegration method for tightly integrating 3-DOF linear velocity with IMU, facilitating full 6-DOF motion directly from the raw measurements. Our approach demonstrates remarkable performance (less than 1% vertical drift) in public datasets with meticulous conditions, illustrating substantial improvement in elevation accuracy. The code will be released as open source for the community: https://github.com/wooseongY/Go-RIO.","sentences":["Radar ensures robust sensing capabilities in adverse weather conditions, yet challenges remain due to its high inherent noise level.","Existing radar odometry has overcome these challenges with strategies such as filtering spurious points, exploiting Doppler velocity, or integrating with inertial measurements.","This paper presents two novel improvements beyond the existing radar-inertial odometry: ground-optimized noise filtering and continuous velocity preintegration.","Despite the widespread use of ground planes in LiDAR odometry, imprecise ground point distributions of radar measurements cause naive plane fitting to fail.","Unlike plane fitting in LiDAR, we introduce a zone-based uncertainty-aware ground modeling specifically designed for radar.","Secondly, we note that radar velocity measurements can be better combined with IMU for a more accurate preintegration in radar-inertial odometry.","Existing methods often ignore temporal discrepancies between radar and IMU by simplifying the complexities of asynchronous data streams with discretized propagation models.","Tackling this issue, we leverage GP and formulate a continuous preintegration method for tightly integrating 3-DOF linear velocity with IMU, facilitating full 6-DOF motion directly from the raw measurements.","Our approach demonstrates remarkable performance (less than 1% vertical drift) in public datasets with meticulous conditions, illustrating substantial improvement in elevation accuracy.","The code will be released as open source for the community: https://github.com/wooseongY/Go-RIO."],"url":"http://arxiv.org/abs/2502.08093v1"}
{"created":"2025-02-12 03:33:06","title":"GCoT: Chain-of-Thought Prompt Learning for Graphs","abstract":"Chain-of-thought (CoT) prompting has achieved remarkable success in natural language processing (NLP). However, its vast potential remains largely unexplored for graphs. This raises an interesting question: How can we design CoT prompting for graphs to guide graph models to learn step by step? On one hand, unlike natural languages, graphs are non-linear and characterized by complex topological structures. On the other hand, many graphs lack textual data, making it difficult to formulate language-based CoT prompting. In this work, we propose the first CoT prompt learning framework for text-free graphs, GCoT. Specifically, we decompose the adaptation process for each downstream task into a series of inference steps, with each step consisting of prompt-based inference, ``thought'' generation, and thought-conditioned prompt learning. While the steps mimic CoT prompting in NLP, the exact mechanism differs significantly. Specifically, at each step, an input graph, along with a prompt, is first fed into a pre-trained graph encoder for prompt-based inference. We then aggregate the hidden layers of the encoder to construct a ``thought'', which captures the working state of each node in the current step. Conditioned on this thought, we learn a prompt specific to each node based on the current state. These prompts are fed into the next inference step, repeating the cycle. To evaluate and analyze the effectiveness of GCoT, we conduct comprehensive experiments on eight public datasets, which demonstrate the advantage of our approach.","sentences":["Chain-of-thought (CoT) prompting has achieved remarkable success in natural language processing (NLP).","However, its vast potential remains largely unexplored for graphs.","This raises an interesting question: How can we design CoT prompting for graphs to guide graph models to learn step by step?","On one hand, unlike natural languages, graphs are non-linear and characterized by complex topological structures.","On the other hand, many graphs lack textual data, making it difficult to formulate language-based CoT prompting.","In this work, we propose the first CoT prompt learning framework for text-free graphs, GCoT.","Specifically, we decompose the adaptation process for each downstream task into a series of inference steps, with each step consisting of prompt-based inference, ``thought'' generation, and thought-conditioned prompt learning.","While the steps mimic CoT prompting in NLP, the exact mechanism differs significantly.","Specifically, at each step, an input graph, along with a prompt, is first fed into a pre-trained graph encoder for prompt-based inference.","We then aggregate the hidden layers of the encoder to construct a ``thought'', which captures the working state of each node in the current step.","Conditioned on this thought, we learn a prompt specific to each node based on the current state.","These prompts are fed into the next inference step, repeating the cycle.","To evaluate and analyze the effectiveness of GCoT, we conduct comprehensive experiments on eight public datasets, which demonstrate the advantage of our approach."],"url":"http://arxiv.org/abs/2502.08092v1"}
{"created":"2025-02-12 03:10:26","title":"Mixture of Decoupled Message Passing Experts with Entropy Constraint for General Node Classification","abstract":"The varying degrees of homophily and heterophily in real-world graphs persistently constrain the universality of graph neural networks (GNNs) for node classification. Adopting a data-centric perspective, this work reveals an inherent preference of different graphs towards distinct message encoding schemes: homophilous graphs favor local propagation, while heterophilous graphs exhibit preference for flexible combinations of propagation and transformation. To address this, we propose GNNMoE, a universal node classification framework based on the Mixture-of-Experts (MoE) mechanism. The framework first constructs diverse message-passing experts through recombination of fine-grained encoding operators, then designs soft and hard gating layers to allocate the most suitable expert networks for each node's representation learning, thereby enhancing both model expressiveness and adaptability to diverse graphs. Furthermore, considering that soft gating might introduce encoding noise in homophilous scenarios, we introduce an entropy constraint to guide sharpening of soft gates, achieving organic integration of weighted combination and Top-K selection. Extensive experiments demonstrate that GNNMoE significantly outperforms mainstream GNNs, heterophilous GNNs, and graph transformers in both node classification performance and universality across diverse graph datasets.","sentences":["The varying degrees of homophily and heterophily in real-world graphs persistently constrain the universality of graph neural networks (GNNs) for node classification.","Adopting a data-centric perspective, this work reveals an inherent preference of different graphs towards distinct message encoding schemes: homophilous graphs favor local propagation, while heterophilous graphs exhibit preference for flexible combinations of propagation and transformation.","To address this, we propose GNNMoE, a universal node classification framework based on the Mixture-of-Experts (MoE) mechanism.","The framework first constructs diverse message-passing experts through recombination of fine-grained encoding operators, then designs soft and hard gating layers to allocate the most suitable expert networks for each node's representation learning, thereby enhancing both model expressiveness and adaptability to diverse graphs.","Furthermore, considering that soft gating might introduce encoding noise in homophilous scenarios, we introduce an entropy constraint to guide sharpening of soft gates, achieving organic integration of weighted combination and Top-K selection.","Extensive experiments demonstrate that GNNMoE significantly outperforms mainstream GNNs, heterophilous GNNs, and graph transformers in both node classification performance and universality across diverse graph datasets."],"url":"http://arxiv.org/abs/2502.08083v1"}
{"created":"2025-02-12 02:29:52","title":"Large language models perpetuate bias in palliative care: development and analysis of the Palliative Care Adversarial Dataset (PCAD)","abstract":"Bias and inequity in palliative care disproportionately affect marginalised groups. Large language models (LLMs), such as GPT-4o, hold potential to enhance care but risk perpetuating biases present in their training data. This study aimed to systematically evaluate whether GPT-4o propagates biases in palliative care responses using adversarially designed datasets. In July 2024, GPT-4o was probed using the Palliative Care Adversarial Dataset (PCAD), and responses were evaluated by three palliative care experts in Canada and the United Kingdom using validated bias rubrics. The PCAD comprised PCAD-Direct (100 adversarial questions) and PCAD-Counterfactual (84 paired scenarios). These datasets targeted four care dimensions (access to care, pain management, advance care planning, and place of death preferences) and three identity axes (ethnicity, age, and diagnosis). Bias was detected in a substantial proportion of responses. For adversarial questions, the pooled bias rate was 0.33 (95% confidence interval [CI]: 0.28, 0.38); \"allows biased premise\" was the most frequently identified source of bias (0.47; 95% CI: 0.39, 0.55), such as failing to challenge stereotypes. For counterfactual scenarios, the pooled bias rate was 0.26 (95% CI: 0.20, 0.31), with \"potential for withholding\" as the most frequently identified source of bias (0.25; 95% CI: 0.18, 0.34), such as withholding interventions based on identity. Bias rates were consistent across care dimensions and identity axes. GPT-4o perpetuates biases in palliative care, with implications for clinical decision-making and equity. The PCAD datasets provide novel tools to assess and address LLM bias in palliative care.","sentences":["Bias and inequity in palliative care disproportionately affect marginalised groups.","Large language models (LLMs), such as GPT-4o, hold potential to enhance care but risk perpetuating biases present in their training data.","This study aimed to systematically evaluate whether GPT-4o propagates biases in palliative care responses using adversarially designed datasets.","In July 2024, GPT-4o was probed using the Palliative Care Adversarial Dataset (PCAD), and responses were evaluated by three palliative care experts in Canada and the United Kingdom using validated bias rubrics.","The PCAD comprised PCAD-Direct (100 adversarial questions) and PCAD-Counterfactual (84 paired scenarios).","These datasets targeted four care dimensions (access to care, pain management, advance care planning, and place of death preferences) and three identity axes (ethnicity, age, and diagnosis).","Bias was detected in a substantial proportion of responses.","For adversarial questions, the pooled bias rate was 0.33 (95% confidence interval","[CI]: 0.28, 0.38); \"allows biased premise\" was the most frequently identified source of bias (0.47; 95% CI: 0.39, 0.55), such as failing to challenge stereotypes.","For counterfactual scenarios, the pooled bias rate was 0.26 (95% CI: 0.20, 0.31), with \"potential for withholding\" as the most frequently identified source of bias (0.25; 95% CI: 0.18, 0.34), such as withholding interventions based on identity.","Bias rates were consistent across care dimensions and identity axes.","GPT-4o perpetuates biases in palliative care, with implications for clinical decision-making and equity.","The PCAD datasets provide novel tools to assess and address LLM bias in palliative care."],"url":"http://arxiv.org/abs/2502.08073v1"}
{"created":"2025-02-12 02:24:26","title":"Collaborative Filtering Meets Spectrum Shift: Connecting User-Item Interaction with Graph-Structured Side Information","abstract":"Graph Neural Network (GNN) has demonstrated their superiority in collaborative filtering, where the user-item (U-I) interaction bipartite graph serves as the fundamental data format. However, when graph-structured side information (e.g., multimodal similarity graphs or social networks) is integrated into the U-I bipartite graph, existing graph collaborative filtering methods fall short of achieving satisfactory performance. We quantitatively analyze this problem from a spectral perspective. Recall that a bipartite graph possesses a full spectrum within the range of [-1, 1], with the highest frequency exactly achievable at -1 and the lowest frequency at 1; however, we observe as more side information is incorporated, the highest frequency of the augmented adjacency matrix progressively shifts rightward. This spectrum shift phenomenon has caused previous approaches built for the full spectrum [-1, 1] to assign mismatched importance to different frequencies. To this end, we propose Spectrum Shift Correction (dubbed SSC), incorporating shifting and scaling factors to enable spectral GNNs to adapt to the shifted spectrum. Unlike previous paradigms of leveraging side information, which necessitate tailored designs for diverse data types, SSC directly connects traditional graph collaborative filtering with any graph-structured side information. Experiments on social and multimodal recommendation demonstrate the effectiveness of SSC, achieving relative improvements of up to 23% without incurring any additional computational overhead.","sentences":["Graph Neural Network (GNN) has demonstrated their superiority in collaborative filtering, where the user-item (U-I) interaction bipartite graph serves as the fundamental data format.","However, when graph-structured side information (e.g., multimodal similarity graphs or social networks) is integrated into the U-I bipartite graph, existing graph collaborative filtering methods fall short of achieving satisfactory performance.","We quantitatively analyze this problem from a spectral perspective.","Recall that a bipartite graph possesses a full spectrum within the range of [-1, 1], with the highest frequency exactly achievable at -1 and the lowest frequency at 1; however, we observe as more side information is incorporated, the highest frequency of the augmented adjacency matrix progressively shifts rightward.","This spectrum shift phenomenon has caused previous approaches built for the full spectrum [-1, 1] to assign mismatched importance to different frequencies.","To this end, we propose Spectrum Shift Correction (dubbed SSC), incorporating shifting and scaling factors to enable spectral GNNs to adapt to the shifted spectrum.","Unlike previous paradigms of leveraging side information, which necessitate tailored designs for diverse data types, SSC directly connects traditional graph collaborative filtering with any graph-structured side information.","Experiments on social and multimodal recommendation demonstrate the effectiveness of SSC, achieving relative improvements of up to 23% without incurring any additional computational overhead."],"url":"http://arxiv.org/abs/2502.08071v1"}
{"created":"2025-02-12 02:04:46","title":"Multi-Agent Performative Prediction Beyond the Insensitivity Assumption: A Case Study for Mortgage Competition","abstract":"Performative prediction models account for feedback loops in decision-making processes where predictions influence future data distributions. While existing work largely assumes insensitivity of data distributions to small strategy changes, this assumption usually fails in real-world competitive (i.e. multi-agent) settings. For example, in Bertrand-type competitions, a small reduction in one firm's price can lead that firm to capture the entire demand, while all others sharply lose all of their customers.   We study a representative setting of multi-agent performative prediction in which insensitivity assumptions do not hold, and investigate the convergence of natural dynamics. To do so, we focus on a specific game that we call the ''Bank Game'', where two lenders compete over interest rates and credit score thresholds. Consumers act similarly as to in a Bertrand Competition, with each consumer selecting the firm with the lowest interest rate that they are eligible for based on the firms' credit thresholds. Our analysis characterizes the equilibria of this game and demonstrates that when both firms use a common and natural no-regret learning dynamic -- exponential weights -- with proper initialization, the dynamics always converge to stable outcomes despite the general-sum structure. Notably, our setting admits multiple stable equilibria, with convergence dependent on initial conditions. We also provide theoretical convergence results in the stochastic case when the utility matrix is not fully known, but each learner can observe sufficiently many samples of consumers at each time step to estimate it, showing robustness to slight mis-specifications. Finally, we provide experimental results that validate our theoretical findings.","sentences":["Performative prediction models account for feedback loops in decision-making processes where predictions influence future data distributions.","While existing work largely assumes insensitivity of data distributions to small strategy changes, this assumption usually fails in real-world competitive (i.e. multi-agent) settings.","For example, in Bertrand-type competitions, a small reduction in one firm's price can lead that firm to capture the entire demand, while all others sharply lose all of their customers.   ","We study a representative setting of multi-agent performative prediction in which insensitivity assumptions do not hold, and investigate the convergence of natural dynamics.","To do so, we focus on a specific game that we call the ''Bank Game'', where two lenders compete over interest rates and credit score thresholds.","Consumers act similarly as to in a Bertrand Competition, with each consumer selecting the firm with the lowest interest rate that they are eligible for based on the firms' credit thresholds.","Our analysis characterizes the equilibria of this game and demonstrates that when both firms use a common and natural no-regret learning dynamic -- exponential weights -- with proper initialization, the dynamics always converge to stable outcomes despite the general-sum structure.","Notably, our setting admits multiple stable equilibria, with convergence dependent on initial conditions.","We also provide theoretical convergence results in the stochastic case when the utility matrix is not fully known, but each learner can observe sufficiently many samples of consumers at each time step to estimate it, showing robustness to slight mis-specifications.","Finally, we provide experimental results that validate our theoretical findings."],"url":"http://arxiv.org/abs/2502.08063v1"}
{"created":"2025-02-12 01:54:21","title":"On Mechanistic Circuits for Extractive Question-Answering","abstract":"Large language models are increasingly used to process documents and facilitate question-answering on them. In our paper, we extract mechanistic circuits for this real-world language modeling task: context-augmented language modeling for extractive question-answering (QA) tasks and understand the potential benefits of circuits towards downstream applications such as data attribution to context information. We extract circuits as a function of internal model components (e.g., attention heads, MLPs) using causal mediation analysis techniques. Leveraging the extracted circuits, we first understand the interplay between the model's usage of parametric memory and retrieved context towards a better mechanistic understanding of context-augmented language models. We then identify a small set of attention heads in our circuit which performs reliable data attribution by default, thereby obtaining attribution for free in just the model's forward pass. Using this insight, we then introduce ATTNATTRIB, a fast data attribution algorithm which obtains state-of-the-art attribution results across various extractive QA benchmarks. Finally, we show the possibility to steer the language model towards answering from the context, instead of the parametric memory by using the attribution from ATTNATTRIB as an additional signal during the forward pass. Beyond mechanistic understanding, our paper provides tangible applications of circuits in the form of reliable data attribution and model steering.","sentences":["Large language models are increasingly used to process documents and facilitate question-answering on them.","In our paper, we extract mechanistic circuits for this real-world language modeling task: context-augmented language modeling for extractive question-answering (QA) tasks and understand the potential benefits of circuits towards downstream applications such as data attribution to context information.","We extract circuits as a function of internal model components (e.g., attention heads, MLPs) using causal mediation analysis techniques.","Leveraging the extracted circuits, we first understand the interplay between the model's usage of parametric memory and retrieved context towards a better mechanistic understanding of context-augmented language models.","We then identify a small set of attention heads in our circuit which performs reliable data attribution by default, thereby obtaining attribution for free in just the model's forward pass.","Using this insight, we then introduce ATTNATTRIB, a fast data attribution algorithm which obtains state-of-the-art attribution results across various extractive QA benchmarks.","Finally, we show the possibility to steer the language model towards answering from the context, instead of the parametric memory by using the attribution from ATTNATTRIB as an additional signal during the forward pass.","Beyond mechanistic understanding, our paper provides tangible applications of circuits in the form of reliable data attribution and model steering."],"url":"http://arxiv.org/abs/2502.08059v1"}
{"created":"2025-02-12 01:36:27","title":"Cognify: Supercharging Gen-AI Workflows With Hierarchical Autotuning","abstract":"Today's gen-AI workflows that involve multiple ML model calls, tool/API calls, data retrieval, or generic code execution are often tuned manually in an ad-hoc way that is both time-consuming and error-prone. In this paper, we propose a systematic approach for automatically tuning gen-AI workflows. Our key insight is that gen-AI workflows can benefit from structure, operator, and prompt changes, but unique properties of gen-AI workflows require new optimization techniques. We propose AdaSeek, an adaptive hierarchical search algorithm for autotuning gen-AI workflows. AdaSeek organizes workflow tuning methods into different layers based on the user-specified total search budget and distributes the budget across different layers based on the complexity of each layer. During its hierarchical search, AdaSeek redistributes the search budget from less useful to more promising tuning configurations based on workflow-level evaluation results. We implement AdaSeek in a workflow autotuning framework called Cognify and evaluate Cognify using six types of workflows such as RAG-based QA and text-to-SQL transformation. Overall, Cognify improves these workflows' generation quality by up to 2.8x, reduces execution monetary cost by up to 10x, and reduces end-to-end latency by 2.7x.","sentences":["Today's gen-AI workflows that involve multiple ML model calls, tool/API calls, data retrieval, or generic code execution are often tuned manually in an ad-hoc way that is both time-consuming and error-prone.","In this paper, we propose a systematic approach for automatically tuning gen-AI workflows.","Our key insight is that gen-AI workflows can benefit from structure, operator, and prompt changes, but unique properties of gen-AI workflows require new optimization techniques.","We propose AdaSeek, an adaptive hierarchical search algorithm for autotuning gen-AI workflows.","AdaSeek organizes workflow tuning methods into different layers based on the user-specified total search budget and distributes the budget across different layers based on the complexity of each layer.","During its hierarchical search, AdaSeek redistributes the search budget from less useful to more promising tuning configurations based on workflow-level evaluation results.","We implement AdaSeek in a workflow autotuning framework called Cognify and evaluate Cognify using six types of workflows such as RAG-based QA and text-to-SQL transformation.","Overall, Cognify improves these workflows' generation quality by up to 2.8x, reduces execution monetary cost by up to 10x, and reduces end-to-end latency by 2.7x."],"url":"http://arxiv.org/abs/2502.08056v1"}
{"created":"2025-02-12 01:31:39","title":"SLVR: Securely Leveraging Client Validation for Robust Federated Learning","abstract":"Federated Learning (FL) enables collaborative model training while keeping client data private. However, exposing individual client updates makes FL vulnerable to reconstruction attacks. Secure aggregation mitigates such privacy risks but prevents the server from verifying the validity of each client update, creating a privacy-robustness tradeoff. Recent efforts attempt to address this tradeoff by enforcing checks on client updates using zero-knowledge proofs, but they support limited predicates and often depend on public validation data. We propose SLVR, a general framework that securely leverages clients' private data through secure multi-party computation. By utilizing clients' data, SLVR not only eliminates the need for public validation data, but also enables a wider range of checks for robustness, including cross-client accuracy validation. It also adapts naturally to distribution shifts in client data as it can securely refresh its validation data up-to-date. Our empirical evaluations show that SLVR improves robustness against model poisoning attacks, particularly outperforming existing methods by up to 50% under adaptive attacks. Additionally, SLVR demonstrates effective adaptability and stable convergence under various distribution shift scenarios.","sentences":["Federated Learning (FL) enables collaborative model training while keeping client data private.","However, exposing individual client updates makes FL vulnerable to reconstruction attacks.","Secure aggregation mitigates such privacy risks but prevents the server from verifying the validity of each client update, creating a privacy-robustness tradeoff.","Recent efforts attempt to address this tradeoff by enforcing checks on client updates using zero-knowledge proofs, but they support limited predicates and often depend on public validation data.","We propose SLVR, a general framework that securely leverages clients' private data through secure multi-party computation.","By utilizing clients' data, SLVR not only eliminates the need for public validation data, but also enables a wider range of checks for robustness, including cross-client accuracy validation.","It also adapts naturally to distribution shifts in client data as it can securely refresh its validation data up-to-date.","Our empirical evaluations show that SLVR improves robustness against model poisoning attacks, particularly outperforming existing methods by up to 50% under adaptive attacks.","Additionally, SLVR demonstrates effective adaptability and stable convergence under various distribution shift scenarios."],"url":"http://arxiv.org/abs/2502.08055v1"}
{"created":"2025-02-12 01:13:52","title":"Can Machine Learning Support the Selection of Studies for Systematic Literature Review Updates?","abstract":"[Background] Systematic literature reviews (SLRs) are essential for synthesizing evidence in Software Engineering (SE), but keeping them up-to-date requires substantial effort. Study selection, one of the most labor-intensive steps, involves reviewing numerous studies and requires multiple reviewers to minimize bias and avoid loss of evidence. [Objective] This study aims to evaluate if Machine Learning (ML) text classification models can support reviewers in the study selection for SLR updates. [Method] We reproduce the study selection of an SLR update performed by three SE researchers. We trained two supervised ML models (Random Forest and Support Vector Machines) with different configurations using data from the original SLR. We calculated the study selection effectiveness of the ML models for the SLR update in terms of precision, recall, and F-measure. We also compared the performance of human-ML pairs with human-only pairs when selecting studies. [Results] The ML models achieved a modest F-score of 0.33, which is insufficient for reliable automation. However, we found that such models can reduce the study selection effort by 33.9% without loss of evidence (keeping a 100% recall). Our analysis also showed that the initial screening by pairs of human reviewers produces results that are much better aligned with the final SLR update result. [Conclusion] Based on our results, we conclude that although ML models can help reduce the effort involved in SLR updates, achieving rigorous and reliable outcomes still requires the expertise of experienced human reviewers for the initial screening phase.","sentences":["[Background] Systematic literature reviews (SLRs) are essential for synthesizing evidence in Software Engineering (SE), but keeping them up-to-date requires substantial effort.","Study selection, one of the most labor-intensive steps, involves reviewing numerous studies and requires multiple reviewers to minimize bias and avoid loss of evidence.","[Objective] This study aims to evaluate if Machine Learning (ML) text classification models can support reviewers in the study selection for SLR updates.","[Method] We reproduce the study selection of an SLR update performed by three SE researchers.","We trained two supervised ML models (Random Forest and Support Vector Machines) with different configurations using data from the original SLR.","We calculated the study selection effectiveness of the ML models for the SLR update in terms of precision, recall, and F-measure.","We also compared the performance of human-ML pairs with human-only pairs when selecting studies.","[Results] The ML models achieved a modest F-score of 0.33, which is insufficient for reliable automation.","However, we found that such models can reduce the study selection effort by 33.9% without loss of evidence (keeping a 100% recall).","Our analysis also showed that the initial screening by pairs of human reviewers produces results that are much better aligned with the final SLR update result.","[Conclusion] Based on our results, we conclude that although ML models can help reduce the effort involved in SLR updates, achieving rigorous and reliable outcomes still requires the expertise of experienced human reviewers for the initial screening phase."],"url":"http://arxiv.org/abs/2502.08050v1"}
{"created":"2025-02-12 00:58:31","title":"Parallel $k$-Core Decomposition: Theory and Practice","abstract":"This paper proposes efficient solutions for $k$-core decomposition with high parallelism. The problem of $k$-core decomposition is fundamental in graph analysis and has applications across various domains. However, existing algorithms face significant challenges in achieving work-efficiency in theory and/or high parallelism in practice, and suffer from various performance bottlenecks.   We present a simple, work-efficient parallel framework for $k$-core decomposition that is easy to implement and adaptable to various strategies for improving work-efficiency. We introduce two techniques to enhance parallelism: a sampling scheme to reduce contention on high-degree vertices, and vertical granularity control (VGC) to mitigate scheduling overhead for low-degree vertices. Furthermore, we design a hierarchical bucket structure to optimize performance for graphs with high coreness values.   We evaluate our algorithm on a diverse set of real-world and synthetic graphs. Compared to state-of-the-art parallel algorithms, including ParK, PKC, and Julienne, our approach demonstrates superior performance on 23 out of 25 graphs when tested on a 96-core machine. Our algorithm shows speedups of up to 315$\\times$ over ParK, 33.4$\\times$ over PKC, and 52.5$\\times$ over Julienne.","sentences":["This paper proposes efficient solutions for $k$-core decomposition with high parallelism.","The problem of $k$-core decomposition is fundamental in graph analysis and has applications across various domains.","However, existing algorithms face significant challenges in achieving work-efficiency in theory and/or high parallelism in practice, and suffer from various performance bottlenecks.   ","We present a simple, work-efficient parallel framework for $k$-core decomposition that is easy to implement and adaptable to various strategies for improving work-efficiency.","We introduce two techniques to enhance parallelism: a sampling scheme to reduce contention on high-degree vertices, and vertical granularity control (VGC) to mitigate scheduling overhead for low-degree vertices.","Furthermore, we design a hierarchical bucket structure to optimize performance for graphs with high coreness values.   ","We evaluate our algorithm on a diverse set of real-world and synthetic graphs.","Compared to state-of-the-art parallel algorithms, including ParK, PKC, and Julienne, our approach demonstrates superior performance on 23 out of 25 graphs when tested on a 96-core machine.","Our algorithm shows speedups of up to 315$\\times$ over ParK, 33.4$\\times$ over PKC, and 52.5$\\times$ over Julienne."],"url":"http://arxiv.org/abs/2502.08042v1"}
{"created":"2025-02-12 00:57:53","title":"The Art of Misclassification: Too Many Classes, Not Enough Points","abstract":"Classification is a ubiquitous and fundamental problem in artificial intelligence and machine learning, with extensive efforts dedicated to developing more powerful classifiers and larger datasets. However, the classification task is ultimately constrained by the intrinsic properties of datasets, independently of computational power or model complexity. In this work, we introduce a formal entropy-based measure of classificability, which quantifies the inherent difficulty of a classification problem by assessing the uncertainty in class assignments given feature representations. This measure captures the degree of class overlap and aligns with human intuition, serving as an upper bound on classification performance for classification problems. Our results establish a theoretical limit beyond which no classifier can improve the classification accuracy, regardless of the architecture or amount of data, in a given problem. Our approach provides a principled framework for understanding when classification is inherently fallible and fundamentally ambiguous.","sentences":["Classification is a ubiquitous and fundamental problem in artificial intelligence and machine learning, with extensive efforts dedicated to developing more powerful classifiers and larger datasets.","However, the classification task is ultimately constrained by the intrinsic properties of datasets, independently of computational power or model complexity.","In this work, we introduce a formal entropy-based measure of classificability, which quantifies the inherent difficulty of a classification problem by assessing the uncertainty in class assignments given feature representations.","This measure captures the degree of class overlap and aligns with human intuition, serving as an upper bound on classification performance for classification problems.","Our results establish a theoretical limit beyond which no classifier can improve the classification accuracy, regardless of the architecture or amount of data, in a given problem.","Our approach provides a principled framework for understanding when classification is inherently fallible and fundamentally ambiguous."],"url":"http://arxiv.org/abs/2502.08041v1"}
{"created":"2025-02-12 00:38:11","title":"Franken-Adapter: Cross-Lingual Adaptation of LLMs by Embedding Surgery","abstract":"The capabilities of Large Language Models (LLMs) in low-resource languages lag far behind those in English, making their universal accessibility a significant challenge. To alleviate this, we present $\\textit{Franken-Adapter}$, a modular language adaptation approach for decoder-only LLMs with embedding surgery. Our method begins by creating customized vocabularies for target languages and performing language adaptation through embedding tuning on multilingual data. These pre-trained embeddings are subsequently integrated with LLMs that have been instruction-tuned on English alignment data to enable zero-shot cross-lingual transfer. Our experiments on $\\texttt{Gemma2}$ models with up to 27B parameters demonstrate improvements of up to 20% across 96 languages, spanning both discriminative and generative tasks, with minimal regressions ($<$1%) in English. Further in-depth analysis reveals the critical role of customizing tokenizers in enhancing language adaptation, while boosting inference efficiency. Additionally, we show the versatility of our method by achieving a 14% improvement over a math-optimized LLM across 20 languages, offering a modular solution to transfer reasoning abilities across languages post hoc.","sentences":["The capabilities of Large Language Models (LLMs) in low-resource languages lag far behind those in English, making their universal accessibility a significant challenge.","To alleviate this, we present $\\textit{Franken-Adapter}$, a modular language adaptation approach for decoder-only LLMs with embedding surgery.","Our method begins by creating customized vocabularies for target languages and performing language adaptation through embedding tuning on multilingual data.","These pre-trained embeddings are subsequently integrated with LLMs that have been instruction-tuned on English alignment data to enable zero-shot cross-lingual transfer.","Our experiments on $\\texttt{Gemma2}$ models with up to 27B parameters demonstrate improvements of up to 20% across 96 languages, spanning both discriminative and generative tasks, with minimal regressions ($<$1%) in English.","Further in-depth analysis reveals the critical role of customizing tokenizers in enhancing language adaptation, while boosting inference efficiency.","Additionally, we show the versatility of our method by achieving a 14% improvement over a math-optimized LLM across 20 languages, offering a modular solution to transfer reasoning abilities across languages post hoc."],"url":"http://arxiv.org/abs/2502.08037v1"}
{"created":"2025-02-12 00:26:01","title":"End-to-End Predictive Planner for Autonomous Driving with Consistency Models","abstract":"Trajectory prediction and planning are fundamental components for autonomous vehicles to navigate safely and efficiently in dynamic environments. Traditionally, these components have often been treated as separate modules, limiting the ability to perform interactive planning and leading to computational inefficiency in multi-agent scenarios. In this paper, we present a novel unified and data-driven framework that integrates prediction and planning with a single consistency model. Trained on real-world human driving datasets, our consistency model generates samples from high-dimensional, multimodal joint trajectory distributions of the ego and multiple surrounding agents, enabling end-to-end predictive planning. It effectively produces interactive behaviors, such as proactive nudging and yielding to ensure both safe and efficient interactions with other road users. To incorporate additional planning constraints on the ego vehicle, we propose an alternating direction method for multi-objective guidance in online guided sampling. Compared to diffusion models, our consistency model achieves better performance with fewer sampling steps, making it more suitable for real-time deployment. Experimental results on Waymo Open Motion Dataset (WOMD) demonstrate our method's superiority in trajectory quality, constraint satisfaction, and interactive behavior compared to various existing approaches.","sentences":["Trajectory prediction and planning are fundamental components for autonomous vehicles to navigate safely and efficiently in dynamic environments.","Traditionally, these components have often been treated as separate modules, limiting the ability to perform interactive planning and leading to computational inefficiency in multi-agent scenarios.","In this paper, we present a novel unified and data-driven framework that integrates prediction and planning with a single consistency model.","Trained on real-world human driving datasets, our consistency model generates samples from high-dimensional, multimodal joint trajectory distributions of the ego and multiple surrounding agents, enabling end-to-end predictive planning.","It effectively produces interactive behaviors, such as proactive nudging and yielding to ensure both safe and efficient interactions with other road users.","To incorporate additional planning constraints on the ego vehicle, we propose an alternating direction method for multi-objective guidance in online guided sampling.","Compared to diffusion models, our consistency model achieves better performance with fewer sampling steps, making it more suitable for real-time deployment.","Experimental results on Waymo Open Motion Dataset (WOMD) demonstrate our method's superiority in trajectory quality, constraint satisfaction, and interactive behavior compared to various existing approaches."],"url":"http://arxiv.org/abs/2502.08033v1"}
{"created":"2025-02-12 00:18:53","title":"Shortcuts and Transitive-Closure Spanners Approximation","abstract":"We study polynomial-time approximation algorithms for two closely-related problems, namely computing shortcuts and transitive-closure spanners (TC spanner). For a directed unweighted graph $G=(V, E)$ and an integer $d$, a set of edges $E'\\subseteq V\\times V$ is called a $d$-TC spanner of $G$ if the graph $H:=(V, E')$ has (i) the same transitive-closure as $G$ and (ii) diameter at most $d.$ The set $E''\\subseteq V\\times V$ is a $d$-shortcut of $G$ if $E\\cup E''$ is a $d$-TC spanner of $G$. Our focus is on the following $(\\alpha_D, \\alpha_S)$-approximation algorithm: given a directed graph $G$ and integers $d$ and $s$ such that $G$ admits a $d$-shortcut (respectively $d$-TC spanner) of size $s$, find a $(d\\alpha_D)$-shortcut (resp. $(d\\alpha_D)$-TC spanner) with $s\\alpha_S$ edges, for as small $\\alpha_S$ and $\\alpha_D$ as possible.   As our main result, we show that, under the Projection Game Conjecture (PGC), there exists a small constant $\\epsilon>0$, such that no polynomial-time $(n^{\\epsilon},n^{\\epsilon})$-approximation algorithm exists for finding $d$-shortcuts as well as $d$-TC spanners of size $s$. Previously, super-constant lower bounds were known only for $d$-TC spanners with constant $d$ and ${\\alpha_D}=1$ [Bhattacharyya, Grigorescu, Jung, Raskhodnikova, Woodruff 2009]. Similar lower bounds for super-constant $d$ were previously known only for a more general case of directed spanners [Elkin, Peleg 2000]. No hardness of approximation result was known for shortcuts prior to our result.   As a side contribution, we complement the above with an upper bound of the form $(n^{\\gamma_D}, n^{\\gamma_S})$-approximation which holds for $3\\gamma_D + 2\\gamma_S > 1$ (e.g., $(n^{1/5+o(1)}, n^{1/5+o(1)})$-approximation).","sentences":["We study polynomial-time approximation algorithms for two closely-related problems, namely computing shortcuts and transitive-closure spanners (TC spanner).","For a directed unweighted graph $G=(V, E)$ and an integer $d$, a set of edges $E'\\subseteq V\\times V$ is called a $d$-TC spanner of $G$ if the graph $H:=(V, E')$ has (i) the same transitive-closure as $G$ and (ii) diameter at most $d.$ The set $E''\\subseteq V\\times V$ is a $d$-shortcut of $G$ if $E\\cup E''$ is a $d$-TC spanner of $G$. Our focus is on the following $(\\alpha_D, \\alpha_S)$-approximation algorithm: given a directed graph $G$ and integers $d$ and $s$ such that $G$ admits a $d$-shortcut (respectively $d$-TC spanner) of size $s$, find a $(d\\alpha_D)$-shortcut (resp.","$(d\\alpha_D)$-TC spanner) with $s\\alpha_S$ edges, for as small $\\alpha_S$ and $\\alpha_D$ as possible.   ","As our main result, we show that, under the Projection Game Conjecture (PGC), there exists a small constant $\\epsilon>0$, such that no polynomial-time $(n^{\\epsilon},n^{\\epsilon})$-approximation algorithm exists for finding $d$-shortcuts as well as $d$-TC spanners of size $s$. Previously, super-constant lower bounds were known only for $d$-TC spanners with constant $d$ and","${\\alpha_D}=1$","[Bhattacharyya, Grigorescu, Jung, Raskhodnikova, Woodruff 2009].","Similar lower bounds for super-constant $d$ were previously known only for a more general case of directed spanners","[Elkin, Peleg 2000].","No hardness of approximation result was known for shortcuts prior to our result.   ","As a side contribution, we complement the above with an upper bound of the form $(n^{\\gamma_D}, n^{\\gamma_S})$-approximation which holds for $3\\gamma_D + 2\\gamma_S > 1$ (e.g., $(n^{1/5+o(1)}, n^{1/5+o(1)})$-approximation)."],"url":"http://arxiv.org/abs/2502.08032v1"}
{"created":"2025-02-12 00:09:18","title":"Understanding the Kronecker Matrix-Vector Complexity of Linear Algebra","abstract":"We study the computational model where we can access a matrix $\\mathbf{A}$ only by computing matrix-vector products $\\mathbf{A}\\mathrm{x}$ for vectors of the form $\\mathrm{x} = \\mathrm{x}_1 \\otimes \\cdots \\otimes \\mathrm{x}_q$. We prove exponential lower bounds on the number of queries needed to estimate various properties, including the trace and the top eigenvalue of $\\mathbf{A}$. Our proofs hold for all adaptive algorithms, modulo a mild conditioning assumption on the algorithm's queries. We further prove that algorithms whose queries come from a small alphabet (e.g., $\\mathrm{x}_i \\in \\{\\pm1\\}^n$) cannot test if $\\mathbf{A}$ is identically zero with polynomial complexity, despite the fact that a single query using Gaussian vectors solves the problem with probability 1. In steep contrast to the non-Kronecker case, this shows that sketching $\\mathbf{A}$ with different distributions of the same subguassian norm can yield exponentially different query complexities. Our proofs follow from the observation that random vectors with Kronecker structure have exponentially smaller inner products than their non-Kronecker counterparts.","sentences":["We study the computational model where we can access a matrix $\\mathbf{A}$ only by computing matrix-vector products $\\mathbf{A}\\mathrm{x}$ for vectors of the form $\\mathrm{x} = \\mathrm{x}_1 \\otimes \\cdots \\otimes \\mathrm{x}_q$. We prove exponential lower bounds on the number of queries needed to estimate various properties, including the trace and the top eigenvalue of $\\mathbf{A}$. Our proofs hold for all adaptive algorithms, modulo a mild conditioning assumption on the algorithm's queries.","We further prove that algorithms whose queries come from a small alphabet (e.g., $\\mathrm{x}_i \\in \\{\\pm1\\}^n$) cannot test if $\\mathbf{A}$ is identically zero with polynomial complexity, despite the fact that a single query using Gaussian vectors solves the problem with probability 1.","In steep contrast to the non-Kronecker case, this shows that sketching $\\mathbf{A}$ with different distributions of the same subguassian norm can yield exponentially different query complexities.","Our proofs follow from the observation that random vectors with Kronecker structure have exponentially smaller inner products than their non-Kronecker counterparts."],"url":"http://arxiv.org/abs/2502.08029v1"}
{"created":"2025-02-11 23:55:16","title":"From Brainwaves to Brain Scans: A Robust Neural Network for EEG-to-fMRI Synthesis","abstract":"While functional magnetic resonance imaging (fMRI) offers rich spatial resolution, it is limited by high operational costs and significant infrastructural demands. In contrast, electroencephalography (EEG) provides millisecond-level precision in capturing electrical activity but lacks the spatial resolution necessary for precise neural localization. To bridge these gaps, we introduce E2fNet, a simple yet effective deep learning model for synthesizing fMRI images from low-cost EEG data. E2fNet is specifically designed to capture and translate meaningful features from EEG across electrode channels into accurate fMRI representations. Extensive evaluations across three datasets demonstrate that E2fNet consistently outperforms existing methods, achieving state-of-the-art results in terms of the structural similarity index measure (SSIM). Our findings suggest that E2fNet is a promising, cost-effective solution for enhancing neuroimaging capabilities. The code is available at https://github.com/kgr20/E2fNet.","sentences":["While functional magnetic resonance imaging (fMRI) offers rich spatial resolution, it is limited by high operational costs and significant infrastructural demands.","In contrast, electroencephalography (EEG) provides millisecond-level precision in capturing electrical activity but lacks the spatial resolution necessary for precise neural localization.","To bridge these gaps, we introduce E2fNet, a simple yet effective deep learning model for synthesizing fMRI images from low-cost EEG data.","E2fNet is specifically designed to capture and translate meaningful features from EEG across electrode channels into accurate fMRI representations.","Extensive evaluations across three datasets demonstrate that E2fNet consistently outperforms existing methods, achieving state-of-the-art results in terms of the structural similarity index measure (SSIM).","Our findings suggest that E2fNet is a promising, cost-effective solution for enhancing neuroimaging capabilities.","The code is available at https://github.com/kgr20/E2fNet."],"url":"http://arxiv.org/abs/2502.08025v1"}
{"created":"2025-02-11 23:53:16","title":"Initialization Matters: Unraveling the Impact of Pre-Training on Federated Learning","abstract":"Initializing with pre-trained models when learning on downstream tasks is becoming standard practice in machine learning. Several recent works explore the benefits of pre-trained initialization in a federated learning (FL) setting, where the downstream training is performed at the edge clients with heterogeneous data distribution. These works show that starting from a pre-trained model can substantially reduce the adverse impact of data heterogeneity on the test performance of a model trained in a federated setting, with no changes to the standard FedAvg training algorithm. In this work, we provide a deeper theoretical understanding of this phenomenon. To do so, we study the class of two-layer convolutional neural networks (CNNs) and provide bounds on the training error convergence and test error of such a network trained with FedAvg. We introduce the notion of aligned and misaligned filters at initialization and show that the data heterogeneity only affects learning on misaligned filters. Starting with a pre-trained model typically results in fewer misaligned filters at initialization, thus producing a lower test error even when the model is trained in a federated setting with data heterogeneity. Experiments in synthetic settings and practical FL training on CNNs verify our theoretical findings.","sentences":["Initializing with pre-trained models when learning on downstream tasks is becoming standard practice in machine learning.","Several recent works explore the benefits of pre-trained initialization in a federated learning (FL) setting, where the downstream training is performed at the edge clients with heterogeneous data distribution.","These works show that starting from a pre-trained model can substantially reduce the adverse impact of data heterogeneity on the test performance of a model trained in a federated setting, with no changes to the standard FedAvg training algorithm.","In this work, we provide a deeper theoretical understanding of this phenomenon.","To do so, we study the class of two-layer convolutional neural networks (CNNs) and provide bounds on the training error convergence and test error of such a network trained with FedAvg.","We introduce the notion of aligned and misaligned filters at initialization and show that the data heterogeneity only affects learning on misaligned filters.","Starting with a pre-trained model typically results in fewer misaligned filters at initialization, thus producing a lower test error even when the model is trained in a federated setting with data heterogeneity.","Experiments in synthetic settings and practical FL training on CNNs verify our theoretical findings."],"url":"http://arxiv.org/abs/2502.08024v1"}
{"created":"2025-02-11 23:40:55","title":"Model Selection for Off-policy Evaluation: New Algorithms and Experimental Protocol","abstract":"Holdout validation and hyperparameter tuning from data is a long-standing problem in offline reinforcement learning (RL). A standard framework is to use off-policy evaluation (OPE) methods to evaluate and select the policies, but OPE either incurs exponential variance (e.g., importance sampling) or has hyperparameters on their own (e.g., FQE and model-based). In this work we focus on hyperparameter tuning for OPE itself, which is even more under-investigated. Concretely, we select among candidate value functions (\"model-free\") or dynamics (\"model-based\") to best assess the performance of a target policy. Our contributions are two fold. We develop: (1) new model-free and model-based selectors with theoretical guarantees, and (2) a new experimental protocol for empirically evaluating them. Compared to the model-free protocol in prior works, our new protocol allows for more stable generation of candidate value functions, better control of misspecification, and evaluation of model-free and model-based methods alike. We exemplify the protocol on a Gym environment, and find that our new model-free selector, LSTD-Tournament, demonstrates promising empirical performance.","sentences":["Holdout validation and hyperparameter tuning from data is a long-standing problem in offline reinforcement learning (RL).","A standard framework is to use off-policy evaluation (OPE) methods to evaluate and select the policies, but OPE either incurs exponential variance (e.g., importance sampling) or has hyperparameters on their own (e.g., FQE and model-based).","In this work we focus on hyperparameter tuning for OPE itself, which is even more under-investigated.","Concretely, we select among candidate value functions (\"model-free\") or dynamics (\"model-based\") to best assess the performance of a target policy.","Our contributions are two fold.","We develop: (1) new model-free and model-based selectors with theoretical guarantees, and (2) a new experimental protocol for empirically evaluating them.","Compared to the model-free protocol in prior works, our new protocol allows for more stable generation of candidate value functions, better control of misspecification, and evaluation of model-free and model-based methods alike.","We exemplify the protocol on a Gym environment, and find that our new model-free selector, LSTD-Tournament, demonstrates promising empirical performance."],"url":"http://arxiv.org/abs/2502.08021v1"}
{"created":"2025-02-11 23:14:39","title":"Training-Free Safe Denoisers for Safe Use of Diffusion Models","abstract":"There is growing concern over the safety of powerful diffusion models (DMs), as they are often misused to produce inappropriate, not-safe-for-work (NSFW) content or generate copyrighted material or data of individuals who wish to be forgotten. Many existing methods tackle these issues by heavily relying on text-based negative prompts or extensively retraining DMs to eliminate certain features or samples. In this paper, we take a radically different approach, directly modifying the sampling trajectory by leveraging a negation set (e.g., unsafe images, copyrighted data, or datapoints needed to be excluded) to avoid specific regions of data distribution, without needing to retrain or fine-tune DMs. We formally derive the relationship between the expected denoised samples that are safe and those that are not safe, leading to our $\\textit{safe}$ denoiser which ensures its final samples are away from the area to be negated. Inspired by the derivation, we develop a practical algorithm that successfully produces high-quality samples while avoiding negation areas of the data distribution in text-conditional, class-conditional, and unconditional image generation scenarios. These results hint at the great potential of our training-free safe denoiser for using DMs more safely.","sentences":["There is growing concern over the safety of powerful diffusion models (DMs), as they are often misused to produce inappropriate, not-safe-for-work (NSFW) content or generate copyrighted material or data of individuals who wish to be forgotten.","Many existing methods tackle these issues by heavily relying on text-based negative prompts or extensively retraining DMs to eliminate certain features or samples.","In this paper, we take a radically different approach, directly modifying the sampling trajectory by leveraging a negation set (e.g., unsafe images, copyrighted data, or datapoints needed to be excluded) to avoid specific regions of data distribution, without needing to retrain or fine-tune DMs.","We formally derive the relationship between the expected denoised samples that are safe and those that are not safe, leading to our $\\textit{safe}$ denoiser which ensures its final samples are away from the area to be negated.","Inspired by the derivation, we develop a practical algorithm that successfully produces high-quality samples while avoiding negation areas of the data distribution in text-conditional, class-conditional, and unconditional image generation scenarios.","These results hint at the great potential of our training-free safe denoiser for using DMs more safely."],"url":"http://arxiv.org/abs/2502.08011v1"}
{"created":"2025-02-11 23:07:14","title":"An Interactive Framework for Implementing Privacy-Preserving Federated Learning: Experiments on Large Language Models","abstract":"Federated learning (FL) enhances privacy by keeping user data on local devices. However, emerging attacks have demonstrated that the updates shared by users during training can reveal significant information about their data. This has greatly thwart the adoption of FL methods for training robust AI models in sensitive applications. Differential Privacy (DP) is considered the gold standard for safeguarding user data. However, DP guarantees are highly conservative, providing worst-case privacy guarantees. This can result in overestimating privacy needs, which may compromise the model's accuracy. Additionally, interpretations of these privacy guarantees have proven to be challenging in different contexts. This is further exacerbated when other factors, such as the number of training iterations, data distribution, and specific application requirements, can add further complexity to this problem. In this work, we proposed a framework that integrates a human entity as a privacy practitioner to determine an optimal trade-off between the model's privacy and utility. Our framework is the first to address the variable memory requirement of existing DP methods in FL settings, where resource-limited devices (e.g., cell phones) can participate. To support such settings, we adopt a recent DP method with fixed memory usage to ensure scalable private FL. We evaluated our proposed framework by fine-tuning a BERT-based LLM model using the GLUE dataset (a common approach in literature), leveraging the new accountant, and employing diverse data partitioning strategies to mimic real-world conditions. As a result, we achieved stable memory usage, with an average accuracy reduction of 1.33% for $\\epsilon = 10$ and 1.9% for $\\epsilon = 6$, when compared to the state-of-the-art DP accountant which does not support fixed memory usage.","sentences":["Federated learning (FL) enhances privacy by keeping user data on local devices.","However, emerging attacks have demonstrated that the updates shared by users during training can reveal significant information about their data.","This has greatly thwart the adoption of FL methods for training robust AI models in sensitive applications.","Differential Privacy (DP) is considered the gold standard for safeguarding user data.","However, DP guarantees are highly conservative, providing worst-case privacy guarantees.","This can result in overestimating privacy needs, which may compromise the model's accuracy.","Additionally, interpretations of these privacy guarantees have proven to be challenging in different contexts.","This is further exacerbated when other factors, such as the number of training iterations, data distribution, and specific application requirements, can add further complexity to this problem.","In this work, we proposed a framework that integrates a human entity as a privacy practitioner to determine an optimal trade-off between the model's privacy and utility.","Our framework is the first to address the variable memory requirement of existing DP methods in FL settings, where resource-limited devices (e.g., cell phones) can participate.","To support such settings, we adopt a recent DP method with fixed memory usage to ensure scalable private FL.","We evaluated our proposed framework by fine-tuning a BERT-based LLM model using the GLUE dataset (a common approach in literature), leveraging the new accountant, and employing diverse data partitioning strategies to mimic real-world conditions.","As a result, we achieved stable memory usage, with an average accuracy reduction of 1.33% for $\\epsilon = 10$ and 1.9% for $\\epsilon = 6$, when compared to the state-of-the-art DP accountant which does not support fixed memory usage."],"url":"http://arxiv.org/abs/2502.08008v1"}
{"created":"2025-02-11 22:48:49","title":"Unveiling Client Privacy Leakage from Public Dataset Usage in Federated Distillation","abstract":"Federated Distillation (FD) has emerged as a popular federated training framework, enabling clients to collaboratively train models without sharing private data. Public Dataset-Assisted Federated Distillation (PDA-FD), which leverages public datasets for knowledge sharing, has become widely adopted. Although PDA-FD enhances privacy compared to traditional Federated Learning, we demonstrate that the use of public datasets still poses significant privacy risks to clients' private training data. This paper presents the first comprehensive privacy analysis of PDA-FD in presence of an honest-but-curious server. We show that the server can exploit clients' inference results on public datasets to extract two critical types of private information: label distributions and membership information of the private training dataset. To quantify these vulnerabilities, we introduce two novel attacks specifically designed for the PDA-FD setting: a label distribution inference attack and innovative membership inference methods based on Likelihood Ratio Attack (LiRA). Through extensive evaluation of three representative PDA-FD frameworks (FedMD, DS-FL, and Cronus), our attacks achieve state-of-the-art performance, with label distribution attacks reaching minimal KL-divergence and membership inference attacks maintaining high True Positive Rates under low False Positive Rate constraints. Our findings reveal significant privacy risks in current PDA-FD frameworks and emphasize the need for more robust privacy protection mechanisms in collaborative learning systems.","sentences":["Federated Distillation (FD) has emerged as a popular federated training framework, enabling clients to collaboratively train models without sharing private data.","Public Dataset-Assisted Federated Distillation (PDA-FD), which leverages public datasets for knowledge sharing, has become widely adopted.","Although PDA-FD enhances privacy compared to traditional Federated Learning, we demonstrate that the use of public datasets still poses significant privacy risks to clients' private training data.","This paper presents the first comprehensive privacy analysis of PDA-FD in presence of an honest-but-curious server.","We show that the server can exploit clients' inference results on public datasets to extract two critical types of private information: label distributions and membership information of the private training dataset.","To quantify these vulnerabilities, we introduce two novel attacks specifically designed for the PDA-FD setting: a label distribution inference attack and innovative membership inference methods based on Likelihood Ratio Attack (LiRA).","Through extensive evaluation of three representative PDA-FD frameworks (FedMD, DS-FL, and Cronus), our attacks achieve state-of-the-art performance, with label distribution attacks reaching minimal KL-divergence and membership inference attacks maintaining high True Positive Rates under low False Positive Rate constraints.","Our findings reveal significant privacy risks in current PDA-FD frameworks and emphasize the need for more robust privacy protection mechanisms in collaborative learning systems."],"url":"http://arxiv.org/abs/2502.08001v1"}
{"created":"2025-02-11 22:34:49","title":"Adaptive kernel predictors from feature-learning infinite limits of neural networks","abstract":"Previous influential work showed that infinite width limits of neural networks in the lazy training regime are described by kernel machines. Here, we show that neural networks trained in the rich, feature learning infinite-width regime in two different settings are also described by kernel machines, but with data-dependent kernels. For both cases, we provide explicit expressions for the kernel predictors and prescriptions to numerically calculate them. To derive the first predictor, we study the large-width limit of feature-learning Bayesian networks, showing how feature learning leads to task-relevant adaptation of layer kernels and preactivation densities. The saddle point equations governing this limit result in a min-max optimization problem that defines the kernel predictor. To derive the second predictor, we study gradient flow training of randomly initialized networks trained with weight decay in the infinite-width limit using dynamical mean field theory (DMFT). The fixed point equations of the arising DMFT defines the task-adapted internal representations and the kernel predictor. We compare our kernel predictors to kernels derived from lazy regime and demonstrate that our adaptive kernels achieve lower test loss on benchmark datasets.","sentences":["Previous influential work showed that infinite width limits of neural networks in the lazy training regime are described by kernel machines.","Here, we show that neural networks trained in the rich, feature learning infinite-width regime in two different settings are also described by kernel machines, but with data-dependent kernels.","For both cases, we provide explicit expressions for the kernel predictors and prescriptions to numerically calculate them.","To derive the first predictor, we study the large-width limit of feature-learning Bayesian networks, showing how feature learning leads to task-relevant adaptation of layer kernels and preactivation densities.","The saddle point equations governing this limit result in a min-max optimization problem that defines the kernel predictor.","To derive the second predictor, we study gradient flow training of randomly initialized networks trained with weight decay in the infinite-width limit using dynamical mean field theory (DMFT).","The fixed point equations of the arising DMFT defines the task-adapted internal representations and the kernel predictor.","We compare our kernel predictors to kernels derived from lazy regime and demonstrate that our adaptive kernels achieve lower test loss on benchmark datasets."],"url":"http://arxiv.org/abs/2502.07998v1"}
{"created":"2025-02-11 22:14:30","title":"Learning Effective Dynamics across Spatio-Temporal Scales of Complex Flows","abstract":"Modeling and simulation of complex fluid flows with dynamics that span multiple spatio-temporal scales is a fundamental challenge in many scientific and engineering domains. Full-scale resolving simulations for systems such as highly turbulent flows are not feasible in the foreseeable future, and reduced-order models must capture dynamics that involve interactions across scales. In the present work, we propose a novel framework, Graph-based Learning of Effective Dynamics (Graph-LED), that leverages graph neural networks (GNNs), as well as an attention-based autoregressive model, to extract the effective dynamics from a small amount of simulation data. GNNs represent flow fields on unstructured meshes as graphs and effectively handle complex geometries and non-uniform grids. The proposed method combines a GNN based, dimensionality reduction for variable-size unstructured meshes with an autoregressive temporal attention model that can learn temporal dependencies automatically. We evaluated the proposed approach on a suite of fluid dynamics problems, including flow past a cylinder and flow over a backward-facing step over a range of Reynolds numbers. The results demonstrate robust and effective forecasting of spatio-temporal physics; in the case of the flow past a cylinder, both small-scale effects that occur close to the cylinder as well as its wake are accurately captured.","sentences":["Modeling and simulation of complex fluid flows with dynamics that span multiple spatio-temporal scales is a fundamental challenge in many scientific and engineering domains.","Full-scale resolving simulations for systems such as highly turbulent flows are not feasible in the foreseeable future, and reduced-order models must capture dynamics that involve interactions across scales.","In the present work, we propose a novel framework, Graph-based Learning of Effective Dynamics (Graph-LED), that leverages graph neural networks (GNNs), as well as an attention-based autoregressive model, to extract the effective dynamics from a small amount of simulation data.","GNNs represent flow fields on unstructured meshes as graphs and effectively handle complex geometries and non-uniform grids.","The proposed method combines a GNN based, dimensionality reduction for variable-size unstructured meshes with an autoregressive temporal attention model that can learn temporal dependencies automatically.","We evaluated the proposed approach on a suite of fluid dynamics problems, including flow past a cylinder and flow over a backward-facing step over a range of Reynolds numbers.","The results demonstrate robust and effective forecasting of spatio-temporal physics; in the case of the flow past a cylinder, both small-scale effects that occur close to the cylinder as well as its wake are accurately captured."],"url":"http://arxiv.org/abs/2502.07990v1"}
{"created":"2025-02-11 21:59:19","title":"Welzijn.AI: A Conversational AI System for Monitoring Mental Well-being and a Use Case for Responsible AI Development","abstract":"We present Welzijn.AI as new digital solution for monitoring mental well-being in the elderly, as a use case illustrating how recent guidelines on responsible Artificial Intelligence can inform Welzijn.AI's Technology and Value dimensions. Here Technology concerns the description of an open, well-documented and interpretable envisioned architecture in light of the system's goals; Value concerns stakeholder evaluations of Welzijn.AI. Stakeholders included, among others, informal/professional caregivers, a developer, patient and physician federations, and the elderly. Brief empirical evaluations comprised a SWOT-analysis, co-creation session, and user evaluation of a proof-of-concept implementation of Welzijn.AI. The SWOT analysis summarises stakeholder evaluations of Welzijn.AI in terms of its Strengths, Weaknesses, Opportunities and Threats. The co-creation session ranks technical, environmental and user-related requirements of Welzijn.AI with the Hundred Dollar Method. User evaluation comprises (dis)agreement on statements targeting Welzijn.AI's main characteristics, and a ranking of desired social characteristics. We found that stakeholders stress different aspects of Welzijn.AI. For example, medical professionals highlight in the SWOT analysis Welzijn.AI as the key unlocking an individual's social network, whereas in the co-creation session, more user-related aspects such as demo and practice sessions were emphasised. Stakeholders aligned on the importance of safe data storage and access. The elderly evaluated Welzijn.AI's accessibility and perceived trust positively, but user comprehensibility and satisfaction negatively. All in all, Welzijn.AI's architecture draws mostly on open models, as precondition for explainable language analysis. Also, we identified various stakeholder perspectives useful for researchers developing AI in health and beyond.","sentences":["We present Welzijn.","AI as new digital solution for monitoring mental well-being in the elderly, as a use case illustrating how recent guidelines on responsible Artificial Intelligence can inform Welzijn.","AI's Technology and Value dimensions.","Here Technology concerns the description of an open, well-documented and interpretable envisioned architecture in light of the system's goals; Value concerns stakeholder evaluations of Welzijn.AI.","Stakeholders included, among others, informal/professional caregivers, a developer, patient and physician federations, and the elderly.","Brief empirical evaluations comprised a SWOT-analysis, co-creation session, and user evaluation of a proof-of-concept implementation of Welzijn.AI.","The SWOT analysis summarises stakeholder evaluations of Welzijn.","AI in terms of its Strengths, Weaknesses, Opportunities and Threats.","The co-creation session ranks technical, environmental and user-related requirements of Welzijn.","AI with the Hundred Dollar Method.","User evaluation comprises (dis)agreement on statements targeting Welzijn.","AI's main characteristics, and a ranking of desired social characteristics.","We found that stakeholders stress different aspects of Welzijn.AI.","For example, medical professionals highlight in the SWOT analysis Welzijn.","AI as the key unlocking an individual's social network, whereas in the co-creation session, more user-related aspects such as demo and practice sessions were emphasised.","Stakeholders aligned on the importance of safe data storage and access.","The elderly evaluated Welzijn.","AI's accessibility and perceived trust positively, but user comprehensibility and satisfaction negatively.","All in all, Welzijn.","AI's architecture draws mostly on open models, as precondition for explainable language analysis.","Also, we identified various stakeholder perspectives useful for researchers developing AI in health and beyond."],"url":"http://arxiv.org/abs/2502.07983v1"}
{"created":"2025-02-11 21:48:10","title":"RESIST: Resilient Decentralized Learning Using Consensus Gradient Descent","abstract":"Empirical risk minimization (ERM) is a cornerstone of modern machine learning (ML), supported by advances in optimization theory that ensure efficient solutions with provable algorithmic convergence rates, which measure the speed at which optimization algorithms approach a solution, and statistical learning rates, which characterize how well the solution generalizes to unseen data. Privacy, memory, computational, and communications constraints increasingly necessitate data collection, processing, and storage across network-connected devices. In many applications, these networks operate in decentralized settings where a central server cannot be assumed, requiring decentralized ML algorithms that are both efficient and resilient. Decentralized learning, however, faces significant challenges, including an increased attack surface for adversarial interference during decentralized learning processes. This paper focuses on the man-in-the-middle (MITM) attack, which can cause models to deviate significantly from their intended ERM solutions. To address this challenge, we propose RESIST (Resilient dEcentralized learning using conSensus gradIent deScenT), an optimization algorithm designed to be robust against adversarially compromised communication links. RESIST achieves algorithmic and statistical convergence for strongly convex, Polyak-Lojasiewicz, and nonconvex ERM problems. Experimental results demonstrate the robustness and scalability of RESIST for real-world decentralized learning in adversarial environments.","sentences":["Empirical risk minimization (ERM) is a cornerstone of modern machine learning (ML), supported by advances in optimization theory that ensure efficient solutions with provable algorithmic convergence rates, which measure the speed at which optimization algorithms approach a solution, and statistical learning rates, which characterize how well the solution generalizes to unseen data.","Privacy, memory, computational, and communications constraints increasingly necessitate data collection, processing, and storage across network-connected devices.","In many applications, these networks operate in decentralized settings where a central server cannot be assumed, requiring decentralized ML algorithms that are both efficient and resilient.","Decentralized learning, however, faces significant challenges, including an increased attack surface for adversarial interference during decentralized learning processes.","This paper focuses on the man-in-the-middle (MITM) attack, which can cause models to deviate significantly from their intended ERM solutions.","To address this challenge, we propose RESIST (Resilient dEcentralized learning using conSensus gradIent deScenT), an optimization algorithm designed to be robust against adversarially compromised communication links.","RESIST achieves algorithmic and statistical convergence for strongly convex, Polyak-Lojasiewicz, and nonconvex ERM problems.","Experimental results demonstrate the robustness and scalability of RESIST for real-world decentralized learning in adversarial environments."],"url":"http://arxiv.org/abs/2502.07977v1"}
{"created":"2025-02-11 21:36:31","title":"Training Sparse Mixture Of Experts Text Embedding Models","abstract":"Transformer-based text embedding models have improved their performance on benchmarks like MIRACL and BEIR by increasing their parameter counts. However, this scaling approach introduces significant deployment challenges, including increased inference latency and memory usage. These challenges are particularly severe in retrieval-augmented generation (RAG) applications, where large models' increased memory requirements constrain dataset ingestion capacity, and their higher latency directly impacts query-time performance. While causal language models have addressed similar efficiency challenges using Mixture of Experts (MoE) architectures, this approach hasn't been successfully adapted to the general text embedding setting. In this paper, we introduce Nomic Embed v2, the first general purpose MoE text embedding model. Our model outperforms models in the same parameter class on both monolingual and multilingual benchmarks while also maintaining competitive performance with models twice its size. We open-source all code, models, and evaluation data to ensure full reproducibility of our training pipeline.","sentences":["Transformer-based text embedding models have improved their performance on benchmarks like MIRACL and BEIR by increasing their parameter counts.","However, this scaling approach introduces significant deployment challenges, including increased inference latency and memory usage.","These challenges are particularly severe in retrieval-augmented generation (RAG) applications, where large models' increased memory requirements constrain dataset ingestion capacity, and their higher latency directly impacts query-time performance.","While causal language models have addressed similar efficiency challenges using Mixture of Experts (MoE) architectures, this approach hasn't been successfully adapted to the general text embedding setting.","In this paper, we introduce Nomic Embed v2, the first general purpose MoE text embedding model.","Our model outperforms models in the same parameter class on both monolingual and multilingual benchmarks while also maintaining competitive performance with models twice its size.","We open-source all code, models, and evaluation data to ensure full reproducibility of our training pipeline."],"url":"http://arxiv.org/abs/2502.07972v1"}
{"created":"2025-02-11 21:24:13","title":"Generative Risk Minimization for Out-of-Distribution Generalization on Graphs","abstract":"Out-of-distribution (OOD) generalization on graphs aims at dealing with scenarios where the test graph distribution differs from the training graph distributions. Compared to i.i.d. data like images, the OOD generalization problem on graph-structured data remains challenging due to the non-i.i.d. property and complex structural information on graphs. Recently, several works on graph OOD generalization have explored extracting invariant subgraphs that share crucial classification information across different distributions. Nevertheless, such a strategy could be suboptimal for entirely capturing the invariant information, as the extraction of discrete structures could potentially lead to the loss of invariant information or the involvement of spurious information. In this paper, we propose an innovative framework, named Generative Risk Minimization (GRM), designed to generate an invariant subgraph for each input graph to be classified, instead of extraction. To address the challenge of optimization in the absence of optimal invariant subgraphs (i.e., ground truths), we derive a tractable form of the proposed GRM objective by introducing a latent causal variable, and its effectiveness is validated by our theoretical analysis. We further conduct extensive experiments across a variety of real-world graph datasets for both node-level and graph-level OOD generalization, and the results demonstrate the superiority of our framework GRM.","sentences":["Out-of-distribution (OOD) generalization on graphs aims at dealing with scenarios where the test graph distribution differs from the training graph distributions.","Compared to i.i.d. data like images, the OOD generalization problem on graph-structured data remains challenging due to the non-i.i.d. property and complex structural information on graphs.","Recently, several works on graph OOD generalization have explored extracting invariant subgraphs that share crucial classification information across different distributions.","Nevertheless, such a strategy could be suboptimal for entirely capturing the invariant information, as the extraction of discrete structures could potentially lead to the loss of invariant information or the involvement of spurious information.","In this paper, we propose an innovative framework, named Generative Risk Minimization (GRM), designed to generate an invariant subgraph for each input graph to be classified, instead of extraction.","To address the challenge of optimization in the absence of optimal invariant subgraphs (i.e., ground truths), we derive a tractable form of the proposed GRM objective by introducing a latent causal variable, and its effectiveness is validated by our theoretical analysis.","We further conduct extensive experiments across a variety of real-world graph datasets for both node-level and graph-level OOD generalization, and the results demonstrate the superiority of our framework GRM."],"url":"http://arxiv.org/abs/2502.07968v1"}
{"created":"2025-02-11 21:11:47","title":"Intrinsic Bias is Predicted by Pretraining Data and Correlates with Downstream Performance in Vision-Language Encoders","abstract":"While recent work has found that vision-language models trained under the Contrastive Language Image Pre-training (CLIP) framework contain intrinsic social biases, the extent to which different upstream pre-training features of the framework relate to these biases, and hence how intrinsic bias and downstream performance are connected has been unclear. In this work, we present the largest comprehensive analysis to-date of how the upstream pre-training factors and downstream performance of CLIP models relate to their intrinsic biases. Studying 131 unique CLIP models, trained on 26 datasets, using 55 architectures, and in a variety of sizes, we evaluate bias in each model using 26 well-established unimodal and cross-modal principled Embedding Association Tests. We find that the choice of pre-training dataset is the most significant upstream predictor of bias, whereas architectural variations have minimal impact. Additionally, datasets curated using sophisticated filtering techniques aimed at enhancing downstream model performance tend to be associated with higher levels of intrinsic bias. Finally, we observe that intrinsic bias is often significantly correlated with downstream performance ($0.3 \\leq r \\leq 0.8$), suggesting that models optimized for performance inadvertently learn to amplify representational biases. Comparisons between unimodal and cross-modal association tests reveal that social group bias depends heavily on the modality. Our findings imply that more sophisticated strategies are needed to address intrinsic model bias for vision-language models across the entire model development pipeline.","sentences":["While recent work has found that vision-language models trained under the Contrastive Language Image Pre-training (CLIP) framework contain intrinsic social biases, the extent to which different upstream pre-training features of the framework relate to these biases, and hence how intrinsic bias and downstream performance are connected has been unclear.","In this work, we present the largest comprehensive analysis to-date of how the upstream pre-training factors and downstream performance of CLIP models relate to their intrinsic biases.","Studying 131 unique CLIP models, trained on 26 datasets, using 55 architectures, and in a variety of sizes, we evaluate bias in each model using 26 well-established unimodal and cross-modal principled Embedding Association Tests.","We find that the choice of pre-training dataset is the most significant upstream predictor of bias, whereas architectural variations have minimal impact.","Additionally, datasets curated using sophisticated filtering techniques aimed at enhancing downstream model performance tend to be associated with higher levels of intrinsic bias.","Finally, we observe that intrinsic bias is often significantly correlated with downstream performance ($0.3 \\leq r \\leq 0.8$), suggesting that models optimized for performance inadvertently learn to amplify representational biases.","Comparisons between unimodal and cross-modal association tests reveal that social group bias depends heavily on the modality.","Our findings imply that more sophisticated strategies are needed to address intrinsic model bias for vision-language models across the entire model development pipeline."],"url":"http://arxiv.org/abs/2502.07957v1"}
{"created":"2025-02-11 21:05:45","title":"Improving VANET Simulation Channel Model in an Urban Environment via Calibration Using Real-World Communication Data","abstract":"Wireless communication channels in Vehicular Ad-hoc NETworks (VANETs) suffer from packet losses, which severely influences the performance of their applications. There are several reasons for this loss, including but not limited to signal interference with itself after being reflected from the ground and other objects, the doppler effect caused by the speed of the vehicle, and buildings and other vehicles blocking the signal. As a result, VANET simulators must be calibrated in order to mimic the behavior of real-world vehicular communication channels effectively. In this paper, we calibrated an OMNET++(Objective Modular Network Testbed in C++)/Veins simulator for VANET's dedicated short-range communications (DSRC) protocol using the field data from the urban testbed in Downtown Chattanooga, TN. Channel propagation models, as well as physical layer parameters, were calibrated using a Genetic Algorithm (GA). The performance of the calibrated simulator was improved significantly in comparison with the default settings in Veins. The final results were compared to the real-world data collected from the testbed and performance shows that the final calibrated channel model performs better than uncalibrated models in simulating the packet delivery pattern of DSRC channels.","sentences":["Wireless communication channels in Vehicular Ad-hoc NETworks (VANETs) suffer from packet losses, which severely influences the performance of their applications.","There are several reasons for this loss, including but not limited to signal interference with itself after being reflected from the ground and other objects, the doppler effect caused by the speed of the vehicle, and buildings and other vehicles blocking the signal.","As a result, VANET simulators must be calibrated in order to mimic the behavior of real-world vehicular communication channels effectively.","In this paper, we calibrated an OMNET++(Objective Modular Network Testbed in C++)/Veins simulator for VANET's dedicated short-range communications (DSRC) protocol using the field data from the urban testbed in Downtown Chattanooga, TN.","Channel propagation models, as well as physical layer parameters, were calibrated using a Genetic Algorithm (GA).","The performance of the calibrated simulator was improved significantly in comparison with the default settings in Veins.","The final results were compared to the real-world data collected from the testbed and performance shows that the final calibrated channel model performs better than uncalibrated models in simulating the packet delivery pattern of DSRC channels."],"url":"http://arxiv.org/abs/2502.07954v1"}
