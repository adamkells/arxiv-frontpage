{"created":"2025-01-28 18:59:44","title":"SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training","abstract":"Supervised fine-tuning (SFT) and reinforcement learning (RL) are widely used post-training techniques for foundation models. However, their roles in enhancing model generalization capabilities remain unclear. This paper studies the difference between SFT and RL on generalization and memorization, focusing on text-based rule variants and visual variants. We introduce GeneralPoints, an arithmetic reasoning card game, and adopt V-IRL, a real-world navigation environment, to assess how models trained with SFT and RL generalize to unseen variants in both textual and visual domains. We show that RL, especially when trained with an outcome-based reward, generalizes across both rule-based textual and visual variants. SFT, in contrast, tends to memorize training data and struggles to generalize out-of-distribution scenarios. Further analysis reveals that RL improves the model's underlying visual recognition capabilities, contributing to its enhanced generalization in the visual domain. Despite RL's superior generalization, we show that SFT remains essential for effective RL training; SFT stabilizes the model's output format, enabling subsequent RL to achieve its performance gains. These findings demonstrates the capability of RL for acquiring generalizable knowledge in complex, multi-modal tasks.","sentences":["Supervised fine-tuning (SFT) and reinforcement learning (RL) are widely used post-training techniques for foundation models.","However, their roles in enhancing model generalization capabilities remain unclear.","This paper studies the difference between SFT and RL on generalization and memorization, focusing on text-based rule variants and visual variants.","We introduce GeneralPoints, an arithmetic reasoning card game, and adopt V-IRL, a real-world navigation environment, to assess how models trained with SFT and RL generalize to unseen variants in both textual and visual domains.","We show that RL, especially when trained with an outcome-based reward, generalizes across both rule-based textual and visual variants.","SFT, in contrast, tends to memorize training data and struggles to generalize out-of-distribution scenarios.","Further analysis reveals that RL improves the model's underlying visual recognition capabilities, contributing to its enhanced generalization in the visual domain.","Despite RL's superior generalization, we show that SFT remains essential for effective RL training; SFT stabilizes the model's output format, enabling subsequent RL to achieve its performance gains.","These findings demonstrates the capability of RL for acquiring generalizable knowledge in complex, multi-modal tasks."],"url":"http://arxiv.org/abs/2501.17161v1"}
{"created":"2025-01-28 18:53:14","title":"Scanning Trojaned Models Using Out-of-Distribution Samples","abstract":"Scanning for trojan (backdoor) in deep neural networks is crucial due to their significant real-world applications. There has been an increasing focus on developing effective general trojan scanning methods across various trojan attacks. Despite advancements, there remains a shortage of methods that perform effectively without preconceived assumptions about the backdoor attack method. Additionally, we have observed that current methods struggle to identify classifiers trojaned using adversarial training. Motivated by these challenges, our study introduces a novel scanning method named TRODO (TROjan scanning by Detection of adversarial shifts in Out-of-distribution samples). TRODO leverages the concept of \"blind spots\"--regions where trojaned classifiers erroneously identify out-of-distribution (OOD) samples as in-distribution (ID). We scan for these blind spots by adversarially shifting OOD samples towards in-distribution. The increased likelihood of perturbed OOD samples being classified as ID serves as a signature for trojan detection. TRODO is both trojan and label mapping agnostic, effective even against adversarially trained trojaned classifiers. It is applicable even in scenarios where training data is absent, demonstrating high accuracy and adaptability across various scenarios and datasets, highlighting its potential as a robust trojan scanning strategy.","sentences":["Scanning for trojan (backdoor) in deep neural networks is crucial due to their significant real-world applications.","There has been an increasing focus on developing effective general trojan scanning methods across various trojan attacks.","Despite advancements, there remains a shortage of methods that perform effectively without preconceived assumptions about the backdoor attack method.","Additionally, we have observed that current methods struggle to identify classifiers trojaned using adversarial training.","Motivated by these challenges, our study introduces a novel scanning method named TRODO (TROjan scanning by Detection of adversarial shifts in Out-of-distribution samples).","TRODO leverages the concept of \"blind spots\"--regions where trojaned classifiers erroneously identify out-of-distribution (OOD) samples as in-distribution (ID).","We scan for these blind spots by adversarially shifting OOD samples towards in-distribution.","The increased likelihood of perturbed OOD samples being classified as ID serves as a signature for trojan detection.","TRODO is both trojan and label mapping agnostic, effective even against adversarially trained trojaned classifiers.","It is applicable even in scenarios where training data is absent, demonstrating high accuracy and adaptability across various scenarios and datasets, highlighting its potential as a robust trojan scanning strategy."],"url":"http://arxiv.org/abs/2501.17151v1"}
{"created":"2025-01-28 18:45:07","title":"FactCG: Enhancing Fact Checkers with Graph-Based Multi-Hop Data","abstract":"Prior research on training grounded factuality classification models to detect hallucinations in large language models (LLMs) has relied on public natural language inference (NLI) data and synthetic data. However, conventional NLI datasets are not well-suited for document-level reasoning, which is critical for detecting LLM hallucinations. Recent approaches to document-level synthetic data generation involve iteratively removing sentences from documents and annotating factuality using LLM-based prompts. While effective, this method is computationally expensive for long documents and limited by the LLM's capabilities. In this work, we analyze the differences between existing synthetic training data used in state-of-the-art models and real LLM output claims. Based on our findings, we propose a novel approach for synthetic data generation, CG2C, that leverages multi-hop reasoning on context graphs extracted from documents. Our fact checker model, FactCG, demonstrates improved performance with more connected reasoning, using the same backbone models. Experiments show it even outperforms GPT-4-o on the LLM-Aggrefact benchmark with much smaller model size.","sentences":["Prior research on training grounded factuality classification models to detect hallucinations in large language models (LLMs) has relied on public natural language inference (NLI) data and synthetic data.","However, conventional NLI datasets are not well-suited for document-level reasoning, which is critical for detecting LLM hallucinations.","Recent approaches to document-level synthetic data generation involve iteratively removing sentences from documents and annotating factuality using LLM-based prompts.","While effective, this method is computationally expensive for long documents and limited by the LLM's capabilities.","In this work, we analyze the differences between existing synthetic training data used in state-of-the-art models and real LLM output claims.","Based on our findings, we propose a novel approach for synthetic data generation, CG2C, that leverages multi-hop reasoning on context graphs extracted from documents.","Our fact checker model, FactCG, demonstrates improved performance with more connected reasoning, using the same backbone models.","Experiments show it even outperforms GPT-4-o on the LLM-Aggrefact benchmark with much smaller model size."],"url":"http://arxiv.org/abs/2501.17144v1"}
{"created":"2025-01-28 18:23:12","title":"Scenario Understanding of Traffic Scenes Through Large Visual Language Models","abstract":"Deep learning models for autonomous driving, encompassing perception, planning, and control, depend on vast datasets to achieve their high performance. However, their generalization often suffers due to domain-specific data distributions, making an effective scene-based categorization of samples necessary to improve their reliability across diverse domains. Manual captioning, though valuable, is both labor-intensive and time-consuming, creating a bottleneck in the data annotation process. Large Visual Language Models (LVLMs) present a compelling solution by automating image analysis and categorization through contextual queries, often without requiring retraining for new categories. In this study, we evaluate the capabilities of LVLMs, including GPT-4 and LLaVA, to understand and classify urban traffic scenes on both an in-house dataset and the BDD100K. We propose a scalable captioning pipeline that integrates state-of-the-art models, enabling a flexible deployment on new datasets. Our analysis, combining quantitative metrics with qualitative insights, demonstrates the effectiveness of LVLMs to understand urban traffic scenarios and highlights their potential as an efficient tool for data-driven advancements in autonomous driving.","sentences":["Deep learning models for autonomous driving, encompassing perception, planning, and control, depend on vast datasets to achieve their high performance.","However, their generalization often suffers due to domain-specific data distributions, making an effective scene-based categorization of samples necessary to improve their reliability across diverse domains.","Manual captioning, though valuable, is both labor-intensive and time-consuming, creating a bottleneck in the data annotation process.","Large Visual Language Models (LVLMs) present a compelling solution by automating image analysis and categorization through contextual queries, often without requiring retraining for new categories.","In this study, we evaluate the capabilities of LVLMs, including GPT-4 and LLaVA, to understand and classify urban traffic scenes on both an in-house dataset and the BDD100K. We propose a scalable captioning pipeline that integrates state-of-the-art models, enabling a flexible deployment on new datasets.","Our analysis, combining quantitative metrics with qualitative insights, demonstrates the effectiveness of LVLMs to understand urban traffic scenarios and highlights their potential as an efficient tool for data-driven advancements in autonomous driving."],"url":"http://arxiv.org/abs/2501.17131v1"}
{"created":"2025-01-28 18:16:36","title":"Enhancements to P4TG: Performance, Protocols, and Automation","abstract":"The P4-based traffic generator (P4TG) is a hardware-based traffic generator (TG) running on the Intel Tofino 1 ASIC. The TG can generate up to 1 Tb/s of traffic and directly measures rates, packet loss, and other metrics in the data plane. Many researchers and industrial partners have used it since its publication in 2023 and new features have been requested to be incorporated into P4TG. In this work, we provide an overview of the recently added features of P4TG. These enhancements include new traffic generation capabilities including IPv6 and segment routing v6 (SRv6) support and various encapsulation protocols such as VLAN, QinQ, VxLAN, and MPLS. Further, P4TG is ported to the Intel Tofino 2 platform enabling a generation capability of up to 4 Tb/s. The enhancements to P4TG also provide an improved user experience facilitating automated testing based on RFC 2544, report generation, and visualization.","sentences":["The P4-based traffic generator (P4TG) is a hardware-based traffic generator (TG) running on the Intel Tofino 1 ASIC.","The TG can generate up to 1 Tb/s of traffic and directly measures rates, packet loss, and other metrics in the data plane.","Many researchers and industrial partners have used it since its publication in 2023 and new features have been requested to be incorporated into P4TG.","In this work, we provide an overview of the recently added features of P4TG.","These enhancements include new traffic generation capabilities including IPv6 and segment routing v6 (SRv6) support and various encapsulation protocols such as VLAN, QinQ, VxLAN, and MPLS.","Further, P4TG is ported to the Intel Tofino 2 platform enabling a generation capability of up to 4 Tb/s. The enhancements to P4TG also provide an improved user experience facilitating automated testing based on RFC 2544, report generation, and visualization."],"url":"http://arxiv.org/abs/2501.17127v1"}
{"created":"2025-01-28 18:16:05","title":"ECLYPSE: a Python Framework for Simulation and Emulation of the Cloud-Edge Continuum","abstract":"The Cloud-Edge continuum enhances application performance by bringing computation closer to data sources. However, it presents considerable challenges in managing resources and determining service placement, as these tasks require navigating diverse, dynamic environments characterised by fluctuating network conditions. Addressing these challenges calls for tools combining simulation and emulation of Cloud-Edge systems to rigorously assess novel application and resource management strategies. In this paper, we introduce ECLYPSE, a Python-based framework that enables the simulation and emulation of the Cloud-Edge continuum via adaptable resource allocation and service placement models. ECLYPSE features an event-driven architecture for dynamically adapting network configurations and resources. It also supports seamless transitions between simulated and emulated setups. In this work, ECLYPSE capabilities are illustrated over three use cases, showing how the framework supports rapid prototyping across diverse experimental settings.","sentences":["The Cloud-Edge continuum enhances application performance by bringing computation closer to data sources.","However, it presents considerable challenges in managing resources and determining service placement, as these tasks require navigating diverse, dynamic environments characterised by fluctuating network conditions.","Addressing these challenges calls for tools combining simulation and emulation of Cloud-Edge systems to rigorously assess novel application and resource management strategies.","In this paper, we introduce ECLYPSE, a Python-based framework that enables the simulation and emulation of the Cloud-Edge continuum via adaptable resource allocation and service placement models.","ECLYPSE features an event-driven architecture for dynamically adapting network configurations and resources.","It also supports seamless transitions between simulated and emulated setups.","In this work, ECLYPSE capabilities are illustrated over three use cases, showing how the framework supports rapid prototyping across diverse experimental settings."],"url":"http://arxiv.org/abs/2501.17126v1"}
{"created":"2025-01-28 18:14:43","title":"Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks Detection: A Comparative Analysis","abstract":"Cache side channel attacks are a sophisticated and persistent threat that exploit vulnerabilities in modern processors to extract sensitive information. These attacks leverage weaknesses in shared computational resources, particularly the last level cache, to infer patterns in data access and execution flows, often bypassing traditional security defenses. Such attacks are especially dangerous as they can be executed remotely without requiring physical access to the victim's device. This study focuses on a specific class of these threats: fingerprinting attacks, where an adversary monitors and analyzes the behavior of co-located processes via cache side channels. This can potentially reveal confidential information, such as encryption keys or user activity patterns. A comprehensive threat model illustrates how attackers sharing computational resources with target systems exploit these side channels to compromise sensitive data. To mitigate such risks, a hybrid deep learning model is proposed for detecting cache side channel attacks. Its performance is compared with five widely used deep learning models: Multi-Layer Perceptron, Convolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term Memory, and Gated Recurrent Unit. The experimental results demonstrate that the hybrid model achieves a detection rate of up to 99.96%. These findings highlight the limitations of existing models, the need for enhanced defensive mechanisms, and directions for future research to secure sensitive data against evolving side channel threats.","sentences":["Cache side channel attacks are a sophisticated and persistent threat that exploit vulnerabilities in modern processors to extract sensitive information.","These attacks leverage weaknesses in shared computational resources, particularly the last level cache, to infer patterns in data access and execution flows, often bypassing traditional security defenses.","Such attacks are especially dangerous as they can be executed remotely without requiring physical access to the victim's device.","This study focuses on a specific class of these threats: fingerprinting attacks, where an adversary monitors and analyzes the behavior of co-located processes via cache side channels.","This can potentially reveal confidential information, such as encryption keys or user activity patterns.","A comprehensive threat model illustrates how attackers sharing computational resources with target systems exploit these side channels to compromise sensitive data.","To mitigate such risks, a hybrid deep learning model is proposed for detecting cache side channel attacks.","Its performance is compared with five widely used deep learning models: Multi-Layer Perceptron, Convolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term Memory, and Gated Recurrent Unit.","The experimental results demonstrate that the hybrid model achieves a detection rate of up to 99.96%.","These findings highlight the limitations of existing models, the need for enhanced defensive mechanisms, and directions for future research to secure sensitive data against evolving side channel threats."],"url":"http://arxiv.org/abs/2501.17123v1"}
{"created":"2025-01-28 18:07:30","title":"Histoires Morales: A French Dataset for Assessing Moral Alignment","abstract":"Aligning language models with human values is crucial, especially as they become more integrated into everyday life. While models are often adapted to user preferences, it is equally important to ensure they align with moral norms and behaviours in real-world social situations. Despite significant progress in languages like English and Chinese, French has seen little attention in this area, leaving a gap in understanding how LLMs handle moral reasoning in this language. To address this gap, we introduce Histoires Morales, a French dataset derived from Moral Stories, created through translation and subsequently refined with the assistance of native speakers to guarantee grammatical accuracy and adaptation to the French cultural context. We also rely on annotations of the moral values within the dataset to ensure their alignment with French norms. Histoires Morales covers a wide range of social situations, including differences in tipping practices, expressions of honesty in relationships, and responsibilities toward animals. To foster future research, we also conduct preliminary experiments on the alignment of multilingual models on French and English data and the robustness of the alignment. We find that while LLMs are generally aligned with human moral norms by default, they can be easily influenced with user-preference optimization for both moral and immoral data.","sentences":["Aligning language models with human values is crucial, especially as they become more integrated into everyday life.","While models are often adapted to user preferences, it is equally important to ensure they align with moral norms and behaviours in real-world social situations.","Despite significant progress in languages like English and Chinese, French has seen little attention in this area, leaving a gap in understanding how LLMs handle moral reasoning in this language.","To address this gap, we introduce Histoires Morales, a French dataset derived from Moral Stories, created through translation and subsequently refined with the assistance of native speakers to guarantee grammatical accuracy and adaptation to the French cultural context.","We also rely on annotations of the moral values within the dataset to ensure their alignment with French norms.","Histoires Morales covers a wide range of social situations, including differences in tipping practices, expressions of honesty in relationships, and responsibilities toward animals.","To foster future research, we also conduct preliminary experiments on the alignment of multilingual models on French and English data and the robustness of the alignment.","We find that while LLMs are generally aligned with human moral norms by default, they can be easily influenced with user-preference optimization for both moral and immoral data."],"url":"http://arxiv.org/abs/2501.17117v1"}
{"created":"2025-01-28 17:44:04","title":"COS(M+O)S: Curiosity and RL-Enhanced MCTS for Exploring Story Space via Language Models","abstract":"We present COS(M+O)S, a System 2-inspired framework for open-ended plot development that systematically explores the vast space of possible story expansions, enabling a 3B-parameter language model to approach the plot quality of a 70B model on select short-story tasks. The method accomplishes this by combining Monte Carlo Tree Search (MCTS), guided by a step-level value model that rewards moderate surprisal (curiosity) while penalizing incoherence, and Odds Ratio Preference Optimization (ORPO) to fine-tune the policy on high-value plot expansions. This iterative reinforcement learning loop systematically explores multiple candidate plot branches, backpropagates quality signals, and adapts the policy for faster convergence, notably shifting the policy from puzzle-based Chain-of-Thought to more character-driven storytelling. In small-scale tests with short-story prompts, 67%-77% of participants favored COS(M+O)S's highest-rated expansions over lower-rated ones, suggesting that our learned value function aligns. GPT-4o ratings further show that COS(M+O)S surpasses naive single-pass decoding from Llama 3.2 3B by 0.59 SD, coming within 0.06 SD of Llama 3.1 70B (no significant difference, p=0.93). Pairwise comparisons with o1 place COS(M+O)S 1.5 SD above the 3B baseline and find no statistically significant gap from 70B. Nevertheless, absolute story quality remains modest, constrained by the small model's capacity and limited training data.","sentences":["We present COS(M+O)S, a System 2-inspired framework for open-ended plot development that systematically explores the vast space of possible story expansions, enabling a 3B-parameter language model to approach the plot quality of a 70B model on select short-story tasks.","The method accomplishes this by combining Monte Carlo Tree Search (MCTS), guided by a step-level value model that rewards moderate surprisal (curiosity) while penalizing incoherence, and Odds Ratio Preference Optimization (ORPO) to fine-tune the policy on high-value plot expansions.","This iterative reinforcement learning loop systematically explores multiple candidate plot branches, backpropagates quality signals, and adapts the policy for faster convergence, notably shifting the policy from puzzle-based Chain-of-Thought to more character-driven storytelling.","In small-scale tests with short-story prompts, 67%-77% of participants favored COS(M+O)S's highest-rated expansions over lower-rated ones, suggesting that our learned value function aligns.","GPT-4o ratings further show that COS(M+O)S surpasses naive single-pass decoding from Llama 3.2 3B by 0.59 SD, coming within 0.06 SD of Llama 3.1 70B (no significant difference, p=0.93).","Pairwise comparisons with o1 place COS(M+O)S 1.5 SD above the 3B baseline and find no statistically significant gap from 70B.","Nevertheless, absolute story quality remains modest, constrained by the small model's capacity and limited training data."],"url":"http://arxiv.org/abs/2501.17104v1"}
{"created":"2025-01-28 17:23:45","title":"CRSet: Non-Interactive Verifiable Credential Revocation with Metadata Privacy for Issuers and Everyone Else","abstract":"Like any digital certificate, Verifiable Credentials (VCs) require a way to revoke them in case of an error or key compromise. Existing solutions for VC revocation, most prominently Bitstring Status List, are not viable for many use cases since they leak the issuer's behavior, which in turn leaks internal business metrics. For instance, exact staff fluctuation through issuance and revocation of employee IDs. We introduce CRSet, a revocation mechanism that allows an issuer to encode revocation information for years worth of VCs as a Bloom filter cascade. Padding is used to provide deniability for issuer metrics. Issuers periodically publish this filter cascade on a decentralized storage system. Relying Parties (RPs) can download it to perform any number of revocation checks locally. Compared to existing solutions, CRSet protects the metadata of subject, RPs, and issuer equally. At the same time, it is non-interactive, making it work with wallet devices having limited hardware power and drop-in compatible with existing VC exchange protocols and wallet applications. We present a prototype using the Ethereum blockchain as decentralized storage. The recently introduced blob-carrying transactions, enabling cheaper data writes, allow us to write each CRSet directly to the chain. We built software for issuers and RPs that we successfully tested end-to-end with an existing publicly available wallet agents and the OpenID for Verifiable Credentials protocols. Storage and bandwidth costs paid by issuers and RP are higher than for Bitstring Status List, but still manageable at around 1 MB for an issuer issuing hundreds of thousands of VCs annually and covering decades.","sentences":["Like any digital certificate, Verifiable Credentials (VCs) require a way to revoke them in case of an error or key compromise.","Existing solutions for VC revocation, most prominently Bitstring Status List, are not viable for many use cases since they leak the issuer's behavior, which in turn leaks internal business metrics.","For instance, exact staff fluctuation through issuance and revocation of employee IDs.","We introduce CRSet, a revocation mechanism that allows an issuer to encode revocation information for years worth of VCs as a Bloom filter cascade.","Padding is used to provide deniability for issuer metrics.","Issuers periodically publish this filter cascade on a decentralized storage system.","Relying Parties (RPs) can download it to perform any number of revocation checks locally.","Compared to existing solutions, CRSet protects the metadata of subject, RPs, and issuer equally.","At the same time, it is non-interactive, making it work with wallet devices having limited hardware power and drop-in compatible with existing VC exchange protocols and wallet applications.","We present a prototype using the Ethereum blockchain as decentralized storage.","The recently introduced blob-carrying transactions, enabling cheaper data writes, allow us to write each CRSet directly to the chain.","We built software for issuers and RPs that we successfully tested end-to-end with an existing publicly available wallet agents and the OpenID for Verifiable Credentials protocols.","Storage and bandwidth costs paid by issuers and RP are higher than for Bitstring Status List, but still manageable at around 1 MB for an issuer issuing hundreds of thousands of VCs annually and covering decades."],"url":"http://arxiv.org/abs/2501.17089v1"}
{"created":"2025-01-28 17:01:42","title":"DINOSTAR: Deep Iterative Neural Object Detector Self-Supervised Training for Roadside LiDAR Applications","abstract":"Recent advancements in deep-learning methods for object detection in point-cloud data have enabled numerous roadside applications, fostering improvements in transportation safety and management. However, the intricate nature of point-cloud data poses significant challenges for human-supervised labeling, resulting in substantial expenditures of time and capital. This paper addresses the issue by developing an end-to-end, scalable, and self-supervised framework for training deep object detectors tailored for roadside point-cloud data. The proposed framework leverages self-supervised, statistically modeled teachers to train off-the-shelf deep object detectors, thus circumventing the need for human supervision. The teacher models follow fine-tuned set standard practices of background filtering, object clustering, bounding-box fitting, and classification to generate noisy labels. It is presented that by training the student model over the combined noisy annotations from multitude of teachers enhances its capacity to discern background/foreground more effectively and forces it to learn diverse point-cloud-representations for object categories of interest. The evaluations, involving publicly available roadside datasets and state-of-art deep object detectors, demonstrate that the proposed framework achieves comparable performance to deep object detectors trained on human-annotated labels, despite not utilizing such human-annotations in its training process.","sentences":["Recent advancements in deep-learning methods for object detection in point-cloud data have enabled numerous roadside applications, fostering improvements in transportation safety and management.","However, the intricate nature of point-cloud data poses significant challenges for human-supervised labeling, resulting in substantial expenditures of time and capital.","This paper addresses the issue by developing an end-to-end, scalable, and self-supervised framework for training deep object detectors tailored for roadside point-cloud data.","The proposed framework leverages self-supervised, statistically modeled teachers to train off-the-shelf deep object detectors, thus circumventing the need for human supervision.","The teacher models follow fine-tuned set standard practices of background filtering, object clustering, bounding-box fitting, and classification to generate noisy labels.","It is presented that by training the student model over the combined noisy annotations from multitude of teachers enhances its capacity to discern background/foreground more effectively and forces it to learn diverse point-cloud-representations for object categories of interest.","The evaluations, involving publicly available roadside datasets and state-of-art deep object detectors, demonstrate that the proposed framework achieves comparable performance to deep object detectors trained on human-annotated labels, despite not utilizing such human-annotations in its training process."],"url":"http://arxiv.org/abs/2501.17076v1"}
{"created":"2025-01-28 16:58:45","title":"DataLens: ML-Oriented Interactive Tabular Data Quality Dashboard","abstract":"Maintaining high data quality is crucial for reliable data analysis and machine learning (ML). However, existing data quality management tools often lack automation, interactivity, and integration with ML workflows. This demonstration paper introduces DataLens, a novel interactive dashboard designed to streamline and automate the data quality management process for tabular data. DataLens integrates a suite of data profiling, error detection, and repair tools, including statistical, rule-based, and ML-based methods. It features a user-in-the-loop module for interactive rule validation, data labeling, and custom rule definition, enabling domain experts to guide the cleaning process. Furthermore, DataLens implements an iterative cleaning module that automatically selects optimal cleaning tools based on downstream ML model performance. To ensure reproducibility, DataLens generates DataSheets capturing essential metadata and integrates with MLflow and Delta Lake for experiment tracking and data version control. This demonstration showcases DataLens's capabilities in effectively identifying and correcting data errors, improving data quality for downstream tasks, and promoting reproducibility in data cleaning pipelines.","sentences":["Maintaining high data quality is crucial for reliable data analysis and machine learning (ML).","However, existing data quality management tools often lack automation, interactivity, and integration with ML workflows.","This demonstration paper introduces DataLens, a novel interactive dashboard designed to streamline and automate the data quality management process for tabular data.","DataLens integrates a suite of data profiling, error detection, and repair tools, including statistical, rule-based, and ML-based methods.","It features a user-in-the-loop module for interactive rule validation, data labeling, and custom rule definition, enabling domain experts to guide the cleaning process.","Furthermore, DataLens implements an iterative cleaning module that automatically selects optimal cleaning tools based on downstream ML model performance.","To ensure reproducibility, DataLens generates DataSheets capturing essential metadata and integrates with MLflow and Delta Lake for experiment tracking and data version control.","This demonstration showcases DataLens's capabilities in effectively identifying and correcting data errors, improving data quality for downstream tasks, and promoting reproducibility in data cleaning pipelines."],"url":"http://arxiv.org/abs/2501.17074v1"}
{"created":"2025-01-28 15:59:01","title":"Standardised schema and taxonomy for AI incident databases in critical digital infrastructure","abstract":"The rapid deployment of Artificial Intelligence (AI) in critical digital infrastructure introduces significant risks, necessitating a robust framework for systematically collecting AI incident data to prevent future incidents. Existing databases lack the granularity as well as the standardized structure required for consistent data collection and analysis, impeding effective incident management. This work proposes a standardized schema and taxonomy for AI incident databases, addressing these challenges by enabling detailed and structured documentation of AI incidents across sectors. Key contributions include developing a unified schema, introducing new fields such as incident severity, causes, and harms caused, and proposing a taxonomy for classifying AI incidents in critical digital infrastructure. The proposed solution facilitates more effective incident data collection and analysis, thus supporting evidence-based policymaking, enhancing industry safety measures, and promoting transparency. This work lays the foundation for a coordinated global response to AI incidents, ensuring trust, safety, and accountability in using AI across regions.","sentences":["The rapid deployment of Artificial Intelligence (AI) in critical digital infrastructure introduces significant risks, necessitating a robust framework for systematically collecting AI incident data to prevent future incidents.","Existing databases lack the granularity as well as the standardized structure required for consistent data collection and analysis, impeding effective incident management.","This work proposes a standardized schema and taxonomy for AI incident databases, addressing these challenges by enabling detailed and structured documentation of AI incidents across sectors.","Key contributions include developing a unified schema, introducing new fields such as incident severity, causes, and harms caused, and proposing a taxonomy for classifying AI incidents in critical digital infrastructure.","The proposed solution facilitates more effective incident data collection and analysis, thus supporting evidence-based policymaking, enhancing industry safety measures, and promoting transparency.","This work lays the foundation for a coordinated global response to AI incidents, ensuring trust, safety, and accountability in using AI across regions."],"url":"http://arxiv.org/abs/2501.17037v1"}
{"created":"2025-01-28 15:49:51","title":"Approach Towards Semi-Automated Certification for Low Criticality ML-Enabled Airborne Applications","abstract":"As Machine Learning (ML) makes its way into aviation, ML enabled systems including low criticality systems require a reliable certification process to ensure safety and performance. Traditional standards, like DO 178C, which are used for critical software in aviation, do not fully cover the unique aspects of ML. This paper proposes a semi automated certification approach, specifically for low criticality ML systems, focusing on data and model validation, resilience assessment, and usability assurance while integrating manual and automated processes. Key aspects include structured classification to guide certification rigor on system attributes, an Assurance Profile that consolidates evaluation outcomes into a confidence measure the ML component, and methodologies for integrating human oversight into certification activities. Through a case study with a YOLOv8 based object detection system designed to classify military and civilian vehicles in real time for reconnaissance and surveillance aircraft, we show how this approach supports the certification of ML systems in low criticality airborne applications.","sentences":["As Machine Learning (ML) makes its way into aviation, ML enabled systems including low criticality systems require a reliable certification process to ensure safety and performance.","Traditional standards, like DO 178C, which are used for critical software in aviation, do not fully cover the unique aspects of ML.","This paper proposes a semi automated certification approach, specifically for low criticality ML systems, focusing on data and model validation, resilience assessment, and usability assurance while integrating manual and automated processes.","Key aspects include structured classification to guide certification rigor on system attributes, an Assurance Profile that consolidates evaluation outcomes into a confidence measure the ML component, and methodologies for integrating human oversight into certification activities.","Through a case study with a YOLOv8 based object detection system designed to classify military and civilian vehicles in real time for reconnaissance and surveillance aircraft, we show how this approach supports the certification of ML systems in low criticality airborne applications."],"url":"http://arxiv.org/abs/2501.17028v1"}
{"created":"2025-01-28 15:39:07","title":"Mobile Manipulation Instruction Generation from Multiple Images with Automatic Metric Enhancement","abstract":"We consider the problem of generating free-form mobile manipulation instructions based on a target object image and receptacle image. Conventional image captioning models are not able to generate appropriate instructions because their architectures are typically optimized for single-image. In this study, we propose a model that handles both the target object and receptacle to generate free-form instruction sentences for mobile manipulation tasks. Moreover, we introduce a novel training method that effectively incorporates the scores from both learning-based and n-gram based automatic evaluation metrics as rewards. This method enables the model to learn the co-occurrence relationships between words and appropriate paraphrases. Results demonstrate that our proposed method outperforms baseline methods including representative multimodal large language models on standard automatic evaluation metrics. Moreover, physical experiments reveal that using our method to augment data on language instructions improves the performance of an existing multimodal language understanding model for mobile manipulation.","sentences":["We consider the problem of generating free-form mobile manipulation instructions based on a target object image and receptacle image.","Conventional image captioning models are not able to generate appropriate instructions because their architectures are typically optimized for single-image.","In this study, we propose a model that handles both the target object and receptacle to generate free-form instruction sentences for mobile manipulation tasks.","Moreover, we introduce a novel training method that effectively incorporates the scores from both learning-based and n-gram based automatic evaluation metrics as rewards.","This method enables the model to learn the co-occurrence relationships between words and appropriate paraphrases.","Results demonstrate that our proposed method outperforms baseline methods including representative multimodal large language models on standard automatic evaluation metrics.","Moreover, physical experiments reveal that using our method to augment data on language instructions improves the performance of an existing multimodal language understanding model for mobile manipulation."],"url":"http://arxiv.org/abs/2501.17022v1"}
{"created":"2025-01-28 15:32:41","title":"Six-Degree-of-Freedom Motion Emulation for Data-Driven Modeling of Underwater Vehicles","abstract":"This article presents a collaborative research effort aimed at developing a novel six-degree-of-freedom (6-DOF) motion platform for the empirical characterization of hydrodynamic forces crucial for the control and stability of surface and subsurface vehicles. Traditional experimental methods, such as the Planar Motion Mechanism (PMM), are limited by the number of simultaneously articulated DOFs and are limited to single-frequency testing, making such systems impractical for resolving frequency-dependent added mass or damping matrices. The 6 DOF platform, termed a hexapod, overcomes these limitations by offering enhanced maneuverability and the ability to test broad-banded frequency spectra in multiple degrees of freedom in a single experiment.","sentences":["This article presents a collaborative research effort aimed at developing a novel six-degree-of-freedom (6-DOF) motion platform for the empirical characterization of hydrodynamic forces crucial for the control and stability of surface and subsurface vehicles.","Traditional experimental methods, such as the Planar Motion Mechanism (PMM), are limited by the number of simultaneously articulated DOFs and are limited to single-frequency testing, making such systems impractical for resolving frequency-dependent added mass or damping matrices.","The 6 DOF platform, termed a hexapod, overcomes these limitations by offering enhanced maneuverability and the ability to test broad-banded frequency spectra in multiple degrees of freedom in a single experiment."],"url":"http://arxiv.org/abs/2501.17018v1"}
{"created":"2025-01-28 15:26:25","title":"Revisit Mixture Models for Multi-Agent Simulation: Experimental Study within a Unified Framework","abstract":"Simulation plays a crucial role in assessing autonomous driving systems, where the generation of realistic multi-agent behaviors is a key aspect. In multi-agent simulation, the primary challenges include behavioral multimodality and closed-loop distributional shifts. In this study, we revisit mixture models for generating multimodal agent behaviors, which can cover the mainstream methods including continuous mixture models and GPT-like discrete models. Furthermore, we introduce a closed-loop sample generation approach tailored for mixture models to mitigate distributional shifts. Within the unified mixture model~(UniMM) framework, we recognize critical configurations from both model and data perspectives. We conduct a systematic examination of various model configurations, including positive component matching, continuous regression, prediction horizon, and the number of components. Moreover, our investigation into the data configuration highlights the pivotal role of closed-loop samples in achieving realistic simulations. To extend the benefits of closed-loop samples across a broader range of mixture models, we further address the shortcut learning and off-policy learning issues. Leveraging insights from our exploration, the distinct variants proposed within the UniMM framework, including discrete, anchor-free, and anchor-based models, all achieve state-of-the-art performance on the WOSAC benchmark.","sentences":["Simulation plays a crucial role in assessing autonomous driving systems, where the generation of realistic multi-agent behaviors is a key aspect.","In multi-agent simulation, the primary challenges include behavioral multimodality and closed-loop distributional shifts.","In this study, we revisit mixture models for generating multimodal agent behaviors, which can cover the mainstream methods including continuous mixture models and GPT-like discrete models.","Furthermore, we introduce a closed-loop sample generation approach tailored for mixture models to mitigate distributional shifts.","Within the unified mixture model~(UniMM) framework, we recognize critical configurations from both model and data perspectives.","We conduct a systematic examination of various model configurations, including positive component matching, continuous regression, prediction horizon, and the number of components.","Moreover, our investigation into the data configuration highlights the pivotal role of closed-loop samples in achieving realistic simulations.","To extend the benefits of closed-loop samples across a broader range of mixture models, we further address the shortcut learning and off-policy learning issues.","Leveraging insights from our exploration, the distinct variants proposed within the UniMM framework, including discrete, anchor-free, and anchor-based models, all achieve state-of-the-art performance on the WOSAC benchmark."],"url":"http://arxiv.org/abs/2501.17015v1"}
{"created":"2025-01-28 15:25:48","title":"How an AI-ready National Data Library would help UK science","abstract":"In this paper, we provide a technical vision for key enabling elements for the architecture of the UK National Data Library (NDL) with a strong focus on building it as an AI-ready data infrastructure through standardised vocabularies, automated analysis tools, and interoperability services. We follow the ODI Multilayer Interoperability Framework (MIF) for data stewardship, covering central socio-technical aspects for the NDL including user-centric approaches to service design and governance.","sentences":["In this paper, we provide a technical vision for key enabling elements for the architecture of the UK National Data Library (NDL) with a strong focus on building it as an AI-ready data infrastructure through standardised vocabularies, automated analysis tools, and interoperability services.","We follow the ODI Multilayer Interoperability Framework (MIF) for data stewardship, covering central socio-technical aspects for the NDL including user-centric approaches to service design and governance."],"url":"http://arxiv.org/abs/2501.17013v1"}
{"created":"2025-01-28 14:46:38","title":"FedEFM: Federated Endovascular Foundation Model with Unseen Data","abstract":"In endovascular surgery, the precise identification of catheters and guidewires in X-ray images is essential for reducing intervention risks. However, accurately segmenting catheter and guidewire structures is challenging due to the limited availability of labeled data. Foundation models offer a promising solution by enabling the collection of similar domain data to train models whose weights can be fine-tuned for downstream tasks. Nonetheless, large-scale data collection for training is constrained by the necessity of maintaining patient privacy. This paper proposes a new method to train a foundation model in a decentralized federated learning setting for endovascular intervention. To ensure the feasibility of the training, we tackle the unseen data issue using differentiable Earth Mover's Distance within a knowledge distillation framework. Once trained, our foundation model's weights provide valuable initialization for downstream tasks, thereby enhancing task-specific performance. Intensive experiments show that our approach achieves new state-of-the-art results, contributing to advancements in endovascular intervention and robotic-assisted endovascular surgery, while addressing the critical issue of data sharing in the medical domain.","sentences":["In endovascular surgery, the precise identification of catheters and guidewires in X-ray images is essential for reducing intervention risks.","However, accurately segmenting catheter and guidewire structures is challenging due to the limited availability of labeled data.","Foundation models offer a promising solution by enabling the collection of similar domain data to train models whose weights can be fine-tuned for downstream tasks.","Nonetheless, large-scale data collection for training is constrained by the necessity of maintaining patient privacy.","This paper proposes a new method to train a foundation model in a decentralized federated learning setting for endovascular intervention.","To ensure the feasibility of the training, we tackle the unseen data issue using differentiable Earth Mover's Distance within a knowledge distillation framework.","Once trained, our foundation model's weights provide valuable initialization for downstream tasks, thereby enhancing task-specific performance.","Intensive experiments show that our approach achieves new state-of-the-art results, contributing to advancements in endovascular intervention and robotic-assisted endovascular surgery, while addressing the critical issue of data sharing in the medical domain."],"url":"http://arxiv.org/abs/2501.16992v1"}
{"created":"2025-01-28 14:28:55","title":"Modulating CNN Features with Pre-Trained ViT Representations for Open-Vocabulary Object Detection","abstract":"Owing to large-scale image-text contrastive training, pre-trained vision language model (VLM) like CLIP shows superior open-vocabulary recognition ability. Most existing open-vocabulary object detectors attempt to utilize the pre-trained VLM to attain generative representation. F-ViT uses the pre-trained visual encoder as the backbone network and freezes it during training. However, the frozen backbone doesn't benefit from the labeled data to strengthen the representation. Therefore, we propose a novel two-branch backbone network design, named as ViT-Feature-Modulated Multi-Scale Convolutional network (VMCNet). VMCNet consists of a trainable convolutional branch, a frozen pre-trained ViT branch and a feature modulation module. The trainable CNN branch could be optimized with labeled data while the frozen pre-trained ViT branch could keep the representation ability derived from large-scale pre-training. Then, the proposed feature modulation module could modulate the multi-scale CNN features with the representations from ViT branch. With the proposed mixed structure, detector is more likely to discover novel categories. Evaluated on two popular benchmarks, our method boosts the detection performance on novel category and outperforms the baseline. On OV-COCO, the proposed method achieves 44.3 AP$_{50}^{\\mathrm{novel}}$ with ViT-B/16 and 48.5 AP$_{50}^{\\mathrm{novel}}$ with ViT-L/14. On OV-LVIS, VMCNet with ViT-B/16 and ViT-L/14 reaches 27.8 and 38.4 mAP$_{r}$.","sentences":["Owing to large-scale image-text contrastive training, pre-trained vision language model (VLM) like CLIP shows superior open-vocabulary recognition ability.","Most existing open-vocabulary object detectors attempt to utilize the pre-trained VLM to attain generative representation.","F-ViT uses the pre-trained visual encoder as the backbone network and freezes it during training.","However, the frozen backbone doesn't benefit from the labeled data to strengthen the representation.","Therefore, we propose a novel two-branch backbone network design, named as ViT-Feature-Modulated Multi-Scale Convolutional network (VMCNet).","VMCNet consists of a trainable convolutional branch, a frozen pre-trained ViT branch and a feature modulation module.","The trainable CNN branch could be optimized with labeled data while the frozen pre-trained ViT branch could keep the representation ability derived from large-scale pre-training.","Then, the proposed feature modulation module could modulate the multi-scale CNN features with the representations from ViT branch.","With the proposed mixed structure, detector is more likely to discover novel categories.","Evaluated on two popular benchmarks, our method boosts the detection performance on novel category and outperforms the baseline.","On OV-COCO, the proposed method achieves 44.3 AP$_{50}^{\\mathrm{novel}}$ with ViT-B/16 and 48.5 AP$_{50}^{\\mathrm{novel}}$ with ViT-L/14.","On OV-LVIS, VMCNet with ViT-B/16 and ViT-L/14 reaches 27.8 and 38.4 mAP$_{r}$."],"url":"http://arxiv.org/abs/2501.16981v1"}
{"created":"2025-01-28 14:13:17","title":"RODEO: Robust Outlier Detection via Exposing Adaptive Out-of-Distribution Samples","abstract":"In recent years, there have been significant improvements in various forms of image outlier detection. However, outlier detection performance under adversarial settings lags far behind that in standard settings. This is due to the lack of effective exposure to adversarial scenarios during training, especially on unseen outliers, leading to detection models failing to learn robust features. To bridge this gap, we introduce RODEO, a data-centric approach that generates effective outliers for robust outlier detection. More specifically, we show that incorporating outlier exposure (OE) and adversarial training can be an effective strategy for this purpose, as long as the exposed training outliers meet certain characteristics, including diversity, and both conceptual differentiability and analogy to the inlier samples. We leverage a text-to-image model to achieve this goal. We demonstrate both quantitatively and qualitatively that our adaptive OE method effectively generates ``diverse'' and ``near-distribution'' outliers, leveraging information from both text and image domains. Moreover, our experimental results show that utilizing our synthesized outliers significantly enhances the performance of the outlier detector, particularly in adversarial settings.","sentences":["In recent years, there have been significant improvements in various forms of image outlier detection.","However, outlier detection performance under adversarial settings lags far behind that in standard settings.","This is due to the lack of effective exposure to adversarial scenarios during training, especially on unseen outliers, leading to detection models failing to learn robust features.","To bridge this gap, we introduce RODEO, a data-centric approach that generates effective outliers for robust outlier detection.","More specifically, we show that incorporating outlier exposure (OE) and adversarial training can be an effective strategy for this purpose, as long as the exposed training outliers meet certain characteristics, including diversity, and both conceptual differentiability and analogy to the inlier samples.","We leverage a text-to-image model to achieve this goal.","We demonstrate both quantitatively and qualitatively that our adaptive OE method effectively generates ``diverse'' and ``near-distribution'' outliers, leveraging information from both text and image domains.","Moreover, our experimental results show that utilizing our synthesized outliers significantly enhances the performance of the outlier detector, particularly in adversarial settings."],"url":"http://arxiv.org/abs/2501.16971v1"}
{"created":"2025-01-28 14:12:32","title":"What Really Matters for Learning-based LiDAR-Camera Calibration","abstract":"Calibration is an essential prerequisite for the accurate data fusion of LiDAR and camera sensors. Traditional calibration techniques often require specific targets or suitable scenes to obtain reliable 2D-3D correspondences. To tackle the challenge of target-less and online calibration, deep neural networks have been introduced to solve the problem in a data-driven manner. While previous learning-based methods have achieved impressive performance on specific datasets, they still struggle in complex real-world scenarios. Most existing works focus on improving calibration accuracy but overlook the underlying mechanisms. In this paper, we revisit the development of learning-based LiDAR-Camera calibration and encourage the community to pay more attention to the underlying principles to advance practical applications. We systematically analyze the paradigm of mainstream learning-based methods, and identify the critical limitations of regression-based methods with the widely used data generation pipeline. Our findings reveal that most learning-based methods inadvertently operate as retrieval networks, focusing more on single-modality distributions rather than cross-modality correspondences. We also investigate how the input data format and preprocessing operations impact network performance and summarize the regression clues to inform further improvements.","sentences":["Calibration is an essential prerequisite for the accurate data fusion of LiDAR and camera sensors.","Traditional calibration techniques often require specific targets or suitable scenes to obtain reliable 2D-3D correspondences.","To tackle the challenge of target-less and online calibration, deep neural networks have been introduced to solve the problem in a data-driven manner.","While previous learning-based methods have achieved impressive performance on specific datasets, they still struggle in complex real-world scenarios.","Most existing works focus on improving calibration accuracy but overlook the underlying mechanisms.","In this paper, we revisit the development of learning-based LiDAR-Camera calibration and encourage the community to pay more attention to the underlying principles to advance practical applications.","We systematically analyze the paradigm of mainstream learning-based methods, and identify the critical limitations of regression-based methods with the widely used data generation pipeline.","Our findings reveal that most learning-based methods inadvertently operate as retrieval networks, focusing more on single-modality distributions rather than cross-modality correspondences.","We also investigate how the input data format and preprocessing operations impact network performance and summarize the regression clues to inform further improvements."],"url":"http://arxiv.org/abs/2501.16969v1"}
{"created":"2025-01-28 14:08:57","title":"Heterogeneity-aware Personalized Federated Learning via Adaptive Dual-Agent Reinforcement Learning","abstract":"Federated Learning (FL) empowers multiple clients to collaboratively train machine learning models without sharing local data, making it highly applicable in heterogeneous Internet of Things (IoT) environments. However, intrinsic heterogeneity in clients' model architectures and computing capabilities often results in model accuracy loss and the intractable straggler problem, which significantly impairs training effectiveness. To tackle these challenges, this paper proposes a novel Heterogeneity-aware Personalized Federated Learning method, named HAPFL, via multi-level Reinforcement Learning (RL) mechanisms. HAPFL optimizes the training process by incorporating three strategic components: 1) An RL-based heterogeneous model allocation mechanism. The parameter server employs a Proximal Policy Optimization (PPO)-based RL agent to adaptively allocate appropriately sized, differentiated models to clients based on their performance, effectively mitigating performance disparities. 2) An RL-based training intensity adjustment scheme. The parameter server leverages another PPO-based RL agent to dynamically fine-tune the training intensity for each client to further enhance training efficiency and reduce straggling latency. 3) A knowledge distillation-based mutual learning mechanism. Each client deploys both a heterogeneous local model and a homogeneous lightweight model named LiteModel, where these models undergo mutual learning through knowledge distillation. This uniform LiteModel plays a pivotal role in aggregating and sharing global knowledge, significantly enhancing the effectiveness of personalized local training. Experimental results across multiple benchmark datasets demonstrate that HAPFL not only achieves high accuracy but also substantially reduces the overall training time by 20.9%-40.4% and decreases straggling latency by 19.0%-48.0% compared to existing solutions.","sentences":["Federated Learning (FL) empowers multiple clients to collaboratively train machine learning models without sharing local data, making it highly applicable in heterogeneous Internet of Things (IoT) environments.","However, intrinsic heterogeneity in clients' model architectures and computing capabilities often results in model accuracy loss and the intractable straggler problem, which significantly impairs training effectiveness.","To tackle these challenges, this paper proposes a novel Heterogeneity-aware Personalized Federated Learning method, named HAPFL, via multi-level Reinforcement Learning (RL) mechanisms.","HAPFL optimizes the training process by incorporating three strategic components: 1) An RL-based heterogeneous model allocation mechanism.","The parameter server employs a Proximal Policy Optimization (PPO)-based RL agent to adaptively allocate appropriately sized, differentiated models to clients based on their performance, effectively mitigating performance disparities.","2) An RL-based training intensity adjustment scheme.","The parameter server leverages another PPO-based RL agent to dynamically fine-tune the training intensity for each client to further enhance training efficiency and reduce straggling latency.","3) A knowledge distillation-based mutual learning mechanism.","Each client deploys both a heterogeneous local model and a homogeneous lightweight model named LiteModel, where these models undergo mutual learning through knowledge distillation.","This uniform LiteModel plays a pivotal role in aggregating and sharing global knowledge, significantly enhancing the effectiveness of personalized local training.","Experimental results across multiple benchmark datasets demonstrate that HAPFL not only achieves high accuracy but also substantially reduces the overall training time by 20.9%-40.4% and decreases straggling latency by 19.0%-48.0% compared to existing solutions."],"url":"http://arxiv.org/abs/2501.16966v1"}
{"created":"2025-01-28 14:07:52","title":"Few Edges Are Enough: Few-Shot Network Attack Detection with Graph Neural Networks","abstract":"Detecting cyberattacks using Graph Neural Networks (GNNs) has seen promising results recently. Most of the state-of-the-art models that leverage these techniques require labeled examples, hard to obtain in many real-world scenarios. To address this issue, unsupervised learning and Self-Supervised Learning (SSL) have emerged as interesting approaches to reduce the dependency on labeled data. Nonetheless, these methods tend to yield more anomalous detection algorithms rather than effective attack detection systems. This paper introduces Few Edges Are Enough (FEAE), a GNN-based architecture trained with SSL and Few-Shot Learning (FSL) to better distinguish between false positive anomalies and actual attacks. To maximize the potential of few-shot examples, our model employs a hybrid self-supervised objective that combines the advantages of contrastive-based and reconstruction-based SSL. By leveraging only a minimal number of labeled attack events, represented as attack edges, FEAE achieves competitive performance on two well-known network datasets compared to both supervised and unsupervised methods. Remarkably, our experimental results unveil that employing only 1 malicious event for each attack type in the dataset is sufficient to achieve substantial improvements. FEAE not only outperforms self-supervised GNN baselines but also surpasses some supervised approaches on one of the datasets.","sentences":["Detecting cyberattacks using Graph Neural Networks (GNNs) has seen promising results recently.","Most of the state-of-the-art models that leverage these techniques require labeled examples, hard to obtain in many real-world scenarios.","To address this issue, unsupervised learning and Self-Supervised Learning (SSL) have emerged as interesting approaches to reduce the dependency on labeled data.","Nonetheless, these methods tend to yield more anomalous detection algorithms rather than effective attack detection systems.","This paper introduces Few Edges Are Enough (FEAE), a GNN-based architecture trained with SSL and Few-Shot Learning (FSL) to better distinguish between false positive anomalies and actual attacks.","To maximize the potential of few-shot examples, our model employs a hybrid self-supervised objective that combines the advantages of contrastive-based and reconstruction-based SSL.","By leveraging only a minimal number of labeled attack events, represented as attack edges, FEAE achieves competitive performance on two well-known network datasets compared to both supervised and unsupervised methods.","Remarkably, our experimental results unveil that employing only 1 malicious event for each attack type in the dataset is sufficient to achieve substantial improvements.","FEAE not only outperforms self-supervised GNN baselines but also surpasses some supervised approaches on one of the datasets."],"url":"http://arxiv.org/abs/2501.16964v1"}
{"created":"2025-01-28 13:49:39","title":"Multiple Abstraction Level Retrieve Augment Generation","abstract":"A Retrieval-Augmented Generation (RAG) model powered by a large language model (LLM) provides a faster and more cost-effective solution for adapting to new data and knowledge. It also delivers more specialized responses compared to pre-trained LLMs. However, most existing approaches rely on retrieving prefix-sized chunks as references to support question-answering (Q/A). This approach is often deployed to address information needs at a single level of abstraction, as it struggles to generate answers across multiple levels of abstraction. In an RAG setting, while LLMs can summarize and answer questions effectively when provided with sufficient details, retrieving excessive information often leads to the 'lost in the middle' problem and exceeds token limitations. We propose a novel RAG approach that uses chunks of multiple abstraction levels (MAL), including multi-sentence-level, paragraph-level, section-level, and document-level. The effectiveness of our approach is demonstrated in an under-explored scientific domain of Glycoscience. Compared to traditional single-level RAG approaches, our approach improves AI evaluated answer correctness of Q/A by 25.739\\% on Glyco-related papers.","sentences":["A Retrieval-Augmented Generation (RAG) model powered by a large language model (LLM) provides a faster and more cost-effective solution for adapting to new data and knowledge.","It also delivers more specialized responses compared to pre-trained LLMs.","However, most existing approaches rely on retrieving prefix-sized chunks as references to support question-answering (Q/A).","This approach is often deployed to address information needs at a single level of abstraction, as it struggles to generate answers across multiple levels of abstraction.","In an RAG setting, while LLMs can summarize and answer questions effectively when provided with sufficient details, retrieving excessive information often leads to the 'lost in the middle' problem and exceeds token limitations.","We propose a novel RAG approach that uses chunks of multiple abstraction levels (MAL), including multi-sentence-level, paragraph-level, section-level, and document-level.","The effectiveness of our approach is demonstrated in an under-explored scientific domain of Glycoscience.","Compared to traditional single-level RAG approaches, our approach improves AI evaluated answer correctness of Q/A by 25.739\\% on Glyco-related papers."],"url":"http://arxiv.org/abs/2501.16952v1"}
{"created":"2025-01-28 13:46:11","title":"Stack Overflow Meets Replication: Security Research Amid Evolving Code Snippets (Extended Version)","abstract":"We study the impact of Stack Overflow code evolution on the stability of prior research findings derived from Stack Overflow data and provide recommendations for future studies. We systematically reviewed papers published between 2005--2023 to identify key aspects of Stack Overflow that can affect study results, such as the language or context of code snippets. Our analysis reveals that certain aspects are non-stationary over time, which could lead to different conclusions if experiments are repeated at different times. We replicated six studies using a more recent dataset to demonstrate this risk. Our findings show that four papers produced significantly different results than the original findings, preventing the same conclusions from being drawn with a newer dataset version. Consequently, we recommend treating Stack Overflow as a time series data source to provide context for interpreting cross-sectional research conclusions.","sentences":["We study the impact of Stack Overflow code evolution on the stability of prior research findings derived from Stack Overflow data and provide recommendations for future studies.","We systematically reviewed papers published between 2005--2023 to identify key aspects of Stack Overflow that can affect study results, such as the language or context of code snippets.","Our analysis reveals that certain aspects are non-stationary over time, which could lead to different conclusions if experiments are repeated at different times.","We replicated six studies using a more recent dataset to demonstrate this risk.","Our findings show that four papers produced significantly different results than the original findings, preventing the same conclusions from being drawn with a newer dataset version.","Consequently, we recommend treating Stack Overflow as a time series data source to provide context for interpreting cross-sectional research conclusions."],"url":"http://arxiv.org/abs/2501.16948v1"}
{"created":"2025-01-28 13:46:01","title":"Image-based Geo-localization for Robotics: Are Black-box Vision-Language Models there yet?","abstract":"The advances in Vision-Language models (VLMs) offer exciting opportunities for robotic applications involving image geo-localization, the problem of identifying the geo-coordinates of a place based on visual data only. Recent research works have focused on using a VLM as embeddings extractor for geo-localization, however, the most sophisticated VLMs may only be available as black boxes that are accessible through an API, and come with a number of limitations: there is no access to training data, model features and gradients; retraining is not possible; the number of predictions may be limited by the API; training on model outputs is often prohibited; and queries are open-ended. The utilization of a VLM as a stand-alone, zero-shot geo-localization system using a single text-based prompt is largely unexplored. To bridge this gap, this paper undertakes the first systematic study, to the best of our knowledge, to investigate the potential of some of the state-of-the-art VLMs as stand-alone, zero-shot geo-localization systems in a black-box setting with realistic constraints. We consider three main scenarios for this thorough investigation: a) fixed text-based prompt; b) semantically-equivalent text-based prompts; and c) semantically-equivalent query images. We also take into account the auto-regressive and probabilistic generation process of the VLMs when investigating their utility for geo-localization task by using model consistency as a metric in addition to traditional accuracy. Our work provides new insights in the capabilities of different VLMs for the above-mentioned scenarios.","sentences":["The advances in Vision-Language models (VLMs) offer exciting opportunities for robotic applications involving image geo-localization, the problem of identifying the geo-coordinates of a place based on visual data only.","Recent research works have focused on using a VLM as embeddings extractor for geo-localization, however, the most sophisticated VLMs may only be available as black boxes that are accessible through an API, and come with a number of limitations: there is no access to training data, model features and gradients; retraining is not possible; the number of predictions may be limited by the API; training on model outputs is often prohibited; and queries are open-ended.","The utilization of a VLM as a stand-alone, zero-shot geo-localization system using a single text-based prompt is largely unexplored.","To bridge this gap, this paper undertakes the first systematic study, to the best of our knowledge, to investigate the potential of some of the state-of-the-art VLMs as stand-alone, zero-shot geo-localization systems in a black-box setting with realistic constraints.","We consider three main scenarios for this thorough investigation: a) fixed text-based prompt; b) semantically-equivalent text-based prompts; and c) semantically-equivalent query images.","We also take into account the auto-regressive and probabilistic generation process of the VLMs when investigating their utility for geo-localization task by using model consistency as a metric in addition to traditional accuracy.","Our work provides new insights in the capabilities of different VLMs for the above-mentioned scenarios."],"url":"http://arxiv.org/abs/2501.16947v1"}
{"created":"2025-01-28 13:37:44","title":"Exact Computation of Any-Order Shapley Interactions for Graph Neural Networks","abstract":"Albeit the ubiquitous use of Graph Neural Networks (GNNs) in machine learning (ML) prediction tasks involving graph-structured data, their interpretability remains challenging. In explainable artificial intelligence (XAI), the Shapley Value (SV) is the predominant method to quantify contributions of individual features to a ML model's output. Addressing the limitations of SVs in complex prediction models, Shapley Interactions (SIs) extend the SV to groups of features. In this work, we explain single graph predictions of GNNs with SIs that quantify node contributions and interactions among multiple nodes. By exploiting the GNN architecture, we show that the structure of interactions in node embeddings are preserved for graph prediction. As a result, the exponential complexity of SIs depends only on the receptive fields, i.e. the message-passing ranges determined by the connectivity of the graph and the number of convolutional layers. Based on our theoretical results, we introduce GraphSHAP-IQ, an efficient approach to compute any-order SIs exactly. GraphSHAP-IQ is applicable to popular message passing techniques in conjunction with a linear global pooling and output layer. We showcase that GraphSHAP-IQ substantially reduces the exponential complexity of computing exact SIs on multiple benchmark datasets. Beyond exact computation, we evaluate GraphSHAP-IQ's approximation of SIs on popular GNN architectures and compare with existing baselines. Lastly, we visualize SIs of real-world water distribution networks and molecule structures using a SI-Graph.","sentences":["Albeit the ubiquitous use of Graph Neural Networks (GNNs) in machine learning (ML) prediction tasks involving graph-structured data, their interpretability remains challenging.","In explainable artificial intelligence (XAI), the Shapley Value (SV) is the predominant method to quantify contributions of individual features to a ML model's output.","Addressing the limitations of SVs in complex prediction models, Shapley Interactions (SIs) extend the SV to groups of features.","In this work, we explain single graph predictions of GNNs with SIs that quantify node contributions and interactions among multiple nodes.","By exploiting the GNN architecture, we show that the structure of interactions in node embeddings are preserved for graph prediction.","As a result, the exponential complexity of SIs depends only on the receptive fields, i.e. the message-passing ranges determined by the connectivity of the graph and the number of convolutional layers.","Based on our theoretical results, we introduce GraphSHAP-IQ, an efficient approach to compute any-order SIs exactly.","GraphSHAP-IQ is applicable to popular message passing techniques in conjunction with a linear global pooling and output layer.","We showcase that GraphSHAP-IQ substantially reduces the exponential complexity of computing exact SIs on multiple benchmark datasets.","Beyond exact computation, we evaluate GraphSHAP-IQ's approximation of SIs on popular GNN architectures and compare with existing baselines.","Lastly, we visualize SIs of real-world water distribution networks and molecule structures using a SI-Graph."],"url":"http://arxiv.org/abs/2501.16944v1"}
{"created":"2025-01-28 13:21:59","title":"Online-BLS: An Accurate and Efficient Online Broad Learning System for Data Stream Classification","abstract":"The state-of-the-art online learning models generally conduct a single online gradient descent when a new sample arrives and thus suffer from suboptimal model weights. To this end, we introduce an online broad learning system framework with closed-form solutions for each online update. Different from employing existing incremental broad learning algorithms for online learning tasks, which tend to incur degraded accuracy and expensive online update overhead, we design an effective weight estimation algorithm and an efficient online updating strategy to remedy the above two deficiencies, respectively. Specifically, an effective weight estimation algorithm is first developed by replacing notorious matrix inverse operations with Cholesky decomposition and forward-backward substitution to improve model accuracy. Second, we devise an efficient online updating strategy that dramatically reduces online update time. Theoretical analysis exhibits the splendid error bound and low time complexity of our model. The most popular test-then-training evaluation experiments on various real-world datasets prove its superiority and efficiency. Furthermore, our framework is naturally extended to data stream scenarios with concept drift and exceeds state-of-the-art baselines.","sentences":["The state-of-the-art online learning models generally conduct a single online gradient descent when a new sample arrives and thus suffer from suboptimal model weights.","To this end, we introduce an online broad learning system framework with closed-form solutions for each online update.","Different from employing existing incremental broad learning algorithms for online learning tasks, which tend to incur degraded accuracy and expensive online update overhead, we design an effective weight estimation algorithm and an efficient online updating strategy to remedy the above two deficiencies, respectively.","Specifically, an effective weight estimation algorithm is first developed by replacing notorious matrix inverse operations with Cholesky decomposition and forward-backward substitution to improve model accuracy.","Second, we devise an efficient online updating strategy that dramatically reduces online update time.","Theoretical analysis exhibits the splendid error bound and low time complexity of our model.","The most popular test-then-training evaluation experiments on various real-world datasets prove its superiority and efficiency.","Furthermore, our framework is naturally extended to data stream scenarios with concept drift and exceeds state-of-the-art baselines."],"url":"http://arxiv.org/abs/2501.16932v1"}
{"created":"2025-01-28 13:15:07","title":"Detecting harassment and defamation in cyberbullying with emotion-adaptive training","abstract":"Existing research on detecting cyberbullying incidents on social media has primarily concentrated on harassment and is typically approached as a binary classification task. However, cyberbullying encompasses various forms, such as denigration and harassment, which celebrities frequently face. Furthermore, suitable training data for these diverse forms of cyberbullying remains scarce. In this study, we first develop a celebrity cyberbullying dataset that encompasses two distinct types of incidents: harassment and defamation. We investigate various types of transformer-based models, namely masked (RoBERTa, Bert and DistilBert), replacing(Electra), autoregressive (XLnet), masked&permuted (Mpnet), text-text (T5) and large language models (Llama2 and Llama3) under low source settings. We find that they perform competitively on explicit harassment binary detection. However, their performance is substantially lower on harassment and denigration multi-classification tasks. Therefore, we propose an emotion-adaptive training framework (EAT) that helps transfer knowledge from the domain of emotion detection to the domain of cyberbullying detection to help detect indirect cyberbullying events. EAT consistently improves the average macro F1, precision and recall by 20% in cyberbullying detection tasks across nine transformer-based models under low-resource settings. Our claims are supported by intuitive theoretical insights and extensive experiments.","sentences":["Existing research on detecting cyberbullying incidents on social media has primarily concentrated on harassment and is typically approached as a binary classification task.","However, cyberbullying encompasses various forms, such as denigration and harassment, which celebrities frequently face.","Furthermore, suitable training data for these diverse forms of cyberbullying remains scarce.","In this study, we first develop a celebrity cyberbullying dataset that encompasses two distinct types of incidents: harassment and defamation.","We investigate various types of transformer-based models, namely masked (RoBERTa, Bert and DistilBert), replacing(Electra), autoregressive (XLnet), masked&permuted (Mpnet), text-text (T5) and large language models (Llama2 and Llama3) under low source settings.","We find that they perform competitively on explicit harassment binary detection.","However, their performance is substantially lower on harassment and denigration multi-classification tasks.","Therefore, we propose an emotion-adaptive training framework (EAT) that helps transfer knowledge from the domain of emotion detection to the domain of cyberbullying detection to help detect indirect cyberbullying events.","EAT consistently improves the average macro F1, precision and recall by 20% in cyberbullying detection tasks across nine transformer-based models under low-resource settings.","Our claims are supported by intuitive theoretical insights and extensive experiments."],"url":"http://arxiv.org/abs/2501.16925v1"}
{"created":"2025-01-28 13:09:08","title":"Agential AI for Integrated Continual Learning, Deliberative Behavior, and Comprehensible Models","abstract":"Contemporary machine learning paradigm excels in statistical data analysis, solving problems that classical AI couldn't. However, it faces key limitations, such as a lack of integration with planning, incomprehensible internal structure, and inability to learn continually. We present the initial design for an AI system, Agential AI (AAI), in principle operating independently or on top of statistical methods, designed to overcome these issues. AAI's core is a learning method that models temporal dynamics with guarantees of completeness, minimality, and continual learning, using component-level variation and selection to learn the structure of the environment. It integrates this with a behavior algorithm that plans on a learned model and encapsulates high-level behavior patterns. Preliminary experiments on a simple environment show AAI's effectiveness and potential.","sentences":["Contemporary machine learning paradigm excels in statistical data analysis, solving problems that classical AI couldn't.","However, it faces key limitations, such as a lack of integration with planning, incomprehensible internal structure, and inability to learn continually.","We present the initial design for an AI system, Agential AI (AAI), in principle operating independently or on top of statistical methods, designed to overcome these issues.","AAI's core is a learning method that models temporal dynamics with guarantees of completeness, minimality, and continual learning, using component-level variation and selection to learn the structure of the environment.","It integrates this with a behavior algorithm that plans on a learned model and encapsulates high-level behavior patterns.","Preliminary experiments on a simple environment show AAI's effectiveness and potential."],"url":"http://arxiv.org/abs/2501.16922v1"}
{"created":"2025-01-28 13:02:52","title":"On Rollouts in Model-Based Reinforcement Learning","abstract":"Model-based reinforcement learning (MBRL) seeks to enhance data efficiency by learning a model of the environment and generating synthetic rollouts from it. However, accumulated model errors during these rollouts can distort the data distribution, negatively impacting policy learning and hindering long-term planning. Thus, the accumulation of model errors is a key bottleneck in current MBRL methods. We propose Infoprop, a model-based rollout mechanism that separates aleatoric from epistemic model uncertainty and reduces the influence of the latter on the data distribution. Further, Infoprop keeps track of accumulated model errors along a model rollout and provides termination criteria to limit data corruption. We demonstrate the capabilities of Infoprop in the Infoprop-Dyna algorithm, reporting state-of-the-art performance in Dyna-style MBRL on common MuJoCo benchmark tasks while substantially increasing rollout length and data quality.","sentences":["Model-based reinforcement learning (MBRL) seeks to enhance data efficiency by learning a model of the environment and generating synthetic rollouts from it.","However, accumulated model errors during these rollouts can distort the data distribution, negatively impacting policy learning and hindering long-term planning.","Thus, the accumulation of model errors is a key bottleneck in current MBRL methods.","We propose Infoprop, a model-based rollout mechanism that separates aleatoric from epistemic model uncertainty and reduces the influence of the latter on the data distribution.","Further, Infoprop keeps track of accumulated model errors along a model rollout and provides termination criteria to limit data corruption.","We demonstrate the capabilities of Infoprop in the Infoprop-Dyna algorithm, reporting state-of-the-art performance in Dyna-style MBRL on common MuJoCo benchmark tasks while substantially increasing rollout length and data quality."],"url":"http://arxiv.org/abs/2501.16918v1"}
{"created":"2025-01-28 12:44:27","title":"Adversarial Masked Autoencoder Purifier with Defense Transferability","abstract":"The study of adversarial defense still struggles to combat with advanced adversarial attacks. In contrast to most prior studies that rely on the diffusion model for test-time defense to remarkably increase the inference time, we propose Masked AutoEncoder Purifier (MAEP), which integrates Masked AutoEncoder (MAE) into an adversarial purifier framework for test-time purification. While MAEP achieves promising adversarial robustness, it particularly features model defense transferability and attack generalization without relying on using additional data that is different from the training dataset. To our knowledge, MAEP is the first study of adversarial purifier based on MAE. Extensive experimental results demonstrate that our method can not only maintain clear accuracy with only a slight drop but also exhibit a close gap between the clean and robust accuracy. Notably, MAEP trained on CIFAR10 achieves state-of-the-art performance even when tested directly on ImageNet, outperforming existing diffusion-based models trained specifically on ImageNet.","sentences":["The study of adversarial defense still struggles to combat with advanced adversarial attacks.","In contrast to most prior studies that rely on the diffusion model for test-time defense to remarkably increase the inference time, we propose Masked AutoEncoder Purifier (MAEP), which integrates Masked AutoEncoder (MAE) into an adversarial purifier framework for test-time purification.","While MAEP achieves promising adversarial robustness, it particularly features model defense transferability and attack generalization without relying on using additional data that is different from the training dataset.","To our knowledge, MAEP is the first study of adversarial purifier based on MAE.","Extensive experimental results demonstrate that our method can not only maintain clear accuracy with only a slight drop but also exhibit a close gap between the clean and robust accuracy.","Notably, MAEP trained on CIFAR10 achieves state-of-the-art performance even when tested directly on ImageNet, outperforming existing diffusion-based models trained specifically on ImageNet."],"url":"http://arxiv.org/abs/2501.16904v1"}
{"created":"2025-01-28 12:36:47","title":"RAINER: A Robust Ensemble Learning Grid Search-Tuned Framework for Rainfall Patterns Prediction","abstract":"Rainfall prediction remains a persistent challenge due to the highly nonlinear and complex nature of meteorological data. Existing approaches lack systematic utilization of grid search for optimal hyperparameter tuning, relying instead on heuristic or manual selection, frequently resulting in sub-optimal results. Additionally, these methods rarely incorporate newly constructed meteorological features such as differences between temperature and humidity to capture critical weather dynamics. Furthermore, there is a lack of systematic evaluation of ensemble learning techniques and limited exploration of diverse advanced models introduced in the past one or two years. To address these limitations, we propose a robust ensemble learning grid search-tuned framework (RAINER) for rainfall prediction. RAINER incorporates a comprehensive feature engineering pipeline, including outlier removal, imputation of missing values, feature reconstruction, and dimensionality reduction via Principal Component Analysis (PCA). The framework integrates novel meteorological features to capture dynamic weather patterns and systematically evaluates non-learning mathematical-based methods and a variety of machine learning models, from weak classifiers to advanced neural networks such as Kolmogorov-Arnold Networks (KAN). By leveraging grid search for hyperparameter tuning and ensemble voting techniques, RAINER achieves promising results within real-world datasets.","sentences":["Rainfall prediction remains a persistent challenge due to the highly nonlinear and complex nature of meteorological data.","Existing approaches lack systematic utilization of grid search for optimal hyperparameter tuning, relying instead on heuristic or manual selection, frequently resulting in sub-optimal results.","Additionally, these methods rarely incorporate newly constructed meteorological features such as differences between temperature and humidity to capture critical weather dynamics.","Furthermore, there is a lack of systematic evaluation of ensemble learning techniques and limited exploration of diverse advanced models introduced in the past one or two years.","To address these limitations, we propose a robust ensemble learning grid search-tuned framework (RAINER) for rainfall prediction.","RAINER incorporates a comprehensive feature engineering pipeline, including outlier removal, imputation of missing values, feature reconstruction, and dimensionality reduction via Principal Component Analysis (PCA).","The framework integrates novel meteorological features to capture dynamic weather patterns and systematically evaluates non-learning mathematical-based methods and a variety of machine learning models, from weak classifiers to advanced neural networks such as Kolmogorov-Arnold Networks (KAN).","By leveraging grid search for hyperparameter tuning and ensemble voting techniques, RAINER achieves promising results within real-world datasets."],"url":"http://arxiv.org/abs/2501.16900v1"}
{"created":"2025-01-28 12:26:07","title":"DBSCAN in domains with periodic boundary conditions","abstract":"Many scientific problems involve data that is embedded in a space with periodic boundary conditions. This can for instance be related to an inherent cyclic or rotational symmetry in the data or a spatially extended periodicity. When analyzing such data, well-tailored methods are needed to obtain efficient approaches that obey the periodic boundary conditions of the problem. In this work, we present a method for applying a clustering algorithm to data embedded in a periodic domain based on the DBSCAN algorithm, a widely used unsupervised machine learning method that identifies clusters in data. The proposed method internally leverages the conventional DBSCAN algorithm for domains with open boundaries, such that it remains compatible with all optimized implementations for neighborhood searches in open domains. In this way, it retains the same optimized runtime complexity of $O(N\\log N)$. We demonstrate the workings of the proposed method using synthetic data in one, two and three dimensions and also apply it to a real-world example involving the clustering of bubbles in a turbulent flow. The proposed approach is implemented in a ready-to-use Python package that we make publicly available.","sentences":["Many scientific problems involve data that is embedded in a space with periodic boundary conditions.","This can for instance be related to an inherent cyclic or rotational symmetry in the data or a spatially extended periodicity.","When analyzing such data, well-tailored methods are needed to obtain efficient approaches that obey the periodic boundary conditions of the problem.","In this work, we present a method for applying a clustering algorithm to data embedded in a periodic domain based on the DBSCAN algorithm, a widely used unsupervised machine learning method that identifies clusters in data.","The proposed method internally leverages the conventional DBSCAN algorithm for domains with open boundaries, such that it remains compatible with all optimized implementations for neighborhood searches in open domains.","In this way, it retains the same optimized runtime complexity of $O(N\\log N)$. We demonstrate the workings of the proposed method using synthetic data in one, two and three dimensions and also apply it to a real-world example involving the clustering of bubbles in a turbulent flow.","The proposed approach is implemented in a ready-to-use Python package that we make publicly available."],"url":"http://arxiv.org/abs/2501.16894v1"}
{"created":"2025-01-28 12:18:09","title":"Secure Federated Graph-Filtering for Recommender Systems","abstract":"Recommender systems often rely on graph-based filters, such as normalized item-item adjacency matrices and low-pass filters. While effective, the centralized computation of these components raises concerns about privacy, security, and the ethical use of user data. This work proposes two decentralized frameworks for securely computing these critical graph components without centralizing sensitive information. The first approach leverages lightweight Multi-Party Computation and distributed singular vector computations to privately compute key graph filters. The second extends this framework by incorporating low-rank approximations, enabling a trade-off between communication efficiency and predictive performance. Empirical evaluations on benchmark datasets demonstrate that the proposed methods achieve comparable accuracy to centralized state-of-the-art systems while ensuring data confidentiality and maintaining low communication costs. Our results highlight the potential for privacy-preserving decentralized architectures to bridge the gap between utility and user data protection in modern recommender systems.","sentences":["Recommender systems often rely on graph-based filters, such as normalized item-item adjacency matrices and low-pass filters.","While effective, the centralized computation of these components raises concerns about privacy, security, and the ethical use of user data.","This work proposes two decentralized frameworks for securely computing these critical graph components without centralizing sensitive information.","The first approach leverages lightweight Multi-Party Computation and distributed singular vector computations to privately compute key graph filters.","The second extends this framework by incorporating low-rank approximations, enabling a trade-off between communication efficiency and predictive performance.","Empirical evaluations on benchmark datasets demonstrate that the proposed methods achieve comparable accuracy to centralized state-of-the-art systems while ensuring data confidentiality and maintaining low communication costs.","Our results highlight the potential for privacy-preserving decentralized architectures to bridge the gap between utility and user data protection in modern recommender systems."],"url":"http://arxiv.org/abs/2501.16888v1"}
{"created":"2025-01-28 12:13:53","title":"\"My Whereabouts, my Location, it's Directly Linked to my Physical Security\": An Exploratory Qualitative Study of Location-Dependent Security and Privacy Perceptions among Activist Tech Users","abstract":"Digital-safety research with at-risk users is particularly urgent. At-risk users are more likely to be digitally attacked or targeted by surveillance and could be disproportionately harmed by attacks that facilitate physical assaults. One group of such at-risk users are activists and politically active individuals. For them, as for other at-risk users, the rise of smart environments harbors new risks. Since digitization and datafication are no longer limited to a series of personal devices that can be switched on and off, but increasingly and continuously surround users, granular geolocation poses new safety challenges. Drawing on eight exploratory qualitative interviews of an ongoing research project, this contribution highlights what activists with powerful adversaries think about evermore data traces, including location data, and how they intend to deal with emerging risks. Responses of activists include attempts to control one's immediate technological surroundings and to more carefully manage device-related location data. For some activists, threat modeling has also shaped provider choices based on geopolitical considerations. Since many activists have not enough digital-safety knowledge for effective protection, feelings of insecurity and paranoia are widespread. Channeling the concerns and fears of our interlocutors, we call for more research on how activists can protect themselves against evermore fine-grained location data tracking.","sentences":["Digital-safety research with at-risk users is particularly urgent.","At-risk users are more likely to be digitally attacked or targeted by surveillance and could be disproportionately harmed by attacks that facilitate physical assaults.","One group of such at-risk users are activists and politically active individuals.","For them, as for other at-risk users, the rise of smart environments harbors new risks.","Since digitization and datafication are no longer limited to a series of personal devices that can be switched on and off, but increasingly and continuously surround users, granular geolocation poses new safety challenges.","Drawing on eight exploratory qualitative interviews of an ongoing research project, this contribution highlights what activists with powerful adversaries think about evermore data traces, including location data, and how they intend to deal with emerging risks.","Responses of activists include attempts to control one's immediate technological surroundings and to more carefully manage device-related location data.","For some activists, threat modeling has also shaped provider choices based on geopolitical considerations.","Since many activists have not enough digital-safety knowledge for effective protection, feelings of insecurity and paranoia are widespread.","Channeling the concerns and fears of our interlocutors, we call for more research on how activists can protect themselves against evermore fine-grained location data tracking."],"url":"http://arxiv.org/abs/2501.16885v1"}
{"created":"2025-01-28 12:00:45","title":"Enhancing Web Service Anomaly Detection via Fine-grained Multi-modal Association and Frequency Domain Analysis","abstract":"Anomaly detection is crucial for ensuring the stability and reliability of web service systems. Logs and metrics contain multiple information that can reflect the system's operational state and potential anomalies. Thus, existing anomaly detection methods use logs and metrics to detect web service systems' anomalies through data fusion approaches. They associate logs and metrics using coarse-grained time window alignment and capture the normal patterns of system operation through reconstruction. However, these methods have two issues that limit their performance in anomaly detection. First, due to asynchrony between logs and metrics, coarse-grained time window alignment cannot achieve a precise association between the two modalities. Second, reconstruction-based methods suffer from severe overgeneralization problems, resulting in anomalies being accurately reconstructed. In this paper, we propose a novel anomaly detection method named FFAD to address these two issues. On the one hand, FFAD employs graph-based alignment to mine and extract associations between the modalities from the constructed log-metric relation graph, achieving precise associations between logs and metrics. On the other hand, we improve the model's fit to normal data distributions through Fourier Frequency Focus, thereby enhancing the effectiveness of anomaly detection. We validated the effectiveness of our model on two real-world industrial datasets and one open-source dataset. The results show that our method achieves an average anomaly detection F1-score of 93.6%, representing an 8.8% improvement over previous state-of-the-art methods.","sentences":["Anomaly detection is crucial for ensuring the stability and reliability of web service systems.","Logs and metrics contain multiple information that can reflect the system's operational state and potential anomalies.","Thus, existing anomaly detection methods use logs and metrics to detect web service systems' anomalies through data fusion approaches.","They associate logs and metrics using coarse-grained time window alignment and capture the normal patterns of system operation through reconstruction.","However, these methods have two issues that limit their performance in anomaly detection.","First, due to asynchrony between logs and metrics, coarse-grained time window alignment cannot achieve a precise association between the two modalities.","Second, reconstruction-based methods suffer from severe overgeneralization problems, resulting in anomalies being accurately reconstructed.","In this paper, we propose a novel anomaly detection method named FFAD to address these two issues.","On the one hand, FFAD employs graph-based alignment to mine and extract associations between the modalities from the constructed log-metric relation graph, achieving precise associations between logs and metrics.","On the other hand, we improve the model's fit to normal data distributions through Fourier Frequency Focus, thereby enhancing the effectiveness of anomaly detection.","We validated the effectiveness of our model on two real-world industrial datasets and one open-source dataset.","The results show that our method achieves an average anomaly detection F1-score of 93.6%, representing an 8.8% improvement over previous state-of-the-art methods."],"url":"http://arxiv.org/abs/2501.16875v1"}
{"created":"2025-01-28 11:28:22","title":"A methodology and a platform for high-quality rich personal data collection","abstract":"In the last years the pervasive use of sensors, as they exist in smart devices, e.g., phones, watches, medical devices, has increased dramatically the availability of personal data. However, existing research on data collection primarily focuses on the objective view of reality, as provided, for instance, by sensors, often neglecting the integration of subjective human input, as provided, for instance, by user answers to questionnaires. This limits substantially the exploitability of the collected data. In this paper we present a methodology and a platform specifically designed for the collection of a combination of large-scale sensor data and qualitative human feedback. The methodology has been designed to be deployed on top, and enriches the functionalities of, an existing data collection APP, called iLog, which has been used in large scale, worldwide data collection experiments. The main goal is to put the key actors involved in an experiment, i.e., the researcher in charge, the participant, and iLog in better control of the experiment itself, thus enabling a much improved quality and richness of the data collected. The novel functionalities of the resulting platform are: (i) a time-wise representation of the situational context within which the data collection is performed, (ii) an explicit representation of the temporal context within which the data collection is performed, (iii) a calendar-based dashboard for the real-time monitoring of the data collection context(s), and, finally, (iv) a mechanism for the run-time revision of the data collection plan. The practicality and utility of the proposed functionalities are demonstrated by showing how they apply to a case study involving 350 University students.","sentences":["In the last years the pervasive use of sensors, as they exist in smart devices, e.g., phones, watches, medical devices, has increased dramatically the availability of personal data.","However, existing research on data collection primarily focuses on the objective view of reality, as provided, for instance, by sensors, often neglecting the integration of subjective human input, as provided, for instance, by user answers to questionnaires.","This limits substantially the exploitability of the collected data.","In this paper we present a methodology and a platform specifically designed for the collection of a combination of large-scale sensor data and qualitative human feedback.","The methodology has been designed to be deployed on top, and enriches the functionalities of, an existing data collection APP, called iLog, which has been used in large scale, worldwide data collection experiments.","The main goal is to put the key actors involved in an experiment, i.e., the researcher in charge, the participant, and iLog in better control of the experiment itself, thus enabling a much improved quality and richness of the data collected.","The novel functionalities of the resulting platform are: (i) a time-wise representation of the situational context within which the data collection is performed, (ii) an explicit representation of the temporal context within which the data collection is performed, (iii) a calendar-based dashboard for the real-time monitoring of the data collection context(s), and, finally, (iv) a mechanism for the run-time revision of the data collection plan.","The practicality and utility of the proposed functionalities are demonstrated by showing how they apply to a case study involving 350 University students."],"url":"http://arxiv.org/abs/2501.16864v1"}
{"created":"2025-01-28 10:41:48","title":"Hybrid Phenology Modeling for Predicting Temperature Effects on Tree Dormancy","abstract":"Biophysical models offer valuable insights into climate-phenology relationships in both natural and agricultural settings. However, there are substantial structural discrepancies across models which require site-specific recalibration, often yielding inconsistent predictions under similar climate scenarios. Machine learning methods offer data-driven solutions, but often lack interpretability and alignment with existing knowledge. We present a phenology model describing dormancy in fruit trees, integrating conventional biophysical models with a neural network to address their structural disparities. We evaluate our hybrid model in an extensive case study predicting cherry tree phenology in Japan, South Korea and Switzerland. Our approach consistently outperforms both traditional biophysical and machine learning models in predicting blooming dates across years. Additionally, the neural network's adaptability facilitates parameter learning for specific tree varieties, enabling robust generalization to new sites without site-specific recalibration. This hybrid model leverages both biophysical constraints and data-driven flexibility, offering a promising avenue for accurate and interpretable phenology modeling.","sentences":["Biophysical models offer valuable insights into climate-phenology relationships in both natural and agricultural settings.","However, there are substantial structural discrepancies across models which require site-specific recalibration, often yielding inconsistent predictions under similar climate scenarios.","Machine learning methods offer data-driven solutions, but often lack interpretability and alignment with existing knowledge.","We present a phenology model describing dormancy in fruit trees, integrating conventional biophysical models with a neural network to address their structural disparities.","We evaluate our hybrid model in an extensive case study predicting cherry tree phenology in Japan, South Korea and Switzerland.","Our approach consistently outperforms both traditional biophysical and machine learning models in predicting blooming dates across years.","Additionally, the neural network's adaptability facilitates parameter learning for specific tree varieties, enabling robust generalization to new sites without site-specific recalibration.","This hybrid model leverages both biophysical constraints and data-driven flexibility, offering a promising avenue for accurate and interpretable phenology modeling."],"url":"http://arxiv.org/abs/2501.16848v1"}
{"created":"2025-01-28 10:33:30","title":"Bones of Contention: Exploring Query-Efficient Attacks Against Skeleton Recognition Systems","abstract":"Skeleton action recognition models have secured more attention than video-based ones in various applications due to privacy preservation and lower storage requirements. Skeleton data are typically transmitted to cloud servers for action recognition, with results returned to clients via Apps/APIs. However, the vulnerability of skeletal models against adversarial perturbations gradually reveals the unreliability of these systems. Existing black-box attacks all operate in a decision-based manner, resulting in numerous queries that hinder efficiency and feasibility in real-world applications. Moreover, all attacks off the shelf focus on only restricted perturbations, while ignoring model weaknesses when encountered with non-semantic perturbations. In this paper, we propose two query-effIcient Skeletal Adversarial AttaCks, ISAAC-K and ISAAC-N. As a black-box attack, ISAAC-K utilizes Grad-CAM in a surrogate model to extract key joints where minor sparse perturbations are then added to fool the classifier. To guarantee natural adversarial motions, we introduce constraints of both bone length and temporal consistency. ISAAC-K finds stronger adversarial examples on $\\ell_\\infty$ norm, which can encompass those on other norms. Exhaustive experiments substantiate that ISAAC-K can uplift the attack efficiency of the perturbations under 10 skeletal models. Additionally, as a byproduct, ISAAC-N fools the classifier by replacing skeletons unrelated to the action. We surprisingly find that skeletal models are vulnerable to large perturbations where the part-wise non-semantic joints are just replaced, leading to a query-free no-box attack without any prior knowledge. Based on that, four adaptive defenses are eventually proposed to improve the robustness of skeleton recognition models.","sentences":["Skeleton action recognition models have secured more attention than video-based ones in various applications due to privacy preservation and lower storage requirements.","Skeleton data are typically transmitted to cloud servers for action recognition, with results returned to clients via Apps/APIs.","However, the vulnerability of skeletal models against adversarial perturbations gradually reveals the unreliability of these systems.","Existing black-box attacks all operate in a decision-based manner, resulting in numerous queries that hinder efficiency and feasibility in real-world applications.","Moreover, all attacks off the shelf focus on only restricted perturbations, while ignoring model weaknesses when encountered with non-semantic perturbations.","In this paper, we propose two query-effIcient Skeletal Adversarial AttaCks, ISAAC-K and ISAAC-N. As a black-box attack, ISAAC-K utilizes Grad-CAM in a surrogate model to extract key joints where minor sparse perturbations are then added to fool the classifier.","To guarantee natural adversarial motions, we introduce constraints of both bone length and temporal consistency.","ISAAC-K finds stronger adversarial examples on $\\ell_\\infty$ norm, which can encompass those on other norms.","Exhaustive experiments substantiate that ISAAC-K can uplift the attack efficiency of the perturbations under 10 skeletal models.","Additionally, as a byproduct, ISAAC-N fools the classifier by replacing skeletons unrelated to the action.","We surprisingly find that skeletal models are vulnerable to large perturbations where the part-wise non-semantic joints are just replaced, leading to a query-free no-box attack without any prior knowledge.","Based on that, four adaptive defenses are eventually proposed to improve the robustness of skeleton recognition models."],"url":"http://arxiv.org/abs/2501.16843v1"}
{"created":"2025-01-28 10:33:01","title":"Adapting Network Information to Semantics for Generalizable and Plug-and-Play Multi-Scenario Network Diagnosis","abstract":"Network fault diagnosis is a core challenge in ensuring the stability and reliability of modern network operations. Traditional approaches, limited by their training on specific performance metrics for predefined scenarios, struggle to generalize across diverse faults and anomalies in varying network environments. In recent years, large language models (LLMs) have demonstrated strong generalization capabilities across various domains. Building on this success, we propose NetSemantic, a plug-and-play intelligent network fault diagnosis framework based on LLMs. NetSemantic transforms multimodal network information into unified textual representations, enabling LLMs to perform reasoning and generate efficient fault resolutions and health assessment reports. To further enhance the logical reasoning capabilities of LLMs, we introduce a novel symbolic representation method that transforms logically strong network information into symbols. Additionally, we propose a self-adaptive data updating mechanism that dynamically incorporates network information into a knowledge graph to ensure the validity and timeliness of the knowledge base. Experimental results demonstrate that NetSemantic excels in network fault diagnosis across various complex scenarios, significantly improving diagnostic accuracy and reliability.","sentences":["Network fault diagnosis is a core challenge in ensuring the stability and reliability of modern network operations.","Traditional approaches, limited by their training on specific performance metrics for predefined scenarios, struggle to generalize across diverse faults and anomalies in varying network environments.","In recent years, large language models (LLMs) have demonstrated strong generalization capabilities across various domains.","Building on this success, we propose NetSemantic, a plug-and-play intelligent network fault diagnosis framework based on LLMs.","NetSemantic transforms multimodal network information into unified textual representations, enabling LLMs to perform reasoning and generate efficient fault resolutions and health assessment reports.","To further enhance the logical reasoning capabilities of LLMs, we introduce a novel symbolic representation method that transforms logically strong network information into symbols.","Additionally, we propose a self-adaptive data updating mechanism that dynamically incorporates network information into a knowledge graph to ensure the validity and timeliness of the knowledge base.","Experimental results demonstrate that NetSemantic excels in network fault diagnosis across various complex scenarios, significantly improving diagnostic accuracy and reliability."],"url":"http://arxiv.org/abs/2501.16842v1"}
{"created":"2025-01-28 10:26:04","title":"Misspellings in Natural Language Processing: A survey","abstract":"This survey provides an overview of the challenges of misspellings in natural language processing (NLP). While often unintentional, misspellings have become ubiquitous in digital communication, especially with the proliferation of Web 2.0, user-generated content, and informal text mediums such as social media, blogs, and forums. Even if humans can generally interpret misspelled text, NLP models frequently struggle to handle it: this causes a decline in performance in common tasks like text classification and machine translation. In this paper, we reconstruct a history of misspellings as a scientific problem. We then discuss the latest advancements to address the challenge of misspellings in NLP. Main strategies to mitigate the effect of misspellings include data augmentation, double step, character-order agnostic, and tuple-based methods, among others. This survey also examines dedicated data challenges and competitions to spur progress in the field. Critical safety and ethical concerns are also examined, for example, the voluntary use of misspellings to inject malicious messages and hate speech on social networks. Furthermore, the survey explores psycholinguistic perspectives on how humans process misspellings, potentially informing innovative computational techniques for text normalization and representation. Finally, the misspelling-related challenges and opportunities associated with modern large language models are also analyzed, including benchmarks, datasets, and performances of the most prominent language models against misspellings. This survey aims to be an exhaustive resource for researchers seeking to mitigate the impact of misspellings in the rapidly evolving landscape of NLP.","sentences":["This survey provides an overview of the challenges of misspellings in natural language processing (NLP).","While often unintentional, misspellings have become ubiquitous in digital communication, especially with the proliferation of Web 2.0, user-generated content, and informal text mediums such as social media, blogs, and forums.","Even if humans can generally interpret misspelled text, NLP models frequently struggle to handle it: this causes a decline in performance in common tasks like text classification and machine translation.","In this paper, we reconstruct a history of misspellings as a scientific problem.","We then discuss the latest advancements to address the challenge of misspellings in NLP.","Main strategies to mitigate the effect of misspellings include data augmentation, double step, character-order agnostic, and tuple-based methods, among others.","This survey also examines dedicated data challenges and competitions to spur progress in the field.","Critical safety and ethical concerns are also examined, for example, the voluntary use of misspellings to inject malicious messages and hate speech on social networks.","Furthermore, the survey explores psycholinguistic perspectives on how humans process misspellings, potentially informing innovative computational techniques for text normalization and representation.","Finally, the misspelling-related challenges and opportunities associated with modern large language models are also analyzed, including benchmarks, datasets, and performances of the most prominent language models against misspellings.","This survey aims to be an exhaustive resource for researchers seeking to mitigate the impact of misspellings in the rapidly evolving landscape of NLP."],"url":"http://arxiv.org/abs/2501.16836v1"}
{"created":"2025-01-28 10:21:49","title":"Data-Driven vs Traditional Approaches to Power Transformer's Top-Oil Temperature Estimation","abstract":"Power transformers are subjected to electrical currents and temperature fluctuations that, if not properly controlled, can lead to major deterioration of their insulation system. Therefore, monitoring the temperature of a power transformer is fundamental to ensure a long-term operational life. Models presented in the IEC 60076-7 and IEEE standards, for example, monitor the temperature by calculating the top-oil and the hot-spot temperatures. However, these models are not very accurate and rely on the power transformers' properties. This paper focuses on finding an alternative method to predict the top-oil temperatures given previous measurements. Given the large quantities of data available, machine learning methods for time series forecasting are analyzed and compared to the real measurements and the corresponding prediction of the IEC standard. The methods tested are Artificial Neural Networks (ANNs), Time-series Dense Encoder (TiDE), and Temporal Convolutional Networks (TCN) using different combinations of historical measurements. Each of these methods outperformed the IEC 60076-7 model and they are extended to estimate the temperature rise over ambient. To enhance prediction reliability, we explore the application of quantile regression to construct prediction intervals for the expected top-oil temperature ranges. The best-performing model successfully estimates conditional quantiles that provide sufficient coverage.","sentences":["Power transformers are subjected to electrical currents and temperature fluctuations that, if not properly controlled, can lead to major deterioration of their insulation system.","Therefore, monitoring the temperature of a power transformer is fundamental to ensure a long-term operational life.","Models presented in the IEC 60076-7 and IEEE standards, for example, monitor the temperature by calculating the top-oil and the hot-spot temperatures.","However, these models are not very accurate and rely on the power transformers' properties.","This paper focuses on finding an alternative method to predict the top-oil temperatures given previous measurements.","Given the large quantities of data available, machine learning methods for time series forecasting are analyzed and compared to the real measurements and the corresponding prediction of the IEC standard.","The methods tested are Artificial Neural Networks (ANNs), Time-series Dense Encoder (TiDE), and Temporal Convolutional Networks (TCN) using different combinations of historical measurements.","Each of these methods outperformed the IEC 60076-7 model and they are extended to estimate the temperature rise over ambient.","To enhance prediction reliability, we explore the application of quantile regression to construct prediction intervals for the expected top-oil temperature ranges.","The best-performing model successfully estimates conditional quantiles that provide sufficient coverage."],"url":"http://arxiv.org/abs/2501.16831v1"}
{"created":"2025-01-28 10:21:37","title":"Statistical Analysis of Risk Assessment Factors and Metrics to Evaluate Radicalisation in Twitter","abstract":"Nowadays, Social Networks have become an essential communication tools producing a large amount of information about their users and their interactions, which can be analysed with Data Mining methods. In the last years, Social Networks are being used to radicalise people. In this paper, we study the performance of a set of indicators and their respective metrics, devoted to assess the risk of radicalisation of a precise individual on three different datasets. Keyword-based metrics, even though depending on the written language, performs well when measuring frustration, perception of discrimination as well as declaration of negative and positive ideas about Western society and Jihadism, respectively. However, metrics based on frequent habits such as writing ellipses are not well enough to characterise a user in risk of radicalisation. The paper presents a detailed description of both, the set of indicators used to asses the radicalisation in Social Networks and the set of datasets used to evaluate them. Finally, an experimental study over these datasets are carried out to evaluate the performance of the metrics considered.","sentences":["Nowadays, Social Networks have become an essential communication tools producing a large amount of information about their users and their interactions, which can be analysed with Data Mining methods.","In the last years, Social Networks are being used to radicalise people.","In this paper, we study the performance of a set of indicators and their respective metrics, devoted to assess the risk of radicalisation of a precise individual on three different datasets.","Keyword-based metrics, even though depending on the written language, performs well when measuring frustration, perception of discrimination as well as declaration of negative and positive ideas about Western society and Jihadism, respectively.","However, metrics based on frequent habits such as writing ellipses are not well enough to characterise a user in risk of radicalisation.","The paper presents a detailed description of both, the set of indicators used to asses the radicalisation in Social Networks and the set of datasets used to evaluate them.","Finally, an experimental study over these datasets are carried out to evaluate the performance of the metrics considered."],"url":"http://arxiv.org/abs/2501.16830v1"}
{"created":"2025-01-28 09:30:29","title":"Whispers of Sound-Enhancing Information Extraction from Depression Patients' Unstructured Data through Audio and Text Emotion Recognition and Llama Fine-tuning","abstract":"This study proposes an innovative multimodal fusion model based on a teacher-student architecture to enhance the accuracy of depression classification. Our designed model addresses the limitations of traditional methods in feature fusion and modality weight allocation by introducing multi-head attention mechanisms and weighted multimodal transfer learning. Leveraging the DAIC-WOZ dataset, the student fusion model, guided by textual and auditory teacher models, achieves significant improvements in classification accuracy. Ablation experiments demonstrate that the proposed model attains an F1 score of 99. 1% on the test set, significantly outperforming unimodal and conventional approaches. Our method effectively captures the complementarity between textual and audio features while dynamically adjusting the contributions of the teacher models to enhance generalization capabilities. The experimental results highlight the robustness and adaptability of the proposed framework in handling complex multimodal data. This research provides a novel technical framework for multimodal large model learning in depression analysis, offering new insights into addressing the limitations of existing methods in modality fusion and feature extraction.","sentences":["This study proposes an innovative multimodal fusion model based on a teacher-student architecture to enhance the accuracy of depression classification.","Our designed model addresses the limitations of traditional methods in feature fusion and modality weight allocation by introducing multi-head attention mechanisms and weighted multimodal transfer learning.","Leveraging the DAIC-WOZ dataset, the student fusion model, guided by textual and auditory teacher models, achieves significant improvements in classification accuracy.","Ablation experiments demonstrate that the proposed model attains an F1 score of 99.","1% on the test set, significantly outperforming unimodal and conventional approaches.","Our method effectively captures the complementarity between textual and audio features while dynamically adjusting the contributions of the teacher models to enhance generalization capabilities.","The experimental results highlight the robustness and adaptability of the proposed framework in handling complex multimodal data.","This research provides a novel technical framework for multimodal large model learning in depression analysis, offering new insights into addressing the limitations of existing methods in modality fusion and feature extraction."],"url":"http://arxiv.org/abs/2501.16813v1"}
{"created":"2025-01-28 09:08:31","title":"RG-Attn: Radian Glue Attention for Multi-modality Multi-agent Cooperative Perception","abstract":"Cooperative perception offers an optimal solution to overcome the perception limitations of single-agent systems by leveraging Vehicle-to-Everything (V2X) communication for data sharing and fusion across multiple agents. However, most existing approaches focus on single-modality data exchange, limiting the potential of both homogeneous and heterogeneous fusion across agents. This overlooks the opportunity to utilize multi-modality data per agent, restricting the system's performance. In the automotive industry, manufacturers adopt diverse sensor configurations, resulting in heterogeneous combinations of sensor modalities across agents. To harness the potential of every possible data source for optimal performance, we design a robust LiDAR and camera cross-modality fusion module, Radian-Glue-Attention (RG-Attn), applicable to both intra-agent cross-modality fusion and inter-agent cross-modality fusion scenarios, owing to the convenient coordinate conversion by transformation matrix and the unified sampling/inversion mechanism. We also propose two different architectures, named Paint-To-Puzzle (PTP) and Co-Sketching-Co-Coloring (CoS-CoCo), for conducting cooperative perception. PTP aims for maximum precision performance and achieves smaller data packet size by limiting cross-agent fusion to a single instance, but requiring all participants to be equipped with LiDAR. In contrast, CoS-CoCo supports agents with any configuration-LiDAR-only, camera-only, or LiDAR-camera-both, presenting more generalization ability. Our approach achieves state-of-the-art (SOTA) performance on both real and simulated cooperative perception datasets. The code will be released at GitHub in early 2025.","sentences":["Cooperative perception offers an optimal solution to overcome the perception limitations of single-agent systems by leveraging Vehicle-to-Everything (V2X) communication for data sharing and fusion across multiple agents.","However, most existing approaches focus on single-modality data exchange, limiting the potential of both homogeneous and heterogeneous fusion across agents.","This overlooks the opportunity to utilize multi-modality data per agent, restricting the system's performance.","In the automotive industry, manufacturers adopt diverse sensor configurations, resulting in heterogeneous combinations of sensor modalities across agents.","To harness the potential of every possible data source for optimal performance, we design a robust LiDAR and camera cross-modality fusion module, Radian-Glue-Attention (RG-Attn), applicable to both intra-agent cross-modality fusion and inter-agent cross-modality fusion scenarios, owing to the convenient coordinate conversion by transformation matrix and the unified sampling/inversion mechanism.","We also propose two different architectures, named Paint-To-Puzzle (PTP) and Co-Sketching-Co-Coloring (CoS-CoCo), for conducting cooperative perception.","PTP aims for maximum precision performance and achieves smaller data packet size by limiting cross-agent fusion to a single instance, but requiring all participants to be equipped with LiDAR.","In contrast, CoS-CoCo supports agents with any configuration-LiDAR-only, camera-only, or LiDAR-camera-both, presenting more generalization ability.","Our approach achieves state-of-the-art (SOTA) performance on both real and simulated cooperative perception datasets.","The code will be released at GitHub in early 2025."],"url":"http://arxiv.org/abs/2501.16803v1"}
{"created":"2025-01-28 09:05:03","title":"DIRIGENt: End-To-End Robotic Imitation of Human Demonstrations Based on a Diffusion Model","abstract":"There has been substantial progress in humanoid robots, with new skills continuously being taught, ranging from navigation to manipulation. While these abilities may seem impressive, the teaching methods often remain inefficient. To enhance the process of teaching robots, we propose leveraging a mechanism effectively used by humans: teaching by demonstrating. In this paper, we introduce DIRIGENt (DIrect Robotic Imitation GENeration model), a novel end-to-end diffusion approach that directly generates joint values from observing human demonstrations, enabling a robot to imitate these actions without any existing mapping between it and humans. We create a dataset in which humans imitate a robot and then use this collected data to train a diffusion model that enables a robot to imitate humans. The following three aspects are the core of our contribution. First is our novel dataset with natural pairs between human and robot poses, allowing our approach to imitate humans accurately despite the gap between their anatomies. Second, the diffusion input to our model alleviates the challenge of redundant joint configurations, limiting the search space. And finally, our end-to-end architecture from perception to action leads to an improved learning capability. Through our experimental analysis, we show that combining these three aspects allows DIRIGENt to outperform existing state-of-the-art approaches in the field of generating joint values from RGB images.","sentences":["There has been substantial progress in humanoid robots, with new skills continuously being taught, ranging from navigation to manipulation.","While these abilities may seem impressive, the teaching methods often remain inefficient.","To enhance the process of teaching robots, we propose leveraging a mechanism effectively used by humans: teaching by demonstrating.","In this paper, we introduce DIRIGENt (DIrect Robotic Imitation GENeration model), a novel end-to-end diffusion approach that directly generates joint values from observing human demonstrations, enabling a robot to imitate these actions without any existing mapping between it and humans.","We create a dataset in which humans imitate a robot and then use this collected data to train a diffusion model that enables a robot to imitate humans.","The following three aspects are the core of our contribution.","First is our novel dataset with natural pairs between human and robot poses, allowing our approach to imitate humans accurately despite the gap between their anatomies.","Second, the diffusion input to our model alleviates the challenge of redundant joint configurations, limiting the search space.","And finally, our end-to-end architecture from perception to action leads to an improved learning capability.","Through our experimental analysis, we show that combining these three aspects allows DIRIGENt to outperform existing state-of-the-art approaches in the field of generating joint values from RGB images."],"url":"http://arxiv.org/abs/2501.16800v1"}
{"created":"2025-01-28 08:05:22","title":"AVE Speech Dataset: A Comprehensive Benchmark for Multi-Modal Speech Recognition Integrating Audio, Visual, and Electromyographic Signals","abstract":"The global aging population faces considerable challenges, particularly in communication, due to the prevalence of hearing and speech impairments. To address these, we introduce the AVE speech dataset, a comprehensive multi-modal benchmark for speech recognition tasks. The dataset includes a 100-sentence Mandarin Chinese corpus with audio signals, lip-region video recordings, and six-channel electromyography (EMG) data, collected from 100 participants. Each subject read the entire corpus ten times, with each sentence averaging approximately two seconds in duration, resulting in over 55 hours of multi-modal speech data per modality. Experiments demonstrate that combining these modalities significantly improves recognition performance, particularly in cross-subject and high-noise environments. To our knowledge, this is the first publicly available sentence-level dataset integrating these three modalities for large-scale Mandarin speech recognition. We expect this dataset to drive advancements in both acoustic and non-acoustic speech recognition research, enhancing cross-modal learning and human-machine interaction.","sentences":["The global aging population faces considerable challenges, particularly in communication, due to the prevalence of hearing and speech impairments.","To address these, we introduce the AVE speech dataset, a comprehensive multi-modal benchmark for speech recognition tasks.","The dataset includes a 100-sentence Mandarin Chinese corpus with audio signals, lip-region video recordings, and six-channel electromyography (EMG) data, collected from 100 participants.","Each subject read the entire corpus ten times, with each sentence averaging approximately two seconds in duration, resulting in over 55 hours of multi-modal speech data per modality.","Experiments demonstrate that combining these modalities significantly improves recognition performance, particularly in cross-subject and high-noise environments.","To our knowledge, this is the first publicly available sentence-level dataset integrating these three modalities for large-scale Mandarin speech recognition.","We expect this dataset to drive advancements in both acoustic and non-acoustic speech recognition research, enhancing cross-modal learning and human-machine interaction."],"url":"http://arxiv.org/abs/2501.16780v1"}
{"created":"2025-01-28 07:49:52","title":"Beyond-Labels: Advancing Open-Vocabulary Segmentation With Vision-Language Models","abstract":"Self-supervised learning can resolve numerous image or linguistic processing problems when effectively trained. This study investigated simple yet efficient methods for adaping previously learned foundation models for open-vocabulary semantic segmentation tasks. Our research proposed \"Beyond-Labels,\" a lightweight transformer-based fusion module that uses a handful of image segmentation data to fuse frozen image representations with language concepts. Furthermore, we efficiently captured positional information in images using Fourier embeddings, thus improving the generalization across various image sizes. Extensive ablation tests were performed to investigate the important components of our proposed method; when tested against the common benchmark PASCAL-5i, it demonstrated superior performance despite being trained on frozen image and language characteristics.","sentences":["Self-supervised learning can resolve numerous image or linguistic processing problems when effectively trained.","This study investigated simple yet efficient methods for adaping previously learned foundation models for open-vocabulary semantic segmentation tasks.","Our research proposed \"Beyond-Labels,\" a lightweight transformer-based fusion module that uses a handful of image segmentation data to fuse frozen image representations with language concepts.","Furthermore, we efficiently captured positional information in images using Fourier embeddings, thus improving the generalization across various image sizes.","Extensive ablation tests were performed to investigate the important components of our proposed method; when tested against the common benchmark PASCAL-5i, it demonstrated superior performance despite being trained on frozen image and language characteristics."],"url":"http://arxiv.org/abs/2501.16769v1"}
{"created":"2025-01-28 07:33:01","title":"Rate-Distortion under Neural Tracking of Speech: A Directed Redundancy Approach","abstract":"The data acquired at different scalp EEG electrodes when human subjects are exposed to speech stimuli are highly redundant. The redundancy is partly due to volume conduction effects and partly due to localized regions of the brain synchronizing their activity in response to the stimuli. In a competing talker scenario, we use a recent measure of directed redundancy to assess the amount of redundant information that is causally conveyed from the attended stimuli to the left temporal region of the brain. We observe that for the attended stimuli, the transfer entropy as well as the directed redundancy is proportional to the correlation between the speech stimuli and the reconstructed signal from the EEG signals.   This demonstrates that both the rate as well as the rate-redundancy are inversely proportional to the distortion in neural speech tracking. Thus, a greater rate indicates a greater redundancy between the electrode signals, and a greater correlation between the reconstructed signal and the attended stimuli. A similar relationship is not observed for the distracting stimuli.","sentences":["The data acquired at different scalp EEG electrodes when human subjects are exposed to speech stimuli are highly redundant.","The redundancy is partly due to volume conduction effects and partly due to localized regions of the brain synchronizing their activity in response to the stimuli.","In a competing talker scenario, we use a recent measure of directed redundancy to assess the amount of redundant information that is causally conveyed from the attended stimuli to the left temporal region of the brain.","We observe that for the attended stimuli, the transfer entropy as well as the directed redundancy is proportional to the correlation between the speech stimuli and the reconstructed signal from the EEG signals.   ","This demonstrates that both the rate as well as the rate-redundancy are inversely proportional to the distortion in neural speech tracking.","Thus, a greater rate indicates a greater redundancy between the electrode signals, and a greater correlation between the reconstructed signal and the attended stimuli.","A similar relationship is not observed for the distracting stimuli."],"url":"http://arxiv.org/abs/2501.16762v1"}
{"created":"2025-01-28 07:31:09","title":"AdaSemSeg: An Adaptive Few-shot Semantic Segmentation of Seismic Facies","abstract":"Automated interpretation of seismic images using deep learning methods is challenging because of the limited availability of training data. Few-shot learning is a suitable learning paradigm in such scenarios due to its ability to adapt to a new task with limited supervision (small training budget). Existing few-shot semantic segmentation (FSSS) methods fix the number of target classes. Therefore, they do not support joint training on multiple datasets varying in the number of classes. In the context of the interpretation of seismic facies, fixing the number of target classes inhibits the generalization capability of a model trained on one facies dataset to another, which is likely to have a different number of facies. To address this shortcoming, we propose a few-shot semantic segmentation method for interpreting seismic facies that can adapt to the varying number of facies across the dataset, dubbed the AdaSemSeg. In general, the backbone network of FSSS methods is initialized with the statistics learned from the ImageNet dataset for better performance. The lack of such a huge annotated dataset for seismic images motivates using a self-supervised algorithm on seismic datasets to initialize the backbone network. We have trained the AdaSemSeg on three public seismic facies datasets with different numbers of facies and evaluated the proposed method on multiple metrics. The performance of the AdaSemSeg on unseen datasets (not used in training) is better than the prototype-based few-shot method and baselines.","sentences":["Automated interpretation of seismic images using deep learning methods is challenging because of the limited availability of training data.","Few-shot learning is a suitable learning paradigm in such scenarios due to its ability to adapt to a new task with limited supervision (small training budget).","Existing few-shot semantic segmentation (FSSS) methods fix the number of target classes.","Therefore, they do not support joint training on multiple datasets varying in the number of classes.","In the context of the interpretation of seismic facies, fixing the number of target classes inhibits the generalization capability of a model trained on one facies dataset to another, which is likely to have a different number of facies.","To address this shortcoming, we propose a few-shot semantic segmentation method for interpreting seismic facies that can adapt to the varying number of facies across the dataset, dubbed the AdaSemSeg.","In general, the backbone network of FSSS methods is initialized with the statistics learned from the ImageNet dataset for better performance.","The lack of such a huge annotated dataset for seismic images motivates using a self-supervised algorithm on seismic datasets to initialize the backbone network.","We have trained the AdaSemSeg on three public seismic facies datasets with different numbers of facies and evaluated the proposed method on multiple metrics.","The performance of the AdaSemSeg on unseen datasets (not used in training) is better than the prototype-based few-shot method and baselines."],"url":"http://arxiv.org/abs/2501.16760v1"}
{"created":"2025-01-28 07:30:35","title":"Are Joins over LSM-trees Ready: Take RocksDB as an Example","abstract":"LSM-tree-based data stores are widely adopted in industries for their excellent performance. As data scales increase, disk-based join operations become indispensable yet costly for the database, making the selection of suitable join methods crucial for system optimization. Current LSM-based stores generally adhere to conventional relational database practices and support only a limited number of join methods. However, the LSM-tree delivers distinct read and write efficiency compared to the relational databases, which could accordingly impact the performance of various join methods. Therefore, it is necessary to reconsider the selection of join methods in this context to fully explore the potential of various join algorithms and index designs. In this work, we present a systematic study and an exhaustive benchmark for joins over LSM-trees. We define a configuration space for join methods, encompassing various join algorithms, secondary index types, and consistency strategies. We also summarize a theoretical analysis to evaluate the overhead of each join method for an in-depth understanding. Furthermore, we implement all join methods in the configuration space on a unified platform and compare their performance through extensive experiments. Our theoretical and experimental results yield several insights and takeaways tailored to joins in LSM-based stores that aid developers in choosing proper join methods based on their working conditions.","sentences":["LSM-tree-based data stores are widely adopted in industries for their excellent performance.","As data scales increase, disk-based join operations become indispensable yet costly for the database, making the selection of suitable join methods crucial for system optimization.","Current LSM-based stores generally adhere to conventional relational database practices and support only a limited number of join methods.","However, the LSM-tree delivers distinct read and write efficiency compared to the relational databases, which could accordingly impact the performance of various join methods.","Therefore, it is necessary to reconsider the selection of join methods in this context to fully explore the potential of various join algorithms and index designs.","In this work, we present a systematic study and an exhaustive benchmark for joins over LSM-trees.","We define a configuration space for join methods, encompassing various join algorithms, secondary index types, and consistency strategies.","We also summarize a theoretical analysis to evaluate the overhead of each join method for an in-depth understanding.","Furthermore, we implement all join methods in the configuration space on a unified platform and compare their performance through extensive experiments.","Our theoretical and experimental results yield several insights and takeaways tailored to joins in LSM-based stores that aid developers in choosing proper join methods based on their working conditions."],"url":"http://arxiv.org/abs/2501.16759v1"}
{"created":"2025-01-28 07:24:24","title":"Meta-Federated Learning: A Novel Approach for Real-Time Traffic Flow Management","abstract":"Efficient management of traffic flow in urban environments presents a significant challenge, exacerbated by dynamic changes and the sheer volume of data generated by modern transportation networks. Traditional centralized traffic management systems often struggle with scalability and privacy concerns, hindering their effectiveness. This paper introduces a novel approach by combining Federated Learning (FL) and Meta-Learning (ML) to create a decentralized, scalable, and adaptive traffic management system. Our approach, termed Meta-Federated Learning, leverages the distributed nature of FL to process data locally at the edge, thereby enhancing privacy and reducing latency. Simultaneously, ML enables the system to quickly adapt to new traffic conditions without the need for extensive retraining. We implement our model across a simulated network of smart traffic devices, demonstrating that Meta-Federated Learning significantly outperforms traditional models in terms of prediction accuracy and response time. Furthermore, our approach shows remarkable adaptability to sudden changes in traffic patterns, suggesting a scalable solution for real-time traffic management in smart cities. This study not only paves the way for more resilient urban traffic systems but also exemplifies the potential of integrated FL and ML in other real-world applications.","sentences":["Efficient management of traffic flow in urban environments presents a significant challenge, exacerbated by dynamic changes and the sheer volume of data generated by modern transportation networks.","Traditional centralized traffic management systems often struggle with scalability and privacy concerns, hindering their effectiveness.","This paper introduces a novel approach by combining Federated Learning (FL) and Meta-Learning (ML) to create a decentralized, scalable, and adaptive traffic management system.","Our approach, termed Meta-Federated Learning, leverages the distributed nature of FL to process data locally at the edge, thereby enhancing privacy and reducing latency.","Simultaneously, ML enables the system to quickly adapt to new traffic conditions without the need for extensive retraining.","We implement our model across a simulated network of smart traffic devices, demonstrating that Meta-Federated Learning significantly outperforms traditional models in terms of prediction accuracy and response time.","Furthermore, our approach shows remarkable adaptability to sudden changes in traffic patterns, suggesting a scalable solution for real-time traffic management in smart cities.","This study not only paves the way for more resilient urban traffic systems but also exemplifies the potential of integrated FL and ML in other real-world applications."],"url":"http://arxiv.org/abs/2501.16758v1"}
{"created":"2025-01-28 07:22:27","title":"Random Forest Calibration","abstract":"The Random Forest (RF) classifier is often claimed to be relatively well calibrated when compared with other machine learning methods. Moreover, the existing literature suggests that traditional calibration methods, such as isotonic regression, do not substantially enhance the calibration of RF probability estimates unless supplied with extensive calibration data sets, which can represent a significant obstacle in cases of limited data availability. Nevertheless, there seems to be no comprehensive study validating such claims and systematically comparing state-of-the-art calibration methods specifically for RF. To close this gap, we investigate a broad spectrum of calibration methods tailored to or at least applicable to RF, ranging from scaling techniques to more advanced algorithms. Our results based on synthetic as well as real-world data unravel the intricacies of RF probability estimates, scrutinize the impacts of hyper-parameters, compare calibration methods in a systematic way. We show that a well-optimized RF performs as well as or better than leading calibration approaches.","sentences":["The Random Forest (RF) classifier is often claimed to be relatively well calibrated when compared with other machine learning methods.","Moreover, the existing literature suggests that traditional calibration methods, such as isotonic regression, do not substantially enhance the calibration of RF probability estimates unless supplied with extensive calibration data sets, which can represent a significant obstacle in cases of limited data availability.","Nevertheless, there seems to be no comprehensive study validating such claims and systematically comparing state-of-the-art calibration methods specifically for RF.","To close this gap, we investigate a broad spectrum of calibration methods tailored to or at least applicable to RF, ranging from scaling techniques to more advanced algorithms.","Our results based on synthetic as well as real-world data unravel the intricacies of RF probability estimates, scrutinize the impacts of hyper-parameters, compare calibration methods in a systematic way.","We show that a well-optimized RF performs as well as or better than leading calibration approaches."],"url":"http://arxiv.org/abs/2501.16756v1"}
{"created":"2025-01-28 07:12:29","title":"Overcoming Semantic Dilution in Transformer-Based Next Frame Prediction","abstract":"Next-frame prediction in videos is crucial for applications such as autonomous driving, object tracking, and motion prediction. The primary challenge in next-frame prediction lies in effectively capturing and processing both spatial and temporal information from previous video sequences. The transformer architecture, known for its prowess in handling sequence data, has made remarkable progress in this domain. However, transformer-based next-frame prediction models face notable issues: (a) The multi-head self-attention (MHSA) mechanism requires the input embedding to be split into $N$ chunks, where $N$ is the number of heads. Each segment captures only a fraction of the original embeddings information, which distorts the representation of the embedding in the latent space, resulting in a semantic dilution problem; (b) These models predict the embeddings of the next frames rather than the frames themselves, but the loss function based on the errors of the reconstructed frames, not the predicted embeddings -- this creates a discrepancy between the training objective and the model output. We propose a Semantic Concentration Multi-Head Self-Attention (SCMHSA) architecture, which effectively mitigates semantic dilution in transformer-based next-frame prediction. Additionally, we introduce a loss function that optimizes SCMHSA in the latent space, aligning the training objective more closely with the model output. Our method demonstrates superior performance compared to the original transformer-based predictors.","sentences":["Next-frame prediction in videos is crucial for applications such as autonomous driving, object tracking, and motion prediction.","The primary challenge in next-frame prediction lies in effectively capturing and processing both spatial and temporal information from previous video sequences.","The transformer architecture, known for its prowess in handling sequence data, has made remarkable progress in this domain.","However, transformer-based next-frame prediction models face notable issues: (a) The multi-head self-attention (MHSA) mechanism requires the input embedding to be split into $N$ chunks, where $N$ is the number of heads.","Each segment captures only a fraction of the original embeddings information, which distorts the representation of the embedding in the latent space, resulting in a semantic dilution problem; (b) These models predict the embeddings of the next frames rather than the frames themselves, but the loss function based on the errors of the reconstructed frames, not the predicted embeddings -- this creates a discrepancy between the training objective and the model output.","We propose a Semantic Concentration Multi-Head Self-Attention (SCMHSA) architecture, which effectively mitigates semantic dilution in transformer-based next-frame prediction.","Additionally, we introduce a loss function that optimizes SCMHSA in the latent space, aligning the training objective more closely with the model output.","Our method demonstrates superior performance compared to the original transformer-based predictors."],"url":"http://arxiv.org/abs/2501.16753v1"}
{"created":"2025-01-28 07:08:20","title":"DebugAgent: Efficient and Interpretable Error Slice Discovery for Comprehensive Model Debugging","abstract":"Despite the significant success of deep learning models in computer vision, they often exhibit systematic failures on specific data subsets, known as error slices. Identifying and mitigating these error slices is crucial to enhancing model robustness and reliability in real-world scenarios. In this paper, we introduce DebugAgent, an automated framework for error slice discovery and model repair. DebugAgent first generates task-specific visual attributes to highlight instances prone to errors through an interpretable and structured process. It then employs an efficient slice enumeration algorithm to systematically identify error slices, overcoming the combinatorial challenges that arise during slice exploration. Additionally, DebugAgent extends its capabilities by predicting error slices beyond the validation set, addressing a key limitation of prior approaches. Extensive experiments across multiple domains, including image classification, pose estimation, and object detection - show that DebugAgent not only improves the coherence and precision of identified error slices but also significantly enhances the model repair capabilities.","sentences":["Despite the significant success of deep learning models in computer vision, they often exhibit systematic failures on specific data subsets, known as error slices.","Identifying and mitigating these error slices is crucial to enhancing model robustness and reliability in real-world scenarios.","In this paper, we introduce DebugAgent, an automated framework for error slice discovery and model repair.","DebugAgent first generates task-specific visual attributes to highlight instances prone to errors through an interpretable and structured process.","It then employs an efficient slice enumeration algorithm to systematically identify error slices, overcoming the combinatorial challenges that arise during slice exploration.","Additionally, DebugAgent extends its capabilities by predicting error slices beyond the validation set, addressing a key limitation of prior approaches.","Extensive experiments across multiple domains, including image classification, pose estimation, and object detection - show that DebugAgent not only improves the coherence and precision of identified error slices but also significantly enhances the model repair capabilities."],"url":"http://arxiv.org/abs/2501.16751v1"}
{"created":"2025-01-28 06:42:37","title":"Toward Relative Positional Encoding in Spiking Transformers","abstract":"Spiking neural networks (SNNs) are bio-inspired networks that model how neurons in the brain communicate through discrete spikes, which have great potential in various tasks due to their energy efficiency and temporal processing capabilities. SNNs with self-attention mechanisms (Spiking Transformers) have recently shown great advancements in various tasks such as sequential modeling and image classifications. However, integrating positional information, which is essential for capturing sequential relationships in data, remains a challenge in Spiking Transformers. In this paper, we introduce an approximate method for relative positional encoding (RPE) in Spiking Transformers, leveraging Gray Code as the foundation for our approach. We provide comprehensive proof of the method's effectiveness in partially capturing relative positional information for sequential tasks. Additionally, we extend our RPE approach by adapting it to a two-dimensional form suitable for image patch processing. We evaluate the proposed RPE methods on several tasks, including time series forecasting, text classification, and patch-based image classification. Our experimental results demonstrate that the incorporation of RPE significantly enhances performance by effectively capturing relative positional information.","sentences":["Spiking neural networks (SNNs) are bio-inspired networks that model how neurons in the brain communicate through discrete spikes, which have great potential in various tasks due to their energy efficiency and temporal processing capabilities.","SNNs with self-attention mechanisms (Spiking Transformers) have recently shown great advancements in various tasks such as sequential modeling and image classifications.","However, integrating positional information, which is essential for capturing sequential relationships in data, remains a challenge in Spiking Transformers.","In this paper, we introduce an approximate method for relative positional encoding (RPE) in Spiking Transformers, leveraging Gray Code as the foundation for our approach.","We provide comprehensive proof of the method's effectiveness in partially capturing relative positional information for sequential tasks.","Additionally, we extend our RPE approach by adapting it to a two-dimensional form suitable for image patch processing.","We evaluate the proposed RPE methods on several tasks, including time series forecasting, text classification, and patch-based image classification.","Our experimental results demonstrate that the incorporation of RPE significantly enhances performance by effectively capturing relative positional information."],"url":"http://arxiv.org/abs/2501.16745v1"}
{"created":"2025-01-28 06:41:37","title":"LLM Assisted Anomaly Detection Service for Site Reliability Engineers: Enhancing Cloud Infrastructure Resilience","abstract":"This paper introduces a scalable Anomaly Detection Service with a generalizable API tailored for industrial time-series data, designed to assist Site Reliability Engineers (SREs) in managing cloud infrastructure. The service enables efficient anomaly detection in complex data streams, supporting proactive identification and resolution of issues. Furthermore, it presents an innovative approach to anomaly modeling in cloud infrastructure by utilizing Large Language Models (LLMs) to understand key components, their failure modes, and behaviors. A suite of algorithms for detecting anomalies is offered in univariate and multivariate time series data, including regression-based, mixture-model-based, and semi-supervised approaches. We provide insights into the usage patterns of the service, with over 500 users and 200,000 API calls in a year. The service has been successfully applied in various industrial settings, including IoT-based AI applications. We have also evaluated our system on public anomaly benchmarks to show its effectiveness. By leveraging it, SREs can proactively identify potential issues before they escalate, reducing downtime and improving response times to incidents, ultimately enhancing the overall customer experience. We plan to extend the system to include time series foundation models, enabling zero-shot anomaly detection capabilities.","sentences":["This paper introduces a scalable Anomaly Detection Service with a generalizable API tailored for industrial time-series data, designed to assist Site Reliability Engineers (SREs) in managing cloud infrastructure.","The service enables efficient anomaly detection in complex data streams, supporting proactive identification and resolution of issues.","Furthermore, it presents an innovative approach to anomaly modeling in cloud infrastructure by utilizing Large Language Models (LLMs) to understand key components, their failure modes, and behaviors.","A suite of algorithms for detecting anomalies is offered in univariate and multivariate time series data, including regression-based, mixture-model-based, and semi-supervised approaches.","We provide insights into the usage patterns of the service, with over 500 users and 200,000 API calls in a year.","The service has been successfully applied in various industrial settings, including IoT-based AI applications.","We have also evaluated our system on public anomaly benchmarks to show its effectiveness.","By leveraging it, SREs can proactively identify potential issues before they escalate, reducing downtime and improving response times to incidents, ultimately enhancing the overall customer experience.","We plan to extend the system to include time series foundation models, enabling zero-shot anomaly detection capabilities."],"url":"http://arxiv.org/abs/2501.16744v1"}
{"created":"2025-01-28 06:12:24","title":"Optimizing Efficiency of Mixed Traffic through Reinforcement Learning: A Topology-Independent Approach and Benchmark","abstract":"This paper presents a mixed traffic control policy designed to optimize traffic efficiency across diverse road topologies, addressing issues of congestion prevalent in urban environments. A model-free reinforcement learning (RL) approach is developed to manage large-scale traffic flow, using data collected by autonomous vehicles to influence human-driven vehicles. A real-world mixed traffic control benchmark is also released, which includes 444 scenarios from 20 countries, representing a wide geographic distribution and covering a variety of scenarios and road topologies. This benchmark serves as a foundation for future research, providing a realistic simulation environment for the development of effective policies. Comprehensive experiments demonstrate the effectiveness and adaptability of the proposed method, achieving better performance than existing traffic control methods in both intersection and roundabout scenarios. To the best of our knowledge, this is the first project to introduce a real-world complex scenarios mixed traffic control benchmark. Videos and code of our work are available at https://sites.google.com/berkeley.edu/mixedtrafficplus/home","sentences":["This paper presents a mixed traffic control policy designed to optimize traffic efficiency across diverse road topologies, addressing issues of congestion prevalent in urban environments.","A model-free reinforcement learning (RL) approach is developed to manage large-scale traffic flow, using data collected by autonomous vehicles to influence human-driven vehicles.","A real-world mixed traffic control benchmark is also released, which includes 444 scenarios from 20 countries, representing a wide geographic distribution and covering a variety of scenarios and road topologies.","This benchmark serves as a foundation for future research, providing a realistic simulation environment for the development of effective policies.","Comprehensive experiments demonstrate the effectiveness and adaptability of the proposed method, achieving better performance than existing traffic control methods in both intersection and roundabout scenarios.","To the best of our knowledge, this is the first project to introduce a real-world complex scenarios mixed traffic control benchmark.","Videos and code of our work are available at https://sites.google.com/berkeley.edu/mixedtrafficplus/home"],"url":"http://arxiv.org/abs/2501.16728v1"}
{"created":"2025-01-28 05:50:54","title":"Outlier Synthesis via Hamiltonian Monte Carlo for Out-of-Distribution Detection","abstract":"Out-of-distribution (OOD) detection is crucial for developing trustworthy and reliable machine learning systems. Recent advances in training with auxiliary OOD data demonstrate efficacy in enhancing detection capabilities. Nonetheless, these methods heavily rely on acquiring a large pool of high-quality natural outliers. Some prior methods try to alleviate this problem by synthesizing virtual outliers but suffer from either poor quality or high cost due to the monotonous sampling strategy and the heavy-parameterized generative models. In this paper, we overcome all these problems by proposing the Hamiltonian Monte Carlo Outlier Synthesis (HamOS) framework, which views the synthesis process as sampling from Markov chains. Based solely on the in-distribution data, the Markov chains can extensively traverse the feature space and generate diverse and representative outliers, hence exposing the model to miscellaneous potential OOD scenarios. The Hamiltonian Monte Carlo with sampling acceptance rate almost close to 1 also makes our framework enjoy great efficiency. By empirically competing with SOTA baselines on both standard and large-scale benchmarks, we verify the efficacy and efficiency of our proposed HamOS.","sentences":["Out-of-distribution (OOD) detection is crucial for developing trustworthy and reliable machine learning systems.","Recent advances in training with auxiliary OOD data demonstrate efficacy in enhancing detection capabilities.","Nonetheless, these methods heavily rely on acquiring a large pool of high-quality natural outliers.","Some prior methods try to alleviate this problem by synthesizing virtual outliers but suffer from either poor quality or high cost due to the monotonous sampling strategy and the heavy-parameterized generative models.","In this paper, we overcome all these problems by proposing the Hamiltonian Monte Carlo Outlier Synthesis (HamOS) framework, which views the synthesis process as sampling from Markov chains.","Based solely on the in-distribution data, the Markov chains can extensively traverse the feature space and generate diverse and representative outliers, hence exposing the model to miscellaneous potential OOD scenarios.","The Hamiltonian Monte Carlo with sampling acceptance rate almost close to 1 also makes our framework enjoy great efficiency.","By empirically competing with SOTA baselines on both standard and large-scale benchmarks, we verify the efficacy and efficiency of our proposed HamOS."],"url":"http://arxiv.org/abs/2501.16718v1"}
{"created":"2025-01-28 05:49:35","title":"Strawberry Robotic Operation Interface: An Open-Source Device for Collecting Dexterous Manipulation Data in Robotic Strawberry Farming","abstract":"The strawberry farming is labor-intensive, particularly in tasks requiring dexterous manipulation such as picking occluded strawberries. To address this challenge, we present the Strawberry Robotic Operation Interface (SROI), an open-source device designed for collecting dexterous manipulation data in robotic strawberry farming. The SROI features a handheld unit with a modular end effector, a stereo robotic camera, enabling the easy collection of demonstration data in field environments. A data post-processing pipeline is introduced to extract spatial trajectories and gripper states from the collected data. Additionally, we release an open-source dataset of strawberry picking demonstrations to facilitate research in dexterous robotic manipulation. The SROI represents a step toward automating complex strawberry farming tasks, reducing reliance on manual labor.","sentences":["The strawberry farming is labor-intensive, particularly in tasks requiring dexterous manipulation such as picking occluded strawberries.","To address this challenge, we present the Strawberry Robotic Operation Interface (SROI), an open-source device designed for collecting dexterous manipulation data in robotic strawberry farming.","The SROI features a handheld unit with a modular end effector, a stereo robotic camera, enabling the easy collection of demonstration data in field environments.","A data post-processing pipeline is introduced to extract spatial trajectories and gripper states from the collected data.","Additionally, we release an open-source dataset of strawberry picking demonstrations to facilitate research in dexterous robotic manipulation.","The SROI represents a step toward automating complex strawberry farming tasks, reducing reliance on manual labor."],"url":"http://arxiv.org/abs/2501.16717v1"}
{"created":"2025-01-28 05:47:50","title":"Point Cloud Upsampling as Statistical Shape Model for Pelvic","abstract":"We propose a novel framework that integrates medical image segmentation and point cloud upsampling for accurate shape reconstruction of pelvic models. Using the SAM-Med3D model for segmentation and a point cloud upsampling network trained on the MedShapeNet dataset, our method transforms sparse medical imaging data into high-resolution 3D bone models. This framework leverages prior knowledge of anatomical shapes, achieving smoother and more complete reconstructions. Quantitative evaluations using metrics such as Chamfer Distance etc, demonstrate the effectiveness of the point cloud upsampling in pelvic model. Our approach offers potential applications in reconstructing other skeletal structures, providing a robust solution for medical image analysis and statistical shape modeling.","sentences":["We propose a novel framework that integrates medical image segmentation and point cloud upsampling for accurate shape reconstruction of pelvic models.","Using the SAM-Med3D model for segmentation and a point cloud upsampling network trained on the MedShapeNet dataset, our method transforms sparse medical imaging data into high-resolution 3D bone models.","This framework leverages prior knowledge of anatomical shapes, achieving smoother and more complete reconstructions.","Quantitative evaluations using metrics such as Chamfer Distance etc, demonstrate the effectiveness of the point cloud upsampling in pelvic model.","Our approach offers potential applications in reconstructing other skeletal structures, providing a robust solution for medical image analysis and statistical shape modeling."],"url":"http://arxiv.org/abs/2501.16716v1"}
{"created":"2025-01-28 04:33:28","title":"Determining Mosaic Resilience in Sugarcane Plants using Hyperspectral Images","abstract":"Sugarcane mosaic disease poses a serious threat to the Australian sugarcane industry, leading to yield losses of up to 30% in susceptible varieties. Existing manual inspection methods for detecting mosaic resilience are inefficient and impractical for large-scale application. This study introduces a novel approach using hyperspectral imaging and machine learning to detect mosaic resilience by leveraging global feature representation from local spectral patches. Hyperspectral data were collected from eight sugarcane varieties under controlled and field conditions. Local spectral patches were analyzed to capture spatial and spectral variations, which were then aggregated into global feature representations using a ResNet18 deep learning architecture. While classical methods like Support Vector Machines struggled to utilize spatial-spectral relationships effectively, the deep learning model achieved high classification accuracy, demonstrating its capacity to identify mosaic resilience from fine-grained hyperspectral data. This approach enhances early detection capabilities, enabling more efficient management of susceptible strains and contributing to sustainable sugarcane production.","sentences":["Sugarcane mosaic disease poses a serious threat to the Australian sugarcane industry, leading to yield losses of up to 30% in susceptible varieties.","Existing manual inspection methods for detecting mosaic resilience are inefficient and impractical for large-scale application.","This study introduces a novel approach using hyperspectral imaging and machine learning to detect mosaic resilience by leveraging global feature representation from local spectral patches.","Hyperspectral data were collected from eight sugarcane varieties under controlled and field conditions.","Local spectral patches were analyzed to capture spatial and spectral variations, which were then aggregated into global feature representations using a ResNet18 deep learning architecture.","While classical methods like Support Vector Machines struggled to utilize spatial-spectral relationships effectively, the deep learning model achieved high classification accuracy, demonstrating its capacity to identify mosaic resilience from fine-grained hyperspectral data.","This approach enhances early detection capabilities, enabling more efficient management of susceptible strains and contributing to sustainable sugarcane production."],"url":"http://arxiv.org/abs/2501.16700v1"}
{"created":"2025-01-28 04:31:19","title":"3D-MoE: A Mixture-of-Experts Multi-modal LLM for 3D Vision and Pose Diffusion via Rectified Flow","abstract":"3D vision and spatial reasoning have long been recognized as preferable for accurately perceiving our three-dimensional world, especially when compared with traditional visual reasoning based on 2D images. Due to the difficulties in collecting high-quality 3D data, research in this area has only recently gained momentum. With the advent of powerful large language models (LLMs), multi-modal LLMs for 3D vision have been developed over the past few years. However, most of these models focus primarily on the vision encoder for 3D data. In this paper, we propose converting existing densely activated LLMs into mixture-of-experts (MoE) models, which have proven effective for multi-modal data processing. In addition to leveraging these models' instruction-following capabilities, we further enable embodied task planning by attaching a diffusion head, Pose-DiT, that employs a novel rectified flow diffusion scheduler. Experimental results on 3D question answering and task-planning tasks demonstrate that our 3D-MoE framework achieves improved performance with fewer activated parameters.","sentences":["3D vision and spatial reasoning have long been recognized as preferable for accurately perceiving our three-dimensional world, especially when compared with traditional visual reasoning based on 2D images.","Due to the difficulties in collecting high-quality 3D data, research in this area has only recently gained momentum.","With the advent of powerful large language models (LLMs), multi-modal LLMs for 3D vision have been developed over the past few years.","However, most of these models focus primarily on the vision encoder for 3D data.","In this paper, we propose converting existing densely activated LLMs into mixture-of-experts (MoE) models, which have proven effective for multi-modal data processing.","In addition to leveraging these models' instruction-following capabilities, we further enable embodied task planning by attaching a diffusion head, Pose-DiT, that employs a novel rectified flow diffusion scheduler.","Experimental results on 3D question answering and task-planning tasks demonstrate that our 3D-MoE framework achieves improved performance with fewer activated parameters."],"url":"http://arxiv.org/abs/2501.16698v1"}
{"created":"2025-01-28 03:56:17","title":"MME-Industry: A Cross-Industry Multimodal Evaluation Benchmark","abstract":"With the rapid advancement of Multimodal Large Language Models (MLLMs), numerous evaluation benchmarks have emerged. However, comprehensive assessments of their performance across diverse industrial applications remain limited. In this paper, we introduce MME-Industry, a novel benchmark designed specifically for evaluating MLLMs in industrial settings.The benchmark encompasses 21 distinct domain, comprising 1050 question-answer pairs with 50 questions per domain. To ensure data integrity and prevent potential leakage from public datasets, all question-answer pairs were manually crafted and validated by domain experts. Besides, the benchmark's complexity is effectively enhanced by incorporating non-OCR questions that can be answered directly, along with tasks requiring specialized domain knowledge. Moreover, we provide both Chinese and English versions of the benchmark, enabling comparative analysis of MLLMs' capabilities across these languages. Our findings contribute valuable insights into MLLMs' practical industrial applications and illuminate promising directions for future model optimization research.","sentences":["With the rapid advancement of Multimodal Large Language Models (MLLMs), numerous evaluation benchmarks have emerged.","However, comprehensive assessments of their performance across diverse industrial applications remain limited.","In this paper, we introduce MME-Industry, a novel benchmark designed specifically for evaluating MLLMs in industrial settings.","The benchmark encompasses 21 distinct domain, comprising 1050 question-answer pairs with 50 questions per domain.","To ensure data integrity and prevent potential leakage from public datasets, all question-answer pairs were manually crafted and validated by domain experts.","Besides, the benchmark's complexity is effectively enhanced by incorporating non-OCR questions that can be answered directly, along with tasks requiring specialized domain knowledge.","Moreover, we provide both Chinese and English versions of the benchmark, enabling comparative analysis of MLLMs' capabilities across these languages.","Our findings contribute valuable insights into MLLMs' practical industrial applications and illuminate promising directions for future model optimization research."],"url":"http://arxiv.org/abs/2501.16688v1"}
{"created":"2025-01-28 03:29:00","title":"Differentially Private Set Representations","abstract":"We study the problem of differentially private (DP) mechanisms for representing sets of size $k$ from a large universe. Our first construction creates $(\\epsilon,\\delta)$-DP representations with error probability of $1/(e^\\epsilon + 1)$ using space at most $1.05 k \\epsilon \\cdot \\log(e)$ bits where the time to construct a representation is $O(k \\log(1/\\delta))$ while decoding time is $O(\\log(1/\\delta))$. We also present a second algorithm for pure $\\epsilon$-DP representations with the same error using space at most $k \\epsilon \\cdot \\log(e)$ bits, but requiring large decoding times. Our algorithms match our lower bounds on privacy-utility trade-offs (including constants but ignoring $\\delta$ factors) and we also present a new space lower bound matching our constructions up to small constant factors. To obtain our results, we design a new approach embedding sets into random linear systems deviating from most prior approaches that inject noise into non-private solutions.","sentences":["We study the problem of differentially private (DP) mechanisms for representing sets of size $k$ from a large universe.","Our first construction creates $(\\epsilon,\\delta)$-DP representations with error probability of $1/(e^\\epsilon + 1)$ using space at most $1.05 k \\epsilon \\cdot \\log(e)$ bits where the time to construct a representation is $O(k \\log(1/\\delta))$ while decoding time is $O(\\log(1/\\delta))$. We also present a second algorithm for pure $\\epsilon$-DP representations with the same error using space at most $k \\epsilon \\cdot \\log(e)$ bits, but requiring large decoding times.","Our algorithms match our lower bounds on privacy-utility trade-offs (including constants but ignoring $\\delta$ factors) and we also present a new space lower bound matching our constructions up to small constant factors.","To obtain our results, we design a new approach embedding sets into random linear systems deviating from most prior approaches that inject noise into non-private solutions."],"url":"http://arxiv.org/abs/2501.16680v1"}
{"created":"2025-01-28 03:18:48","title":"Auto-Differentiating Any LLM Workflow: A Farewell to Manual Prompting","abstract":"Large Language Models (LLMs) have reshaped natural language processing, powering applications from multi-hop retrieval and question answering to autonomous agent workflows. Yet, prompt engineering -- the task of crafting textual inputs to effectively direct LLMs -- remains difficult and labor-intensive, particularly for complex pipelines that combine multiple LLM calls with functional operations like retrieval and data formatting. We introduce LLM-AutoDiff: a novel framework for Automatic Prompt Engineering (APE) that extends textual gradient-based methods (such as Text-Grad) to multi-component, potentially cyclic LLM architectures. Implemented within the AdalFlow library, LLM-AutoDiff treats each textual input as a trainable parameter and uses a frozen backward engine LLM to generate feedback-akin to textual gradients -- that guide iterative prompt updates. Unlike prior single-node approaches, LLM-AutoDiff inherently accommodates functional nodes, preserves time-sequential behavior in repeated calls (e.g., multi-hop loops), and combats the \"lost-in-the-middle\" problem by isolating distinct sub-prompts (instructions, formats, or few-shot examples). It further boosts training efficiency by focusing on error-prone samples through selective gradient computation. Across diverse tasks, including single-step classification, multi-hop retrieval-based QA, and agent-driven pipelines, LLM-AutoDiff consistently outperforms existing textual gradient baselines in both accuracy and training cost. By unifying prompt optimization through a graph-centric lens, LLM-AutoDiff offers a powerful new paradigm for scaling and automating LLM workflows - mirroring the transformative role that automatic differentiation libraries have long played in neural network research.","sentences":["Large Language Models (LLMs) have reshaped natural language processing, powering applications from multi-hop retrieval and question answering to autonomous agent workflows.","Yet, prompt engineering -- the task of crafting textual inputs to effectively direct LLMs -- remains difficult and labor-intensive, particularly for complex pipelines that combine multiple LLM calls with functional operations like retrieval and data formatting.","We introduce LLM-AutoDiff: a novel framework for Automatic Prompt Engineering (APE) that extends textual gradient-based methods (such as Text-Grad) to multi-component, potentially cyclic LLM architectures.","Implemented within the AdalFlow library, LLM-AutoDiff treats each textual input as a trainable parameter and uses a frozen backward engine LLM to generate feedback-akin to textual gradients -- that guide iterative prompt updates.","Unlike prior single-node approaches, LLM-AutoDiff inherently accommodates functional nodes, preserves time-sequential behavior in repeated calls (e.g., multi-hop loops), and combats the \"lost-in-the-middle\" problem by isolating distinct sub-prompts (instructions, formats, or few-shot examples).","It further boosts training efficiency by focusing on error-prone samples through selective gradient computation.","Across diverse tasks, including single-step classification, multi-hop retrieval-based QA, and agent-driven pipelines, LLM-AutoDiff consistently outperforms existing textual gradient baselines in both accuracy and training cost.","By unifying prompt optimization through a graph-centric lens, LLM-AutoDiff offers a powerful new paradigm for scaling and automating LLM workflows - mirroring the transformative role that automatic differentiation libraries have long played in neural network research."],"url":"http://arxiv.org/abs/2501.16673v1"}
{"created":"2025-01-28 03:12:57","title":"Data-Free Model-Related Attacks: Unleashing the Potential of Generative AI","abstract":"Generative AI technology has become increasingly integrated into our daily lives, offering powerful capabilities to enhance productivity. However, these same capabilities can be exploited by adversaries for malicious purposes. While existing research on adversarial applications of generative AI predominantly focuses on cyberattacks, less attention has been given to attacks targeting deep learning models. In this paper, we introduce the use of generative AI for facilitating model-related attacks, including model extraction, membership inference, and model inversion. Our study reveals that adversaries can launch a variety of model-related attacks against both image and text models in a data-free and black-box manner, achieving comparable performance to baseline methods that have access to the target models' training data and parameters in a white-box manner. This research serves as an important early warning to the community about the potential risks associated with generative AI-powered attacks on deep learning models.","sentences":["Generative AI technology has become increasingly integrated into our daily lives, offering powerful capabilities to enhance productivity.","However, these same capabilities can be exploited by adversaries for malicious purposes.","While existing research on adversarial applications of generative AI predominantly focuses on cyberattacks, less attention has been given to attacks targeting deep learning models.","In this paper, we introduce the use of generative AI for facilitating model-related attacks, including model extraction, membership inference, and model inversion.","Our study reveals that adversaries can launch a variety of model-related attacks against both image and text models in a data-free and black-box manner, achieving comparable performance to baseline methods that have access to the target models' training data and parameters in a white-box manner.","This research serves as an important early warning to the community about the potential risks associated with generative AI-powered attacks on deep learning models."],"url":"http://arxiv.org/abs/2501.16671v1"}
{"created":"2025-01-28 02:52:51","title":"Data Duplication: A Novel Multi-Purpose Attack Paradigm in Machine Unlearning","abstract":"Duplication is a prevalent issue within datasets. Existing research has demonstrated that the presence of duplicated data in training datasets can significantly influence both model performance and data privacy. However, the impact of data duplication on the unlearning process remains largely unexplored. This paper addresses this gap by pioneering a comprehensive investigation into the role of data duplication, not only in standard machine unlearning but also in federated and reinforcement unlearning paradigms. Specifically, we propose an adversary who duplicates a subset of the target model's training set and incorporates it into the training set. After training, the adversary requests the model owner to unlearn this duplicated subset, and analyzes the impact on the unlearned model. For example, the adversary can challenge the model owner by revealing that, despite efforts to unlearn it, the influence of the duplicated subset remains in the model. Moreover, to circumvent detection by de-duplication techniques, we propose three novel near-duplication methods for the adversary, each tailored to a specific unlearning paradigm. We then examine their impacts on the unlearning process when de-duplication techniques are applied. Our findings reveal several crucial insights: 1) the gold standard unlearning method, retraining from scratch, fails to effectively conduct unlearning under certain conditions; 2) unlearning duplicated data can lead to significant model degradation in specific scenarios; and 3) meticulously crafted duplicates can evade detection by de-duplication methods.","sentences":["Duplication is a prevalent issue within datasets.","Existing research has demonstrated that the presence of duplicated data in training datasets can significantly influence both model performance and data privacy.","However, the impact of data duplication on the unlearning process remains largely unexplored.","This paper addresses this gap by pioneering a comprehensive investigation into the role of data duplication, not only in standard machine unlearning but also in federated and reinforcement unlearning paradigms.","Specifically, we propose an adversary who duplicates a subset of the target model's training set and incorporates it into the training set.","After training, the adversary requests the model owner to unlearn this duplicated subset, and analyzes the impact on the unlearned model.","For example, the adversary can challenge the model owner by revealing that, despite efforts to unlearn it, the influence of the duplicated subset remains in the model.","Moreover, to circumvent detection by de-duplication techniques, we propose three novel near-duplication methods for the adversary, each tailored to a specific unlearning paradigm.","We then examine their impacts on the unlearning process when de-duplication techniques are applied.","Our findings reveal several crucial insights: 1) the gold standard unlearning method, retraining from scratch, fails to effectively conduct unlearning under certain conditions; 2) unlearning duplicated data can lead to significant model degradation in specific scenarios; and 3) meticulously crafted duplicates can evade detection by de-duplication methods."],"url":"http://arxiv.org/abs/2501.16663v1"}
{"created":"2025-01-28 02:52:04","title":"Vision-based autonomous structural damage detection using data-driven methods","abstract":"This study addresses the urgent need for efficient and accurate damage detection in wind turbine structures, a crucial component of renewable energy infrastructure. Traditional inspection methods, such as manual assessments and non-destructive testing (NDT), are often costly, time-consuming, and prone to human error. To tackle these challenges, this research investigates advanced deep learning algorithms for vision-based structural health monitoring (SHM). A dataset of wind turbine surface images, featuring various damage types and pollution, was prepared and augmented for enhanced model training. Three algorithms-YOLOv7, its lightweight variant, and Faster R-CNN- were employed to detect and classify surface damage. The models were trained and evaluated on a dataset split into training, testing, and evaluation subsets (80%-10%-10%). Results indicate that YOLOv7 outperformed the others, achieving 82.4% mAP@50 and high processing speed, making it suitable for real-time inspections. By optimizing hyperparameters like learning rate and batch size, the models' accuracy and efficiency improved further. YOLOv7 demonstrated significant advancements in detection precision and execution speed, especially for real-time applications. However, challenges such as dataset limitations and environmental variability were noted, suggesting future work on segmentation methods and larger datasets. This research underscores the potential of vision-based deep learning techniques to transform SHM practices by reducing costs, enhancing safety, and improving reliability, thus contributing to the sustainable maintenance of critical infrastructure and supporting the longevity of wind energy systems.","sentences":["This study addresses the urgent need for efficient and accurate damage detection in wind turbine structures, a crucial component of renewable energy infrastructure.","Traditional inspection methods, such as manual assessments and non-destructive testing (NDT), are often costly, time-consuming, and prone to human error.","To tackle these challenges, this research investigates advanced deep learning algorithms for vision-based structural health monitoring (SHM).","A dataset of wind turbine surface images, featuring various damage types and pollution, was prepared and augmented for enhanced model training.","Three algorithms-YOLOv7, its lightweight variant, and Faster R-CNN- were employed to detect and classify surface damage.","The models were trained and evaluated on a dataset split into training, testing, and evaluation subsets (80%-10%-10%).","Results indicate that YOLOv7 outperformed the others, achieving 82.4% mAP@50 and high processing speed, making it suitable for real-time inspections.","By optimizing hyperparameters like learning rate and batch size, the models' accuracy and efficiency improved further.","YOLOv7 demonstrated significant advancements in detection precision and execution speed, especially for real-time applications.","However, challenges such as dataset limitations and environmental variability were noted, suggesting future work on segmentation methods and larger datasets.","This research underscores the potential of vision-based deep learning techniques to transform SHM practices by reducing costs, enhancing safety, and improving reliability, thus contributing to the sustainable maintenance of critical infrastructure and supporting the longevity of wind energy systems."],"url":"http://arxiv.org/abs/2501.16662v1"}
{"created":"2025-01-28 02:49:45","title":"Jupybara: Operationalizing a Design Space for Actionable Data Analysis and Storytelling with LLMs","abstract":"Mining and conveying actionable insights from complex data is a key challenge of exploratory data analysis (EDA) and storytelling. To address this challenge, we present a design space for actionable EDA and storytelling. Synthesizing theory and expert interviews, we highlight how semantic precision, rhetorical persuasion, and pragmatic relevance underpin effective EDA and storytelling. We also show how this design space subsumes common challenges in actionable EDA and storytelling, such as identifying appropriate analytical strategies and leveraging relevant domain knowledge. Building on the potential of LLMs to generate coherent narratives with commonsense reasoning, we contribute Jupybara, an AI-enabled assistant for actionable EDA and storytelling implemented as a Jupyter Notebook extension. Jupybara employs two strategies -- design-space-aware prompting and multi-agent architectures -- to operationalize our design space. An expert evaluation confirms Jupybara's usability, steerability, explainability, and reparability, as well as the effectiveness of our strategies in operationalizing the design space framework with LLMs.","sentences":["Mining and conveying actionable insights from complex data is a key challenge of exploratory data analysis (EDA) and storytelling.","To address this challenge, we present a design space for actionable EDA and storytelling.","Synthesizing theory and expert interviews, we highlight how semantic precision, rhetorical persuasion, and pragmatic relevance underpin effective EDA and storytelling.","We also show how this design space subsumes common challenges in actionable EDA and storytelling, such as identifying appropriate analytical strategies and leveraging relevant domain knowledge.","Building on the potential of LLMs to generate coherent narratives with commonsense reasoning, we contribute Jupybara, an AI-enabled assistant for actionable EDA and storytelling implemented as a Jupyter Notebook extension.","Jupybara employs two strategies -- design-space-aware prompting and multi-agent architectures -- to operationalize our design space.","An expert evaluation confirms Jupybara's usability, steerability, explainability, and reparability, as well as the effectiveness of our strategies in operationalizing the design space framework with LLMs."],"url":"http://arxiv.org/abs/2501.16661v1"}
{"created":"2025-01-28 02:44:31","title":"Contextual Reinforcement in Multimodal Token Compression for Large Language Models","abstract":"Effective token compression remains a critical challenge for scaling models to handle increasingly complex and diverse datasets. A novel mechanism based on contextual reinforcement is introduced, dynamically adjusting token importance through interdependencies and semantic relevance. This approach enables substantial reductions in token usage while preserving the quality and coherence of information representation. Incorporating graph-based algorithms and adaptive weighting, the method captures subtle contextual relationships across textual and multimodal data, ensuring robust alignment and performance in downstream tasks. Evaluations across varied domains reveal significant improvements in accuracy and semantic retention, particularly for tasks requiring detailed cross-modal interactions. Memory usage analyses demonstrate improved computational efficiency, with minimal overhead despite the additional reinforcement processes. Performance gains are further validated through error distribution analyses, showing reduced semantic loss and syntactic inconsistencies compared to baseline models. The modular architecture ensures compatibility with a wide range of open-source frameworks, facilitating scalable implementation for real-world applications. These findings highlight the potential of contextual reinforcement in redefining token management strategies and advancing large-scale model design.","sentences":["Effective token compression remains a critical challenge for scaling models to handle increasingly complex and diverse datasets.","A novel mechanism based on contextual reinforcement is introduced, dynamically adjusting token importance through interdependencies and semantic relevance.","This approach enables substantial reductions in token usage while preserving the quality and coherence of information representation.","Incorporating graph-based algorithms and adaptive weighting, the method captures subtle contextual relationships across textual and multimodal data, ensuring robust alignment and performance in downstream tasks.","Evaluations across varied domains reveal significant improvements in accuracy and semantic retention, particularly for tasks requiring detailed cross-modal interactions.","Memory usage analyses demonstrate improved computational efficiency, with minimal overhead despite the additional reinforcement processes.","Performance gains are further validated through error distribution analyses, showing reduced semantic loss and syntactic inconsistencies compared to baseline models.","The modular architecture ensures compatibility with a wide range of open-source frameworks, facilitating scalable implementation for real-world applications.","These findings highlight the potential of contextual reinforcement in redefining token management strategies and advancing large-scale model design."],"url":"http://arxiv.org/abs/2501.16658v1"}
{"created":"2025-01-28 02:41:43","title":"Data Mining in Transportation Networks with Graph Neural Networks: A Review and Outlook","abstract":"Data mining in transportation networks (DMTNs) refers to using diverse types of spatio-temporal data for various transportation tasks, including pattern analysis, traffic prediction, and traffic controls. Graph neural networks (GNNs) are essential in many DMTN problems due to their capability to represent spatial correlations between entities. Between 2016 and 2024, the notable applications of GNNs in DMTNs have extended to multiple fields such as traffic prediction and operation. However, existing reviews have primarily focused on traffic prediction tasks. To fill this gap, this study provides a timely and insightful summary of GNNs in DMTNs, highlighting new progress in prediction and operation from academic and industry perspectives since 2023. First, we present and analyze various DMTN problems, followed by classical and recent GNN models. Second, we delve into key works in three areas: (1) traffic prediction, (2) traffic operation, and (3) industry involvement, such as Google Maps, Amap, and Baidu Maps. Along these directions, we discuss new research opportunities based on the significance of transportation problems and data availability. Finally, we compile resources such as data, code, and other learning materials to foster interdisciplinary communication. This review, driven by recent trends in GNNs in DMTN studies since 2023, could democratize abundant datasets and efficient GNN methods for various transportation problems including prediction and operation.","sentences":["Data mining in transportation networks (DMTNs) refers to using diverse types of spatio-temporal data for various transportation tasks, including pattern analysis, traffic prediction, and traffic controls.","Graph neural networks (GNNs) are essential in many DMTN problems due to their capability to represent spatial correlations between entities.","Between 2016 and 2024, the notable applications of GNNs in DMTNs have extended to multiple fields such as traffic prediction and operation.","However, existing reviews have primarily focused on traffic prediction tasks.","To fill this gap, this study provides a timely and insightful summary of GNNs in DMTNs, highlighting new progress in prediction and operation from academic and industry perspectives since 2023.","First, we present and analyze various DMTN problems, followed by classical and recent GNN models.","Second, we delve into key works in three areas: (1) traffic prediction, (2) traffic operation, and (3) industry involvement, such as Google Maps, Amap, and Baidu Maps.","Along these directions, we discuss new research opportunities based on the significance of transportation problems and data availability.","Finally, we compile resources such as data, code, and other learning materials to foster interdisciplinary communication.","This review, driven by recent trends in GNNs in DMTN studies since 2023, could democratize abundant datasets and efficient GNN methods for various transportation problems including prediction and operation."],"url":"http://arxiv.org/abs/2501.16656v1"}
{"created":"2025-01-28 02:35:02","title":"Molecular-driven Foundation Model for Oncologic Pathology","abstract":"Foundation models are reshaping computational pathology by enabling transfer learning, where models pre-trained on vast datasets can be adapted for downstream diagnostic, prognostic, and therapeutic response tasks. Despite these advances, foundation models are still limited in their ability to encode the entire gigapixel whole-slide images without additional training and often lack complementary multimodal data. Here, we introduce Threads, a slide-level foundation model capable of generating universal representations of whole-slide images of any size. Threads was pre-trained using a multimodal learning approach on a diverse cohort of 47,171 hematoxylin and eosin (H&E)-stained tissue sections, paired with corresponding genomic and transcriptomic profiles - the largest such paired dataset to be used for foundation model development to date. This unique training paradigm enables Threads to capture the tissue's underlying molecular composition, yielding powerful representations applicable to a wide array of downstream tasks. In extensive benchmarking across 54 oncology tasks, including clinical subtyping, grading, mutation prediction, immunohistochemistry status determination, treatment response prediction, and survival prediction, Threads outperformed all baselines while demonstrating remarkable generalizability and label efficiency. It is particularly well suited for predicting rare events, further emphasizing its clinical utility. We intend to make the model publicly available for the broader community.","sentences":["Foundation models are reshaping computational pathology by enabling transfer learning, where models pre-trained on vast datasets can be adapted for downstream diagnostic, prognostic, and therapeutic response tasks.","Despite these advances, foundation models are still limited in their ability to encode the entire gigapixel whole-slide images without additional training and often lack complementary multimodal data.","Here, we introduce Threads, a slide-level foundation model capable of generating universal representations of whole-slide images of any size.","Threads was pre-trained using a multimodal learning approach on a diverse cohort of 47,171 hematoxylin and eosin (H&E)-stained tissue sections, paired with corresponding genomic and transcriptomic profiles - the largest such paired dataset to be used for foundation model development to date.","This unique training paradigm enables Threads to capture the tissue's underlying molecular composition, yielding powerful representations applicable to a wide array of downstream tasks.","In extensive benchmarking across 54 oncology tasks, including clinical subtyping, grading, mutation prediction, immunohistochemistry status determination, treatment response prediction, and survival prediction, Threads outperformed all baselines while demonstrating remarkable generalizability and label efficiency.","It is particularly well suited for predicting rare events, further emphasizing its clinical utility.","We intend to make the model publicly available for the broader community."],"url":"http://arxiv.org/abs/2501.16652v1"}
{"created":"2025-01-28 02:29:40","title":"instancespace: a Python Package for Insightful Algorithm Testing through Instance Space Analysis","abstract":"Instance Space Analysis is a methodology to evaluate algorithm performance across diverse problem fields. Through visualisation and exploratory data analysis techniques, Instance Space Analysis offers objective, data-driven insights into the diversity of test instances, algorithm behaviour, and algorithm strengths and weaknesses. As such, it supports automated algorithm selection and synthetic test instance generation, increasing testing reliability in optimisation, machine learning, and scheduling fields. This paper introduces instancespace, a Python package that implements an automated pipeline for Instance Space Analysis. This package supports research by streamlining the testing process, providing unbiased metrics, and facilitating more informed algorithmic design and deployment decisions, particularly for complex and safety-critical systems.","sentences":["Instance Space Analysis is a methodology to evaluate algorithm performance across diverse problem fields.","Through visualisation and exploratory data analysis techniques, Instance Space Analysis offers objective, data-driven insights into the diversity of test instances, algorithm behaviour, and algorithm strengths and weaknesses.","As such, it supports automated algorithm selection and synthetic test instance generation, increasing testing reliability in optimisation, machine learning, and scheduling fields.","This paper introduces instancespace, a Python package that implements an automated pipeline for Instance Space Analysis.","This package supports research by streamlining the testing process, providing unbiased metrics, and facilitating more informed algorithmic design and deployment decisions, particularly for complex and safety-critical systems."],"url":"http://arxiv.org/abs/2501.16646v1"}
{"created":"2025-01-28 02:20:34","title":"Analysis of Zero Day Attack Detection Using MLP and XAI","abstract":"Any exploit taking advantage of zero-day is called a zero-day attack. Previous research and social media trends show a massive demand for research in zero-day attack detection. This paper analyzes Machine Learning (ML) and Deep Learning (DL) based approaches to create Intrusion Detection Systems (IDS) and scrutinizing them using Explainable AI (XAI) by training an explainer based on randomly sampled data from the testing set. The focus is on using the KDD99 dataset, which has the most research done among all the datasets for detecting zero-day attacks. The paper aims to synthesize the dataset to have fewer classes for multi-class classification, test ML and DL approaches on pattern recognition, establish the robustness and dependability of the model, and establish the interpretability and scalability of the model. We evaluated the performance of four multilayer perceptron (MLP) trained on the KDD99 dataset, including baseline ML models, weighted ML models, truncated ML models, and weighted truncated ML models. Our results demonstrate that the truncated ML model achieves the highest accuracy (99.62%), precision, and recall, while weighted truncated ML model shows lower accuracy (97.26%) but better class representation (less bias) among all the classes with improved unweighted recall score. We also used Shapely Additive exPlanations (SHAP) to train explainer for our truncated models to check for feature importance among the two weighted and unweighted models.","sentences":["Any exploit taking advantage of zero-day is called a zero-day attack.","Previous research and social media trends show a massive demand for research in zero-day attack detection.","This paper analyzes Machine Learning (ML) and Deep Learning (DL) based approaches to create Intrusion Detection Systems (IDS) and scrutinizing them using Explainable AI (XAI) by training an explainer based on randomly sampled data from the testing set.","The focus is on using the KDD99 dataset, which has the most research done among all the datasets for detecting zero-day attacks.","The paper aims to synthesize the dataset to have fewer classes for multi-class classification, test ML and DL approaches on pattern recognition, establish the robustness and dependability of the model, and establish the interpretability and scalability of the model.","We evaluated the performance of four multilayer perceptron (MLP) trained on the KDD99 dataset, including baseline ML models, weighted ML models, truncated ML models, and weighted truncated ML models.","Our results demonstrate that the truncated ML model achieves the highest accuracy (99.62%), precision, and recall, while weighted truncated ML model shows lower accuracy (97.26%) but better class representation (less bias) among all the classes with improved unweighted recall score.","We also used Shapely Additive exPlanations (SHAP) to train explainer for our truncated models to check for feature importance among the two weighted and unweighted models."],"url":"http://arxiv.org/abs/2501.16638v1"}
{"created":"2025-01-28 02:16:18","title":"Why Do We Laugh? Annotation and Taxonomy Generation for Laughable Contexts in Spontaneous Text Conversation","abstract":"Laughter serves as a multifaceted communicative signal in human interaction, yet its identification within dialogue presents a significant challenge for conversational AI systems. This study addresses this challenge by annotating laughable contexts in Japanese spontaneous text conversation data and developing a taxonomy to classify the underlying reasons for such contexts. Initially, multiple annotators manually labeled laughable contexts using a binary decision (laughable or non-laughable). Subsequently, an LLM was used to generate explanations for the binary annotations of laughable contexts, which were then categorized into a taxonomy comprising ten categories, including \"Empathy and Affinity\" and \"Humor and Surprise,\" highlighting the diverse range of laughter-inducing scenarios. The study also evaluated GPT-4's performance in recognizing the majority labels of laughable contexts, achieving an F1 score of 43.14%. These findings contribute to the advancement of conversational AI by establishing a foundation for more nuanced recognition and generation of laughter, ultimately fostering more natural and engaging human-AI interactions.","sentences":["Laughter serves as a multifaceted communicative signal in human interaction, yet its identification within dialogue presents a significant challenge for conversational AI systems.","This study addresses this challenge by annotating laughable contexts in Japanese spontaneous text conversation data and developing a taxonomy to classify the underlying reasons for such contexts.","Initially, multiple annotators manually labeled laughable contexts using a binary decision (laughable or non-laughable).","Subsequently, an LLM was used to generate explanations for the binary annotations of laughable contexts, which were then categorized into a taxonomy comprising ten categories, including \"Empathy and Affinity\" and \"Humor and Surprise,\" highlighting the diverse range of laughter-inducing scenarios.","The study also evaluated GPT-4's performance in recognizing the majority labels of laughable contexts, achieving an F1 score of 43.14%.","These findings contribute to the advancement of conversational AI by establishing a foundation for more nuanced recognition and generation of laughter, ultimately fostering more natural and engaging human-AI interactions."],"url":"http://arxiv.org/abs/2501.16635v1"}
{"created":"2025-01-28 01:56:55","title":"More Efficient Sybil Detection Mechanisms Leveraging Resistance of Users to Attack Requests","abstract":"We investigate the problem of sybil (fake account) detection in social networks from a graph algorithms perspective, where graph structural information is used to classify users as sybil and benign. We introduce the novel notion of user resistance to attack requests (friendship requests from sybil accounts). Building on this notion, we propose a synthetic graph data generation framework that supports various attack strategies. We then study the optimization problem where we are allowed to reveal the resistance of a subset of users with the aim to maximize the number of users which are discovered to be benign and the number of potential attack edges (connections from a sybil to a benign user). Furthermore, we devise efficient algorithms for this problem and investigate their theoretical guarantees. Finally, through a large set of experiments, we demonstrate that our proposed algorithms improve detection performance notably when applied as a preprocessing step for different sybil detection algorithms. The code and data used in this work are publicly available on GitHub https://github.com/aSafarpoor/AAMAS2025-Paper/tree/main","sentences":["We investigate the problem of sybil (fake account) detection in social networks from a graph algorithms perspective, where graph structural information is used to classify users as sybil and benign.","We introduce the novel notion of user resistance to attack requests (friendship requests from sybil accounts).","Building on this notion, we propose a synthetic graph data generation framework that supports various attack strategies.","We then study the optimization problem where we are allowed to reveal the resistance of a subset of users with the aim to maximize the number of users which are discovered to be benign and the number of potential attack edges (connections from a sybil to a benign user).","Furthermore, we devise efficient algorithms for this problem and investigate their theoretical guarantees.","Finally, through a large set of experiments, we demonstrate that our proposed algorithms improve detection performance notably when applied as a preprocessing step for different sybil detection algorithms.","The code and data used in this work are publicly available on GitHub https://github.com/aSafarpoor/AAMAS2025-Paper/tree/main"],"url":"http://arxiv.org/abs/2501.16624v1"}
{"created":"2025-01-28 01:39:35","title":"Chinese Stock Prediction Based on a Multi-Modal Transformer Framework: Macro-Micro Information Fusion","abstract":"This paper proposes an innovative Multi-Modal Transformer framework (MMF-Trans) designed to significantly improve the prediction accuracy of the Chinese stock market by integrating multi-source heterogeneous information including macroeconomy, micro-market, financial text, and event knowledge. The framework consists of four core modules: (1) A four-channel parallel encoder that processes technical indicators, financial text, macro data, and event knowledge graph respectively for independent feature extraction of multi-modal data; (2) A dynamic gated cross-modal fusion mechanism that adaptively learns the importance of different modalities through differentiable weight allocation for effective information integration; (3) A time-aligned mixed-frequency processing layer that uses an innovative position encoding method to effectively fuse data of different time frequencies and solves the time alignment problem of heterogeneous data; (4) A graph attention-based event impact quantification module that captures the dynamic impact of events on the market through event knowledge graph and quantifies the event impact coefficient. We introduce a hybrid-frequency Transformer and Event2Vec algorithm to effectively fuse data of different frequencies and quantify the event impact. Experimental results show that in the prediction task of CSI 300 constituent stocks, the root mean square error (RMSE) of the MMF-Trans framework is reduced by 23.7% compared to the baseline model, the event response prediction accuracy is improved by 41.2%, and the Sharpe ratio is improved by 32.6%.","sentences":["This paper proposes an innovative Multi-Modal Transformer framework (MMF-Trans) designed to significantly improve the prediction accuracy of the Chinese stock market by integrating multi-source heterogeneous information including macroeconomy, micro-market, financial text, and event knowledge.","The framework consists of four core modules: (1) A four-channel parallel encoder that processes technical indicators, financial text, macro data, and event knowledge graph respectively for independent feature extraction of multi-modal data; (2) A dynamic gated cross-modal fusion mechanism that adaptively learns the importance of different modalities through differentiable weight allocation for effective information integration; (3) A time-aligned mixed-frequency processing layer that uses an innovative position encoding method to effectively fuse data of different time frequencies and solves the time alignment problem of heterogeneous data; (4) A graph attention-based event impact quantification module that captures the dynamic impact of events on the market through event knowledge graph and quantifies the event impact coefficient.","We introduce a hybrid-frequency Transformer and Event2Vec algorithm to effectively fuse data of different frequencies and quantify the event impact.","Experimental results show that in the prediction task of CSI 300 constituent stocks, the root mean square error (RMSE) of the MMF-Trans framework is reduced by 23.7% compared to the baseline model, the event response prediction accuracy is improved by 41.2%, and the Sharpe ratio is improved by 32.6%."],"url":"http://arxiv.org/abs/2501.16621v1"}
{"created":"2025-01-28 01:33:03","title":"SHIELD: Secure Host-Independent Extensible Logging for SATA/Network Storage Towards Ransomware Detection","abstract":"As malware such as ransomware becomes sophisticated, the ability to find and neutralize it requires more robust and tamper-resistant solutions. Current methods rely on data from compromised hosts, lack hardware isolation, and cannot detect emerging threats. To address these limitations, we introduce SHIELD - a detection architecture leveraging FPGA-based open-source SATA and Network Block Device (NBD) technology to provide off-host, tamper-proof measurements for continuous observation of disk activity for software executing on a target device. SHIELD provides three distinct contributions: It (1) develops a framework to obtain and analyze multi-level hardware metrics at NBD, FPGA, and SATA storage levels, and shows their ability to differentiate between harmless and malicious software; (2) Broadens the functionality of an open-source FPGA-driven SATA Host Bus Adapter (HBA) to offer complete data storage capabilities through NBD without relying on the host system; (3) Provides a foundation for using the methodology and metrics in automated machine learning-assisted detection and ASIC integration for advanced mitigation capabilities in data storage devices. SHIELD analyzes 10 benign programs and 10 modern ransomware families to illustrate its capacity for real-time monitoring and use in distinguishing between ransomware and benign software. Experimental evidence shows SHIELD's robust host-independent and hardware-assisted metrics are a basis for detection, allowing to observe program execution and detect malicious activities at the storage level.","sentences":["As malware such as ransomware becomes sophisticated, the ability to find and neutralize it requires more robust and tamper-resistant solutions.","Current methods rely on data from compromised hosts, lack hardware isolation, and cannot detect emerging threats.","To address these limitations, we introduce SHIELD - a detection architecture leveraging FPGA-based open-source SATA and Network Block Device (NBD) technology to provide off-host, tamper-proof measurements for continuous observation of disk activity for software executing on a target device.","SHIELD provides three distinct contributions: It (1) develops a framework to obtain and analyze multi-level hardware metrics at NBD, FPGA, and SATA storage levels, and shows their ability to differentiate between harmless and malicious software; (2) Broadens the functionality of an open-source FPGA-driven SATA Host Bus Adapter (HBA) to offer complete data storage capabilities through NBD without relying on the host system; (3) Provides a foundation for using the methodology and metrics in automated machine learning-assisted detection and ASIC integration for advanced mitigation capabilities in data storage devices.","SHIELD analyzes 10 benign programs and 10 modern ransomware families to illustrate its capacity for real-time monitoring and use in distinguishing between ransomware and benign software.","Experimental evidence shows SHIELD's robust host-independent and hardware-assisted metrics are a basis for detection, allowing to observe program execution and detect malicious activities at the storage level."],"url":"http://arxiv.org/abs/2501.16619v1"}
{"created":"2025-01-28 01:26:22","title":"Few-Shot Optimized Framework for Hallucination Detection in Resource-Limited NLP Systems","abstract":"Hallucination detection in text generation remains an ongoing struggle for natural language processing (NLP) systems, frequently resulting in unreliable outputs in applications such as machine translation and definition modeling. Existing methods struggle with data scarcity and the limitations of unlabeled datasets, as highlighted by the SHROOM shared task at SemEval-2024. In this work, we propose a novel framework to address these challenges, introducing DeepSeek Few-shot optimization to enhance weak label generation through iterative prompt engineering. We achieved high-quality annotations that considerably enhanced the performance of downstream models by restructuring data to align with instruct generative models. We further fine-tuned the Mistral-7B-Instruct-v0.3 model on these optimized annotations, enabling it to accurately detect hallucinations in resource-limited settings. Combining this fine-tuned model with ensemble learning strategies, our approach achieved 85.5% accuracy on the test set, setting a new benchmark for the SHROOM task. This study demonstrates the effectiveness of data restructuring, few-shot optimization, and fine-tuning in building scalable and robust hallucination detection frameworks for resource-constrained NLP systems.","sentences":["Hallucination detection in text generation remains an ongoing struggle for natural language processing (NLP) systems, frequently resulting in unreliable outputs in applications such as machine translation and definition modeling.","Existing methods struggle with data scarcity and the limitations of unlabeled datasets, as highlighted by the SHROOM shared task at SemEval-2024.","In this work, we propose a novel framework to address these challenges, introducing DeepSeek Few-shot optimization to enhance weak label generation through iterative prompt engineering.","We achieved high-quality annotations that considerably enhanced the performance of downstream models by restructuring data to align with instruct generative models.","We further fine-tuned the Mistral-7B-Instruct-v0.3 model on these optimized annotations, enabling it to accurately detect hallucinations in resource-limited settings.","Combining this fine-tuned model with ensemble learning strategies, our approach achieved 85.5% accuracy on the test set, setting a new benchmark for the SHROOM task.","This study demonstrates the effectiveness of data restructuring, few-shot optimization, and fine-tuning in building scalable and robust hallucination detection frameworks for resource-constrained NLP systems."],"url":"http://arxiv.org/abs/2501.16616v1"}
{"created":"2025-01-28 01:24:16","title":"Sparse Autoencoders Trained on the Same Data Learn Different Features","abstract":"Sparse autoencoders (SAEs) are a useful tool for uncovering human-interpretable features in the activations of large language models (LLMs). While some expect SAEs to find the true underlying features used by a model, our research shows that SAEs trained on the same model and data, differing only in the random seed used to initialize their weights, identify different sets of features. For example, in an SAE with 131K latents trained on a feedforward network in Llama 3 8B, only 30% of the features were shared across different seeds. We observed this phenomenon across multiple layers of three different LLMs, two datasets, and several SAE architectures. While ReLU SAEs trained with the L1 sparsity loss showed greater stability across seeds, SAEs using the state-of-the-art TopK activation function were more seed-dependent, even when controlling for the level of sparsity. Our results suggest that the set of features uncovered by an SAE should be viewed as a pragmatically useful decomposition of activation space, rather than an exhaustive and universal list of features \"truly used\" by the model.","sentences":["Sparse autoencoders (SAEs) are a useful tool for uncovering human-interpretable features in the activations of large language models (LLMs).","While some expect SAEs to find the true underlying features used by a model, our research shows that SAEs trained on the same model and data, differing only in the random seed used to initialize their weights, identify different sets of features.","For example, in an SAE with 131K latents trained on a feedforward network in Llama 3 8B, only 30% of the features were shared across different seeds.","We observed this phenomenon across multiple layers of three different LLMs, two datasets, and several SAE architectures.","While ReLU SAEs trained with the L1 sparsity loss showed greater stability across seeds, SAEs using the state-of-the-art TopK activation function were more seed-dependent, even when controlling for the level of sparsity.","Our results suggest that the set of features uncovered by an SAE should be viewed as a pragmatically useful decomposition of activation space, rather than an exhaustive and universal list of features \"truly used\" by the model."],"url":"http://arxiv.org/abs/2501.16615v1"}
{"created":"2025-01-28 01:19:07","title":"FUNU: Boosting Machine Unlearning Efficiency by Filtering Unnecessary Unlearning","abstract":"Machine unlearning is an emerging field that selectively removes specific data samples from a trained model. This capability is crucial for addressing privacy concerns, complying with data protection regulations, and correcting errors or biases introduced by certain data. Unlike traditional machine learning, where models are typically static once trained, machine unlearning facilitates dynamic updates that enable the model to ``forget'' information without requiring complete retraining from scratch. There are various machine unlearning methods, some of which are more time-efficient when data removal requests are fewer.   To decrease the execution time of such machine unlearning methods, we aim to reduce the size of data removal requests based on the fundamental assumption that the removal of certain data would not result in a distinguishable retrained model. We first propose the concept of unnecessary unlearning, which indicates that the model would not alter noticeably after removing some data points. Subsequently, we review existing solutions that can be used to solve our problem. We highlight their limitations in adaptability to different unlearning scenarios and their reliance on manually selected parameters. We consequently put forward FUNU, a method to identify data points that lead to unnecessary unlearning. FUNU circumvents the limitations of existing solutions. The idea is to discover data points within the removal requests that have similar neighbors in the remaining dataset. We utilize a reference model to set parameters for finding neighbors, inspired from the area of model memorization. We provide a theoretical analysis of the privacy guarantee offered by FUNU and conduct extensive experiments to validate its efficacy.","sentences":["Machine unlearning is an emerging field that selectively removes specific data samples from a trained model.","This capability is crucial for addressing privacy concerns, complying with data protection regulations, and correcting errors or biases introduced by certain data.","Unlike traditional machine learning, where models are typically static once trained, machine unlearning facilitates dynamic updates that enable the model to ``forget'' information without requiring complete retraining from scratch.","There are various machine unlearning methods, some of which are more time-efficient when data removal requests are fewer.   ","To decrease the execution time of such machine unlearning methods, we aim to reduce the size of data removal requests based on the fundamental assumption that the removal of certain data would not result in a distinguishable retrained model.","We first propose the concept of unnecessary unlearning, which indicates that the model would not alter noticeably after removing some data points.","Subsequently, we review existing solutions that can be used to solve our problem.","We highlight their limitations in adaptability to different unlearning scenarios and their reliance on manually selected parameters.","We consequently put forward FUNU, a method to identify data points that lead to unnecessary unlearning.","FUNU circumvents the limitations of existing solutions.","The idea is to discover data points within the removal requests that have similar neighbors in the remaining dataset.","We utilize a reference model to set parameters for finding neighbors, inspired from the area of model memorization.","We provide a theoretical analysis of the privacy guarantee offered by FUNU and conduct extensive experiments to validate its efficacy."],"url":"http://arxiv.org/abs/2501.16614v1"}
{"created":"2025-01-28 01:09:50","title":"Embracing Reconfigurable Antennas in the Tri-hybrid MIMO Architecture for 6G","abstract":"Multiple-input multiple-output (MIMO) communication has led to immense enhancements in data rates and efficient spectrum management. The evolution of MIMO has been accompanied by increased hardware complexity and array sizes, causing system power consumption to rise as a result. Despite past advances in power-efficient hybrid architectures, new solutions are needed to enable extremely large-scale MIMO deployments for 6G and beyond. In this paper, we introduce a novel architecture that integrates low-power reconfigurable antennas with both digital and analog precoding. This \\emph{tri-hybrid} approach addresses key limitations in traditional and hybrid MIMO systems by improving power consumption and adding new layer for signal processing. We provide a comprehensive analysis of the proposed architecture and compare its performance with existing solutions, including fully-digital and hybrid MIMO systems. The results demonstrate significant improvements in energy efficiency, highlighting the potential of the tri-hybrid system to meet the growing demands of future wireless networks. We also discuss several design and implementation challenges, including the need for technological advancements in reconfigurable array hardware and tunable antenna parameters.","sentences":["Multiple-input multiple-output (MIMO) communication has led to immense enhancements in data rates and efficient spectrum management.","The evolution of MIMO has been accompanied by increased hardware complexity and array sizes, causing system power consumption to rise as a result.","Despite past advances in power-efficient hybrid architectures, new solutions are needed to enable extremely large-scale MIMO deployments for 6G and beyond.","In this paper, we introduce a novel architecture that integrates low-power reconfigurable antennas with both digital and analog precoding.","This \\emph{tri-hybrid} approach addresses key limitations in traditional and hybrid MIMO systems by improving power consumption and adding new layer for signal processing.","We provide a comprehensive analysis of the proposed architecture and compare its performance with existing solutions, including fully-digital and hybrid MIMO systems.","The results demonstrate significant improvements in energy efficiency, highlighting the potential of the tri-hybrid system to meet the growing demands of future wireless networks.","We also discuss several design and implementation challenges, including the need for technological advancements in reconfigurable array hardware and tunable antenna parameters."],"url":"http://arxiv.org/abs/2501.16610v1"}
{"created":"2025-01-28 00:56:53","title":"CowPilot: A Framework for Autonomous and Human-Agent Collaborative Web Navigation","abstract":"While much work on web agents emphasizes the promise of autonomously performing tasks on behalf of users, in reality, agents often fall short on complex tasks in real-world contexts and modeling user preference. This presents an opportunity for humans to collaborate with the agent and leverage the agent's capabilities effectively. We propose CowPilot, a framework supporting autonomous as well as human-agent collaborative web navigation, and evaluation across task success and task efficiency. CowPilot reduces the number of steps humans need to perform by allowing agents to propose next steps, while users are able to pause, reject, or take alternative actions. During execution, users can interleave their actions with the agent by overriding suggestions or resuming agent control when needed. We conducted case studies on five common websites and found that the human-agent collaborative mode achieves the highest success rate of 95% while requiring humans to perform only 15.2% of the total steps. Even with human interventions during task execution, the agent successfully drives up to half of task success on its own. CowPilot can serve as a useful tool for data collection and agent evaluation across websites, which we believe will enable research in how users and agents can work together. Video demonstrations are available at https://oaishi.github.io/cowpilot.html","sentences":["While much work on web agents emphasizes the promise of autonomously performing tasks on behalf of users, in reality, agents often fall short on complex tasks in real-world contexts and modeling user preference.","This presents an opportunity for humans to collaborate with the agent and leverage the agent's capabilities effectively.","We propose CowPilot, a framework supporting autonomous as well as human-agent collaborative web navigation, and evaluation across task success and task efficiency.","CowPilot reduces the number of steps humans need to perform by allowing agents to propose next steps, while users are able to pause, reject, or take alternative actions.","During execution, users can interleave their actions with the agent by overriding suggestions or resuming agent control when needed.","We conducted case studies on five common websites and found that the human-agent collaborative mode achieves the highest success rate of 95% while requiring humans to perform only 15.2% of the total steps.","Even with human interventions during task execution, the agent successfully drives up to half of task success on its own.","CowPilot can serve as a useful tool for data collection and agent evaluation across websites, which we believe will enable research in how users and agents can work together.","Video demonstrations are available at https://oaishi.github.io/cowpilot.html"],"url":"http://arxiv.org/abs/2501.16609v1"}
{"created":"2025-01-28 00:44:38","title":"Impact and influence of modern AI in metadata management","abstract":"Metadata management plays a critical role in data governance, resource discovery, and decision-making in the data-driven era. While traditional metadata approaches have primarily focused on organization, classification, and resource reuse, the integration of modern artificial intelligence (AI) technologies has significantly transformed these processes. This paper investigates both traditional and AI-driven metadata approaches by examining open-source solutions, commercial tools, and research initiatives. A comparative analysis of traditional and AI-driven metadata management methods is provided, highlighting existing challenges and their impact on next-generation datasets. The paper also presents an innovative AI-assisted metadata management framework designed to address these challenges. This framework leverages more advanced modern AI technologies to automate metadata generation, enhance governance, and improve the accessibility and usability of modern datasets. Finally, the paper outlines future directions for research and development, proposing opportunities to further advance metadata management in the context of AI-driven innovation and complex datasets.","sentences":["Metadata management plays a critical role in data governance, resource discovery, and decision-making in the data-driven era.","While traditional metadata approaches have primarily focused on organization, classification, and resource reuse, the integration of modern artificial intelligence (AI) technologies has significantly transformed these processes.","This paper investigates both traditional and AI-driven metadata approaches by examining open-source solutions, commercial tools, and research initiatives.","A comparative analysis of traditional and AI-driven metadata management methods is provided, highlighting existing challenges and their impact on next-generation datasets.","The paper also presents an innovative AI-assisted metadata management framework designed to address these challenges.","This framework leverages more advanced modern AI technologies to automate metadata generation, enhance governance, and improve the accessibility and usability of modern datasets.","Finally, the paper outlines future directions for research and development, proposing opportunities to further advance metadata management in the context of AI-driven innovation and complex datasets."],"url":"http://arxiv.org/abs/2501.16605v1"}
{"created":"2025-01-28 00:28:16","title":"Toward Safe Integration of UAM in Terminal Airspace: UAM Route Feasibility Assessment using Probabilistic Aircraft Trajectory Prediction","abstract":"Integrating Urban Air Mobility (UAM) into airspace managed by Air Traffic Control (ATC) poses significant challenges, particularly in congested terminal environments. This study proposes a framework to assess the feasibility of UAM route integration using probabilistic aircraft trajectory prediction. By leveraging conditional Normalizing Flows, the framework predicts short-term trajectory distributions of conventional aircraft, enabling UAM vehicles to dynamically adjust speeds and maintain safe separations. The methodology was applied to airspace over Seoul metropolitan area, encompassing interactions between UAM and conventional traffic at multiple altitudes and lanes. The results reveal that different physical locations of lanes and routes experience varying interaction patterns and encounter dynamics. For instance, Lane 1 at lower altitudes (1,500 ft and 2,000 ft) exhibited minimal interactions with conventional aircraft, resulting in the largest separations and the most stable delay proportions. In contrast, Lane 4 near the airport experienced more frequent and complex interactions due to its proximity to departing traffic. The limited trajectory data for departing aircraft in this region occasionally led to tighter separations and increased operational challenges. This study underscores the potential of predictive modeling in facilitating UAM integration while highlighting critical trade-offs between safety and efficiency. The findings contribute to refining airspace management strategies and offer insights for scaling UAM operations in complex urban environments.","sentences":["Integrating Urban Air Mobility (UAM) into airspace managed by Air Traffic Control (ATC) poses significant challenges, particularly in congested terminal environments.","This study proposes a framework to assess the feasibility of UAM route integration using probabilistic aircraft trajectory prediction.","By leveraging conditional Normalizing Flows, the framework predicts short-term trajectory distributions of conventional aircraft, enabling UAM vehicles to dynamically adjust speeds and maintain safe separations.","The methodology was applied to airspace over Seoul metropolitan area, encompassing interactions between UAM and conventional traffic at multiple altitudes and lanes.","The results reveal that different physical locations of lanes and routes experience varying interaction patterns and encounter dynamics.","For instance, Lane 1 at lower altitudes (1,500 ft and 2,000 ft) exhibited minimal interactions with conventional aircraft, resulting in the largest separations and the most stable delay proportions.","In contrast, Lane 4 near the airport experienced more frequent and complex interactions due to its proximity to departing traffic.","The limited trajectory data for departing aircraft in this region occasionally led to tighter separations and increased operational challenges.","This study underscores the potential of predictive modeling in facilitating UAM integration while highlighting critical trade-offs between safety and efficiency.","The findings contribute to refining airspace management strategies and offer insights for scaling UAM operations in complex urban environments."],"url":"http://arxiv.org/abs/2501.16599v1"}
{"created":"2025-01-28 00:12:26","title":"Applying Ensemble Models based on Graph Neural Network and Reinforcement Learning for Wind Power Forecasting","abstract":"Accurately predicting the wind power output of a wind farm across various time scales utilizing Wind Power Forecasting (WPF) is a critical issue in wind power trading and utilization. The WPF problem remains unresolved due to numerous influencing variables, such as wind speed, temperature, latitude, and longitude. Furthermore, achieving high prediction accuracy is crucial for maintaining electric grid stability and ensuring supply security. In this paper, we model all wind turbines within a wind farm as graph nodes in a graph built by their geographical locations. Accordingly, we propose an ensemble model based on graph neural networks and reinforcement learning (EMGRL) for WPF. Our approach includes: (1) applying graph neural networks to capture the time-series data from neighboring wind farms relevant to the target wind farm; (2) establishing a general state embedding that integrates the target wind farm's data with the historical performance of base models on the target wind farm; (3) ensembling and leveraging the advantages of all base models through an actor-critic reinforcement learning framework for WPF.","sentences":["Accurately predicting the wind power output of a wind farm across various time scales utilizing Wind Power Forecasting (WPF) is a critical issue in wind power trading and utilization.","The WPF problem remains unresolved due to numerous influencing variables, such as wind speed, temperature, latitude, and longitude.","Furthermore, achieving high prediction accuracy is crucial for maintaining electric grid stability and ensuring supply security.","In this paper, we model all wind turbines within a wind farm as graph nodes in a graph built by their geographical locations.","Accordingly, we propose an ensemble model based on graph neural networks and reinforcement learning (EMGRL) for WPF.","Our approach includes: (1) applying graph neural networks to capture the time-series data from neighboring wind farms relevant to the target wind farm; (2) establishing a general state embedding that integrates the target wind farm's data with the historical performance of base models on the target wind farm; (3) ensembling and leveraging the advantages of all base models through an actor-critic reinforcement learning framework for WPF."],"url":"http://arxiv.org/abs/2501.16591v1"}
{"created":"2025-01-28 00:02:00","title":"Fine-Tuned Language Models as Space Systems Controllers","abstract":"Large language models (LLMs), or foundation models (FMs), are pretrained transformers that coherently complete sentences auto-regressively. In this paper, we show that LLMs can control simplified space systems after some additional training, called fine-tuning. We look at relatively small language models, ranging between 7 and 13 billion parameters. We focus on four problems: a three-dimensional spring toy problem, low-thrust orbit transfer, low-thrust cislunar control, and powered descent guidance. The fine-tuned LLMs are capable of controlling systems by generating sufficiently accurate outputs that are multi-dimensional vectors with up to 10 significant digits. We show that for several problems the amount of data required to perform fine-tuning is smaller than what is generally required of traditional deep neural networks (DNNs), and that fine-tuned LLMs are good at generalizing outside of the training dataset. Further, the same LLM can be fine-tuned with data from different problems, with only minor performance degradation with respect to LLMs trained for a single application. This work is intended as a first step towards the development of a general space systems controller.","sentences":["Large language models (LLMs), or foundation models (FMs), are pretrained transformers that coherently complete sentences auto-regressively.","In this paper, we show that LLMs can control simplified space systems after some additional training, called fine-tuning.","We look at relatively small language models, ranging between 7 and 13 billion parameters.","We focus on four problems: a three-dimensional spring toy problem, low-thrust orbit transfer, low-thrust cislunar control, and powered descent guidance.","The fine-tuned LLMs are capable of controlling systems by generating sufficiently accurate outputs that are multi-dimensional vectors with up to 10 significant digits.","We show that for several problems the amount of data required to perform fine-tuning is smaller than what is generally required of traditional deep neural networks (DNNs), and that fine-tuned LLMs are good at generalizing outside of the training dataset.","Further, the same LLM can be fine-tuned with data from different problems, with only minor performance degradation with respect to LLMs trained for a single application.","This work is intended as a first step towards the development of a general space systems controller."],"url":"http://arxiv.org/abs/2501.16588v1"}
{"created":"2025-01-27 23:53:04","title":"DialUp! Modeling the Language Continuum by Adapting Models to Dialects and Dialects to Models","abstract":"Most of the world's languages and dialects are low-resource, and lack support in mainstream machine translation (MT) models. However, many of them have a closely-related high-resource language (HRL) neighbor, and differ in linguistically regular ways from it. This underscores the importance of model robustness to dialectical variation and cross-lingual generalization to the HRL dialect continuum. We present DialUp, consisting of a training-time technique for adapting a pretrained model to dialectical data (M->D), and an inference-time intervention adapting dialectical data to the model expertise (D->M). M->D induces model robustness to potentially unseen and unknown dialects by exposure to synthetic data exemplifying linguistic mechanisms of dialectical variation, whereas D->M treats dialectical divergence for known target dialects. These methods show considerable performance gains for several dialects from four language families, and modest gains for two other language families. We also conduct feature and error analyses, which show that language varieties with low baseline MT performance are more likely to benefit from these approaches.","sentences":["Most of the world's languages and dialects are low-resource, and lack support in mainstream machine translation (MT) models.","However, many of them have a closely-related high-resource language (HRL) neighbor, and differ in linguistically regular ways from it.","This underscores the importance of model robustness to dialectical variation and cross-lingual generalization to the HRL dialect continuum.","We present DialUp, consisting of a training-time technique for adapting a pretrained model to dialectical data (M->D), and an inference-time intervention adapting dialectical data to the model expertise (D->M).","M->D induces model robustness to potentially unseen and unknown dialects by exposure to synthetic data exemplifying linguistic mechanisms of dialectical variation, whereas D->M treats dialectical divergence for known target dialects.","These methods show considerable performance gains for several dialects from four language families, and modest gains for two other language families.","We also conduct feature and error analyses, which show that language varieties with low baseline MT performance are more likely to benefit from these approaches."],"url":"http://arxiv.org/abs/2501.16581v1"}
{"created":"2025-01-27 23:41:13","title":"Generative AI Uses and Risks for Knowledge Workers in a Science Organization","abstract":"Generative AI could enhance scientific discovery by supporting knowledge workers in science organizations. However, the real-world applications and perceived concerns of generative AI use in these organizations are uncertain. In this paper, we report on a collaborative study with a US national laboratory with employees spanning Science and Operations about their use of generative AI tools. We surveyed 66 employees, interviewed a subset (N=22), and measured early adoption of an internal generative AI interface called Argo lab-wide. We have four findings: (1) Argo usage data shows small but increasing use by Science and Operations employees; Common current and envisioned use cases for generative AI in this context conceptually fall into either a (2) copilot or (3) workflow agent modality; and (4) Concerns include sensitive data security, academic publishing, and job impacts. Based on our findings, we make recommendations for generative AI use in science and other organizations.","sentences":["Generative AI could enhance scientific discovery by supporting knowledge workers in science organizations.","However, the real-world applications and perceived concerns of generative AI use in these organizations are uncertain.","In this paper, we report on a collaborative study with a US national laboratory with employees spanning Science and Operations about their use of generative AI tools.","We surveyed 66 employees, interviewed a subset (N=22), and measured early adoption of an internal generative AI interface called Argo lab-wide.","We have four findings: (1) Argo usage data shows small but increasing use by Science and Operations employees; Common current and envisioned use cases for generative AI in this context conceptually fall into either a (2) copilot or (3) workflow agent modality; and (4) Concerns include sensitive data security, academic publishing, and job impacts.","Based on our findings, we make recommendations for generative AI use in science and other organizations."],"url":"http://arxiv.org/abs/2501.16577v1"}
{"created":"2025-01-27 23:18:39","title":"AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding with Multimodal Large Language Models","abstract":"The emergence of multimodal large language models (MLLMs) advances multimodal emotion recognition (MER) to the next level-from naive discriminative tasks to complex emotion understanding with advanced video understanding abilities and natural language description. However, the current community suffers from a lack of large-scale datasets with intensive, descriptive emotion annotations, as well as a multimodal-centric framework to maximize the potential of MLLMs for emotion understanding. To address this, we establish a new benchmark for MLLM-based emotion understanding with a novel dataset (MER-Caption), and a new model (AffectGPT). Utilizing our model-based crowd-sourcing data collection strategy, we construct the largest descriptive emotion dataset to date (by far), featuring over 2K fine-grained emotion categories across 115K samples. We also introduce the AffectGPT model, designed with pre-fusion operations to enhance multimodal integration. Finally, we present MER-UniBench, a unified benchmark with evaluation metrics tailored for both typical MER tasks and the free-form, natural language output style of MLLMs. Extensive experimental results demonstrate AffectGPT's robust performance across various MER tasks. We are publicly releasing both the AffectGPT model and the MER-Caption dataset to foster further research and development in emotion understanding.","sentences":["The emergence of multimodal large language models (MLLMs) advances multimodal emotion recognition (MER) to the next level-from naive discriminative tasks to complex emotion understanding with advanced video understanding abilities and natural language description.","However, the current community suffers from a lack of large-scale datasets with intensive, descriptive emotion annotations, as well as a multimodal-centric framework to maximize the potential of MLLMs for emotion understanding.","To address this, we establish a new benchmark for MLLM-based emotion understanding with a novel dataset (MER-Caption), and a new model (AffectGPT).","Utilizing our model-based crowd-sourcing data collection strategy, we construct the largest descriptive emotion dataset to date (by far), featuring over 2K fine-grained emotion categories across 115K samples.","We also introduce the AffectGPT model, designed with pre-fusion operations to enhance multimodal integration.","Finally, we present MER-UniBench, a unified benchmark with evaluation metrics tailored for both typical MER tasks and the free-form, natural language output style of MLLMs.","Extensive experimental results demonstrate AffectGPT's robust performance across various MER tasks.","We are publicly releasing both the AffectGPT model and the MER-Caption dataset to foster further research and development in emotion understanding."],"url":"http://arxiv.org/abs/2501.16566v1"}
{"created":"2025-01-27 23:12:18","title":"C-HDNet: A Fast Hyperdimensional Computing Based Method for Causal Effect Estimation from Networked Observational Data","abstract":"We consider the problem of estimating causal effects from observational data in the presence of network confounding. In this context, an individual's treatment assignment and outcomes may be affected by their neighbors within the network. We propose a novel matching technique which leverages hyperdimensional computing to model network information and improve predictive performance. We present results of extensive experiments which show that the proposed method outperforms or is competitive with the state-of-the-art methods for causal effect estimation from network data, including advanced computationally demanding deep learning methods. Further, our technique benefits from simplicity and speed, with roughly an order of magnitude lower runtime compared to state-of-the-art methods, while offering similar causal effect estimation error rates.","sentences":["We consider the problem of estimating causal effects from observational data in the presence of network confounding.","In this context, an individual's treatment assignment and outcomes may be affected by their neighbors within the network.","We propose a novel matching technique which leverages hyperdimensional computing to model network information and improve predictive performance.","We present results of extensive experiments which show that the proposed method outperforms or is competitive with the state-of-the-art methods for causal effect estimation from network data, including advanced computationally demanding deep learning methods.","Further, our technique benefits from simplicity and speed, with roughly an order of magnitude lower runtime compared to state-of-the-art methods, while offering similar causal effect estimation error rates."],"url":"http://arxiv.org/abs/2501.16562v1"}
{"created":"2025-01-27 23:02:24","title":"LoRA-X: Bridging Foundation Models with Training-Free Cross-Model Adaptation","abstract":"The rising popularity of large foundation models has led to a heightened demand for parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), which offer performance comparable to full model fine-tuning while requiring only a few additional parameters tailored to the specific base model. When such base models are deprecated and replaced, all associated LoRA modules must be retrained, requiring access to either the original training data or a substantial amount of synthetic data that mirrors the original distribution. However, the original data is often inaccessible due to privacy or licensing issues, and generating synthetic data may be impractical and insufficiently representative. These factors complicate the fine-tuning process considerably. To address this challenge, we introduce a new adapter, Cross-Model Low-Rank Adaptation (LoRA-X), which enables the training-free transfer of LoRA parameters across source and target models, eliminating the need for original or synthetic training data. Our approach imposes the adapter to operate within the subspace of the source base model. This constraint is necessary because our prior knowledge of the target model is limited to its weights, and the criteria for ensuring the adapter's transferability are restricted to the target base model's weights and subspace. To facilitate the transfer of LoRA parameters of the source model to a target model, we employ the adapter only in the layers of the target model that exhibit an acceptable level of subspace similarity. Our extensive experiments demonstrate the effectiveness of LoRA-X for text-to-image generation, including Stable Diffusion v1.5 and Stable Diffusion XL.","sentences":["The rising popularity of large foundation models has led to a heightened demand for parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), which offer performance comparable to full model fine-tuning while requiring only a few additional parameters tailored to the specific base model.","When such base models are deprecated and replaced, all associated LoRA modules must be retrained, requiring access to either the original training data or a substantial amount of synthetic data that mirrors the original distribution.","However, the original data is often inaccessible due to privacy or licensing issues, and generating synthetic data may be impractical and insufficiently representative.","These factors complicate the fine-tuning process considerably.","To address this challenge, we introduce a new adapter, Cross-Model Low-Rank Adaptation (LoRA-X), which enables the training-free transfer of LoRA parameters across source and target models, eliminating the need for original or synthetic training data.","Our approach imposes the adapter to operate within the subspace of the source base model.","This constraint is necessary because our prior knowledge of the target model is limited to its weights, and the criteria for ensuring the adapter's transferability are restricted to the target base model's weights and subspace.","To facilitate the transfer of LoRA parameters of the source model to a target model, we employ the adapter only in the layers of the target model that exhibit an acceptable level of subspace similarity.","Our extensive experiments demonstrate the effectiveness of LoRA-X for text-to-image generation, including Stable Diffusion v1.5 and Stable Diffusion XL."],"url":"http://arxiv.org/abs/2501.16559v1"}
{"created":"2025-01-27 22:48:36","title":"PhysAnimator: Physics-Guided Generative Cartoon Animation","abstract":"Creating hand-drawn animation sequences is labor-intensive and demands professional expertise. We introduce PhysAnimator, a novel approach for generating physically plausible meanwhile anime-stylized animation from static anime illustrations. Our method seamlessly integrates physics-based simulations with data-driven generative models to produce dynamic and visually compelling animations. To capture the fluidity and exaggeration characteristic of anime, we perform image-space deformable body simulations on extracted mesh geometries. We enhance artistic control by introducing customizable energy strokes and incorporating rigging point support, enabling the creation of tailored animation effects such as wind interactions. Finally, we extract and warp sketches from the simulation sequence, generating a texture-agnostic representation, and employ a sketch-guided video diffusion model to synthesize high-quality animation frames. The resulting animations exhibit temporal consistency and visual plausibility, demonstrating the effectiveness of our method in creating dynamic anime-style animations.","sentences":["Creating hand-drawn animation sequences is labor-intensive and demands professional expertise.","We introduce PhysAnimator, a novel approach for generating physically plausible meanwhile anime-stylized animation from static anime illustrations.","Our method seamlessly integrates physics-based simulations with data-driven generative models to produce dynamic and visually compelling animations.","To capture the fluidity and exaggeration characteristic of anime, we perform image-space deformable body simulations on extracted mesh geometries.","We enhance artistic control by introducing customizable energy strokes and incorporating rigging point support, enabling the creation of tailored animation effects such as wind interactions.","Finally, we extract and warp sketches from the simulation sequence, generating a texture-agnostic representation, and employ a sketch-guided video diffusion model to synthesize high-quality animation frames.","The resulting animations exhibit temporal consistency and visual plausibility, demonstrating the effectiveness of our method in creating dynamic anime-style animations."],"url":"http://arxiv.org/abs/2501.16550v1"}
{"created":"2025-01-27 22:48:20","title":"Reconciling Predictive Multiplicity in Practice","abstract":"Many machine learning applications predict individual probabilities, such as the likelihood that a person develops a particular illness. Since these probabilities are unknown, a key question is how to address situations in which different models trained on the same dataset produce varying predictions for certain individuals. This issue is exemplified by the model multiplicity (MM) phenomenon, where a set of comparable models yield inconsistent predictions. Roth, Tolbert, and Weinstein recently introduced a reconciliation procedure, the Reconcile algorithm, to address this problem. Given two disagreeing models, the algorithm leverages their disagreement to falsify and improve at least one of the models. In this paper, we empirically analyze the Reconcile algorithm using five widely-used fairness datasets: COMPAS, Communities and Crime, Adult, Statlog (German Credit Data), and the ACS Dataset. We examine how Reconcile fits within the model multiplicity literature and compare it to existing MM solutions, demonstrating its effectiveness. We also discuss potential improvements to the Reconcile algorithm theoretically and practically. Finally, we extend the Reconcile algorithm to the setting of causal inference, given that different competing estimators can again disagree on specific causal average treatment effect (CATE) values. We present the first extension of the Reconcile algorithm in causal inference, analyze its theoretical properties, and conduct empirical tests. Our results confirm the practical effectiveness of Reconcile and its applicability across various domains.","sentences":["Many machine learning applications predict individual probabilities, such as the likelihood that a person develops a particular illness.","Since these probabilities are unknown, a key question is how to address situations in which different models trained on the same dataset produce varying predictions for certain individuals.","This issue is exemplified by the model multiplicity (MM) phenomenon, where a set of comparable models yield inconsistent predictions.","Roth, Tolbert, and Weinstein recently introduced a reconciliation procedure, the Reconcile algorithm, to address this problem.","Given two disagreeing models, the algorithm leverages their disagreement to falsify and improve at least one of the models.","In this paper, we empirically analyze the Reconcile algorithm using five widely-used fairness datasets: COMPAS, Communities and Crime, Adult, Statlog (German Credit Data), and the ACS Dataset.","We examine how Reconcile fits within the model multiplicity literature and compare it to existing MM solutions, demonstrating its effectiveness.","We also discuss potential improvements to the Reconcile algorithm theoretically and practically.","Finally, we extend the Reconcile algorithm to the setting of causal inference, given that different competing estimators can again disagree on specific causal average treatment effect (CATE) values.","We present the first extension of the Reconcile algorithm in causal inference, analyze its theoretical properties, and conduct empirical tests.","Our results confirm the practical effectiveness of Reconcile and its applicability across various domains."],"url":"http://arxiv.org/abs/2501.16549v1"}
{"created":"2025-01-27 22:45:06","title":"From Efficiency Gains to Rebound Effects: The Problem of Jevons' Paradox in AI's Polarized Environmental Debate","abstract":"As the climate crisis deepens, artificial intelligence (AI) has emerged as a contested force: some champion its potential to advance renewable energy, materials discovery, and large-scale emissions monitoring, while others underscore its growing carbon footprint, water consumption, and material resource demands. Much of this debate has concentrated on direct impact -- energy and water usage in data centers, e-waste from frequent hardware upgrades -- without addressing the significant indirect effects. This paper examines how the problem of Jevons' Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption. We argue that understanding these second-order impacts requires an interdisciplinary approach, combining lifecycle assessments with socio-economic analyses. Rebound effects undermine the assumption that improved technical efficiency alone will ensure net reductions in environmental harm. Instead, the trajectory of AI's impact also hinges on business incentives and market logics, governance and policymaking, and broader social and cultural norms. We contend that a narrow focus on direct emissions misrepresents AI's true climate footprint, limiting the scope for meaningful interventions. We conclude with recommendations that address rebound effects and challenge the market-driven imperatives fueling uncontrolled AI growth. By broadening the analysis to include both direct and indirect consequences, we aim to inform a more comprehensive, evidence-based dialogue on AI's role in the climate crisis.","sentences":["As the climate crisis deepens, artificial intelligence (AI) has emerged as a contested force: some champion its potential to advance renewable energy, materials discovery, and large-scale emissions monitoring, while others underscore its growing carbon footprint, water consumption, and material resource demands.","Much of this debate has concentrated on direct impact -- energy and water usage in data centers, e-waste from frequent hardware upgrades -- without addressing the significant indirect effects.","This paper examines how the problem of Jevons' Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption.","We argue that understanding these second-order impacts requires an interdisciplinary approach, combining lifecycle assessments with socio-economic analyses.","Rebound effects undermine the assumption that improved technical efficiency alone will ensure net reductions in environmental harm.","Instead, the trajectory of AI's impact also hinges on business incentives and market logics, governance and policymaking, and broader social and cultural norms.","We contend that a narrow focus on direct emissions misrepresents AI's true climate footprint, limiting the scope for meaningful interventions.","We conclude with recommendations that address rebound effects and challenge the market-driven imperatives fueling uncontrolled AI growth.","By broadening the analysis to include both direct and indirect consequences, we aim to inform a more comprehensive, evidence-based dialogue on AI's role in the climate crisis."],"url":"http://arxiv.org/abs/2501.16548v1"}
{"created":"2025-01-27 22:14:43","title":"Latency Guarantees for Caching with Delayed Hits","abstract":"In the classical caching problem, when a requested page is not present in the cache (i.e., a \"miss\"), it is assumed to travel from the backing store into the cache \"before\" the next request arrives. However, in many real-life applications, such as content delivery networks, this assumption is unrealistic.   The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and Berger, accounts for the latency between a missed cache request and the corresponding arrival from the backing store. This theoretical model has two parameters: the \"delay\" $Z$, representing the ratio between the retrieval delay and the inter-request delay in an application, and the \"cache size\" $k$, as in classical caching. Classical caching corresponds to $Z=1$, whereas larger values of $Z$ model applications where retrieving missed requests is expensive. Despite the practical relevance of the delayed-hits model, its theoretical underpinnings are still poorly understood.   We present the first tight theoretical guarantee for optimizing delayed-hits caching: The \"Least Recently Used\" algorithm, a natural, deterministic, online algorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at most $O(Zk)$ times more latency than the (offline) optimal schedule. Our result extends to any so-called \"marking\" algorithm.","sentences":["In the classical caching problem, when a requested page is not present in the cache (i.e., a \"miss\"), it is assumed to travel from the backing store into the cache \"before\" the next request arrives.","However, in many real-life applications, such as content delivery networks, this assumption is unrealistic.   ","The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and Berger, accounts for the latency between a missed cache request and the corresponding arrival from the backing store.","This theoretical model has two parameters: the \"delay\" $Z$, representing the ratio between the retrieval delay and the inter-request delay in an application, and the \"cache size\" $k$, as in classical caching.","Classical caching corresponds to $Z=1$, whereas larger values of $Z$ model applications where retrieving missed requests is expensive.","Despite the practical relevance of the delayed-hits model, its theoretical underpinnings are still poorly understood.   ","We present the first tight theoretical guarantee for optimizing delayed-hits caching: The \"Least Recently Used\" algorithm, a natural, deterministic, online algorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at most $O(Zk)$ times more latency than the (offline) optimal schedule.","Our result extends to any so-called \"marking\" algorithm."],"url":"http://arxiv.org/abs/2501.16535v1"}
{"created":"2025-01-27 22:12:09","title":"A comparison of data filtering techniques for English-Polish LLM-based machine translation in the biomedical domain","abstract":"Large Language Models (LLMs) have become state-of-the-art in Machine Translation (MT), often trained on massive bilingual parallel corpora scraped from the web, that contain low-quality entries and redundant information, leading to significant computational challenges. Various data filtering methods exist to reduce dataset sizes, but their effectiveness largely varies based on specific language pairs and domains. This paper evaluates the impact of commonly used data filtering techniques, such as LASER, MUSE, and LaBSE, on English-Polish translation within the biomedical domain. By filtering the UFAL Medical Corpus, we created varying dataset sizes to fine-tune the mBART50 model, which was then evaluated using the SacreBLEU metric on the Khresmoi dataset, having the quality of translations assessed by bilingual speakers. Our results show that both LASER and MUSE can significantly reduce dataset sizes while maintaining or even enhancing performance. We recommend the use of LASER, as it consistently outperforms the other methods and provides the most fluent and natural-sounding translations.","sentences":["Large Language Models (LLMs) have become state-of-the-art in Machine Translation (MT), often trained on massive bilingual parallel corpora scraped from the web, that contain low-quality entries and redundant information, leading to significant computational challenges.","Various data filtering methods exist to reduce dataset sizes, but their effectiveness largely varies based on specific language pairs and domains.","This paper evaluates the impact of commonly used data filtering techniques, such as LASER, MUSE, and LaBSE, on English-Polish translation within the biomedical domain.","By filtering the UFAL Medical Corpus, we created varying dataset sizes to fine-tune the mBART50 model, which was then evaluated using the SacreBLEU metric on the Khresmoi dataset, having the quality of translations assessed by bilingual speakers.","Our results show that both LASER and MUSE can significantly reduce dataset sizes while maintaining or even enhancing performance.","We recommend the use of LASER, as it consistently outperforms the other methods and provides the most fluent and natural-sounding translations."],"url":"http://arxiv.org/abs/2501.16533v1"}
{"created":"2025-01-27 22:10:27","title":"Responsible Generative AI Use by Product Managers: Recoupling Ethical Principles and Practices","abstract":"Since 2022, generative AI (genAI) has rapidly become integrated into workplaces. Though organizations have made commitments to use this technology \"responsibly\", how organizations and their employees prioritize responsibility in their decision-making remains absent from extant management theorizing. In this paper, we examine how product managers - who often serve as gatekeepers in decision-making processes - implement responsible practices in their day-to-day work when using genAI. Using Institutional Theory, we illuminate the factors that constrain or support proactive responsible development and usage of genAI technologies. We employ a mixed methods research design, drawing on 25 interviews with product managers and a global survey of 300 respondents in product management-related roles. The majority of our respondents report (1) widespread uncertainty regarding what \"responsibility\" means or looks like, (2) diffused responsibility given assumed ethical actions by other teams, (3) lack of clear incentives and guidance within organizations, and (4) the importance of leadership buy-in and principles for navigating tensions between ethical commitments and profit motives. However, our study finds that even in highly uncertain environments, absent guidance from leadership, product managers can \"recouple\" ethical commitments and practices by finding responsibility \"micro-moments\". Product managers seek out low-risk, small-scale actions they can take without explicit buy-in from higher-level managers, such as individual or team-wide checks and reviews and safeguarding standards for data. Our research highlights how genAI poses unique challenges to organizations trying to couple ethical principles and daily practices and the role that middle-level management can play in recoupling the two.","sentences":["Since 2022, generative AI (genAI) has rapidly become integrated into workplaces.","Though organizations have made commitments to use this technology \"responsibly\", how organizations and their employees prioritize responsibility in their decision-making remains absent from extant management theorizing.","In this paper, we examine how product managers - who often serve as gatekeepers in decision-making processes - implement responsible practices in their day-to-day work when using genAI.","Using Institutional Theory, we illuminate the factors that constrain or support proactive responsible development and usage of genAI technologies.","We employ a mixed methods research design, drawing on 25 interviews with product managers and a global survey of 300 respondents in product management-related roles.","The majority of our respondents report (1) widespread uncertainty regarding what \"responsibility\" means or looks like, (2) diffused responsibility given assumed ethical actions by other teams, (3) lack of clear incentives and guidance within organizations, and (4) the importance of leadership buy-in and principles for navigating tensions between ethical commitments and profit motives.","However, our study finds that even in highly uncertain environments, absent guidance from leadership, product managers can \"recouple\" ethical commitments and practices by finding responsibility \"micro-moments\".","Product managers seek out low-risk, small-scale actions they can take without explicit buy-in from higher-level managers, such as individual or team-wide checks and reviews and safeguarding standards for data.","Our research highlights how genAI poses unique challenges to organizations trying to couple ethical principles and daily practices and the role that middle-level management can play in recoupling the two."],"url":"http://arxiv.org/abs/2501.16531v1"}
{"created":"2025-01-27 21:48:39","title":"Programming by Examples Meets Historical Linguistics: A Large Language Model Based Approach to Sound Law Induction","abstract":"Historical linguists have long written \"programs\" that convert reconstructed words in an ancestor language into their attested descendants via ordered string rewrite functions (called sound laws) However, writing these programs is time-consuming, motivating the development of automated Sound Law Induction (SLI) which we formulate as Programming by Examples (PBE) with Large Language Models (LLMs) in this paper. While LLMs have been effective for code generation, recent work has shown that PBE is challenging but improvable by fine-tuning, especially with training data drawn from the same distribution as evaluation data. In this paper, we create a conceptual framework of what constitutes a \"similar distribution\" for SLI and propose four kinds of synthetic data generation methods with varying amounts of inductive bias to investigate what leads to the best performance. Based on the results we create a SOTA open-source model for SLI as PBE (+6% pass rate with a third of the parameters of the second-best LLM) and also highlight exciting future directions for PBE research.","sentences":["Historical linguists have long written \"programs\" that convert reconstructed words in an ancestor language into their attested descendants via ordered string rewrite functions (called sound laws)","However, writing these programs is time-consuming, motivating the development of automated Sound Law Induction (SLI) which we formulate as Programming by Examples (PBE) with Large Language Models (LLMs) in this paper.","While LLMs have been effective for code generation, recent work has shown that PBE is challenging but improvable by fine-tuning, especially with training data drawn from the same distribution as evaluation data.","In this paper, we create a conceptual framework of what constitutes a \"similar distribution\" for SLI and propose four kinds of synthetic data generation methods with varying amounts of inductive bias to investigate what leads to the best performance.","Based on the results we create a SOTA open-source model for SLI as PBE (+6% pass rate with a third of the parameters of the second-best LLM) and also highlight exciting future directions for PBE research."],"url":"http://arxiv.org/abs/2501.16524v1"}
{"created":"2025-01-27 21:30:02","title":"How well can LLMs Grade Essays in Arabic?","abstract":"This research assesses the effectiveness of state-of-the-art large language models (LLMs), including ChatGPT, Llama, Aya, Jais, and ACEGPT, in the task of Arabic automated essay scoring (AES) using the AR-AES dataset. It explores various evaluation methodologies, including zero-shot, few-shot in-context learning, and fine-tuning, and examines the influence of instruction-following capabilities through the inclusion of marking guidelines within the prompts. A mixed-language prompting strategy, integrating English prompts with Arabic content, was implemented to improve model comprehension and performance. Among the models tested, ACEGPT demonstrated the strongest performance across the dataset, achieving a Quadratic Weighted Kappa (QWK) of 0.67, but was outperformed by a smaller BERT-based model with a QWK of 0.88. The study identifies challenges faced by LLMs in processing Arabic, including tokenization complexities and higher computational demands. Performance variation across different courses underscores the need for adaptive models capable of handling diverse assessment formats and highlights the positive impact of effective prompt engineering on improving LLM outputs. To the best of our knowledge, this study is the first to empirically evaluate the performance of multiple generative Large Language Models (LLMs) on Arabic essays using authentic student data.","sentences":["This research assesses the effectiveness of state-of-the-art large language models (LLMs), including ChatGPT, Llama, Aya, Jais, and ACEGPT, in the task of Arabic automated essay scoring (AES) using the AR-AES dataset.","It explores various evaluation methodologies, including zero-shot, few-shot in-context learning, and fine-tuning, and examines the influence of instruction-following capabilities through the inclusion of marking guidelines within the prompts.","A mixed-language prompting strategy, integrating English prompts with Arabic content, was implemented to improve model comprehension and performance.","Among the models tested, ACEGPT demonstrated the strongest performance across the dataset, achieving a Quadratic Weighted Kappa (QWK) of 0.67, but was outperformed by a smaller BERT-based model with a QWK of 0.88.","The study identifies challenges faced by LLMs in processing Arabic, including tokenization complexities and higher computational demands.","Performance variation across different courses underscores the need for adaptive models capable of handling diverse assessment formats and highlights the positive impact of effective prompt engineering on improving LLM outputs.","To the best of our knowledge, this study is the first to empirically evaluate the performance of multiple generative Large Language Models (LLMs) on Arabic essays using authentic student data."],"url":"http://arxiv.org/abs/2501.16516v1"}
{"created":"2025-01-27 21:14:18","title":"Characterizing Network Structure of Anti-Trans Actors on TikTok","abstract":"The recent proliferation of short form video social media sites such as TikTok has been effectively utilized for increased visibility, communication, and community connection amongst trans/nonbinary creators online. However, these same platforms have also been exploited by right-wing actors targeting trans/nonbinary people, enabling such anti-trans actors to efficiently spread hate speech and propaganda. Given these divergent groups, what are the differences in network structure between anti-trans and pro-trans communities on TikTok, and to what extent do they amplify the effects of anti-trans content? In this paper, we collect a sample of TikTok videos containing pro and anti-trans content, and develop a taxonomy of trans related sentiment to enable the classification of content on TikTok, and ultimately analyze the reply network structures of pro-trans and anti-trans communities. In order to accomplish this, we worked with hired expert data annotators from the trans/nonbinary community in order to generate a sample of highly accurately labeled data. From this subset, we utilized a novel classification pipeline leveraging Retrieval-Augmented Generation (RAG) with annotated examples and taxonomy definitions to classify content into pro-trans, anti-trans, or neutral categories. We find that incorporating our taxonomy and its logics into our classification engine results in improved ability to differentiate trans related content, and that Results from network analysis indicate many interactions between posters of pro-trans and anti-trans content exist, further demonstrating targeting of trans individuals, and demonstrating the need for better content moderation tools","sentences":["The recent proliferation of short form video social media sites such as TikTok has been effectively utilized for increased visibility, communication, and community connection amongst trans/nonbinary creators online.","However, these same platforms have also been exploited by right-wing actors targeting trans/nonbinary people, enabling such anti-trans actors to efficiently spread hate speech and propaganda.","Given these divergent groups, what are the differences in network structure between anti-trans and pro-trans communities on TikTok, and to what extent do they amplify the effects of anti-trans content?","In this paper, we collect a sample of TikTok videos containing pro and anti-trans content, and develop a taxonomy of trans related sentiment to enable the classification of content on TikTok, and ultimately analyze the reply network structures of pro-trans and anti-trans communities.","In order to accomplish this, we worked with hired expert data annotators from the trans/nonbinary community in order to generate a sample of highly accurately labeled data.","From this subset, we utilized a novel classification pipeline leveraging Retrieval-Augmented Generation (RAG) with annotated examples and taxonomy definitions to classify content into pro-trans, anti-trans, or neutral categories.","We find that incorporating our taxonomy and its logics into our classification engine results in improved ability to differentiate trans related content, and that Results from network analysis indicate many interactions between posters of pro-trans and anti-trans content exist, further demonstrating targeting of trans individuals, and demonstrating the need for better content moderation tools"],"url":"http://arxiv.org/abs/2501.16507v1"}
{"created":"2025-01-27 21:04:05","title":"dApps: Enabling Real-Time AI-Based Open RAN Control","abstract":"Open Radio Access Networks (RANs) leverage disaggregated and programmable RAN functions and open interfaces to enable closed-loop, data-driven radio resource management. This is performed through custom intelligent applications on the RAN Intelligent Controllers (RICs), optimizing RAN policy scheduling, network slicing, user session management, and medium access control, among others. In this context, we have proposed dApps as a key extension of the O-RAN architecture into the real-time and user-plane domains. Deployed directly on RAN nodes, dApps access data otherwise unavailable to RICs due to privacy or timing constraints, enabling the execution of control actions within shorter time intervals. In this paper, we propose for the first time a reference architecture for dApps, defining their life cycle from deployment by the Service Management and Orchestration (SMO) to real-time control loop interactions with the RAN nodes where they are hosted. We introduce a new dApp interface, E3, along with an Application Protocol (AP) that supports structured message exchanges and extensible communication for various service models. By bridging E3 with the existing O-RAN E2 interface, we enable dApps, xApps, and rApps to coexist and coordinate. These applications can then collaborate on complex use cases and employ hierarchical control to resolve shared resource conflicts. Finally, we present and open-source a dApp framework based on OpenAirInterface (OAI). We benchmark its performance in two real-time control use cases, i.e., spectrum sharing and positioning in a 5th generation (5G) Next Generation Node Base (gNB) scenario. Our experimental results show that standardized real-time control loops via dApps are feasible, achieving average control latency below 450 microseconds and allowing optimal use of shared spectral resources.","sentences":["Open Radio Access Networks (RANs) leverage disaggregated and programmable RAN functions and open interfaces to enable closed-loop, data-driven radio resource management.","This is performed through custom intelligent applications on the RAN Intelligent Controllers (RICs), optimizing RAN policy scheduling, network slicing, user session management, and medium access control, among others.","In this context, we have proposed dApps as a key extension of the O-RAN architecture into the real-time and user-plane domains.","Deployed directly on RAN nodes, dApps access data otherwise unavailable to RICs due to privacy or timing constraints, enabling the execution of control actions within shorter time intervals.","In this paper, we propose for the first time a reference architecture for dApps, defining their life cycle from deployment by the Service Management and Orchestration (SMO) to real-time control loop interactions with the RAN nodes where they are hosted.","We introduce a new dApp interface, E3, along with an Application Protocol (AP) that supports structured message exchanges and extensible communication for various service models.","By bridging E3 with the existing O-RAN E2 interface, we enable dApps, xApps, and rApps to coexist and coordinate.","These applications can then collaborate on complex use cases and employ hierarchical control to resolve shared resource conflicts.","Finally, we present and open-source a dApp framework based on OpenAirInterface (OAI).","We benchmark its performance in two real-time control use cases, i.e., spectrum sharing and positioning in a 5th generation (5G)","Next Generation Node Base (gNB) scenario.","Our experimental results show that standardized real-time control loops via dApps are feasible, achieving average control latency below 450 microseconds and allowing optimal use of shared spectral resources."],"url":"http://arxiv.org/abs/2501.16502v1"}
{"created":"2025-01-27 20:55:01","title":"Classroom Activities and New Classroom Apps for Enhancing Children's Understanding of Social Media Mechanisms","abstract":"Young people are increasingly exposed to adverse effects of data-driven profiling, recommending, and manipulation on social media platforms, most of them without adequate understanding of the mechanisms that drive these platforms. In the context of computing education, educating learners about mechanisms and data practices of social media may improve young learners' data agency, digital literacy, and understanding how their digital services work. A four-hour technology -- supported intervention was designed and implemented in 12 schools involving 209 5th and 8th grade learners. Two new classroom apps were developed to support the classroom activities. Using Likert-scale questions borrowed from a data agency questionnaire and open-ended questions that mapped learners' data-driven reasoning on social media phenomena, this article shows significant improvement between pre- and post-tests in learners' data agency and data-driven explanations of social media mechanisms. Results present an example of improving young learners' understanding of social media mechanisms.","sentences":["Young people are increasingly exposed to adverse effects of data-driven profiling, recommending, and manipulation on social media platforms, most of them without adequate understanding of the mechanisms that drive these platforms.","In the context of computing education, educating learners about mechanisms and data practices of social media may improve young learners' data agency, digital literacy, and understanding how their digital services work.","A four-hour technology -- supported intervention was designed and implemented in 12 schools involving 209 5th and 8th grade learners.","Two new classroom apps were developed to support the classroom activities.","Using Likert-scale questions borrowed from a data agency questionnaire and open-ended questions that mapped learners' data-driven reasoning on social media phenomena, this article shows significant improvement between pre- and post-tests in learners' data agency and data-driven explanations of social media mechanisms.","Results present an example of improving young learners' understanding of social media mechanisms."],"url":"http://arxiv.org/abs/2501.16494v1"}
{"created":"2025-01-27 20:48:25","title":"Towards Robust Stability Prediction in Smart Grids: GAN-based Approach under Data Constraints and Adversarial Challenges","abstract":"Smart grids are critical for addressing the growing energy demand due to global population growth and urbanization. They enhance efficiency, reliability, and sustainability by integrating renewable energy. Ensuring their availability and safety requires advanced operational control and safety measures. Researchers employ AI and machine learning to assess grid stability, but challenges like the lack of datasets and cybersecurity threats, including adversarial attacks, persist. In particular, data scarcity is a key issue: obtaining grid instability instances is tough due to the need for significant expertise, resources, and time. However, they are essential to test novel research advancements and security mitigations. In this paper, we introduce a novel framework to detect instability in smart grids by employing only stable data. It relies on a Generative Adversarial Network (GAN) where the generator is trained to create instability data that are used along with stable data to train the discriminator. Moreover, we include a new adversarial training layer to improve robustness against adversarial attacks. Our solution, tested on a dataset composed of real-world stable and unstable samples, achieve accuracy up to 97.5\\% in predicting grid stability and up to 98.9\\% in detecting adversarial attacks. Moreover, we implemented our model in a single-board computer demonstrating efficient real-time decision-making with an average response time of less than 7ms. Our solution improves prediction accuracy and resilience while addressing data scarcity in smart grid management.","sentences":["Smart grids are critical for addressing the growing energy demand due to global population growth and urbanization.","They enhance efficiency, reliability, and sustainability by integrating renewable energy.","Ensuring their availability and safety requires advanced operational control and safety measures.","Researchers employ AI and machine learning to assess grid stability, but challenges like the lack of datasets and cybersecurity threats, including adversarial attacks, persist.","In particular, data scarcity is a key issue: obtaining grid instability instances is tough due to the need for significant expertise, resources, and time.","However, they are essential to test novel research advancements and security mitigations.","In this paper, we introduce a novel framework to detect instability in smart grids by employing only stable data.","It relies on a Generative Adversarial Network (GAN) where the generator is trained to create instability data that are used along with stable data to train the discriminator.","Moreover, we include a new adversarial training layer to improve robustness against adversarial attacks.","Our solution, tested on a dataset composed of real-world stable and unstable samples, achieve accuracy up to 97.5\\% in predicting grid stability and up to 98.9\\% in detecting adversarial attacks.","Moreover, we implemented our model in a single-board computer demonstrating efficient real-time decision-making with an average response time of less than 7ms.","Our solution improves prediction accuracy and resilience while addressing data scarcity in smart grid management."],"url":"http://arxiv.org/abs/2501.16490v1"}
{"created":"2025-01-27 20:41:38","title":"Enhanced Position Estimation in Tactile Internet-Enabled Remote Robotic Surgery Using MOESP-Based Kalman Filter","abstract":"Accurately estimating the position of a patient's side robotic arm in real time during remote surgery is a significant challenge, especially within Tactile Internet (TI) environments. This paper presents a new and efficient method for position estimation using a Kalman Filter (KF) combined with the Multivariable Output-Error State Space (MOESP) method for system identification. Unlike traditional approaches that require prior knowledge of the system's dynamics, this study uses the JIGSAW dataset, a comprehensive collection of robotic surgical data, along with input from the Master Tool Manipulator (MTM) to derive the state-space model directly. The MOESP method allows accurate modeling of the Patient Side Manipulator (PSM) dynamics without prior system models, improving the KF's performance under simulated network conditions, including delays, jitter, and packet loss. These conditions mimic real-world challenges in Tactile Internet applications. The findings demonstrate the KF's improved resilience and accuracy in state estimation, achieving over 95 percent accuracy despite network-induced uncertainties.","sentences":["Accurately estimating the position of a patient's side robotic arm in real time during remote surgery is a significant challenge, especially within Tactile Internet (TI) environments.","This paper presents a new and efficient method for position estimation using a Kalman Filter (KF) combined with the Multivariable Output-Error State Space (MOESP) method for system identification.","Unlike traditional approaches that require prior knowledge of the system's dynamics, this study uses the JIGSAW dataset, a comprehensive collection of robotic surgical data, along with input from the Master Tool Manipulator (MTM) to derive the state-space model directly.","The MOESP method allows accurate modeling of the Patient Side Manipulator (PSM) dynamics without prior system models, improving the KF's performance under simulated network conditions, including delays, jitter, and packet loss.","These conditions mimic real-world challenges in Tactile Internet applications.","The findings demonstrate the KF's improved resilience and accuracy in state estimation, achieving over 95 percent accuracy despite network-induced uncertainties."],"url":"http://arxiv.org/abs/2501.16485v1"}
{"created":"2025-01-27 20:28:01","title":"Generating customized prompts for Zero-Shot Rare Event Medical Image Classification using LLM","abstract":"Rare events, due to their infrequent occurrences, do not have much data, and hence deep learning techniques fail in estimating the distribution for such data. Open-vocabulary models represent an innovative approach to image classification. Unlike traditional models, these models classify images into any set of categories specified with natural language prompts during inference. These prompts usually comprise manually crafted templates (e.g., 'a photo of a {}') that are filled in with the names of each category. This paper introduces a simple yet effective method for generating highly accurate and contextually descriptive prompts containing discriminative characteristics. Rare event detection, especially in medicine, is more challenging due to low inter-class and high intra-class variability. To address these, we propose a novel approach that uses domain-specific expert knowledge on rare events to generate customized and contextually relevant prompts, which are then used by large language models for image classification. Our zero-shot, privacy-preserving method enhances rare event classification without additional training, outperforming state-of-the-art techniques.","sentences":["Rare events, due to their infrequent occurrences, do not have much data, and hence deep learning techniques fail in estimating the distribution for such data.","Open-vocabulary models represent an innovative approach to image classification.","Unlike traditional models, these models classify images into any set of categories specified with natural language prompts during inference.","These prompts usually comprise manually crafted templates (e.g., 'a photo of a {}') that are filled in with the names of each category.","This paper introduces a simple yet effective method for generating highly accurate and contextually descriptive prompts containing discriminative characteristics.","Rare event detection, especially in medicine, is more challenging due to low inter-class and high intra-class variability.","To address these, we propose a novel approach that uses domain-specific expert knowledge on rare events to generate customized and contextually relevant prompts, which are then used by large language models for image classification.","Our zero-shot, privacy-preserving method enhances rare event classification without additional training, outperforming state-of-the-art techniques."],"url":"http://arxiv.org/abs/2501.16481v1"}
{"created":"2025-01-27 20:05:17","title":"SIM: Surface-based fMRI Analysis for Inter-Subject Multimodal Decoding from Movie-Watching Experiments","abstract":"Current AI frameworks for brain decoding and encoding, typically train and test models within the same datasets. This limits their utility for brain computer interfaces (BCI) or neurofeedback, for which it would be useful to pool experiences across individuals to better simulate stimuli not sampled during training. A key obstacle to model generalisation is the degree of variability of inter-subject cortical organisation, which makes it difficult to align or compare cortical signals across participants. In this paper we address this through the use of surface vision transformers, which build a generalisable model of cortical functional dynamics, through encoding the topography of cortical networks and their interactions as a moving image across a surface. This is then combined with tri-modal self-supervised contrastive (CLIP) alignment of audio, video, and fMRI modalities to enable the retrieval of visual and auditory stimuli from patterns of cortical activity (and vice-versa). We validate our approach on 7T task-fMRI data from 174 healthy participants engaged in the movie-watching experiment from the Human Connectome Project (HCP). Results show that it is possible to detect which movie clips an individual is watching purely from their brain activity, even for individuals and movies not seen during training. Further analysis of attention maps reveals that our model captures individual patterns of brain activity that reflect semantic and visual systems. This opens the door to future personalised simulations of brain function. Code & pre-trained models will be made available at https://github.com/metrics-lab/sim, processed data for training will be available upon request at https://gin.g-node.org/Sdahan30/sim.","sentences":["Current AI frameworks for brain decoding and encoding, typically train and test models within the same datasets.","This limits their utility for brain computer interfaces (BCI) or neurofeedback, for which it would be useful to pool experiences across individuals to better simulate stimuli not sampled during training.","A key obstacle to model generalisation is the degree of variability of inter-subject cortical organisation, which makes it difficult to align or compare cortical signals across participants.","In this paper we address this through the use of surface vision transformers, which build a generalisable model of cortical functional dynamics, through encoding the topography of cortical networks and their interactions as a moving image across a surface.","This is then combined with tri-modal self-supervised contrastive (CLIP) alignment of audio, video, and fMRI modalities to enable the retrieval of visual and auditory stimuli from patterns of cortical activity (and vice-versa).","We validate our approach on 7T task-fMRI data from 174 healthy participants engaged in the movie-watching experiment from the Human Connectome Project (HCP).","Results show that it is possible to detect which movie clips an individual is watching purely from their brain activity, even for individuals and movies not seen during training.","Further analysis of attention maps reveals that our model captures individual patterns of brain activity that reflect semantic and visual systems.","This opens the door to future personalised simulations of brain function.","Code & pre-trained models will be made available at https://github.com/metrics-lab/sim, processed data for training will be available upon request at https://gin.g-node.org/Sdahan30/sim."],"url":"http://arxiv.org/abs/2501.16471v1"}
{"created":"2025-01-27 20:02:53","title":"Object Detection for Medical Image Analysis: Insights from the RT-DETR Model","abstract":"Deep learning has emerged as a transformative approach for solving complex pattern recognition and object detection challenges. This paper focuses on the application of a novel detection framework based on the RT-DETR model for analyzing intricate image data, particularly in areas such as diabetic retinopathy detection. Diabetic retinopathy, a leading cause of vision loss globally, requires accurate and efficient image analysis to identify early-stage lesions. The proposed RT-DETR model, built on a Transformer-based architecture, excels at processing high-dimensional and complex visual data with enhanced robustness and accuracy. Comparative evaluations with models such as YOLOv5, YOLOv8, SSD, and DETR demonstrate that RT-DETR achieves superior performance across precision, recall, mAP50, and mAP50-95 metrics, particularly in detecting small-scale objects and densely packed targets. This study underscores the potential of Transformer-based models like RT-DETR for advancing object detection tasks, offering promising applications in medical imaging and beyond.","sentences":["Deep learning has emerged as a transformative approach for solving complex pattern recognition and object detection challenges.","This paper focuses on the application of a novel detection framework based on the RT-DETR model for analyzing intricate image data, particularly in areas such as diabetic retinopathy detection.","Diabetic retinopathy, a leading cause of vision loss globally, requires accurate and efficient image analysis to identify early-stage lesions.","The proposed RT-DETR model, built on a Transformer-based architecture, excels at processing high-dimensional and complex visual data with enhanced robustness and accuracy.","Comparative evaluations with models such as YOLOv5, YOLOv8, SSD, and DETR demonstrate that RT-DETR achieves superior performance across precision, recall, mAP50, and mAP50-95 metrics, particularly in detecting small-scale objects and densely packed targets.","This study underscores the potential of Transformer-based models like RT-DETR for advancing object detection tasks, offering promising applications in medical imaging and beyond."],"url":"http://arxiv.org/abs/2501.16469v1"}
{"created":"2025-01-27 19:58:29","title":"On the Feasibility of Using LLMs to Execute Multistage Network Attacks","abstract":"LLMs have shown preliminary promise in some security tasks and CTF challenges. However, it is unclear whether LLMs are able to realize multistage network attacks, which involve executing a wide variety of actions across multiple hosts such as conducting reconnaissance, exploiting vulnerabilities to gain initial access, leveraging internal hosts to move laterally, and using multiple compromised hosts to exfiltrate data. We evaluate LLMs across 10 multistage networks and find that popular LLMs are unable to realize these attacks. To enable LLMs to realize these attacks, we introduce Incalmo, an LLM-agnostic high-level attack abstraction layer that sits between an LLM and the environment. Rather than LLMs issuing low-level command-line instructions, which can lead to incorrect implementations, Incalmo allows LLMs to specify high-level tasks (e.g., infect a host, scan a network), which are then carried out by Incalmo. Incalmo realizes these tasks by translating them into low-level primitives (e.g., commands to exploit tools). Incalmo also provides an environment state service and an attack graph service to provide structure to LLMs in selecting actions relevant to a multistage attack. Across 9 out of 10 realistic emulated networks (from 25 to 50 hosts), LLMs using Incalmo can successfully autonomously execute multistage attacks. We also conduct an ablation analysis to show the key role the high-level abstractions play. For instance, we find that both Incalmo's high-level tasks and services are crucial. Furthermore, even smaller-parameter LLMs with Incalmo can fully succeed in 5 of 10 environments, while larger-parameter LLMs without Incalmo do not fully succeed in any.","sentences":["LLMs have shown preliminary promise in some security tasks and CTF challenges.","However, it is unclear whether LLMs are able to realize multistage network attacks, which involve executing a wide variety of actions across multiple hosts such as conducting reconnaissance, exploiting vulnerabilities to gain initial access, leveraging internal hosts to move laterally, and using multiple compromised hosts to exfiltrate data.","We evaluate LLMs across 10 multistage networks and find that popular LLMs are unable to realize these attacks.","To enable LLMs to realize these attacks, we introduce Incalmo, an LLM-agnostic high-level attack abstraction layer that sits between an LLM and the environment.","Rather than LLMs issuing low-level command-line instructions, which can lead to incorrect implementations, Incalmo allows LLMs to specify high-level tasks (e.g., infect a host, scan a network), which are then carried out by Incalmo.","Incalmo realizes these tasks by translating them into low-level primitives (e.g., commands to exploit tools).","Incalmo also provides an environment state service and an attack graph service to provide structure to LLMs in selecting actions relevant to a multistage attack.","Across 9 out of 10 realistic emulated networks (from 25 to 50 hosts), LLMs using Incalmo can successfully autonomously execute multistage attacks.","We also conduct an ablation analysis to show the key role the high-level abstractions play.","For instance, we find that both Incalmo's high-level tasks and services are crucial.","Furthermore, even smaller-parameter LLMs with Incalmo can fully succeed in 5 of 10 environments, while larger-parameter LLMs without Incalmo do not fully succeed in any."],"url":"http://arxiv.org/abs/2501.16466v1"}
{"created":"2025-01-27 19:37:18","title":"BiFold: Bimanual Cloth Folding with Language Guidance","abstract":"Cloth folding is a complex task due to the inevitable self-occlusions of clothes, their complicated dynamics, and the disparate materials, geometries, and textures that garments can have. In this work, we learn folding actions conditioned on text commands. Translating high-level, abstract instructions into precise robotic actions requires sophisticated language understanding and manipulation capabilities. To do that, we leverage a pre-trained vision-language model and repurpose it to predict manipulation actions. Our model, BiFold, can take context into account and achieves state-of-the-art performance on an existing language-conditioned folding benchmark. Given the lack of annotated bimanual folding data, we devise a procedure to automatically parse actions of a simulated dataset and tag them with aligned text instructions. BiFold attains the best performance on our dataset and can transfer to new instructions, garments, and environments.","sentences":["Cloth folding is a complex task due to the inevitable self-occlusions of clothes, their complicated dynamics, and the disparate materials, geometries, and textures that garments can have.","In this work, we learn folding actions conditioned on text commands.","Translating high-level, abstract instructions into precise robotic actions requires sophisticated language understanding and manipulation capabilities.","To do that, we leverage a pre-trained vision-language model and repurpose it to predict manipulation actions.","Our model, BiFold, can take context into account and achieves state-of-the-art performance on an existing language-conditioned folding benchmark.","Given the lack of annotated bimanual folding data, we devise a procedure to automatically parse actions of a simulated dataset and tag them with aligned text instructions.","BiFold attains the best performance on our dataset and can transfer to new instructions, garments, and environments."],"url":"http://arxiv.org/abs/2501.16458v1"}
{"created":"2025-01-27 19:29:11","title":"CoCoNUT: Structural Code Understanding does not fall out of a tree","abstract":"Large Language Models (LLMs) have shown impressive performance across a wide array of tasks involving both structured and unstructured textual data. Recent results on various benchmarks for code generation, repair, or completion suggest that certain models have programming abilities comparable to or even surpass humans. In this work, we demonstrate that high performance on such benchmarks does not correlate to humans' innate ability to understand structural control flow in code. To this end, we extract solutions from the HumanEval benchmark, which the relevant models perform strongly on, and trace their execution path using function calls sampled from the respective test set. Using this dataset, we investigate the ability of seven state-of-the-art LLMs to match the execution trace and find that, despite their ability to generate semantically identical code, they possess limited ability to trace execution paths, especially for longer traces and specific control structures. We find that even the top-performing model, Gemini, can fully and correctly generate only 47% of HumanEval task traces. Additionally, we introduce a subset for three key structures not contained in HumanEval: Recursion, Parallel Processing, and Object-Oriented Programming, including concepts like Inheritance and Polymorphism. Besides OOP, we show that none of the investigated models achieve an accuracy over 5% on the relevant traces. Aggregating these specialized parts with HumanEval tasks, we present Benchmark CoCoNUT: Code Control Flow for Navigation Understanding and Testing, which measures a model's ability to trace execution of code upon relevant calls, including advanced structural components. We conclude that current LLMs need significant improvement to enhance code reasoning abilities. We hope our dataset helps researchers bridge this gap.","sentences":["Large Language Models (LLMs) have shown impressive performance across a wide array of tasks involving both structured and unstructured textual data.","Recent results on various benchmarks for code generation, repair, or completion suggest that certain models have programming abilities comparable to or even surpass humans.","In this work, we demonstrate that high performance on such benchmarks does not correlate to humans' innate ability to understand structural control flow in code.","To this end, we extract solutions from the HumanEval benchmark, which the relevant models perform strongly on, and trace their execution path using function calls sampled from the respective test set.","Using this dataset, we investigate the ability of seven state-of-the-art LLMs to match the execution trace and find that, despite their ability to generate semantically identical code, they possess limited ability to trace execution paths, especially for longer traces and specific control structures.","We find that even the top-performing model, Gemini, can fully and correctly generate only 47% of HumanEval task traces.","Additionally, we introduce a subset for three key structures not contained in HumanEval: Recursion, Parallel Processing, and Object-Oriented Programming, including concepts like Inheritance and Polymorphism.","Besides OOP, we show that none of the investigated models achieve an accuracy over 5% on the relevant traces.","Aggregating these specialized parts with HumanEval tasks, we present Benchmark CoCoNUT: Code Control Flow for Navigation Understanding and Testing, which measures a model's ability to trace execution of code upon relevant calls, including advanced structural components.","We conclude that current LLMs need significant improvement to enhance code reasoning abilities.","We hope our dataset helps researchers bridge this gap."],"url":"http://arxiv.org/abs/2501.16456v1"}
{"created":"2025-01-27 19:14:52","title":"360Brew: A Decoder-only Foundation Model for Personalized Ranking and Recommendation","abstract":"Ranking and recommendation systems are the foundation for numerous online experiences, ranging from search results to personalized content delivery. These systems have evolved into complex, multilayered architectures that leverage vast datasets and often incorporate thousands of predictive models. The maintenance and enhancement of these models is a labor intensive process that requires extensive feature engineering. This approach not only exacerbates technical debt but also hampers innovation in extending these systems to emerging problem domains. In this report, we present our research to address these challenges by utilizing a large foundation model with a textual interface for ranking and recommendation tasks. We illustrate several key advantages of our approach: (1) a single model can manage multiple predictive tasks involved in ranking and recommendation, (2) decoder models with textual interface due to their comprehension of reasoning capabilities, can generalize to new recommendation surfaces and out-of-domain problems, and (3) by employing natural language interfaces for task definitions and verbalizing member behaviors and their social connections, we eliminate the need for feature engineering and the maintenance of complex directed acyclic graphs of model dependencies. We introduce our research pre-production model, 360Brew V1.0, a 150B parameter, decoder-only model that has been trained and fine-tuned on LinkedIn's data and tasks. This model is capable of solving over 30 predictive tasks across various segments of the LinkedIn platform, achieving performance levels comparable to or exceeding those of current production systems based on offline metrics, without task-specific fine-tuning. Notably, each of these tasks is conventionally addressed by dedicated models that have been developed and maintained over multiple years by teams of a similar or larger size than our own.","sentences":["Ranking and recommendation systems are the foundation for numerous online experiences, ranging from search results to personalized content delivery.","These systems have evolved into complex, multilayered architectures that leverage vast datasets and often incorporate thousands of predictive models.","The maintenance and enhancement of these models is a labor intensive process that requires extensive feature engineering.","This approach not only exacerbates technical debt but also hampers innovation in extending these systems to emerging problem domains.","In this report, we present our research to address these challenges by utilizing a large foundation model with a textual interface for ranking and recommendation tasks.","We illustrate several key advantages of our approach: (1) a single model can manage multiple predictive tasks involved in ranking and recommendation, (2) decoder models with textual interface due to their comprehension of reasoning capabilities, can generalize to new recommendation surfaces and out-of-domain problems, and (3) by employing natural language interfaces for task definitions and verbalizing member behaviors and their social connections, we eliminate the need for feature engineering and the maintenance of complex directed acyclic graphs of model dependencies.","We introduce our research pre-production model, 360Brew V1.0, a 150B parameter, decoder-only model that has been trained and fine-tuned on LinkedIn's data and tasks.","This model is capable of solving over 30 predictive tasks across various segments of the LinkedIn platform, achieving performance levels comparable to or exceeding those of current production systems based on offline metrics, without task-specific fine-tuning.","Notably, each of these tasks is conventionally addressed by dedicated models that have been developed and maintained over multiple years by teams of a similar or larger size than our own."],"url":"http://arxiv.org/abs/2501.16450v1"}
{"created":"2025-01-27 19:13:39","title":"What is Harm? Baby Don't Hurt Me! On the Impossibility of Complete Harm Specification in AI Alignment","abstract":"\"First, do no harm\" faces a fundamental challenge in artificial intelligence: how can we specify what constitutes harm? While prior work treats harm specification as a technical hurdle to be overcome through better algorithms or more data, we argue this assumption is unsound. Drawing on information theory, we demonstrate that complete harm specification is fundamentally impossible for any system where harm is defined external to its specifications. This impossibility arises from an inescapable information-theoretic gap: the entropy of harm H(O) always exceeds the mutual information I(O;I) between ground truth harm O and a system's specifications I.   We introduce two novel metrics: semantic entropy H(S) and the safety-capability ratio I(O;I)/H(O), to quantify these limitations. Through a progression of increasingly sophisticated specification attempts, we show why each approach must fail and why the resulting gaps are not mere engineering challenges but fundamental constraints akin to the halting problem. These results suggest a paradigm shift: rather than pursuing complete specifications, AI alignment research should focus on developing systems that can operate safely despite irreducible specification uncertainty.","sentences":["\"First, do no harm\" faces a fundamental challenge in artificial intelligence: how can we specify what constitutes harm?","While prior work treats harm specification as a technical hurdle to be overcome through better algorithms or more data, we argue this assumption is unsound.","Drawing on information theory, we demonstrate that complete harm specification is fundamentally impossible for any system where harm is defined external to its specifications.","This impossibility arises from an inescapable information-theoretic gap: the entropy of harm H(O) always exceeds the mutual information I(O;I) between ground truth harm O and a system's specifications I.   ","We introduce two novel metrics: semantic entropy H(S) and the safety-capability ratio I(O;I)/H(O), to quantify these limitations.","Through a progression of increasingly sophisticated specification attempts, we show why each approach must fail and why the resulting gaps are not mere engineering challenges but fundamental constraints akin to the halting problem.","These results suggest a paradigm shift: rather than pursuing complete specifications, AI alignment research should focus on developing systems that can operate safely despite irreducible specification uncertainty."],"url":"http://arxiv.org/abs/2501.16448v1"}
