{"created":"2024-08-15 17:59:57","title":"Can Large Language Models Understand Symbolic Graphics Programs?","abstract":"Assessing the capabilities of large language models (LLMs) is often challenging, in part, because it is hard to find tasks to which they have not been exposed during training. We take one step to address this challenge by turning to a new task: focusing on symbolic graphics programs, which are a popular representation for graphics content that procedurally generates visual data. LLMs have shown exciting promise towards program synthesis, but do they understand symbolic graphics programs? Unlike conventional programs, symbolic graphics programs can be translated to graphics content. Here, we characterize an LLM's understanding of symbolic programs in terms of their ability to answer questions related to the graphics content. This task is challenging as the questions are difficult to answer from the symbolic programs alone -- yet, they would be easy to answer from the corresponding graphics content as we verify through a human experiment. To understand symbolic programs, LLMs may need to possess the ability to imagine how the corresponding graphics content would look without directly accessing the rendered visual content. We use this task to evaluate LLMs by creating a large benchmark for the semantic understanding of symbolic graphics programs. This benchmark is built via program-graphics correspondence, hence requiring minimal human efforts. We evaluate current LLMs on our benchmark to elucidate a preliminary assessment of their ability to reason about visual scenes from programs. We find that this task distinguishes existing LLMs and models considered good at reasoning perform better. Lastly, we introduce Symbolic Instruction Tuning (SIT) to improve this ability. Specifically, we query GPT4-o with questions and images generated by symbolic programs. Such data are then used to finetune an LLM. We also find that SIT data can improve the general instruction following ability of LLMs.","sentences":["Assessing the capabilities of large language models (LLMs) is often challenging, in part, because it is hard to find tasks to which they have not been exposed during training.","We take one step to address this challenge by turning to a new task: focusing on symbolic graphics programs, which are a popular representation for graphics content that procedurally generates visual data.","LLMs have shown exciting promise towards program synthesis, but do they understand symbolic graphics programs?","Unlike conventional programs, symbolic graphics programs can be translated to graphics content.","Here, we characterize an LLM's understanding of symbolic programs in terms of their ability to answer questions related to the graphics content.","This task is challenging as the questions are difficult to answer from the symbolic programs alone -- yet, they would be easy to answer from the corresponding graphics content as we verify through a human experiment.","To understand symbolic programs, LLMs may need to possess the ability to imagine how the corresponding graphics content would look without directly accessing the rendered visual content.","We use this task to evaluate LLMs by creating a large benchmark for the semantic understanding of symbolic graphics programs.","This benchmark is built via program-graphics correspondence, hence requiring minimal human efforts.","We evaluate current LLMs on our benchmark to elucidate a preliminary assessment of their ability to reason about visual scenes from programs.","We find that this task distinguishes existing LLMs and models considered good at reasoning perform better.","Lastly, we introduce Symbolic Instruction Tuning (SIT) to improve this ability.","Specifically, we query GPT4-o with questions and images generated by symbolic programs.","Such data are then used to finetune an LLM.","We also find that SIT data can improve the general instruction following ability of LLMs."],"url":"http://arxiv.org/abs/2408.08313v1"}
{"created":"2024-08-15 17:59:53","title":"HyperTaxel: Hyper-Resolution for Taxel-Based Tactile Signals Through Contrastive Learning","abstract":"To achieve dexterity comparable to that of humans, robots must intelligently process tactile sensor data. Taxel-based tactile signals often have low spatial-resolution, with non-standardized representations. In this paper, we propose a novel framework, HyperTaxel, for learning a geometrically-informed representation of taxel-based tactile signals to address challenges associated with their spatial resolution. We use this representation and a contrastive learning objective to encode and map sparse low-resolution taxel signals to high-resolution contact surfaces. To address the uncertainty inherent in these signals, we leverage joint probability distributions across multiple simultaneous contacts to improve taxel hyper-resolution. We evaluate our representation by comparing it with two baselines and present results that suggest our representation outperforms the baselines. Furthermore, we present qualitative results that demonstrate the learned representation captures the geometric features of the contact surface, such as flatness, curvature, and edges, and generalizes across different objects and sensor configurations. Moreover, we present results that suggest our representation improves the performance of various downstream tasks, such as surface classification, 6D in-hand pose estimation, and sim-to-real transfer.","sentences":["To achieve dexterity comparable to that of humans, robots must intelligently process tactile sensor data.","Taxel-based tactile signals often have low spatial-resolution, with non-standardized representations.","In this paper, we propose a novel framework, HyperTaxel, for learning a geometrically-informed representation of taxel-based tactile signals to address challenges associated with their spatial resolution.","We use this representation and a contrastive learning objective to encode and map sparse low-resolution taxel signals to high-resolution contact surfaces.","To address the uncertainty inherent in these signals, we leverage joint probability distributions across multiple simultaneous contacts to improve taxel hyper-resolution.","We evaluate our representation by comparing it with two baselines and present results that suggest our representation outperforms the baselines.","Furthermore, we present qualitative results that demonstrate the learned representation captures the geometric features of the contact surface, such as flatness, curvature, and edges, and generalizes across different objects and sensor configurations.","Moreover, we present results that suggest our representation improves the performance of various downstream tasks, such as surface classification, 6D in-hand pose estimation, and sim-to-real transfer."],"url":"http://arxiv.org/abs/2408.08312v1"}
{"created":"2024-08-15 17:59:30","title":"ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws","abstract":"High-quality data is crucial for the pre-training performance of large language models. Unfortunately, existing quality filtering methods rely on a known high-quality dataset as reference, which can introduce potential bias and compromise diversity. In this paper, we propose ScalingFilter, a novel approach that evaluates text quality based on the perplexity difference between two language models trained on the same data, thereby eliminating the influence of the reference dataset in the filtering process. An theoretical analysis shows that ScalingFilter is equivalent to an inverse utilization of scaling laws. Through training models with 1.3B parameters on the same data source processed by various quality filters, we find ScalingFilter can improve zero-shot performance of pre-trained models in downstream tasks. To assess the bias introduced by quality filtering, we introduce semantic diversity, a metric of utilizing text embedding models for semantic representations. Extensive experiments reveal that semantic diversity is a reliable indicator of dataset diversity, and ScalingFilter achieves an optimal balance between downstream performance and semantic diversity.","sentences":["High-quality data is crucial for the pre-training performance of large language models.","Unfortunately, existing quality filtering methods rely on a known high-quality dataset as reference, which can introduce potential bias and compromise diversity.","In this paper, we propose ScalingFilter, a novel approach that evaluates text quality based on the perplexity difference between two language models trained on the same data, thereby eliminating the influence of the reference dataset in the filtering process.","An theoretical analysis shows that ScalingFilter is equivalent to an inverse utilization of scaling laws.","Through training models with 1.3B parameters on the same data source processed by various quality filters, we find ScalingFilter can improve zero-shot performance of pre-trained models in downstream tasks.","To assess the bias introduced by quality filtering, we introduce semantic diversity, a metric of utilizing text embedding models for semantic representations.","Extensive experiments reveal that semantic diversity is a reliable indicator of dataset diversity, and ScalingFilter achieves an optimal balance between downstream performance and semantic diversity."],"url":"http://arxiv.org/abs/2408.08310v1"}
{"created":"2024-08-15 17:59:06","title":"Understanding the Local Geometry of Generative Model Manifolds","abstract":"Deep generative models learn continuous representations of complex data manifolds using a finite number of samples during training. For a pre-trained generative model, the common way to evaluate the quality of the manifold representation learned, is by computing global metrics like Fr\\'echet Inception Distance using a large number of generated and real samples. However, generative model performance is not uniform across the learned manifold, e.g., for \\textit{foundation models} like Stable Diffusion generation performance can vary significantly based on the conditioning or initial noise vector being denoised. In this paper we study the relationship between the \\textit{local geometry of the learned manifold} and downstream generation. Based on the theory of continuous piecewise-linear (CPWL) generators, we use three geometric descriptors - scaling ($\\psi$), rank ($\\nu$), and complexity ($\\delta$) - to characterize a pre-trained generative model manifold locally. We provide quantitative and qualitative evidence showing that for a given latent, the local descriptors are correlated with generation aesthetics, artifacts, uncertainty, and even memorization. Finally we demonstrate that training a \\textit{reward model} on the local geometry can allow controlling the likelihood of a generated sample under the learned distribution.","sentences":["Deep generative models learn continuous representations of complex data manifolds using a finite number of samples during training.","For a pre-trained generative model, the common way to evaluate the quality of the manifold representation learned, is by computing global metrics like Fr\\'echet Inception Distance using a large number of generated and real samples.","However, generative model performance is not uniform across the learned manifold, e.g., for \\textit{foundation models} like Stable Diffusion generation performance can vary significantly based on the conditioning or initial noise vector being denoised.","In this paper we study the relationship between the \\textit{local geometry of the learned manifold} and downstream generation.","Based on the theory of continuous piecewise-linear (CPWL) generators, we use three geometric descriptors - scaling ($\\psi$), rank ($\\nu$), and complexity ($\\delta$) - to characterize a pre-trained generative model manifold locally.","We provide quantitative and qualitative evidence showing that for a given latent, the local descriptors are correlated with generation aesthetics, artifacts, uncertainty, and even memorization.","Finally we demonstrate that training a \\textit{reward model} on the local geometry can allow controlling the likelihood of a generated sample under the learned distribution."],"url":"http://arxiv.org/abs/2408.08307v1"}
{"created":"2024-08-15 17:46:54","title":"The ShareLM Collection and Plugin: Contributing Human-Model Chats for the Benefit of the Community","abstract":"Human-model conversations provide a window into users' real-world scenarios, behavior, and needs, and thus are a valuable resource for model development and research. While for-profit companies collect user data through the APIs of their models, using it internally to improve their own models, the open source and research community lags behind.   We introduce the ShareLM collection, a unified set of human conversations with large language models, and its accompanying plugin, a Web extension for voluntarily contributing user-model conversations. Where few platforms share their chats, the ShareLM plugin adds this functionality, thus, allowing users to share conversations from most platforms. The plugin allows the user to rate their conversations, both at the conversation and the response levels, and delete conversations they prefer to keep private before they ever leave the user's local storage. We release the plugin conversations as part of the ShareLM collection, and call for more community effort in the field of open human-model data.   The code, plugin, and data are available.","sentences":["Human-model conversations provide a window into users' real-world scenarios, behavior, and needs, and thus are a valuable resource for model development and research.","While for-profit companies collect user data through the APIs of their models, using it internally to improve their own models, the open source and research community lags behind.   ","We introduce the ShareLM collection, a unified set of human conversations with large language models, and its accompanying plugin, a Web extension for voluntarily contributing user-model conversations.","Where few platforms share their chats, the ShareLM plugin adds this functionality, thus, allowing users to share conversations from most platforms.","The plugin allows the user to rate their conversations, both at the conversation and the response levels, and delete conversations they prefer to keep private before they ever leave the user's local storage.","We release the plugin conversations as part of the ShareLM collection, and call for more community effort in the field of open human-model data.   ","The code, plugin, and data are available."],"url":"http://arxiv.org/abs/2408.08291v1"}
{"created":"2024-08-15 17:19:12","title":"BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts","abstract":"The Mixture of Experts (MoE) framework has become a popular architecture for large language models due to its superior performance over dense models. However, training MoEs from scratch in a large-scale regime is prohibitively expensive. Existing methods mitigate this by pre-training multiple dense expert models independently and using them to initialize an MoE. This is done by using experts' feed-forward network (FFN) to initialize the MoE's experts while merging other parameters. However, this method limits the reuse of dense model parameters to only the FFN layers, thereby constraining the advantages when \"upcycling\" these models into MoEs. We propose BAM (Branch-Attend-Mix), a simple yet effective method that addresses this shortcoming. BAM makes full use of specialized dense models by not only using their FFN to initialize the MoE layers but also leveraging experts' attention parameters fully by initializing them into a soft-variant of Mixture of Attention (MoA) layers. We explore two methods for upcycling attention parameters: 1) initializing separate attention experts from dense models including all attention parameters for the best model performance; and 2) sharing key and value parameters across all experts to facilitate for better inference efficiency. To further improve efficiency, we adopt a parallel attention transformer architecture to MoEs, which allows the attention experts and FFN experts to be computed concurrently. Our experiments on seed models ranging from 590 million to 2 billion parameters demonstrate that BAM surpasses baselines in both perplexity and downstream task performance, within the same computational and data constraints.","sentences":["The Mixture of Experts (MoE) framework has become a popular architecture for large language models due to its superior performance over dense models.","However, training MoEs from scratch in a large-scale regime is prohibitively expensive.","Existing methods mitigate this by pre-training multiple dense expert models independently and using them to initialize an MoE. This is done by using experts' feed-forward network (FFN) to initialize the MoE's experts while merging other parameters.","However, this method limits the reuse of dense model parameters to only the FFN layers, thereby constraining the advantages when \"upcycling\" these models into MoEs.","We propose BAM (Branch-Attend-Mix), a simple yet effective method that addresses this shortcoming.","BAM makes full use of specialized dense models by not only using their FFN to initialize the MoE layers but also leveraging experts' attention parameters fully by initializing them into a soft-variant of Mixture of Attention (MoA) layers.","We explore two methods for upcycling attention parameters: 1) initializing separate attention experts from dense models including all attention parameters for the best model performance; and 2) sharing key and value parameters across all experts to facilitate for better inference efficiency.","To further improve efficiency, we adopt a parallel attention transformer architecture to MoEs, which allows the attention experts and FFN experts to be computed concurrently.","Our experiments on seed models ranging from 590 million to 2 billion parameters demonstrate that BAM surpasses baselines in both perplexity and downstream task performance, within the same computational and data constraints."],"url":"http://arxiv.org/abs/2408.08274v1"}
{"created":"2024-08-15 17:14:57","title":"HeightLane: BEV Heightmap guided 3D Lane Detection","abstract":"Accurate 3D lane detection from monocular images presents significant challenges due to depth ambiguity and imperfect ground modeling. Previous attempts to model the ground have often used a planar ground assumption with limited degrees of freedom, making them unsuitable for complex road environments with varying slopes. Our study introduces HeightLane, an innovative method that predicts a height map from monocular images by creating anchors based on a multi-slope assumption. This approach provides a detailed and accurate representation of the ground. HeightLane employs the predicted heightmap along with a deformable attention-based spatial feature transform framework to efficiently convert 2D image features into 3D bird's eye view (BEV) features, enhancing spatial understanding and lane structure recognition. Additionally, the heightmap is used for the positional encoding of BEV features, further improving their spatial accuracy. This explicit view transformation bridges the gap between front-view perceptions and spatially accurate BEV representations, significantly improving detection performance. To address the lack of the necessary ground truth (GT) height map in the original OpenLane dataset, we leverage the Waymo dataset and accumulate its LiDAR data to generate a height map for the drivable area of each scene. The GT heightmaps are used to train the heightmap extraction module from monocular images. Extensive experiments on the OpenLane validation set show that HeightLane achieves state-of-the-art performance in terms of F-score, highlighting its potential in real-world applications.","sentences":["Accurate 3D lane detection from monocular images presents significant challenges due to depth ambiguity and imperfect ground modeling.","Previous attempts to model the ground have often used a planar ground assumption with limited degrees of freedom, making them unsuitable for complex road environments with varying slopes.","Our study introduces HeightLane, an innovative method that predicts a height map from monocular images by creating anchors based on a multi-slope assumption.","This approach provides a detailed and accurate representation of the ground.","HeightLane employs the predicted heightmap along with a deformable attention-based spatial feature transform framework to efficiently convert 2D image features into 3D bird's eye view (BEV) features, enhancing spatial understanding and lane structure recognition.","Additionally, the heightmap is used for the positional encoding of BEV features, further improving their spatial accuracy.","This explicit view transformation bridges the gap between front-view perceptions and spatially accurate BEV representations, significantly improving detection performance.","To address the lack of the necessary ground truth (GT) height map in the original OpenLane dataset, we leverage the Waymo dataset and accumulate its LiDAR data to generate a height map for the drivable area of each scene.","The GT heightmaps are used to train the heightmap extraction module from monocular images.","Extensive experiments on the OpenLane validation set show that HeightLane achieves state-of-the-art performance in terms of F-score, highlighting its potential in real-world applications."],"url":"http://arxiv.org/abs/2408.08270v1"}
{"created":"2024-08-15 17:01:57","title":"mhGPT: A Lightweight Generative Pre-Trained Transformer for Mental Health Text Analysis","abstract":"This paper introduces mhGPT, a lightweight generative pre-trained transformer trained on mental health-related social media and PubMed articles. Fine-tuned for specific mental health tasks, mhGPT was evaluated under limited hardware constraints and compared with state-of-the-art models like MentaLLaMA and Gemma. Despite having only 1.98 billion parameters and using just 5% of the dataset, mhGPT outperformed larger models and matched the performance of models trained on significantly more data. The key contributions include integrating diverse mental health data, creating a custom tokenizer, and optimizing a smaller architecture for low-resource settings. This research could advance AI-driven mental health care, especially in areas with limited computing power.","sentences":["This paper introduces mhGPT, a lightweight generative pre-trained transformer trained on mental health-related social media and PubMed articles.","Fine-tuned for specific mental health tasks, mhGPT was evaluated under limited hardware constraints and compared with state-of-the-art models like MentaLLaMA and Gemma.","Despite having only 1.98 billion parameters and using just 5% of the dataset, mhGPT outperformed larger models and matched the performance of models trained on significantly more data.","The key contributions include integrating diverse mental health data, creating a custom tokenizer, and optimizing a smaller architecture for low-resource settings.","This research could advance AI-driven mental health care, especially in areas with limited computing power."],"url":"http://arxiv.org/abs/2408.08261v1"}
{"created":"2024-08-15 16:55:15","title":"Palette Sparsification for Graphs with Sparse Neighborhoods","abstract":"A celebrated palette sparsification result of Assadi, Chen, and Khanna states that in every $n$-vertex graph of maximum degree $\\Delta$, sampling $\\Theta(\\log n)$ colors per vertex from $\\{1,\\ldots,\\Delta+1\\}$ almost certainly allows for a proper coloring from the sampled colors. Alon and Assadi extended this work proving a similar result for $O(\\Delta/\\log \\Delta)$-coloring triangle-free graphs. Apart from being interesting results from a combinatorial standpoint, their results have various applications to the design of graph coloring algorithms in different models of computation.   In this work, we focus on graphs with sparse neighborhoods. We say a graph $G = (V,E)$ is $k$-locally-sparse if for each vertex $v \\in V$, the subgraph $G[N(v)]$ contains at most $k$ edges. A celebrated result of Alon, Krivelevich, and Sudakov shows that such graphs are $O(\\Delta/\\log (\\Delta/\\sqrt{k}))$-colorable. For any $\\alpha\\in(0,1)$ and $k\\ll\\Delta^{2\\alpha}$, let $G$ be a $k$-locally-sparse graph. We show the following for $q=\\Theta\\left(\\Delta/\\log\\left(\\Delta^\\alpha/\\sqrt{k}\\right)\\right)$:   1. Sampling $O(\\Delta^\\alpha+\\sqrt{\\log n})$ colors per vertex is sufficient to obtain a proper $q$-coloring of $G$ from the sampled colors.   2. There exists a single-pass streaming algorithm which computes a proper $q$-coloring of $G$ with high probability using $\\tilde O(n\\Delta^{2\\alpha})$ space.   3. There exists a randomized non-adaptive sublinear-time algorithm which computes a proper $q$-coloring of $G$ with high probability using at most $\\tilde O\\left(n^{\\frac32+\\frac{\\alpha}{2-2\\alpha}}\\right)$ queries.   Our results recover and improve upon earlier work of Alon and Assadi. A key element in our proof is a proposition regarding correspondence coloring in the so-called color-degree setting, which improves upon recent work of Anderson, Kuchukova, and the author and is of independent interest.","sentences":["A celebrated palette sparsification result of Assadi, Chen, and Khanna states that in every $n$-vertex graph of maximum degree $\\Delta$, sampling $\\Theta(\\log n)$ colors per vertex from $\\{1,\\ldots,\\Delta+1\\}$ almost certainly allows for a proper coloring from the sampled colors.","Alon and Assadi extended this work proving a similar result for $O(\\Delta/\\log \\Delta)$-coloring triangle-free graphs.","Apart from being interesting results from a combinatorial standpoint, their results have various applications to the design of graph coloring algorithms in different models of computation.   ","In this work, we focus on graphs with sparse neighborhoods.","We say a graph $G = (V,E)$ is $k$-locally-sparse if for each vertex $v \\in V$, the subgraph $G[N(v)]$ contains at most $k$ edges.","A celebrated result of Alon, Krivelevich, and Sudakov shows that such graphs are $O(\\Delta/\\log (\\Delta/\\sqrt{k}))$-colorable.","For any $\\alpha\\in(0,1)$ and $k\\ll\\Delta^{2\\alpha}$, let $G$ be a $k$-locally-sparse graph.","We show the following for $q=\\Theta\\left(\\Delta/\\log\\left(\\Delta^\\alpha/\\sqrt{k}\\right)\\right)$:   1.","Sampling $O(\\Delta^\\alpha+\\sqrt{\\log n})$ colors per vertex is sufficient to obtain a proper $q$-coloring of $G$ from the sampled colors.   ","2.","There exists a single-pass streaming algorithm which computes a proper $q$-coloring of $G$ with high probability using $\\tilde O(n\\Delta^{2\\alpha})$ space.   ","3.","There exists a randomized non-adaptive sublinear-time algorithm which computes a proper $q$-coloring of $G$ with high probability using at most $\\tilde O\\left(n^{\\frac32+\\frac{\\alpha}{2-2\\alpha}}\\right)$ queries.   ","Our results recover and improve upon earlier work of Alon and Assadi.","A key element in our proof is a proposition regarding correspondence coloring in the so-called color-degree setting, which improves upon recent work of Anderson, Kuchukova, and the author and is of independent interest."],"url":"http://arxiv.org/abs/2408.08256v1"}
{"created":"2024-08-15 16:41:55","title":"Computer Vision Model Compression Techniques for Embedded Systems: A Survey","abstract":"Deep neural networks have consistently represented the state of the art in most computer vision problems. In these scenarios, larger and more complex models have demonstrated superior performance to smaller architectures, especially when trained with plenty of representative data. With the recent adoption of Vision Transformer (ViT) based architectures and advanced Convolutional Neural Networks (CNNs), the total number of parameters of leading backbone architectures increased from 62M parameters in 2012 with AlexNet to 7B parameters in 2024 with AIM-7B. Consequently, deploying such deep architectures faces challenges in environments with processing and runtime constraints, particularly in embedded systems. This paper covers the main model compression techniques applied for computer vision tasks, enabling modern models to be used in embedded systems. We present the characteristics of compression subareas, compare different approaches, and discuss how to choose the best technique and expected variations when analyzing it on various embedded devices. We also share codes to assist researchers and new practitioners in overcoming initial implementation challenges for each subarea and present trends for Model Compression. Case studies for compression models are available at \\href{https://github.com/venturusbr/cv-model-compression}{https://github.com/venturusbr/cv-model-compression}.","sentences":["Deep neural networks have consistently represented the state of the art in most computer vision problems.","In these scenarios, larger and more complex models have demonstrated superior performance to smaller architectures, especially when trained with plenty of representative data.","With the recent adoption of Vision Transformer (ViT) based architectures and advanced Convolutional Neural Networks (CNNs), the total number of parameters of leading backbone architectures increased from 62M parameters in 2012 with AlexNet to 7B parameters in 2024 with AIM-7B. Consequently, deploying such deep architectures faces challenges in environments with processing and runtime constraints, particularly in embedded systems.","This paper covers the main model compression techniques applied for computer vision tasks, enabling modern models to be used in embedded systems.","We present the characteristics of compression subareas, compare different approaches, and discuss how to choose the best technique and expected variations when analyzing it on various embedded devices.","We also share codes to assist researchers and new practitioners in overcoming initial implementation challenges for each subarea and present trends for Model Compression.","Case studies for compression models are available at \\href{https://github.com/venturusbr/cv-model-compression}{https://github.com/venturusbr/cv-model-compression}."],"url":"http://arxiv.org/abs/2408.08250v1"}
{"created":"2024-08-15 16:06:44","title":"Strong Data Processing Inequalities and their Applications to Reliable Computation","abstract":"In 1952, von Neumann gave a series of groundbreaking lectures that proved it was possible for circuits consisting of 3-input majority gates that have a sufficiently small independent probability $\\delta > 0$ of malfunctioning to reliably compute Boolean functions. In 1999, Evans and Schulman used a strong data-processing inequality (SDPI) to establish the tightest known necessary condition $\\delta < \\frac{1}{2} - \\frac{1}{2\\sqrt{k}}$ for reliable computation when the circuit consists of components that have at most $k$ inputs. In 2017, Polyanskiy and Wu distilled Evans and Schulman's SDPI argument to establish a general result on the contraction of mutual information in Bayesian networks.   In this essay, we will first introduce the problem of reliable computation from unreliable components and establish the existence of noise thresholds. We will then provide an exposition of von Neumann's result with 3-input majority gates and extend it to minority gates. We will then provide an introduction to SDPIs, which have many applications, including in statistical mechanics, portfolio theory, and lower bounds on statistical estimation under privacy constraints. We will then use the introduced material to provide an exposition of Polyanskiy and Wu's 2017 result on Bayesian networks, from which the 1999 result of Evans-Schulman follows.","sentences":["In 1952, von Neumann gave a series of groundbreaking lectures that proved it was possible for circuits consisting of 3-input majority gates that have a sufficiently small independent probability $\\delta > 0$ of malfunctioning to reliably compute Boolean functions.","In 1999, Evans and Schulman used a strong data-processing inequality (SDPI) to establish the tightest known necessary condition $\\delta < \\frac{1}{2} - \\frac{1}{2\\sqrt{k}}$ for reliable computation when the circuit consists of components that have at most $k$ inputs.","In 2017, Polyanskiy and Wu distilled Evans and Schulman's SDPI argument to establish a general result on the contraction of mutual information in Bayesian networks.   ","In this essay, we will first introduce the problem of reliable computation from unreliable components and establish the existence of noise thresholds.","We will then provide an exposition of von Neumann's result with 3-input majority gates and extend it to minority gates.","We will then provide an introduction to SDPIs, which have many applications, including in statistical mechanics, portfolio theory, and lower bounds on statistical estimation under privacy constraints.","We will then use the introduced material to provide an exposition of Polyanskiy and Wu's 2017 result on Bayesian networks, from which the 1999 result of Evans-Schulman follows."],"url":"http://arxiv.org/abs/2408.08239v1"}
{"created":"2024-08-15 15:54:25","title":"Evolving A* to Efficiently Solve the k Shortest-Path Problem (Extended Version)","abstract":"The problem of finding the shortest path in a graph G(V, E) has been widely studied. However, in many applications it is necessary to compute an arbitrary number of them, k. Even though the problem has raised a lot of interest from different research communities and many applications of it are known, it has not been addressed to the same extent as the single shortest path problem. The best algorithm known for efficiently solving this task has a time complexity of O (|E| + |V|log{|V|}+k|V|)$ when computing paths in explicit form, and is based on best-first search. This paper introduces a new search algorithm with the same time complexity, which results from a natural evolution of A* thus, it preserves all its interesting properties, making it widely applicable to many different domains. Experiments in various testbeds show a significant improvement in performance over the state of the art, often by one or two orders of magnitude.","sentences":["The problem of finding the shortest path in a graph G(V, E) has been widely studied.","However, in many applications it is necessary to compute an arbitrary number of them, k.","Even though the problem has raised a lot of interest from different research communities and many applications of it are known, it has not been addressed to the same extent as the single shortest path problem.","The best algorithm known for efficiently solving this task has a time complexity of O (|E| + |V|log{|V|}+k|V|)$ when computing paths in explicit form, and is based on best-first search.","This paper introduces a new search algorithm with the same time complexity, which results from a natural evolution of A* thus, it preserves all its interesting properties, making it widely applicable to many different domains.","Experiments in various testbeds show a significant improvement in performance over the state of the art, often by one or two orders of magnitude."],"url":"http://arxiv.org/abs/2408.08227v1"}
{"created":"2024-08-15 15:28:37","title":"RED-CT: A Systems Design Methodology for Using LLM-labeled Data to Train and Deploy Edge Classifiers for Computational Social Science","abstract":"Large language models (LLMs) have enhanced our ability to rapidly analyze and classify unstructured natural language data. However, concerns regarding cost, network limitations, and security constraints have posed challenges for their integration into work processes. In this study, we adopt a systems design approach to employing LLMs as imperfect data annotators for downstream supervised learning tasks, introducing novel system intervention measures aimed at improving classification performance. Our methodology outperforms LLM-generated labels in seven of eight tests, demonstrating an effective strategy for incorporating LLMs into the design and deployment of specialized, supervised learning models present in many industry use cases.","sentences":["Large language models (LLMs) have enhanced our ability to rapidly analyze and classify unstructured natural language data.","However, concerns regarding cost, network limitations, and security constraints have posed challenges for their integration into work processes.","In this study, we adopt a systems design approach to employing LLMs as imperfect data annotators for downstream supervised learning tasks, introducing novel system intervention measures aimed at improving classification performance.","Our methodology outperforms LLM-generated labels in seven of eight tests, demonstrating an effective strategy for incorporating LLMs into the design and deployment of specialized, supervised learning models present in many industry use cases."],"url":"http://arxiv.org/abs/2408.08217v1"}
{"created":"2024-08-15 15:23:37","title":"Moving Healthcare AI-Support Systems for Visually Detectable Diseases onto Constrained Devices","abstract":"Image classification usually requires connectivity and access to the cloud which is often limited in many parts of the world, including hard to reach rural areas. TinyML aims to solve this problem by hosting AI assistants on constrained devices, eliminating connectivity issues by processing data within the device itself, without internet or cloud access. This pilot study explores the use of tinyML to provide healthcare support with low spec devices in low connectivity environments, focusing on diagnosis of skin diseases and the ethical use of AI assistants in a healthcare setting. To investigate this, 10,000 images of skin lesions were used to train a model for classifying visually detectable diseases (VDDs). The model weights were then offloaded to a Raspberry Pi with a webcam attached, to be used for the classification of skin lesions without internet access. It was found that the developed prototype achieved a test accuracy of 78% and a test loss of 1.08.","sentences":["Image classification usually requires connectivity and access to the cloud which is often limited in many parts of the world, including hard to reach rural areas.","TinyML aims to solve this problem by hosting AI assistants on constrained devices, eliminating connectivity issues by processing data within the device itself, without internet or cloud access.","This pilot study explores the use of tinyML to provide healthcare support with low spec devices in low connectivity environments, focusing on diagnosis of skin diseases and the ethical use of AI assistants in a healthcare setting.","To investigate this, 10,000 images of skin lesions were used to train a model for classifying visually detectable diseases (VDDs).","The model weights were then offloaded to a Raspberry Pi with a webcam attached, to be used for the classification of skin lesions without internet access.","It was found that the developed prototype achieved a test accuracy of 78% and a test loss of 1.08."],"url":"http://arxiv.org/abs/2408.08215v1"}
{"created":"2024-08-15 15:23:32","title":"Federated Fairness Analytics: Quantifying Fairness in Federated Learning","abstract":"Federated Learning (FL) is a privacy-enhancing technology for distributed ML. By training models locally and aggregating updates - a federation learns together, while bypassing centralised data collection. FL is increasingly popular in healthcare, finance and personal computing. However, it inherits fairness challenges from classical ML and introduces new ones, resulting from differences in data quality, client participation, communication constraints, aggregation methods and underlying hardware. Fairness remains an unresolved issue in FL and the community has identified an absence of succinct definitions and metrics to quantify fairness; to address this, we propose Federated Fairness Analytics - a methodology for measuring fairness. Our definition of fairness comprises four notions with novel, corresponding metrics. They are symptomatically defined and leverage techniques originating from XAI, cooperative game-theory and networking engineering. We tested a range of experimental settings, varying the FL approach, ML task and data settings. The results show that statistical heterogeneity and client participation affect fairness and fairness conscious approaches such as Ditto and q-FedAvg marginally improve fairness-performance trade-offs. Using our techniques, FL practitioners can uncover previously unobtainable insights into their system's fairness, at differing levels of granularity in order to address fairness challenges in FL. We have open-sourced our work at: https://github.com/oscardilley/federated-fairness.","sentences":["Federated Learning (FL) is a privacy-enhancing technology for distributed ML.","By training models locally and aggregating updates - a federation learns together, while bypassing centralised data collection.","FL is increasingly popular in healthcare, finance and personal computing.","However, it inherits fairness challenges from classical ML and introduces new ones, resulting from differences in data quality, client participation, communication constraints, aggregation methods and underlying hardware.","Fairness remains an unresolved issue in FL and the community has identified an absence of succinct definitions and metrics to quantify fairness; to address this, we propose Federated Fairness Analytics - a methodology for measuring fairness.","Our definition of fairness comprises four notions with novel, corresponding metrics.","They are symptomatically defined and leverage techniques originating from XAI, cooperative game-theory and networking engineering.","We tested a range of experimental settings, varying the FL approach, ML task and data settings.","The results show that statistical heterogeneity and client participation affect fairness and fairness conscious approaches such as Ditto and q-FedAvg marginally improve fairness-performance trade-offs.","Using our techniques, FL practitioners can uncover previously unobtainable insights into their system's fairness, at differing levels of granularity in order to address fairness challenges in FL.","We have open-sourced our work at: https://github.com/oscardilley/federated-fairness."],"url":"http://arxiv.org/abs/2408.08214v1"}
{"created":"2024-08-15 15:16:49","title":"WaterSplatting: Fast Underwater 3D Scene Reconstruction Using Gaussian Splatting","abstract":"The underwater 3D scene reconstruction is a challenging, yet interesting problem with applications ranging from naval robots to VR experiences. The problem was successfully tackled by fully volumetric NeRF-based methods which can model both the geometry and the medium (water). Unfortunately, these methods are slow to train and do not offer real-time rendering. More recently, 3D Gaussian Splatting (3DGS) method offered a fast alternative to NeRFs. However, because it is an explicit method that renders only the geometry, it cannot render the medium and is therefore unsuited for underwater reconstruction. Therefore, we propose a novel approach that fuses volumetric rendering with 3DGS to handle underwater data effectively. Our method employs 3DGS for explicit geometry representation and a separate volumetric field (queried once per pixel) for capturing the scattering medium. This dual representation further allows the restoration of the scenes by removing the scattering medium. Our method outperforms state-of-the-art NeRF-based methods in rendering quality on the underwater SeaThru-NeRF dataset. Furthermore, it does so while offering real-time rendering performance, addressing the efficiency limitations of existing methods. Web: https://water-splatting.github.io","sentences":["The underwater 3D scene reconstruction is a challenging, yet interesting problem with applications ranging from naval robots to VR experiences.","The problem was successfully tackled by fully volumetric NeRF-based methods which can model both the geometry and the medium (water).","Unfortunately, these methods are slow to train and do not offer real-time rendering.","More recently, 3D Gaussian Splatting (3DGS) method offered a fast alternative to NeRFs.","However, because it is an explicit method that renders only the geometry, it cannot render the medium and is therefore unsuited for underwater reconstruction.","Therefore, we propose a novel approach that fuses volumetric rendering with 3DGS to handle underwater data effectively.","Our method employs 3DGS for explicit geometry representation and a separate volumetric field (queried once per pixel) for capturing the scattering medium.","This dual representation further allows the restoration of the scenes by removing the scattering medium.","Our method outperforms state-of-the-art NeRF-based methods in rendering quality on the underwater SeaThru-NeRF dataset.","Furthermore, it does so while offering real-time rendering performance, addressing the efficiency limitations of existing methods.","Web: https://water-splatting.github.io"],"url":"http://arxiv.org/abs/2408.08206v1"}
{"created":"2024-08-15 15:10:01","title":"Towards Practical Human Motion Prediction with LiDAR Point Clouds","abstract":"Human motion prediction is crucial for human-centric multimedia understanding and interacting. Current methods typically rely on ground truth human poses as observed input, which is not practical for real-world scenarios where only raw visual sensor data is available. To implement these methods in practice, a pre-phrase of pose estimation is essential. However, such two-stage approaches often lead to performance degradation due to the accumulation of errors. Moreover, reducing raw visual data to sparse keypoint representations significantly diminishes the density of information, resulting in the loss of fine-grained features. In this paper, we propose \\textit{LiDAR-HMP}, the first single-LiDAR-based 3D human motion prediction approach, which receives the raw LiDAR point cloud as input and forecasts future 3D human poses directly. Building upon our novel structure-aware body feature descriptor, LiDAR-HMP adaptively maps the observed motion manifold to future poses and effectively models the spatial-temporal correlations of human motions for further refinement of prediction results. Extensive experiments show that our method achieves state-of-the-art performance on two public benchmarks and demonstrates remarkable robustness and efficacy in real-world deployments.","sentences":["Human motion prediction is crucial for human-centric multimedia understanding and interacting.","Current methods typically rely on ground truth human poses as observed input, which is not practical for real-world scenarios where only raw visual sensor data is available.","To implement these methods in practice, a pre-phrase of pose estimation is essential.","However, such two-stage approaches often lead to performance degradation due to the accumulation of errors.","Moreover, reducing raw visual data to sparse keypoint representations significantly diminishes the density of information, resulting in the loss of fine-grained features.","In this paper, we propose \\textit{LiDAR-HMP}, the first single-LiDAR-based 3D human motion prediction approach, which receives the raw LiDAR point cloud as input and forecasts future 3D human poses directly.","Building upon our novel structure-aware body feature descriptor, LiDAR-HMP adaptively maps the observed motion manifold to future poses and effectively models the spatial-temporal correlations of human motions for further refinement of prediction results.","Extensive experiments show that our method achieves state-of-the-art performance on two public benchmarks and demonstrates remarkable robustness and efficacy in real-world deployments."],"url":"http://arxiv.org/abs/2408.08202v1"}
{"created":"2024-08-15 14:42:02","title":"Not Every Image is Worth a Thousand Words: Quantifying Originality in Stable Diffusion","abstract":"This work addresses the challenge of quantifying originality in text-to-image (T2I) generative diffusion models, with a focus on copyright originality. We begin by evaluating T2I models' ability to innovate and generalize through controlled experiments, revealing that stable diffusion models can effectively recreate unseen elements with sufficiently diverse training data. Then, our key insight is that concepts and combinations of image elements the model is familiar with, and saw more during training, are more concisly represented in the model's latent space. We hence propose a method that leverages textual inversion to measure the originality of an image based on the number of tokens required for its reconstruction by the model. Our approach is inspired by legal definitions of originality and aims to assess whether a model can produce original content without relying on specific prompts or having the training data of the model. We demonstrate our method using both a pre-trained stable diffusion model and a synthetic dataset, showing a correlation between the number of tokens and image originality. This work contributes to the understanding of originality in generative models and has implications for copyright infringement cases.","sentences":["This work addresses the challenge of quantifying originality in text-to-image (T2I) generative diffusion models, with a focus on copyright originality.","We begin by evaluating T2I models' ability to innovate and generalize through controlled experiments, revealing that stable diffusion models can effectively recreate unseen elements with sufficiently diverse training data.","Then, our key insight is that concepts and combinations of image elements the model is familiar with, and saw more during training, are more concisly represented in the model's latent space.","We hence propose a method that leverages textual inversion to measure the originality of an image based on the number of tokens required for its reconstruction by the model.","Our approach is inspired by legal definitions of originality and aims to assess whether a model can produce original content without relying on specific prompts or having the training data of the model.","We demonstrate our method using both a pre-trained stable diffusion model and a synthetic dataset, showing a correlation between the number of tokens and image originality.","This work contributes to the understanding of originality in generative models and has implications for copyright infringement cases."],"url":"http://arxiv.org/abs/2408.08184v1"}
{"created":"2024-08-15 14:36:07","title":"Your Turn: Real-World Turning Angle Estimation for Parkinson's Disease Severity Assessment","abstract":"People with Parkinson's Disease (PD) often experience progressively worsening gait, including changes in how they turn around, as the disease progresses. Existing clinical rating tools are not capable of capturing hour-by-hour variations of PD symptoms, as they are confined to brief assessments within clinic settings. Measuring real-world gait turning angles continuously and passively is a component step towards using gait characteristics as sensitive indicators of disease progression in PD. This paper presents a deep learning-based approach to automatically quantify turning angles by extracting 3D skeletons from videos and calculating the rotation of hip and knee joints. We utilise state-of-the-art human pose estimation models, Fastpose and Strided Transformer, on a total of 1386 turning video clips from 24 subjects (12 people with PD and 12 healthy control volunteers), trimmed from a PD dataset of unscripted free-living videos in a home-like setting (Turn-REMAP). We also curate a turning video dataset, Turn-H3.6M, from the public Human3.6M human pose benchmark with 3D ground truth, to further validate our method. Previous gait research has primarily taken place in clinics or laboratories evaluating scripted gait outcomes, but this work focuses on real-world settings where complexities exist, such as baggy clothing and poor lighting. Due to difficulties in obtaining accurate ground truth data in a free-living setting, we quantise the angle into the nearest bin $45^\\circ$ based on the manual labelling of expert clinicians. Our method achieves a turning calculation accuracy of 41.6%, a Mean Absolute Error (MAE) of 34.7{\\deg}, and a weighted precision WPrec of 68.3% for Turn-REMAP. This is the first work to explore the use of single monocular camera data to quantify turns by PD patients in a home setting.","sentences":["People with Parkinson's Disease (PD) often experience progressively worsening gait, including changes in how they turn around, as the disease progresses.","Existing clinical rating tools are not capable of capturing hour-by-hour variations of PD symptoms, as they are confined to brief assessments within clinic settings.","Measuring real-world gait turning angles continuously and passively is a component step towards using gait characteristics as sensitive indicators of disease progression in PD.","This paper presents a deep learning-based approach to automatically quantify turning angles by extracting 3D skeletons from videos and calculating the rotation of hip and knee joints.","We utilise state-of-the-art human pose estimation models, Fastpose and Strided Transformer, on a total of 1386 turning video clips from 24 subjects (12 people with PD and 12 healthy control volunteers), trimmed from a PD dataset of unscripted free-living videos in a home-like setting (Turn-REMAP).","We also curate a turning video dataset, Turn-H3.6M, from the public Human3.6M human pose benchmark with 3D ground truth, to further validate our method.","Previous gait research has primarily taken place in clinics or laboratories evaluating scripted gait outcomes, but this work focuses on real-world settings where complexities exist, such as baggy clothing and poor lighting.","Due to difficulties in obtaining accurate ground truth data in a free-living setting, we quantise the angle into the nearest bin $45^\\circ$ based on the manual labelling of expert clinicians.","Our method achieves a turning calculation accuracy of 41.6%, a Mean Absolute Error (MAE) of 34.7{\\deg}, and a weighted precision WPrec of 68.3% for Turn-REMAP.","This is the first work to explore the use of single monocular camera data to quantify turns by PD patients in a home setting."],"url":"http://arxiv.org/abs/2408.08182v1"}
{"created":"2024-08-15 14:19:13","title":"Towards flexible perception with visual memory","abstract":"Training a neural network is a monolithic endeavor, akin to carving knowledge into stone: once the process is completed, editing the knowledge in a network is nearly impossible, since all information is distributed across the network's weights. We here explore a simple, compelling alternative by marrying the representational power of deep neural networks with the flexibility of a database. Decomposing the task of image classification into image similarity (from a pre-trained embedding) and search (via fast nearest neighbor retrieval from a knowledge database), we build a simple and flexible visual memory that has the following key capabilities: (1.) The ability to flexibly add data across scales: from individual samples all the way to entire classes and billion-scale data; (2.) The ability to remove data through unlearning and memory pruning; (3.) An interpretable decision-mechanism on which we can intervene to control its behavior. Taken together, these capabilities comprehensively demonstrate the benefits of an explicit visual memory. We hope that it might contribute to a conversation on how knowledge should be represented in deep vision models -- beyond carving it in ``stone'' weights.","sentences":["Training a neural network is a monolithic endeavor, akin to carving knowledge into stone: once the process is completed, editing the knowledge in a network is nearly impossible, since all information is distributed across the network's weights.","We here explore a simple, compelling alternative by marrying the representational power of deep neural networks with the flexibility of a database.","Decomposing the task of image classification into image similarity (from a pre-trained embedding) and search (via fast nearest neighbor retrieval from a knowledge database), we build a simple and flexible visual memory that has the following key capabilities: (1.)","The ability to flexibly add data across scales: from individual samples all the way to entire classes and billion-scale data; (2.)","The ability to remove data through unlearning and memory pruning; (3.)","An interpretable decision-mechanism on which we can intervene to control its behavior.","Taken together, these capabilities comprehensively demonstrate the benefits of an explicit visual memory.","We hope that it might contribute to a conversation on how knowledge should be represented in deep vision models -- beyond carving it in ``stone'' weights."],"url":"http://arxiv.org/abs/2408.08172v1"}
{"created":"2024-08-15 13:48:44","title":"EmBARDiment: an Embodied AI Agent for Productivity in XR","abstract":"XR devices running chat-bots powered by Large Language Models (LLMs) have tremendous potential as always-on agents that can enable much better productivity scenarios. However, screen based chat-bots do not take advantage of the the full-suite of natural inputs available in XR, including inward facing sensor data, instead they over-rely on explicit voice or text prompts, sometimes paired with multi-modal data dropped as part of the query. We propose a solution that leverages an attention framework that derives context implicitly from user actions, eye-gaze, and contextual memory within the XR environment. This minimizes the need for engineered explicit prompts, fostering grounded and intuitive interactions that glean user insights for the chat-bot. Our user studies demonstrate the imminent feasibility and transformative potential of our approach to streamline user interaction in XR with chat-bots, while offering insights for the design of future XR-embodied LLM agents.","sentences":["XR devices running chat-bots powered by Large Language Models (LLMs) have tremendous potential as always-on agents that can enable much better productivity scenarios.","However, screen based chat-bots do not take advantage of the the full-suite of natural inputs available in XR, including inward facing sensor data, instead they over-rely on explicit voice or text prompts, sometimes paired with multi-modal data dropped as part of the query.","We propose a solution that leverages an attention framework that derives context implicitly from user actions, eye-gaze, and contextual memory within the XR environment.","This minimizes the need for engineered explicit prompts, fostering grounded and intuitive interactions that glean user insights for the chat-bot.","Our user studies demonstrate the imminent feasibility and transformative potential of our approach to streamline user interaction in XR with chat-bots, while offering insights for the design of future XR-embodied LLM agents."],"url":"http://arxiv.org/abs/2408.08158v1"}
{"created":"2024-08-15 13:35:59","title":"Unsupervised Variational Translator for Bridging Image Restoration and High-Level Vision Tasks","abstract":"Recent research tries to extend image restoration capabilities from human perception to machine perception, thereby enhancing the performance of high-level vision tasks in degraded environments. These methods, primarily based on supervised learning, typically involve the retraining of restoration networks or high-level vision networks. However, collecting paired data in real-world scenarios and retraining large-scale models are challenge. To this end, we propose an unsupervised learning method called \\textbf{Va}riational \\textbf{T}ranslator (VaT), which does not require retraining existing restoration and high-level vision networks. Instead, it establishes a lightweight network that serves as an intermediate bridge between them. By variational inference, VaT approximates the joint distribution of restoration output and high-level vision input, dividing the optimization objective into preserving content and maximizing marginal likelihood associated with high-level vision tasks. By cleverly leveraging self-training paradigms, VaT achieves the above optimization objective without requiring labels. As a result, the translated images maintain a close resemblance to their original content while also demonstrating exceptional performance on high-level vision tasks. Extensive experiments in dehazing and low-light enhancement for detection and classification show the superiority of our method over other state-of-the-art unsupervised counterparts, even significantly surpassing supervised methods in some complex real-world scenarios.","sentences":["Recent research tries to extend image restoration capabilities from human perception to machine perception, thereby enhancing the performance of high-level vision tasks in degraded environments.","These methods, primarily based on supervised learning, typically involve the retraining of restoration networks or high-level vision networks.","However, collecting paired data in real-world scenarios and retraining large-scale models are challenge.","To this end, we propose an unsupervised learning method called \\textbf{Va}riational \\textbf{T}ranslator (VaT), which does not require retraining existing restoration and high-level vision networks.","Instead, it establishes a lightweight network that serves as an intermediate bridge between them.","By variational inference, VaT approximates the joint distribution of restoration output and high-level vision input, dividing the optimization objective into preserving content and maximizing marginal likelihood associated with high-level vision tasks.","By cleverly leveraging self-training paradigms, VaT achieves the above optimization objective without requiring labels.","As a result, the translated images maintain a close resemblance to their original content while also demonstrating exceptional performance on high-level vision tasks.","Extensive experiments in dehazing and low-light enhancement for detection and classification show the superiority of our method over other state-of-the-art unsupervised counterparts, even significantly surpassing supervised methods in some complex real-world scenarios."],"url":"http://arxiv.org/abs/2408.08149v1"}
{"created":"2024-08-15 13:33:20","title":"Early Detection of Performance Regressions by Bridging Local Performance Data and Architectural Models","abstract":"During software development, developers often make numerous modifications to the software to address existing issues or implement new features. However, certain changes may inadvertently have a detrimental impact on the overall system performance. To ensure that the performance of new software releases does not degrade, existing practices rely on system-level performance testing, such as load testing, or component-level performance testing to detect performance regressions. However, performance testing for the entire system is often expensive and time-consuming, posing challenges to adapting to the rapid release cycles common in modern DevOps practices. System-level performance testing cannot be conducted until the system is fully built and deployed. On the other hand, component-level testing focuses on isolated components, neglecting overall system performance and the impact of system workloads.   In this paper, we propose a novel approach to early detection of performance regressions by bridging the local performance data generated by component-level testing and the system-level architectural models. Our approach uses local performance data to identify deviations at the component level, and then propagate these deviations to the architectural model. We then use the architectural model to predict regressions in the performance of the overall system. We evaluate our approach on two open-source benchmark systems and show that it can effectively detect end-to-end system performance regressions from local performance deviations with different intensities and under various system workloads. More importantly, our approach can detect regressions as early as in the development phase, in contrast to existing approaches that require the system to be fully built and deployed. Our approach is lightweight and can complement traditional system performance testing when testing resources are scarce.","sentences":["During software development, developers often make numerous modifications to the software to address existing issues or implement new features.","However, certain changes may inadvertently have a detrimental impact on the overall system performance.","To ensure that the performance of new software releases does not degrade, existing practices rely on system-level performance testing, such as load testing, or component-level performance testing to detect performance regressions.","However, performance testing for the entire system is often expensive and time-consuming, posing challenges to adapting to the rapid release cycles common in modern DevOps practices.","System-level performance testing cannot be conducted until the system is fully built and deployed.","On the other hand, component-level testing focuses on isolated components, neglecting overall system performance and the impact of system workloads.   ","In this paper, we propose a novel approach to early detection of performance regressions by bridging the local performance data generated by component-level testing and the system-level architectural models.","Our approach uses local performance data to identify deviations at the component level, and then propagate these deviations to the architectural model.","We then use the architectural model to predict regressions in the performance of the overall system.","We evaluate our approach on two open-source benchmark systems and show that it can effectively detect end-to-end system performance regressions from local performance deviations with different intensities and under various system workloads.","More importantly, our approach can detect regressions as early as in the development phase, in contrast to existing approaches that require the system to be fully built and deployed.","Our approach is lightweight and can complement traditional system performance testing when testing resources are scarce."],"url":"http://arxiv.org/abs/2408.08148v1"}
{"created":"2024-08-15 13:26:13","title":"Unlearnable Examples Detection via Iterative Filtering","abstract":"Deep neural networks are proven to be vulnerable to data poisoning attacks. Recently, a specific type of data poisoning attack known as availability attacks has led to the failure of data utilization for model learning by adding imperceptible perturbations to images. Consequently, it is quite beneficial and challenging to detect poisoned samples, also known as Unlearnable Examples (UEs), from a mixed dataset. In response, we propose an Iterative Filtering approach for UEs identification. This method leverages the distinction between the inherent semantic mapping rules and shortcuts, without the need for any additional information. We verify that when training a classifier on a mixed dataset containing both UEs and clean data, the model tends to quickly adapt to the UEs compared to the clean data. Due to the accuracy gaps between training with clean/poisoned samples, we employ a model to misclassify clean samples while correctly identifying the poisoned ones. The incorporation of additional classes and iterative refinement enhances the model's ability to differentiate between clean and poisoned samples. Extensive experiments demonstrate the superiority of our method over state-of-the-art detection approaches across various attacks, datasets, and poison ratios, significantly reducing the Half Total Error Rate (HTER) compared to existing methods.","sentences":["Deep neural networks are proven to be vulnerable to data poisoning attacks.","Recently, a specific type of data poisoning attack known as availability attacks has led to the failure of data utilization for model learning by adding imperceptible perturbations to images.","Consequently, it is quite beneficial and challenging to detect poisoned samples, also known as Unlearnable Examples (UEs), from a mixed dataset.","In response, we propose an Iterative Filtering approach for UEs identification.","This method leverages the distinction between the inherent semantic mapping rules and shortcuts, without the need for any additional information.","We verify that when training a classifier on a mixed dataset containing both UEs and clean data, the model tends to quickly adapt to the UEs compared to the clean data.","Due to the accuracy gaps between training with clean/poisoned samples, we employ a model to misclassify clean samples while correctly identifying the poisoned ones.","The incorporation of additional classes and iterative refinement enhances the model's ability to differentiate between clean and poisoned samples.","Extensive experiments demonstrate the superiority of our method over state-of-the-art detection approaches across various attacks, datasets, and poison ratios, significantly reducing the Half Total Error Rate (HTER) compared to existing methods."],"url":"http://arxiv.org/abs/2408.08143v1"}
{"created":"2024-08-15 13:23:59","title":"Impact of Comprehensive Data Preprocessing on Predictive Modelling of COVID-19 Mortality","abstract":"Accurate predictive models are crucial for analysing COVID-19 mortality trends. This study evaluates the impact of a custom data preprocessing pipeline on ten machine learning models predicting COVID-19 mortality using data from Our World in Data (OWID). Our pipeline differs from a standard preprocessing pipeline through four key steps. Firstly, it transforms weekly reported totals into daily updates, correcting reporting biases and providing more accurate estimates. Secondly, it uses localised outlier detection and processing to preserve data variance and enhance accuracy. Thirdly, it utilises computational dependencies among columns to ensure data consistency. Finally, it incorporates an iterative feature selection process to optimise the feature set and improve model performance. Results show a significant improvement with the custom pipeline: the MLP Regressor achieved a test RMSE of 66.556 and a test R-squared of 0.991, surpassing the DecisionTree Regressor from the standard pipeline, which had a test RMSE of 222.858 and a test R-squared of 0.817. These findings highlight the importance of tailored preprocessing techniques in enhancing predictive modelling accuracy for COVID-19 mortality. Although specific to this study, these methodologies offer valuable insights into diverse datasets and domains, improving predictive performance across various contexts.","sentences":["Accurate predictive models are crucial for analysing COVID-19 mortality trends.","This study evaluates the impact of a custom data preprocessing pipeline on ten machine learning models predicting COVID-19 mortality using data from Our World in Data (OWID).","Our pipeline differs from a standard preprocessing pipeline through four key steps.","Firstly, it transforms weekly reported totals into daily updates, correcting reporting biases and providing more accurate estimates.","Secondly, it uses localised outlier detection and processing to preserve data variance and enhance accuracy.","Thirdly, it utilises computational dependencies among columns to ensure data consistency.","Finally, it incorporates an iterative feature selection process to optimise the feature set and improve model performance.","Results show a significant improvement with the custom pipeline: the MLP Regressor achieved a test RMSE of 66.556 and a test R-squared of 0.991, surpassing the DecisionTree Regressor from the standard pipeline, which had a test RMSE of 222.858 and a test R-squared of 0.817.","These findings highlight the importance of tailored preprocessing techniques in enhancing predictive modelling accuracy for COVID-19 mortality.","Although specific to this study, these methodologies offer valuable insights into diverse datasets and domains, improving predictive performance across various contexts."],"url":"http://arxiv.org/abs/2408.08142v1"}
{"created":"2024-08-15 13:19:55","title":"Visual Integration of Static and Dynamic Software Analysis in Code Reviews via Software City Visualization","abstract":"Software visualization approaches for code reviews are often implemented as standalone applications, which use static code analysis. The goal is to visualize the structural changes introduced by a pull / merge request to facilitate the review process. In this way, for example, structural changes that hinder code evolution can be more easily identified, but understanding the changed program behavior is still mainly done by reading the code. For software visualization to be successful in code review, tools must be provided that go beyond an alternative representation of code changes and integrate well into the developers' daily workflow. In this paper, we report on the novel and in-progress design and implementation of a web-based approach capable of combining static and dynamic analysis data in software city visualizations. Our architectural tool design incorporates modern web technologies such as the integration into common Git hosting services. As a result, code reviewers can explore how the modified software evolves and execute its use cases, which is especially helpful for distributed software systems. In this context, developers can be directly linked from the Git hosting service's issue tracking system to the corresponding software city visualization. This approach eliminates the recurring action of manual data collection and setup. We implement our design by extending the web-based software visualization tool ExplorViz. We invite other researchers to extend our open source software and jointly research this approach. Video URL: https://youtu.be/DYxijdCEdrY","sentences":["Software visualization approaches for code reviews are often implemented as standalone applications, which use static code analysis.","The goal is to visualize the structural changes introduced by a pull / merge request to facilitate the review process.","In this way, for example, structural changes that hinder code evolution can be more easily identified, but understanding the changed program behavior is still mainly done by reading the code.","For software visualization to be successful in code review, tools must be provided that go beyond an alternative representation of code changes and integrate well into the developers' daily workflow.","In this paper, we report on the novel and in-progress design and implementation of a web-based approach capable of combining static and dynamic analysis data in software city visualizations.","Our architectural tool design incorporates modern web technologies such as the integration into common Git hosting services.","As a result, code reviewers can explore how the modified software evolves and execute its use cases, which is especially helpful for distributed software systems.","In this context, developers can be directly linked from the Git hosting service's issue tracking system to the corresponding software city visualization.","This approach eliminates the recurring action of manual data collection and setup.","We implement our design by extending the web-based software visualization tool ExplorViz.","We invite other researchers to extend our open source software and jointly research this approach.","Video URL: https://youtu.be/DYxijdCEdrY"],"url":"http://arxiv.org/abs/2408.08141v1"}
{"created":"2024-08-15 13:07:51","title":"EXPLAIN, AGREE, LEARN: Scaling Learning for Neural Probabilistic Logic","abstract":"Neural probabilistic logic systems follow the neuro-symbolic (NeSy) paradigm by combining the perceptive and learning capabilities of neural networks with the robustness of probabilistic logic. Learning corresponds to likelihood optimization of the neural networks. However, to obtain the likelihood exactly, expensive probabilistic logic inference is required. To scale learning to more complex systems, we therefore propose to instead optimize a sampling based objective. We prove that the objective has a bounded error with respect to the likelihood, which vanishes when increasing the sample count. Furthermore, the error vanishes faster by exploiting a new concept of sample diversity. We then develop the EXPLAIN, AGREE, LEARN (EXAL) method that uses this objective. EXPLAIN samples explanations for the data. AGREE reweighs each explanation in concordance with the neural component. LEARN uses the reweighed explanations as a signal for learning. In contrast to previous NeSy methods, EXAL can scale to larger problem sizes while retaining theoretical guarantees on the error. Experimentally, our theoretical claims are verified and EXAL outperforms recent NeSy methods when scaling up the MNIST addition and Warcraft pathfinding problems.","sentences":["Neural probabilistic logic systems follow the neuro-symbolic (NeSy) paradigm by combining the perceptive and learning capabilities of neural networks with the robustness of probabilistic logic.","Learning corresponds to likelihood optimization of the neural networks.","However, to obtain the likelihood exactly, expensive probabilistic logic inference is required.","To scale learning to more complex systems, we therefore propose to instead optimize a sampling based objective.","We prove that the objective has a bounded error with respect to the likelihood, which vanishes when increasing the sample count.","Furthermore, the error vanishes faster by exploiting a new concept of sample diversity.","We then develop the EXPLAIN, AGREE, LEARN (EXAL) method that uses this objective.","EXPLAIN samples explanations for the data.","AGREE reweighs each explanation in concordance with the neural component.","LEARN uses the reweighed explanations as a signal for learning.","In contrast to previous NeSy methods, EXAL can scale to larger problem sizes while retaining theoretical guarantees on the error.","Experimentally, our theoretical claims are verified and EXAL outperforms recent NeSy methods when scaling up the MNIST addition and Warcraft pathfinding problems."],"url":"http://arxiv.org/abs/2408.08133v1"}
{"created":"2024-08-15 12:52:06","title":"Decoding Memes: A Comparative Study of Machine Learning Models for Template Identification","abstract":"Image-with-text memes combine text with imagery to achieve comedy, but in today's world, they also play a pivotal role in online communication, influencing politics, marketing, and social norms. A \"meme template\" is a preexisting layout or format that is used to create memes. It typically includes specific visual elements, characters, or scenes with blank spaces or captions that can be customized, allowing users to easily create their versions of popular meme templates by adding personal or contextually relevant content. Despite extensive research on meme virality, the task of automatically identifying meme templates remains a challenge.   This paper presents a comprehensive comparison and evaluation of existing meme template identification methods, including both established approaches from the literature and novel techniques. We introduce a rigorous evaluation framework that not only assesses the ability of various methods to correctly identify meme templates but also tests their capacity to reject non-memes without false assignments. Our study involves extensive data collection from sites that provide meme annotations (Imgflip) and various social media platforms (Reddit, X, and Facebook) to ensure a diverse and representative dataset. We compare meme template identification methods, highlighting their strengths and limitations. These include supervised and unsupervised approaches, such as convolutional neural networks, distance-based classification, and density-based clustering. Our analysis helps researchers and practitioners choose suitable methods and points to future research directions in this evolving field.","sentences":["Image-with-text memes combine text with imagery to achieve comedy, but in today's world, they also play a pivotal role in online communication, influencing politics, marketing, and social norms.","A \"meme template\" is a preexisting layout or format that is used to create memes.","It typically includes specific visual elements, characters, or scenes with blank spaces or captions that can be customized, allowing users to easily create their versions of popular meme templates by adding personal or contextually relevant content.","Despite extensive research on meme virality, the task of automatically identifying meme templates remains a challenge.   ","This paper presents a comprehensive comparison and evaluation of existing meme template identification methods, including both established approaches from the literature and novel techniques.","We introduce a rigorous evaluation framework that not only assesses the ability of various methods to correctly identify meme templates but also tests their capacity to reject non-memes without false assignments.","Our study involves extensive data collection from sites that provide meme annotations (Imgflip) and various social media platforms (Reddit, X, and Facebook) to ensure a diverse and representative dataset.","We compare meme template identification methods, highlighting their strengths and limitations.","These include supervised and unsupervised approaches, such as convolutional neural networks, distance-based classification, and density-based clustering.","Our analysis helps researchers and practitioners choose suitable methods and points to future research directions in this evolving field."],"url":"http://arxiv.org/abs/2408.08126v1"}
{"created":"2024-08-15 12:51:57","title":"Category-Prompt Refined Feature Learning for Long-Tailed Multi-Label Image Classification","abstract":"Real-world data consistently exhibits a long-tailed distribution, often spanning multiple categories. This complexity underscores the challenge of content comprehension, particularly in scenarios requiring Long-Tailed Multi-Label image Classification (LTMLC). In such contexts, imbalanced data distribution and multi-object recognition pose significant hurdles. To address this issue, we propose a novel and effective approach for LTMLC, termed Category-Prompt Refined Feature Learning (CPRFL), utilizing semantic correlations between different categories and decoupling category-specific visual representations for each category. Specifically, CPRFL initializes category-prompts from the pretrained CLIP's embeddings and decouples category-specific visual representations through interaction with visual features, thereby facilitating the establishment of semantic correlations between the head and tail classes. To mitigate the visual-semantic domain bias, we design a progressive Dual-Path Back-Propagation mechanism to refine the prompts by progressively incorporating context-related visual information into prompts. Simultaneously, the refinement process facilitates the progressive purification of the category-specific visual representations under the guidance of the refined prompts. Furthermore, taking into account the negative-positive sample imbalance, we adopt the Asymmetric Loss as our optimization objective to suppress negative samples across all classes and potentially enhance the head-to-tail recognition performance. We validate the effectiveness of our method on two LTMLC benchmarks and extensive experiments demonstrate the superiority of our work over baselines.   The code is available at https://github.com/jiexuanyan/CPRFL.","sentences":["Real-world data consistently exhibits a long-tailed distribution, often spanning multiple categories.","This complexity underscores the challenge of content comprehension, particularly in scenarios requiring Long-Tailed Multi-Label image Classification (LTMLC).","In such contexts, imbalanced data distribution and multi-object recognition pose significant hurdles.","To address this issue, we propose a novel and effective approach for LTMLC, termed Category-Prompt Refined Feature Learning (CPRFL), utilizing semantic correlations between different categories and decoupling category-specific visual representations for each category.","Specifically, CPRFL initializes category-prompts from the pretrained CLIP's embeddings and decouples category-specific visual representations through interaction with visual features, thereby facilitating the establishment of semantic correlations between the head and tail classes.","To mitigate the visual-semantic domain bias, we design a progressive Dual-Path Back-Propagation mechanism to refine the prompts by progressively incorporating context-related visual information into prompts.","Simultaneously, the refinement process facilitates the progressive purification of the category-specific visual representations under the guidance of the refined prompts.","Furthermore, taking into account the negative-positive sample imbalance, we adopt the Asymmetric Loss as our optimization objective to suppress negative samples across all classes and potentially enhance the head-to-tail recognition performance.","We validate the effectiveness of our method on two LTMLC benchmarks and extensive experiments demonstrate the superiority of our work over baselines.   ","The code is available at https://github.com/jiexuanyan/CPRFL."],"url":"http://arxiv.org/abs/2408.08125v1"}
{"created":"2024-08-15 12:38:10","title":"The Unreasonable Effectiveness of Solving Inverse Problems with Neural Networks","abstract":"Finding model parameters from data is an essential task in science and engineering, from weather and climate forecasts to plasma control. Previous works have employed neural networks to greatly accelerate finding solutions to inverse problems. Of particular interest are end-to-end models which utilize differentiable simulations in order to backpropagate feedback from the simulated process to the network weights and enable roll-out of multiple time steps. So far, it has been assumed that, while model inference is faster than classical optimization, this comes at the cost of a decrease in solution accuracy. We show that this is generally not true. In fact, neural networks trained to learn solutions to inverse problems can find better solutions than classical optimizers even on their training set. To demonstrate this, we perform both a theoretical analysis as well an extensive empirical evaluation on challenging problems involving local minima, chaos, and zero-gradient regions. Our findings suggest an alternative use for neural networks: rather than generalizing to new data for fast inference, they can also be used to find better solutions on known data.","sentences":["Finding model parameters from data is an essential task in science and engineering, from weather and climate forecasts to plasma control.","Previous works have employed neural networks to greatly accelerate finding solutions to inverse problems.","Of particular interest are end-to-end models which utilize differentiable simulations in order to backpropagate feedback from the simulated process to the network weights and enable roll-out of multiple time steps.","So far, it has been assumed that, while model inference is faster than classical optimization, this comes at the cost of a decrease in solution accuracy.","We show that this is generally not true.","In fact, neural networks trained to learn solutions to inverse problems can find better solutions than classical optimizers even on their training set.","To demonstrate this, we perform both a theoretical analysis as well an extensive empirical evaluation on challenging problems involving local minima, chaos, and zero-gradient regions.","Our findings suggest an alternative use for neural networks: rather than generalizing to new data for fast inference, they can also be used to find better solutions on known data."],"url":"http://arxiv.org/abs/2408.08119v1"}
{"created":"2024-08-15 12:10:50","title":"Adaptation of uncertainty-penalized Bayesian information criterion for parametric partial differential equation discovery","abstract":"Data-driven discovery of partial differential equations (PDEs) has emerged as a promising approach for deriving governing physics when domain knowledge about observed data is limited. Despite recent progress, the identification of governing equations and their parametric dependencies using conventional information criteria remains challenging in noisy situations, as the criteria tend to select overly complex PDEs. In this paper, we introduce an extension of the uncertainty-penalized Bayesian information criterion (UBIC), which is adapted to solve parametric PDE discovery problems efficiently without requiring computationally expensive PDE simulations. This extended UBIC uses quantified PDE uncertainty over different temporal or spatial points to prevent overfitting in model selection. The UBIC is computed with data transformation based on power spectral densities to discover the governing parametric PDE that truly captures qualitative features in frequency space with a few significant terms and their parametric dependencies (i.e., the varying PDE coefficients), evaluated with confidence intervals. Numerical experiments on canonical PDEs demonstrate that our extended UBIC can identify the true number of terms and their varying coefficients accurately, even in the presence of noise. The code is available at \\url{https://github.com/Pongpisit-Thanasutives/parametric-discovery}.","sentences":["Data-driven discovery of partial differential equations (PDEs) has emerged as a promising approach for deriving governing physics when domain knowledge about observed data is limited.","Despite recent progress, the identification of governing equations and their parametric dependencies using conventional information criteria remains challenging in noisy situations, as the criteria tend to select overly complex PDEs.","In this paper, we introduce an extension of the uncertainty-penalized Bayesian information criterion (UBIC), which is adapted to solve parametric PDE discovery problems efficiently without requiring computationally expensive PDE simulations.","This extended UBIC uses quantified PDE uncertainty over different temporal or spatial points to prevent overfitting in model selection.","The UBIC is computed with data transformation based on power spectral densities to discover the governing parametric PDE that truly captures qualitative features in frequency space with a few significant terms and their parametric dependencies (i.e., the varying PDE coefficients), evaluated with confidence intervals.","Numerical experiments on canonical PDEs demonstrate that our extended UBIC can identify the true number of terms and their varying coefficients accurately, even in the presence of noise.","The code is available at \\url{https://github.com/Pongpisit-Thanasutives/parametric-discovery}."],"url":"http://arxiv.org/abs/2408.08106v1"}
{"created":"2024-08-15 11:53:56","title":"Exploring Uncertainty Visualization for Degenerate Tensors in 3D Symmetric Second-Order Tensor Field Ensembles","abstract":"Symmetric second-order tensors are fundamental in various scientific and engineering domains, as they can represent properties such as material stresses or diffusion processes in brain tissue. In recent years, several approaches have been introduced and improved to analyze these fields using topological features, such as degenerate tensor locations, i.e., the tensor has repeated eigenvalues, or normal surfaces. Traditionally, the identification of such features has been limited to single tensor fields. However, it has become common to create ensembles to account for uncertainties and variability in simulations and measurements. In this work, we explore novel methods for describing and visualizing degenerate tensor locations in 3D symmetric second-order tensor field ensembles. We base our considerations on the tensor mode and analyze its practicality in characterizing the uncertainty of degenerate tensor locations before proposing a variety of visualization strategies to effectively communicate degenerate tensor information. We demonstrate our techniques for synthetic and simulation data sets. The results indicate that the interplay of different descriptions for uncertainty can effectively convey information on degenerate tensor locations.","sentences":["Symmetric second-order tensors are fundamental in various scientific and engineering domains, as they can represent properties such as material stresses or diffusion processes in brain tissue.","In recent years, several approaches have been introduced and improved to analyze these fields using topological features, such as degenerate tensor locations, i.e., the tensor has repeated eigenvalues, or normal surfaces.","Traditionally, the identification of such features has been limited to single tensor fields.","However, it has become common to create ensembles to account for uncertainties and variability in simulations and measurements.","In this work, we explore novel methods for describing and visualizing degenerate tensor locations in 3D symmetric second-order tensor field ensembles.","We base our considerations on the tensor mode and analyze its practicality in characterizing the uncertainty of degenerate tensor locations before proposing a variety of visualization strategies to effectively communicate degenerate tensor information.","We demonstrate our techniques for synthetic and simulation data sets.","The results indicate that the interplay of different descriptions for uncertainty can effectively convey information on degenerate tensor locations."],"url":"http://arxiv.org/abs/2408.08099v1"}
{"created":"2024-08-15 11:43:03","title":"Topological Simplifcation of Jacobi Sets for Piecewise-Linear Bivariate 2D Scalar Fields with Adjustment of the Underlying Data","abstract":"Jacobi sets are an important tool to study the relationship between functions. Defined as the set of all points where the function's gradients are linearly dependent, Jacobi sets extend the notion of critical point to multifields. In practice, Jacobi sets for piecewise-linear approximations of smooth functions can become very complex and large due to noise and numerical errors. Existing methods that simplify Jacobi sets exist, but either do not address how the functions' values have to change in order to have simpler Jacobi sets or remain purely theoretical. In this paper, we present a method that modifies 2D bivariate scalar fields such that Jacobi set components that are due to noise are removed, while preserving the essential structures of the fields. The method uses the Jacobi set to decompose the domain, stores the and weighs the resulting regions in a neighborhood graph, which is then used to determine which regions to join by collapsing the image of the region's cells. We investigate the influence of different tie-breaks when building the neighborhood graphs and the treatment of collapsed cells. We apply our algorithm to a range of datasets, both analytical and real-world and compare its performance to simple data smoothing.","sentences":["Jacobi sets are an important tool to study the relationship between functions.","Defined as the set of all points where the function's gradients are linearly dependent, Jacobi sets extend the notion of critical point to multifields.","In practice, Jacobi sets for piecewise-linear approximations of smooth functions can become very complex and large due to noise and numerical errors.","Existing methods that simplify Jacobi sets exist, but either do not address how the functions' values have to change in order to have simpler Jacobi sets or remain purely theoretical.","In this paper, we present a method that modifies 2D bivariate scalar fields such that Jacobi set components that are due to noise are removed, while preserving the essential structures of the fields.","The method uses the Jacobi set to decompose the domain, stores the and weighs the resulting regions in a neighborhood graph, which is then used to determine which regions to join by collapsing the image of the region's cells.","We investigate the influence of different tie-breaks when building the neighborhood graphs and the treatment of collapsed cells.","We apply our algorithm to a range of datasets, both analytical and real-world and compare its performance to simple data smoothing."],"url":"http://arxiv.org/abs/2408.08097v1"}
{"created":"2024-08-15 11:39:58","title":"Evaluating Time-Dependent Methods and Seasonal Effects in Code Technical Debt Prediction","abstract":"Code Technical Debt prediction has become a popular research niche in recent software engineering literature. Technical Debt is an important metric in software projects as it measures professionals' effort to clean the code. Therefore, predicting its future behavior becomes a crucial task. However, no well-defined and consistent approach can completely capture the features that impact the evolution of Code Technical Debt. The goal of this study is to evaluate the impact of considering time-dependent techniques as well as seasonal effects in temporal data in the prediction performance within the context of Code Technical Debt. The study adopts existing, yet not extensively adopted, time-dependent prediction techniques and compares their prediction performance to commonly used Machine Learning models. Further, the study strengthens the evaluation of time-dependent methods by extending the analysis to capture the impact of seasonality in Code Technical Debt data. We trained 11 prediction models using the commit history of 31 open-source projects developed with Java. We predicted the future observations of the SQALE index to evaluate their predictive performance. Our study confirms the positive impact of considering time-dependent techniques. The adopted multivariate time series analysis model ARIMAX overcame the rest of the adopted models. Incorporating seasonal effects led to an enhancement in the predictive performance of the adopted time-dependent techniques. However, the impact of this effect was found to be relatively modest. The findings of this study corroborate our position in favor of implementing techniques that capture the existing time dependence within historical data of software metrics, specifically in the context of this study, namely, Code Technical Debt. This necessitates the utilization of techniques that can effectively address this evidence.","sentences":["Code Technical Debt prediction has become a popular research niche in recent software engineering literature.","Technical Debt is an important metric in software projects as it measures professionals' effort to clean the code.","Therefore, predicting its future behavior becomes a crucial task.","However, no well-defined and consistent approach can completely capture the features that impact the evolution of Code Technical Debt.","The goal of this study is to evaluate the impact of considering time-dependent techniques as well as seasonal effects in temporal data in the prediction performance within the context of Code Technical Debt.","The study adopts existing, yet not extensively adopted, time-dependent prediction techniques and compares their prediction performance to commonly used Machine Learning models.","Further, the study strengthens the evaluation of time-dependent methods by extending the analysis to capture the impact of seasonality in Code Technical Debt data.","We trained 11 prediction models using the commit history of 31 open-source projects developed with Java.","We predicted the future observations of the SQALE index to evaluate their predictive performance.","Our study confirms the positive impact of considering time-dependent techniques.","The adopted multivariate time series analysis model ARIMAX overcame the rest of the adopted models.","Incorporating seasonal effects led to an enhancement in the predictive performance of the adopted time-dependent techniques.","However, the impact of this effect was found to be relatively modest.","The findings of this study corroborate our position in favor of implementing techniques that capture the existing time dependence within historical data of software metrics, specifically in the context of this study, namely, Code Technical Debt.","This necessitates the utilization of techniques that can effectively address this evidence."],"url":"http://arxiv.org/abs/2408.08095v1"}
{"created":"2024-08-15 11:32:46","title":"KGV: Integrating Large Language Models with Knowledge Graphs for Cyber Threat Intelligence Credibility Assessment","abstract":"Cyber threat intelligence is a critical tool that many organizations and individuals use to protect themselves from sophisticated, organized, persistent, and weaponized cyber attacks. However, few studies have focused on the quality assessment of threat intelligence provided by intelligence platforms, and this work still requires manual analysis by cybersecurity experts. In this paper, we propose a knowledge graph-based verifier, a novel Cyber Threat Intelligence (CTI) quality assessment framework that combines knowledge graphs and Large Language Models (LLMs). Our approach introduces LLMs to automatically extract OSCTI key claims to be verified and utilizes a knowledge graph consisting of paragraphs for fact-checking. This method differs from the traditional way of constructing complex knowledge graphs with entities as nodes. By constructing knowledge graphs with paragraphs as nodes and semantic similarity as edges, it effectively enhances the semantic understanding ability of the model and simplifies labeling requirements. Additionally, to fill the gap in the research field, we created and made public the first dataset for threat intelligence assessment from heterogeneous sources. To the best of our knowledge, this work is the first to create a dataset on threat intelligence reliability verification, providing a reference for future research. Experimental results show that KGV (Knowledge Graph Verifier) significantly improves the performance of LLMs in intelligence quality assessment. Compared with traditional methods, we reduce a large amount of data annotation while the model still exhibits strong reasoning capabilities. Finally, our method can achieve XXX accuracy in network threat assessment.","sentences":["Cyber threat intelligence is a critical tool that many organizations and individuals use to protect themselves from sophisticated, organized, persistent, and weaponized cyber attacks.","However, few studies have focused on the quality assessment of threat intelligence provided by intelligence platforms, and this work still requires manual analysis by cybersecurity experts.","In this paper, we propose a knowledge graph-based verifier, a novel Cyber Threat Intelligence (CTI) quality assessment framework that combines knowledge graphs and Large Language Models (LLMs).","Our approach introduces LLMs to automatically extract OSCTI key claims to be verified and utilizes a knowledge graph consisting of paragraphs for fact-checking.","This method differs from the traditional way of constructing complex knowledge graphs with entities as nodes.","By constructing knowledge graphs with paragraphs as nodes and semantic similarity as edges, it effectively enhances the semantic understanding ability of the model and simplifies labeling requirements.","Additionally, to fill the gap in the research field, we created and made public the first dataset for threat intelligence assessment from heterogeneous sources.","To the best of our knowledge, this work is the first to create a dataset on threat intelligence reliability verification, providing a reference for future research.","Experimental results show that KGV (Knowledge Graph Verifier) significantly improves the performance of LLMs in intelligence quality assessment.","Compared with traditional methods, we reduce a large amount of data annotation while the model still exhibits strong reasoning capabilities.","Finally, our method can achieve XXX accuracy in network threat assessment."],"url":"http://arxiv.org/abs/2408.08088v1"}
{"created":"2024-08-15 11:01:35","title":"A Survey on Integrated Sensing, Communication, and Computation","abstract":"The forthcoming generation of wireless technology, 6G, promises a revolutionary leap beyond traditional data-centric services. It aims to usher in an era of ubiquitous intelligent services, where everything is interconnected and intelligent. This vision requires the seamless integration of three fundamental modules: Sensing for information acquisition, communication for information sharing, and computation for information processing and decision-making. These modules are intricately linked, especially in complex tasks such as edge learning and inference. However, the performance of these modules is interdependent, creating a resource competition for time, energy, and bandwidth. Existing techniques like integrated communication and computation (ICC), integrated sensing and computation (ISC), and integrated sensing and communication (ISAC) have made partial strides in addressing this challenge, but they fall short of meeting the extreme performance requirements. To overcome these limitations, it is essential to develop new techniques that comprehensively integrate sensing, communication, and computation. This integrated approach, known as Integrated Sensing, Communication, and Computation (ISCC), offers a systematic perspective for enhancing task performance. This paper begins with a comprehensive survey of historic and related techniques such as ICC, ISC, and ISAC, highlighting their strengths and limitations. It then explores the state-of-the-art signal designs for ISCC, along with network resource management strategies specifically tailored for ISCC. Furthermore, this paper discusses the exciting research opportunities that lie ahead for implementing ISCC in future advanced networks. By embracing ISCC, we can unlock the full potential of intelligent connectivity, paving the way for groundbreaking applications and services.","sentences":["The forthcoming generation of wireless technology, 6G, promises a revolutionary leap beyond traditional data-centric services.","It aims to usher in an era of ubiquitous intelligent services, where everything is interconnected and intelligent.","This vision requires the seamless integration of three fundamental modules: Sensing for information acquisition, communication for information sharing, and computation for information processing and decision-making.","These modules are intricately linked, especially in complex tasks such as edge learning and inference.","However, the performance of these modules is interdependent, creating a resource competition for time, energy, and bandwidth.","Existing techniques like integrated communication and computation (ICC), integrated sensing and computation (ISC), and integrated sensing and communication (ISAC) have made partial strides in addressing this challenge, but they fall short of meeting the extreme performance requirements.","To overcome these limitations, it is essential to develop new techniques that comprehensively integrate sensing, communication, and computation.","This integrated approach, known as Integrated Sensing, Communication, and Computation (ISCC), offers a systematic perspective for enhancing task performance.","This paper begins with a comprehensive survey of historic and related techniques such as ICC, ISC, and ISAC, highlighting their strengths and limitations.","It then explores the state-of-the-art signal designs for ISCC, along with network resource management strategies specifically tailored for ISCC.","Furthermore, this paper discusses the exciting research opportunities that lie ahead for implementing ISCC in future advanced networks.","By embracing ISCC, we can unlock the full potential of intelligent connectivity, paving the way for groundbreaking applications and services."],"url":"http://arxiv.org/abs/2408.08074v1"}
{"created":"2024-08-15 10:44:38","title":"I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm","abstract":"Large Language Models (LLMs) have achieved significant advancements, however, the common learning paradigm treats LLMs as passive information repositories, neglecting their potential for active learning and alignment. Some approaches train LLMs using their own generated synthetic data, exploring the possibility of active alignment. However, there is still a huge gap between these one-time alignment methods and the continuous automatic alignment of humans. In this paper, we introduce \\textbf{I-SHEEP}, an \\textbf{I}terative \\textbf{S}elf-En\\textbf{H}anc\\textbf{E}m\\textbf{E}nt \\textbf{P}aradigm.This human-like paradigm enables LLMs to \\textbf{continuously self-align from scratch with nothing}. Compared to the one-time alignment method Dromedary \\cite{sun2023principledriven}, which refers to the first iteration in this paper, I-SHEEP can significantly enhance capacities on both Qwen and Llama models. I-SHEEP achieves a maximum relative improvement of 78.2\\% in the Alpaca Eval, 24.0\\% in the MT Bench, and an absolute increase of 8.88\\% in the IFEval accuracy over subsequent iterations in Qwen-1.5 72B model. Additionally, I-SHEEP surpasses the base model in various standard benchmark generation tasks, achieving an average improvement of 24.77\\% in code generation tasks, 12.04\\% in TrivialQA, and 20.29\\% in SQuAD. We also provide new insights based on the experiment results. Our codes, datasets, and models are available at \\textbf{https://anonymous.4open.science/r/I-SHEEP}.","sentences":["Large Language Models (LLMs) have achieved significant advancements, however, the common learning paradigm treats LLMs as passive information repositories, neglecting their potential for active learning and alignment.","Some approaches train LLMs using their own generated synthetic data, exploring the possibility of active alignment.","However, there is still a huge gap between these one-time alignment methods and the continuous automatic alignment of humans.","In this paper, we introduce \\textbf{I-SHEEP}, an \\textbf{I}terative \\textbf{S}elf-En\\textbf{H}anc\\textbf{E}m\\textbf{E}nt \\textbf{P}aradigm.","This human-like paradigm enables LLMs to \\textbf{continuously self-align from scratch with nothing}.","Compared to the one-time alignment method Dromedary \\cite{sun2023principledriven}, which refers to the first iteration in this paper, I-SHEEP can significantly enhance capacities on both Qwen and Llama models.","I-SHEEP achieves a maximum relative improvement of 78.2\\% in the Alpaca Eval, 24.0\\% in the MT Bench, and an absolute increase of 8.88\\% in the IFEval accuracy over subsequent iterations in Qwen-1.5 72B model.","Additionally, I-SHEEP surpasses the base model in various standard benchmark generation tasks, achieving an average improvement of 24.77\\% in code generation tasks, 12.04\\% in TrivialQA, and 20.29\\% in SQuAD.","We also provide new insights based on the experiment results.","Our codes, datasets, and models are available at \\textbf{https://anonymous.4open.science/r/I-SHEEP}."],"url":"http://arxiv.org/abs/2408.08072v1"}
{"created":"2024-08-15 10:31:46","title":"The Paradox of Spreadsheet Self-Efficacy: Social Incentives for Informal Knowledge Sharing in End-User Programming","abstract":"Informal Knowledge Sharing (KS) is vital for end-user programmers to gain expertise. To better understand how personal (self-efficacy), social (reputational gains, trust between colleagues), and software-related (codification effort) variables influence spreadsheet KS intention, we conducted a multiple regressions analysis based on survey data from spreadsheet users (\\textit{n}=100) in administrative and finance roles. We found that high levels of spreadsheet self-efficacy and a perception that sharing would result in reputational gains predicted higher KS intention, but individuals who found knowledge codification effortful showed lower KS intention. We also observed that regardless of occupation, users tended to report a lower sense of self-efficacy in their general spreadsheet proficiency, despite also reporting high self-efficacy in spreadsheet use for job-related contexts. Our findings suggest that acknowledging and designing for these social and personal variables can help avoid situations where experienced individuals refrain unnecessarily from sharing, with implications for spreadsheet design.","sentences":["Informal Knowledge Sharing (KS) is vital for end-user programmers to gain expertise.","To better understand how personal (self-efficacy), social (reputational gains, trust between colleagues), and software-related (codification effort) variables influence spreadsheet KS intention, we conducted a multiple regressions analysis based on survey data from spreadsheet users (\\textit{n}=100) in administrative and finance roles.","We found that high levels of spreadsheet self-efficacy and a perception that sharing would result in reputational gains predicted higher KS intention, but individuals who found knowledge codification effortful showed lower KS intention.","We also observed that regardless of occupation, users tended to report a lower sense of self-efficacy in their general spreadsheet proficiency, despite also reporting high self-efficacy in spreadsheet use for job-related contexts.","Our findings suggest that acknowledging and designing for these social and personal variables can help avoid situations where experienced individuals refrain unnecessarily from sharing, with implications for spreadsheet design."],"url":"http://arxiv.org/abs/2408.08068v1"}
{"created":"2024-08-15 10:02:46","title":"Security Challenges of Complex Space Applications: An Empirical Study","abstract":"Software applications in the space and defense industries have their unique characteristics: They are complex in structure, mission-critical, and often targets of state-of-the-art cyber attacks sponsored by adversary nation states. These applications have typically a high number of stakeholders in their software component supply chain, data supply chain, and user base. The aforementioned factors make such software applications potentially vulnerable to bad actors, as the widely adopted DevOps tools and practices were not designed for high-complexity and high-risk environments.   In this study, I investigate the security challenges of the development and management of complex space applications, which differentiate the process from the commonly used practices. My findings are based on interviews with five domain experts from the industry and are further supported by a comprehensive review of relevant publications.   To illustrate the dynamics of the problem, I present and discuss an actual software supply chain structure used by Thales Alenia Space, which is one of the largest suppliers of the European Space Agency. Subsequently, I discuss the four most critical security challenges identified by the interviewed experts: Verification of software artifacts, verification of the deployed application, single point of security failure, and data tampering by trusted stakeholders. Furthermore, I present best practices which could be used to overcome each of the given challenges, and whether the interviewed experts think their organization has access to the right tools to address them. Finally, I propose future research of new DevSecOps strategies, practices, and tools which would enable better methods of software integrity verification in the space and defense industries.","sentences":["Software applications in the space and defense industries have their unique characteristics: They are complex in structure, mission-critical, and often targets of state-of-the-art cyber attacks sponsored by adversary nation states.","These applications have typically a high number of stakeholders in their software component supply chain, data supply chain, and user base.","The aforementioned factors make such software applications potentially vulnerable to bad actors, as the widely adopted DevOps tools and practices were not designed for high-complexity and high-risk environments.   ","In this study, I investigate the security challenges of the development and management of complex space applications, which differentiate the process from the commonly used practices.","My findings are based on interviews with five domain experts from the industry and are further supported by a comprehensive review of relevant publications.   ","To illustrate the dynamics of the problem, I present and discuss an actual software supply chain structure used by Thales Alenia Space, which is one of the largest suppliers of the European Space Agency.","Subsequently, I discuss the four most critical security challenges identified by the interviewed experts: Verification of software artifacts, verification of the deployed application, single point of security failure, and data tampering by trusted stakeholders.","Furthermore, I present best practices which could be used to overcome each of the given challenges, and whether the interviewed experts think their organization has access to the right tools to address them.","Finally, I propose future research of new DevSecOps strategies, practices, and tools which would enable better methods of software integrity verification in the space and defense industries."],"url":"http://arxiv.org/abs/2408.08061v1"}
{"created":"2024-08-15 09:55:51","title":"Navigating Data Scarcity using Foundation Models: A Benchmark of Few-Shot and Zero-Shot Learning Approaches in Medical Imaging","abstract":"Data scarcity is a major limiting factor for applying modern machine learning techniques to clinical tasks. Although sufficient data exists for some well-studied medical tasks, there remains a long tail of clinically relevant tasks with poor data availability. Recently, numerous foundation models have demonstrated high suitability for few-shot learning (FSL) and zero-shot learning (ZSL), potentially making them more accessible to practitioners. However, it remains unclear which foundation model performs best on FSL medical image analysis tasks and what the optimal methods are for learning from limited data. We conducted a comprehensive benchmark study of ZSL and FSL using 16 pretrained foundation models on 19 diverse medical imaging datasets. Our results indicate that BiomedCLIP, a model pretrained exclusively on medical data, performs best on average for very small training set sizes, while very large CLIP models pretrained on LAION-2B perform best with slightly more training samples. However, simply fine-tuning a ResNet-18 pretrained on ImageNet performs similarly with more than five training examples per class. Our findings also highlight the need for further research on foundation models specifically tailored for medical applications and the collection of more datasets to train these models.","sentences":["Data scarcity is a major limiting factor for applying modern machine learning techniques to clinical tasks.","Although sufficient data exists for some well-studied medical tasks, there remains a long tail of clinically relevant tasks with poor data availability.","Recently, numerous foundation models have demonstrated high suitability for few-shot learning (FSL) and zero-shot learning (ZSL), potentially making them more accessible to practitioners.","However, it remains unclear which foundation model performs best on FSL medical image analysis tasks and what the optimal methods are for learning from limited data.","We conducted a comprehensive benchmark study of ZSL and FSL using 16 pretrained foundation models on 19 diverse medical imaging datasets.","Our results indicate that BiomedCLIP, a model pretrained exclusively on medical data, performs best on average for very small training set sizes, while very large CLIP models pretrained on LAION-2B perform best with slightly more training samples.","However, simply fine-tuning a ResNet-18 pretrained on ImageNet performs similarly with more than five training examples per class.","Our findings also highlight the need for further research on foundation models specifically tailored for medical applications and the collection of more datasets to train these models."],"url":"http://arxiv.org/abs/2408.08058v1"}
{"created":"2024-08-15 09:50:11","title":"DATTA: Towards Diversity Adaptive Test-Time Adaptation in Dynamic Wild World","abstract":"Test-time adaptation (TTA) effectively addresses distribution shifts between training and testing data by adjusting models on test samples, which is crucial for improving model inference in real-world applications. However, traditional TTA methods typically follow a fixed pattern to address the dynamic data patterns (low-diversity or high-diversity patterns) often leading to performance degradation and consequently a decline in Quality of Experience (QoE). The primary issues we observed are:Different scenarios require different normalization methods (e.g., Instance Normalization is optimal in mixed domains but not in static domains). Model fine-tuning can potentially harm the model and waste time.Hence, it is crucial to design strategies for effectively measuring and managing distribution diversity to minimize its negative impact on model performance. Based on these observations, this paper proposes a new general method, named Diversity Adaptive Test-Time Adaptation (DATTA), aimed at improving QoE. DATTA dynamically selects the best batch normalization methods and fine-tuning strategies by leveraging the Diversity Score to differentiate between high and low diversity score batches. It features three key components: Diversity Discrimination (DD) to assess batch diversity, Diversity Adaptive Batch Normalization (DABN) to tailor normalization methods based on DD insights, and Diversity Adaptive Fine-Tuning (DAFT) to selectively fine-tune the model. Experimental results show that our method achieves up to a 21% increase in accuracy compared to state-of-the-art methodologies, indicating that our method maintains good model performance while demonstrating its robustness. Our code will be released soon.","sentences":["Test-time adaptation (TTA) effectively addresses distribution shifts between training and testing data by adjusting models on test samples, which is crucial for improving model inference in real-world applications.","However, traditional TTA methods typically follow a fixed pattern to address the dynamic data patterns (low-diversity or high-diversity patterns) often leading to performance degradation and consequently a decline in Quality of Experience (QoE).","The primary issues we observed are:Different scenarios require different normalization methods (e.g., Instance Normalization is optimal in mixed domains but not in static domains).","Model fine-tuning can potentially harm the model and waste time.","Hence, it is crucial to design strategies for effectively measuring and managing distribution diversity to minimize its negative impact on model performance.","Based on these observations, this paper proposes a new general method, named Diversity Adaptive Test-Time Adaptation (DATTA), aimed at improving QoE. DATTA dynamically selects the best batch normalization methods and fine-tuning strategies by leveraging the Diversity Score to differentiate between high and low diversity score batches.","It features three key components: Diversity Discrimination (DD) to assess batch diversity, Diversity Adaptive Batch Normalization (DABN) to tailor normalization methods based on DD insights, and Diversity Adaptive Fine-Tuning (DAFT) to selectively fine-tune the model.","Experimental results show that our method achieves up to a 21% increase in accuracy compared to state-of-the-art methodologies, indicating that our method maintains good model performance while demonstrating its robustness.","Our code will be released soon."],"url":"http://arxiv.org/abs/2408.08056v1"}
{"created":"2024-08-15 09:26:26","title":"An Efficient Continuous Control Perspective for Reinforcement-Learning-based Sequential Recommendation","abstract":"Sequential recommendation, where user preference is dynamically inferred from sequential historical behaviors, is a critical task in recommender systems (RSs). To further optimize long-term user engagement, offline reinforcement-learning-based RSs have become a mainstream technique as they provide an additional advantage in avoiding global explorations that may harm online users' experiences. However, previous studies mainly focus on discrete action and policy spaces, which might have difficulties in handling dramatically growing items efficiently.   To mitigate this issue, in this paper, we aim to design an algorithmic framework applicable to continuous policies. To facilitate the control in the low-dimensional but dense user preference space, we propose an \\underline{\\textbf{E}}fficient \\underline{\\textbf{Co}}ntinuous \\underline{\\textbf{C}}ontrol framework (ECoC). Based on a statistically tested assumption, we first propose the novel unified action representation abstracted from normalized user and item spaces. Then, we develop the corresponding policy evaluation and policy improvement procedures. During this process, strategic exploration and directional control in terms of unified actions are carefully designed and crucial to final recommendation decisions. Moreover, beneficial from unified actions, the conservatism regularization for policies and value functions are combined and perfectly compatible with the continuous framework. The resulting dual regularization ensures the successful offline training of RL-based recommendation policies. Finally, we conduct extensive experiments to validate the effectiveness of our framework. The results show that compared to the discrete baselines, our ECoC is trained far more efficiently. Meanwhile, the final policies outperform baselines in both capturing the offline data and gaining long-term rewards.","sentences":["Sequential recommendation, where user preference is dynamically inferred from sequential historical behaviors, is a critical task in recommender systems (RSs).","To further optimize long-term user engagement, offline reinforcement-learning-based RSs have become a mainstream technique as they provide an additional advantage in avoiding global explorations that may harm online users' experiences.","However, previous studies mainly focus on discrete action and policy spaces, which might have difficulties in handling dramatically growing items efficiently.   ","To mitigate this issue, in this paper, we aim to design an algorithmic framework applicable to continuous policies.","To facilitate the control in the low-dimensional but dense user preference space, we propose an \\underline{\\textbf{E}}fficient \\underline{\\textbf{Co}}ntinuous \\underline{\\textbf{C}}ontrol framework (ECoC).","Based on a statistically tested assumption, we first propose the novel unified action representation abstracted from normalized user and item spaces.","Then, we develop the corresponding policy evaluation and policy improvement procedures.","During this process, strategic exploration and directional control in terms of unified actions are carefully designed and crucial to final recommendation decisions.","Moreover, beneficial from unified actions, the conservatism regularization for policies and value functions are combined and perfectly compatible with the continuous framework.","The resulting dual regularization ensures the successful offline training of RL-based recommendation policies.","Finally, we conduct extensive experiments to validate the effectiveness of our framework.","The results show that compared to the discrete baselines, our ECoC is trained far more efficiently.","Meanwhile, the final policies outperform baselines in both capturing the offline data and gaining long-term rewards."],"url":"http://arxiv.org/abs/2408.08047v1"}
{"created":"2024-08-15 09:23:15","title":"Crystalline Material Discovery in the Era of Artificial Intelligence","abstract":"Crystalline materials, with their symmetrical and periodic structures, possess a diverse array of properties and have been widely used in various fields, e.g., sustainable development. To discover crystalline materials, traditional experimental and computational approaches are often time-consuming and expensive. In these years, thanks to the explosive amount of crystalline materials data, great interest has been given to data-driven materials discovery. Particularly, recent advancements have exploited the expressive representation ability of deep learning to model the highly complex atomic systems within crystalline materials, opening up new avenues for fast and accurate materials discovery. These works typically focus on four types of tasks, including physicochemical property prediction, crystalline material synthesis, aiding characterization, and force field development; these tasks are essential for scientific research and development in crystalline materials science. Despite the remarkable progress, there is still a lack of systematic research to summarize their correlations, distinctions, and limitations. To fill this gap, we systematically investigated the progress made in deep learning-based material discovery in recent years. We first introduce several data representations of the crystalline materials. Based on the representations, we summarize various fundamental deep learning models and their tailored usages in material discovery tasks. We also point out the remaining challenges and propose several future directions. The main goal of this review is to offer comprehensive and valuable insights and foster progress in the intersection of artificial intelligence and material science.","sentences":["Crystalline materials, with their symmetrical and periodic structures, possess a diverse array of properties and have been widely used in various fields, e.g., sustainable development.","To discover crystalline materials, traditional experimental and computational approaches are often time-consuming and expensive.","In these years, thanks to the explosive amount of crystalline materials data, great interest has been given to data-driven materials discovery.","Particularly, recent advancements have exploited the expressive representation ability of deep learning to model the highly complex atomic systems within crystalline materials, opening up new avenues for fast and accurate materials discovery.","These works typically focus on four types of tasks, including physicochemical property prediction, crystalline material synthesis, aiding characterization, and force field development; these tasks are essential for scientific research and development in crystalline materials science.","Despite the remarkable progress, there is still a lack of systematic research to summarize their correlations, distinctions, and limitations.","To fill this gap, we systematically investigated the progress made in deep learning-based material discovery in recent years.","We first introduce several data representations of the crystalline materials.","Based on the representations, we summarize various fundamental deep learning models and their tailored usages in material discovery tasks.","We also point out the remaining challenges and propose several future directions.","The main goal of this review is to offer comprehensive and valuable insights and foster progress in the intersection of artificial intelligence and material science."],"url":"http://arxiv.org/abs/2408.08044v1"}
{"created":"2024-08-15 08:43:28","title":"Causal Discovery from Time-Series Data with Short-Term Invariance-Based Convolutional Neural Networks","abstract":"Causal discovery from time-series data aims to capture both intra-slice (contemporaneous) and inter-slice (time-lagged) causality between variables within the temporal chain, which is crucial for various scientific disciplines. Compared to causal discovery from non-time-series data, causal discovery from time-series data necessitates more serialized samples with a larger amount of observed time steps. To address the challenges, we propose a novel gradient-based causal discovery approach STIC, which focuses on \\textbf{S}hort-\\textbf{T}erm \\textbf{I}nvariance using \\textbf{C}onvolutional neural networks to uncover the causal relationships from time-series data. Specifically, STIC leverages both the short-term time and mechanism invariance of causality within each window observation, which possesses the property of independence, to enhance sample efficiency. Furthermore, we construct two causal convolution kernels, which correspond to the short-term time and mechanism invariance respectively, to estimate the window causal graph. To demonstrate the necessity of convolutional neural networks for causal discovery from time-series data, we theoretically derive the equivalence between convolution and the underlying generative principle of time-series data under the assumption that the additive noise model is identifiable. Experimental evaluations conducted on both synthetic and FMRI benchmark datasets demonstrate that our STIC outperforms baselines significantly and achieves the state-of-the-art performance, particularly when the datasets contain a limited number of observed time steps. Code is available at \\url{https://github.com/HITshenrj/STIC}.","sentences":["Causal discovery from time-series data aims to capture both intra-slice (contemporaneous) and inter-slice (time-lagged) causality between variables within the temporal chain, which is crucial for various scientific disciplines.","Compared to causal discovery from non-time-series data, causal discovery from time-series data necessitates more serialized samples with a larger amount of observed time steps.","To address the challenges, we propose a novel gradient-based causal discovery approach STIC, which focuses on \\textbf{S}hort-\\textbf{T}erm \\textbf{I}nvariance using \\textbf{C}onvolutional neural networks to uncover the causal relationships from time-series data.","Specifically, STIC leverages both the short-term time and mechanism invariance of causality within each window observation, which possesses the property of independence, to enhance sample efficiency.","Furthermore, we construct two causal convolution kernels, which correspond to the short-term time and mechanism invariance respectively, to estimate the window causal graph.","To demonstrate the necessity of convolutional neural networks for causal discovery from time-series data, we theoretically derive the equivalence between convolution and the underlying generative principle of time-series data under the assumption that the additive noise model is identifiable.","Experimental evaluations conducted on both synthetic and FMRI benchmark datasets demonstrate that our STIC outperforms baselines significantly and achieves the state-of-the-art performance, particularly when the datasets contain a limited number of observed time steps.","Code is available at \\url{https://github.com/HITshenrj/STIC}."],"url":"http://arxiv.org/abs/2408.08023v1"}
{"created":"2024-08-15 08:15:06","title":"Inversion-DeepONet: A Novel DeepONet-Based Network with Encoder-Decoder for Full Waveform Inversion","abstract":"Full waveform inversion (FWI) plays a crucial role in the field of geophysics. There has been lots of research about applying deep learning (DL) methods to FWI. The success of DL-FWI relies significantly on the quantity and diversity of the datasets. Nevertheless, existing FWI datasets, like OpenFWI, where sources have fixed locations or identical frequencies, provide limited information and do not represent the complex real-world scene. For instance, low frequencies help in resolving larger-scale structures. High frequencies allow for a more detailed subsurface features. %A single source frequency is insufficient to describe subsurface structural properties. We consider that simultaneously using sources with different frequencies, instead of performing inversion using low frequencies data and then gradually introducing higher frequencies data, has rationale and potential advantages. Hence, we develop three enhanced datasets based on OpenFWI where each source have varying locations, frequencies or both. Moreover, we propose a novel deep operator network (DeepONet) architecture Inversion-DeepONet for FWI. We utilize convolutional neural network (CNN) to extract the features from seismic data in branch net. Source parameters, such as locations and frequencies, are fed to trunk net. Then another CNN is employed as the decoder of DeepONet to reconstruct the velocity models more effectively. Through experiments, we confirm the superior performance on accuracy and generalization ability of our network, compared with existing data-driven FWI methods.","sentences":["Full waveform inversion (FWI) plays a crucial role in the field of geophysics.","There has been lots of research about applying deep learning (DL) methods to FWI.","The success of DL-FWI relies significantly on the quantity and diversity of the datasets.","Nevertheless, existing FWI datasets, like OpenFWI, where sources have fixed locations or identical frequencies, provide limited information and do not represent the complex real-world scene.","For instance, low frequencies help in resolving larger-scale structures.","High frequencies allow for a more detailed subsurface features.","%A single source frequency is insufficient to describe subsurface structural properties.","We consider that simultaneously using sources with different frequencies, instead of performing inversion using low frequencies data and then gradually introducing higher frequencies data, has rationale and potential advantages.","Hence, we develop three enhanced datasets based on OpenFWI where each source have varying locations, frequencies or both.","Moreover, we propose a novel deep operator network (DeepONet) architecture Inversion-DeepONet for FWI.","We utilize convolutional neural network (CNN) to extract the features from seismic data in branch net.","Source parameters, such as locations and frequencies, are fed to trunk net.","Then another CNN is employed as the decoder of DeepONet to reconstruct the velocity models more effectively.","Through experiments, we confirm the superior performance on accuracy and generalization ability of our network, compared with existing data-driven FWI methods."],"url":"http://arxiv.org/abs/2408.08005v1"}
{"created":"2024-08-15 08:12:52","title":"Leveraging Web-Crawled Data for High-Quality Fine-Tuning","abstract":"Most large language models are fine-tuned using either expensive human-annotated data or GPT-4 generated data which cannot guarantee performance in certain domains. We argue that although the web-crawled data often has formatting errors causing semantic inaccuracies, it can still serve as a valuable source for high-quality supervised fine-tuning in specific domains without relying on advanced models like GPT-4. To this end, we create a paired training dataset automatically by aligning web-crawled data with a smaller set of high-quality data. By training a language model on this dataset, we can convert web data with irregular formats into high-quality ones. Our experiments show that training with the model-transformed data yields better results, surpassing training with only high-quality data by an average score of 9.4% in Chinese math problems. Additionally, our 7B model outperforms several open-source models larger than 32B and surpasses well-known closed-source models such as GPT-3.5, highlighting the efficacy of our approach.","sentences":["Most large language models are fine-tuned using either expensive human-annotated data or GPT-4 generated data which cannot guarantee performance in certain domains.","We argue that although the web-crawled data often has formatting errors causing semantic inaccuracies, it can still serve as a valuable source for high-quality supervised fine-tuning in specific domains without relying on advanced models like GPT-4.","To this end, we create a paired training dataset automatically by aligning web-crawled data with a smaller set of high-quality data.","By training a language model on this dataset, we can convert web data with irregular formats into high-quality ones.","Our experiments show that training with the model-transformed data yields better results, surpassing training with only high-quality data by an average score of 9.4% in Chinese math problems.","Additionally, our 7B model outperforms several open-source models larger than 32B and surpasses well-known closed-source models such as GPT-3.5, highlighting the efficacy of our approach."],"url":"http://arxiv.org/abs/2408.08003v1"}
{"created":"2024-08-15 08:12:07","title":"Practical Privacy-Preserving Identity Verification using Third-Party Cloud Services and FHE (Role of Data Encoding in Circuit Depth Management)","abstract":"National digital identity verification systems have played a critical role in the effective distribution of goods and services, particularly, in developing countries. Due to the cost involved in deploying and maintaining such systems, combined with a lack of in-house technical expertise, governments seek to outsource this service to third-party cloud service providers to the extent possible. This leads to increased concerns regarding the privacy of users' personal data. In this work, we propose a practical privacy-preserving digital identity (ID) verification protocol where the third-party cloud services process the identity data encrypted using a (single-key) Fully Homomorphic Encryption (FHE) scheme such as BFV. Though the role of a trusted entity such as government is not completely eliminated, our protocol does significantly reduces the computation load on such parties.   A challenge in implementing a privacy-preserving ID verification protocol using FHE is to support various types of queries such as exact and/or fuzzy demographic and biometric matches including secure age comparisons. From a cryptographic engineering perspective, our main technical contribution is a user data encoding scheme that encodes demographic and biometric user data in only two BFV ciphertexts and yet facilitates us to outsource various types of ID verification queries to a third-party cloud. Our encoding scheme also ensures that the only computation done by the trusted entity is a query-agnostic \"extended\" decryption. This is in stark contrast with recent works that outsource all the non-arithmetic operations to a trusted server. We implement our protocol using the Microsoft SEAL FHE library and demonstrate its practicality.","sentences":["National digital identity verification systems have played a critical role in the effective distribution of goods and services, particularly, in developing countries.","Due to the cost involved in deploying and maintaining such systems, combined with a lack of in-house technical expertise, governments seek to outsource this service to third-party cloud service providers to the extent possible.","This leads to increased concerns regarding the privacy of users' personal data.","In this work, we propose a practical privacy-preserving digital identity (ID) verification protocol where the third-party cloud services process the identity data encrypted using a (single-key) Fully Homomorphic Encryption (FHE) scheme such as BFV.","Though the role of a trusted entity such as government is not completely eliminated, our protocol does significantly reduces the computation load on such parties.   ","A challenge in implementing a privacy-preserving ID verification protocol using FHE is to support various types of queries such as exact and/or fuzzy demographic and biometric matches including secure age comparisons.","From a cryptographic engineering perspective, our main technical contribution is a user data encoding scheme that encodes demographic and biometric user data in only two BFV ciphertexts and yet facilitates us to outsource various types of ID verification queries to a third-party cloud.","Our encoding scheme also ensures that the only computation done by the trusted entity is a query-agnostic \"extended\" decryption.","This is in stark contrast with recent works that outsource all the non-arithmetic operations to a trusted server.","We implement our protocol using the Microsoft SEAL FHE library and demonstrate its practicality."],"url":"http://arxiv.org/abs/2408.08002v1"}
{"created":"2024-08-15 07:37:24","title":"FuseChat: Knowledge Fusion of Chat Models","abstract":"While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, it incurs substantial costs and may lead to redundancy in competencies. Knowledge fusion aims to integrate existing LLMs of diverse architectures and capabilities into a more potent LLM through lightweight continual training, thereby reducing the need for costly LLM development. In this work, we propose a new framework for the knowledge fusion of chat LLMs through two main stages, resulting in FuseChat. Firstly, we conduct pairwise knowledge fusion on source chat LLMs of varying structures and scales to create multiple target LLMs with identical structure and size via lightweight fine-tuning. During this process, a statistics-based token alignment approach is introduced as the cornerstone for fusing LLMs with different structures. Secondly, we merge these target LLMs within the parameter space, where we propose a novel method for determining the merging coefficients based on the magnitude of parameter updates before and after fine-tuning. We implement and validate FuseChat using six prominent chat LLMs with diverse architectures and scales, including OpenChat-3.5-7B, Starling-LM-7B-alpha, NH2-SOLAR-10.7B, InternLM2-Chat-20B, Mixtral-8x7B-Instruct, and Qwen-1.5-Chat-72B. Experimental results on two instruction-following benchmarks, AlpacaEval 2.0 and MT-Bench, demonstrate the superiority of FuseChat-7B over baselines of various sizes. Our model is even comparable to the larger Mixtral-8x7B-Instruct and approaches GPT-3.5-Turbo-1106 on MT-Bench. Our code, model weights, and data are public at \\url{https://github.com/fanqiwan/FuseAI}.","sentences":["While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, it incurs substantial costs and may lead to redundancy in competencies.","Knowledge fusion aims to integrate existing LLMs of diverse architectures and capabilities into a more potent LLM through lightweight continual training, thereby reducing the need for costly LLM development.","In this work, we propose a new framework for the knowledge fusion of chat LLMs through two main stages, resulting in FuseChat.","Firstly, we conduct pairwise knowledge fusion on source chat LLMs of varying structures and scales to create multiple target LLMs with identical structure and size via lightweight fine-tuning.","During this process, a statistics-based token alignment approach is introduced as the cornerstone for fusing LLMs with different structures.","Secondly, we merge these target LLMs within the parameter space, where we propose a novel method for determining the merging coefficients based on the magnitude of parameter updates before and after fine-tuning.","We implement and validate FuseChat using six prominent chat LLMs with diverse architectures and scales, including OpenChat-3.5-7B, Starling-LM-7B-alpha, NH2-SOLAR-10.7B, InternLM2-Chat-20B, Mixtral-8x7B-Instruct, and Qwen-1.5-Chat-72B. Experimental results on two instruction-following benchmarks, AlpacaEval 2.0 and MT-Bench, demonstrate the superiority of FuseChat-7B over baselines of various sizes.","Our model is even comparable to the larger Mixtral-8x7B-Instruct and approaches GPT-3.5-Turbo-1106 on MT-Bench.","Our code, model weights, and data are public at \\url{https://github.com/fanqiwan/FuseAI}."],"url":"http://arxiv.org/abs/2408.07990v1"}
{"created":"2024-08-15 07:30:47","title":"IIU: Independent Inference Units for Knowledge-based Visual Question Answering","abstract":"Knowledge-based visual question answering requires external knowledge beyond visible content to answer the question correctly. One limitation of existing methods is that they focus more on modeling the inter-modal and intra-modal correlations, which entangles complex multimodal clues by implicit embeddings and lacks interpretability and generalization ability. The key challenge to solve the above problem is to separate the information and process it separately at the functional level. By reusing each processing unit, the generalization ability of the model to deal with different data can be increased. In this paper, we propose Independent Inference Units (IIU) for fine-grained multi-modal reasoning to decompose intra-modal information by the functionally independent units. Specifically, IIU processes each semantic-specific intra-modal clue by an independent inference unit, which also collects complementary information by communication from different units. To further reduce the impact of redundant information, we propose a memory update module to maintain semantic-relevant memory along with the reasoning process gradually. In comparison with existing non-pretrained multi-modal reasoning models on standard datasets, our model achieves a new state-of-the-art, enhancing performance by 3%, and surpassing basic pretrained multi-modal models. The experimental results show that our IIU model is effective in disentangling intra-modal clues as well as reasoning units to provide explainable reasoning evidence. Our code is available at https://github.com/Lilidamowang/IIU.","sentences":["Knowledge-based visual question answering requires external knowledge beyond visible content to answer the question correctly.","One limitation of existing methods is that they focus more on modeling the inter-modal and intra-modal correlations, which entangles complex multimodal clues by implicit embeddings and lacks interpretability and generalization ability.","The key challenge to solve the above problem is to separate the information and process it separately at the functional level.","By reusing each processing unit, the generalization ability of the model to deal with different data can be increased.","In this paper, we propose Independent Inference Units (IIU) for fine-grained multi-modal reasoning to decompose intra-modal information by the functionally independent units.","Specifically, IIU processes each semantic-specific intra-modal clue by an independent inference unit, which also collects complementary information by communication from different units.","To further reduce the impact of redundant information, we propose a memory update module to maintain semantic-relevant memory along with the reasoning process gradually.","In comparison with existing non-pretrained multi-modal reasoning models on standard datasets, our model achieves a new state-of-the-art, enhancing performance by 3%, and surpassing basic pretrained multi-modal models.","The experimental results show that our IIU model is effective in disentangling intra-modal clues as well as reasoning units to provide explainable reasoning evidence.","Our code is available at https://github.com/Lilidamowang/IIU."],"url":"http://arxiv.org/abs/2408.07989v1"}
{"created":"2024-08-15 07:30:21","title":"Exploring learning environments for label\\-efficient cancer diagnosis","abstract":"Despite significant research efforts and advancements, cancer remains a leading cause of mortality. Early cancer prediction has become a crucial focus in cancer research to streamline patient care and improve treatment outcomes. Manual tumor detection by histopathologists can be time consuming, prompting the need for computerized methods to expedite treatment planning. Traditional approaches to tumor detection rely on supervised learning, necessitates a large amount of annotated data for model training. However, acquiring such extensive labeled data can be laborious and time\\-intensive. This research examines the three learning environments: supervised learning (SL), semi\\-supervised learning (Semi\\-SL), and self\\-supervised learning (Self\\-SL): to predict kidney, lung, and breast cancer. Three pre\\-trained deep learning models (Residual Network\\-50, Visual Geometry Group\\-16, and EfficientNetB0) are evaluated based on these learning settings using seven carefully curated training sets. To create the first training set (TS1), SL is applied to all annotated image samples. Five training sets (TS2\\-TS6) with different ratios of labeled and unlabeled cancer images are used to evaluateSemi\\-SL. Unlabeled cancer images from the final training set (TS7) are utilized for Self\\-SL assessment. Among different learning environments, outcomes from the Semi\\-SL setting show a strong degree of agreement with the outcomes achieved in the SL setting. The uniform pattern of observations from the pre\\-trained models across all three datasets validates the methodology and techniques of the research. Based on modest number of labeled samples and minimal computing cost, our study suggests that the Semi\\-SL option can be a highly viable replacement for the SL option under label annotation constraint scenarios.","sentences":["Despite significant research efforts and advancements, cancer remains a leading cause of mortality.","Early cancer prediction has become a crucial focus in cancer research to streamline patient care and improve treatment outcomes.","Manual tumor detection by histopathologists can be time consuming, prompting the need for computerized methods to expedite treatment planning.","Traditional approaches to tumor detection rely on supervised learning, necessitates a large amount of annotated data for model training.","However, acquiring such extensive labeled data can be laborious and time\\-intensive.","This research examines the three learning environments: supervised learning (SL), semi\\-supervised learning (Semi\\-SL), and self\\-supervised learning (Self\\-SL): to predict kidney, lung, and breast cancer.","Three pre\\-trained deep learning models (Residual Network\\-50, Visual Geometry Group\\-16, and EfficientNetB0) are evaluated based on these learning settings using seven carefully curated training sets.","To create the first training set (TS1), SL is applied to all annotated image samples.","Five training sets (TS2\\-TS6) with different ratios of labeled and unlabeled cancer images are used to evaluateSemi\\-SL.","Unlabeled cancer images from the final training set (TS7) are utilized for Self\\-SL assessment.","Among different learning environments, outcomes from the Semi\\-SL setting show a strong degree of agreement with the outcomes achieved in the SL setting.","The uniform pattern of observations from the pre\\-trained models across all three datasets validates the methodology and techniques of the research.","Based on modest number of labeled samples and minimal computing cost, our study suggests that the Semi\\-SL option can be a highly viable replacement for the SL option under label annotation constraint scenarios."],"url":"http://arxiv.org/abs/2408.07988v1"}
{"created":"2024-08-15 07:25:52","title":"Experimental evaluation of offline reinforcement learning for HVAC control in buildings","abstract":"Reinforcement learning (RL) techniques have been increasingly investigated for dynamic HVAC control in buildings. However, most studies focus on exploring solutions in online or off-policy scenarios without discussing in detail the implementation feasibility or effectiveness of dealing with purely offline datasets or trajectories. The lack of these works limits the real-world deployment of RL-based HVAC controllers, especially considering the abundance of historical data. To this end, this paper comprehensively evaluates the strengths and limitations of state-of-the-art offline RL algorithms by conducting analytical and numerical studies. The analysis is conducted from two perspectives: algorithms and dataset characteristics. As a prerequisite, the necessity of applying offline RL algorithms is first confirmed in two building environments. The ability of observation history modeling to reduce violations and enhance performance is subsequently studied. Next, the performance of RL-based controllers under datasets with different qualitative and quantitative conditions is investigated, including constraint satisfaction and power consumption. Finally, the sensitivity of certain hyperparameters is also evaluated. The results indicate that datasets of a certain suboptimality level and relatively small scale can be utilized to effectively train a well-performed RL-based HVAC controller. Specifically, such controllers can reduce at most 28.5% violation ratios of indoor temperatures and achieve at most 12.1% power savings compared to the baseline controller. In summary, this paper presents our well-structured investigations and new findings when applying offline reinforcement learning to building HVAC systems.","sentences":["Reinforcement learning (RL) techniques have been increasingly investigated for dynamic HVAC control in buildings.","However, most studies focus on exploring solutions in online or off-policy scenarios without discussing in detail the implementation feasibility or effectiveness of dealing with purely offline datasets or trajectories.","The lack of these works limits the real-world deployment of RL-based HVAC controllers, especially considering the abundance of historical data.","To this end, this paper comprehensively evaluates the strengths and limitations of state-of-the-art offline RL algorithms by conducting analytical and numerical studies.","The analysis is conducted from two perspectives: algorithms and dataset characteristics.","As a prerequisite, the necessity of applying offline RL algorithms is first confirmed in two building environments.","The ability of observation history modeling to reduce violations and enhance performance is subsequently studied.","Next, the performance of RL-based controllers under datasets with different qualitative and quantitative conditions is investigated, including constraint satisfaction and power consumption.","Finally, the sensitivity of certain hyperparameters is also evaluated.","The results indicate that datasets of a certain suboptimality level and relatively small scale can be utilized to effectively train a well-performed RL-based HVAC controller.","Specifically, such controllers can reduce at most 28.5% violation ratios of indoor temperatures and achieve at most 12.1% power savings compared to the baseline controller.","In summary, this paper presents our well-structured investigations and new findings when applying offline reinforcement learning to building HVAC systems."],"url":"http://arxiv.org/abs/2408.07986v1"}
{"created":"2024-08-15 07:00:20","title":"LLaVA-Surg: Towards Multimodal Surgical Assistant via Structured Surgical Video Learning","abstract":"Multimodal large language models (LLMs) have achieved notable success across various domains, while research in the medical field has largely focused on unimodal images. Meanwhile, current general-domain multimodal models for videos still lack the capabilities to understand and engage in conversations about surgical videos. One major contributing factor is the absence of datasets in the surgical field. In this paper, we create a new dataset, Surg-QA, consisting of 102,000 surgical video-instruction pairs, the largest of its kind so far. To build such a dataset, we propose a novel two-stage question-answer generation pipeline with LLM to learn surgical knowledge in a structured manner from the publicly available surgical lecture videos. The pipeline breaks down the generation process into two stages to significantly reduce the task complexity, allowing us to use a more affordable, locally deployed open-source LLM than the premium paid LLM services. It also mitigates the risk of LLM hallucinations during question-answer generation, thereby enhancing the overall quality of the generated data. We further train LLaVA-Surg, a novel vision-language conversational assistant capable of answering open-ended questions about surgical videos, on this Surg-QA dataset, and conduct comprehensive evaluations on zero-shot surgical video question-answering tasks. We show that LLaVA-Surg significantly outperforms all previous general-domain models, demonstrating exceptional multimodal conversational skills in answering open-ended questions about surgical videos. We will release our code, model, and the instruction-tuning dataset.","sentences":["Multimodal large language models (LLMs) have achieved notable success across various domains, while research in the medical field has largely focused on unimodal images.","Meanwhile, current general-domain multimodal models for videos still lack the capabilities to understand and engage in conversations about surgical videos.","One major contributing factor is the absence of datasets in the surgical field.","In this paper, we create a new dataset, Surg-QA, consisting of 102,000 surgical video-instruction pairs, the largest of its kind so far.","To build such a dataset, we propose a novel two-stage question-answer generation pipeline with LLM to learn surgical knowledge in a structured manner from the publicly available surgical lecture videos.","The pipeline breaks down the generation process into two stages to significantly reduce the task complexity, allowing us to use a more affordable, locally deployed open-source LLM than the premium paid LLM services.","It also mitigates the risk of LLM hallucinations during question-answer generation, thereby enhancing the overall quality of the generated data.","We further train LLaVA-Surg, a novel vision-language conversational assistant capable of answering open-ended questions about surgical videos, on this Surg-QA dataset, and conduct comprehensive evaluations on zero-shot surgical video question-answering tasks.","We show that LLaVA-Surg significantly outperforms all previous general-domain models, demonstrating exceptional multimodal conversational skills in answering open-ended questions about surgical videos.","We will release our code, model, and the instruction-tuning dataset."],"url":"http://arxiv.org/abs/2408.07981v1"}
{"created":"2024-08-15 06:52:24","title":"Coupling without Communication and Drafter-Invariant Speculative Decoding","abstract":"Suppose Alice has a distribution $P$ and Bob has a distribution $Q$. Alice wants to generate a sample $a\\sim P$ and Bob a sample $b \\sim Q$ such that $a = b$ with has as high of probability as possible. It is well-known that, by sampling from an optimal coupling between the distributions, Alice and Bob can achieve $Pr[a = b] = 1 - D_{TV}(P,Q)$, where $D_{TV}(P,Q)$ is the total variation distance. What if Alice and Bob must solve this same problem without communicating at all? Perhaps surprisingly, with access to public randomness, they can still achieve $Pr[a = b] \\geq \\frac{1 - D_{TV}(P,Q)}{1 + D_{TV}(P,Q)} \\geq 1-2D_{TV}(P,Q)$. In fact, this bound can be obtained using a simple protocol based on the Weighted MinHash algorithm. In this work, we explore the communication-free coupling in greater depth. First, we show that an equally simple protocol based on Gumbel sampling matches the worst-case guarantees of the Weighted MinHash approach, but tends to perform better in practice. Conversely, we prove that both approaches are actually sharp: no communication-free protocol can achieve $Pr[a=b]>\\frac{1 - D_{TV}(P,Q)}{1 + D_{TV}(P,Q)}$ in the worst-case. Finally, we prove that, for distributions over $n$ items, there exists a scheme that uses just $O(\\log(n/\\epsilon))$ bits of communication to achieve $Pr[a = b] = 1 - D_{TV}(P,Q) - \\epsilon$, i.e. to essentially match optimal coupling. Beyond our theoretical results, we demonstrate an application of communication-free coupling to speculative decoding, a recent method for accelerating autoregressive large language models [Leviathan, Kalman, Matias, ICML 2023]. We show that communication-free protocols yield a variant of speculative decoding that we call Drafter-Invariant Speculative Decoding, which has the desirable property that the output of the method is fixed given a fixed random seed, regardless of what drafter is used for speculation.","sentences":["Suppose Alice has a distribution $P$ and Bob has a distribution $Q$. Alice wants to generate a sample $a\\sim P$ and Bob a sample $b \\sim Q$ such that $a = b$ with has as high of probability as possible.","It is well-known that, by sampling from an optimal coupling between the distributions, Alice and Bob can achieve $Pr[a = b] = 1 - D_{TV}(P,Q)$, where $D_{TV}(P,Q)$ is the total variation distance.","What if Alice and Bob must solve this same problem without communicating at all?","Perhaps surprisingly, with access to public randomness, they can still achieve $Pr[a = b] \\geq \\frac{1 - D_{TV}(P,Q)}{1 + D_{TV}(P,Q)} \\geq 1-2D_{TV}(P,Q)$.","In fact, this bound can be obtained using a simple protocol based on the Weighted MinHash algorithm.","In this work, we explore the communication-free coupling in greater depth.","First, we show that an equally simple protocol based on Gumbel sampling matches the worst-case guarantees of the Weighted MinHash approach, but tends to perform better in practice.","Conversely, we prove that both approaches are actually sharp:","no communication-free protocol can achieve $Pr[a=b]>\\frac{1 - D_{TV}(P,Q)}{1 + D_{TV}(P,Q)}$ in the worst-case.","Finally, we prove that, for distributions over $n$ items, there exists a scheme that uses just $O(\\log(n/\\epsilon))$ bits of communication to achieve $Pr[a = b] = 1 - D_{TV}(P,Q) - \\epsilon$, i.e. to essentially match optimal coupling.","Beyond our theoretical results, we demonstrate an application of communication-free coupling to speculative decoding, a recent method for accelerating autoregressive large language models [Leviathan, Kalman, Matias, ICML 2023].","We show that communication-free protocols yield a variant of speculative decoding that we call Drafter-Invariant Speculative Decoding, which has the desirable property that the output of the method is fixed given a fixed random seed, regardless of what drafter is used for speculation."],"url":"http://arxiv.org/abs/2408.07978v1"}
{"created":"2024-08-15 06:40:38","title":"Polaris: Open-ended Interactive Robotic Manipulation via Syn2Real Visual Grounding and Large Language Models","abstract":"This paper investigates the task of the open-ended interactive robotic manipulation on table-top scenarios. While recent Large Language Models (LLMs) enhance robots' comprehension of user instructions, their lack of visual grounding constrains their ability to physically interact with the environment. This is because the robot needs to locate the target object for manipulation within the physical workspace. To this end, we introduce an interactive robotic manipulation framework called Polaris, which integrates perception and interaction by utilizing GPT-4 alongside grounded vision models. For precise manipulation, it is essential that such grounded vision models produce detailed object pose for the target object, rather than merely identifying pixels belonging to them in the image. Consequently, we propose a novel Synthetic-to-Real (Syn2Real) pose estimation pipeline. This pipeline utilizes rendered synthetic data for training and is then transferred to real-world manipulation tasks. The real-world performance demonstrates the efficacy of our proposed pipeline and underscores its potential for extension to more general categories. Moreover, real-robot experiments have showcased the impressive performance of our framework in grasping and executing multiple manipulation tasks. This indicates its potential to generalize to scenarios beyond the tabletop. More information and video results are available here: https://star-uu-wang.github.io/Polaris/","sentences":["This paper investigates the task of the open-ended interactive robotic manipulation on table-top scenarios.","While recent Large Language Models (LLMs) enhance robots' comprehension of user instructions, their lack of visual grounding constrains their ability to physically interact with the environment.","This is because the robot needs to locate the target object for manipulation within the physical workspace.","To this end, we introduce an interactive robotic manipulation framework called Polaris, which integrates perception and interaction by utilizing GPT-4 alongside grounded vision models.","For precise manipulation, it is essential that such grounded vision models produce detailed object pose for the target object, rather than merely identifying pixels belonging to them in the image.","Consequently, we propose a novel Synthetic-to-Real (Syn2Real) pose estimation pipeline.","This pipeline utilizes rendered synthetic data for training and is then transferred to real-world manipulation tasks.","The real-world performance demonstrates the efficacy of our proposed pipeline and underscores its potential for extension to more general categories.","Moreover, real-robot experiments have showcased the impressive performance of our framework in grasping and executing multiple manipulation tasks.","This indicates its potential to generalize to scenarios beyond the tabletop.","More information and video results are available here: https://star-uu-wang.github.io/Polaris/"],"url":"http://arxiv.org/abs/2408.07975v1"}
{"created":"2024-08-15 06:36:27","title":"Predicting Lung Cancer Patient Prognosis with Large Language Models","abstract":"Prognosis prediction is crucial for determining optimal treatment plans for lung cancer patients. Traditionally, such predictions relied on models developed from retrospective patient data. Recently, large language models (LLMs) have gained attention for their ability to process and generate text based on extensive learned knowledge. In this study, we evaluate the potential of GPT-4o mini and GPT-3.5 in predicting the prognosis of lung cancer patients. We collected two prognosis datasets, i.e., survival and post-operative complication datasets, and designed multiple tasks to assess the models' performance comprehensively. Logistic regression models were also developed as baselines for comparison. The experimental results demonstrate that LLMs can achieve competitive, and in some tasks superior, performance in lung cancer prognosis prediction compared to data-driven logistic regression models despite not using additional patient data. These findings suggest that LLMs can be effective tools for prognosis prediction in lung cancer, particularly when patient data is limited or unavailable.","sentences":["Prognosis prediction is crucial for determining optimal treatment plans for lung cancer patients.","Traditionally, such predictions relied on models developed from retrospective patient data.","Recently, large language models (LLMs) have gained attention for their ability to process and generate text based on extensive learned knowledge.","In this study, we evaluate the potential of GPT-4o mini and GPT-3.5 in predicting the prognosis of lung cancer patients.","We collected two prognosis datasets, i.e., survival and post-operative complication datasets, and designed multiple tasks to assess the models' performance comprehensively.","Logistic regression models were also developed as baselines for comparison.","The experimental results demonstrate that LLMs can achieve competitive, and in some tasks superior, performance in lung cancer prognosis prediction compared to data-driven logistic regression models despite not using additional patient data.","These findings suggest that LLMs can be effective tools for prognosis prediction in lung cancer, particularly when patient data is limited or unavailable."],"url":"http://arxiv.org/abs/2408.07971v1"}
{"created":"2024-08-15 06:26:46","title":"Addressing Skewed Heterogeneity via Federated Prototype Rectification with Personalization","abstract":"Federated learning is an efficient framework designed to facilitate collaborative model training across multiple distributed devices while preserving user data privacy. A significant challenge of federated learning is data-level heterogeneity, i.e., skewed or long-tailed distribution of private data. Although various methods have been proposed to address this challenge, most of them assume that the underlying global data is uniformly distributed across all clients. This paper investigates data-level heterogeneity federated learning with a brief review and redefines a more practical and challenging setting called Skewed Heterogeneous Federated Learning (SHFL). Accordingly, we propose a novel Federated Prototype Rectification with Personalization which consists of two parts: Federated Personalization and Federated Prototype Rectification. The former aims to construct balanced decision boundaries between dominant and minority classes based on private data, while the latter exploits both inter-class discrimination and intra-class consistency to rectify empirical prototypes. Experiments on three popular benchmarks show that the proposed approach outperforms current state-of-the-art methods and achieves balanced performance in both personalization and generalization.","sentences":["Federated learning is an efficient framework designed to facilitate collaborative model training across multiple distributed devices while preserving user data privacy.","A significant challenge of federated learning is data-level heterogeneity, i.e., skewed or long-tailed distribution of private data.","Although various methods have been proposed to address this challenge, most of them assume that the underlying global data is uniformly distributed across all clients.","This paper investigates data-level heterogeneity federated learning with a brief review and redefines a more practical and challenging setting called Skewed Heterogeneous Federated Learning (SHFL).","Accordingly, we propose a novel Federated Prototype Rectification with Personalization which consists of two parts: Federated Personalization and Federated Prototype Rectification.","The former aims to construct balanced decision boundaries between dominant and minority classes based on private data, while the latter exploits both inter-class discrimination and intra-class consistency to rectify empirical prototypes.","Experiments on three popular benchmarks show that the proposed approach outperforms current state-of-the-art methods and achieves balanced performance in both personalization and generalization."],"url":"http://arxiv.org/abs/2408.07966v1"}
{"created":"2024-08-15 06:11:59","title":"Joint Optimization of Buffer Delay and HARQ for Video Communications","abstract":"To improve the quality of experience (QoE) in video communication over lossy networks, this paper presents a transmission method that jointly optimizes buffer delay and Hybrid Automatic Repeat request (HARQ), referred to as BD-HARQ. This method operates on packet group and employs dynamic buffer delay combined with HARQ strategy for transmission. By defining the QoE based on metrics such as buffer delay, Forward Error Correction (FEC) redundancy, and data recovery rate, the proposed method derives its closed-form expression through rigorous mathematical modeling and analysis. The optimal transmission parameters, i.e., the buffer delay and the FEC redundancy, are then determined and implemented, guaranteeing the real-time performance, transmission efficiency, and data recovery rate of video communication. Experimental results demonstrate that the proposed method aligns well with its theoretical expectations, and that it can provide up to 13.7% higher QoE compared to existing methods and increase the tolerance for packet loss rate from 15%-22% to up to 31% while maintaining a high QoE.","sentences":["To improve the quality of experience (QoE) in video communication over lossy networks, this paper presents a transmission method that jointly optimizes buffer delay and Hybrid Automatic Repeat request (HARQ), referred to as BD-HARQ.","This method operates on packet group and employs dynamic buffer delay combined with HARQ strategy for transmission.","By defining the QoE based on metrics such as buffer delay, Forward Error Correction (FEC) redundancy, and data recovery rate, the proposed method derives its closed-form expression through rigorous mathematical modeling and analysis.","The optimal transmission parameters, i.e., the buffer delay and the FEC redundancy, are then determined and implemented, guaranteeing the real-time performance, transmission efficiency, and data recovery rate of video communication.","Experimental results demonstrate that the proposed method aligns well with its theoretical expectations, and that it can provide up to 13.7% higher QoE compared to existing methods and increase the tolerance for packet loss rate from 15%-22% to up to 31% while maintaining a high QoE."],"url":"http://arxiv.org/abs/2408.07957v1"}
{"created":"2024-08-15 06:09:19","title":"RandomNet: Clustering Time Series Using Untrained Deep Neural Networks","abstract":"Neural networks are widely used in machine learning and data mining. Typically, these networks need to be trained, implying the adjustment of weights (parameters) within the network based on the input data. In this work, we propose a novel approach, RandomNet, that employs untrained deep neural networks to cluster time series. RandomNet uses different sets of random weights to extract diverse representations of time series and then ensembles the clustering relationships derived from these different representations to build the final clustering results. By extracting diverse representations, our model can effectively handle time series with different characteristics. Since all parameters are randomly generated, no training is required during the process. We provide a theoretical analysis of the effectiveness of the method. To validate its performance, we conduct extensive experiments on all of the 128 datasets in the well-known UCR time series archive and perform statistical analysis of the results. These datasets have different sizes, sequence lengths, and they are from diverse fields. The experimental results show that the proposed method is competitive compared with existing state-of-the-art methods.","sentences":["Neural networks are widely used in machine learning and data mining.","Typically, these networks need to be trained, implying the adjustment of weights (parameters) within the network based on the input data.","In this work, we propose a novel approach, RandomNet, that employs untrained deep neural networks to cluster time series.","RandomNet uses different sets of random weights to extract diverse representations of time series and then ensembles the clustering relationships derived from these different representations to build the final clustering results.","By extracting diverse representations, our model can effectively handle time series with different characteristics.","Since all parameters are randomly generated, no training is required during the process.","We provide a theoretical analysis of the effectiveness of the method.","To validate its performance, we conduct extensive experiments on all of the 128 datasets in the well-known UCR time series archive and perform statistical analysis of the results.","These datasets have different sizes, sequence lengths, and they are from diverse fields.","The experimental results show that the proposed method is competitive compared with existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2408.07956v1"}
{"created":"2024-08-15 05:35:52","title":"Training Spatial-Frequency Visual Prompts and Probabilistic Clusters for Accurate Black-Box Transfer Learning","abstract":"Despite the growing prevalence of black-box pre-trained models (PTMs) such as prediction API services, there remains a significant challenge in directly applying general models to real-world scenarios due to the data distribution gap. Considering a data deficiency and constrained computational resource scenario, this paper proposes a novel parameter-efficient transfer learning framework for vision recognition models in the black-box setting. Our framework incorporates two novel training techniques. First, we align the input space (i.e., image) of PTMs to the target data distribution by generating visual prompts of spatial and frequency domain. Along with the novel spatial-frequency hybrid visual prompter, we design a novel training technique based on probabilistic clusters, which can enhance class separation in the output space (i.e., prediction probabilities). In experiments, our model demonstrates superior performance in a few-shot transfer learning setting across extensive visual recognition datasets, surpassing state-of-the-art baselines. Additionally, we show that the proposed method efficiently reduces computational costs for training and inference phases.","sentences":["Despite the growing prevalence of black-box pre-trained models (PTMs) such as prediction API services, there remains a significant challenge in directly applying general models to real-world scenarios due to the data distribution gap.","Considering a data deficiency and constrained computational resource scenario, this paper proposes a novel parameter-efficient transfer learning framework for vision recognition models in the black-box setting.","Our framework incorporates two novel training techniques.","First, we align the input space (i.e., image) of PTMs to the target data distribution by generating visual prompts of spatial and frequency domain.","Along with the novel spatial-frequency hybrid visual prompter, we design a novel training technique based on probabilistic clusters, which can enhance class separation in the output space (i.e., prediction probabilities).","In experiments, our model demonstrates superior performance in a few-shot transfer learning setting across extensive visual recognition datasets, surpassing state-of-the-art baselines.","Additionally, we show that the proposed method efficiently reduces computational costs for training and inference phases."],"url":"http://arxiv.org/abs/2408.07944v1"}
{"created":"2024-08-15 04:59:12","title":"Surgical SAM 2: Real-time Segment Anything in Surgical Video by Efficient Frame Pruning","abstract":"Surgical video segmentation is a critical task in computer-assisted surgery and is vital for enhancing surgical quality and patient outcomes. Recently, the Segment Anything Model 2 (SAM2) framework has shown superior advancements in image and video segmentation. However, SAM2 struggles with efficiency due to the high computational demands of processing high-resolution images and complex and long-range temporal dynamics in surgical videos. To address these challenges, we introduce Surgical SAM 2 (SurgSAM-2), an advanced model to utilize SAM2 with an Efficient Frame Pruning (EFP) mechanism, to facilitate real-time surgical video segmentation. The EFP mechanism dynamically manages the memory bank by selectively retaining only the most informative frames, reducing memory usage and computational cost while maintaining high segmentation accuracy. Our extensive experiments demonstrate that SurgSAM-2 significantly improves both efficiency and segmentation accuracy compared to the vanilla SAM2. Remarkably, SurgSAM-2 achieves a 3$\\times$ FPS compared with SAM2, while also delivering state-of-the-art performance after fine-tuning with lower-resolution data. These advancements establish SurgSAM-2 as a leading model for surgical video analysis, making real-time surgical video segmentation in resource-constrained environments a feasible reality.","sentences":["Surgical video segmentation is a critical task in computer-assisted surgery and is vital for enhancing surgical quality and patient outcomes.","Recently, the Segment Anything Model 2 (SAM2) framework has shown superior advancements in image and video segmentation.","However, SAM2 struggles with efficiency due to the high computational demands of processing high-resolution images and complex and long-range temporal dynamics in surgical videos.","To address these challenges, we introduce Surgical SAM 2 (SurgSAM-2), an advanced model to utilize SAM2 with an Efficient Frame Pruning (EFP) mechanism, to facilitate real-time surgical video segmentation.","The EFP mechanism dynamically manages the memory bank by selectively retaining only the most informative frames, reducing memory usage and computational cost while maintaining high segmentation accuracy.","Our extensive experiments demonstrate that SurgSAM-2 significantly improves both efficiency and segmentation accuracy compared to the vanilla SAM2.","Remarkably, SurgSAM-2 achieves a 3$\\times$ FPS compared with SAM2, while also delivering state-of-the-art performance after fine-tuning with lower-resolution data.","These advancements establish SurgSAM-2 as a leading model for surgical video analysis, making real-time surgical video segmentation in resource-constrained environments a feasible reality."],"url":"http://arxiv.org/abs/2408.07931v1"}
{"created":"2024-08-15 04:18:40","title":"A Deep Features-Based Approach Using Modified ResNet50 and Gradient Boosting for Visual Sentiments Classification","abstract":"The versatile nature of Visual Sentiment Analysis (VSA) is one reason for its rising profile. It isn't easy to efficiently manage social media data with visual information since previous research has concentrated on Sentiment Analysis (SA) of single modalities, like textual. In addition, most visual sentiment studies need to adequately classify sentiment because they are mainly focused on simply merging modal attributes without investigating their intricate relationships. This prompted the suggestion of developing a fusion of deep learning and machine learning algorithms. In this research, a deep feature-based method for multiclass classification has been used to extract deep features from modified ResNet50. Furthermore, gradient boosting algorithm has been used to classify photos containing emotional content. The approach is thoroughly evaluated on two benchmarked datasets, CrowdFlower and GAPED. Finally, cutting-edge deep learning and machine learning models were used to compare the proposed strategy. When compared to state-of-the-art approaches, the proposed method demonstrates exceptional performance on the datasets presented.","sentences":["The versatile nature of Visual Sentiment Analysis (VSA) is one reason for its rising profile.","It isn't easy to efficiently manage social media data with visual information since previous research has concentrated on Sentiment Analysis (SA) of single modalities, like textual.","In addition, most visual sentiment studies need to adequately classify sentiment because they are mainly focused on simply merging modal attributes without investigating their intricate relationships.","This prompted the suggestion of developing a fusion of deep learning and machine learning algorithms.","In this research, a deep feature-based method for multiclass classification has been used to extract deep features from modified ResNet50.","Furthermore, gradient boosting algorithm has been used to classify photos containing emotional content.","The approach is thoroughly evaluated on two benchmarked datasets, CrowdFlower and GAPED.","Finally, cutting-edge deep learning and machine learning models were used to compare the proposed strategy.","When compared to state-of-the-art approaches, the proposed method demonstrates exceptional performance on the datasets presented."],"url":"http://arxiv.org/abs/2408.07922v1"}
{"created":"2024-08-15 04:16:45","title":"Physics-Informed Neural Network for Predicting Out-of-Training-Range TCAD Solution with Minimized Domain Expertise","abstract":"Machine learning (ML) is promising in assisting technology computer-aided design (TCAD) simulations to alleviate difficulty in convergence and prolonged simulation time. While ML is widely used in TCAD, they either require access to the internal solver, require extensive domain expertise, are only trained by terminal quantities such as currents and voltages, and/or lack out-of-training-range prediction capability. In this paper, using Si nanowire as an example, we demonstrate that it is possible to use a physics-informed neural network (PINN) to predict out-of-training-range TCAD solutions without accessing the internal solver and with minimal domain expertise. The machine not only can predict a 2.5 times larger range than the training but also can predict the inversion region by only being trained with subthreshold region data. The physics-informed module is also trained with data without the need for human-coded equations making this easier to be extended to more sophisticated systems.","sentences":["Machine learning (ML) is promising in assisting technology computer-aided design (TCAD) simulations to alleviate difficulty in convergence and prolonged simulation time.","While ML is widely used in TCAD, they either require access to the internal solver, require extensive domain expertise, are only trained by terminal quantities such as currents and voltages, and/or lack out-of-training-range prediction capability.","In this paper, using Si nanowire as an example, we demonstrate that it is possible to use a physics-informed neural network (PINN) to predict out-of-training-range TCAD solutions without accessing the internal solver and with minimal domain expertise.","The machine not only can predict a 2.5 times larger range than the training but also can predict the inversion region by only being trained with subthreshold region data.","The physics-informed module is also trained with data without the need for human-coded equations making this easier to be extended to more sophisticated systems."],"url":"http://arxiv.org/abs/2408.07921v1"}
{"created":"2024-08-15 03:54:33","title":"GOReloc: Graph-based Object-Level Relocalization for Visual SLAM","abstract":"This article introduces a novel method for object-level relocalization of robotic systems. It determines the pose of a camera sensor by robustly associating the object detections in the current frame with 3D objects in a lightweight object-level map. Object graphs, considering semantic uncertainties, are constructed for both the incoming camera frame and the pre-built map. Objects are represented as graph nodes, and each node employs unique semantic descriptors based on our devised graph kernels. We extract a subgraph from the target map graph by identifying potential object associations for each object detection, then refine these associations and pose estimations using a RANSAC-inspired strategy. Experiments on various datasets demonstrate that our method achieves more accurate data association and significantly increases relocalization success rates compared to baseline methods. The implementation of our method is released at \\url{https://github.com/yutongwangBIT/GOReloc}.","sentences":["This article introduces a novel method for object-level relocalization of robotic systems.","It determines the pose of a camera sensor by robustly associating the object detections in the current frame with 3D objects in a lightweight object-level map.","Object graphs, considering semantic uncertainties, are constructed for both the incoming camera frame and the pre-built map.","Objects are represented as graph nodes, and each node employs unique semantic descriptors based on our devised graph kernels.","We extract a subgraph from the target map graph by identifying potential object associations for each object detection, then refine these associations and pose estimations using a RANSAC-inspired strategy.","Experiments on various datasets demonstrate that our method achieves more accurate data association and significantly increases relocalization success rates compared to baseline methods.","The implementation of our method is released at \\url{https://github.com/yutongwangBIT/GOReloc}."],"url":"http://arxiv.org/abs/2408.07917v1"}
{"created":"2024-08-15 03:49:56","title":"GridSE: Towards Practical Secure Geographic Search via Prefix Symmetric Searchable Encryption (Full Version)","abstract":"The proliferation of location-based services and applications has brought significant attention to data and location privacy. While general secure computation and privacy-enhancing techniques can partially address this problem, one outstanding challenge is to provide near latency-free search and compatibility with mainstream geographic search techniques, especially the Discrete Global Grid Systems (DGGS). This paper proposes a new construction, namely GridSE, for efficient and DGGS-compatible Secure Geographic Search (SGS) with both backward and forward privacy. We first formulate the notion of a semantic-secure primitive called \\textit{symmetric prefix predicate encryption} (SP$^2$E), for predicting whether or not a keyword contains a given prefix, and provide a construction. Then we extend SP$^2$E for dynamic \\textit{prefix symmetric searchable encryption} (pSSE), namely GridSE, which supports both backward and forward privacy. GridSE only uses lightweight primitives including cryptographic hash and XOR operations and is extremely efficient. Furthermore, we provide a generic pSSE framework that enables prefix search for traditional dynamic SSE that supports only full keyword search. Experimental results over real-world geographic databases of sizes (by the number of entries) from $10^3$ to $10^7$ and mainstream DGGS techniques show that GridSE achieves a speedup of $150\\times$ - $5000\\times$ on search latency and a saving of $99\\%$ on communication overhead as compared to the state-of-the-art. Interestingly, even compared to plaintext search, GridSE introduces only $1.4\\times$ extra computational cost and $0.9\\times$ additional communication cost. Source code of our scheme is available at https://github.com/rykieguo1771/GridSE-RAM.","sentences":["The proliferation of location-based services and applications has brought significant attention to data and location privacy.","While general secure computation and privacy-enhancing techniques can partially address this problem, one outstanding challenge is to provide near latency-free search and compatibility with mainstream geographic search techniques, especially the Discrete Global Grid Systems (DGGS).","This paper proposes a new construction, namely GridSE, for efficient and DGGS-compatible Secure Geographic Search (SGS) with both backward and forward privacy.","We first formulate the notion of a semantic-secure primitive called \\textit{symmetric prefix predicate encryption} (SP$^2$E), for predicting whether or not a keyword contains a given prefix, and provide a construction.","Then we extend SP$^2$E for dynamic \\textit{prefix symmetric searchable encryption} (pSSE), namely GridSE, which supports both backward and forward privacy.","GridSE only uses lightweight primitives including cryptographic hash and XOR operations and is extremely efficient.","Furthermore, we provide a generic pSSE framework that enables prefix search for traditional dynamic SSE that supports only full keyword search.","Experimental results over real-world geographic databases of sizes (by the number of entries) from $10^3$ to $10^7$ and mainstream DGGS techniques show that GridSE achieves a speedup of $150\\times$ - $5000\\times$ on search latency and a saving of $99\\%$ on communication overhead as compared to the state-of-the-art.","Interestingly, even compared to plaintext search, GridSE introduces only $1.4\\times$ extra computational cost and $0.9\\times$ additional communication cost.","Source code of our scheme is available at https://github.com/rykieguo1771/GridSE-RAM."],"url":"http://arxiv.org/abs/2408.07916v1"}
{"created":"2024-08-15 03:40:44","title":"A Systematic Mapping Study of Crowd Knowledge Enhanced Software Engineering Research Using Stack Overflow","abstract":"Developers continuously interact in crowd-sourced community-based question-answer (Q&A) sites. Reportedly, 30% of all software professionals visit the most popular Q&A site StackOverflow (SO) every day. Software engineering (SE) research studies are also increasingly using SO data. To find out the trend, implication, impact, and future research potential utilizing SO data, a systematic mapping study needs to be conducted. Following a rigorous reproducible mapping study approach, from 18 reputed SE journals and conferences, we collected 384 SO-based research articles and categorized them into 10 facets (i.e., themes). We found that SO contributes to 85% of SE research compared with popular Q&A sites such as Quora, and Reddit. We found that 18 SE domains directly benefited from SO data whereas Recommender Systems, and API Design and Evolution domains use SO data the most (15% and 16% of all SO-based research studies, respectively). API Design and Evolution, and Machine Learning with/for SE domains have consistent upward publication. Deep Learning Bug Analysis and Code Cloning research areas have the highest potential research impact recently. With the insights, recommendations, and facet-based categorized paper list from this mapping study, SE researchers can find potential research areas according to their interest to utilize large-scale SO data.","sentences":["Developers continuously interact in crowd-sourced community-based question-answer (Q&A) sites.","Reportedly, 30% of all software professionals visit the most popular Q&A site StackOverflow (SO) every day.","Software engineering (SE) research studies are also increasingly using SO data.","To find out the trend, implication, impact, and future research potential utilizing SO data, a systematic mapping study needs to be conducted.","Following a rigorous reproducible mapping study approach, from 18 reputed SE journals and conferences, we collected 384 SO-based research articles and categorized them into 10 facets (i.e., themes).","We found that SO contributes to 85% of SE research compared with popular Q&A sites such as Quora, and Reddit.","We found that 18 SE domains directly benefited from SO data whereas Recommender Systems, and API Design and Evolution domains use SO data the most (15% and 16% of all SO-based research studies, respectively).","API Design and Evolution, and Machine Learning with/for SE domains have consistent upward publication.","Deep Learning Bug Analysis and Code Cloning research areas have the highest potential research impact recently.","With the insights, recommendations, and facet-based categorized paper list from this mapping study, SE researchers can find potential research areas according to their interest to utilize large-scale SO data."],"url":"http://arxiv.org/abs/2408.07913v1"}
{"created":"2024-08-15 03:34:53","title":"CEGRL-TKGR: A Causal Enhanced Graph Representation Learning Framework for Improving Temporal Knowledge Graph Extrapolation Reasoning","abstract":"Temporal knowledge graph reasoning (TKGR) is increasingly gaining attention for its ability to extrapolate new events from historical data, thereby enriching the inherently incomplete temporal knowledge graphs. Existing graph-based representation learning frameworks have made significant strides in developing evolving representations for both entities and relational embeddings. Despite these achievements, there's a notable tendency in these models to inadvertently learn biased data representations and mine spurious correlations, consequently failing to discern the causal relationships between events. This often leads to incorrect predictions based on these false correlations. To address this, we propose an innovative causal enhanced graph representation learning framework for TKGR (named CEGRL-TKGR). This framework introduces causal structures in graph-based representation learning to unveil the essential causal relationships between events, ultimately enhancing task performance. Specifically, we first disentangle the evolutionary representations of entities and relations in a temporal graph sequence into two distinct components, namely causal representations and confounding representations. Then, drawing on causal intervention theory, we advocate the utilization of causal representations for predictions, aiming to mitigate the effects of erroneous correlations caused by confounding features, thus achieving more robust and accurate predictions. Finally, extensive experimental results on six benchmark datasets demonstrate the superior performance of our model in the link prediction task.","sentences":["Temporal knowledge graph reasoning (TKGR) is increasingly gaining attention for its ability to extrapolate new events from historical data, thereby enriching the inherently incomplete temporal knowledge graphs.","Existing graph-based representation learning frameworks have made significant strides in developing evolving representations for both entities and relational embeddings.","Despite these achievements, there's a notable tendency in these models to inadvertently learn biased data representations and mine spurious correlations, consequently failing to discern the causal relationships between events.","This often leads to incorrect predictions based on these false correlations.","To address this, we propose an innovative causal enhanced graph representation learning framework for TKGR (named CEGRL-TKGR).","This framework introduces causal structures in graph-based representation learning to unveil the essential causal relationships between events, ultimately enhancing task performance.","Specifically, we first disentangle the evolutionary representations of entities and relations in a temporal graph sequence into two distinct components, namely causal representations and confounding representations.","Then, drawing on causal intervention theory, we advocate the utilization of causal representations for predictions, aiming to mitigate the effects of erroneous correlations caused by confounding features, thus achieving more robust and accurate predictions.","Finally, extensive experimental results on six benchmark datasets demonstrate the superior performance of our model in the link prediction task."],"url":"http://arxiv.org/abs/2408.07911v1"}
{"created":"2024-08-15 03:27:23","title":"Time-Dependent VAE for Building Latent Factor from Visual Neural Activity with Complex Dynamics","abstract":"Seeking high-quality neural latent representations to reveal the intrinsic correlation between neural activity and behavior or sensory stimulation has attracted much interest. Currently, some deep latent variable models rely on behavioral information (e.g., movement direction and position) as an aid to build expressive embeddings while being restricted by fixed time scales. Visual neural activity from passive viewing lacks clearly correlated behavior or task information, and high-dimensional visual stimulation leads to intricate neural dynamics. To cope with such conditions, we propose Time-Dependent SwapVAE, following the approach of separating content and style spaces in Swap-VAE, on the basis of which we introduce state variables to construct conditional distributions with temporal dependence for the above two spaces. Our model progressively generates latent variables along neural activity sequences, and we apply self-supervised contrastive learning to shape its latent space. In this way, it can effectively analyze complex neural dynamics from sequences of arbitrary length, even without task or behavioral data as auxiliary inputs. We compare TiDe-SwapVAE with alternative models on synthetic data and neural data from mouse visual cortex. The results show that our model not only accurately decodes complex visual stimuli but also extracts explicit temporal neural dynamics, demonstrating that it builds latent representations more relevant to visual stimulation.","sentences":["Seeking high-quality neural latent representations to reveal the intrinsic correlation between neural activity and behavior or sensory stimulation has attracted much interest.","Currently, some deep latent variable models rely on behavioral information (e.g., movement direction and position) as an aid to build expressive embeddings while being restricted by fixed time scales.","Visual neural activity from passive viewing lacks clearly correlated behavior or task information, and high-dimensional visual stimulation leads to intricate neural dynamics.","To cope with such conditions, we propose Time-Dependent SwapVAE, following the approach of separating content and style spaces in Swap-VAE, on the basis of which we introduce state variables to construct conditional distributions with temporal dependence for the above two spaces.","Our model progressively generates latent variables along neural activity sequences, and we apply self-supervised contrastive learning to shape its latent space.","In this way, it can effectively analyze complex neural dynamics from sequences of arbitrary length, even without task or behavioral data as auxiliary inputs.","We compare TiDe-SwapVAE with alternative models on synthetic data and neural data from mouse visual cortex.","The results show that our model not only accurately decodes complex visual stimuli but also extracts explicit temporal neural dynamics, demonstrating that it builds latent representations more relevant to visual stimulation."],"url":"http://arxiv.org/abs/2408.07908v1"}
{"created":"2024-08-15 03:25:56","title":"AIE: Auction Information Enhanced Framework for CTR Prediction in Online Advertising","abstract":"Click-Through Rate (CTR) prediction is a fundamental technique for online advertising recommendation and the complex online competitive auction process also brings many difficulties to CTR optimization. Recent studies have shown that introducing posterior auction information contributes to the performance of CTR prediction. However, existing work doesn't fully capitalize on the benefits of auction information and overlooks the data bias brought by the auction, leading to biased and suboptimal results. To address these limitations, we propose Auction Information Enhanced Framework (AIE) for CTR prediction in online advertising, which delves into the problem of insufficient utilization of auction signals and first reveals the auction bias. Specifically, AIE introduces two pluggable modules, namely Adaptive Market-price Auxiliary Module (AM2) and Bid Calibration Module (BCM), which work collaboratively to excavate the posterior auction signals better and enhance the performance of CTR prediction. Furthermore, the two proposed modules are lightweight, model-agnostic, and friendly to inference latency. Extensive experiments are conducted on a public dataset and an industrial dataset to demonstrate the effectiveness and compatibility of AIE. Besides, a one-month online A/B test in a large-scale advertising platform shows that AIE improves the base model by 5.76% and 2.44% in terms of eCPM and CTR, respectively.","sentences":["Click-Through Rate (CTR) prediction is a fundamental technique for online advertising recommendation and the complex online competitive auction process also brings many difficulties to CTR optimization.","Recent studies have shown that introducing posterior auction information contributes to the performance of CTR prediction.","However, existing work doesn't fully capitalize on the benefits of auction information and overlooks the data bias brought by the auction, leading to biased and suboptimal results.","To address these limitations, we propose Auction Information Enhanced Framework (AIE) for CTR prediction in online advertising, which delves into the problem of insufficient utilization of auction signals and first reveals the auction bias.","Specifically, AIE introduces two pluggable modules, namely Adaptive Market-price Auxiliary Module (AM2) and Bid Calibration Module (BCM), which work collaboratively to excavate the posterior auction signals better and enhance the performance of CTR prediction.","Furthermore, the two proposed modules are lightweight, model-agnostic, and friendly to inference latency.","Extensive experiments are conducted on a public dataset and an industrial dataset to demonstrate the effectiveness and compatibility of AIE.","Besides, a one-month online A/B test in a large-scale advertising platform shows that AIE improves the base model by 5.76% and 2.44% in terms of eCPM and CTR, respectively."],"url":"http://arxiv.org/abs/2408.07907v1"}
{"created":"2024-08-15 03:24:00","title":"Persistence Image from 3D Medical Image: Superpixel and Optimized Gaussian Coefficient","abstract":"Topological data analysis (TDA) uncovers crucial properties of objects in medical imaging. Methods based on persistent homology have demonstrated their advantages in capturing topological features that traditional deep learning methods cannot detect in both radiology and pathology. However, previous research primarily focused on 2D image analysis, neglecting the comprehensive 3D context. In this paper, we propose an innovative 3D TDA approach that incorporates the concept of superpixels to transform 3D medical image features into point cloud data. By Utilizing Optimized Gaussian Coefficient, the proposed 3D TDA method, for the first time, efficiently generate holistic Persistence Images for 3D volumetric data. Our 3D TDA method exhibits superior performance on the MedMNist3D dataset when compared to other traditional methods, showcasing its potential effectiveness in modeling 3D persistent homology-based topological analysis when it comes to classification tasks. The source code is publicly available at https://github.com/hrlblab/TopologicalDataAnalysis3D.","sentences":["Topological data analysis (TDA) uncovers crucial properties of objects in medical imaging.","Methods based on persistent homology have demonstrated their advantages in capturing topological features that traditional deep learning methods cannot detect in both radiology and pathology.","However, previous research primarily focused on 2D image analysis, neglecting the comprehensive 3D context.","In this paper, we propose an innovative 3D TDA approach that incorporates the concept of superpixels to transform 3D medical image features into point cloud data.","By Utilizing Optimized Gaussian Coefficient, the proposed 3D TDA method, for the first time, efficiently generate holistic Persistence Images for 3D volumetric data.","Our 3D TDA method exhibits superior performance on the MedMNist3D dataset when compared to other traditional methods, showcasing its potential effectiveness in modeling 3D persistent homology-based topological analysis when it comes to classification tasks.","The source code is publicly available at https://github.com/hrlblab/TopologicalDataAnalysis3D."],"url":"http://arxiv.org/abs/2408.07905v1"}
{"created":"2024-08-15 02:55:30","title":"The doctor will polygraph you now: ethical concerns with AI for fact-checking patients","abstract":"Clinical artificial intelligence (AI) methods have been proposed for predicting social behaviors which could be reasonably understood from patient-reported data. This raises ethical concerns about respect, privacy, and patient awareness/control over how their health data is used. Ethical concerns surrounding clinical AI systems for social behavior verification were divided into three main categories: (1) the use of patient data retrospectively without informed consent for the specific task of verification, (2) the potential for inaccuracies or biases within such systems, and (3) the impact on trust in patient-provider relationships with the introduction of automated AI systems for fact-checking. Additionally, this report showed the simulated misuse of a verification system and identified a potential LLM bias against patient-reported information in favor of multimodal data, published literature, and the outputs of other AI methods (i.e., AI self-trust). Finally, recommendations were presented for mitigating the risk that AI verification systems will cause harm to patients or undermine the purpose of the healthcare system.","sentences":["Clinical artificial intelligence (AI) methods have been proposed for predicting social behaviors which could be reasonably understood from patient-reported data.","This raises ethical concerns about respect, privacy, and patient awareness/control over how their health data is used.","Ethical concerns surrounding clinical AI systems for social behavior verification were divided into three main categories: (1) the use of patient data retrospectively without informed consent for the specific task of verification, (2) the potential for inaccuracies or biases within such systems, and (3) the impact on trust in patient-provider relationships with the introduction of automated AI systems for fact-checking.","Additionally, this report showed the simulated misuse of a verification system and identified a potential LLM bias against patient-reported information in favor of multimodal data, published literature, and the outputs of other AI methods (i.e., AI self-trust).","Finally, recommendations were presented for mitigating the risk that AI verification systems will cause harm to patients or undermine the purpose of the healthcare system."],"url":"http://arxiv.org/abs/2408.07896v1"}
{"created":"2024-08-15 02:52:02","title":"System States Forecasting of Microservices with Dynamic Spatio-Temporal Data","abstract":"In the AIOps (Artificial Intelligence for IT Operations) era, accurately forecasting system states is crucial. In microservices systems, this task encounters the challenge of dynamic and complex spatio-temporal relationships among microservice instances, primarily due to dynamic deployments, diverse call paths, and cascading effects among instances. Current time-series forecasting methods, which focus mainly on intrinsic patterns, are insufficient in environments where spatial relationships are critical. Similarly, spatio-temporal graph approaches often neglect the nature of temporal trend, concentrating mostly on message passing between nodes. Moreover, current research in microservices domain frequently underestimates the importance of network metrics and topological structures in capturing the evolving dynamics of systems. This paper introduces STMformer, a model tailored for forecasting system states in microservices environments, capable of handling multi-node and multivariate time series. Our method leverages dynamic network connection data and topological information to assist in modeling the intricate spatio-temporal relationships within the system. Additionally, we integrate the PatchCrossAttention module to compute the impact of cascading effects globally. We have developed a dataset based on a microservices system and conducted comprehensive experiments with STMformer against leading methods. In both short-term and long-term forecasting tasks, our model consistently achieved a 8.6% reduction in MAE(Mean Absolute Error) and a 2.2% reduction in MSE (Mean Squared Error). The source code is available at https://github.com/xuyifeiiie/STMformer.","sentences":["In the AIOps (Artificial Intelligence for IT Operations) era, accurately forecasting system states is crucial.","In microservices systems, this task encounters the challenge of dynamic and complex spatio-temporal relationships among microservice instances, primarily due to dynamic deployments, diverse call paths, and cascading effects among instances.","Current time-series forecasting methods, which focus mainly on intrinsic patterns, are insufficient in environments where spatial relationships are critical.","Similarly, spatio-temporal graph approaches often neglect the nature of temporal trend, concentrating mostly on message passing between nodes.","Moreover, current research in microservices domain frequently underestimates the importance of network metrics and topological structures in capturing the evolving dynamics of systems.","This paper introduces STMformer, a model tailored for forecasting system states in microservices environments, capable of handling multi-node and multivariate time series.","Our method leverages dynamic network connection data and topological information to assist in modeling the intricate spatio-temporal relationships within the system.","Additionally, we integrate the PatchCrossAttention module to compute the impact of cascading effects globally.","We have developed a dataset based on a microservices system and conducted comprehensive experiments with STMformer against leading methods.","In both short-term and long-term forecasting tasks, our model consistently achieved a 8.6% reduction in MAE(Mean Absolute Error) and a 2.2% reduction in MSE (Mean Squared Error).","The source code is available at https://github.com/xuyifeiiie/STMformer."],"url":"http://arxiv.org/abs/2408.07894v1"}
{"created":"2024-08-15 02:22:48","title":"Fine-tuning Large Language Models with Human-inspired Learning Strategies in Medical Question Answering","abstract":"Training Large Language Models (LLMs) incurs substantial data-related costs, motivating the development of data-efficient training methods through optimised data ordering and selection. Human-inspired learning strategies, such as curriculum learning, offer possibilities for efficient training by organising data according to common human learning practices. Despite evidence that fine-tuning with curriculum learning improves the performance of LLMs for natural language understanding tasks, its effectiveness is typically assessed using a single model. In this work, we extend previous research by evaluating both curriculum-based and non-curriculum-based learning strategies across multiple LLMs, using human-defined and automated data labels for medical question answering. Our results indicate a moderate impact of using human-inspired learning strategies for fine-tuning LLMs, with maximum accuracy gains of 1.77% per model and 1.81% per dataset. Crucially, we demonstrate that the effectiveness of these strategies varies significantly across different model-dataset combinations, emphasising that the benefits of a specific human-inspired strategy for fine-tuning LLMs do not generalise. Additionally, we find evidence that curriculum learning using LLM-defined question difficulty outperforms human-defined difficulty, highlighting the potential of using model-generated measures for optimal curriculum design.","sentences":["Training Large Language Models (LLMs) incurs substantial data-related costs, motivating the development of data-efficient training methods through optimised data ordering and selection.","Human-inspired learning strategies, such as curriculum learning, offer possibilities for efficient training by organising data according to common human learning practices.","Despite evidence that fine-tuning with curriculum learning improves the performance of LLMs for natural language understanding tasks, its effectiveness is typically assessed using a single model.","In this work, we extend previous research by evaluating both curriculum-based and non-curriculum-based learning strategies across multiple LLMs, using human-defined and automated data labels for medical question answering.","Our results indicate a moderate impact of using human-inspired learning strategies for fine-tuning LLMs, with maximum accuracy gains of 1.77% per model and 1.81% per dataset.","Crucially, we demonstrate that the effectiveness of these strategies varies significantly across different model-dataset combinations, emphasising that the benefits of a specific human-inspired strategy for fine-tuning LLMs do not generalise.","Additionally, we find evidence that curriculum learning using LLM-defined question difficulty outperforms human-defined difficulty, highlighting the potential of using model-generated measures for optimal curriculum design."],"url":"http://arxiv.org/abs/2408.07888v1"}
{"created":"2024-08-15 01:54:39","title":"To Impute or Not: Recommendations for Multibiometric Fusion","abstract":"Combining match scores from different biometric systems via fusion is a well-established approach to improving recognition accuracy. However, missing scores can degrade performance as well as limit the possible fusion techniques that can be applied. Imputation is a promising technique in multibiometric systems for replacing missing data. In this paper, we evaluate various score imputation approaches on three multimodal biometric score datasets, viz. NIST BSSR1, BIOCOP2008, and MIT LL Trimodal, and investigate the factors which might influence the effectiveness of imputation. Our studies reveal three key observations: (1) Imputation is preferable over not imputing missing scores, even when the fusion rule does not require complete score data. (2) Balancing the classes in the training data is crucial to mitigate negative biases in the imputation technique towards the under-represented class, even if it involves dropping a substantial number of score vectors. (3) Multivariate imputation approaches seem to be beneficial when scores between modalities are correlated, while univariate approaches seem to benefit scenarios where scores between modalities are less correlated.","sentences":["Combining match scores from different biometric systems via fusion is a well-established approach to improving recognition accuracy.","However, missing scores can degrade performance as well as limit the possible fusion techniques that can be applied.","Imputation is a promising technique in multibiometric systems for replacing missing data.","In this paper, we evaluate various score imputation approaches on three multimodal biometric score datasets, viz.","NIST BSSR1, BIOCOP2008, and MIT LL Trimodal, and investigate the factors which might influence the effectiveness of imputation.","Our studies reveal three key observations: (1) Imputation is preferable over not imputing missing scores, even when the fusion rule does not require complete score data.","(2) Balancing the classes in the training data is crucial to mitigate negative biases in the imputation technique towards the under-represented class, even if it involves dropping a substantial number of score vectors.","(3) Multivariate imputation approaches seem to be beneficial when scores between modalities are correlated, while univariate approaches seem to benefit scenarios where scores between modalities are less correlated."],"url":"http://arxiv.org/abs/2408.07883v1"}
{"created":"2024-08-15 01:23:49","title":"Incremental Structure Discovery of Classification via Sequential Monte Carlo","abstract":"Gaussian Processes (GPs) provide a powerful framework for making predictions and understanding uncertainty for classification with kernels and Bayesian non-parametric learning. Building such models typically requires strong prior knowledge to define preselect kernels, which could be ineffective for online applications of classification that sequentially process data because features of data may shift during the process. To alleviate the requirement of prior knowledge used in GPs and learn new features from data that arrive successively, this paper presents a novel method to automatically discover models of classification on complex data with little prior knowledge. Our method adapts a recently proposed technique for GP-based time-series structure discovery, which integrates GPs and Sequential Monte Carlo (SMC). We extend the technique to handle extra latent variables in GP classification, such that our method can effectively and adaptively learn a-priori unknown structures of classification from continuous input. In addition, our method adapts new batch of data with updated structures of models. Our experiments show that our method is able to automatically incorporate various features of kernels on synthesized data and real-world data for classification. In the experiments of real-world data, our method outperforms various classification methods on both online and offline setting achieving a 10\\% accuracy improvement on one benchmark.","sentences":["Gaussian Processes (GPs) provide a powerful framework for making predictions and understanding uncertainty for classification with kernels and Bayesian non-parametric learning.","Building such models typically requires strong prior knowledge to define preselect kernels, which could be ineffective for online applications of classification that sequentially process data because features of data may shift during the process.","To alleviate the requirement of prior knowledge used in GPs and learn new features from data that arrive successively, this paper presents a novel method to automatically discover models of classification on complex data with little prior knowledge.","Our method adapts a recently proposed technique for GP-based time-series structure discovery, which integrates GPs and Sequential Monte Carlo (SMC).","We extend the technique to handle extra latent variables in GP classification, such that our method can effectively and adaptively learn a-priori unknown structures of classification from continuous input.","In addition, our method adapts new batch of data with updated structures of models.","Our experiments show that our method is able to automatically incorporate various features of kernels on synthesized data and real-world data for classification.","In the experiments of real-world data, our method outperforms various classification methods on both online and offline setting achieving a 10\\% accuracy improvement on one benchmark."],"url":"http://arxiv.org/abs/2408.07875v1"}
{"created":"2024-08-15 01:00:28","title":"Words Matter: Reducing Stigma in Online Conversations about Substance Use with Large Language Models","abstract":"Stigma is a barrier to treatment for individuals struggling with substance use disorders (SUD), which leads to significantly lower treatment engagement rates. With only 7% of those affected receiving any form of help, societal stigma not only discourages individuals with SUD from seeking help but isolates them, hindering their recovery journey and perpetuating a cycle of shame and self-doubt. This study investigates how stigma manifests on social media, particularly Reddit, where anonymity can exacerbate discriminatory behaviors. We analyzed over 1.2 million posts, identifying 3,207 that exhibited stigmatizing language towards people who use substances (PWUS). Using Informed and Stylized LLMs, we develop a model for de-stigmatization of these expressions into empathetic language, resulting in 1,649 reformed phrase pairs. Our paper contributes to the field by proposing a computational framework for analyzing stigma and destigmatizing online content, and delving into the linguistic features that propagate stigma towards PWUS. Our work not only enhances understanding of stigma's manifestations online but also provides practical tools for fostering a more supportive digital environment for those affected by SUD. Code and data will be made publicly available upon acceptance.","sentences":["Stigma is a barrier to treatment for individuals struggling with substance use disorders (SUD), which leads to significantly lower treatment engagement rates.","With only 7% of those affected receiving any form of help, societal stigma not only discourages individuals with SUD from seeking help but isolates them, hindering their recovery journey and perpetuating a cycle of shame and self-doubt.","This study investigates how stigma manifests on social media, particularly Reddit, where anonymity can exacerbate discriminatory behaviors.","We analyzed over 1.2 million posts, identifying 3,207 that exhibited stigmatizing language towards people who use substances (PWUS).","Using Informed and Stylized LLMs, we develop a model for de-stigmatization of these expressions into empathetic language, resulting in 1,649 reformed phrase pairs.","Our paper contributes to the field by proposing a computational framework for analyzing stigma and destigmatizing online content, and delving into the linguistic features that propagate stigma towards PWUS.","Our work not only enhances understanding of stigma's manifestations online but also provides practical tools for fostering a more supportive digital environment for those affected by SUD.","Code and data will be made publicly available upon acceptance."],"url":"http://arxiv.org/abs/2408.07873v1"}
{"created":"2024-08-15 00:53:09","title":"A Systematic Evaluation of Generated Time Series and Their Effects in Self-Supervised Pretraining","abstract":"Self-supervised Pretrained Models (PTMs) have demonstrated remarkable performance in computer vision and natural language processing tasks. These successes have prompted researchers to design PTMs for time series data. In our experiments, most self-supervised time series PTMs were surpassed by simple supervised models. We hypothesize this undesired phenomenon may be caused by data scarcity. In response, we test six time series generation methods, use the generated data in pretraining in lieu of the real data, and examine the effects on classification performance. Our results indicate that replacing a real-data pretraining set with a greater volume of only generated samples produces noticeable improvement.","sentences":["Self-supervised Pretrained Models (PTMs) have demonstrated remarkable performance in computer vision and natural language processing tasks.","These successes have prompted researchers to design PTMs for time series data.","In our experiments, most self-supervised time series PTMs were surpassed by simple supervised models.","We hypothesize this undesired phenomenon may be caused by data scarcity.","In response, we test six time series generation methods, use the generated data in pretraining in lieu of the real data, and examine the effects on classification performance.","Our results indicate that replacing a real-data pretraining set with a greater volume of only generated samples produces noticeable improvement."],"url":"http://arxiv.org/abs/2408.07869v1"}
{"created":"2024-08-15 00:22:32","title":"Zero Day Ransomware Detection with Pulse: Function Classification with Transformer Models and Assembly Language","abstract":"Finding automated AI techniques to proactively defend against malware has become increasingly critical. The ability of an AI model to correctly classify novel malware is dependent on the quality of the features it is trained with and the authenticity of the features is dependent on the analysis tool. Peekaboo, a Dynamic Binary Instrumentation tool defeats evasive malware to capture its genuine behavior. The ransomware Assembly instructions captured by Peekaboo, follow Zipf's law, a principle also observed in natural languages, indicating Transformer models are particularly well suited to binary classification. We propose Pulse, a novel framework for zero day ransomware detection with Transformer models and Assembly language. Pulse, trained with the Peekaboo ransomware and benign software data, uniquely identify truly new samples with high accuracy. Pulse eliminates any familiar functionality across the test and training samples, forcing the Transformer model to detect malicious behavior based solely on context and novel Assembly instruction combinations.","sentences":["Finding automated AI techniques to proactively defend against malware has become increasingly critical.","The ability of an AI model to correctly classify novel malware is dependent on the quality of the features it is trained with and the authenticity of the features is dependent on the analysis tool.","Peekaboo, a Dynamic Binary Instrumentation tool defeats evasive malware to capture its genuine behavior.","The ransomware Assembly instructions captured by Peekaboo, follow Zipf's law, a principle also observed in natural languages, indicating Transformer models are particularly well suited to binary classification.","We propose Pulse, a novel framework for zero day ransomware detection with Transformer models and Assembly language.","Pulse, trained with the Peekaboo ransomware and benign software data, uniquely identify truly new samples with high accuracy.","Pulse eliminates any familiar functionality across the test and training samples, forcing the Transformer model to detect malicious behavior based solely on context and novel Assembly instruction combinations."],"url":"http://arxiv.org/abs/2408.07862v1"}
{"created":"2024-08-14 23:45:21","title":"CON-FOLD -- Explainable Machine Learning with Confidence","abstract":"FOLD-RM is an explainable machine learning classification algorithm that uses training data to create a set of classification rules. In this paper we introduce CON-FOLD which extends FOLD-RM in several ways. CON-FOLD assigns probability-based confidence scores to rules learned for a classification task. This allows users to know how confident they should be in a prediction made by the model. We present a confidence-based pruning algorithm that uses the unique structure of FOLD-RM rules to efficiently prune rules and prevent overfitting. Furthermore, CON-FOLD enables the user to provide pre-existing knowledge in the form of logic program rules that are either (fixed) background knowledge or (modifiable) initial rule candidates. The paper describes our method in detail and reports on practical experiments. We demonstrate the performance of the algorithm on benchmark datasets from the UCI Machine Learning Repository. For that, we introduce a new metric, Inverse Brier Score, to evaluate the accuracy of the produced confidence scores. Finally we apply this extension to a real world example that requires explainability: marking of student responses to a short answer question from the Australian Physics Olympiad.","sentences":["FOLD-RM is an explainable machine learning classification algorithm that uses training data to create a set of classification rules.","In this paper we introduce CON-FOLD which extends FOLD-RM in several ways.","CON-FOLD assigns probability-based confidence scores to rules learned for a classification task.","This allows users to know how confident they should be in a prediction made by the model.","We present a confidence-based pruning algorithm that uses the unique structure of FOLD-RM rules to efficiently prune rules and prevent overfitting.","Furthermore, CON-FOLD enables the user to provide pre-existing knowledge in the form of logic program rules that are either (fixed) background knowledge or (modifiable) initial rule candidates.","The paper describes our method in detail and reports on practical experiments.","We demonstrate the performance of the algorithm on benchmark datasets from the UCI Machine Learning Repository.","For that, we introduce a new metric, Inverse Brier Score, to evaluate the accuracy of the produced confidence scores.","Finally we apply this extension to a real world example that requires explainability: marking of student responses to a short answer question from the Australian Physics Olympiad."],"url":"http://arxiv.org/abs/2408.07854v1"}
{"created":"2024-08-14 23:34:28","title":"Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability","abstract":"While many capabilities of language models (LMs) improve with increased training budget, the influence of scale on hallucinations is not yet fully understood. Hallucinations come in many forms, and there is no universally accepted definition. We thus focus on studying only those hallucinations where a correct answer appears verbatim in the training set. To fully control the training data content, we construct a knowledge graph (KG)-based dataset, and use it to train a set of increasingly large LMs. We find that for a fixed dataset, larger and longer-trained LMs hallucinate less. However, hallucinating on $\\leq5$% of the training data requires an order of magnitude larger model, and thus an order of magnitude more compute, than Hoffmann et al. (2022) reported was optimal. Given this costliness, we study how hallucination detectors depend on scale. While we see detector size improves performance on fixed LM's outputs, we find an inverse relationship between the scale of the LM and the detectability of its hallucinations.","sentences":["While many capabilities of language models (LMs) improve with increased training budget, the influence of scale on hallucinations is not yet fully understood.","Hallucinations come in many forms, and there is no universally accepted definition.","We thus focus on studying only those hallucinations where a correct answer appears verbatim in the training set.","To fully control the training data content, we construct a knowledge graph (KG)-based dataset, and use it to train a set of increasingly large LMs.","We find that for a fixed dataset, larger and longer-trained LMs hallucinate less.","However, hallucinating on $\\leq5$% of the training data requires an order of magnitude larger model, and thus an order of magnitude more compute, than Hoffmann et al. (2022) reported was optimal.","Given this costliness, we study how hallucination detectors depend on scale.","While we see detector size improves performance on fixed LM's outputs, we find an inverse relationship between the scale of the LM and the detectability of its hallucinations."],"url":"http://arxiv.org/abs/2408.07852v1"}
{"created":"2024-08-14 23:33:10","title":"SER Evals: In-domain and Out-of-domain Benchmarking for Speech Emotion Recognition","abstract":"Speech emotion recognition (SER) has made significant strides with the advent of powerful self-supervised learning (SSL) models. However, the generalization of these models to diverse languages and emotional expressions remains a challenge. We propose a large-scale benchmark to evaluate the robustness and adaptability of state-of-the-art SER models in both in-domain and out-of-domain settings. Our benchmark includes a diverse set of multilingual datasets, focusing on less commonly used corpora to assess generalization to new data. We employ logit adjustment to account for varying class distributions and establish a single dataset cluster for systematic evaluation. Surprisingly, we find that the Whisper model, primarily designed for automatic speech recognition, outperforms dedicated SSL models in cross-lingual SER. Our results highlight the need for more robust and generalizable SER models, and our benchmark serves as a valuable resource to drive future research in this direction.","sentences":["Speech emotion recognition (SER) has made significant strides with the advent of powerful self-supervised learning (SSL) models.","However, the generalization of these models to diverse languages and emotional expressions remains a challenge.","We propose a large-scale benchmark to evaluate the robustness and adaptability of state-of-the-art SER models in both in-domain and out-of-domain settings.","Our benchmark includes a diverse set of multilingual datasets, focusing on less commonly used corpora to assess generalization to new data.","We employ logit adjustment to account for varying class distributions and establish a single dataset cluster for systematic evaluation.","Surprisingly, we find that the Whisper model, primarily designed for automatic speech recognition, outperforms dedicated SSL models in cross-lingual SER.","Our results highlight the need for more robust and generalizable SER models, and our benchmark serves as a valuable resource to drive future research in this direction."],"url":"http://arxiv.org/abs/2408.07851v1"}
{"created":"2024-08-14 23:01:02","title":"Enhancing Equitable Access to AI in Housing and Homelessness System of Care through Federated Learning","abstract":"The top priority of a Housing and Homelessness System of Care (HHSC) is to connect people experiencing homelessness to supportive housing. An HHSC typically consists of many agencies serving the same population. Information technology platforms differ in type and quality between agencies, so their data are usually isolated from one agency to another. Larger agencies may have sufficient data to train and test artificial intelligence (AI) tools but smaller agencies typically do not. To address this gap, we introduce a Federated Learning (FL) approach enabling all agencies to train a predictive model collaboratively without sharing their sensitive data. We demonstrate how FL can be used within an HHSC to provide all agencies equitable access to quality AI and further assist human decision-makers in the allocation of resources within HHSC. This is achieved while preserving the privacy of the people within the data by not sharing identifying information between agencies without their consent. Our experimental results using real-world HHSC data from Calgary, Alberta, demonstrate that our FL approach offers comparable performance with the idealized scenario of training the predictive model with data fully shared and linked between agencies.","sentences":["The top priority of a Housing and Homelessness System of Care (HHSC) is to connect people experiencing homelessness to supportive housing.","An HHSC typically consists of many agencies serving the same population.","Information technology platforms differ in type and quality between agencies, so their data are usually isolated from one agency to another.","Larger agencies may have sufficient data to train and test artificial intelligence (AI) tools but smaller agencies typically do not.","To address this gap, we introduce a Federated Learning (FL) approach enabling all agencies to train a predictive model collaboratively without sharing their sensitive data.","We demonstrate how FL can be used within an HHSC to provide all agencies equitable access to quality AI and further assist human decision-makers in the allocation of resources within HHSC.","This is achieved while preserving the privacy of the people within the data by not sharing identifying information between agencies without their consent.","Our experimental results using real-world HHSC data from Calgary, Alberta, demonstrate that our FL approach offers comparable performance with the idealized scenario of training the predictive model with data fully shared and linked between agencies."],"url":"http://arxiv.org/abs/2408.07845v1"}
{"created":"2024-08-14 22:45:46","title":"Portability of Fortran's `do concurrent' on GPUs","abstract":"There is a continuing interest in using standard language constructs for accelerated computing in order to avoid (sometimes vendor-specific) external APIs. For Fortran codes, the {\\tt do concurrent} (DC) loop has been successfully demonstrated on the NVIDIA platform. However, support for DC on other platforms has taken longer to implement. Recently, Intel has added DC GPU offload support to its compiler, as has HPE for AMD GPUs. In this paper, we explore the current portability of using DC across GPU vendors using the in-production solar surface flux evolution code, HipFT. We discuss implementation and compilation details, including when/where using directive APIs for data movement is needed/desired compared to using a unified memory system. The performance achieved on both data center and consumer platforms is shown.","sentences":["There is a continuing interest in using standard language constructs for accelerated computing in order to avoid (sometimes vendor-specific) external APIs.","For Fortran codes, the {\\tt do concurrent} (DC) loop has been successfully demonstrated on the NVIDIA platform.","However, support for DC on other platforms has taken longer to implement.","Recently, Intel has added DC GPU offload support to its compiler, as has HPE for AMD GPUs.","In this paper, we explore the current portability of using DC across GPU vendors using the in-production solar surface flux evolution code, HipFT.","We discuss implementation and compilation details, including when/where using directive APIs for data movement is needed/desired compared to using a unified memory system.","The performance achieved on both data center and consumer platforms is shown."],"url":"http://arxiv.org/abs/2408.07843v1"}
{"created":"2024-08-14 22:43:52","title":"SustainDC -- Benchmarking for Sustainable Data Center Control","abstract":"Machine learning has driven an exponential increase in computational demand, leading to massive data centers that consume significant amounts of energy and contribute to climate change. This makes sustainable data center control a priority. In this paper, we introduce SustainDC, a set of Python environments for benchmarking multi-agent reinforcement learning (MARL) algorithms for data centers (DC). SustainDC supports custom DC configurations and tasks such as workload scheduling, cooling optimization, and auxiliary battery management, with multiple agents managing these operations while accounting for the effects of each other. We evaluate various MARL algorithms on SustainDC, showing their performance across diverse DC designs, locations, weather conditions, grid carbon intensity, and workload requirements. Our results highlight significant opportunities for improvement of data center operations using MARL algorithms. Given the increasing use of DC due to AI, SustainDC provides a crucial platform for the development and benchmarking of advanced algorithms essential for achieving sustainable computing and addressing other heterogeneous real-world challenges.","sentences":["Machine learning has driven an exponential increase in computational demand, leading to massive data centers that consume significant amounts of energy and contribute to climate change.","This makes sustainable data center control a priority.","In this paper, we introduce SustainDC, a set of Python environments for benchmarking multi-agent reinforcement learning (MARL) algorithms for data centers (DC).","SustainDC supports custom DC configurations and tasks such as workload scheduling, cooling optimization, and auxiliary battery management, with multiple agents managing these operations while accounting for the effects of each other.","We evaluate various MARL algorithms on SustainDC, showing their performance across diverse DC designs, locations, weather conditions, grid carbon intensity, and workload requirements.","Our results highlight significant opportunities for improvement of data center operations using MARL algorithms.","Given the increasing use of DC due to AI, SustainDC provides a crucial platform for the development and benchmarking of advanced algorithms essential for achieving sustainable computing and addressing other heterogeneous real-world challenges."],"url":"http://arxiv.org/abs/2408.07841v1"}
{"created":"2024-08-14 22:28:19","title":"ONSEP: A Novel Online Neural-Symbolic Framework for Event Prediction Based on Large Language Model","abstract":"In the realm of event prediction, temporal knowledge graph forecasting (TKGF) stands as a pivotal technique. Previous approaches face the challenges of not utilizing experience during testing and relying on a single short-term history, which limits adaptation to evolving data. In this paper, we introduce the Online Neural-Symbolic Event Prediction (ONSEP) framework, which innovates by integrating dynamic causal rule mining (DCRM) and dual history augmented generation (DHAG). DCRM dynamically constructs causal rules from real-time data, allowing for swift adaptation to new causal relationships. In parallel, DHAG merges short-term and long-term historical contexts, leveraging a bi-branch approach to enrich event prediction. Our framework demonstrates notable performance enhancements across diverse datasets, with significant Hit@k (k=1,3,10) improvements, showcasing its ability to augment large language models (LLMs) for event prediction without necessitating extensive retraining. The ONSEP framework not only advances the field of TKGF but also underscores the potential of neural-symbolic approaches in adapting to dynamic data environments.","sentences":["In the realm of event prediction, temporal knowledge graph forecasting (TKGF) stands as a pivotal technique.","Previous approaches face the challenges of not utilizing experience during testing and relying on a single short-term history, which limits adaptation to evolving data.","In this paper, we introduce the Online Neural-Symbolic Event Prediction (ONSEP) framework, which innovates by integrating dynamic causal rule mining (DCRM) and dual history augmented generation (DHAG).","DCRM dynamically constructs causal rules from real-time data, allowing for swift adaptation to new causal relationships.","In parallel, DHAG merges short-term and long-term historical contexts, leveraging a bi-branch approach to enrich event prediction.","Our framework demonstrates notable performance enhancements across diverse datasets, with significant Hit@k (k=1,3,10) improvements, showcasing its ability to augment large language models (LLMs) for event prediction without necessitating extensive retraining.","The ONSEP framework not only advances the field of TKGF but also underscores the potential of neural-symbolic approaches in adapting to dynamic data environments."],"url":"http://arxiv.org/abs/2408.07840v1"}
{"created":"2024-08-14 22:08:06","title":"CarbonClipper: Optimal Algorithms for Carbon-Aware Spatiotemporal Workload Management","abstract":"We study carbon-aware spatiotemporal workload management, which seeks to address the growing environmental impact of data centers. We formalize this as an online problem called spatiotemporal online allocation with deadline constraints ($\\mathsf{SOAD}$), in which an online player completes a workload (e.g., a batch compute job) by moving and scheduling the workload across a network subject to a deadline $T$. At each time step, a service cost function is revealed, representing, e.g., the carbon intensity of servicing a workload at each location, and the player must irrevocably decide the current allocation. Furthermore, whenever the player moves the allocation, it incurs a movement cost defined by a metric space $(X,d)$ that captures, e.g., the overhead of migrating a compute job. $\\mathsf{SOAD}$ formalizes the open problem of combining general metrics and deadline constraints in the online algorithms literature, unifying problems such as metrical task systems and online search. We propose a competitive algorithm for $\\mathsf{SOAD}$ along with a matching lower bound that proves it is optimal. Our main algorithm, ${\\rm C{\\scriptsize ARBON}C{\\scriptsize LIPPER}}$, is a learning-augmented algorithm that takes advantage of predictions (e.g., carbon intensity forecasts) and achieves an optimal consistency-robustness trade-off. We evaluate our proposed algorithms for carbon-aware spatiotemporal workload management on a simulated global data center network, showing that ${\\rm C{\\scriptsize ARBON}C{\\scriptsize LIPPER}}$ significantly improves performance compared to baseline methods and delivers meaningful carbon reductions.","sentences":["We study carbon-aware spatiotemporal workload management, which seeks to address the growing environmental impact of data centers.","We formalize this as an online problem called spatiotemporal online allocation with deadline constraints ($\\mathsf{SOAD}$), in which an online player completes a workload (e.g., a batch compute job) by moving and scheduling the workload across a network subject to a deadline $T$. At each time step, a service cost function is revealed, representing, e.g., the carbon intensity of servicing a workload at each location, and the player must irrevocably decide the current allocation.","Furthermore, whenever the player moves the allocation, it incurs a movement cost defined by a metric space $(X,d)$ that captures, e.g., the overhead of migrating a compute job.","$\\mathsf{SOAD}$ formalizes the open problem of combining general metrics and deadline constraints in the online algorithms literature, unifying problems such as metrical task systems and online search.","We propose a competitive algorithm for $\\mathsf{SOAD}$ along with a matching lower bound that proves it is optimal.","Our main algorithm, ${\\rm C{\\scriptsize ARBON}C{\\scriptsize LIPPER}}$, is a learning-augmented algorithm that takes advantage of predictions (e.g., carbon intensity forecasts) and achieves an optimal consistency-robustness trade-off.","We evaluate our proposed algorithms for carbon-aware spatiotemporal workload management on a simulated global data center network, showing that ${\\rm C{\\scriptsize ARBON}C{\\scriptsize LIPPER}}$ significantly improves performance compared to baseline methods and delivers meaningful carbon reductions."],"url":"http://arxiv.org/abs/2408.07831v1"}
{"created":"2024-08-14 21:10:15","title":"How Industry Tackles Anomalies during Runtime: Approaches and Key Monitoring Parameters","abstract":"Deviations from expected behavior during runtime, known as anomalies, have become more common due to the systems' complexity, especially for microservices. Consequently, analyzing runtime monitoring data, such as logs, traces for microservices, and metrics, is challenging due to the large volume of data collected. Developing effective rules or AI algorithms requires a deep understanding of this data to reliably detect unforeseen anomalies. This paper seeks to comprehend anomalies and current anomaly detection approaches across diverse industrial sectors. Additionally, it aims to pinpoint the parameters necessary for identifying anomalies via runtime monitoring data.   Therefore, we conducted semi-structured interviews with fifteen industry participants who rely on anomaly detection during runtime. Additionally, to supplement information from the interviews, we performed a literature review focusing on anomaly detection approaches applied to industrial real-life datasets.   Our paper (1) demonstrates the diversity of interpretations and examples of software anomalies during runtime and (2) explores the reasons behind choosing rule-based approaches in the industry over self-developed AI approaches. AI-based approaches have become prominent in published industry-related papers in the last three years. Furthermore, we (3) identified key monitoring parameters collected during runtime (logs, traces, and metrics) that assist practitioners in detecting anomalies during runtime without introducing bias in their anomaly detection approach due to inconclusive parameters.","sentences":["Deviations from expected behavior during runtime, known as anomalies, have become more common due to the systems' complexity, especially for microservices.","Consequently, analyzing runtime monitoring data, such as logs, traces for microservices, and metrics, is challenging due to the large volume of data collected.","Developing effective rules or AI algorithms requires a deep understanding of this data to reliably detect unforeseen anomalies.","This paper seeks to comprehend anomalies and current anomaly detection approaches across diverse industrial sectors.","Additionally, it aims to pinpoint the parameters necessary for identifying anomalies via runtime monitoring data.   ","Therefore, we conducted semi-structured interviews with fifteen industry participants who rely on anomaly detection during runtime.","Additionally, to supplement information from the interviews, we performed a literature review focusing on anomaly detection approaches applied to industrial real-life datasets.   ","Our paper (1) demonstrates the diversity of interpretations and examples of software anomalies during runtime and (2) explores the reasons behind choosing rule-based approaches in the industry over self-developed AI approaches.","AI-based approaches have become prominent in published industry-related papers in the last three years.","Furthermore, we (3) identified key monitoring parameters collected during runtime (logs, traces, and metrics) that assist practitioners in detecting anomalies during runtime without introducing bias in their anomaly detection approach due to inconclusive parameters."],"url":"http://arxiv.org/abs/2408.07816v1"}
{"created":"2024-08-14 19:07:28","title":"Knowledge-based Neural Ordinary Differential Equations for Cosserat Rod-based Soft Robots","abstract":"Soft robots have many advantages over rigid robots thanks to their compliant and passive nature. However, it is generally challenging to model the dynamics of soft robots due to their high spatial dimensionality, making it difficult to use model-based methods to accurately control soft robots. It often requires direct numerical simulation of partial differential equations to simulate soft robots. This not only requires an accurate numerical model, but also makes soft robot modeling slow and expensive. Deep learning algorithms have shown promises in data-driven modeling of soft robots. However, these algorithms usually require a large amount of data, which are difficult to obtain in either simulation or real-world experiments of soft robots. In this work, we propose KNODE-Cosserat, a framework that combines first-principle physics models and neural ordinary differential equations. We leverage the best from both worlds -- the generalization ability of physics-based models and the fast speed of deep learning methods. We validate our framework in both simulation and real-world experiments. In both cases, we show that the robot model significantly improves over the baseline models under different metrics.","sentences":["Soft robots have many advantages over rigid robots thanks to their compliant and passive nature.","However, it is generally challenging to model the dynamics of soft robots due to their high spatial dimensionality, making it difficult to use model-based methods to accurately control soft robots.","It often requires direct numerical simulation of partial differential equations to simulate soft robots.","This not only requires an accurate numerical model, but also makes soft robot modeling slow and expensive.","Deep learning algorithms have shown promises in data-driven modeling of soft robots.","However, these algorithms usually require a large amount of data, which are difficult to obtain in either simulation or real-world experiments of soft robots.","In this work, we propose KNODE-Cosserat, a framework that combines first-principle physics models and neural ordinary differential equations.","We leverage the best from both worlds -- the generalization ability of physics-based models and the fast speed of deep learning methods.","We validate our framework in both simulation and real-world experiments.","In both cases, we show that the robot model significantly improves over the baseline models under different metrics."],"url":"http://arxiv.org/abs/2408.07776v1"}
{"created":"2024-08-14 18:57:05","title":"MedTsLLM: Leveraging LLMs for Multimodal Medical Time Series Analysis","abstract":"The complexity and heterogeneity of data in many real-world applications pose significant challenges for traditional machine learning and signal processing techniques. For instance, in medicine, effective analysis of diverse physiological signals is crucial for patient monitoring and clinical decision-making and yet highly challenging. We introduce MedTsLLM, a general multimodal large language model (LLM) framework that effectively integrates time series data and rich contextual information in the form of text to analyze physiological signals, performing three tasks with clinical relevance: semantic segmentation, boundary detection, and anomaly detection in time series. These critical tasks enable deeper analysis of physiological signals and can provide actionable insights for clinicians. We utilize a reprogramming layer to align embeddings of time series patches with a pretrained LLM's embedding space and make effective use of raw time series, in conjunction with textual context. Given the multivariate nature of medical datasets, we develop methods to handle multiple covariates. We additionally tailor the text prompt to include patient-specific information. Our model outperforms state-of-the-art baselines, including deep learning models, other LLMs, and clinical methods across multiple medical domains, specifically electrocardiograms and respiratory waveforms. MedTsLLM presents a promising step towards harnessing the power of LLMs for medical time series analysis that can elevate data-driven tools for clinicians and improve patient outcomes.","sentences":["The complexity and heterogeneity of data in many real-world applications pose significant challenges for traditional machine learning and signal processing techniques.","For instance, in medicine, effective analysis of diverse physiological signals is crucial for patient monitoring and clinical decision-making and yet highly challenging.","We introduce MedTsLLM, a general multimodal large language model (LLM) framework that effectively integrates time series data and rich contextual information in the form of text to analyze physiological signals, performing three tasks with clinical relevance: semantic segmentation, boundary detection, and anomaly detection in time series.","These critical tasks enable deeper analysis of physiological signals and can provide actionable insights for clinicians.","We utilize a reprogramming layer to align embeddings of time series patches with a pretrained LLM's embedding space and make effective use of raw time series, in conjunction with textual context.","Given the multivariate nature of medical datasets, we develop methods to handle multiple covariates.","We additionally tailor the text prompt to include patient-specific information.","Our model outperforms state-of-the-art baselines, including deep learning models, other LLMs, and clinical methods across multiple medical domains, specifically electrocardiograms and respiratory waveforms.","MedTsLLM presents a promising step towards harnessing the power of LLMs for medical time series analysis that can elevate data-driven tools for clinicians and improve patient outcomes."],"url":"http://arxiv.org/abs/2408.07773v1"}
{"created":"2024-08-14 18:49:27","title":"Out-of-Distribution Learning with Human Feedback","abstract":"Out-of-distribution (OOD) learning often relies heavily on statistical approaches or predefined assumptions about OOD data distributions, hindering their efficacy in addressing multifaceted challenges of OOD generalization and OOD detection in real-world deployment environments. This paper presents a novel framework for OOD learning with human feedback, which can provide invaluable insights into the nature of OOD shifts and guide effective model adaptation. Our framework capitalizes on the freely available unlabeled data in the wild that captures the environmental test-time OOD distributions under both covariate and semantic shifts. To harness such data, our key idea is to selectively provide human feedback and label a small number of informative samples from the wild data distribution, which are then used to train a multi-class classifier and an OOD detector. By exploiting human feedback, we enhance the robustness and reliability of machine learning models, equipping them with the capability to handle OOD scenarios with greater precision. We provide theoretical insights on the generalization error bounds to justify our algorithm. Extensive experiments show the superiority of our method, outperforming the current state-of-the-art by a significant margin.","sentences":["Out-of-distribution (OOD) learning often relies heavily on statistical approaches or predefined assumptions about OOD data distributions, hindering their efficacy in addressing multifaceted challenges of OOD generalization and OOD detection in real-world deployment environments.","This paper presents a novel framework for OOD learning with human feedback, which can provide invaluable insights into the nature of OOD shifts and guide effective model adaptation.","Our framework capitalizes on the freely available unlabeled data in the wild that captures the environmental test-time OOD distributions under both covariate and semantic shifts.","To harness such data, our key idea is to selectively provide human feedback and label a small number of informative samples from the wild data distribution, which are then used to train a multi-class classifier and an OOD detector.","By exploiting human feedback, we enhance the robustness and reliability of machine learning models, equipping them with the capability to handle OOD scenarios with greater precision.","We provide theoretical insights on the generalization error bounds to justify our algorithm.","Extensive experiments show the superiority of our method, outperforming the current state-of-the-art by a significant margin."],"url":"http://arxiv.org/abs/2408.07772v1"}
{"created":"2024-08-14 18:40:01","title":"On learning capacities of Sugeno integrals with systems of fuzzy relational equations","abstract":"In this article, we introduce a method for learning a capacity underlying a Sugeno integral according to training data based on systems of fuzzy relational equations. To the training data, we associate two systems of equations: a $\\max-\\min$ system and a $\\min-\\max$ system. By solving these two systems (in the case that they are consistent) using Sanchez's results, we show that we can directly obtain the extremal capacities representing the training data. By reducing the $\\max-\\min$ (resp. $\\min-\\max$) system of equations to subsets of criteria of cardinality less than or equal to $q$ (resp. of cardinality greater than or equal to $n-q$), where $n$ is the number of criteria, we give a sufficient condition for deducing, from its potential greatest solution (resp. its potential lowest solution), a $q$-maxitive (resp. $q$-minitive) capacity. Finally, if these two reduced systems of equations are inconsistent, we show how to obtain the greatest approximate $q$-maxitive capacity and the lowest approximate $q$-minitive capacity, using recent results to handle the inconsistency of systems of fuzzy relational equations.","sentences":["In this article, we introduce a method for learning a capacity underlying a Sugeno integral according to training data based on systems of fuzzy relational equations.","To the training data, we associate two systems of equations: a $\\max-\\min$ system and a $\\min-\\max$ system.","By solving these two systems (in the case that they are consistent) using Sanchez's results, we show that we can directly obtain the extremal capacities representing the training data.","By reducing the $\\max-\\min$ (resp.","$\\min-\\max$) system of equations to subsets of criteria of cardinality less than or equal to $q$ (resp. of cardinality greater than or equal to $n-q$), where $n$ is the number of criteria, we give a sufficient condition for deducing, from its potential greatest solution (resp.","its potential lowest solution), a $q$-maxitive (resp.","$q$-minitive) capacity.","Finally, if these two reduced systems of equations are inconsistent, we show how to obtain the greatest approximate $q$-maxitive capacity and the lowest approximate $q$-minitive capacity, using recent results to handle the inconsistency of systems of fuzzy relational equations."],"url":"http://arxiv.org/abs/2408.07768v1"}
{"created":"2024-08-14 18:14:40","title":"RAVE Checklist: Recommendations for Overcoming Challenges in Retrospective Safety Studies of Automated Driving Systems","abstract":"The public, regulators, and domain experts alike seek to understand the effect of deployed SAE level 4 automated driving system (ADS) technologies on safety. The recent expansion of ADS technology deployments is paving the way for early stage safety impact evaluations, whereby the observational data from both an ADS and a representative benchmark fleet are compared to quantify safety performance. In January 2024, a working group of experts across academia, insurance, and industry came together in Washington, DC to discuss the current and future challenges in performing such evaluations. A subset of this working group then met, virtually, on multiple occasions to produce this paper. This paper presents the RAVE (Retrospective Automated Vehicle Evaluation) checklist, a set of fifteen recommendations for performing and evaluating retrospective ADS performance comparisons. The recommendations are centered around the concepts of (1) quality and validity, (2) transparency, and (3) interpretation. Over time, it is anticipated there will be a large and varied body of work evaluating the observed performance of these ADS fleets. Establishing and promoting good scientific practices benefits the work of stakeholders, many of whom may not be subject matter experts. This working group's intentions are to: i) strengthen individual research studies and ii) make the at-large community more informed on how to evaluate this collective body of work.","sentences":["The public, regulators, and domain experts alike seek to understand the effect of deployed SAE level 4 automated driving system (ADS) technologies on safety.","The recent expansion of ADS technology deployments is paving the way for early stage safety impact evaluations, whereby the observational data from both an ADS and a representative benchmark fleet are compared to quantify safety performance.","In January 2024, a working group of experts across academia, insurance, and industry came together in Washington, DC to discuss the current and future challenges in performing such evaluations.","A subset of this working group then met, virtually, on multiple occasions to produce this paper.","This paper presents the RAVE (Retrospective Automated Vehicle Evaluation) checklist, a set of fifteen recommendations for performing and evaluating retrospective ADS performance comparisons.","The recommendations are centered around the concepts of (1) quality and validity, (2) transparency, and (3) interpretation.","Over time, it is anticipated there will be a large and varied body of work evaluating the observed performance of these ADS fleets.","Establishing and promoting good scientific practices benefits the work of stakeholders, many of whom may not be subject matter experts.","This working group's intentions are to: i) strengthen individual research studies and ii) make the at-large community more informed on how to evaluate this collective body of work."],"url":"http://arxiv.org/abs/2408.07758v1"}
{"created":"2024-08-14 18:03:59","title":"How to Solve Contextual Goal-Oriented Problems with Offline Datasets?","abstract":"We present a novel method, Contextual goal-Oriented Data Augmentation (CODA), which uses commonly available unlabeled trajectories and context-goal pairs to solve Contextual Goal-Oriented (CGO) problems. By carefully constructing an action-augmented MDP that is equivalent to the original MDP, CODA creates a fully labeled transition dataset under training contexts without additional approximation error. We conduct a novel theoretical analysis to demonstrate CODA's capability to solve CGO problems in the offline data setup. Empirical results also showcase the effectiveness of CODA, which outperforms other baseline methods across various context-goal relationships of CGO problem. This approach offers a promising direction to solving CGO problems using offline datasets.","sentences":["We present a novel method, Contextual goal-Oriented Data Augmentation (CODA), which uses commonly available unlabeled trajectories and context-goal pairs to solve Contextual Goal-Oriented (CGO) problems.","By carefully constructing an action-augmented MDP that is equivalent to the original MDP, CODA creates a fully labeled transition dataset under training contexts without additional approximation error.","We conduct a novel theoretical analysis to demonstrate CODA's capability to solve CGO problems in the offline data setup.","Empirical results also showcase the effectiveness of CODA, which outperforms other baseline methods across various context-goal relationships of CGO problem.","This approach offers a promising direction to solving CGO problems using offline datasets."],"url":"http://arxiv.org/abs/2408.07753v1"}
{"created":"2024-08-14 17:18:12","title":"Polarization dynamics: a study of individuals shifting between political communities on social media","abstract":"Individuals engaging on social media often tend to establish online communities where interactions predominantly occur among like-minded peers. While considerable efforts have been devoted to studying and delineating these communities, there has been limited attention directed towards individuals who diverge from these patterns. In this study, we examine the community structure of re-post networks within the context of a polarized political environment at two different times. We specifically identify individuals who consistently switch between opposing communities and analyze the key features that distinguish them. Our investigation focuses on two crucial aspects of these users: the topological properties of their interactions and the political bias in the content of their posts. Our analysis is based on a dataset comprising 2 million tweets related to US President Donald Trump, coupled with data from over 100 000 individual user accounts spanning the 2020 US presidential election year. Our findings indicate that individuals who switch communities exhibit disparities compared to those who remain within the same communities, both in terms of the topological aspects of their interaction patterns (pagerank, degree, betweenness centrality.) and in the sentiment bias of their content towards Donald Trump.","sentences":["Individuals engaging on social media often tend to establish online communities where interactions predominantly occur among like-minded peers.","While considerable efforts have been devoted to studying and delineating these communities, there has been limited attention directed towards individuals who diverge from these patterns.","In this study, we examine the community structure of re-post networks within the context of a polarized political environment at two different times.","We specifically identify individuals who consistently switch between opposing communities and analyze the key features that distinguish them.","Our investigation focuses on two crucial aspects of these users: the topological properties of their interactions and the political bias in the content of their posts.","Our analysis is based on a dataset comprising 2 million tweets related to US President Donald Trump, coupled with data from over 100 000 individual user accounts spanning the 2020 US presidential election year.","Our findings indicate that individuals who switch communities exhibit disparities compared to those who remain within the same communities, both in terms of the topological aspects of their interaction patterns (pagerank, degree, betweenness centrality.)","and in the sentiment bias of their content towards Donald Trump."],"url":"http://arxiv.org/abs/2408.07731v1"}
{"created":"2024-08-14 17:11:36","title":"Extending Network Intrusion Detection with Enhanced Particle Swarm Optimization Techniques","abstract":"The present research investigates how to improve Network Intrusion Detection Systems (NIDS) by combining Machine Learning (ML) and Deep Learning (DL) techniques, addressing the growing challenge of cybersecurity threats. A thorough process for data preparation, comprising activities like cleaning, normalization, and segmentation into training and testing sets, lays the framework for model training and evaluation. The study uses the CSE-CIC-IDS 2018 and LITNET-2020 datasets to compare ML methods (Decision Trees, Random Forest, XGBoost) and DL models (CNNs, RNNs, DNNs, MLP) against key performance metrics (Accuracy, Precision, Recall, and F1-Score). The Decision Tree model performed better across all measures after being fine-tuned with Enhanced Particle Swarm Optimization (EPSO), demonstrating the model's ability to detect network breaches effectively. The findings highlight EPSO's importance in improving ML classifiers for cybersecurity, proposing a strong framework for NIDS with high precision and dependability. This extensive analysis not only contributes to the cybersecurity arena by providing a road to robust intrusion detection solutions, but it also proposes future approaches for improving ML models to combat the changing landscape of network threats.","sentences":["The present research investigates how to improve Network Intrusion Detection Systems (NIDS) by combining Machine Learning (ML) and Deep Learning (DL) techniques, addressing the growing challenge of cybersecurity threats.","A thorough process for data preparation, comprising activities like cleaning, normalization, and segmentation into training and testing sets, lays the framework for model training and evaluation.","The study uses the CSE-CIC-IDS 2018 and LITNET-2020 datasets to compare ML methods (Decision Trees, Random Forest, XGBoost) and DL models (CNNs, RNNs, DNNs, MLP) against key performance metrics (Accuracy, Precision, Recall, and F1-Score).","The Decision Tree model performed better across all measures after being fine-tuned with Enhanced Particle Swarm Optimization (EPSO), demonstrating the model's ability to detect network breaches effectively.","The findings highlight EPSO's importance in improving ML classifiers for cybersecurity, proposing a strong framework for NIDS with high precision and dependability.","This extensive analysis not only contributes to the cybersecurity arena by providing a road to robust intrusion detection solutions, but it also proposes future approaches for improving ML models to combat the changing landscape of network threats."],"url":"http://arxiv.org/abs/2408.07729v1"}
