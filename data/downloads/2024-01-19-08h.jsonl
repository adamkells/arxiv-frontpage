{"created":"2024-01-18 18:59:58","title":"ParaHome: Parameterizing Everyday Home Activities Towards 3D Generative Modeling of Human-Object Interactions","abstract":"To enable machines to learn how humans interact with the physical world in our daily activities, it is crucial to provide rich data that encompasses the 3D motion of humans as well as the motion of objects in a learnable 3D representation. Ideally, this data should be collected in a natural setup, capturing the authentic dynamic 3D signals during human-object interactions. To address this challenge, we introduce the ParaHome system, designed to capture and parameterize dynamic 3D movements of humans and objects within a common home environment. Our system consists of a multi-view setup with 70 synchronized RGB cameras, as well as wearable motion capture devices equipped with an IMU-based body suit and hand motion capture gloves. By leveraging the ParaHome system, we collect a novel large-scale dataset of human-object interaction. Notably, our dataset offers key advancement over existing datasets in three main aspects: (1) capturing 3D body and dexterous hand manipulation motion alongside 3D object movement within a contextual home environment during natural activities; (2) encompassing human interaction with multiple objects in various episodic scenarios with corresponding descriptions in texts; (3) including articulated objects with multiple parts expressed with parameterized articulations. Building upon our dataset, we introduce new research tasks aimed at building a generative model for learning and synthesizing human-object interactions in a real-world room setting.","sentences":["To enable machines to learn how humans interact with the physical world in our daily activities, it is crucial to provide rich data that encompasses the 3D motion of humans as well as the motion of objects in a learnable 3D representation.","Ideally, this data should be collected in a natural setup, capturing the authentic dynamic 3D signals during human-object interactions.","To address this challenge, we introduce the ParaHome system, designed to capture and parameterize dynamic 3D movements of humans and objects within a common home environment.","Our system consists of a multi-view setup with 70 synchronized RGB cameras, as well as wearable motion capture devices equipped with an IMU-based body suit and hand motion capture gloves.","By leveraging the ParaHome system, we collect a novel large-scale dataset of human-object interaction.","Notably, our dataset offers key advancement over existing datasets in three main aspects: (1) capturing 3D body and dexterous hand manipulation motion alongside 3D object movement within a contextual home environment during natural activities; (2) encompassing human interaction with multiple objects in various episodic scenarios with corresponding descriptions in texts; (3) including articulated objects with multiple parts expressed with parameterized articulations.","Building upon our dataset, we introduce new research tasks aimed at building a generative model for learning and synthesizing human-object interactions in a real-world room setting."],"url":"http://arxiv.org/abs/2401.10232v1"}
{"created":"2024-01-18 18:59:11","title":"ChatQA: Building GPT-4 Level Conversational QA Models","abstract":"In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.","sentences":["In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies.","Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs).","To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost.","Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models."],"url":"http://arxiv.org/abs/2401.10225v1"}
{"created":"2024-01-18 18:58:49","title":"AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data","abstract":"Foundation models encode rich representations that can be adapted to a desired task by fine-tuning on task-specific data. However, fine-tuning a model on one particular data distribution often compromises the model's original performance on other distributions. Current methods for robust fine-tuning utilize hand-crafted regularization techniques to constrain the fine-tuning process towards the base foundation model. Yet, it is hard to precisely specify what characteristics of the foundation model to retain during fine-tuning, as this depends on how the pre-training, fine-tuning, and evaluation data distributions relate to each other. We propose AutoFT, a data-driven approach for guiding foundation model fine-tuning. AutoFT optimizes fine-tuning hyperparameters to maximize performance on a small out-of-distribution (OOD) validation set. To guide fine-tuning in a granular way, AutoFT searches a highly expressive hyperparameter space that includes weight coefficients for many different losses, in addition to learning rate and weight decay values. We evaluate AutoFT on nine natural distribution shifts which include domain shifts and subpopulation shifts. Our experiments show that AutoFT significantly improves generalization to new OOD data, outperforming existing robust fine-tuning methods. Notably, AutoFT achieves new state-of-the-art performance on the WILDS-iWildCam and WILDS-FMoW benchmarks, outperforming the previous best methods by $6.0\\%$ and $1.5\\%$, respectively.","sentences":["Foundation models encode rich representations that can be adapted to a desired task by fine-tuning on task-specific data.","However, fine-tuning a model on one particular data distribution often compromises the model's original performance on other distributions.","Current methods for robust fine-tuning utilize hand-crafted regularization techniques to constrain the fine-tuning process towards the base foundation model.","Yet, it is hard to precisely specify what characteristics of the foundation model to retain during fine-tuning, as this depends on how the pre-training, fine-tuning, and evaluation data distributions relate to each other.","We propose AutoFT, a data-driven approach for guiding foundation model fine-tuning.","AutoFT optimizes fine-tuning hyperparameters to maximize performance on a small out-of-distribution (OOD) validation set.","To guide fine-tuning in a granular way, AutoFT searches a highly expressive hyperparameter space that includes weight coefficients for many different losses, in addition to learning rate and weight decay values.","We evaluate AutoFT on nine natural distribution shifts which include domain shifts and subpopulation shifts.","Our experiments show that AutoFT significantly improves generalization to new OOD data, outperforming existing robust fine-tuning methods.","Notably, AutoFT achieves new state-of-the-art performance on the WILDS-iWildCam and WILDS-FMoW benchmarks, outperforming the previous best methods by $6.0\\%$ and $1.5\\%$, respectively."],"url":"http://arxiv.org/abs/2401.10220v1"}
{"created":"2024-01-18 18:57:10","title":"Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor Products","abstract":"Developing equivariant neural networks for the E(3) group plays an important role in modeling 3D data across real-world applications. Enforcing this equivariance primarily involves the tensor products of irreducible representations (irreps). However, the computational complexity of such operations increases significantly as higher-order tensors are used. In this work, we propose a systematic approach to substantially accelerate the computation of the tensor products of irreps. We mathematically connect the commonly used Clebsch-Gordan coefficients to the Gaunt coefficients, which are integrals of products of three spherical harmonics. Through Gaunt coefficients, the tensor product of irreps becomes equivalent to the multiplication between spherical functions represented by spherical harmonics. This perspective further allows us to change the basis for the equivariant operations from spherical harmonics to a 2D Fourier basis. Consequently, the multiplication between spherical functions represented by a 2D Fourier basis can be efficiently computed via the convolution theorem and Fast Fourier Transforms. This transformation reduces the complexity of full tensor products of irreps from $\\mathcal{O}(L^6)$ to $\\mathcal{O}(L^3)$, where $L$ is the max degree of irreps. Leveraging this approach, we introduce the Gaunt Tensor Product, which serves as a new method to construct efficient equivariant operations across different model architectures. Our experiments on the Open Catalyst Project and 3BPA datasets demonstrate both the increased efficiency and improved performance of our approach.","sentences":["Developing equivariant neural networks for the E(3) group plays an important role in modeling 3D data across real-world applications.","Enforcing this equivariance primarily involves the tensor products of irreducible representations (irreps).","However, the computational complexity of such operations increases significantly as higher-order tensors are used.","In this work, we propose a systematic approach to substantially accelerate the computation of the tensor products of irreps.","We mathematically connect the commonly used Clebsch-Gordan coefficients to the Gaunt coefficients, which are integrals of products of three spherical harmonics.","Through Gaunt coefficients, the tensor product of irreps becomes equivalent to the multiplication between spherical functions represented by spherical harmonics.","This perspective further allows us to change the basis for the equivariant operations from spherical harmonics to a 2D Fourier basis.","Consequently, the multiplication between spherical functions represented by a 2D Fourier basis can be efficiently computed via the convolution theorem and Fast Fourier Transforms.","This transformation reduces the complexity of full tensor products of irreps from $\\mathcal{O}(L^6)$ to $\\mathcal{O}(L^3)$, where $L$ is the max degree of irreps.","Leveraging this approach, we introduce the Gaunt Tensor Product, which serves as a new method to construct efficient equivariant operations across different model architectures.","Our experiments on the Open Catalyst Project and 3BPA datasets demonstrate both the increased efficiency and improved performance of our approach."],"url":"http://arxiv.org/abs/2401.10216v1"}
{"created":"2024-01-18 18:55:27","title":"Tailoring Semantic Communication at Network Edge: A Novel Approach Using Dynamic Knowledge Distillation","abstract":"Semantic Communication (SemCom) systems, empowered by deep learning (DL), represent a paradigm shift in data transmission. These systems prioritize the significance of content over sheer data volume. However, existing SemCom designs face challenges when applied to diverse computational capabilities and network conditions, particularly in time-sensitive applications. A key challenge is the assumption that diverse devices can uniformly benefit from a standard, large DL model in SemCom systems. This assumption becomes increasingly impractical, especially in high-speed, high-reliability applications such as industrial automation or critical healthcare. Therefore, this paper introduces a novel SemCom framework tailored for heterogeneous, resource-constrained edge devices and computation-intensive servers. Our approach employs dynamic knowledge distillation (KD) to customize semantic models for each device, balancing computational and communication constraints while ensuring Quality of Service (QoS). We formulate an optimization problem and develop an adaptive algorithm that iteratively refines semantic knowledge on edge devices, resulting in better models tailored to their resource profiles. This algorithm strategically adjusts the granularity of distilled knowledge, enabling devices to maintain high semantic accuracy for precise inference tasks, even under unstable network conditions. Extensive simulations demonstrate that our approach significantly reduces model complexity for edge devices, leading to better semantic extraction and achieving the desired QoS.","sentences":["Semantic Communication (SemCom) systems, empowered by deep learning (DL), represent a paradigm shift in data transmission.","These systems prioritize the significance of content over sheer data volume.","However, existing SemCom designs face challenges when applied to diverse computational capabilities and network conditions, particularly in time-sensitive applications.","A key challenge is the assumption that diverse devices can uniformly benefit from a standard, large DL model in SemCom systems.","This assumption becomes increasingly impractical, especially in high-speed, high-reliability applications such as industrial automation or critical healthcare.","Therefore, this paper introduces a novel SemCom framework tailored for heterogeneous, resource-constrained edge devices and computation-intensive servers.","Our approach employs dynamic knowledge distillation (KD) to customize semantic models for each device, balancing computational and communication constraints while ensuring Quality of Service (QoS).","We formulate an optimization problem and develop an adaptive algorithm that iteratively refines semantic knowledge on edge devices, resulting in better models tailored to their resource profiles.","This algorithm strategically adjusts the granularity of distilled knowledge, enabling devices to maintain high semantic accuracy for precise inference tasks, even under unstable network conditions.","Extensive simulations demonstrate that our approach significantly reduces model complexity for edge devices, leading to better semantic extraction and achieving the desired QoS."],"url":"http://arxiv.org/abs/2401.10214v1"}
{"created":"2024-01-18 18:50:16","title":"MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer","abstract":"Developing generative models for interleaved image-text data has both research and practical value. It requires models to understand the interleaved sequences and subsequently generate images and text. However, existing attempts are limited by the issue that the fixed number of visual tokens cannot efficiently capture image details, which is particularly problematic in the multi-image scenarios. To address this, this paper presents MM-Interleaved, an end-to-end generative model for interleaved image-text data. It introduces a multi-scale and multi-image feature synchronizer module, allowing direct access to fine-grained image features in the previous context during the generation process. MM-Interleaved is end-to-end pre-trained on both paired and interleaved image-text corpora. It is further enhanced through a supervised fine-tuning phase, wherein the model improves its ability to follow complex multi-modal instructions. Experiments demonstrate the versatility of MM-Interleaved in recognizing visual details following multi-modal instructions and generating consistent images following both textual and visual conditions. Code and models are available at \\url{https://github.com/OpenGVLab/MM-Interleaved}.","sentences":["Developing generative models for interleaved image-text data has both research and practical value.","It requires models to understand the interleaved sequences and subsequently generate images and text.","However, existing attempts are limited by the issue that the fixed number of visual tokens cannot efficiently capture image details, which is particularly problematic in the multi-image scenarios.","To address this, this paper presents MM-Interleaved, an end-to-end generative model for interleaved image-text data.","It introduces a multi-scale and multi-image feature synchronizer module, allowing direct access to fine-grained image features in the previous context during the generation process.","MM-Interleaved is end-to-end pre-trained on both paired and interleaved image-text corpora.","It is further enhanced through a supervised fine-tuning phase, wherein the model improves its ability to follow complex multi-modal instructions.","Experiments demonstrate the versatility of MM-Interleaved in recognizing visual details following multi-modal instructions and generating consistent images following both textual and visual conditions.","Code and models are available at \\url{https://github.com/OpenGVLab/MM-Interleaved}."],"url":"http://arxiv.org/abs/2401.10208v1"}
{"created":"2024-01-18 18:25:29","title":"Divide and not forget: Ensemble of selectively trained experts in Continual Learning","abstract":"Class-incremental learning is becoming more popular as it helps models widen their applicability while not forgetting what they already know. A trend in this area is to use a mixture-of-expert technique, where different models work together to solve the task. However, the experts are usually trained all at once using whole task data, which makes them all prone to forgetting and increasing computational burden. To address this limitation, we introduce a novel approach named SEED. SEED selects only one, the most optimal expert for a considered task, and uses data from this task to fine-tune only this expert. For this purpose, each expert represents each class with a Gaussian distribution, and the optimal expert is selected based on the similarity of those distributions. Consequently, SEED increases diversity and heterogeneity within the experts while maintaining the high stability of this ensemble method. The extensive experiments demonstrate that SEED achieves state-of-the-art performance in exemplar-free settings across various scenarios, showing the potential of expert diversification through data in continual learning.","sentences":["Class-incremental learning is becoming more popular as it helps models widen their applicability while not forgetting what they already know.","A trend in this area is to use a mixture-of-expert technique, where different models work together to solve the task.","However, the experts are usually trained all at once using whole task data, which makes them all prone to forgetting and increasing computational burden.","To address this limitation, we introduce a novel approach named SEED.","SEED selects only one, the most optimal expert for a considered task, and uses data from this task to fine-tune only this expert.","For this purpose, each expert represents each class with a Gaussian distribution, and the optimal expert is selected based on the similarity of those distributions.","Consequently, SEED increases diversity and heterogeneity within the experts while maintaining the high stability of this ensemble method.","The extensive experiments demonstrate that SEED achieves state-of-the-art performance in exemplar-free settings across various scenarios, showing the potential of expert diversification through data in continual learning."],"url":"http://arxiv.org/abs/2401.10191v1"}
{"created":"2024-01-18 18:15:46","title":"Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on Data-to-Text Generation","abstract":"We investigate to which extent open large language models (LLMs) can generate coherent and relevant text from structured data. To prevent bias from benchmarks leaked into LLM training data, we collect Quintd-1: an ad-hoc benchmark for five data-to-text (D2T) generation tasks, consisting of structured data records in standard formats gathered from public APIs. We leverage reference-free evaluation metrics and LLMs' in-context learning capabilities, allowing us to test the models with no human-written references. Our evaluation focuses on annotating semantic accuracy errors on token-level, combining human annotators and a metric based on GPT-4. Our systematic examination of the models' behavior across domains and tasks suggests that state-of-the-art open LLMs with 7B parameters can generate fluent and coherent text from various standard data formats in zero-shot settings. However, we also show that semantic accuracy of the outputs remains a major issue: on our benchmark, 80% of outputs of open LLMs contain a semantic error according to human annotators (91% according to GPT-4). Our code, data, and model outputs are available at https://d2t-llm.github.io.","sentences":["We investigate to which extent open large language models (LLMs) can generate coherent and relevant text from structured data.","To prevent bias from benchmarks leaked into LLM training data, we collect Quintd-1: an ad-hoc benchmark for five data-to-text (D2T) generation tasks, consisting of structured data records in standard formats gathered from public APIs.","We leverage reference-free evaluation metrics and LLMs' in-context learning capabilities, allowing us to test the models with no human-written references.","Our evaluation focuses on annotating semantic accuracy errors on token-level, combining human annotators and a metric based on GPT-4.","Our systematic examination of the models' behavior across domains and tasks suggests that state-of-the-art open LLMs with 7B parameters can generate fluent and coherent text from various standard data formats in zero-shot settings.","However, we also show that semantic accuracy of the outputs remains a major issue: on our benchmark, 80% of outputs of open LLMs contain a semantic error according to human annotators (91% according to GPT-4).","Our code, data, and model outputs are available at https://d2t-llm.github.io."],"url":"http://arxiv.org/abs/2401.10186v1"}
{"created":"2024-01-18 18:12:35","title":"Transfer Learning in Human Activity Recognition: A Survey","abstract":"Sensor-based human activity recognition (HAR) has been an active research area, owing to its applications in smart environments, assisted living, fitness, healthcare, etc. Recently, deep learning based end-to-end training has resulted in state-of-the-art performance in domains such as computer vision and natural language, where large amounts of annotated data are available. However, large quantities of annotated data are not available for sensor-based HAR. Moreover, the real-world settings on which the HAR is performed differ in terms of sensor modalities, classification tasks, and target users. To address this problem, transfer learning has been employed extensively. In this survey, we focus on these transfer learning methods in the application domains of smart home and wearables-based HAR. In particular, we provide a problem-solution perspective by categorizing and presenting the works in terms of their contributions and the challenges they address. We also present an updated view of the state-of-the-art for both application domains. Based on our analysis of 205 papers, we highlight the gaps in the literature and provide a roadmap for addressing them. This survey provides a reference to the HAR community, by summarizing the existing works and providing a promising research agenda.","sentences":["Sensor-based human activity recognition (HAR) has been an active research area, owing to its applications in smart environments, assisted living, fitness, healthcare, etc.","Recently, deep learning based end-to-end training has resulted in state-of-the-art performance in domains such as computer vision and natural language, where large amounts of annotated data are available.","However, large quantities of annotated data are not available for sensor-based HAR.","Moreover, the real-world settings on which the HAR is performed differ in terms of sensor modalities, classification tasks, and target users.","To address this problem, transfer learning has been employed extensively.","In this survey, we focus on these transfer learning methods in the application domains of smart home and wearables-based HAR.","In particular, we provide a problem-solution perspective by categorizing and presenting the works in terms of their contributions and the challenges they address.","We also present an updated view of the state-of-the-art for both application domains.","Based on our analysis of 205 papers, we highlight the gaps in the literature and provide a roadmap for addressing them.","This survey provides a reference to the HAR community, by summarizing the existing works and providing a promising research agenda."],"url":"http://arxiv.org/abs/2401.10185v1"}
{"created":"2024-01-18 18:05:35","title":"Comprehensive OOD Detection Improvements","abstract":"As machine learning becomes increasingly prevalent in impactful decisions, recognizing when inference data is outside the model's expected input distribution is paramount for giving context to predictions. Out-of-distribution (OOD) detection methods have been created for this task. Such methods can be split into representation-based or logit-based methods from whether they respectively utilize the model's embeddings or predictions for OOD detection. In contrast to most papers which solely focus on one such group, we address both. We employ dimensionality reduction on feature embeddings in representation-based methods for both time speedups and improved performance. Additionally, we propose DICE-COL, a modification of the popular logit-based method Directed Sparsification (DICE) that resolves an unnoticed flaw. We demonstrate the effectiveness of our methods on the OpenOODv1.5 benchmark framework, where they significantly improve performance and set state-of-the-art results.","sentences":["As machine learning becomes increasingly prevalent in impactful decisions, recognizing when inference data is outside the model's expected input distribution is paramount for giving context to predictions.","Out-of-distribution (OOD) detection methods have been created for this task.","Such methods can be split into representation-based or logit-based methods from whether they respectively utilize the model's embeddings or predictions for OOD detection.","In contrast to most papers which solely focus on one such group, we address both.","We employ dimensionality reduction on feature embeddings in representation-based methods for both time speedups and improved performance.","Additionally, we propose DICE-COL, a modification of the popular logit-based method Directed Sparsification (DICE) that resolves an unnoticed flaw.","We demonstrate the effectiveness of our methods on the OpenOODv1.5 benchmark framework, where they significantly improve performance and set state-of-the-art results."],"url":"http://arxiv.org/abs/2401.10176v1"}
{"created":"2024-01-18 18:03:07","title":"DualTake: Predicting Takeovers across Mobilities for Future Personalized Mobility Services","abstract":"A hybrid society is expected to emerge in the near future, with different mobilities interacting together, including cars, micro-mobilities, pedestrians, and robots. People may utilize multiple types of mobilities in their daily lives. As vehicle automation advances, driver modeling flourishes to provide personalized intelligent services. Thus, modeling drivers across mobilities would pave the road for future society mobility-as-a-service, and it is particularly interesting to predict driver behaviors in newer mobilities with traditional mobility data. In this work, we present takeover prediction on a micro-mobility, with car simulation data.The promising model performance demonstrates the feasibility of driver modeling across mobilities, as the first in the field.","sentences":["A hybrid society is expected to emerge in the near future, with different mobilities interacting together, including cars, micro-mobilities, pedestrians, and robots.","People may utilize multiple types of mobilities in their daily lives.","As vehicle automation advances, driver modeling flourishes to provide personalized intelligent services.","Thus, modeling drivers across mobilities would pave the road for future society mobility-as-a-service, and it is particularly interesting to predict driver behaviors in newer mobilities with traditional mobility data.","In this work, we present takeover prediction on a micro-mobility, with car simulation data.","The promising model performance demonstrates the feasibility of driver modeling across mobilities, as the first in the field."],"url":"http://arxiv.org/abs/2401.10175v1"}
{"created":"2024-01-18 17:22:11","title":"Explicitly Disentangled Representations in Object-Centric Learning","abstract":"Extracting structured representations from raw visual data is an important and long-standing challenge in machine learning. Recently, techniques for unsupervised learning of object-centric representations have raised growing interest. In this context, enhancing the robustness of the latent features can improve the efficiency and effectiveness of the training of downstream tasks. A promising step in this direction is to disentangle the factors that cause variation in the data. Previously, Invariant Slot Attention disentangled position, scale, and orientation from the remaining features. Extending this approach, we focus on separating the shape and texture components. In particular, we propose a novel architecture that biases object-centric models toward disentangling shape and texture components into two non-overlapping subsets of the latent space dimensions. These subsets are known a priori, hence before the training process. Experiments on a range of object-centric benchmarks reveal that our approach achieves the desired disentanglement while also numerically improving baseline performance in most cases. In addition, we show that our method can generate novel textures for a specific object or transfer textures between objects with distinct shapes.","sentences":["Extracting structured representations from raw visual data is an important and long-standing challenge in machine learning.","Recently, techniques for unsupervised learning of object-centric representations have raised growing interest.","In this context, enhancing the robustness of the latent features can improve the efficiency and effectiveness of the training of downstream tasks.","A promising step in this direction is to disentangle the factors that cause variation in the data.","Previously, Invariant Slot Attention disentangled position, scale, and orientation from the remaining features.","Extending this approach, we focus on separating the shape and texture components.","In particular, we propose a novel architecture that biases object-centric models toward disentangling shape and texture components into two non-overlapping subsets of the latent space dimensions.","These subsets are known a priori, hence before the training process.","Experiments on a range of object-centric benchmarks reveal that our approach achieves the desired disentanglement while also numerically improving baseline performance in most cases.","In addition, we show that our method can generate novel textures for a specific object or transfer textures between objects with distinct shapes."],"url":"http://arxiv.org/abs/2401.10148v1"}
{"created":"2024-01-18 17:05:10","title":"The Role of Data Filtering in Open Source Software Ranking and Selection","abstract":"Faced with over 100M open source projects most empirical investigations select a subset. Most research papers in leading venues investigated filtering projects by some measure of popularity with explicit or implicit arguments that unpopular projects are not of interest, may not even represent \"real\" software projects, or that less popular projects are not worthy of study. However, such filtering may have enormous effects on the results of the studies if and precisely because the sought-out response or prediction is in any way related to the filtering criteria.   We exemplify the impact of this practice on research outcomes: how filtering of projects listed on GitHub affects the assessment of their popularity. We randomly sample over 100,000 repositories and use multiple regression to model the number of stars (a proxy for popularity) based on the number of commits, the duration of the project, the number of authors, and the number of core developers. Comparing control with the entire dataset with a filtered model projects having ten or more authors we find that while certain characteristics of the repository consistently predict popularity, the filtering process significantly alters the relation ships between these characteristics and the response. The number of commits exhibited a positive correlation with popularity in the control sample but showed a negative correlation in the filtered sample. These findings highlight the potential biases introduced by data filtering and emphasize the need for careful sample selection in empirical research of mining software repositories. We recommend that empirical work should either analyze complete datasets such as World of Code, or employ stratified random sampling from a complete dataset to ensure that filtering is not biasing the results.","sentences":["Faced with over 100M open source projects most empirical investigations select a subset.","Most research papers in leading venues investigated filtering projects by some measure of popularity with explicit or implicit arguments that unpopular projects are not of interest, may not even represent \"real\" software projects, or that less popular projects are not worthy of study.","However, such filtering may have enormous effects on the results of the studies if and precisely because the sought-out response or prediction is in any way related to the filtering criteria.   ","We exemplify the impact of this practice on research outcomes: how filtering of projects listed on GitHub affects the assessment of their popularity.","We randomly sample over 100,000 repositories and use multiple regression to model the number of stars (a proxy for popularity) based on the number of commits, the duration of the project, the number of authors, and the number of core developers.","Comparing control with the entire dataset with a filtered model projects having ten or more authors we find that while certain characteristics of the repository consistently predict popularity, the filtering process significantly alters the relation ships between these characteristics and the response.","The number of commits exhibited a positive correlation with popularity in the control sample but showed a negative correlation in the filtered sample.","These findings highlight the potential biases introduced by data filtering and emphasize the need for careful sample selection in empirical research of mining software repositories.","We recommend that empirical work should either analyze complete datasets such as World of Code, or employ stratified random sampling from a complete dataset to ensure that filtering is not biasing the results."],"url":"http://arxiv.org/abs/2401.10136v1"}
{"created":"2024-01-18 17:03:59","title":"Spatial-Temporal Large Language Model for Traffic Prediction","abstract":"Traffic prediction, a critical component for intelligent transportation systems, endeavors to foresee future traffic at specific locations using historical data. Although existing traffic prediction models often emphasize developing complex neural network structures, their accuracy has not seen improvements accordingly. Recently, Large Language Models (LLMs) have shown outstanding capabilities in time series analysis. Differing from existing models, LLMs progress mainly through parameter expansion and extensive pre-training while maintaining their fundamental structures. In this paper, we propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic prediction. Specifically, ST-LLM redefines the timesteps at each location as tokens and incorporates a spatial-temporal embedding module to learn the spatial location and global temporal representations of tokens. Then these representations are fused to provide each token with unified spatial and temporal information. Furthermore, we propose a novel partially frozen attention strategy of the LLM, which is designed to capture spatial-temporal dependencies for traffic prediction. Comprehensive experiments on real traffic datasets offer evidence that ST-LLM outperforms state-of-the-art models. Notably, the ST-LLM also exhibits robust performance in both few-shot and zero-shot prediction scenarios.","sentences":["Traffic prediction, a critical component for intelligent transportation systems, endeavors to foresee future traffic at specific locations using historical data.","Although existing traffic prediction models often emphasize developing complex neural network structures, their accuracy has not seen improvements accordingly.","Recently, Large Language Models (LLMs) have shown outstanding capabilities in time series analysis.","Differing from existing models, LLMs progress mainly through parameter expansion and extensive pre-training while maintaining their fundamental structures.","In this paper, we propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic prediction.","Specifically, ST-LLM redefines the timesteps at each location as tokens and incorporates a spatial-temporal embedding module to learn the spatial location and global temporal representations of tokens.","Then these representations are fused to provide each token with unified spatial and temporal information.","Furthermore, we propose a novel partially frozen attention strategy of the LLM, which is designed to capture spatial-temporal dependencies for traffic prediction.","Comprehensive experiments on real traffic datasets offer evidence that ST-LLM outperforms state-of-the-art models.","Notably, the ST-LLM also exhibits robust performance in both few-shot and zero-shot prediction scenarios."],"url":"http://arxiv.org/abs/2401.10134v1"}
{"created":"2024-01-18 16:47:17","title":"Techniques for Authenticating Quantile Digests","abstract":"We investigate two possible techniques to authenticate the q-digest data structure, along with a worst-case study of the computational complexity both in time and space of the proposed solutions, and considerations on the feasibility of the presented approaches in real-world scenarios. We conclude the discussion by presenting some considerations on the information complexity of the queries in the two proposed approaches, and by presenting some interesting ideas that could be the subject of future studies on the topic.","sentences":["We investigate two possible techniques to authenticate the q-digest data structure, along with a worst-case study of the computational complexity both in time and space of the proposed solutions, and considerations on the feasibility of the presented approaches in real-world scenarios.","We conclude the discussion by presenting some considerations on the information complexity of the queries in the two proposed approaches, and by presenting some interesting ideas that could be the subject of future studies on the topic."],"url":"http://arxiv.org/abs/2401.10118v1"}
{"created":"2024-01-18 16:27:18","title":"Marrying Adapters and Mixup to Efficiently Enhance the Adversarial Robustness of Pre-Trained Language Models for Text Classification","abstract":"Existing works show that augmenting training data of neural networks using both clean and adversarial examples can enhance their generalizability under adversarial attacks. However, this training approach often leads to performance degradation on clean inputs. Additionally, it requires frequent re-training of the entire model to account for new attack types, resulting in significant and costly computations. Such limitations make adversarial training mechanisms less practical, particularly for complex Pre-trained Language Models (PLMs) with millions or even billions of parameters. To overcome these challenges while still harnessing the theoretical benefits of adversarial training, this study combines two concepts: (1) adapters, which enable parameter-efficient fine-tuning, and (2) Mixup, which train NNs via convex combinations of pairs data pairs. Intuitively, we propose to fine-tune PLMs through convex combinations of non-data pairs of fine-tuned adapters, one trained with clean and another trained with adversarial examples. Our experiments show that the proposed method achieves the best trade-off between training efficiency and predictive performance, both with and without attacks compared to other baselines on a variety of downstream tasks.","sentences":["Existing works show that augmenting training data of neural networks using both clean and adversarial examples can enhance their generalizability under adversarial attacks.","However, this training approach often leads to performance degradation on clean inputs.","Additionally, it requires frequent re-training of the entire model to account for new attack types, resulting in significant and costly computations.","Such limitations make adversarial training mechanisms less practical, particularly for complex Pre-trained Language Models (PLMs) with millions or even billions of parameters.","To overcome these challenges while still harnessing the theoretical benefits of adversarial training, this study combines two concepts: (1) adapters, which enable parameter-efficient fine-tuning, and (2) Mixup, which train NNs via convex combinations of pairs data pairs.","Intuitively, we propose to fine-tune PLMs through convex combinations of non-data pairs of fine-tuned adapters, one trained with clean and another trained with adversarial examples.","Our experiments show that the proposed method achieves the best trade-off between training efficiency and predictive performance, both with and without attacks compared to other baselines on a variety of downstream tasks."],"url":"http://arxiv.org/abs/2401.10111v1"}
{"created":"2024-01-18 16:10:07","title":"Counterfactual Reasoning with Probabilistic Graphical Models for Analyzing Socioecological Systems","abstract":"Causal and counterfactual reasoning are emerging directions in data science that allow us to reason about hypothetical scenarios. This is particularly useful in domains where experimental data are usually not available. In the context of environmental and ecological sciences, causality enables us, for example, to predict how an ecosystem would respond to hypothetical interventions. A structural causal model is a class of probabilistic graphical models for causality, which, due to its intuitive nature, can be easily understood by experts in multiple fields. However, certain queries, called unidentifiable, cannot be calculated in an exact and precise manner. This paper proposes applying a novel and recent technique for bounding unidentifiable queries within the domain of socioecological systems. Our findings indicate that traditional statistical analysis, including probabilistic graphical models, can identify the influence between variables. However, such methods do not offer insights into the nature of the relationship, specifically whether it involves necessity or sufficiency. This is where counterfactual reasoning becomes valuable.","sentences":["Causal and counterfactual reasoning are emerging directions in data science that allow us to reason about hypothetical scenarios.","This is particularly useful in domains where experimental data are usually not available.","In the context of environmental and ecological sciences, causality enables us, for example, to predict how an ecosystem would respond to hypothetical interventions.","A structural causal model is a class of probabilistic graphical models for causality, which, due to its intuitive nature, can be easily understood by experts in multiple fields.","However, certain queries, called unidentifiable, cannot be calculated in an exact and precise manner.","This paper proposes applying a novel and recent technique for bounding unidentifiable queries within the domain of socioecological systems.","Our findings indicate that traditional statistical analysis, including probabilistic graphical models, can identify the influence between variables.","However, such methods do not offer insights into the nature of the relationship, specifically whether it involves necessity or sufficiency.","This is where counterfactual reasoning becomes valuable."],"url":"http://arxiv.org/abs/2401.10101v1"}
{"created":"2024-01-18 15:56:23","title":"Cross-Modality Perturbation Synergy Attack for Person Re-identification","abstract":"In recent years, there has been significant research focusing on addressing security concerns in single-modal person re-identification (ReID) systems that are based on RGB images. However, the safety of cross-modality scenarios, which are more commonly encountered in practical applications involving images captured by infrared cameras, has not received adequate attention. The main challenge in cross-modality ReID lies in effectively dealing with visual differences between different modalities. For instance, infrared images are typically grayscale, unlike visible images that contain color information. Existing attack methods have primarily focused on the characteristics of the visible image modality, overlooking the features of other modalities and the variations in data distribution among different modalities. This oversight can potentially undermine the effectiveness of these methods in image retrieval across diverse modalities. This study represents the first exploration into the security of cross-modality ReID models and proposes a universal perturbation attack specifically designed for cross-modality ReID. This attack optimizes perturbations by leveraging gradients from diverse modality data, thereby disrupting the discriminator and reinforcing the differences between modalities. We conducted experiments on two widely used cross-modality datasets, namely RegDB and SYSU, which not only demonstrated the effectiveness of our method but also provided insights for future enhancements in the robustness of cross-modality ReID systems.","sentences":["In recent years, there has been significant research focusing on addressing security concerns in single-modal person re-identification (ReID) systems that are based on RGB images.","However, the safety of cross-modality scenarios, which are more commonly encountered in practical applications involving images captured by infrared cameras, has not received adequate attention.","The main challenge in cross-modality ReID lies in effectively dealing with visual differences between different modalities.","For instance, infrared images are typically grayscale, unlike visible images that contain color information.","Existing attack methods have primarily focused on the characteristics of the visible image modality, overlooking the features of other modalities and the variations in data distribution among different modalities.","This oversight can potentially undermine the effectiveness of these methods in image retrieval across diverse modalities.","This study represents the first exploration into the security of cross-modality ReID models and proposes a universal perturbation attack specifically designed for cross-modality ReID.","This attack optimizes perturbations by leveraging gradients from diverse modality data, thereby disrupting the discriminator and reinforcing the differences between modalities.","We conducted experiments on two widely used cross-modality datasets, namely RegDB and SYSU, which not only demonstrated the effectiveness of our method but also provided insights for future enhancements in the robustness of cross-modality ReID systems."],"url":"http://arxiv.org/abs/2401.10090v1"}
{"created":"2024-01-18 15:39:38","title":"Communication-Efficient Personalized Federated Learning for Speech-to-Text Tasks","abstract":"To protect privacy and meet legal regulations, federated learning (FL) has gained significant attention for training speech-to-text (S2T) systems, including automatic speech recognition (ASR) and speech translation (ST). However, the commonly used FL approach (i.e., \\textsc{FedAvg}) in S2T tasks typically suffers from extensive communication overhead due to multi-round interactions based on the whole model and performance degradation caused by data heterogeneity among clients.To address these issues, we propose a personalized federated S2T framework that introduces \\textsc{FedLoRA}, a lightweight LoRA module for client-side tuning and interaction with the server to minimize communication overhead, and \\textsc{FedMem}, a global model equipped with a $k$-nearest-neighbor ($k$NN) classifier that captures client-specific distributional shifts to achieve personalization and overcome data heterogeneity. Extensive experiments based on Conformer and Whisper backbone models on CoVoST and GigaSpeech benchmarks show that our approach significantly reduces the communication overhead on all S2T tasks and effectively personalizes the global model to overcome data heterogeneity.","sentences":["To protect privacy and meet legal regulations, federated learning (FL) has gained significant attention for training speech-to-text (S2T) systems, including automatic speech recognition (ASR) and speech translation (ST).","However, the commonly used FL approach (i.e., \\textsc{FedAvg}) in S2T tasks typically suffers from extensive communication overhead due to multi-round interactions based on the whole model and performance degradation caused by data heterogeneity among clients.","To address these issues, we propose a personalized federated S2T framework that introduces \\textsc{FedLoRA}, a lightweight LoRA module for client-side tuning and interaction with the server to minimize communication overhead, and \\textsc{FedMem}, a global model equipped with a $k$-nearest-neighbor ($k$NN) classifier that captures client-specific distributional shifts to achieve personalization and overcome data heterogeneity.","Extensive experiments based on Conformer and Whisper backbone models on CoVoST and GigaSpeech benchmarks show that our approach significantly reduces the communication overhead on all S2T tasks and effectively personalizes the global model to overcome data heterogeneity."],"url":"http://arxiv.org/abs/2401.10070v1"}
{"created":"2024-01-18 15:33:50","title":"GPU Acceleration of a Conjugate Exponential Model for Cancer Tissue Heterogeneity","abstract":"Heterogeneity in the cell population of cancer tissues poses many challenges in cancer diagnosis and treatment. Studying the heterogeneity in cell populations from gene expression measurement data in the context of cancer research is a problem of paramount importance. In addition, reducing the computation time of the algorithms that deal with high volumes of data has its obvious merits. Parallelizable models using Markov chain Monte Carlo methods are typically slow. This paper shows a novel, computationally efficient, and parallelizable model to analyze heterogeneity in cancer tissues using GPUs. Because our model is parallelizable, the input data size does not affect the computation time much, provided the hardware resources are not exhausted. Our model uses qPCR (quantitative polymerase chain reaction) gene expression measurements to study heterogeneity in cancer tissue. We compute the cell proportion breakup by accelerating variational methods on a GPU. We test this model on synthetic and real-world gene expression data collected from fibroblasts and compare the performance of our algorithm with those of MCMC and Expectation Maximization. Our new model is computationally less complex and faster than existing Bayesian models for cancer tissue heterogeneity.","sentences":["Heterogeneity in the cell population of cancer tissues poses many challenges in cancer diagnosis and treatment.","Studying the heterogeneity in cell populations from gene expression measurement data in the context of cancer research is a problem of paramount importance.","In addition, reducing the computation time of the algorithms that deal with high volumes of data has its obvious merits.","Parallelizable models using Markov chain Monte Carlo methods are typically slow.","This paper shows a novel, computationally efficient, and parallelizable model to analyze heterogeneity in cancer tissues using GPUs.","Because our model is parallelizable, the input data size does not affect the computation time much, provided the hardware resources are not exhausted.","Our model uses qPCR (quantitative polymerase chain reaction) gene expression measurements to study heterogeneity in cancer tissue.","We compute the cell proportion breakup by accelerating variational methods on a GPU.","We test this model on synthetic and real-world gene expression data collected from fibroblasts and compare the performance of our algorithm with those of MCMC and Expectation Maximization.","Our new model is computationally less complex and faster than existing Bayesian models for cancer tissue heterogeneity."],"url":"http://arxiv.org/abs/2401.10068v1"}
{"created":"2024-01-18 15:15:32","title":"ContextMix: A context-aware data augmentation method for industrial visual inspection systems","abstract":"While deep neural networks have achieved remarkable performance, data augmentation has emerged as a crucial strategy to mitigate overfitting and enhance network performance. These techniques hold particular significance in industrial manufacturing contexts. Recently, image mixing-based methods have been introduced, exhibiting improved performance on public benchmark datasets. However, their application to industrial tasks remains challenging. The manufacturing environment generates massive amounts of unlabeled data on a daily basis, with only a few instances of abnormal data occurrences. This leads to severe data imbalance. Thus, creating well-balanced datasets is not straightforward due to the high costs associated with labeling. Nonetheless, this is a crucial step for enhancing productivity. For this reason, we introduce ContextMix, a method tailored for industrial applications and benchmark datasets. ContextMix generates novel data by resizing entire images and integrating them into other images within the batch. This approach enables our method to learn discriminative features based on varying sizes from resized images and train informative secondary features for object recognition using occluded images. With the minimal additional computation cost of image resizing, ContextMix enhances performance compared to existing augmentation techniques. We evaluate its effectiveness across classification, detection, and segmentation tasks using various network architectures on public benchmark datasets. Our proposed method demonstrates improved results across a range of robustness tasks. Its efficacy in real industrial environments is particularly noteworthy, as demonstrated using the passive component dataset.","sentences":["While deep neural networks have achieved remarkable performance, data augmentation has emerged as a crucial strategy to mitigate overfitting and enhance network performance.","These techniques hold particular significance in industrial manufacturing contexts.","Recently, image mixing-based methods have been introduced, exhibiting improved performance on public benchmark datasets.","However, their application to industrial tasks remains challenging.","The manufacturing environment generates massive amounts of unlabeled data on a daily basis, with only a few instances of abnormal data occurrences.","This leads to severe data imbalance.","Thus, creating well-balanced datasets is not straightforward due to the high costs associated with labeling.","Nonetheless, this is a crucial step for enhancing productivity.","For this reason, we introduce ContextMix, a method tailored for industrial applications and benchmark datasets.","ContextMix generates novel data by resizing entire images and integrating them into other images within the batch.","This approach enables our method to learn discriminative features based on varying sizes from resized images and train informative secondary features for object recognition using occluded images.","With the minimal additional computation cost of image resizing, ContextMix enhances performance compared to existing augmentation techniques.","We evaluate its effectiveness across classification, detection, and segmentation tasks using various network architectures on public benchmark datasets.","Our proposed method demonstrates improved results across a range of robustness tasks.","Its efficacy in real industrial environments is particularly noteworthy, as demonstrated using the passive component dataset."],"url":"http://arxiv.org/abs/2401.10050v1"}
{"created":"2024-01-18 15:00:28","title":"Depth Over RGB: Automatic Evaluation of Open Surgery Skills Using Depth Camera","abstract":"Purpose: In this paper, we present a novel approach to the automatic evaluation of open surgery skills using depth cameras. This work is intended to show that depth cameras achieve similar results to RGB cameras, which is the common method in the automatic evaluation of open surgery skills. Moreover, depth cameras offer advantages such as robustness to lighting variations, camera positioning, simplified data compression, and enhanced privacy, making them a promising alternative to RGB cameras.   Methods: Experts and novice surgeons completed two simulators of open suturing. We focused on hand and tool detection, and action segmentation in suturing procedures. YOLOv8 was used for tool detection in RGB and depth videos. Furthermore, UVAST and MSTCN++ were used for action segmentation. Our study includes the collection and annotation of a dataset recorded with Azure Kinect.   Results: We demonstrated that using depth cameras in object detection and action segmentation achieves comparable results to RGB cameras. Furthermore, we analyzed 3D hand path length, revealing significant differences between experts and novice surgeons, emphasizing the potential of depth cameras in capturing surgical skills. We also investigated the influence of camera angles on measurement accuracy, highlighting the advantages of 3D cameras in providing a more accurate representation of hand movements.   Conclusion: Our research contributes to advancing the field of surgical skill assessment by leveraging depth cameras for more reliable and privacy evaluations. The findings suggest that depth cameras can be valuable in assessing surgical skills and provide a foundation for future research in this area.","sentences":["Purpose: In this paper, we present a novel approach to the automatic evaluation of open surgery skills using depth cameras.","This work is intended to show that depth cameras achieve similar results to RGB cameras, which is the common method in the automatic evaluation of open surgery skills.","Moreover, depth cameras offer advantages such as robustness to lighting variations, camera positioning, simplified data compression, and enhanced privacy, making them a promising alternative to RGB cameras.   ","Methods: Experts and novice surgeons completed two simulators of open suturing.","We focused on hand and tool detection, and action segmentation in suturing procedures.","YOLOv8 was used for tool detection in RGB and depth videos.","Furthermore, UVAST and MSTCN++ were used for action segmentation.","Our study includes the collection and annotation of a dataset recorded with Azure Kinect.   ","Results:","We demonstrated that using depth cameras in object detection and action segmentation achieves comparable results to RGB cameras.","Furthermore, we analyzed 3D hand path length, revealing significant differences between experts and novice surgeons, emphasizing the potential of depth cameras in capturing surgical skills.","We also investigated the influence of camera angles on measurement accuracy, highlighting the advantages of 3D cameras in providing a more accurate representation of hand movements.   ","Conclusion: Our research contributes to advancing the field of surgical skill assessment by leveraging depth cameras for more reliable and privacy evaluations.","The findings suggest that depth cameras can be valuable in assessing surgical skills and provide a foundation for future research in this area."],"url":"http://arxiv.org/abs/2401.10037v1"}
{"created":"2024-01-18 14:58:17","title":"Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap","abstract":"Large Language Models (LLMs), built upon Transformer-based architectures with massive pretraining on diverse data, have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and Evolutionary Algorithms (EAs), despite differing in objectives and methodologies, reveals intriguing parallels, especially in their shared optimization nature, black-box characteristics, and proficiency in handling complex problems. Meanwhile, EA can not only provide an optimization framework for LLM's further enhancement under black-box settings but also empower LLM with flexible global search and iterative mechanism in applications. On the other hand, LLM's abundant domain knowledge enables EA to perform smarter searches, while its text processing capability assist in deploying EA across various tasks. Based on their complementary advantages, this paper presents a comprehensive review and forward-looking roadmap, categorizing their mutual inspiration into LLM-enhanced evolutionary optimization and EA-enhanced LLM. Some integrated synergy methods are further introduced to exemplify the amalgamation of LLMs and EAs in various application scenarios, including neural architecture search, code generation, software engineering, and text generation. As the first comprehensive review specifically focused on the EA research in the era of LLMs, this paper provides a foundational stepping stone for understanding and harnessing the collaborative potential of LLMs and EAs. By presenting a comprehensive review, categorization, and critical analysis, we contribute to the ongoing discourse on the cross-disciplinary study of these two powerful paradigms. The identified challenges and future directions offer guidance to unlock the full potential of this innovative collaboration.","sentences":["Large Language Models (LLMs), built upon Transformer-based architectures with massive pretraining on diverse data, have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence.","The interplay between LLMs and Evolutionary Algorithms (EAs), despite differing in objectives and methodologies, reveals intriguing parallels, especially in their shared optimization nature, black-box characteristics, and proficiency in handling complex problems.","Meanwhile, EA can not only provide an optimization framework for LLM's further enhancement under black-box settings but also empower LLM with flexible global search and iterative mechanism in applications.","On the other hand, LLM's abundant domain knowledge enables EA to perform smarter searches, while its text processing capability assist in deploying EA across various tasks.","Based on their complementary advantages, this paper presents a comprehensive review and forward-looking roadmap, categorizing their mutual inspiration into LLM-enhanced evolutionary optimization and EA-enhanced LLM.","Some integrated synergy methods are further introduced to exemplify the amalgamation of LLMs and EAs in various application scenarios, including neural architecture search, code generation, software engineering, and text generation.","As the first comprehensive review specifically focused on the EA research in the era of LLMs, this paper provides a foundational stepping stone for understanding and harnessing the collaborative potential of LLMs and EAs.","By presenting a comprehensive review, categorization, and critical analysis, we contribute to the ongoing discourse on the cross-disciplinary study of these two powerful paradigms.","The identified challenges and future directions offer guidance to unlock the full potential of this innovative collaboration."],"url":"http://arxiv.org/abs/2401.10034v1"}
{"created":"2024-01-18 14:55:03","title":"Cardiac Digital Twin Pipeline for Virtual Therapy Evaluation","abstract":"Cardiac digital twins are computational tools capturing key functional and anatomical characteristics of patient hearts for investigating disease phenotypes and predicting responses to therapy. When paired with large-scale computational resources and large clinical datasets, digital twin technology can enable virtual clinical trials on virtual cohorts to fast-track therapy development. Here, we present an automated pipeline for personalising ventricular anatomy and electrophysiological function based on routinely acquired cardiac magnetic resonance (CMR) imaging data and the standard 12-lead electrocardiogram (ECG). Using CMR-based anatomical models, a sequential Monte-Carlo approximate Bayesian computational inference method is extended to infer electrical activation and repolarisation characteristics from the ECG. Fast simulations are conducted with a reaction-Eikonal model, including the Purkinje network and biophysically-detailed subcellular ionic current dynamics for repolarisation. For each patient, parameter uncertainty is represented by inferring a population of ventricular models rather than a single one, which means that parameter uncertainty can be propagated to therapy evaluation. Furthermore, we have developed techniques for translating from reaction-Eikonal to monodomain simulations, which allows more realistic simulations of cardiac electrophysiology. The pipeline is demonstrated in a healthy female subject, where our inferred reaction-Eikonal models reproduced the patient's ECG with a Pearson's correlation coefficient of 0.93, and the translated monodomain simulations have a correlation coefficient of 0.89. We then apply the effect of Dofetilide to the monodomain population of models for this subject and show dose-dependent QT and T-peak to T-end prolongations that are in keeping with large population drug response data.","sentences":["Cardiac digital twins are computational tools capturing key functional and anatomical characteristics of patient hearts for investigating disease phenotypes and predicting responses to therapy.","When paired with large-scale computational resources and large clinical datasets, digital twin technology can enable virtual clinical trials on virtual cohorts to fast-track therapy development.","Here, we present an automated pipeline for personalising ventricular anatomy and electrophysiological function based on routinely acquired cardiac magnetic resonance (CMR) imaging data and the standard 12-lead electrocardiogram (ECG).","Using CMR-based anatomical models, a sequential Monte-Carlo approximate Bayesian computational inference method is extended to infer electrical activation and repolarisation characteristics from the ECG.","Fast simulations are conducted with a reaction-Eikonal model, including the Purkinje network and biophysically-detailed subcellular ionic current dynamics for repolarisation.","For each patient, parameter uncertainty is represented by inferring a population of ventricular models rather than a single one, which means that parameter uncertainty can be propagated to therapy evaluation.","Furthermore, we have developed techniques for translating from reaction-Eikonal to monodomain simulations, which allows more realistic simulations of cardiac electrophysiology.","The pipeline is demonstrated in a healthy female subject, where our inferred reaction-Eikonal models reproduced the patient's ECG with a Pearson's correlation coefficient of 0.93, and the translated monodomain simulations have a correlation coefficient of 0.89.","We then apply the effect of Dofetilide to the monodomain population of models for this subject and show dose-dependent QT and T-peak to T-end prolongations that are in keeping with large population drug response data."],"url":"http://arxiv.org/abs/2401.10029v1"}
{"created":"2024-01-18 14:31:11","title":"Optimizing Medication Decisions for Patients with Atrial Fibrillation through Path Development Network","abstract":"Atrial fibrillation (AF) is a common cardiac arrhythmia characterized by rapid and irregular contractions of the atria. It significantly elevates the risk of strokes due to slowed blood flow in the atria, especially in the left atrial appendage, which is prone to blood clot formation. Such clots can migrate into cerebral arteries, leading to ischemic stroke. To assess whether AF patients should be prescribed anticoagulants, doctors often use the CHA2DS2-VASc scoring system. However, anticoagulant use must be approached with caution as it can impact clotting functions. This study introduces a machine learning algorithm that predicts whether patients with AF should be recommended anticoagulant therapy using 12-lead ECG data. In this model, we use STOME to enhance time-series data and then process it through a Convolutional Neural Network (CNN). By incorporating a path development layer, the model achieves a specificity of 30.6% under the condition of an NPV of 1. In contrast, LSTM algorithms without path development yield a specificity of only 2.7% under the same NPV condition.","sentences":["Atrial fibrillation (AF) is a common cardiac arrhythmia characterized by rapid and irregular contractions of the atria.","It significantly elevates the risk of strokes due to slowed blood flow in the atria, especially in the left atrial appendage, which is prone to blood clot formation.","Such clots can migrate into cerebral arteries, leading to ischemic stroke.","To assess whether AF patients should be prescribed anticoagulants, doctors often use the CHA2DS2-VASc scoring system.","However, anticoagulant use must be approached with caution as it can impact clotting functions.","This study introduces a machine learning algorithm that predicts whether patients with AF should be recommended anticoagulant therapy using 12-lead ECG data.","In this model, we use STOME to enhance time-series data and then process it through a Convolutional Neural Network (CNN).","By incorporating a path development layer, the model achieves a specificity of 30.6% under the condition of an NPV of 1.","In contrast, LSTM algorithms without path development yield a specificity of only 2.7% under the same NPV condition."],"url":"http://arxiv.org/abs/2401.10014v1"}
{"created":"2024-01-18 14:17:40","title":"Distantly Supervised Morpho-Syntactic Model for Relation Extraction","abstract":"The task of Information Extraction (IE) involves automatically converting unstructured textual content into structured data. Most research in this field concentrates on extracting all facts or a specific set of relationships from documents. In this paper, we present a method for the extraction and categorisation of an unrestricted set of relationships from text. Our method relies on morpho-syntactic extraction patterns obtained by a distant supervision method, and creates Syntactic and Semantic Indices to extract and classify candidate graphs. We evaluate our approach on six datasets built on Wikidata and Wikipedia. The evaluation shows that our approach can achieve Precision scores of up to 0.85, but with lower Recall and F1 scores. Our approach allows to quickly create rule-based systems for Information Extraction and to build annotated datasets to train machine-learning and deep-learning based classifiers.","sentences":["The task of Information Extraction (IE) involves automatically converting unstructured textual content into structured data.","Most research in this field concentrates on extracting all facts or a specific set of relationships from documents.","In this paper, we present a method for the extraction and categorisation of an unrestricted set of relationships from text.","Our method relies on morpho-syntactic extraction patterns obtained by a distant supervision method, and creates Syntactic and Semantic Indices to extract and classify candidate graphs.","We evaluate our approach on six datasets built on Wikidata and Wikipedia.","The evaluation shows that our approach can achieve Precision scores of up to 0.85, but with lower Recall and F1 scores.","Our approach allows to quickly create rule-based systems for Information Extraction and to build annotated datasets to train machine-learning and deep-learning based classifiers."],"url":"http://arxiv.org/abs/2401.10002v1"}
{"created":"2024-01-18 14:06:29","title":"Developing an AI-based Integrated System for Bee Health Evaluation","abstract":"Honey bees pollinate about one-third of the world's food supply, but bee colonies have alarmingly declined by nearly 40% over the past decade due to several factors, including pesticides and pests. Traditional methods for monitoring beehives, such as human inspection, are subjective, disruptive, and time-consuming. To overcome these limitations, artificial intelligence has been used to assess beehive health. However, previous studies have lacked an end-to-end solution and primarily relied on data from a single source, either bee images or sounds. This study introduces a comprehensive system consisting of bee object detection and health evaluation. Additionally, it utilized a combination of visual and audio signals to analyze bee behaviors. An Attention-based Multimodal Neural Network (AMNN) was developed to adaptively focus on key features from each type of signal for accurate bee health assessment. The AMNN achieved an overall accuracy of 92.61%, surpassing eight existing single-signal Convolutional Neural Networks and Recurrent Neural Networks. It outperformed the best image-based model by 32.51% and the top sound-based model by 13.98% while maintaining efficient processing times. Furthermore, it improved prediction robustness, attaining an F1-score higher than 90% across all four evaluated health conditions. The study also shows that audio signals are more reliable than images for assessing bee health. By seamlessly integrating AMNN with image and sound data in a comprehensive bee health monitoring system, this approach provides a more efficient and non-invasive solution for the early detection of bee diseases and the preservation of bee colonies.","sentences":["Honey bees pollinate about one-third of the world's food supply, but bee colonies have alarmingly declined by nearly 40% over the past decade due to several factors, including pesticides and pests.","Traditional methods for monitoring beehives, such as human inspection, are subjective, disruptive, and time-consuming.","To overcome these limitations, artificial intelligence has been used to assess beehive health.","However, previous studies have lacked an end-to-end solution and primarily relied on data from a single source, either bee images or sounds.","This study introduces a comprehensive system consisting of bee object detection and health evaluation.","Additionally, it utilized a combination of visual and audio signals to analyze bee behaviors.","An Attention-based Multimodal Neural Network (AMNN) was developed to adaptively focus on key features from each type of signal for accurate bee health assessment.","The AMNN achieved an overall accuracy of 92.61%, surpassing eight existing single-signal Convolutional Neural Networks and Recurrent Neural Networks.","It outperformed the best image-based model by 32.51% and the top sound-based model by 13.98% while maintaining efficient processing times.","Furthermore, it improved prediction robustness, attaining an F1-score higher than 90% across all four evaluated health conditions.","The study also shows that audio signals are more reliable than images for assessing bee health.","By seamlessly integrating AMNN with image and sound data in a comprehensive bee health monitoring system, this approach provides a more efficient and non-invasive solution for the early detection of bee diseases and the preservation of bee colonies."],"url":"http://arxiv.org/abs/2401.09988v1"}
{"created":"2024-01-18 14:04:51","title":"A-KIT: Adaptive Kalman-Informed Transformer","abstract":"The extended Kalman filter (EKF) is a widely adopted method for sensor fusion in navigation applications. A crucial aspect of the EKF is the online determination of the process noise covariance matrix reflecting the model uncertainty. While common EKF implementation assumes a constant process noise, in real-world scenarios, the process noise varies, leading to inaccuracies in the estimated state and potentially causing the filter to diverge. To cope with such situations, model-based adaptive EKF methods were proposed and demonstrated performance improvements, highlighting the need for a robust adaptive approach. In this paper, we derive and introduce A-KIT, an adaptive Kalman-informed transformer to learn the varying process noise covariance online. The A-KIT framework is applicable to any type of sensor fusion. Here, we present our approach to nonlinear sensor fusion based on an inertial navigation system and Doppler velocity log. By employing real recorded data from an autonomous underwater vehicle, we show that A-KIT outperforms the conventional EKF by more than 49.5% and model-based adaptive EKF by an average of 35.4% in terms of position accuracy.","sentences":["The extended Kalman filter (EKF) is a widely adopted method for sensor fusion in navigation applications.","A crucial aspect of the EKF is the online determination of the process noise covariance matrix reflecting the model uncertainty.","While common EKF implementation assumes a constant process noise, in real-world scenarios, the process noise varies, leading to inaccuracies in the estimated state and potentially causing the filter to diverge.","To cope with such situations, model-based adaptive EKF methods were proposed and demonstrated performance improvements, highlighting the need for a robust adaptive approach.","In this paper, we derive and introduce A-KIT, an adaptive Kalman-informed transformer to learn the varying process noise covariance online.","The A-KIT framework is applicable to any type of sensor fusion.","Here, we present our approach to nonlinear sensor fusion based on an inertial navigation system and Doppler velocity log.","By employing real recorded data from an autonomous underwater vehicle, we show that A-KIT outperforms the conventional EKF by more than 49.5% and model-based adaptive EKF by an average of 35.4% in terms of position accuracy."],"url":"http://arxiv.org/abs/2401.09987v1"}
{"created":"2024-01-18 14:02:23","title":"FLex&Chill: Improving Local Federated Learning Training with Logit Chilling","abstract":"Federated learning are inherently hampered by data heterogeneity: non-iid distributed training data over local clients. We propose a novel model training approach for federated learning, FLex&Chill, which exploits the Logit Chilling method. Through extensive evaluations, we demonstrate that, in the presence of non-iid data characteristics inherent in federated learning systems, this approach can expedite model convergence and improve inference accuracy. Quantitatively, from our experiments, we observe up to 6X improvement in the global federated learning model convergence time, and up to 3.37% improvement in inference accuracy.","sentences":["Federated learning are inherently hampered by data heterogeneity: non-iid distributed training data over local clients.","We propose a novel model training approach for federated learning, FLex&Chill, which exploits the Logit Chilling method.","Through extensive evaluations, we demonstrate that, in the presence of non-iid data characteristics inherent in federated learning systems, this approach can expedite model convergence and improve inference accuracy.","Quantitatively, from our experiments, we observe up to 6X improvement in the global federated learning model convergence time, and up to 3.37% improvement in inference accuracy."],"url":"http://arxiv.org/abs/2401.09986v1"}
{"created":"2024-01-18 13:45:55","title":"Material-Response-Informed DeepONet and its Application to Polycrystal Stress-strain Prediction in Crystal Plasticity","abstract":"Crystal plasticity (CP) simulations are a tool for understanding how microstructure morphology and texture affect mechanical properties and are an essential component of elucidating the structure-property relations. However, it can be computationally expensive. Hence, data-driven machine learning models have been applied to predict the mean-field response of a polycrystal representative volume element to reduce computation time. In this work, we proposed a novel Deep Operator Network (DeepONet) architecture for predicting microstructure stress-strain response. It employs a convolutional neural network in the trunk to encode the microstructure. To account for different material properties, boundary conditions, and loading, we proposed using single crystal stress-strain curves as inputs to the branch network, furnishing a material-response-informed DeepONet. Using four numerical examples, we demonstrate that the current DeepONet can be trained on a single material and loading and then generalized to new conditions via transfer learning. Results show that using single crystal responses as input outperforms a similar model using material properties as inputs and overcomes limitations with changing boundary conditions and temporal resolution. In all cases, the new model achieved a $R^2$ value of above 0.99, and over 95\\% of predicted stresses have a relative error of $\\le$ 5\\%, indicating superior accuracy. With as few as 20 new data points and under 1min training time, the trained DeepONet can be fine-tuned to generate accurate predictions on different materials and loading. Once trained, the prediction speed is almost $1\\times10^{4}$ times faster the CP simulations. The efficiency and high generalizability of our DeepONet render it a powerful data-driven surrogate model for CP simulations in multi-scale analyses.","sentences":["Crystal plasticity (CP) simulations are a tool for understanding how microstructure morphology and texture affect mechanical properties and are an essential component of elucidating the structure-property relations.","However, it can be computationally expensive.","Hence, data-driven machine learning models have been applied to predict the mean-field response of a polycrystal representative volume element to reduce computation time.","In this work, we proposed a novel Deep Operator Network (DeepONet) architecture for predicting microstructure stress-strain response.","It employs a convolutional neural network in the trunk to encode the microstructure.","To account for different material properties, boundary conditions, and loading, we proposed using single crystal stress-strain curves as inputs to the branch network, furnishing a material-response-informed DeepONet.","Using four numerical examples, we demonstrate that the current DeepONet can be trained on a single material and loading and then generalized to new conditions via transfer learning.","Results show that using single crystal responses as input outperforms a similar model using material properties as inputs and overcomes limitations with changing boundary conditions and temporal resolution.","In all cases, the new model achieved a $R^2$ value of above 0.99, and over 95\\% of predicted stresses have a relative error of $\\le$ 5\\%, indicating superior accuracy.","With as few as 20 new data points and under 1min training time, the trained DeepONet can be fine-tuned to generate accurate predictions on different materials and loading.","Once trained, the prediction speed is almost $1\\times10^{4}$ times faster the CP simulations.","The efficiency and high generalizability of our DeepONet render it a powerful data-driven surrogate model for CP simulations in multi-scale analyses."],"url":"http://arxiv.org/abs/2401.09977v1"}
{"created":"2024-01-18 12:58:53","title":"Through the Dual-Prism: A Spectral Perspective on Graph Data Augmentation for Graph Classification","abstract":"Graph Neural Networks (GNNs) have become the preferred tool to process graph data, with their efficacy being boosted through graph data augmentation techniques. Despite the evolution of augmentation methods, issues like graph property distortions and restricted structural changes persist. This leads to the question: Is it possible to develop more property-conserving and structure-sensitive augmentation methods? Through a spectral lens, we investigate the interplay between graph properties, their augmentation, and their spectral behavior, and found that keeping the low-frequency eigenvalues unchanged can preserve the critical properties at a large scale when generating augmented graphs. These observations inform our introduction of the Dual-Prism (DP) augmentation method, comprising DP-Noise and DP-Mask, which adeptly retains essential graph properties while diversifying augmented graphs. Extensive experiments validate the efficiency of our approach, providing a new and promising direction for graph data augmentation.","sentences":["Graph Neural Networks (GNNs) have become the preferred tool to process graph data, with their efficacy being boosted through graph data augmentation techniques.","Despite the evolution of augmentation methods, issues like graph property distortions and restricted structural changes persist.","This leads to the question: Is it possible to develop more property-conserving and structure-sensitive augmentation methods?","Through a spectral lens, we investigate the interplay between graph properties, their augmentation, and their spectral behavior, and found that keeping the low-frequency eigenvalues unchanged can preserve the critical properties at a large scale when generating augmented graphs.","These observations inform our introduction of the Dual-Prism (DP) augmentation method, comprising DP-Noise and DP-Mask, which adeptly retains essential graph properties while diversifying augmented graphs.","Extensive experiments validate the efficiency of our approach, providing a new and promising direction for graph data augmentation."],"url":"http://arxiv.org/abs/2401.09953v1"}
{"created":"2024-01-18 12:46:26","title":"WindSeer: Real-time volumetric wind prediction over complex terrain aboard a small UAV","abstract":"Real-time high-resolution wind predictions are beneficial for various applications including safe manned and unmanned aviation. Current weather models require too much compute and lack the necessary predictive capabilities as they are valid only at the scale of multiple kilometers and hours - much lower spatial and temporal resolutions than these applications require. Our work, for the first time, demonstrates the ability to predict low-altitude wind in real-time on limited-compute devices, from only sparse measurement data. We train a neural network, WindSeer, using only synthetic data from computational fluid dynamics simulations and show that it can successfully predict real wind fields over terrain with known topography from just a few noisy and spatially clustered wind measurements. WindSeer can generate accurate predictions at different resolutions and domain sizes on previously unseen topography without retraining. We demonstrate that the model successfully predicts historical wind data collected by weather stations and wind measured onboard drones.","sentences":["Real-time high-resolution wind predictions are beneficial for various applications including safe manned and unmanned aviation.","Current weather models require too much compute and lack the necessary predictive capabilities as they are valid only at the scale of multiple kilometers and hours - much lower spatial and temporal resolutions than these applications require.","Our work, for the first time, demonstrates the ability to predict low-altitude wind in real-time on limited-compute devices, from only sparse measurement data.","We train a neural network, WindSeer, using only synthetic data from computational fluid dynamics simulations and show that it can successfully predict real wind fields over terrain with known topography from just a few noisy and spatially clustered wind measurements.","WindSeer can generate accurate predictions at different resolutions and domain sizes on previously unseen topography without retraining.","We demonstrate that the model successfully predicts historical wind data collected by weather stations and wind measured onboard drones."],"url":"http://arxiv.org/abs/2401.09944v1"}
{"created":"2024-01-18 12:41:58","title":"Biases in Expected Goals Models Confound Finishing Ability","abstract":"Expected Goals (xG) has emerged as a popular tool for evaluating finishing skill in soccer analytics. It involves comparing a player's cumulative xG with their actual goal output, where consistent overperformance indicates strong finishing ability. However, the assessment of finishing skill in soccer using xG remains contentious due to players' difficulty in consistently outperforming their cumulative xG. In this paper, we aim to address the limitations and nuances surrounding the evaluation of finishing skill using xG statistics. Specifically, we explore three hypotheses: (1) the deviation between actual and expected goals is an inadequate metric due to the high variance of shot outcomes and limited sample sizes, (2) the inclusion of all shots in cumulative xG calculation may be inappropriate, and (3) xG models contain biases arising from interdependencies in the data that affect skill measurement. We found that sustained overperformance of cumulative xG requires both high shot volumes and exceptional finishing, including all shot types can obscure the finishing ability of proficient strikers, and that there is a persistent bias that makes the actual and expected goals closer for excellent finishers than it really is. Overall, our analysis indicates that we need more nuanced quantitative approaches for investigating a player's finishing ability, which we achieved using a technique from AI fairness to learn an xG model that is calibrated for multiple subgroups of players. As a concrete use case, we show that (1) the standard biased xG model underestimates Messi's GAX by 17% and (2) Messi's GAX is 27% higher than the typical elite high-shot-volume attacker, indicating that Messi is even a more exceptional finisher than people commonly believed.","sentences":["Expected Goals (xG) has emerged as a popular tool for evaluating finishing skill in soccer analytics.","It involves comparing a player's cumulative xG with their actual goal output, where consistent overperformance indicates strong finishing ability.","However, the assessment of finishing skill in soccer using xG remains contentious due to players' difficulty in consistently outperforming their cumulative xG.","In this paper, we aim to address the limitations and nuances surrounding the evaluation of finishing skill using xG statistics.","Specifically, we explore three hypotheses: (1) the deviation between actual and expected goals is an inadequate metric due to the high variance of shot outcomes and limited sample sizes, (2) the inclusion of all shots in cumulative xG calculation may be inappropriate, and (3) xG models contain biases arising from interdependencies in the data that affect skill measurement.","We found that sustained overperformance of cumulative xG requires both high shot volumes and exceptional finishing, including all shot types can obscure the finishing ability of proficient strikers, and that there is a persistent bias that makes the actual and expected goals closer for excellent finishers than it really is.","Overall, our analysis indicates that we need more nuanced quantitative approaches for investigating a player's finishing ability, which we achieved using a technique from AI fairness to learn an xG model that is calibrated for multiple subgroups of players.","As a concrete use case, we show that (1) the standard biased xG model underestimates Messi's GAX by 17% and (2) Messi's GAX is 27% higher than the typical elite high-shot-volume attacker, indicating that Messi is even a more exceptional finisher than people commonly believed."],"url":"http://arxiv.org/abs/2401.09940v1"}
{"created":"2024-01-18 12:41:41","title":"ICGNet: A Unified Approach for Instance-Centric Grasping","abstract":"Accurate grasping is the key to several robotic tasks including assembly and household robotics. Executing a successful grasp in a cluttered environment requires multiple levels of scene understanding: First, the robot needs to analyze the geometric properties of individual objects to find feasible grasps. These grasps need to be compliant with the local object geometry. Second, for each proposed grasp, the robot needs to reason about the interactions with other objects in the scene. Finally, the robot must compute a collision-free grasp trajectory while taking into account the geometry of the target object. Most grasp detection algorithms directly predict grasp poses in a monolithic fashion, which does not capture the composability of the environment. In this paper, we introduce an end-to-end architecture for object-centric grasping. The method uses pointcloud data from a single arbitrary viewing direction as an input and generates an instance-centric representation for each partially observed object in the scene. This representation is further used for object reconstruction and grasp detection in cluttered table-top scenes. We show the effectiveness of the proposed method by extensively evaluating it against state-of-the-art methods on synthetic datasets, indicating superior performance for grasping and reconstruction. Additionally, we demonstrate real-world applicability by decluttering scenes with varying numbers of objects.","sentences":["Accurate grasping is the key to several robotic tasks including assembly and household robotics.","Executing a successful grasp in a cluttered environment requires multiple levels of scene understanding:","First, the robot needs to analyze the geometric properties of individual objects to find feasible grasps.","These grasps need to be compliant with the local object geometry.","Second, for each proposed grasp, the robot needs to reason about the interactions with other objects in the scene.","Finally, the robot must compute a collision-free grasp trajectory while taking into account the geometry of the target object.","Most grasp detection algorithms directly predict grasp poses in a monolithic fashion, which does not capture the composability of the environment.","In this paper, we introduce an end-to-end architecture for object-centric grasping.","The method uses pointcloud data from a single arbitrary viewing direction as an input and generates an instance-centric representation for each partially observed object in the scene.","This representation is further used for object reconstruction and grasp detection in cluttered table-top scenes.","We show the effectiveness of the proposed method by extensively evaluating it against state-of-the-art methods on synthetic datasets, indicating superior performance for grasping and reconstruction.","Additionally, we demonstrate real-world applicability by decluttering scenes with varying numbers of objects."],"url":"http://arxiv.org/abs/2401.09939v1"}
{"created":"2024-01-18 12:07:39","title":"BlenDA: Domain Adaptive Object Detection through diffusion-based blending","abstract":"Unsupervised domain adaptation (UDA) aims to transfer a model learned using labeled data from the source domain to unlabeled data in the target domain. To address the large domain gap issue between the source and target domains, we propose a novel regularization method for domain adaptive object detection, BlenDA, by generating the pseudo samples of the intermediate domains and their corresponding soft domain labels for adaptation training. The intermediate samples are generated by dynamically blending the source images with their corresponding translated images using an off-the-shelf pre-trained text-to-image diffusion model which takes the text label of the target domain as input and has demonstrated superior image-to-image translation quality. Based on experimental results from two adaptation benchmarks, our proposed approach can significantly enhance the performance of the state-of-the-art domain adaptive object detector, Adversarial Query Transformer (AQT). Particularly, in the Cityscapes to Foggy Cityscapes adaptation, we achieve an impressive 53.4% mAP on the Foggy Cityscapes dataset, surpassing the previous state-of-the-art by 1.5%. It is worth noting that our proposed method is also applicable to various paradigms of domain adaptive object detection. The code is available at:https://github.com/aiiu-lab/BlenDA","sentences":["Unsupervised domain adaptation (UDA) aims to transfer a model learned using labeled data from the source domain to unlabeled data in the target domain.","To address the large domain gap issue between the source and target domains, we propose a novel regularization method for domain adaptive object detection, BlenDA, by generating the pseudo samples of the intermediate domains and their corresponding soft domain labels for adaptation training.","The intermediate samples are generated by dynamically blending the source images with their corresponding translated images using an off-the-shelf pre-trained text-to-image diffusion model which takes the text label of the target domain as input and has demonstrated superior image-to-image translation quality.","Based on experimental results from two adaptation benchmarks, our proposed approach can significantly enhance the performance of the state-of-the-art domain adaptive object detector, Adversarial Query Transformer (AQT).","Particularly, in the Cityscapes to Foggy Cityscapes adaptation, we achieve an impressive 53.4% mAP on the Foggy Cityscapes dataset, surpassing the previous state-of-the-art by 1.5%.","It is worth noting that our proposed method is also applicable to various paradigms of domain adaptive object detection.","The code is available at:https://github.com/aiiu-lab/BlenDA"],"url":"http://arxiv.org/abs/2401.09921v1"}
{"created":"2024-01-18 11:49:20","title":"Deep Back-Filling: a Split Window Technique for Deep Online Cluster Job Scheduling","abstract":"Job scheduling is a critical component of workload management systems that can significantly influence system performance, e.g., in HPC clusters. The scheduling objectives are often mixed, such as maximizing resource utilization and minimizing job waiting time. An increasing number of researchers are moving from heuristic-based approaches to Deep Reinforcement Learning approaches in order to optimize scheduling objectives. However, the job scheduler's state space is partially observable to a DRL-based agent because the job queue is practically unbounded. The agent's observation of the state space is constant in size since the input size of the neural networks is predefined. All existing solutions to this problem intuitively allow the agent to observe a fixed window size of jobs at the head of the job queue. In our research, we have seen that such an approach can lead to \"window staleness\" where the window becomes full of jobs that can not be scheduled until the cluster has completed sufficient work. In this paper, we propose a novel general technique that we call \\emph{split window}, which allows the agent to observe both the head \\emph{and tail} of the queue. With this technique, the agent can observe all arriving jobs at least once, which completely eliminates the window staleness problem. By leveraging the split window, the agent can significantly reduce the average job waiting time and average queue length, alternatively allowing the use of much smaller windows and, therefore, faster training times. We show a range of simulation results using HPC job scheduling trace data that supports the effectiveness of our technique.","sentences":["Job scheduling is a critical component of workload management systems that can significantly influence system performance, e.g., in HPC clusters.","The scheduling objectives are often mixed, such as maximizing resource utilization and minimizing job waiting time.","An increasing number of researchers are moving from heuristic-based approaches to Deep Reinforcement Learning approaches in order to optimize scheduling objectives.","However, the job scheduler's state space is partially observable to a DRL-based agent because the job queue is practically unbounded.","The agent's observation of the state space is constant in size since the input size of the neural networks is predefined.","All existing solutions to this problem intuitively allow the agent to observe a fixed window size of jobs at the head of the job queue.","In our research, we have seen that such an approach can lead to \"window staleness\" where the window becomes full of jobs that can not be scheduled until the cluster has completed sufficient work.","In this paper, we propose a novel general technique that we call \\emph{split window}, which allows the agent to observe both the head \\emph{and tail} of the queue.","With this technique, the agent can observe all arriving jobs at least once, which completely eliminates the window staleness problem.","By leveraging the split window, the agent can significantly reduce the average job waiting time and average queue length, alternatively allowing the use of much smaller windows and, therefore, faster training times.","We show a range of simulation results using HPC job scheduling trace data that supports the effectiveness of our technique."],"url":"http://arxiv.org/abs/2401.09910v1"}
{"created":"2024-01-18 11:24:30","title":"Meme-ingful Analysis: Enhanced Understanding of Cyberbullying in Memes Through Multimodal Explanations","abstract":"Internet memes have gained significant influence in communicating political, psychological, and sociocultural ideas. While memes are often humorous, there has been a rise in the use of memes for trolling and cyberbullying. Although a wide variety of effective deep learning-based models have been developed for detecting offensive multimodal memes, only a few works have been done on explainability aspect. Recent laws like \"right to explanations\" of General Data Protection Regulation, have spurred research in developing interpretable models rather than only focusing on performance. Motivated by this, we introduce {\\em MultiBully-Ex}, the first benchmark dataset for multimodal explanation from code-mixed cyberbullying memes. Here, both visual and textual modalities are highlighted to explain why a given meme is cyberbullying. A Contrastive Language-Image Pretraining (CLIP) projection-based multimodal shared-private multitask approach has been proposed for visual and textual explanation of a meme. Experimental results demonstrate that training with multimodal explanations improves performance in generating textual justifications and more accurately identifying the visual evidence supporting a decision with reliable performance improvements.","sentences":["Internet memes have gained significant influence in communicating political, psychological, and sociocultural ideas.","While memes are often humorous, there has been a rise in the use of memes for trolling and cyberbullying.","Although a wide variety of effective deep learning-based models have been developed for detecting offensive multimodal memes, only a few works have been done on explainability aspect.","Recent laws like \"right to explanations\" of General Data Protection Regulation, have spurred research in developing interpretable models rather than only focusing on performance.","Motivated by this, we introduce {\\em MultiBully-Ex}, the first benchmark dataset for multimodal explanation from code-mixed cyberbullying memes.","Here, both visual and textual modalities are highlighted to explain why a given meme is cyberbullying.","A Contrastive Language-Image Pretraining (CLIP) projection-based multimodal shared-private multitask approach has been proposed for visual and textual explanation of a meme.","Experimental results demonstrate that training with multimodal explanations improves performance in generating textual justifications and more accurately identifying the visual evidence supporting a decision with reliable performance improvements."],"url":"http://arxiv.org/abs/2401.09899v1"}
{"created":"2024-01-18 11:20:39","title":"Experimental Shake Gesture Detection API for Apple Watch","abstract":"In this paper we present the WatchShaker project The project involves an experimental API that detects the Apple Watchs shake gesturea surprisingly absent natively feature Through a simple heuristic leveraging the Apple Watchs accelerometer data the API discerns not just the occurrence of shake gestures but also their direction enhancing the interactivity potential of the device Despite the projects simplicity and lack of formal testing it has garnered significant attention indicating a genuine interest and need within the developer community for such functionality The WatchShaker project exemplifies how a minimalistic approach can yield a practical and impactful tool in wearable technology providing a springboard for further research and development in intuitive gesture recognition","sentences":["In this paper we present the WatchShaker project The project involves an experimental API that detects the Apple Watchs shake gesturea surprisingly absent natively feature Through a simple heuristic leveraging the Apple Watchs accelerometer data the API discerns not just the occurrence of shake gestures but also their direction enhancing the interactivity potential of the device Despite the projects simplicity and lack of formal testing it has garnered significant attention indicating a genuine interest and need within the developer community for such functionality The WatchShaker project exemplifies how a minimalistic approach can yield a practical and impactful tool in wearable technology providing a springboard for further research and development in intuitive gesture recognition"],"url":"http://arxiv.org/abs/2401.09896v1"}
{"created":"2024-01-18 10:59:18","title":"Cooperative Edge Caching Based on Elastic Federated and Multi-Agent Deep Reinforcement Learning in Next-Generation Network","abstract":"Edge caching is a promising solution for next-generation networks by empowering caching units in small-cell base stations (SBSs), which allows user equipments (UEs) to fetch users' requested contents that have been pre-cached in SBSs. It is crucial for SBSs to predict accurate popular contents through learning while protecting users' personal information. Traditional federated learning (FL) can protect users' privacy but the data discrepancies among UEs can lead to a degradation in model quality. Therefore, it is necessary to train personalized local models for each UE to predict popular contents accurately. In addition, the cached contents can be shared among adjacent SBSs in next-generation networks, thus caching predicted popular contents in different SBSs may affect the cost to fetch contents. Hence, it is critical to determine where the popular contents are cached cooperatively. To address these issues, we propose a cooperative edge caching scheme based on elastic federated and multi-agent deep reinforcement learning (CEFMR) to optimize the cost in the network. We first propose an elastic FL algorithm to train the personalized model for each UE, where adversarial autoencoder (AAE) model is adopted for training to improve the prediction accuracy, then {a popular} content prediction algorithm is proposed to predict the popular contents for each SBS based on the trained AAE model. Finally, we propose a multi-agent deep reinforcement learning (MADRL) based algorithm to decide where the predicted popular contents are collaboratively cached among SBSs. Our experimental results demonstrate the superiority of our proposed scheme to existing baseline caching schemes.","sentences":["Edge caching is a promising solution for next-generation networks by empowering caching units in small-cell base stations (SBSs), which allows user equipments (UEs) to fetch users' requested contents that have been pre-cached in SBSs.","It is crucial for SBSs to predict accurate popular contents through learning while protecting users' personal information.","Traditional federated learning (FL) can protect users' privacy but the data discrepancies among UEs can lead to a degradation in model quality.","Therefore, it is necessary to train personalized local models for each UE to predict popular contents accurately.","In addition, the cached contents can be shared among adjacent SBSs in next-generation networks, thus caching predicted popular contents in different SBSs may affect the cost to fetch contents.","Hence, it is critical to determine where the popular contents are cached cooperatively.","To address these issues, we propose a cooperative edge caching scheme based on elastic federated and multi-agent deep reinforcement learning (CEFMR) to optimize the cost in the network.","We first propose an elastic FL algorithm to train the personalized model for each UE, where adversarial autoencoder (AAE) model is adopted for training to improve the prediction accuracy, then {a popular} content prediction algorithm is proposed to predict the popular contents for each SBS based on the trained AAE model.","Finally, we propose a multi-agent deep reinforcement learning (MADRL) based algorithm to decide where the predicted popular contents are collaboratively cached among SBSs.","Our experimental results demonstrate the superiority of our proposed scheme to existing baseline caching schemes."],"url":"http://arxiv.org/abs/2401.09886v1"}
{"created":"2024-01-18 10:53:45","title":"GA-SmaAt-GNet: Generative Adversarial Small Attention GNet for Extreme Precipitation Nowcasting","abstract":"In recent years, data-driven modeling approaches have gained considerable traction in various meteorological applications, particularly in the realm of weather forecasting. However, these approaches often encounter challenges when dealing with extreme weather conditions. In light of this, we propose GA-SmaAt-GNet, a novel generative adversarial architecture that makes use of two methodologies aimed at enhancing the performance of deep learning models for extreme precipitation nowcasting. Firstly, it uses a novel SmaAt-GNet built upon the successful SmaAt-UNet architecture as generator. This network incorporates precipitation masks (binarized precipitation maps) as an additional data source, leveraging valuable information for improved predictions. Additionally, GA-SmaAt-GNet utilizes an attention-augmented discriminator inspired by the well-established Pix2Pix architecture. Furthermore, we assess the performance of GA-SmaAt-GNet using real-life precipitation dataset from the Netherlands. Our experimental results reveal a notable improvement in both overall performance and for extreme precipitation events. Furthermore, we conduct uncertainty analysis on the proposed GA-SmaAt-GNet model as well as on the precipitation dataset, providing additional insights into the predictive capabilities of the model. Finally, we offer further insights into the predictions of our proposed model using Grad-CAM. This visual explanation technique generates activation heatmaps, illustrating areas of the input that are more activated for various parts of the network.","sentences":["In recent years, data-driven modeling approaches have gained considerable traction in various meteorological applications, particularly in the realm of weather forecasting.","However, these approaches often encounter challenges when dealing with extreme weather conditions.","In light of this, we propose GA-SmaAt-GNet, a novel generative adversarial architecture that makes use of two methodologies aimed at enhancing the performance of deep learning models for extreme precipitation nowcasting.","Firstly, it uses a novel SmaAt-GNet built upon the successful SmaAt-UNet architecture as generator.","This network incorporates precipitation masks (binarized precipitation maps) as an additional data source, leveraging valuable information for improved predictions.","Additionally, GA-SmaAt-GNet utilizes an attention-augmented discriminator inspired by the well-established Pix2Pix architecture.","Furthermore, we assess the performance of GA-SmaAt-GNet using real-life precipitation dataset from the Netherlands.","Our experimental results reveal a notable improvement in both overall performance and for extreme precipitation events.","Furthermore, we conduct uncertainty analysis on the proposed GA-SmaAt-GNet model as well as on the precipitation dataset, providing additional insights into the predictive capabilities of the model.","Finally, we offer further insights into the predictions of our proposed model using Grad-CAM.","This visual explanation technique generates activation heatmaps, illustrating areas of the input that are more activated for various parts of the network."],"url":"http://arxiv.org/abs/2401.09881v1"}
{"created":"2024-01-18 10:29:10","title":"Boosting Few-Shot Segmentation via Instance-Aware Data Augmentation and Local Consensus Guided Cross Attention","abstract":"Few-shot segmentation aims to train a segmentation model that can fast adapt to a novel task for which only a few annotated images are provided. Most recent models have adopted a prototype-based paradigm for few-shot inference. These approaches may have limited generalization capacity beyond the standard 1- or 5-shot settings. In this paper, we closely examine and reevaluate the fine-tuning based learning scheme that fine-tunes the classification layer of a deep segmentation network pre-trained on diverse base classes. To improve the generalizability of the classification layer optimized with sparsely annotated samples, we introduce an instance-aware data augmentation (IDA) strategy that augments the support images based on the relative sizes of the target objects. The proposed IDA effectively increases the support set's diversity and promotes the distribution consistency between support and query images. On the other hand, the large visual difference between query and support images may hinder knowledge transfer and cripple the segmentation performance. To cope with this challenge, we introduce the local consensus guided cross attention (LCCA) to align the query feature with support features based on their dense correlation, further improving the model's generalizability to the query image. The significant performance improvements on the standard few-shot segmentation benchmarks PASCAL-$5^i$ and COCO-$20^i$ verify the efficacy of our proposed method.","sentences":["Few-shot segmentation aims to train a segmentation model that can fast adapt to a novel task for which only a few annotated images are provided.","Most recent models have adopted a prototype-based paradigm for few-shot inference.","These approaches may have limited generalization capacity beyond the standard 1- or 5-shot settings.","In this paper, we closely examine and reevaluate the fine-tuning based learning scheme that fine-tunes the classification layer of a deep segmentation network pre-trained on diverse base classes.","To improve the generalizability of the classification layer optimized with sparsely annotated samples, we introduce an instance-aware data augmentation (IDA) strategy that augments the support images based on the relative sizes of the target objects.","The proposed IDA effectively increases the support set's diversity and promotes the distribution consistency between support and query images.","On the other hand, the large visual difference between query and support images may hinder knowledge transfer and cripple the segmentation performance.","To cope with this challenge, we introduce the local consensus guided cross attention (LCCA) to align the query feature with support features based on their dense correlation, further improving the model's generalizability to the query image.","The significant performance improvements on the standard few-shot segmentation benchmarks PASCAL-$5^i$ and COCO-$20^i$ verify the efficacy of our proposed method."],"url":"http://arxiv.org/abs/2401.09866v1"}
{"created":"2024-01-18 10:12:40","title":"EDAF: An End-to-End Delay Analytics Framework for 5G-and-Beyond Networks","abstract":"Supporting applications in emerging domains like cyber-physical systems and human-in-the-loop scenarios typically requires adherence to strict end-to-end delay guarantees. Contributions of many tandem processes unfolding layer by layer within the wireless network result in violations of delay constraints, thereby severely degrading application performance. Meeting the application's stringent requirements necessitates coordinated optimization of the end-to-end delay by fine-tuning all contributing processes. To achieve this task, we designed and implemented EDAF, a framework to decompose packets' end-to-end delays and determine each component's significance for 5G network. We showcase EDAF on OpenAirInterface 5G uplink, modified to report timestamps across the data plane. By applying the obtained insights, we optimized end-to-end uplink delay by eliminating segmentation and frame-alignment delays, decreasing average delay from 12ms to 4ms.","sentences":["Supporting applications in emerging domains like cyber-physical systems and human-in-the-loop scenarios typically requires adherence to strict end-to-end delay guarantees.","Contributions of many tandem processes unfolding layer by layer within the wireless network result in violations of delay constraints, thereby severely degrading application performance.","Meeting the application's stringent requirements necessitates coordinated optimization of the end-to-end delay by fine-tuning all contributing processes.","To achieve this task, we designed and implemented EDAF, a framework to decompose packets' end-to-end delays and determine each component's significance for 5G network.","We showcase EDAF on OpenAirInterface 5G uplink, modified to report timestamps across the data plane.","By applying the obtained insights, we optimized end-to-end uplink delay by eliminating segmentation and frame-alignment delays, decreasing average delay from 12ms to 4ms."],"url":"http://arxiv.org/abs/2401.09856v1"}
{"created":"2024-01-18 10:10:25","title":"A Survey on Energy Consumption and Environmental Impact of Video Streaming","abstract":"Climate change challenges require a notable decrease in worldwide greenhouse gas (GHG) emissions across technology sectors. Digital technologies, especially video streaming, accounting for most Internet traffic, make no exception. Video streaming demand increases with remote working, multimedia communication services (e.g., WhatsApp, Skype), video streaming content (e.g., YouTube, Netflix), video resolution (4K/8K, 50 fps/60 fps), and multi-view video, making energy consumption and environmental footprint critical. This survey contributes to a better understanding of sustainable and efficient video streaming technologies by providing insights into the state-of-the-art and potential future directions for researchers, developers, and engineers, service providers, hosting platforms, and consumers. We widen this survey's focus on content provisioning and content consumption based on the observation that continuously active network equipment underneath video streaming consumes substantial energy independent of the transmitted data type. We propose a taxonomy of factors that affect the energy consumption in video streaming, such as encoding schemes, resource requirements, storage, content retrieval, decoding, and display. We identify notable weaknesses in video streaming that require further research for improved energy efficiency: (1) fixed bitrate ladders in HTTP live streaming; (2) inefficient hardware utilization of existing video players; (3) lack of comprehensive open energy measurement dataset covering various device types and coding parameters for reproducible research.","sentences":["Climate change challenges require a notable decrease in worldwide greenhouse gas (GHG) emissions across technology sectors.","Digital technologies, especially video streaming, accounting for most Internet traffic, make no exception.","Video streaming demand increases with remote working, multimedia communication services (e.g., WhatsApp, Skype), video streaming content (e.g., YouTube, Netflix), video resolution (4K/8K, 50 fps/60 fps), and multi-view video, making energy consumption and environmental footprint critical.","This survey contributes to a better understanding of sustainable and efficient video streaming technologies by providing insights into the state-of-the-art and potential future directions for researchers, developers, and engineers, service providers, hosting platforms, and consumers.","We widen this survey's focus on content provisioning and content consumption based on the observation that continuously active network equipment underneath video streaming consumes substantial energy independent of the transmitted data type.","We propose a taxonomy of factors that affect the energy consumption in video streaming, such as encoding schemes, resource requirements, storage, content retrieval, decoding, and display.","We identify notable weaknesses in video streaming that require further research for improved energy efficiency: (1) fixed bitrate ladders in HTTP live streaming; (2) inefficient hardware utilization of existing video players; (3) lack of comprehensive open energy measurement dataset covering various device types and coding parameters for reproducible research."],"url":"http://arxiv.org/abs/2401.09854v1"}
{"created":"2024-01-18 10:05:52","title":"Behavioral Simulation: Exploring A Possible Next Paradigm for Science","abstract":"Simulation technologies have been widely utilized in many scientific research fields such as weather forecasting, fluid mechanics and biological populations. It is the best tool to handle problems in complex systems, where closed-form expressions are unavailable and the target distribution in the representation space is too complex to be fully represented by a deep learning (DL) model. We believe that the development of simulation technologies is consistent with scientific paradigms. This paper induces the evolution of scientific paradigms from the perspective of data, algorithms, and computational power. Building upon this perspective, we divide simulation technologies into three stages aligning with the emergence of new paradigms, and find that advanced simulation technologies are typical instances of paradigms integration. Moreover, we propose the concept of behavioral simulation (BS), specifically sophisticated behavioral simulation (SBS), representing a higher degree of paradigms integration based on foundation models to simulate complex social systems involving sophisticated human strategies and behaviors. BS and further SBS are designed to tackle challenges concerning the complex human system that surpasses the capacity of traditional agent-based modeling simulation (ABMS), which can be regarded as a possible next paradigm for science. Through this work, we look forward to more powerful BS and SBS applications in scientific research branches within social science.","sentences":["Simulation technologies have been widely utilized in many scientific research fields such as weather forecasting, fluid mechanics and biological populations.","It is the best tool to handle problems in complex systems, where closed-form expressions are unavailable and the target distribution in the representation space is too complex to be fully represented by a deep learning (DL) model.","We believe that the development of simulation technologies is consistent with scientific paradigms.","This paper induces the evolution of scientific paradigms from the perspective of data, algorithms, and computational power.","Building upon this perspective, we divide simulation technologies into three stages aligning with the emergence of new paradigms, and find that advanced simulation technologies are typical instances of paradigms integration.","Moreover, we propose the concept of behavioral simulation (BS), specifically sophisticated behavioral simulation (SBS), representing a higher degree of paradigms integration based on foundation models to simulate complex social systems involving sophisticated human strategies and behaviors.","BS and further SBS are designed to tackle challenges concerning the complex human system that surpasses the capacity of traditional agent-based modeling simulation (ABMS), which can be regarded as a possible next paradigm for science.","Through this work, we look forward to more powerful BS and SBS applications in scientific research branches within social science."],"url":"http://arxiv.org/abs/2401.09851v1"}
{"created":"2024-01-18 09:34:40","title":"Boosting Few-Shot Semantic Segmentation Via Segment Anything Model","abstract":"In semantic segmentation, accurate prediction masks are crucial for downstream tasks such as medical image analysis and image editing. Due to the lack of annotated data, few-shot semantic segmentation (FSS) performs poorly in predicting masks with precise contours. Recently, we have noticed that the large foundation model segment anything model (SAM) performs well in processing detailed features. Inspired by SAM, we propose FSS-SAM to boost FSS methods by addressing the issue of inaccurate contour. The FSS-SAM is training-free. It works as a post-processing tool for any FSS methods and can improve the accuracy of predicted masks. Specifically, we use predicted masks from FSS methods to generate prompts and then use SAM to predict new masks. To avoid predicting wrong masks with SAM, we propose a prediction result selection (PRS) algorithm. The algorithm can remarkably decrease wrong predictions. Experiment results on public datasets show that our method is superior to base FSS methods in both quantitative and qualitative aspects.","sentences":["In semantic segmentation, accurate prediction masks are crucial for downstream tasks such as medical image analysis and image editing.","Due to the lack of annotated data, few-shot semantic segmentation (FSS) performs poorly in predicting masks with precise contours.","Recently, we have noticed that the large foundation model segment anything model (SAM) performs well in processing detailed features.","Inspired by SAM, we propose FSS-SAM to boost FSS methods by addressing the issue of inaccurate contour.","The FSS-SAM is training-free.","It works as a post-processing tool for any FSS methods and can improve the accuracy of predicted masks.","Specifically, we use predicted masks from FSS methods to generate prompts and then use SAM to predict new masks.","To avoid predicting wrong masks with SAM, we propose a prediction result selection (PRS) algorithm.","The algorithm can remarkably decrease wrong predictions.","Experiment results on public datasets show that our method is superior to base FSS methods in both quantitative and qualitative aspects."],"url":"http://arxiv.org/abs/2401.09826v1"}
{"created":"2024-01-18 09:31:45","title":"Conning the Crypto Conman: End-to-End Analysis of Cryptocurrency-based Technical Support Scams","abstract":"The mainstream adoption of cryptocurrencies has led to a surge in wallet-related issues reported by ordinary users on social media platforms. In parallel, there is an increase in an emerging fraud trend called cryptocurrency-based technical support scam, in which fraudsters offer fake wallet recovery services and target users experiencing wallet-related issues.   In this paper, we perform a comprehensive study of cryptocurrency-based technical support scams. We present an analysis apparatus called HoneyTweet to analyze this kind of scam. Through HoneyTweet, we lure over 9K scammers by posting 25K fake wallet support tweets (so-called honey tweets). We then deploy automated systems to interact with scammers to analyze their modus operandi. In our experiments, we observe that scammers use Twitter as a starting point for the scam, after which they pivot to other communication channels (eg email, Instagram, or Telegram) to complete the fraud activity. We track scammers across those communication channels and bait them into revealing their payment methods. Based on the modes of payment, we uncover two categories of scammers that either request secret key phrase submissions from their victims or direct payments to their digital wallets. Furthermore, we obtain scam confirmation by deploying honey wallet addresses and validating private key theft. We also collaborate with the prominent payment service provider by sharing scammer data collections. The payment service provider feedback was consistent with our findings, thereby supporting our methodology and results. By consolidating our analysis across various vantage points, we provide an end-to-end scam lifecycle analysis and propose recommendations for scam mitigation.","sentences":["The mainstream adoption of cryptocurrencies has led to a surge in wallet-related issues reported by ordinary users on social media platforms.","In parallel, there is an increase in an emerging fraud trend called cryptocurrency-based technical support scam, in which fraudsters offer fake wallet recovery services and target users experiencing wallet-related issues.   ","In this paper, we perform a comprehensive study of cryptocurrency-based technical support scams.","We present an analysis apparatus called HoneyTweet to analyze this kind of scam.","Through HoneyTweet, we lure over 9K scammers by posting 25K fake wallet support tweets (so-called honey tweets).","We then deploy automated systems to interact with scammers to analyze their modus operandi.","In our experiments, we observe that scammers use Twitter as a starting point for the scam, after which they pivot to other communication channels (eg email, Instagram, or Telegram) to complete the fraud activity.","We track scammers across those communication channels and bait them into revealing their payment methods.","Based on the modes of payment, we uncover two categories of scammers that either request secret key phrase submissions from their victims or direct payments to their digital wallets.","Furthermore, we obtain scam confirmation by deploying honey wallet addresses and validating private key theft.","We also collaborate with the prominent payment service provider by sharing scammer data collections.","The payment service provider feedback was consistent with our findings, thereby supporting our methodology and results.","By consolidating our analysis across various vantage points, we provide an end-to-end scam lifecycle analysis and propose recommendations for scam mitigation."],"url":"http://arxiv.org/abs/2401.09824v1"}
{"created":"2024-01-18 09:20:27","title":"PPNet: A Novel Neural Network Structure for End-to-End Near-Optimal Path Planning","abstract":"The classical path planners, such as sampling-based path planners, have the limitations of sensitivity to the initial solution and slow convergence to the optimal solution. However, finding a near-optimal solution in a short period is challenging in many applications such as the autonomous vehicle with limited power/fuel. To achieve an end-to-end near-optimal path planner, we first divide the path planning problem into two subproblems, which are path's space segmentation and waypoints generation in the given path's space. We further propose a two-level cascade neural network named Path Planning Network (PPNet) to solve the path planning problem by solving the abovementioned subproblems. Moreover, we propose a novel efficient data generation method for path planning named EDaGe-PP. The results show the total computation time is less than 1/33 and the success rate of PPNet trained by the dataset that is generated by EDaGe-PP is about $2 \\times$ compared to other methods. We validate PPNet against state-of-the-art path planning methods. The results show PPNet can find a near-optimal solution in 15.3ms, which is much shorter than the state-of-the-art path planners.","sentences":["The classical path planners, such as sampling-based path planners, have the limitations of sensitivity to the initial solution and slow convergence to the optimal solution.","However, finding a near-optimal solution in a short period is challenging in many applications such as the autonomous vehicle with limited power/fuel.","To achieve an end-to-end near-optimal path planner, we first divide the path planning problem into two subproblems, which are path's space segmentation and waypoints generation in the given path's space.","We further propose a two-level cascade neural network named Path Planning Network (PPNet) to solve the path planning problem by solving the abovementioned subproblems.","Moreover, we propose a novel efficient data generation method for path planning named EDaGe-PP.","The results show the total computation time is less than 1/33 and the success rate of PPNet trained by the dataset that is generated by EDaGe-PP is about $2 \\times$ compared to other methods.","We validate PPNet against state-of-the-art path planning methods.","The results show PPNet can find a near-optimal solution in 15.3ms, which is much shorter than the state-of-the-art path planners."],"url":"http://arxiv.org/abs/2401.09819v1"}
{"created":"2024-01-18 09:13:59","title":"Simple and effective data augmentation for compositional generalization","abstract":"Compositional generalization, the ability to predict complex meanings from training on simpler sentences, poses challenges for powerful pretrained seq2seq models. In this paper, we show that data augmentation methods that sample MRs and backtranslate them can be effective for compositional generalization, but only if we sample from the right distribution. Remarkably, sampling from a uniform distribution performs almost as well as sampling from the test distribution, and greatly outperforms earlier methods that sampled from the training distribution. We further conduct experiments to investigate the reason why this happens and where the benefit of such data augmentation methods come from.","sentences":["Compositional generalization, the ability to predict complex meanings from training on simpler sentences, poses challenges for powerful pretrained seq2seq models.","In this paper, we show that data augmentation methods that sample MRs and backtranslate them can be effective for compositional generalization, but only if we sample from the right distribution.","Remarkably, sampling from a uniform distribution performs almost as well as sampling from the test distribution, and greatly outperforms earlier methods that sampled from the training distribution.","We further conduct experiments to investigate the reason why this happens and where the benefit of such data augmentation methods come from."],"url":"http://arxiv.org/abs/2401.09815v1"}
{"created":"2024-01-18 08:57:53","title":"SensoDat: Simulation-based Sensor Dataset of Self-driving Cars","abstract":"Developing tools in the context of autonomous systems [22, 24 ], such as self-driving cars (SDCs), is time-consuming and costly since researchers and practitioners rely on expensive computing hardware and simulation software. We propose SensoDat, a dataset of 32,580 executed simulation-based SDC test cases generated with state-of-the-art test generators for SDCs. The dataset consists of trajectory logs and a variety of sensor data from the SDCs (e.g., rpm, wheel speed, brake thermals, transmission, etc.) represented as a time series. In total, SensoDat provides data from 81 different simulated sensors. Future research in the domain of SDCs does not necessarily depend on executing expensive test cases when using SensoDat. Furthermore, with the high amount and variety of sensor data, we think SensoDat can contribute to research, particularly for AI development, regression testing techniques for simulation-based SDC testing, flakiness in simulation, etc. Link to the dataset: https://doi.org/10.5281/zenodo.10307479","sentences":["Developing tools in the context of autonomous systems","[22, 24 ], such as self-driving cars (SDCs), is time-consuming and costly since researchers and practitioners rely on expensive computing hardware and simulation software.","We propose SensoDat, a dataset of 32,580 executed simulation-based SDC test cases generated with state-of-the-art test generators for SDCs.","The dataset consists of trajectory logs and a variety of sensor data from the SDCs (e.g., rpm, wheel speed, brake thermals, transmission, etc.) represented as a time series.","In total, SensoDat provides data from 81 different simulated sensors.","Future research in the domain of SDCs does not necessarily depend on executing expensive test cases when using SensoDat.","Furthermore, with the high amount and variety of sensor data, we think SensoDat can contribute to research, particularly for AI development, regression testing techniques for simulation-based SDC testing, flakiness in simulation, etc.","Link to the dataset: https://doi.org/10.5281/zenodo.10307479"],"url":"http://arxiv.org/abs/2401.09808v1"}
{"created":"2024-01-18 08:33:09","title":"A Fast, Performant, Secure Distributed Training Framework For Large Language Model","abstract":"The distributed (federated) LLM is an important method for co-training the domain-specific LLM using siloed data. However, maliciously stealing model parameters and data from the server or client side has become an urgent problem to be solved. In this paper, we propose a secure distributed LLM based on model slicing. In this case, we deploy the Trusted Execution Environment (TEE) on both the client and server side, and put the fine-tuned structure (LoRA or embedding of P-tuning v2) into the TEE. Then, secure communication is executed in the TEE and general environments through lightweight encryption. In order to further reduce the equipment cost as well as increase the model performance and accuracy, we propose a split fine-tuning scheme. In particular, we split the LLM by layers and place the latter layers in a server-side TEE (the client does not need a TEE). We then combine the proposed Sparsification Parameter Fine-tuning (SPF) with the LoRA part to improve the accuracy of the downstream task. Numerous experiments have shown that our method guarantees accuracy while maintaining security.","sentences":["The distributed (federated) LLM is an important method for co-training the domain-specific LLM using siloed data.","However, maliciously stealing model parameters and data from the server or client side has become an urgent problem to be solved.","In this paper, we propose a secure distributed LLM based on model slicing.","In this case, we deploy the Trusted Execution Environment (TEE) on both the client and server side, and put the fine-tuned structure (LoRA or embedding of P-tuning v2) into the TEE.","Then, secure communication is executed in the TEE and general environments through lightweight encryption.","In order to further reduce the equipment cost as well as increase the model performance and accuracy, we propose a split fine-tuning scheme.","In particular, we split the LLM by layers and place the latter layers in a server-side TEE (the client does not need a TEE).","We then combine the proposed Sparsification Parameter Fine-tuning (SPF) with the LoRA part to improve the accuracy of the downstream task.","Numerous experiments have shown that our method guarantees accuracy while maintaining security."],"url":"http://arxiv.org/abs/2401.09796v1"}
{"created":"2024-01-18 08:31:38","title":"A Comparative Analysis on Metaheuristic Algorithms Based Vision Transformer Model for Early Detection of Alzheimer's Disease","abstract":"A number of life threatening neuro-degenerative disorders had degraded the quality of life for the older generation in particular. Dementia is one such symptom which may lead to a severe condition called Alzheimer's disease if not detected at an early stage. It has been reported that the progression of such disease from a normal stage is due to the change in several parameters inside the human brain. In this paper, an innovative metaheuristic algorithms based ViT model has been proposed for the identification of dementia at different stage. A sizeable number of test data have been utilized for the validation of the proposed scheme. It has also been demonstrated that our model exhibits superior performance in terms of accuracy, precision, recall as well as F1-score.","sentences":["A number of life threatening neuro-degenerative disorders had degraded the quality of life for the older generation in particular.","Dementia is one such symptom which may lead to a severe condition called Alzheimer's disease if not detected at an early stage.","It has been reported that the progression of such disease from a normal stage is due to the change in several parameters inside the human brain.","In this paper, an innovative metaheuristic algorithms based ViT model has been proposed for the identification of dementia at different stage.","A sizeable number of test data have been utilized for the validation of the proposed scheme.","It has also been demonstrated that our model exhibits superior performance in terms of accuracy, precision, recall as well as F1-score."],"url":"http://arxiv.org/abs/2401.09795v1"}
{"created":"2024-01-18 08:20:19","title":"A Semantic Approach for Big Data Exploration in Industry 4.0","abstract":"The growing trends in automation, Internet of Things, big data and cloud computing technologies have led to the fourth industrial revolution (Industry 4.0), where it is possible to visualize and identify patterns and insights, which results in a better understanding of the data and can improve the manufacturing process. However, many times, the task of data exploration results difficult for manufacturing experts because they might be interested in analyzing also data that does not appear in pre-designed visualizations and therefore they must be assisted by Information Technology experts. In this paper, we present a proposal materialized in a semantic-based visual query system developed for a real Industry 4.0 scenario that allows domain experts to explore and visualize data in a friendly way. The main novelty of the system is the combined use that it makes of captured data that are semantically annotated first, and a 2D customized digital representation of a machine that is also linked with semantic descriptions. Those descriptions are expressed using terms of an ontology, where, among others, the sensors that are used to capture indicators about the performance of a machine that belongs to a Industry 4.0 scenario have been modeled. Moreover, this semantic description allows to: formulate queries at a higher level of abstraction, provide customized graphical visualizations of the results based on the format and nature of the data, and download enriched data enabling further types of analysis.","sentences":["The growing trends in automation, Internet of Things, big data and cloud computing technologies have led to the fourth industrial revolution (Industry 4.0), where it is possible to visualize and identify patterns and insights, which results in a better understanding of the data and can improve the manufacturing process.","However, many times, the task of data exploration results difficult for manufacturing experts because they might be interested in analyzing also data that does not appear in pre-designed visualizations and therefore they must be assisted by Information Technology experts.","In this paper, we present a proposal materialized in a semantic-based visual query system developed for a real Industry 4.0 scenario that allows domain experts to explore and visualize data in a friendly way.","The main novelty of the system is the combined use that it makes of captured data that are semantically annotated first, and a 2D customized digital representation of a machine that is also linked with semantic descriptions.","Those descriptions are expressed using terms of an ontology, where, among others, the sensors that are used to capture indicators about the performance of a machine that belongs to a Industry 4.0 scenario have been modeled.","Moreover, this semantic description allows to: formulate queries at a higher level of abstraction, provide customized graphical visualizations of the results based on the format and nature of the data, and download enriched data enabling further types of analysis."],"url":"http://arxiv.org/abs/2401.09789v1"}
{"created":"2024-01-18 08:12:23","title":"Querying Easily Flip-flopped Samples for Deep Active Learning","abstract":"Active learning is a machine learning paradigm that aims to improve the performance of a model by strategically selecting and querying unlabeled data. One effective selection strategy is to base it on the model's predictive uncertainty, which can be interpreted as a measure of how informative a sample is. The sample's distance to the decision boundary is a natural measure of predictive uncertainty, but it is often intractable to compute, especially for complex decision boundaries formed in multiclass classification tasks. To address this issue, this paper proposes the {\\it least disagree metric} (LDM), defined as the smallest probability of disagreement of the predicted label, and an estimator for LDM proven to be asymptotically consistent under mild assumptions. The estimator is computationally efficient and can be easily implemented for deep learning models using parameter perturbation. The LDM-based active learning is performed by querying unlabeled data with the smallest LDM. Experimental results show that our LDM-based active learning algorithm obtains state-of-the-art overall performance on all considered datasets and deep architectures.","sentences":["Active learning is a machine learning paradigm that aims to improve the performance of a model by strategically selecting and querying unlabeled data.","One effective selection strategy is to base it on the model's predictive uncertainty, which can be interpreted as a measure of how informative a sample is.","The sample's distance to the decision boundary is a natural measure of predictive uncertainty, but it is often intractable to compute, especially for complex decision boundaries formed in multiclass classification tasks.","To address this issue, this paper proposes the {\\it least disagree metric} (LDM), defined as the smallest probability of disagreement of the predicted label, and an estimator for LDM proven to be asymptotically consistent under mild assumptions.","The estimator is computationally efficient and can be easily implemented for deep learning models using parameter perturbation.","The LDM-based active learning is performed by querying unlabeled data with the smallest LDM.","Experimental results show that our LDM-based active learning algorithm obtains state-of-the-art overall performance on all considered datasets and deep architectures."],"url":"http://arxiv.org/abs/2401.09787v1"}
{"created":"2024-01-18 07:42:51","title":"Robotic Test Tube Rearrangement Using Combined Reinforcement Learning and Motion Planning","abstract":"A combined task-level reinforcement learning and motion planning framework is proposed in this paper to address a multi-class in-rack test tube rearrangement problem. At the task level, the framework uses reinforcement learning to infer a sequence of swap actions while ignoring robotic motion details. At the motion level, the framework accepts the swapping action sequences inferred by task-level agents and plans the detailed robotic pick-and-place motion. The task and motion-level planning form a closed loop with the help of a condition set maintained for each rack slot, which allows the framework to perform replanning and effectively find solutions in the presence of low-level failures. Particularly for reinforcement learning, the framework leverages a distributed deep Q-learning structure with the Dueling Double Deep Q Network (D3QN) to acquire near-optimal policies and uses an A${}^\\star$-based post-processing technique to amplify the collected training data. The D3QN and distributed learning help increase training efficiency. The post-processing helps complete unfinished action sequences and remove redundancy, thus making the training data more effective. We carry out both simulations and real-world studies to understand the performance of the proposed framework. The results verify the performance of the RL and post-processing and show that the closed-loop combination improves robustness. The framework is ready to incorporate various sensory feedback. The real-world studies also demonstrated the incorporation.","sentences":["A combined task-level reinforcement learning and motion planning framework is proposed in this paper to address a multi-class in-rack test tube rearrangement problem.","At the task level, the framework uses reinforcement learning to infer a sequence of swap actions while ignoring robotic motion details.","At the motion level, the framework accepts the swapping action sequences inferred by task-level agents and plans the detailed robotic pick-and-place motion.","The task and motion-level planning form a closed loop with the help of a condition set maintained for each rack slot, which allows the framework to perform replanning and effectively find solutions in the presence of low-level failures.","Particularly for reinforcement learning, the framework leverages a distributed deep Q-learning structure with the Dueling Double Deep Q Network (D3QN) to acquire near-optimal policies and uses an A${}^\\star$-based post-processing technique to amplify the collected training data.","The D3QN and distributed learning help increase training efficiency.","The post-processing helps complete unfinished action sequences and remove redundancy, thus making the training data more effective.","We carry out both simulations and real-world studies to understand the performance of the proposed framework.","The results verify the performance of the RL and post-processing and show that the closed-loop combination improves robustness.","The framework is ready to incorporate various sensory feedback.","The real-world studies also demonstrated the incorporation."],"url":"http://arxiv.org/abs/2401.09772v1"}
{"created":"2024-01-18 07:38:18","title":"Reliability-based G1 Continuous Arc Spline Approximation","abstract":"In this paper, we present an algorithm to approximate a set of data points with G1 continuous arcs, using points' covariance data. To the best of our knowledge, previous arc spline approximation approaches assumed that all data points contribute equally (i.e. have the same weights) during the approximation process. However, this assumption may cause serious instability in the algorithm, if the collected data contains outliers. To resolve this issue, a robust method for arc spline approximation is suggested in this work, assuming that the 2D covariance for each data point is given. Starting with the definition of models and parameters for single arc approximation, the framework is extended to multiple-arc approximation for general usage. Then the proposed algorithm is verified using generated noisy data and real-world collected data via vehicle experiment in Sejong City, South Korea.","sentences":["In this paper, we present an algorithm to approximate a set of data points with G1 continuous arcs, using points' covariance data.","To the best of our knowledge, previous arc spline approximation approaches assumed that all data points contribute equally (i.e. have the same weights) during the approximation process.","However, this assumption may cause serious instability in the algorithm, if the collected data contains outliers.","To resolve this issue, a robust method for arc spline approximation is suggested in this work, assuming that the 2D covariance for each data point is given.","Starting with the definition of models and parameters for single arc approximation, the framework is extended to multiple-arc approximation for general usage.","Then the proposed algorithm is verified using generated noisy data and real-world collected data via vehicle experiment in Sejong City, South Korea."],"url":"http://arxiv.org/abs/2401.09770v1"}
{"created":"2024-01-18 07:36:38","title":"Towards Learning from Graphs with Heterophily: Progress and Future","abstract":"Graphs are structured data that models complex relations between real-world entities. Heterophilous graphs, where linked nodes are prone to be with different labels or dissimilar features, have recently attracted significant attention and found many applications. Meanwhile, increasing efforts have been made to advance learning from heterophilous graphs. Although there exist surveys on the relevant topic, they focus on heterophilous GNNs, which are only sub-topics of heterophilous graph learning. In this survey, we comprehensively overview existing works on learning from graphs with heterophily.First, we collect over 180 publications and introduce the development of this field. Then, we systematically categorize existing methods based on a hierarchical taxonomy including learning strategies, model architectures and practical applications. Finally, we discuss the primary challenges of existing studies and highlight promising avenues for future research.More publication details and corresponding open-source codes can be accessed and will be continuously updated at our repositories:https://github.com/gongchenghua/Awesome-Survey-Graphs-with-Heterophily.","sentences":["Graphs are structured data that models complex relations between real-world entities.","Heterophilous graphs, where linked nodes are prone to be with different labels or dissimilar features, have recently attracted significant attention and found many applications.","Meanwhile, increasing efforts have been made to advance learning from heterophilous graphs.","Although there exist surveys on the relevant topic, they focus on heterophilous GNNs, which are only sub-topics of heterophilous graph learning.","In this survey, we comprehensively overview existing works on learning from graphs with heterophily.","First, we collect over 180 publications and introduce the development of this field.","Then, we systematically categorize existing methods based on a hierarchical taxonomy including learning strategies, model architectures and practical applications.","Finally, we discuss the primary challenges of existing studies and highlight promising avenues for future research.","More publication details and corresponding open-source codes can be accessed and will be continuously updated at our repositories:https://github.com/gongchenghua/Awesome-Survey-Graphs-with-Heterophily."],"url":"http://arxiv.org/abs/2401.09769v1"}
{"created":"2024-01-18 07:29:54","title":"Generation of weighted trees, block trees and block graphs","abstract":"We present a general framework to generate trees every vertex of which has a non-negative weight and a color. The colors are used to impose certain restrictions on the weight and colors of other vertices. We first extend the enumeration algorithms of unweighted trees given in [19, 20] to generate weighted trees that allow zero weight. We avoid isomorphisms by generalizing the concept of centroids to weighted trees and then using the so-called centroid-rooted canonical weighted trees. We provide a time complexity analysis of unranking algorithms and also show that the output delay complexity of enumeration is linear. The framework can be used to generate graph classes taking advantage of their tree-based decompositions/representations. We demonstrate our framework by generating weighted block trees which are in one-to-one correspondence with connected block graphs. All connected block graphs up to 19 vertices are publicly available at [1].","sentences":["We present a general framework to generate trees every vertex of which has a non-negative weight and a color.","The colors are used to impose certain restrictions on the weight and colors of other vertices.","We first extend the enumeration algorithms of unweighted trees given in [19, 20] to generate weighted trees that allow zero weight.","We avoid isomorphisms by generalizing the concept of centroids to weighted trees and then using the so-called centroid-rooted canonical weighted trees.","We provide a time complexity analysis of unranking algorithms and also show that the output delay complexity of enumeration is linear.","The framework can be used to generate graph classes taking advantage of their tree-based decompositions/representations.","We demonstrate our framework by generating weighted block trees which are in one-to-one correspondence with connected block graphs.","All connected block graphs up to 19 vertices are publicly available at [1]."],"url":"http://arxiv.org/abs/2401.09764v1"}
{"created":"2024-01-18 07:28:17","title":"CLIP Model for Images to Textual Prompts Based on Top-k Neighbors","abstract":"Text-to-image synthesis, a subfield of multimodal generation, has gained significant attention in recent years. We propose a cost-effective approach for image-to-prompt generation that leverages generative models to generate textual prompts without the need for large amounts of annotated data. We divide our method into two stages: online stage and offline stage. We use a combination of the CLIP model and K-nearest neighbors (KNN) algorithm. The proposed system consists of two main parts: an offline task and an online task. Our method owns the highest metric 0.612 among these models, which is 0.013, 0.055, 0.011 higher than Clip, Clip + KNN(top 10) respectively.","sentences":["Text-to-image synthesis, a subfield of multimodal generation, has gained significant attention in recent years.","We propose a cost-effective approach for image-to-prompt generation that leverages generative models to generate textual prompts without the need for large amounts of annotated data.","We divide our method into two stages: online stage and offline stage.","We use a combination of the CLIP model and K-nearest neighbors (KNN) algorithm.","The proposed system consists of two main parts: an offline task and an online task.","Our method owns the highest metric 0.612 among these models, which is 0.013, 0.055, 0.011 higher than Clip, Clip + KNN(top 10) respectively."],"url":"http://arxiv.org/abs/2401.09763v1"}
{"created":"2024-01-18 07:23:51","title":"A Comparative Study on Annotation Quality of Crowdsourcing and LLM via Label Aggregation","abstract":"Whether Large Language Models (LLMs) can outperform crowdsourcing on the data annotation task is attracting interest recently. Some works verified this issue with the average performance of individual crowd workers and LLM workers on some specific NLP tasks by collecting new datasets. However, on the one hand, existing datasets for the studies of annotation quality in crowdsourcing are not yet utilized in such evaluations, which potentially provide reliable evaluations from a different viewpoint. On the other hand, the quality of these aggregated labels is crucial because, when utilizing crowdsourcing, the estimated labels aggregated from multiple crowd labels to the same instances are the eventually collected labels. Therefore, in this paper, we first investigate which existing crowdsourcing datasets can be used for a comparative study and create a benchmark. We then compare the quality between individual crowd labels and LLM labels and make the evaluations on the aggregated labels. In addition, we propose a Crowd-LLM hybrid label aggregation method and verify the performance. We find that adding LLM labels from good LLMs to existing crowdsourcing datasets can enhance the quality of the aggregated labels of the datasets, which is also higher than the quality of LLM labels themselves.","sentences":["Whether Large Language Models (LLMs) can outperform crowdsourcing on the data annotation task is attracting interest recently.","Some works verified this issue with the average performance of individual crowd workers and LLM workers on some specific NLP tasks by collecting new datasets.","However, on the one hand, existing datasets for the studies of annotation quality in crowdsourcing are not yet utilized in such evaluations, which potentially provide reliable evaluations from a different viewpoint.","On the other hand, the quality of these aggregated labels is crucial because, when utilizing crowdsourcing, the estimated labels aggregated from multiple crowd labels to the same instances are the eventually collected labels.","Therefore, in this paper, we first investigate which existing crowdsourcing datasets can be used for a comparative study and create a benchmark.","We then compare the quality between individual crowd labels and LLM labels and make the evaluations on the aggregated labels.","In addition, we propose a Crowd-LLM hybrid label aggregation method and verify the performance.","We find that adding LLM labels from good LLMs to existing crowdsourcing datasets can enhance the quality of the aggregated labels of the datasets, which is also higher than the quality of LLM labels themselves."],"url":"http://arxiv.org/abs/2401.09760v1"}
{"created":"2024-01-18 07:07:42","title":"Explaining Drift using Shapley Values","abstract":"Machine learning models often deteriorate in their performance when they are used to predict the outcomes over data on which they were not trained. These scenarios can often arise in real world when the distribution of data changes gradually or abruptly due to major events like a pandemic. There have been many attempts in machine learning research to come up with techniques that are resilient to such Concept drifts. However, there is no principled framework to identify the drivers behind the drift in model performance. In this paper, we propose a novel framework - DBShap that uses Shapley values to identify the main contributors of the drift and quantify their respective contributions. The proposed framework not only quantifies the importance of individual features in driving the drift but also includes the change in the underlying relation between the input and output as a possible driver. The explanation provided by DBShap can be used to understand the root cause behind the drift and use it to make the model resilient to the drift.","sentences":["Machine learning models often deteriorate in their performance when they are used to predict the outcomes over data on which they were not trained.","These scenarios can often arise in real world when the distribution of data changes gradually or abruptly due to major events like a pandemic.","There have been many attempts in machine learning research to come up with techniques that are resilient to such Concept drifts.","However, there is no principled framework to identify the drivers behind the drift in model performance.","In this paper, we propose a novel framework - DBShap that uses Shapley values to identify the main contributors of the drift and quantify their respective contributions.","The proposed framework not only quantifies the importance of individual features in driving the drift but also includes the change in the underlying relation between the input and output as a possible driver.","The explanation provided by DBShap can be used to understand the root cause behind the drift and use it to make the model resilient to the drift."],"url":"http://arxiv.org/abs/2401.09756v1"}
{"created":"2024-01-18 06:57:29","title":"Universally Robust Graph Neural Networks by Preserving Neighbor Similarity","abstract":"Despite the tremendous success of graph neural networks in learning relational data, it has been widely investigated that graph neural networks are vulnerable to structural attacks on homophilic graphs. Motivated by this, a surge of robust models is crafted to enhance the adversarial robustness of graph neural networks on homophilic graphs. However, the vulnerability based on heterophilic graphs remains a mystery to us. To bridge this gap, in this paper, we start to explore the vulnerability of graph neural networks on heterophilic graphs and theoretically prove that the update of the negative classification loss is negatively correlated with the pairwise similarities based on the powered aggregated neighbor features. This theoretical proof explains the empirical observations that the graph attacker tends to connect dissimilar node pairs based on the similarities of neighbor features instead of ego features both on homophilic and heterophilic graphs. In this way, we novelly introduce a novel robust model termed NSPGNN which incorporates a dual-kNN graphs pipeline to supervise the neighbor similarity-guided propagation. This propagation utilizes the low-pass filter to smooth the features of node pairs along the positive kNN graphs and the high-pass filter to discriminate the features of node pairs along the negative kNN graphs. Extensive experiments on both homophilic and heterophilic graphs validate the universal robustness of NSPGNN compared to the state-of-the-art methods.","sentences":["Despite the tremendous success of graph neural networks in learning relational data, it has been widely investigated that graph neural networks are vulnerable to structural attacks on homophilic graphs.","Motivated by this, a surge of robust models is crafted to enhance the adversarial robustness of graph neural networks on homophilic graphs.","However, the vulnerability based on heterophilic graphs remains a mystery to us.","To bridge this gap, in this paper, we start to explore the vulnerability of graph neural networks on heterophilic graphs and theoretically prove that the update of the negative classification loss is negatively correlated with the pairwise similarities based on the powered aggregated neighbor features.","This theoretical proof explains the empirical observations that the graph attacker tends to connect dissimilar node pairs based on the similarities of neighbor features instead of ego features both on homophilic and heterophilic graphs.","In this way, we novelly introduce a novel robust model termed NSPGNN which incorporates a dual-kNN graphs pipeline to supervise the neighbor similarity-guided propagation.","This propagation utilizes the low-pass filter to smooth the features of node pairs along the positive kNN graphs and the high-pass filter to discriminate the features of node pairs along the negative kNN graphs.","Extensive experiments on both homophilic and heterophilic graphs validate the universal robustness of NSPGNN compared to the state-of-the-art methods."],"url":"http://arxiv.org/abs/2401.09754v1"}
{"created":"2024-01-18 06:57:05","title":"Applications of Machine Learning to Optimizing Polyolefin Manufacturing","abstract":"This chapter is a preprint from our book by , focusing on leveraging machine learning (ML) in chemical and polyolefin manufacturing optimization. It's crafted for both novices and seasoned professionals keen on the latest ML applications in chemical processes. We trace the evolution of AI and ML in chemical industries, delineate core ML components, and provide resources for ML beginners. A detailed discussion on various ML methods is presented, covering regression, classification, and unsupervised learning techniques, with performance metrics and examples. Ensemble methods, deep learning networks, including MLP, DNNs, RNNs, CNNs, and transformers, are explored for their growing role in chemical applications. Practical workshops guide readers through predictive modeling using advanced ML algorithms. The chapter culminates with insights into science-guided ML, advocating for a hybrid approach that enhances model accuracy. The extensive bibliography offers resources for further research and practical implementation. This chapter aims to be a thorough primer on ML's practical application in chemical engineering, particularly for polyolefin production, and sets the stage for continued learning in subsequent chapters. Please cite the original work [169,170] when referencing.","sentences":["This chapter is a preprint from our book by , focusing on leveraging machine learning (ML) in chemical and polyolefin manufacturing optimization.","It's crafted for both novices and seasoned professionals keen on the latest ML applications in chemical processes.","We trace the evolution of AI and ML in chemical industries, delineate core ML components, and provide resources for ML beginners.","A detailed discussion on various ML methods is presented, covering regression, classification, and unsupervised learning techniques, with performance metrics and examples.","Ensemble methods, deep learning networks, including MLP, DNNs, RNNs, CNNs, and transformers, are explored for their growing role in chemical applications.","Practical workshops guide readers through predictive modeling using advanced ML algorithms.","The chapter culminates with insights into science-guided ML, advocating for a hybrid approach that enhances model accuracy.","The extensive bibliography offers resources for further research and practical implementation.","This chapter aims to be a thorough primer on ML's practical application in chemical engineering, particularly for polyolefin production, and sets the stage for continued learning in subsequent chapters.","Please cite the original work [169,170] when referencing."],"url":"http://arxiv.org/abs/2401.09753v1"}
{"created":"2024-01-18 06:52:52","title":"Improving Speaker-independent Speech Emotion Recognition Using Dynamic Joint Distribution Adaptation","abstract":"In speaker-independent speech emotion recognition, the training and testing samples are collected from diverse speakers, leading to a multi-domain shift challenge across the feature distributions of data from different speakers. Consequently, when the trained model is confronted with data from new speakers, its performance tends to degrade. To address the issue, we propose a Dynamic Joint Distribution Adaptation (DJDA) method under the framework of multi-source domain adaptation. DJDA firstly utilizes joint distribution adaptation (JDA), involving marginal distribution adaptation (MDA) and conditional distribution adaptation (CDA), to more precisely measure the multi-domain distribution shifts caused by different speakers. This helps eliminate speaker bias in emotion features, allowing for learning discriminative and speaker-invariant speech emotion features from coarse-level to fine-level. Furthermore, we quantify the adaptation contributions of MDA and CDA within JDA by using a dynamic balance factor based on $\\mathcal{A}$-Distance, promoting to effectively handle the unknown distributions encountered in data from new speakers. Experimental results demonstrate the superior performance of our DJDA as compared to other state-of-the-art (SOTA) methods.","sentences":["In speaker-independent speech emotion recognition, the training and testing samples are collected from diverse speakers, leading to a multi-domain shift challenge across the feature distributions of data from different speakers.","Consequently, when the trained model is confronted with data from new speakers, its performance tends to degrade.","To address the issue, we propose a Dynamic Joint Distribution Adaptation (DJDA) method under the framework of multi-source domain adaptation.","DJDA firstly utilizes joint distribution adaptation (JDA), involving marginal distribution adaptation (MDA) and conditional distribution adaptation (CDA), to more precisely measure the multi-domain distribution shifts caused by different speakers.","This helps eliminate speaker bias in emotion features, allowing for learning discriminative and speaker-invariant speech emotion features from coarse-level to fine-level.","Furthermore, we quantify the adaptation contributions of MDA and CDA within JDA by using a dynamic balance factor based on $\\mathcal{A}$-Distance, promoting to effectively handle the unknown distributions encountered in data from new speakers.","Experimental results demonstrate the superior performance of our DJDA as compared to other state-of-the-art (SOTA) methods."],"url":"http://arxiv.org/abs/2401.09752v1"}
{"created":"2024-01-18 06:19:05","title":"Bootstrapping OTS-Funcimg Pre-training Model (Botfip) -- A Comprehensive Symbolic Regression Framework","abstract":"In the field of scientific computing, many problem-solving approaches tend to focus only on the process and final outcome, even in AI for science, there is a lack of deep multimodal information mining behind the data, missing a multimodal framework akin to that in the image-text domain. In this paper, we take Symbolic Regression(SR) as our focal point and, drawing inspiration from the BLIP model in the image-text domain, propose a scientific computing multimodal framework based on Function Images (Funcimg) and Operation Tree Sequence (OTS), named Bootstrapping OTS-Funcimg Pre-training Model (Botfip). In SR experiments, we validate the advantages of Botfip in low-complexity SR problems, showcasing its potential. As a MED framework, Botfip holds promise for future applications in a broader range of scientific computing problems.","sentences":["In the field of scientific computing, many problem-solving approaches tend to focus only on the process and final outcome, even in AI for science, there is a lack of deep multimodal information mining behind the data, missing a multimodal framework akin to that in the image-text domain.","In this paper, we take Symbolic Regression(SR) as our focal point and, drawing inspiration from the BLIP model in the image-text domain, propose a scientific computing multimodal framework based on Function Images (Funcimg) and Operation Tree Sequence (OTS), named Bootstrapping OTS-Funcimg Pre-training Model (Botfip).","In SR experiments, we validate the advantages of Botfip in low-complexity SR problems, showcasing its potential.","As a MED framework, Botfip holds promise for future applications in a broader range of scientific computing problems."],"url":"http://arxiv.org/abs/2401.09748v1"}
{"created":"2024-01-18 05:48:56","title":"Hijacking Attacks against Neural Networks by Analyzing Training Data","abstract":"Backdoors and adversarial examples are the two primary threats currently faced by deep neural networks (DNNs). Both attacks attempt to hijack the model behaviors with unintended outputs by introducing (small) perturbations to the inputs. Backdoor attacks, despite the high success rates, often require a strong assumption, which is not always easy to achieve in reality. Adversarial example attacks, which put relatively weaker assumptions on attackers, often demand high computational resources, yet do not always yield satisfactory success rates when attacking mainstream black-box models in the real world. These limitations motivate the following research question: can model hijacking be achieved more simply, with a higher attack success rate and more reasonable assumptions? In this paper, we propose CleanSheet, a new model hijacking attack that obtains the high performance of backdoor attacks without requiring the adversary to tamper with the model training process. CleanSheet exploits vulnerabilities in DNNs stemming from the training data. Specifically, our key idea is to treat part of the clean training data of the target model as \"poisoned data,\" and capture the characteristics of these data that are more sensitive to the model (typically called robust features) to construct \"triggers.\" These triggers can be added to any input example to mislead the target model, similar to backdoor attacks. We validate the effectiveness of CleanSheet through extensive experiments on 5 datasets, 79 normally trained models, 68 pruned models, and 39 defensive models. Results show that CleanSheet exhibits performance comparable to state-of-the-art backdoor attacks, achieving an average attack success rate (ASR) of 97.5% on CIFAR-100 and 92.4% on GTSRB, respectively. Furthermore, CleanSheet consistently maintains a high ASR, when confronted with various mainstream backdoor defenses.","sentences":["Backdoors and adversarial examples are the two primary threats currently faced by deep neural networks (DNNs).","Both attacks attempt to hijack the model behaviors with unintended outputs by introducing (small) perturbations to the inputs.","Backdoor attacks, despite the high success rates, often require a strong assumption, which is not always easy to achieve in reality.","Adversarial example attacks, which put relatively weaker assumptions on attackers, often demand high computational resources, yet do not always yield satisfactory success rates when attacking mainstream black-box models in the real world.","These limitations motivate the following research question: can model hijacking be achieved more simply, with a higher attack success rate and more reasonable assumptions?","In this paper, we propose CleanSheet, a new model hijacking attack that obtains the high performance of backdoor attacks without requiring the adversary to tamper with the model training process.","CleanSheet exploits vulnerabilities in DNNs stemming from the training data.","Specifically, our key idea is to treat part of the clean training data of the target model as \"poisoned data,\" and capture the characteristics of these data that are more sensitive to the model (typically called robust features) to construct \"triggers.\"","These triggers can be added to any input example to mislead the target model, similar to backdoor attacks.","We validate the effectiveness of CleanSheet through extensive experiments on 5 datasets, 79 normally trained models, 68 pruned models, and 39 defensive models.","Results show that CleanSheet exhibits performance comparable to state-of-the-art backdoor attacks, achieving an average attack success rate (ASR) of 97.5% on CIFAR-100 and 92.4% on GTSRB, respectively.","Furthermore, CleanSheet consistently maintains a high ASR, when confronted with various mainstream backdoor defenses."],"url":"http://arxiv.org/abs/2401.09740v1"}
{"created":"2024-01-18 05:31:53","title":"Measuring the Discrepancy between 3D Geometric Models using Directional Distance Fields","abstract":"Qualifying the discrepancy between 3D geometric models, which could be represented with either point clouds or triangle meshes, is a pivotal issue with board applications. Existing methods mainly focus on directly establishing the correspondence between two models and then aggregating point-wise distance between corresponding points, resulting in them being either inefficient or ineffective. In this paper, we propose DirDist, an efficient, effective, robust, and differentiable distance metric for 3D geometry data. Specifically, we construct DirDist based on the proposed implicit representation of 3D models, namely directional distance field (DDF), which defines the directional distances of 3D points to a model to capture its local surface geometry. We then transfer the discrepancy between two 3D geometric models as the discrepancy between their DDFs defined on an identical domain, naturally establishing model correspondence. To demonstrate the advantage of our DirDist, we explore various distance metric-driven 3D geometric modeling tasks, including template surface fitting, rigid registration, non-rigid registration, scene flow estimation and human pose optimization. Extensive experiments show that our DirDist achieves significantly higher accuracy under all tasks. As a generic distance metric, DirDist has the potential to advance the field of 3D geometric modeling. The source code is available at \\url{https://github.com/rsy6318/DirDist}.","sentences":["Qualifying the discrepancy between 3D geometric models, which could be represented with either point clouds or triangle meshes, is a pivotal issue with board applications.","Existing methods mainly focus on directly establishing the correspondence between two models and then aggregating point-wise distance between corresponding points, resulting in them being either inefficient or ineffective.","In this paper, we propose DirDist, an efficient, effective, robust, and differentiable distance metric for 3D geometry data.","Specifically, we construct DirDist based on the proposed implicit representation of 3D models, namely directional distance field (DDF), which defines the directional distances of 3D points to a model to capture its local surface geometry.","We then transfer the discrepancy between two 3D geometric models as the discrepancy between their DDFs defined on an identical domain, naturally establishing model correspondence.","To demonstrate the advantage of our DirDist, we explore various distance metric-driven 3D geometric modeling tasks, including template surface fitting, rigid registration, non-rigid registration, scene flow estimation and human pose optimization.","Extensive experiments show that our DirDist achieves significantly higher accuracy under all tasks.","As a generic distance metric, DirDist has the potential to advance the field of 3D geometric modeling.","The source code is available at \\url{https://github.com/rsy6318/DirDist}."],"url":"http://arxiv.org/abs/2401.09736v1"}
{"created":"2024-01-18 05:22:03","title":"ASA -- The Adaptive Scheduling Algorithm","abstract":"In High Performance Computing (HPC) infrastructures, the control of resources by batch systems can lead to prolonged queue waiting times and adverse effects on the overall execution times of applications, particularly in data-intensive and low-latency workflows where efficient processing hinges on resource planning and timely allocation. Allocating the maximum capacity upfront ensures the fastest execution but results in spare and idle resources, extended queue waits, and costly usage. Conversely, dynamic allocation based on workflow stage requirements optimizes resource usage but may negatively impact the total workflow makespan. To address these issues, we introduce ASA, the Adaptive Scheduling Algorithm. ASA is a novel, convergence-proven scheduling technique that minimizes jobs inter-stage waiting times by estimating the queue waiting times to proactively submit resource change requests ahead of time. It strikes a balance between exploration and exploitation, considering both learning (waiting times) and applying learnt insights. Real-world experiments over two supercomputers centers with scientific workflows demonstrate ASA's effectiveness, achieving near-optimal resource utilization and accuracy, with up to 10% and 2% reductions in average workflow queue waiting times and makespan, respectively.","sentences":["In High Performance Computing (HPC) infrastructures, the control of resources by batch systems can lead to prolonged queue waiting times and adverse effects on the overall execution times of applications, particularly in data-intensive and low-latency workflows where efficient processing hinges on resource planning and timely allocation.","Allocating the maximum capacity upfront ensures the fastest execution but results in spare and idle resources, extended queue waits, and costly usage.","Conversely, dynamic allocation based on workflow stage requirements optimizes resource usage but may negatively impact the total workflow makespan.","To address these issues, we introduce ASA, the Adaptive Scheduling Algorithm.","ASA is a novel, convergence-proven scheduling technique that minimizes jobs inter-stage waiting times by estimating the queue waiting times to proactively submit resource change requests ahead of time.","It strikes a balance between exploration and exploitation, considering both learning (waiting times) and applying learnt insights.","Real-world experiments over two supercomputers centers with scientific workflows demonstrate ASA's effectiveness, achieving near-optimal resource utilization and accuracy, with up to 10% and 2% reductions in average workflow queue waiting times and makespan, respectively."],"url":"http://arxiv.org/abs/2401.09733v1"}
{"created":"2024-01-18 05:20:07","title":"Instance Brownian Bridge as Texts for Open-vocabulary Video Instance Segmentation","abstract":"Temporally locating objects with arbitrary class texts is the primary pursuit of open-vocabulary Video Instance Segmentation (VIS). Because of the insufficient vocabulary of video data, previous methods leverage image-text pretraining model for recognizing object instances by separately aligning each frame and class texts, ignoring the correlation between frames. As a result, the separation breaks the instance movement context of videos, causing inferior alignment between video and text. To tackle this issue, we propose to link frame-level instance representations as a Brownian Bridge to model instance dynamics and align bridge-level instance representation to class texts for more precisely open-vocabulary VIS (BriVIS). Specifically, we build our system upon a frozen video segmentor to generate frame-level instance queries, and design Temporal Instance Resampler (TIR) to generate queries with temporal context from frame queries. To mold instance queries to follow Brownian bridge and accomplish alignment with class texts, we design Bridge-Text Alignment (BTA) to learn discriminative bridge-level representations of instances via contrastive objectives. Setting MinVIS as the basic video segmentor, BriVIS surpasses the Open-vocabulary SOTA (OV2Seg) by a clear margin. For example, on the challenging large-vocabulary VIS dataset (BURST), BriVIS achieves 7.43 mAP and exhibits 49.49% improvement compared to OV2Seg (4.97 mAP).","sentences":["Temporally locating objects with arbitrary class texts is the primary pursuit of open-vocabulary Video Instance Segmentation (VIS).","Because of the insufficient vocabulary of video data, previous methods leverage image-text pretraining model for recognizing object instances by separately aligning each frame and class texts, ignoring the correlation between frames.","As a result, the separation breaks the instance movement context of videos, causing inferior alignment between video and text.","To tackle this issue, we propose to link frame-level instance representations as a Brownian Bridge to model instance dynamics and align bridge-level instance representation to class texts for more precisely open-vocabulary VIS (BriVIS).","Specifically, we build our system upon a frozen video segmentor to generate frame-level instance queries, and design Temporal Instance Resampler (TIR) to generate queries with temporal context from frame queries.","To mold instance queries to follow Brownian bridge and accomplish alignment with class texts, we design Bridge-Text Alignment (BTA) to learn discriminative bridge-level representations of instances via contrastive objectives.","Setting MinVIS as the basic video segmentor, BriVIS surpasses the Open-vocabulary SOTA (OV2Seg) by a clear margin.","For example, on the challenging large-vocabulary VIS dataset (BURST), BriVIS achieves 7.43 mAP and exhibits 49.49% improvement compared to OV2Seg (4.97 mAP)."],"url":"http://arxiv.org/abs/2401.09732v1"}
{"created":"2024-01-18 04:10:20","title":"SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model","abstract":"Large language models (LLMs) have recently been extended to the vision-language realm, obtaining impressive general multi-modal capabilities. However, the exploration of multi-modal large language models (MLLMs) for remote sensing (RS) data is still in its infancy, and the performance is not satisfactory. In this work, we introduce SkyEyeGPT, a unified multi-modal large language model specifically designed for RS vision-language understanding. To this end, we meticulously curate an RS multi-modal instruction tuning dataset, including single-task and multi-task conversation instructions. After manual verification, we obtain a high-quality RS instruction-following dataset with 968k samples. Our research demonstrates that with a simple yet effective design, SkyEyeGPT works surprisingly well on considerably different tasks without the need for extra encoding modules. Specifically, after projecting RS visual features to the language domain via an alignment layer, they are fed jointly with task-specific instructions into an LLM-based RS decoder to predict answers for RS open-ended tasks. In addition, we design a two-stage tuning method to enhance instruction-following and multi-turn dialogue ability at different granularities. Experiments on 8 datasets for RS vision-language tasks demonstrate SkyEyeGPT's superiority in image-level and region-level tasks, such as captioning and visual grounding. In particular, SkyEyeGPT exhibits encouraging results compared to GPT-4V in some qualitative tests. The online demo, code, and dataset will be released in https://github.com/ZhanYang-nwpu/SkyEyeGPT.","sentences":["Large language models (LLMs) have recently been extended to the vision-language realm, obtaining impressive general multi-modal capabilities.","However, the exploration of multi-modal large language models (MLLMs) for remote sensing (RS) data is still in its infancy, and the performance is not satisfactory.","In this work, we introduce SkyEyeGPT, a unified multi-modal large language model specifically designed for RS vision-language understanding.","To this end, we meticulously curate an RS multi-modal instruction tuning dataset, including single-task and multi-task conversation instructions.","After manual verification, we obtain a high-quality RS instruction-following dataset with 968k samples.","Our research demonstrates that with a simple yet effective design, SkyEyeGPT works surprisingly well on considerably different tasks without the need for extra encoding modules.","Specifically, after projecting RS visual features to the language domain via an alignment layer, they are fed jointly with task-specific instructions into an LLM-based RS decoder to predict answers for RS open-ended tasks.","In addition, we design a two-stage tuning method to enhance instruction-following and multi-turn dialogue ability at different granularities.","Experiments on 8 datasets for RS vision-language tasks demonstrate SkyEyeGPT's superiority in image-level and region-level tasks, such as captioning and visual grounding.","In particular, SkyEyeGPT exhibits encouraging results compared to GPT-4V in some qualitative tests.","The online demo, code, and dataset will be released in https://github.com/ZhanYang-nwpu/SkyEyeGPT."],"url":"http://arxiv.org/abs/2401.09712v1"}
{"created":"2024-01-18 04:04:04","title":"Joint Beam Direction Control and Radio Resource Allocation in Dynamic Multi-beam LEO Satellite Networks","abstract":"Multi-beam low earth orbit (LEO) satellites are emerging as key components in beyond 5G and 6G to provide global coverage and high data rate. To fully unleash the potential of LEO satellite communication, resource management plays a key role. However, the uneven distribution of users, the coupling of multi-dimensional resources, complex inter-beam interference, and time-varying network topologies all impose significant challenges on effective communication resource management. In this paper, we study the joint optimization of beam direction and the allocation of spectrum, time, and power resource in a dynamic multi-beam LEO satellite network. The objective is to improve long-term user sum data rate while taking user fairness into account. Since the concerned resource management problem is mixed-integer non-convex programming, the problem is decomposed into three subproblems, namely beam direction control and time slot allocation, user subchannel assignment, and beam power allocation. Then, these subproblems are solved iteratively by leveraging matching with externalities and successive convex approximation, and the proposed algorithms are analyzed in terms of stability, convergence, and complexity. Extensive simulations are conducted, and the results demonstrate that our proposal can improve the number of served users by up to two times and the sum user data rate by up to 68%, compared to baseline schemes.","sentences":["Multi-beam low earth orbit (LEO) satellites are emerging as key components in beyond 5G and 6G to provide global coverage and high data rate.","To fully unleash the potential of LEO satellite communication, resource management plays a key role.","However, the uneven distribution of users, the coupling of multi-dimensional resources, complex inter-beam interference, and time-varying network topologies all impose significant challenges on effective communication resource management.","In this paper, we study the joint optimization of beam direction and the allocation of spectrum, time, and power resource in a dynamic multi-beam LEO satellite network.","The objective is to improve long-term user sum data rate while taking user fairness into account.","Since the concerned resource management problem is mixed-integer non-convex programming, the problem is decomposed into three subproblems, namely beam direction control and time slot allocation, user subchannel assignment, and beam power allocation.","Then, these subproblems are solved iteratively by leveraging matching with externalities and successive convex approximation, and the proposed algorithms are analyzed in terms of stability, convergence, and complexity.","Extensive simulations are conducted, and the results demonstrate that our proposal can improve the number of served users by up to two times and the sum user data rate by up to 68%, compared to baseline schemes."],"url":"http://arxiv.org/abs/2401.09711v1"}
{"created":"2024-01-18 03:32:10","title":"A HPC Co-Scheduler with Reinforcement Learning","abstract":"Although High Performance Computing (HPC) users understand basic resource requirements such as the number of CPUs and memory limits, internal infrastructural utilization data is exclusively leveraged by cluster operators, who use it to configure batch schedulers. This task is challenging and increasingly complex due to ever larger cluster scales and heterogeneity of modern scientific workflows. As a result, HPC systems achieve low utilization with long job completion times (makespans). To tackle these challenges, we propose a co-scheduling algorithm based on an adaptive reinforcement learning algorithm, where application profiling is combined with cluster monitoring. The resulting cluster scheduler matches resource utilization to application performance in a fine-grained manner (i.e., operating system level). As opposed to nominal allocations, we apply decision trees to model applications' actual resource usage, which are used to estimate how much resource capacity from one allocation can be co-allocated to additional applications. Our algorithm learns from incorrect co-scheduling decisions and adapts from changing environment conditions, and evaluates when such changes cause resource contention that impacts quality of service metrics such as jobs slowdowns. We integrate our algorithm in an HPC resource manager that combines Slurm and Mesos for job scheduling and co-allocation, respectively. Our experimental evaluation performed in a dedicated cluster executing a mix of four real different scientific workflows demonstrates improvements on cluster utilization of up to 51% even in high load scenarios, with 55% average queue makespan reductions under low loads.","sentences":["Although High Performance Computing (HPC) users understand basic resource requirements such as the number of CPUs and memory limits, internal infrastructural utilization data is exclusively leveraged by cluster operators, who use it to configure batch schedulers.","This task is challenging and increasingly complex due to ever larger cluster scales and heterogeneity of modern scientific workflows.","As a result, HPC systems achieve low utilization with long job completion times (makespans).","To tackle these challenges, we propose a co-scheduling algorithm based on an adaptive reinforcement learning algorithm, where application profiling is combined with cluster monitoring.","The resulting cluster scheduler matches resource utilization to application performance in a fine-grained manner (i.e., operating system level).","As opposed to nominal allocations, we apply decision trees to model applications' actual resource usage, which are used to estimate how much resource capacity from one allocation can be co-allocated to additional applications.","Our algorithm learns from incorrect co-scheduling decisions and adapts from changing environment conditions, and evaluates when such changes cause resource contention that impacts quality of service metrics such as jobs slowdowns.","We integrate our algorithm in an HPC resource manager that combines Slurm and Mesos for job scheduling and co-allocation, respectively.","Our experimental evaluation performed in a dedicated cluster executing a mix of four real different scientific workflows demonstrates improvements on cluster utilization of up to 51% even in high load scenarios, with 55% average queue makespan reductions under low loads."],"url":"http://arxiv.org/abs/2401.09706v1"}
{"created":"2024-01-18 03:32:07","title":"Learning Hybrid Policies for MPC with Application to Drone Flight in Unknown Dynamic Environments","abstract":"In recent years, drones have found increased applications in a wide array of real-world tasks. Model predictive control (MPC) has emerged as a practical method for drone flight control, owing to its robustness against modeling errors/uncertainties and external disturbances. However, MPC's sensitivity to manually tuned parameters can lead to rapid performance degradation when faced with unknown environmental dynamics. This paper addresses the challenge of controlling a drone as it traverses a swinging gate characterized by unknown dynamics. This paper introduces a parameterized MPC approach named hyMPC that leverages high-level decision variables to adapt to uncertain environmental conditions. To derive these decision variables, a novel policy search framework aimed at training a high-level Gaussian policy is presented. Subsequently, we harness the power of neural network policies, trained on data gathered through the repeated execution of the Gaussian policy, to provide real-time decision variables. The effectiveness of hyMPC is validated through numerical simulations, achieving a 100\\% success rate in 20 drone flight tests traversing a swinging gate, demonstrating its capability to achieve safe and precise flight with limited prior knowledge of environmental dynamics.","sentences":["In recent years, drones have found increased applications in a wide array of real-world tasks.","Model predictive control (MPC) has emerged as a practical method for drone flight control, owing to its robustness against modeling errors/uncertainties and external disturbances.","However, MPC's sensitivity to manually tuned parameters can lead to rapid performance degradation when faced with unknown environmental dynamics.","This paper addresses the challenge of controlling a drone as it traverses a swinging gate characterized by unknown dynamics.","This paper introduces a parameterized MPC approach named hyMPC that leverages high-level decision variables to adapt to uncertain environmental conditions.","To derive these decision variables, a novel policy search framework aimed at training a high-level Gaussian policy is presented.","Subsequently, we harness the power of neural network policies, trained on data gathered through the repeated execution of the Gaussian policy, to provide real-time decision variables.","The effectiveness of hyMPC is validated through numerical simulations, achieving a 100\\% success rate in 20 drone flight tests traversing a swinging gate, demonstrating its capability to achieve safe and precise flight with limited prior knowledge of environmental dynamics."],"url":"http://arxiv.org/abs/2401.09705v1"}
{"created":"2024-01-18 03:09:17","title":"Fully Dynamic Min-Cut of Superconstant Size in Subpolynomial Time","abstract":"We present a deterministic fully dynamic algorithm with subpolynomial worst-case time per graph update such that after processing each update of the graph, the algorithm outputs a minimum cut of the graph if the graph has a cut of size at most $c$ for some $c = (\\log n)^{o(1)}$. Previously, the best update time was $\\widetilde O(\\sqrt{n})$ for any $c > 2$ and $c = O(\\log n)$ [Thorup, Combinatorica'07].","sentences":["We present a deterministic fully dynamic algorithm with subpolynomial worst-case time per graph update such that after processing each update of the graph, the algorithm outputs a minimum cut of the graph if the graph has a cut of size at most $c$ for some $c = (\\log n)^{o(1)}$. Previously, the best update time was $\\widetilde O(\\sqrt{n})$ for any $c > 2$ and $c = O(\\log n)$","[Thorup, Combinatorica'07]."],"url":"http://arxiv.org/abs/2401.09700v1"}
{"created":"2024-01-18 02:44:18","title":"Imitation Learning Inputting Image Feature to Each Layer of Neural Network","abstract":"Imitation learning enables robots to learn and replicate human behavior from training data. Recent advances in machine learning enable end-to-end learning approaches that directly process high-dimensional observation data, such as images. However, these approaches face a critical challenge when processing data from multiple modalities, inadvertently ignoring data with a lower correlation to the desired output, especially when using short sampling periods. This paper presents a useful method to address this challenge, which amplifies the influence of data with a relatively low correlation to the output by inputting the data into each neural network layer. The proposed approach effectively incorporates diverse data sources into the learning process. Through experiments using a simple pick-and-place operation with raw images and joint information as input, significant improvements in success rates are demonstrated even when dealing with data from short sampling periods.","sentences":["Imitation learning enables robots to learn and replicate human behavior from training data.","Recent advances in machine learning enable end-to-end learning approaches that directly process high-dimensional observation data, such as images.","However, these approaches face a critical challenge when processing data from multiple modalities, inadvertently ignoring data with a lower correlation to the desired output, especially when using short sampling periods.","This paper presents a useful method to address this challenge, which amplifies the influence of data with a relatively low correlation to the output by inputting the data into each neural network layer.","The proposed approach effectively incorporates diverse data sources into the learning process.","Through experiments using a simple pick-and-place operation with raw images and joint information as input, significant improvements in success rates are demonstrated even when dealing with data from short sampling periods."],"url":"http://arxiv.org/abs/2401.09691v1"}
{"created":"2024-01-18 02:21:53","title":"Comparative Study on the Performance of Categorical Variable Encoders in Classification and Regression Tasks","abstract":"Categorical variables often appear in datasets for classification and regression tasks, and they need to be encoded into numerical values before training. Since many encoders have been developed and can significantly impact performance, choosing the appropriate encoder for a task becomes a time-consuming yet important practical issue. This study broadly classifies machine learning models into three categories: 1) ATI models that implicitly perform affine transformations on inputs, such as multi-layer perceptron neural network; 2) Tree-based models that are based on decision trees, such as random forest; and 3) the rest, such as kNN. Theoretically, we prove that the one-hot encoder is the best choice for ATI models in the sense that it can mimic any other encoders by learning suitable weights from the data. We also explain why the target encoder and its variants are the most suitable encoders for tree-based models. This study conducted comprehensive computational experiments to evaluate 14 encoders, including one-hot and target encoders, along with eight common machine-learning models on 28 datasets. The computational results agree with our theoretical analysis. The findings in this study shed light on how to select the suitable encoder for data scientists in fields such as fraud detection, disease diagnosis, etc.","sentences":["Categorical variables often appear in datasets for classification and regression tasks, and they need to be encoded into numerical values before training.","Since many encoders have been developed and can significantly impact performance, choosing the appropriate encoder for a task becomes a time-consuming yet important practical issue.","This study broadly classifies machine learning models into three categories: 1) ATI models that implicitly perform affine transformations on inputs, such as multi-layer perceptron neural network; 2) Tree-based models that are based on decision trees, such as random forest; and 3) the rest, such as kNN.","Theoretically, we prove that the one-hot encoder is the best choice for ATI models in the sense that it can mimic any other encoders by learning suitable weights from the data.","We also explain why the target encoder and its variants are the most suitable encoders for tree-based models.","This study conducted comprehensive computational experiments to evaluate 14 encoders, including one-hot and target encoders, along with eight common machine-learning models on 28 datasets.","The computational results agree with our theoretical analysis.","The findings in this study shed light on how to select the suitable encoder for data scientists in fields such as fraud detection, disease diagnosis, etc."],"url":"http://arxiv.org/abs/2401.09682v1"}
{"created":"2024-01-18 02:21:06","title":"Harnessing Density Ratios for Online Reinforcement Learning","abstract":"The theories of offline and online reinforcement learning, despite having evolved in parallel, have begun to show signs of the possibility for a unification, with algorithms and analysis techniques for one setting often having natural counterparts in the other. However, the notion of density ratio modeling, an emerging paradigm in offline RL, has been largely absent from online RL, perhaps for good reason: the very existence and boundedness of density ratios relies on access to an exploratory dataset with good coverage, but the core challenge in online RL is to collect such a dataset without having one to start. In this work we show -- perhaps surprisingly -- that density ratio-based algorithms have online counterparts. Assuming only the existence of an exploratory distribution with good coverage, a structural condition known as coverability (Xie et al., 2023), we give a new algorithm (GLOW) that uses density ratio realizability and value function realizability to perform sample-efficient online exploration. GLOW addresses unbounded density ratios via careful use of truncation, and combines this with optimism to guide exploration. GLOW is computationally inefficient; we complement it with a more efficient counterpart, HyGLOW, for the Hybrid RL setting (Song et al., 2022) wherein online RL is augmented with additional offline data. HyGLOW is derived as a special case of a more general meta-algorithm that provides a provable black-box reduction from hybrid RL to offline RL, which may be of independent interest.","sentences":["The theories of offline and online reinforcement learning, despite having evolved in parallel, have begun to show signs of the possibility for a unification, with algorithms and analysis techniques for one setting often having natural counterparts in the other.","However, the notion of density ratio modeling, an emerging paradigm in offline RL, has been largely absent from online RL, perhaps for good reason: the very existence and boundedness of density ratios relies on access to an exploratory dataset with good coverage, but the core challenge in online RL is to collect such a dataset without having one to start.","In this work we show -- perhaps surprisingly -- that density ratio-based algorithms have online counterparts.","Assuming only the existence of an exploratory distribution with good coverage, a structural condition known as coverability (Xie et al., 2023), we give a new algorithm (GLOW) that uses density ratio realizability and value function realizability to perform sample-efficient online exploration.","GLOW addresses unbounded density ratios via careful use of truncation, and combines this with optimism to guide exploration.","GLOW is computationally inefficient; we complement it with a more efficient counterpart, HyGLOW, for the Hybrid RL setting (Song et al., 2022) wherein online RL is augmented with additional offline data.","HyGLOW is derived as a special case of a more general meta-algorithm that provides a provable black-box reduction from hybrid RL to offline RL, which may be of independent interest."],"url":"http://arxiv.org/abs/2401.09681v1"}
{"created":"2024-01-18 01:07:00","title":"Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach","abstract":"Unsupervised domain translation (UDT) aims to find functions that convert samples from one domain (e.g., sketches) to another domain (e.g., photos) without changing the high-level semantic meaning (also referred to as ``content''). The translation functions are often sought by probability distribution matching of the transformed source domain and target domain. CycleGAN stands as arguably the most representative approach among this line of work. However, it was noticed in the literature that CycleGAN and variants could fail to identify the desired translation functions and produce content-misaligned translations. This limitation arises due to the presence of multiple translation functions -- referred to as ``measure-preserving automorphism\" (MPA) -- in the solution space of the learning criteria. Despite awareness of such identifiability issues, solutions have remained elusive. This study delves into the core identifiability inquiry and introduces an MPA elimination theory. Our analysis shows that MPA is unlikely to exist, if multiple pairs of diverse cross-domain conditional distributions are matched by the learning function. Our theory leads to a UDT learner using distribution matching over auxiliary variable-induced subsets of the domains -- other than over the entire data domains as in the classical approaches. The proposed framework is the first to rigorously establish translation identifiability under reasonable UDT settings, to our best knowledge. Experiments corroborate with our theoretical claims.","sentences":["Unsupervised domain translation (UDT) aims to find functions that convert samples from one domain (e.g., sketches) to another domain (e.g., photos) without changing the high-level semantic meaning (also referred to as ``content'').","The translation functions are often sought by probability distribution matching of the transformed source domain and target domain.","CycleGAN stands as arguably the most representative approach among this line of work.","However, it was noticed in the literature that CycleGAN and variants could fail to identify the desired translation functions and produce content-misaligned translations.","This limitation arises due to the presence of multiple translation functions -- referred to as ``measure-preserving automorphism\" (MPA) -- in the solution space of the learning criteria.","Despite awareness of such identifiability issues, solutions have remained elusive.","This study delves into the core identifiability inquiry and introduces an MPA elimination theory.","Our analysis shows that MPA is unlikely to exist, if multiple pairs of diverse cross-domain conditional distributions are matched by the learning function.","Our theory leads to a UDT learner using distribution matching over auxiliary variable-induced subsets of the domains -- other than over the entire data domains as in the classical approaches.","The proposed framework is the first to rigorously establish translation identifiability under reasonable UDT settings, to our best knowledge.","Experiments corroborate with our theoretical claims."],"url":"http://arxiv.org/abs/2401.09671v1"}
{"created":"2024-01-18 00:09:54","title":"Mobility Accelerates Learning: Convergence Analysis on Hierarchical Federated Learning in Vehicular Networks","abstract":"Hierarchical federated learning (HFL) enables distributed training of models across multiple devices with the help of several edge servers and a cloud edge server in a privacy-preserving manner. In this paper, we consider HFL with highly mobile devices, mainly targeting at vehicular networks. Through convergence analysis, we show that mobility influences the convergence speed by both fusing the edge data and shuffling the edge models. While mobility is usually considered as a challenge from the perspective of communication, we prove that it increases the convergence speed of HFL with edge-level heterogeneous data, since more diverse data can be incorporated. Furthermore, we demonstrate that a higher speed leads to faster convergence, since it accelerates the fusion of data. Simulation results show that mobility increases the model accuracy of HFL by up to 15.1% when training a convolutional neural network on the CIFAR-10 dataset.","sentences":["Hierarchical federated learning (HFL) enables distributed training of models across multiple devices with the help of several edge servers and a cloud edge server in a privacy-preserving manner.","In this paper, we consider HFL with highly mobile devices, mainly targeting at vehicular networks.","Through convergence analysis, we show that mobility influences the convergence speed by both fusing the edge data and shuffling the edge models.","While mobility is usually considered as a challenge from the perspective of communication, we prove that it increases the convergence speed of HFL with edge-level heterogeneous data, since more diverse data can be incorporated.","Furthermore, we demonstrate that a higher speed leads to faster convergence, since it accelerates the fusion of data.","Simulation results show that mobility increases the model accuracy of HFL by up to 15.1% when training a convolutional neural network on the CIFAR-10 dataset."],"url":"http://arxiv.org/abs/2401.09656v1"}
{"created":"2024-01-17 23:27:48","title":"Functional Linear Non-Gaussian Acyclic Model for Causal Discovery","abstract":"In causal discovery, non-Gaussianity has been used to characterize the complete configuration of a Linear Non-Gaussian Acyclic Model (LiNGAM), encompassing both the causal ordering of variables and their respective connection strengths. However, LiNGAM can only deal with the finite-dimensional case. To expand this concept, we extend the notion of variables to encompass vectors and even functions, leading to the Functional Linear Non-Gaussian Acyclic Model (Func-LiNGAM). Our motivation stems from the desire to identify causal relationships in brain-effective connectivity tasks involving, for example, fMRI and EEG datasets. We demonstrate why the original LiNGAM fails to handle these inherently infinite-dimensional datasets and explain the availability of functional data analysis from both empirical and theoretical perspectives. {We establish theoretical guarantees of the identifiability of the causal relationship among non-Gaussian random vectors and even random functions in infinite-dimensional Hilbert spaces.} To address the issue of sparsity in discrete time points within intrinsic infinite-dimensional functional data, we propose optimizing the coordinates of the vectors using functional principal component analysis. Experimental results on synthetic data verify the ability of the proposed framework to identify causal relationships among multivariate functions using the observed samples. For real data, we focus on analyzing the brain connectivity patterns derived from fMRI data.","sentences":["In causal discovery, non-Gaussianity has been used to characterize the complete configuration of a Linear Non-Gaussian Acyclic Model (LiNGAM), encompassing both the causal ordering of variables and their respective connection strengths.","However, LiNGAM can only deal with the finite-dimensional case.","To expand this concept, we extend the notion of variables to encompass vectors and even functions, leading to the Functional Linear Non-Gaussian Acyclic Model (Func-LiNGAM).","Our motivation stems from the desire to identify causal relationships in brain-effective connectivity tasks involving, for example, fMRI and EEG datasets.","We demonstrate why the original LiNGAM fails to handle these inherently infinite-dimensional datasets and explain the availability of functional data analysis from both empirical and theoretical perspectives.","{We establish theoretical guarantees of the identifiability of the causal relationship among non-Gaussian random vectors and even random functions in infinite-dimensional Hilbert spaces.}","To address the issue of sparsity in discrete time points within intrinsic infinite-dimensional functional data, we propose optimizing the coordinates of the vectors using functional principal component analysis.","Experimental results on synthetic data verify the ability of the proposed framework to identify causal relationships among multivariate functions using the observed samples.","For real data, we focus on analyzing the brain connectivity patterns derived from fMRI data."],"url":"http://arxiv.org/abs/2401.09641v1"}
{"created":"2024-01-17 22:46:51","title":"Physics-Informed Calibration of Aeromagnetic Compensation in Magnetic Navigation Systems using Liquid Time-Constant Networks","abstract":"Magnetic navigation (MagNav) is a rising alternative to the Global Positioning System (GPS) and has proven useful for aircraft navigation. Traditional aircraft navigation systems, while effective, face limitations in precision and reliability in certain environments and against attacks. Airborne MagNav leverages the Earth's magnetic field to provide accurate positional information. However, external magnetic fields induced by aircraft electronics and Earth's large-scale magnetic fields disrupt the weaker signal of interest. We introduce a physics-informed approach using Tolles-Lawson coefficients for compensation and Liquid Time-Constant Networks (LTCs) to remove complex, noisy signals derived from the aircraft's magnetic sources. Using real flight data with magnetometer measurements and aircraft measurements, we observe up to a 64% reduction in aeromagnetic compensation error (RMSE nT), outperforming conventional models. This significant improvement underscores the potential of a physics-informed, machine learning approach for extracting clean, reliable, and accurate magnetic signals for MagNav positional estimation.","sentences":["Magnetic navigation (MagNav) is a rising alternative to the Global Positioning System (GPS) and has proven useful for aircraft navigation.","Traditional aircraft navigation systems, while effective, face limitations in precision and reliability in certain environments and against attacks.","Airborne MagNav leverages the Earth's magnetic field to provide accurate positional information.","However, external magnetic fields induced by aircraft electronics and Earth's large-scale magnetic fields disrupt the weaker signal of interest.","We introduce a physics-informed approach using Tolles-Lawson coefficients for compensation and Liquid Time-Constant Networks (LTCs) to remove complex, noisy signals derived from the aircraft's magnetic sources.","Using real flight data with magnetometer measurements and aircraft measurements, we observe up to a 64% reduction in aeromagnetic compensation error (RMSE nT), outperforming conventional models.","This significant improvement underscores the potential of a physics-informed, machine learning approach for extracting clean, reliable, and accurate magnetic signals for MagNav positional estimation."],"url":"http://arxiv.org/abs/2401.09631v1"}
{"created":"2024-01-17 22:18:00","title":"XTable in Action: Seamless Interoperability in Data Lakes","abstract":"Contemporary approaches to data management are increasingly relying on unified analytics and AI platforms to foster collaboration, interoperability, seamless access to reliable data, and high performance. Data Lakes featuring open standard table formats such as Delta Lake, Apache Hudi, and Apache Iceberg are central components of these data architectures. Choosing the right format for managing a table is crucial for achieving the objectives mentioned above. The challenge lies in selecting the best format, a task that is onerous and can yield temporary results, as the ideal choice may shift over time with data growth, evolving workloads, and the competitive development of table formats and processing engines. Moreover, restricting data access to a single format can hinder data sharing resulting in diminished business value over the long term. The ability to seamlessly interoperate between formats and with negligible overhead can effectively address these challenges. Our solution in this direction is an innovative omni-directional translator, XTable, that facilitates writing data in one format and reading it in any format, thus achieving the desired format interoperability. In this work, we demonstrate the effectiveness of XTable through application scenarios inspired by real-world use cases.","sentences":["Contemporary approaches to data management are increasingly relying on unified analytics and AI platforms to foster collaboration, interoperability, seamless access to reliable data, and high performance.","Data Lakes featuring open standard table formats such as Delta Lake, Apache Hudi, and Apache Iceberg are central components of these data architectures.","Choosing the right format for managing a table is crucial for achieving the objectives mentioned above.","The challenge lies in selecting the best format, a task that is onerous and can yield temporary results, as the ideal choice may shift over time with data growth, evolving workloads, and the competitive development of table formats and processing engines.","Moreover, restricting data access to a single format can hinder data sharing resulting in diminished business value over the long term.","The ability to seamlessly interoperate between formats and with negligible overhead can effectively address these challenges.","Our solution in this direction is an innovative omni-directional translator, XTable, that facilitates writing data in one format and reading it in any format, thus achieving the desired format interoperability.","In this work, we demonstrate the effectiveness of XTable through application scenarios inspired by real-world use cases."],"url":"http://arxiv.org/abs/2401.09621v1"}
{"created":"2024-01-17 21:32:04","title":"Land Cover Image Classification","abstract":"Land Cover (LC) image classification has become increasingly significant in understanding environmental changes, urban planning, and disaster management. However, traditional LC methods are often labor-intensive and prone to human error. This paper explores state-of-the-art deep learning models for enhanced accuracy and efficiency in LC analysis. We compare convolutional neural networks (CNN) against transformer-based methods, showcasing their applications and advantages in LC studies. We used EuroSAT, a patch-based LC classification data set based on Sentinel-2 satellite images and achieved state-of-the-art results using current transformer models.","sentences":["Land Cover (LC) image classification has become increasingly significant in understanding environmental changes, urban planning, and disaster management.","However, traditional LC methods are often labor-intensive and prone to human error.","This paper explores state-of-the-art deep learning models for enhanced accuracy and efficiency in LC analysis.","We compare convolutional neural networks (CNN) against transformer-based methods, showcasing their applications and advantages in LC studies.","We used EuroSAT, a patch-based LC classification data set based on Sentinel-2 satellite images and achieved state-of-the-art results using current transformer models."],"url":"http://arxiv.org/abs/2401.09607v1"}
{"created":"2024-01-17 21:30:45","title":"Charting a Path to Efficient Onboarding: The Role of Software Visualization","abstract":"Background. Within the software industry, it is commonly estimated that software professionals invest a substantial portion of their work hours in the process of understanding existing systems. In this context, an ineffective technical onboarding process, which introduces newcomers to software under development, can result in a prolonged period for them to absorb the necessary knowledge required to become productive in their roles. Goal. The present study aims to explore the familiarity of managers, leaders, and developers with software visualization tools and how these tools are employed to facilitate the technical onboarding of new team members. Method. To address the research problem, we built upon the insights gained through the literature and embraced a sequential exploratory approach. This approach incorporated quantitative and qualitative analyses of data collected from practitioners using questionnaires and semi-structured interviews. Findings. Our findings demonstrate a gap between the concept of software visualization and the practical use of onboarding tools and techniques. Overall, practitioners do not systematically incorporate software visualization tools into their technical onboarding processes due to a lack of conceptual understanding and awareness of their potential benefits. Conclusion. The software industry could benefit from standardized and evolving onboarding models, improved by incorporating software visualization techniques and tools to support program comprehension of newcomers in the software projects.","sentences":["Background.","Within the software industry, it is commonly estimated that software professionals invest a substantial portion of their work hours in the process of understanding existing systems.","In this context, an ineffective technical onboarding process, which introduces newcomers to software under development, can result in a prolonged period for them to absorb the necessary knowledge required to become productive in their roles.","Goal.","The present study aims to explore the familiarity of managers, leaders, and developers with software visualization tools and how these tools are employed to facilitate the technical onboarding of new team members.","Method.","To address the research problem, we built upon the insights gained through the literature and embraced a sequential exploratory approach.","This approach incorporated quantitative and qualitative analyses of data collected from practitioners using questionnaires and semi-structured interviews.","Findings.","Our findings demonstrate a gap between the concept of software visualization and the practical use of onboarding tools and techniques.","Overall, practitioners do not systematically incorporate software visualization tools into their technical onboarding processes due to a lack of conceptual understanding and awareness of their potential benefits.","Conclusion.","The software industry could benefit from standardized and evolving onboarding models, improved by incorporating software visualization techniques and tools to support program comprehension of newcomers in the software projects."],"url":"http://arxiv.org/abs/2401.09605v1"}
{"created":"2024-01-17 21:30:22","title":"MedBlindTuner: Towards Privacy-preserving Fine-tuning on Biomedical Images with Transformers and Fully Homomorphic Encryption","abstract":"Advancements in machine learning (ML) have significantly revolutionized medical image analysis, prompting hospitals to rely on external ML services. However, the exchange of sensitive patient data, such as chest X-rays, poses inherent privacy risks when shared with third parties. Addressing this concern, we propose MedBlindTuner, a privacy-preserving framework leveraging fully homomorphic encryption (FHE) and a data-efficient image transformer (DEiT). MedBlindTuner enables the training of ML models exclusively on FHE-encrypted medical images. Our experimental evaluation demonstrates that MedBlindTuner achieves comparable accuracy to models trained on non-encrypted images, offering a secure solution for outsourcing ML computations while preserving patient data privacy. To the best of our knowledge, this is the first work that uses data-efficient image transformers and fully homomorphic encryption in this domain.","sentences":["Advancements in machine learning (ML) have significantly revolutionized medical image analysis, prompting hospitals to rely on external ML services.","However, the exchange of sensitive patient data, such as chest X-rays, poses inherent privacy risks when shared with third parties.","Addressing this concern, we propose MedBlindTuner, a privacy-preserving framework leveraging fully homomorphic encryption (FHE) and a data-efficient image transformer (DEiT).","MedBlindTuner enables the training of ML models exclusively on FHE-encrypted medical images.","Our experimental evaluation demonstrates that MedBlindTuner achieves comparable accuracy to models trained on non-encrypted images, offering a secure solution for outsourcing ML computations while preserving patient data privacy.","To the best of our knowledge, this is the first work that uses data-efficient image transformers and fully homomorphic encryption in this domain."],"url":"http://arxiv.org/abs/2401.09604v1"}
{"created":"2024-01-17 20:40:51","title":"Bringing Social Computing to Secondary School Classrooms","abstract":"Social computing is the study of how technology shapes human social interactions. This topic has become increasingly relevant to secondary school students (ages 11--18) as more of young people's everyday social experiences take place online, particularly with the continuing effects of the COVID-19 pandemic. However, social computing topics are rarely touched upon in existing middle and high school curricula. We seek to introduce concepts from social computing to secondary school students so they can understand how computing has wide-ranging social implications that touch upon their everyday lives, as well as think critically about both the positive and negative sides of different social technology designs.   In this report, we present a series of six lessons combining presentations and hands-on activities covering topics within social computing and detail our experience teaching these lessons to approximately 1,405 students across 13 middle and high schools in our local school district. We developed lessons covering how social computing relates to the topics of Data Management, Encrypted Messaging, Human-Computer Interaction Careers, Machine Learning and Bias, Misinformation, and Online Behavior. We found that 81.13% of students expressed greater interest in the content of our lessons compared to their interest in STEM overall. We also found from pre- and post-lesson comprehension questions that 63.65% learned new concepts from the main activity. We release all lesson materials on a website for public use. From our experience, we observed that students were engaged in these topics and found enjoyment in finding connections between computing and their own lives.","sentences":["Social computing is the study of how technology shapes human social interactions.","This topic has become increasingly relevant to secondary school students (ages 11--18) as more of young people's everyday social experiences take place online, particularly with the continuing effects of the COVID-19 pandemic.","However, social computing topics are rarely touched upon in existing middle and high school curricula.","We seek to introduce concepts from social computing to secondary school students so they can understand how computing has wide-ranging social implications that touch upon their everyday lives, as well as think critically about both the positive and negative sides of different social technology designs.   ","In this report, we present a series of six lessons combining presentations and hands-on activities covering topics within social computing and detail our experience teaching these lessons to approximately 1,405 students across 13 middle and high schools in our local school district.","We developed lessons covering how social computing relates to the topics of Data Management, Encrypted Messaging, Human-Computer Interaction Careers, Machine Learning and Bias, Misinformation, and Online Behavior.","We found that 81.13% of students expressed greater interest in the content of our lessons compared to their interest in STEM overall.","We also found from pre- and post-lesson comprehension questions that 63.65% learned new concepts from the main activity.","We release all lesson materials on a website for public use.","From our experience, we observed that students were engaged in these topics and found enjoyment in finding connections between computing and their own lives."],"url":"http://arxiv.org/abs/2401.09591v1"}
{"created":"2024-01-17 20:28:15","title":"Bilevel Optimization under Unbounded Smoothness: A New Algorithm and Convergence Analysis","abstract":"Bilevel optimization is an important formulation for many machine learning problems. Current bilevel optimization algorithms assume that the gradient of the upper-level function is Lipschitz. However, recent studies reveal that certain neural networks such as recurrent neural networks (RNNs) and long-short-term memory networks (LSTMs) exhibit potential unbounded smoothness, rendering conventional bilevel optimization algorithms unsuitable. In this paper, we design a new bilevel optimization algorithm, namely BO-REP, to address this challenge. This algorithm updates the upper-level variable using normalized momentum and incorporates two novel techniques for updating the lower-level variable: \\textit{initialization refinement} and \\textit{periodic updates}. Specifically, once the upper-level variable is initialized, a subroutine is invoked to obtain a refined estimate of the corresponding optimal lower-level variable, and the lower-level variable is updated only after every specific period instead of each iteration. When the upper-level problem is nonconvex and unbounded smooth, and the lower-level problem is strongly convex, we prove that our algorithm requires $\\widetilde{\\mathcal{O}}(1/\\epsilon^4)$ iterations to find an $\\epsilon$-stationary point in the stochastic setting, where each iteration involves calling a stochastic gradient or Hessian-vector product oracle. Notably, this result matches the state-of-the-art complexity results under the bounded smoothness setting and without mean-squared smoothness of the stochastic gradient, up to logarithmic factors. Our proof relies on novel technical lemmas for the periodically updated lower-level variable, which are of independent interest. Our experiments on hyper-representation learning, hyperparameter optimization, and data hyper-cleaning for text classification tasks demonstrate the effectiveness of our proposed algorithm.","sentences":["Bilevel optimization is an important formulation for many machine learning problems.","Current bilevel optimization algorithms assume that the gradient of the upper-level function is Lipschitz.","However, recent studies reveal that certain neural networks such as recurrent neural networks (RNNs) and long-short-term memory networks (LSTMs) exhibit potential unbounded smoothness, rendering conventional bilevel optimization algorithms unsuitable.","In this paper, we design a new bilevel optimization algorithm, namely BO-REP, to address this challenge.","This algorithm updates the upper-level variable using normalized momentum and incorporates two novel techniques for updating the lower-level variable: \\textit{initialization refinement} and \\textit{periodic updates}.","Specifically, once the upper-level variable is initialized, a subroutine is invoked to obtain a refined estimate of the corresponding optimal lower-level variable, and the lower-level variable is updated only after every specific period instead of each iteration.","When the upper-level problem is nonconvex and unbounded smooth, and the lower-level problem is strongly convex, we prove that our algorithm requires $\\widetilde{\\mathcal{O}}(1/\\epsilon^4)$ iterations to find an $\\epsilon$-stationary point in the stochastic setting, where each iteration involves calling a stochastic gradient or Hessian-vector product oracle.","Notably, this result matches the state-of-the-art complexity results under the bounded smoothness setting and without mean-squared smoothness of the stochastic gradient, up to logarithmic factors.","Our proof relies on novel technical lemmas for the periodically updated lower-level variable, which are of independent interest.","Our experiments on hyper-representation learning, hyperparameter optimization, and data hyper-cleaning for text classification tasks demonstrate the effectiveness of our proposed algorithm."],"url":"http://arxiv.org/abs/2401.09587v1"}
{"created":"2024-01-17 20:21:46","title":"Lower Bounds on $0$-Extension with Steiner Nodes","abstract":"In the $0$-Extension problem, we are given an edge-weighted graph $G=(V,E,c)$, a set $T\\subseteq V$ of its vertices called terminals, and a semi-metric $D$ over $T$, and the goal is to find an assignment $f$ of each non-terminal vertex to a terminal, minimizing the sum, over all edges $(u,v)\\in E$, the product of the edge weight $c(u,v)$ and the distance $D(f(u),f(v))$ between the terminals that $u,v$ are mapped to. Current best approximation algorithms on $0$-Extension are based on rounding a linear programming relaxation called the \\emph{semi-metric LP relaxation}. The integrality gap of this LP, with best upper bound $O(\\log |T|/\\log\\log |T|)$ and best lower bound $\\Omega((\\log |T|)^{2/3})$, has been shown to be closely related to the best quality of cut and flow vertex sparsifiers.   We study a variant of the $0$-Extension problem where Steiner vertices are allowed. Specifically, we focus on the integrality gap of the same semi-metric LP relaxation to this new problem. Following from previous work, this new integrality gap turns out to be closely related to the quality achievable by cut/flow vertex sparsifiers with Steiner nodes, a major open problem in graph compression. Our main result is that the new integrality gap stays superconstant $\\Omega(\\log\\log |T|)$ even if we allow a super-linear $O(|T|\\log^{1-\\varepsilon}|T|)$ number of Steiner nodes.","sentences":["In the $0$-Extension problem, we are given an edge-weighted graph $G=(V,E,c)$, a set $T\\subseteq V$ of its vertices called terminals, and a semi-metric $D$ over $T$, and the goal is to find an assignment $f$ of each non-terminal vertex to a terminal, minimizing the sum, over all edges $(u,v)\\in E$, the product of the edge weight $c(u,v)$ and the distance $D(f(u),f(v))$ between the terminals that $u,v$ are mapped to.","Current best approximation algorithms on $0$-Extension are based on rounding a linear programming relaxation called the \\emph{semi-metric LP relaxation}.","The integrality gap of this LP, with best upper bound $O(\\log |T|/\\log\\log |T|)$ and best lower bound $\\Omega((\\log |T|)^{2/3})$, has been shown to be closely related to the best quality of cut and flow vertex sparsifiers.   ","We study a variant of the $0$-Extension problem where Steiner vertices are allowed.","Specifically, we focus on the integrality gap of the same semi-metric LP relaxation to this new problem.","Following from previous work, this new integrality gap turns out to be closely related to the quality achievable by cut/flow vertex sparsifiers with Steiner nodes, a major open problem in graph compression.","Our main result is that the new integrality gap stays superconstant $\\Omega(\\log\\log |T|)$ even if we allow a super-linear $O(|T|\\log^{1-\\varepsilon}|T|)$ number of Steiner nodes."],"url":"http://arxiv.org/abs/2401.09585v1"}
{"created":"2024-01-17 20:07:47","title":"eipy: An Open-Source Python Package for Multi-modal Data Integration using Heterogeneous Ensembles","abstract":"In this paper, we introduce eipy--an open-source Python package for developing effective, multi-modal heterogeneous ensembles for classification. eipy simultaneously provides both a rigorous, and user-friendly framework for comparing and selecting the best-performing multi-modal data integration and predictive modeling methods by systematically evaluating their performance using nested cross-validation. The package is designed to leverage scikit-learn-like estimators as components to build multi-modal predictive models. An up-to-date user guide, including API reference and tutorials, for eipy is maintained at https://eipy.readthedocs.io . The main repository for this project can be found on GitHub at https://github.com/GauravPandeyLab/eipy .","sentences":["In this paper, we introduce eipy--an open-source Python package for developing effective, multi-modal heterogeneous ensembles for classification.","eipy simultaneously provides both a rigorous, and user-friendly framework for comparing and selecting the best-performing multi-modal data integration and predictive modeling methods by systematically evaluating their performance using nested cross-validation.","The package is designed to leverage scikit-learn-like estimators as components to build multi-modal predictive models.","An up-to-date user guide, including API reference and tutorials, for eipy is maintained at https://eipy.readthedocs.io .","The main repository for this project can be found on GitHub at https://github.com/GauravPandeyLab/eipy ."],"url":"http://arxiv.org/abs/2401.09582v1"}
{"created":"2024-01-17 19:55:49","title":"Towards Scalable and Robust Model Versioning","abstract":"As the deployment of deep learning models continues to expand across industries, the threat of malicious incursions aimed at gaining access to these deployed models is on the rise. Should an attacker gain access to a deployed model, whether through server breaches, insider attacks, or model inversion techniques, they can then construct white-box adversarial attacks to manipulate the model's classification outcomes, thereby posing significant risks to organizations that rely on these models for critical tasks. Model owners need mechanisms to protect themselves against such losses without the necessity of acquiring fresh training data - a process that typically demands substantial investments in time and capital.   In this paper, we explore the feasibility of generating multiple versions of a model that possess different attack properties, without acquiring new training data or changing model architecture. The model owner can deploy one version at a time and replace a leaked version immediately with a new version. The newly deployed model version can resist adversarial attacks generated leveraging white-box access to one or all previously leaked versions. We show theoretically that this can be accomplished by incorporating parameterized hidden distributions into the model training data, forcing the model to learn task-irrelevant features uniquely defined by the chosen data. Additionally, optimal choices of hidden distributions can produce a sequence of model versions capable of resisting compound transferability attacks over time. Leveraging our analytical insights, we design and implement a practical model versioning method for DNN classifiers, which leads to significant robustness improvements over existing methods. We believe our work presents a promising direction for safeguarding DNN services beyond their initial deployment.","sentences":["As the deployment of deep learning models continues to expand across industries, the threat of malicious incursions aimed at gaining access to these deployed models is on the rise.","Should an attacker gain access to a deployed model, whether through server breaches, insider attacks, or model inversion techniques, they can then construct white-box adversarial attacks to manipulate the model's classification outcomes, thereby posing significant risks to organizations that rely on these models for critical tasks.","Model owners need mechanisms to protect themselves against such losses without the necessity of acquiring fresh training data - a process that typically demands substantial investments in time and capital.   ","In this paper, we explore the feasibility of generating multiple versions of a model that possess different attack properties, without acquiring new training data or changing model architecture.","The model owner can deploy one version at a time and replace a leaked version immediately with a new version.","The newly deployed model version can resist adversarial attacks generated leveraging white-box access to one or all previously leaked versions.","We show theoretically that this can be accomplished by incorporating parameterized hidden distributions into the model training data, forcing the model to learn task-irrelevant features uniquely defined by the chosen data.","Additionally, optimal choices of hidden distributions can produce a sequence of model versions capable of resisting compound transferability attacks over time.","Leveraging our analytical insights, we design and implement a practical model versioning method for DNN classifiers, which leads to significant robustness improvements over existing methods.","We believe our work presents a promising direction for safeguarding DNN services beyond their initial deployment."],"url":"http://arxiv.org/abs/2401.09574v1"}
{"created":"2024-01-17 19:43:43","title":"Aligning Large Language Models with Counterfactual DPO","abstract":"Advancements in large language models (LLMs) have demonstrated remarkable capabilities across a diverse range of applications. These models excel in generating text completions that are contextually coherent and cover an extensive array of subjects. However, the vast datasets required for their training make aligning response styles during the pretraining and instruction tuning phases challenging. Consequently, an additional alignment phase is typically employed, wherein the model is further trained with human preference data to better align its outputs with human expectations. While this process doesn't introduce new capabilities per se, it does accentuate generation styles innate to the model. This paper explores the utilization of counterfactual prompting within the framework of Direct Preference Optimization (DPO) to align the model's style without relying on human intervention. We demonstrate that this method effectively instils desirable behaviour, mitigates undesirable ones, and encourages the model to disregard inappropriate instructions. Our findings suggest that counterfactual prompting with DPO presents a low-resource way to fine-tune LLMs to meet the demands for responsible and ethically aligned AI systems.","sentences":["Advancements in large language models (LLMs) have demonstrated remarkable capabilities across a diverse range of applications.","These models excel in generating text completions that are contextually coherent and cover an extensive array of subjects.","However, the vast datasets required for their training make aligning response styles during the pretraining and instruction tuning phases challenging.","Consequently, an additional alignment phase is typically employed, wherein the model is further trained with human preference data to better align its outputs with human expectations.","While this process doesn't introduce new capabilities per se, it does accentuate generation styles innate to the model.","This paper explores the utilization of counterfactual prompting within the framework of Direct Preference Optimization (DPO) to align the model's style without relying on human intervention.","We demonstrate that this method effectively instils desirable behaviour, mitigates undesirable ones, and encourages the model to disregard inappropriate instructions.","Our findings suggest that counterfactual prompting with DPO presents a low-resource way to fine-tune LLMs to meet the demands for responsible and ethically aligned AI systems."],"url":"http://arxiv.org/abs/2401.09566v1"}
{"created":"2024-01-17 19:13:05","title":"Improving Classification Performance With Human Feedback: Label a few, we label the rest","abstract":"In the realm of artificial intelligence, where a vast majority of data is unstructured, obtaining substantial amounts of labeled data to train supervised machine learning models poses a significant challenge. To address this, we delve into few-shot and active learning, where are goal is to improve AI models with human feedback on a few labeled examples. This paper focuses on understanding how a continuous feedback loop can refine models, thereby enhancing their accuracy, recall, and precision through incremental human input. By employing Large Language Models (LLMs) such as GPT-3.5, BERT, and SetFit, we aim to analyze the efficacy of using a limited number of labeled examples to substantially improve model accuracy. We benchmark this approach on the Financial Phrasebank, Banking, Craigslist, Trec, Amazon Reviews datasets to prove that with just a few labeled examples, we are able to surpass the accuracy of zero shot large language models to provide enhanced text classification performance. We demonstrate that rather than needing to manually label millions of rows of data, we just need to label a few and the model can effectively predict the rest.","sentences":["In the realm of artificial intelligence, where a vast majority of data is unstructured, obtaining substantial amounts of labeled data to train supervised machine learning models poses a significant challenge.","To address this, we delve into few-shot and active learning, where are goal is to improve AI models with human feedback on a few labeled examples.","This paper focuses on understanding how a continuous feedback loop can refine models, thereby enhancing their accuracy, recall, and precision through incremental human input.","By employing Large Language Models (LLMs) such as GPT-3.5, BERT, and SetFit, we aim to analyze the efficacy of using a limited number of labeled examples to substantially improve model accuracy.","We benchmark this approach on the Financial Phrasebank, Banking, Craigslist, Trec, Amazon Reviews datasets to prove that with just a few labeled examples, we are able to surpass the accuracy of zero shot large language models to provide enhanced text classification performance.","We demonstrate that rather than needing to manually label millions of rows of data, we just need to label a few and the model can effectively predict the rest."],"url":"http://arxiv.org/abs/2401.09555v1"}
{"created":"2024-01-17 17:34:52","title":"Privacy Engineering in Smart Home (SH) Systems: A Comprehensive Privacy Threat Analysis and Risk Management Approach","abstract":"Addressing trust concerns in Smart Home (SH) systems is imperative due to the limited study on preservation approaches that focus on analyzing and evaluating privacy threats for effective risk management. While most research focuses primarily on user privacy, device data privacy, especially identity privacy, is almost neglected, which can significantly impact overall user privacy within the SH system. To this end, our study incorporates privacy engineering (PE) principles in the SH system that consider user and device data privacy. We start with a comprehensive reference model for a typical SH system. Based on the initial stage of LINDDUN PRO for the PE framework, we present a data flow diagram (DFD) based on a typical SH reference model to better understand SH system operations. To identify potential areas of privacy threat and perform a privacy threat analysis (PTA), we employ the LINDDUN PRO threat model. Then, a privacy impact assessment (PIA) was carried out to implement privacy risk management by prioritizing privacy threats based on their likelihood of occurrence and potential consequences. Finally, we suggest possible privacy enhancement techniques (PETs) that can mitigate some of these threats. The study aims to elucidate the main threats to privacy, associated risks, and effective prioritization of privacy control in SH systems. The outcomes of this study are expected to benefit SH stakeholders, including vendors, cloud providers, users, researchers, and regulatory bodies in the SH systems domain.","sentences":["Addressing trust concerns in Smart Home (SH) systems is imperative due to the limited study on preservation approaches that focus on analyzing and evaluating privacy threats for effective risk management.","While most research focuses primarily on user privacy, device data privacy, especially identity privacy, is almost neglected, which can significantly impact overall user privacy within the SH system.","To this end, our study incorporates privacy engineering (PE) principles in the SH system that consider user and device data privacy.","We start with a comprehensive reference model for a typical SH system.","Based on the initial stage of LINDDUN PRO for the PE framework, we present a data flow diagram (DFD) based on a typical SH reference model to better understand SH system operations.","To identify potential areas of privacy threat and perform a privacy threat analysis (PTA), we employ the LINDDUN PRO threat model.","Then, a privacy impact assessment (PIA) was carried out to implement privacy risk management by prioritizing privacy threats based on their likelihood of occurrence and potential consequences.","Finally, we suggest possible privacy enhancement techniques (PETs) that can mitigate some of these threats.","The study aims to elucidate the main threats to privacy, associated risks, and effective prioritization of privacy control in SH systems.","The outcomes of this study are expected to benefit SH stakeholders, including vendors, cloud providers, users, researchers, and regulatory bodies in the SH systems domain."],"url":"http://arxiv.org/abs/2401.09519v1"}
