{"created":"2024-03-07 18:56:47","title":"DeepSee: Multidimensional Visualizations of Seabed Ecosystems","abstract":"Scientists studying deep ocean microbial ecosystems use limited numbers of sediment samples collected from the seafloor to characterize important life-sustaining biogeochemical cycles in the environment. Yet conducting fieldwork to sample these extreme remote environments is both expensive and time consuming, requiring tools that enable scientists to explore the sampling history of field sites and predict where taking new samples is likely to maximize scientific return. We conducted a collaborative, user-centered design study with a team of scientific researchers to develop DeepSee, an interactive data workspace that visualizes 2D and 3D interpolations of biogeochemical and microbial processes in context together with sediment sampling history overlaid on 2D seafloor maps. Based on a field deployment and qualitative interviews, we found that DeepSee increased the scientific return from limited sample sizes, catalyzed new research workflows, reduced long-term costs of sharing data, and supported teamwork and communication between team members with diverse research goals.","sentences":["Scientists studying deep ocean microbial ecosystems use limited numbers of sediment samples collected from the seafloor to characterize important life-sustaining biogeochemical cycles in the environment.","Yet conducting fieldwork to sample these extreme remote environments is both expensive and time consuming, requiring tools that enable scientists to explore the sampling history of field sites and predict where taking new samples is likely to maximize scientific return.","We conducted a collaborative, user-centered design study with a team of scientific researchers to develop DeepSee, an interactive data workspace that visualizes 2D and 3D interpolations of biogeochemical and microbial processes in context together with sediment sampling history overlaid on 2D seafloor maps.","Based on a field deployment and qualitative interviews, we found that DeepSee increased the scientific return from limited sample sizes, catalyzed new research workflows, reduced long-term costs of sharing data, and supported teamwork and communication between team members with diverse research goals."],"url":"http://arxiv.org/abs/2403.04761v1"}
{"created":"2024-03-07 18:56:33","title":"Lifelong Intelligence Beyond the Edge using Hyperdimensional Computing","abstract":"On-device learning has emerged as a prevailing trend that avoids the slow response time and costly communication of cloud-based learning. The ability to learn continuously and indefinitely in a changing environment, and with resource constraints, is critical for real sensor deployments. However, existing designs are inadequate for practical scenarios with (i) streaming data input, (ii) lack of supervision and (iii) limited on-board resources. In this paper, we design and deploy the first on-device lifelong learning system called LifeHD for general IoT applications with limited supervision. LifeHD is designed based on a novel neurally-inspired and lightweight learning paradigm called Hyperdimensional Computing (HDC). We utilize a two-tier associative memory organization to intelligently store and manage high-dimensional, low-precision vectors, which represent the historical patterns as cluster centroids. We additionally propose two variants of LifeHD to cope with scarce labeled inputs and power constraints. We implement LifeHD on off-the-shelf edge platforms and perform extensive evaluations across three scenarios. Our measurements show that LifeHD improves the unsupervised clustering accuracy by up to 74.8% compared to the state-of-the-art NN-based unsupervised lifelong learning baselines with as much as 34.3x better energy efficiency. Our code is available at https://github.com/Orienfish/LifeHD.","sentences":["On-device learning has emerged as a prevailing trend that avoids the slow response time and costly communication of cloud-based learning.","The ability to learn continuously and indefinitely in a changing environment, and with resource constraints, is critical for real sensor deployments.","However, existing designs are inadequate for practical scenarios with (i) streaming data input, (ii) lack of supervision and (iii) limited on-board resources.","In this paper, we design and deploy the first on-device lifelong learning system called LifeHD for general IoT applications with limited supervision.","LifeHD is designed based on a novel neurally-inspired and lightweight learning paradigm called Hyperdimensional Computing (HDC).","We utilize a two-tier associative memory organization to intelligently store and manage high-dimensional, low-precision vectors, which represent the historical patterns as cluster centroids.","We additionally propose two variants of LifeHD to cope with scarce labeled inputs and power constraints.","We implement LifeHD on off-the-shelf edge platforms and perform extensive evaluations across three scenarios.","Our measurements show that LifeHD improves the unsupervised clustering accuracy by up to 74.8% compared to the state-of-the-art NN-based unsupervised lifelong learning baselines with as much as 34.3x better energy efficiency.","Our code is available at https://github.com/Orienfish/LifeHD."],"url":"http://arxiv.org/abs/2403.04759v1"}
{"created":"2024-03-07 18:56:16","title":"Preliminary Guidelines For Combining Data Integration and Visual Data Analysis","abstract":"Data integration is often performed to consolidate information from multiple disparate data sources during visual data analysis. However, integration operations are usually separate from visual analytics operations such as encode and filter in both interface design and empirical research. We conducted a preliminary user study to investigate whether and how data integration should be incorporated directly into the visual analytics process. We used two interface alternatives featuring contrasting approaches to the data preparation and analysis workflow: manual file-based ex-situ integration as a separate step from visual analytics operations; and automatic UI-based in-situ integration merged with visual analytics operations. Participants were asked to complete specific and free-form tasks with each interface, browsing for patterns, generating insights, and summarizing relationships between attributes distributed across multiple files. Analyzing participants' interactions and feedback, we found both task completion time and total interactions to be similar across interfaces and tasks, as well as unique integration strategies between interfaces and emergent behaviors related to satisficing and cognitive bias. Participants' time spent and interactions revealed that in-situ integration enabled users to spend more time on analysis tasks compared with ex-situ integration. Participants' integration strategies and analytical behaviors revealed differences in interface usage for generating and tracking hypotheses and insights. With these results, we synthesized preliminary guidelines for designing future visual analytics interfaces that can support integrating attributes throughout an active analysis process.","sentences":["Data integration is often performed to consolidate information from multiple disparate data sources during visual data analysis.","However, integration operations are usually separate from visual analytics operations such as encode and filter in both interface design and empirical research.","We conducted a preliminary user study to investigate whether and how data integration should be incorporated directly into the visual analytics process.","We used two interface alternatives featuring contrasting approaches to the data preparation and analysis workflow: manual file-based ex-situ integration as a separate step from visual analytics operations; and automatic UI-based in-situ integration merged with visual analytics operations.","Participants were asked to complete specific and free-form tasks with each interface, browsing for patterns, generating insights, and summarizing relationships between attributes distributed across multiple files.","Analyzing participants' interactions and feedback, we found both task completion time and total interactions to be similar across interfaces and tasks, as well as unique integration strategies between interfaces and emergent behaviors related to satisficing and cognitive bias.","Participants' time spent and interactions revealed that in-situ integration enabled users to spend more time on analysis tasks compared with ex-situ integration.","Participants' integration strategies and analytical behaviors revealed differences in interface usage for generating and tracking hypotheses and insights.","With these results, we synthesized preliminary guidelines for designing future visual analytics interfaces that can support integrating attributes throughout an active analysis process."],"url":"http://arxiv.org/abs/2403.04757v1"}
{"created":"2024-03-07 18:54:59","title":"Mechanism for Decision-aware Collaborative Federated Learning: A Pitfall of Shapley Values","abstract":"This paper investigates mechanism design for decision-aware collaboration via federated learning (FL) platforms. Our framework consists of a digital platform and multiple decision-aware agents, each endowed with proprietary data sets. The platform offers an infrastructure that enables access to the data, creates incentives for collaborative learning aimed at operational decision-making, and conducts FL to avoid direct raw data sharing. The computation and communication efficiency of the FL process is inherently influenced by the agent participation equilibrium induced by the mechanism. Therefore, assessing the system's efficiency involves two critical factors: the surplus created by coalition formation and the communication costs incurred across the coalition during FL. To evaluate the system efficiency under the intricate interplay between mechanism design, agent participation, operational decision-making, and the performance of FL algorithms, we introduce a multi-action collaborative federated learning (MCFL) framework for decision-aware agents. Under this framework, we further analyze the equilibrium for the renowned Shapley value based mechanisms. Specifically, we examine the issue of false-name manipulation, a form of dishonest behavior where participating agents create duplicate fake identities to split their original data among these identities. By solving the agent participation equilibrium, we demonstrate that while Shapley value effectively maximizes coalition-generated surplus by encouraging full participation, it inadvertently promotes false-name manipulation. This further significantly increases the communication costs when the platform conducts FL. Thus, we highlight a significant pitfall of Shapley value based mechanisms, which implicitly incentivizes data splitting and identity duplication, ultimately impairing the overall efficiency in FL systems.","sentences":["This paper investigates mechanism design for decision-aware collaboration via federated learning (FL) platforms.","Our framework consists of a digital platform and multiple decision-aware agents, each endowed with proprietary data sets.","The platform offers an infrastructure that enables access to the data, creates incentives for collaborative learning aimed at operational decision-making, and conducts FL to avoid direct raw data sharing.","The computation and communication efficiency of the FL process is inherently influenced by the agent participation equilibrium induced by the mechanism.","Therefore, assessing the system's efficiency involves two critical factors: the surplus created by coalition formation and the communication costs incurred across the coalition during FL.","To evaluate the system efficiency under the intricate interplay between mechanism design, agent participation, operational decision-making, and the performance of FL algorithms, we introduce a multi-action collaborative federated learning (MCFL) framework for decision-aware agents.","Under this framework, we further analyze the equilibrium for the renowned Shapley value based mechanisms.","Specifically, we examine the issue of false-name manipulation, a form of dishonest behavior where participating agents create duplicate fake identities to split their original data among these identities.","By solving the agent participation equilibrium, we demonstrate that while Shapley value effectively maximizes coalition-generated surplus by encouraging full participation, it inadvertently promotes false-name manipulation.","This further significantly increases the communication costs when the platform conducts FL.","Thus, we highlight a significant pitfall of Shapley value based mechanisms, which implicitly incentivizes data splitting and identity duplication, ultimately impairing the overall efficiency in FL systems."],"url":"http://arxiv.org/abs/2403.04753v1"}
{"created":"2024-03-07 18:50:51","title":"LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error","abstract":"Tools are essential for large language models (LLMs) to acquire up-to-date information and take consequential actions in external environments. Existing work on tool-augmented LLMs primarily focuses on the broad coverage of tools and the flexibility of adding new tools. However, a critical aspect that has surprisingly been understudied is simply how accurately an LLM uses tools for which it has been trained. We find that existing LLMs, including GPT-4 and open-source LLMs specifically fine-tuned for tool use, only reach a correctness rate in the range of 30% to 60%, far from reliable use in practice. We propose a biologically inspired method for tool-augmented LLMs, simulated trial and error (STE), that orchestrates three key mechanisms for successful tool use behaviors in the biological system: trial and error, imagination, and memory. Specifically, STE leverages an LLM's 'imagination' to simulate plausible scenarios for using a tool, after which the LLM interacts with the tool to learn from its execution feedback. Both short-term and long-term memory are employed to improve the depth and breadth of the exploration, respectively. Comprehensive experiments on ToolBench show that STE substantially improves tool learning for LLMs under both in-context learning and fine-tuning settings, bringing a boost of 46.7% to Mistral-Instruct-7B and enabling it to outperform GPT-4. We also show effective continual learning of tools via a simple experience replay strategy.","sentences":["Tools are essential for large language models (LLMs) to acquire up-to-date information and take consequential actions in external environments.","Existing work on tool-augmented LLMs primarily focuses on the broad coverage of tools and the flexibility of adding new tools.","However, a critical aspect that has surprisingly been understudied is simply how accurately an LLM uses tools for which it has been trained.","We find that existing LLMs, including GPT-4 and open-source LLMs specifically fine-tuned for tool use, only reach a correctness rate in the range of 30% to 60%, far from reliable use in practice.","We propose a biologically inspired method for tool-augmented LLMs, simulated trial and error (STE), that orchestrates three key mechanisms for successful tool use behaviors in the biological system: trial and error, imagination, and memory.","Specifically, STE leverages an LLM's 'imagination' to simulate plausible scenarios for using a tool, after which the LLM interacts with the tool to learn from its execution feedback.","Both short-term and long-term memory are employed to improve the depth and breadth of the exploration, respectively.","Comprehensive experiments on ToolBench show that STE substantially improves tool learning for LLMs under both in-context learning and fine-tuning settings, bringing a boost of 46.7% to Mistral-Instruct-7B and enabling it to outperform GPT-4.","We also show effective continual learning of tools via a simple experience replay strategy."],"url":"http://arxiv.org/abs/2403.04746v1"}
{"created":"2024-03-07 18:49:36","title":"A General Calibrated Regret Metric for Detecting and Mitigating Human-Robot Interaction Failures","abstract":"Robot decision-making increasingly relies on expressive data-driven human prediction models when operating around people. While these models are known to suffer from prediction errors in out-of-distribution interactions, not all prediction errors equally impact downstream robot performance. We identify that the mathematical notion of regret precisely characterizes the degree to which incorrect predictions of future interaction outcomes degraded closed-loop robot performance. However, canonical regret measures are poorly calibrated across diverse deployment interactions. We extend the canonical notion of regret by deriving a calibrated regret metric that generalizes from absolute reward space to probability space. With this transformation, our metric removes the need for explicit reward functions to calculate the robot's regret, enables fairer comparison of interaction anomalies across disparate deployment contexts, and facilitates targetted dataset construction of \"system-level\" prediction failures. We experimentally quantify the value of this high-regret interaction data for aiding the robot in improving its downstream decision-making. In a suite of closed-loop autonomous driving simulations, we find that fine-tuning ego-conditioned behavior predictors exclusively on high-regret human-robot interaction data can improve the robot's overall re-deployment performance with significantly (77%) less data.","sentences":["Robot decision-making increasingly relies on expressive data-driven human prediction models when operating around people.","While these models are known to suffer from prediction errors in out-of-distribution interactions, not all prediction errors equally impact downstream robot performance.","We identify that the mathematical notion of regret precisely characterizes the degree to which incorrect predictions of future interaction outcomes degraded closed-loop robot performance.","However, canonical regret measures are poorly calibrated across diverse deployment interactions.","We extend the canonical notion of regret by deriving a calibrated regret metric that generalizes from absolute reward space to probability space.","With this transformation, our metric removes the need for explicit reward functions to calculate the robot's regret, enables fairer comparison of interaction anomalies across disparate deployment contexts, and facilitates targetted dataset construction of \"system-level\" prediction failures.","We experimentally quantify the value of this high-regret interaction data for aiding the robot in improving its downstream decision-making.","In a suite of closed-loop autonomous driving simulations, we find that fine-tuning ego-conditioned behavior predictors exclusively on high-regret human-robot interaction data can improve the robot's overall re-deployment performance with significantly (77%) less data."],"url":"http://arxiv.org/abs/2403.04745v1"}
{"created":"2024-03-07 18:49:32","title":"SQ Lower Bounds for Non-Gaussian Component Analysis with Weaker Assumptions","abstract":"We study the complexity of Non-Gaussian Component Analysis (NGCA) in the Statistical Query (SQ) model. Prior work developed a general methodology to prove SQ lower bounds for this task that have been applicable to a wide range of contexts. In particular, it was known that for any univariate distribution $A$ satisfying certain conditions, distinguishing between a standard multivariate Gaussian and a distribution that behaves like $A$ in a random hidden direction and like a standard Gaussian in the orthogonal complement, is SQ-hard. The required conditions were that (1) $A$ matches many low-order moments with the standard univariate Gaussian, and (2) the chi-squared norm of $A$ with respect to the standard Gaussian is finite. While the moment-matching condition is necessary for hardness, the chi-squared condition was only required for technical reasons. In this work, we establish that the latter condition is indeed not necessary. In particular, we prove near-optimal SQ lower bounds for NGCA under the moment-matching condition only. Our result naturally generalizes to the setting of a hidden subspace. Leveraging our general SQ lower bound, we obtain near-optimal SQ lower bounds for a range of concrete estimation tasks where existing techniques provide sub-optimal or even vacuous guarantees.","sentences":["We study the complexity of Non-Gaussian Component Analysis (NGCA) in the Statistical Query (SQ) model.","Prior work developed a general methodology to prove SQ lower bounds for this task that have been applicable to a wide range of contexts.","In particular, it was known that for any univariate distribution $A$ satisfying certain conditions, distinguishing between a standard multivariate Gaussian and a distribution that behaves like $A$ in a random hidden direction and like a standard Gaussian in the orthogonal complement, is SQ-hard.","The required conditions were that (1) $A$ matches many low-order moments with the standard univariate Gaussian, and (2) the chi-squared norm of $A$ with respect to the standard Gaussian is finite.","While the moment-matching condition is necessary for hardness, the chi-squared condition was only required for technical reasons.","In this work, we establish that the latter condition is indeed not necessary.","In particular, we prove near-optimal SQ lower bounds for NGCA under the moment-matching condition only.","Our result naturally generalizes to the setting of a hidden subspace.","Leveraging our general SQ lower bound, we obtain near-optimal SQ lower bounds for a range of concrete estimation tasks where existing techniques provide sub-optimal or even vacuous guarantees."],"url":"http://arxiv.org/abs/2403.04744v1"}
{"created":"2024-03-07 18:23:51","title":"A Sub-Quadratic Time Algorithm for Robust Sparse Mean Estimation","abstract":"We study the algorithmic problem of sparse mean estimation in the presence of adversarial outliers. Specifically, the algorithm observes a \\emph{corrupted} set of samples from $\\mathcal{N}(\\mu,\\mathbf{I}_d)$, where the unknown mean $\\mu \\in \\mathbb{R}^d$ is constrained to be $k$-sparse. A series of prior works has developed efficient algorithms for robust sparse mean estimation with sample complexity $\\mathrm{poly}(k,\\log d, 1/\\epsilon)$ and runtime $d^2 \\mathrm{poly}(k,\\log d,1/\\epsilon)$, where $\\epsilon$ is the fraction of contamination. In particular, the fastest runtime of existing algorithms is quadratic ($\\Omega(d^2)$), which can be prohibitive in high dimensions. This quadratic barrier in the runtime stems from the reliance of these algorithms on the sample covariance matrix, which is of size $d^2$. Our main contribution is an algorithm for robust sparse mean estimation which runs in \\emph{subquadratic} time using $\\mathrm{poly}(k,\\log d,1/\\epsilon)$ samples. We also provide analogous results for robust sparse PCA. Our results build on algorithmic advances in detecting weak correlations, a generalized version of the light-bulb problem by Valiant.","sentences":["We study the algorithmic problem of sparse mean estimation in the presence of adversarial outliers.","Specifically, the algorithm observes a \\emph{corrupted} set of samples from $\\mathcal{N}(\\mu,\\mathbf{I}_d)$, where the unknown mean $\\mu \\in \\mathbb{R}^d$ is constrained to be $k$-sparse.","A series of prior works has developed efficient algorithms for robust sparse mean estimation with sample complexity $\\mathrm{poly}(k,\\log d, 1/\\epsilon)$ and runtime $d^2 \\mathrm{poly}(k,\\log d,1/\\epsilon)$, where $\\epsilon$ is the fraction of contamination.","In particular, the fastest runtime of existing algorithms is quadratic ($\\Omega(d^2)$), which can be prohibitive in high dimensions.","This quadratic barrier in the runtime stems from the reliance of these algorithms on the sample covariance matrix, which is of size $d^2$. Our main contribution is an algorithm for robust sparse mean estimation which runs in \\emph{subquadratic} time using $\\mathrm{poly}(k,\\log d,1/\\epsilon)$ samples.","We also provide analogous results for robust sparse PCA.","Our results build on algorithmic advances in detecting weak correlations, a generalized version of the light-bulb problem by Valiant."],"url":"http://arxiv.org/abs/2403.04726v1"}
{"created":"2024-03-07 18:22:03","title":"Masked Capsule Autoencoders","abstract":"We propose Masked Capsule Autoencoders (MCAE), the first Capsule Network that utilises pretraining in a self-supervised manner. Capsule Networks have emerged as a powerful alternative to Convolutional Neural Networks (CNNs), and have shown favourable properties when compared to Vision Transformers (ViT), but have struggled to effectively learn when presented with more complex data, leading to Capsule Network models that do not scale to modern tasks. Our proposed MCAE model alleviates this issue by reformulating the Capsule Network to use masked image modelling as a pretraining stage before finetuning in a supervised manner. Across several experiments and ablations studies we demonstrate that similarly to CNNs and ViTs, Capsule Networks can also benefit from self-supervised pretraining, paving the way for further advancements in this neural network domain. For instance, pretraining on the Imagenette dataset, a dataset of 10 classes of Imagenet-sized images, we achieve not only state-of-the-art results for Capsule Networks but also a 9% improvement compared to purely supervised training. Thus we propose that Capsule Networks benefit from and should be trained within a masked image modelling framework, with a novel capsule decoder, to improve a Capsule Network's performance on realistic-sized images.","sentences":["We propose Masked Capsule Autoencoders (MCAE), the first Capsule Network that utilises pretraining in a self-supervised manner.","Capsule Networks have emerged as a powerful alternative to Convolutional Neural Networks (CNNs), and have shown favourable properties when compared to Vision Transformers (ViT), but have struggled to effectively learn when presented with more complex data, leading to Capsule Network models that do not scale to modern tasks.","Our proposed MCAE model alleviates this issue by reformulating the Capsule Network to use masked image modelling as a pretraining stage before finetuning in a supervised manner.","Across several experiments and ablations studies we demonstrate that similarly to CNNs and ViTs, Capsule Networks can also benefit from self-supervised pretraining, paving the way for further advancements in this neural network domain.","For instance, pretraining on the Imagenette dataset, a dataset of 10 classes of Imagenet-sized images, we achieve not only state-of-the-art results for Capsule Networks but also a 9% improvement compared to purely supervised training.","Thus we propose that Capsule Networks benefit from and should be trained within a masked image modelling framework, with a novel capsule decoder, to improve a Capsule Network's performance on realistic-sized images."],"url":"http://arxiv.org/abs/2403.04724v1"}
{"created":"2024-03-07 18:16:29","title":"Rethinking of Encoder-based Warm-start Methods in Hyperparameter Optimization","abstract":"Effectively representing heterogeneous tabular datasets for meta-learning remains an open problem. Previous approaches rely on predefined meta-features, for example, statistical measures or landmarkers. Encoder-based models, such as Dataset2Vec, allow us to extract significant meta-features automatically without human intervention. This research introduces a novel encoder-based representation of tabular datasets implemented within the liltab package available on GitHub https://github.com/azoz01/liltab. Our package is based on an established model for heterogeneous tabular data proposed in [Iwata and Kumagai, 2020]. The proposed approach employs a different model for encoding feature relationships, generating alternative representations compared to existing methods like Dataset2Vec. Both of them leverage the fundamental assumption of dataset similarity learning. In this work, we evaluate Dataset2Vec and liltab on two common meta-tasks - representing entire datasets and hyperparameter optimization warm-start. However, validation on an independent metaMIMIC dataset highlights the nuanced challenges in representation learning. We show that general representations may not suffice for some meta-tasks where requirements are not explicitly considered during extraction.   [Iwata and Kumagai, 2020] Tomoharu Iwata and Atsutoshi Kumagai. Meta-learning from Tasks with Heterogeneous Attribute Spaces. In Advances in Neural Information Processing Systems, 2020.","sentences":["Effectively representing heterogeneous tabular datasets for meta-learning remains an open problem.","Previous approaches rely on predefined meta-features, for example, statistical measures or landmarkers.","Encoder-based models, such as Dataset2Vec, allow us to extract significant meta-features automatically without human intervention.","This research introduces a novel encoder-based representation of tabular datasets implemented within the liltab package available on GitHub https://github.com/azoz01/liltab.","Our package is based on an established model for heterogeneous tabular data proposed in [Iwata and Kumagai, 2020].","The proposed approach employs a different model for encoding feature relationships, generating alternative representations compared to existing methods like Dataset2Vec.","Both of them leverage the fundamental assumption of dataset similarity learning.","In this work, we evaluate Dataset2Vec and liltab on two common meta-tasks - representing entire datasets and hyperparameter optimization warm-start.","However, validation on an independent metaMIMIC dataset highlights the nuanced challenges in representation learning.","We show that general representations may not suffice for some meta-tasks where requirements are not explicitly considered during extraction.   ","[Iwata and Kumagai, 2020]","Tomoharu Iwata and Atsutoshi Kumagai.","Meta-learning from Tasks with Heterogeneous Attribute Spaces.","In Advances in Neural Information Processing Systems, 2020."],"url":"http://arxiv.org/abs/2403.04720v1"}
{"created":"2024-03-07 18:07:41","title":"GMKF: Generalized Moment Kalman Filter for Polynomial Systems with Arbitrary Noise","abstract":"This paper develops a new filtering approach for state estimation in polynomial systems corrupted by arbitrary noise, which commonly arise in robotics. We first consider a batch setup where we perform state estimation using all data collected from the initial to the current time. We formulate the batch state estimation problem as a Polynomial Optimization Problem (POP) and relax the assumption of Gaussian noise by specifying a finite number of moments of the noise. We solve the resulting POP using a moment relaxation and prove that under suitable conditions on the rank of the relaxation, (i) we can extract a provably optimal estimate from the moment relaxation, and (ii) we can obtain a belief representation from the dual (sum-of-squares) relaxation. We then turn our attention to the filtering setup and apply similar insights to develop a GMKF for recursive state estimation in polynomial systems with arbitrary noise. The GMKF formulates the prediction and update steps as POPs and solves them using moment relaxations, carrying over a possibly non-Gaussian belief. In the linear-Gaussian case, GMKF reduces to the standard Kalman Filter. We demonstrate that GMKF performs well under highly non-Gaussian noise and outperforms common alternatives, including the Extended and Unscented Kalman Filter, and their variants on matrix Lie group.","sentences":["This paper develops a new filtering approach for state estimation in polynomial systems corrupted by arbitrary noise, which commonly arise in robotics.","We first consider a batch setup where we perform state estimation using all data collected from the initial to the current time.","We formulate the batch state estimation problem as a Polynomial Optimization Problem (POP) and relax the assumption of Gaussian noise by specifying a finite number of moments of the noise.","We solve the resulting POP using a moment relaxation and prove that under suitable conditions on the rank of the relaxation, (i) we can extract a provably optimal estimate from the moment relaxation, and (ii) we can obtain a belief representation from the dual (sum-of-squares) relaxation.","We then turn our attention to the filtering setup and apply similar insights to develop a GMKF for recursive state estimation in polynomial systems with arbitrary noise.","The GMKF formulates the prediction and update steps as POPs and solves them using moment relaxations, carrying over a possibly non-Gaussian belief.","In the linear-Gaussian case, GMKF reduces to the standard Kalman Filter.","We demonstrate that GMKF performs well under highly non-Gaussian noise and outperforms common alternatives, including the Extended and Unscented Kalman Filter, and their variants on matrix Lie group."],"url":"http://arxiv.org/abs/2403.04712v1"}
{"created":"2024-03-07 18:00:40","title":"Common 7B Language Models Already Possess Strong Math Capabilities","abstract":"Mathematical capabilities were previously believed to emerge in common language models only at a very large scale or require extensive math-related pre-training. This paper shows that the LLaMA-2 7B model with common pre-training already exhibits strong mathematical abilities, as evidenced by its impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks, respectively, when selecting the best response from 256 random generations. The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities. Notably, the accuracy for the first answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks, respectively. We find that simply scaling up the SFT data can significantly enhance the reliability of generating correct answers. However, the potential for extensive scaling is constrained by the scarcity of publicly available math questions. To overcome this limitation, we employ synthetic data, which proves to be nearly as effective as real data and shows no clear saturation when scaled up to approximately one million samples. This straightforward approach achieves an accuracy of 82.6% on GSM8K and 40.6% on MATH using LLaMA-2 7B models, surpassing previous models by 14.2% and 20.8%, respectively. We also provide insights into scaling behaviors across different reasoning complexities and error types.","sentences":["Mathematical capabilities were previously believed to emerge in common language models only at a very large scale or require extensive math-related pre-training.","This paper shows that the LLaMA-2 7B model with common pre-training already exhibits strong mathematical abilities, as evidenced by its impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks, respectively, when selecting the best response from 256 random generations.","The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities.","Notably, the accuracy for the first answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks, respectively.","We find that simply scaling up the SFT data can significantly enhance the reliability of generating correct answers.","However, the potential for extensive scaling is constrained by the scarcity of publicly available math questions.","To overcome this limitation, we employ synthetic data, which proves to be nearly as effective as real data and shows no clear saturation when scaled up to approximately one million samples.","This straightforward approach achieves an accuracy of 82.6% on GSM8K and 40.6% on MATH using LLaMA-2 7B models, surpassing previous models by 14.2% and 20.8%, respectively.","We also provide insights into scaling behaviors across different reasoning complexities and error types."],"url":"http://arxiv.org/abs/2403.04706v1"}
{"created":"2024-03-07 17:53:37","title":"mmPlace: Robust Place Recognition with Intermediate Frequency Signal of Low-cost Single-chip Millimeter Wave Radar","abstract":"Place recognition is crucial for tasks like loop-closure detection and re-localization. Single-chip millimeter wave radar (single-chip radar in short) emerges as a low-cost sensor option for place recognition, with the advantage of insensitivity to degraded visual environments. However, it encounters two challenges. Firstly, sparse point cloud from single-chip radar leads to poor performance when using current place recognition methods, which assume much denser data. Secondly, its performance significantly declines in scenarios involving rotational and lateral variations, due to limited overlap in its field of view (FOV). We propose mmPlace, a robust place recognition system to address these challenges. Specifically, mmPlace transforms intermediate frequency (IF) signal into range azimuth heatmap and employs a spatial encoder to extract features. Additionally, to improve the performance in scenarios involving rotational and lateral variations, mmPlace employs a rotating platform and concatenates heatmaps in a rotation cycle, effectively expanding the system's FOV. We evaluate mmPlace's performance on the milliSonic dataset, which is collected on the University of Science and Technology of China (USTC) campus, the city roads surrounding the campus, and an underground parking garage. The results demonstrate that mmPlace outperforms point cloud-based methods and achieves 87.37% recall@1 in scenarios involving rotational and lateral variations.","sentences":["Place recognition is crucial for tasks like loop-closure detection and re-localization.","Single-chip millimeter wave radar (single-chip radar in short) emerges as a low-cost sensor option for place recognition, with the advantage of insensitivity to degraded visual environments.","However, it encounters two challenges.","Firstly, sparse point cloud from single-chip radar leads to poor performance when using current place recognition methods, which assume much denser data.","Secondly, its performance significantly declines in scenarios involving rotational and lateral variations, due to limited overlap in its field of view (FOV).","We propose mmPlace, a robust place recognition system to address these challenges.","Specifically, mmPlace transforms intermediate frequency (IF) signal into range azimuth heatmap and employs a spatial encoder to extract features.","Additionally, to improve the performance in scenarios involving rotational and lateral variations, mmPlace employs a rotating platform and concatenates heatmaps in a rotation cycle, effectively expanding the system's FOV.","We evaluate mmPlace's performance on the milliSonic dataset, which is collected on the University of Science and Technology of China (USTC) campus, the city roads surrounding the campus, and an underground parking garage.","The results demonstrate that mmPlace outperforms point cloud-based methods and achieves 87.37% recall@1 in scenarios involving rotational and lateral variations."],"url":"http://arxiv.org/abs/2403.04703v1"}
{"created":"2024-03-07 17:48:47","title":"Delving into the Trajectory Long-tail Distribution for Muti-object Tracking","abstract":"Multiple Object Tracking (MOT) is a critical area within computer vision, with a broad spectrum of practical implementations. Current research has primarily focused on the development of tracking algorithms and enhancement of post-processing techniques. Yet, there has been a lack of thorough examination concerning the nature of tracking data it self. In this study, we pioneer an exploration into the distribution patterns of tracking data and identify a pronounced long-tail distribution issue within existing MOT datasets. We note a significant imbalance in the distribution of trajectory lengths across different pedestrians, a phenomenon we refer to as \"pedestrians trajectory long-tail distribution\". Addressing this challenge, we introduce a bespoke strategy designed to mitigate the effects of this skewed distribution. Specifically, we propose two data augmentation strategies, including Stationary Camera View Data Augmentation (SVA) and Dynamic Camera View Data Augmentation (DVA) , designed for viewpoint states and the Group Softmax (GS) module for Re-ID. SVA is to backtrack and predict the pedestrian trajectory of tail classes, and DVA is to use diffusion model to change the background of the scene. GS divides the pedestrians into unrelated groups and performs softmax operation on each group individually. Our proposed strategies can be integrated into numerous existing tracking systems, and extensive experimentation validates the efficacy of our method in reducing the influence of long-tail distribution on multi-object tracking performance. The code is available at https://github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT.","sentences":["Multiple Object Tracking (MOT) is a critical area within computer vision, with a broad spectrum of practical implementations.","Current research has primarily focused on the development of tracking algorithms and enhancement of post-processing techniques.","Yet, there has been a lack of thorough examination concerning the nature of tracking data it self.","In this study, we pioneer an exploration into the distribution patterns of tracking data and identify a pronounced long-tail distribution issue within existing MOT datasets.","We note a significant imbalance in the distribution of trajectory lengths across different pedestrians, a phenomenon we refer to as \"pedestrians trajectory long-tail distribution\".","Addressing this challenge, we introduce a bespoke strategy designed to mitigate the effects of this skewed distribution.","Specifically, we propose two data augmentation strategies, including Stationary Camera View Data Augmentation (SVA) and Dynamic Camera View Data Augmentation (DVA) , designed for viewpoint states and the Group Softmax (GS) module for Re-ID.","SVA is to backtrack and predict the pedestrian trajectory of tail classes, and DVA is to use diffusion model to change the background of the scene.","GS divides the pedestrians into unrelated groups and performs softmax operation on each group individually.","Our proposed strategies can be integrated into numerous existing tracking systems, and extensive experimentation validates the efficacy of our method in reducing the influence of long-tail distribution on multi-object tracking performance.","The code is available at https://github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT."],"url":"http://arxiv.org/abs/2403.04700v1"}
{"created":"2024-03-07 17:46:50","title":"AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit Detectors","abstract":"Facial Action Units (AU) is a vital concept in the realm of affective computing, and AU detection has always been a hot research topic. Existing methods suffer from overfitting issues due to the utilization of a large number of learnable parameters on scarce AU-annotated datasets or heavy reliance on substantial additional relevant data. Parameter-Efficient Transfer Learning (PETL) provides a promising paradigm to address these challenges, whereas its existing methods lack design for AU characteristics. Therefore, we innovatively investigate PETL paradigm to AU detection, introducing AUFormer and proposing a novel Mixture-of-Knowledge Expert (MoKE) collaboration mechanism. An individual MoKE specific to a certain AU with minimal learnable parameters first integrates personalized multi-scale and correlation knowledge. Then the MoKE collaborates with other MoKEs in the expert group to obtain aggregated information and inject it into the frozen Vision Transformer (ViT) to achieve parameter-efficient AU detection. Additionally, we design a Margin-truncated Difficulty-aware Weighted Asymmetric Loss (MDWA-Loss), which can encourage the model to focus more on activated AUs, differentiate the difficulty of unactivated AUs, and discard potential mislabeled samples. Extensive experiments from various perspectives, including within-domain, cross-domain, data efficiency, and micro-expression domain, demonstrate AUFormer's state-of-the-art performance and robust generalization abilities without relying on additional relevant data. The code for AUFormer is available at https://github.com/yuankaishen2001/AUFormer.","sentences":["Facial Action Units (AU) is a vital concept in the realm of affective computing, and AU detection has always been a hot research topic.","Existing methods suffer from overfitting issues due to the utilization of a large number of learnable parameters on scarce AU-annotated datasets or heavy reliance on substantial additional relevant data.","Parameter-Efficient Transfer Learning (PETL) provides a promising paradigm to address these challenges, whereas its existing methods lack design for AU characteristics.","Therefore, we innovatively investigate PETL paradigm to AU detection, introducing AUFormer and proposing a novel Mixture-of-Knowledge Expert (MoKE) collaboration mechanism.","An individual MoKE specific to a certain AU with minimal learnable parameters first integrates personalized multi-scale and correlation knowledge.","Then the MoKE collaborates with other MoKEs","in the expert group to obtain aggregated information and inject it into the frozen Vision Transformer (ViT) to achieve parameter-efficient AU detection.","Additionally, we design a Margin-truncated Difficulty-aware Weighted Asymmetric Loss (MDWA-Loss), which can encourage the model to focus more on activated AUs, differentiate the difficulty of unactivated AUs, and discard potential mislabeled samples.","Extensive experiments from various perspectives, including within-domain, cross-domain, data efficiency, and micro-expression domain, demonstrate AUFormer's state-of-the-art performance and robust generalization abilities without relying on additional relevant data.","The code for AUFormer is available at https://github.com/yuankaishen2001/AUFormer."],"url":"http://arxiv.org/abs/2403.04697v1"}
{"created":"2024-03-07 17:43:21","title":"On $[1,2]$-Domination in Interval and Circle Graphs","abstract":"A subset $S$ of vertices in a graph $G=(V, E)$ is Dominating Set if each vertex in $V(G)\\setminus S$ is adjacent to at least one vertex in $S$. Chellali et al. in 2013, by restricting the number of neighbors in $S$ of a vertex outside $S$, introduced the concept of $[1,j]$-dominating set. A set $D \\subseteq V$ of a graph $G = (V, E)$ is called $[1,j]$-Dominating Set of $G$ if every vertex not in $D$ has at least one neighbor and at most $j$ neighbors in $D$. The Minimum $[1,j]$-Domination problem is the problem of finding the minimum set $D$. Given a positive integer $k$ and a graph $G = (V, E)$, the $[1,j]$-Domination Decision problem is to decide whether $G$ has $[1,j]$-dominating set of cardinality at most $k$. A polynomial-time algorithm was obtained in split graphs for a constant $j$ in contrast to the classic Dominating Set problem which is NP-hard in split graphs. This result motivates us to investigate the effect of restriction $j$ on the complexity of $[1,j]$-domination problem on various classes of graphs. Although for $j\\geq 3$, it has been proved that the minimum of classical domination is equal to minimum $[1,j]$-domination in interval graphs, the complexity of finding the minimum $[1,2]$-domination in interval graphs is still outstanding. In this paper, we propose a polynomial-time algorithm for computing a minimum $[1,2]$ on non-proper interval graphs by a dynamic programming technique. Next, on the negative side, we show that the minimum $[1,2]$-dominating set problem on circle graphs is $NP$-complete.","sentences":["A subset $S$ of vertices in a graph $G=(V, E)$ is Dominating Set if each vertex in $V(G)\\setminus S$ is adjacent to at least one vertex in $S$. Chellali et al.","in 2013, by restricting the number of neighbors in $S$ of a vertex outside $S$, introduced the concept of $[1,j]$-dominating set.","A set $D \\subseteq V$ of a graph $G = (V, E)$ is called $[1,j]$-Dominating Set of $G$ if every vertex not in $D$ has at least one neighbor and at most $j$ neighbors in $D$. The Minimum $[1,j]$-Domination problem is the problem of finding the minimum set $D$. Given a positive integer $k$ and a graph $G = (V, E)$, the $[1,j]$-Domination Decision problem is to decide whether $G$ has $[1,j]$-dominating set of cardinality at most $k$. A polynomial-time algorithm was obtained in split graphs for a constant $j$ in contrast to the classic Dominating Set problem which is NP-hard in split graphs.","This result motivates us to investigate the effect of restriction $j$ on the complexity of $[1,j]$-domination problem on various classes of graphs.","Although for $j\\geq 3$, it has been proved that the minimum of classical domination is equal to minimum $[1,j]$-domination in interval graphs, the complexity of finding the minimum $[1,2]$-domination in interval graphs is still outstanding.","In this paper, we propose a polynomial-time algorithm for computing a minimum $[1,2]$ on non-proper interval graphs by a dynamic programming technique.","Next, on the negative side, we show that the minimum $[1,2]$-dominating set problem on circle graphs is $NP$-complete."],"url":"http://arxiv.org/abs/2403.04694v1"}
{"created":"2024-03-07 17:41:37","title":"PixArt-\u03a3: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation","abstract":"In this paper, we introduce PixArt-\\Sigma, a Diffusion Transformer model~(DiT) capable of directly generating images at 4K resolution. PixArt-\\Sigma represents a significant advancement over its predecessor, PixArt-\\alpha, offering images of markedly higher fidelity and improved alignment with text prompts. A key feature of PixArt-\\Sigma is its training efficiency. Leveraging the foundational pre-training of PixArt-\\alpha, it evolves from the `weaker' baseline to a `stronger' model via incorporating higher quality data, a process we term \"weak-to-strong training\". The advancements in PixArt-\\Sigma are twofold: (1) High-Quality Training Data: PixArt-\\Sigma incorporates superior-quality image data, paired with more precise and detailed image captions. (2) Efficient Token Compression: we propose a novel attention module within the DiT framework that compresses both keys and values, significantly improving efficiency and facilitating ultra-high-resolution image generation. Thanks to these improvements, PixArt-\\Sigma achieves superior image quality and user prompt adherence capabilities with significantly smaller model size (0.6B parameters) than existing text-to-image diffusion models, such as SDXL (2.6B parameters) and SD Cascade (5.1B parameters). Moreover, PixArt-\\Sigma's capability to generate 4K images supports the creation of high-resolution posters and wallpapers, efficiently bolstering the production of high-quality visual content in industries such as film and gaming.","sentences":["In this paper, we introduce PixArt-\\Sigma, a Diffusion Transformer model~(DiT) capable of directly generating images at 4K resolution.","PixArt-\\Sigma represents a significant advancement over its predecessor, PixArt-\\alpha, offering images of markedly higher fidelity and improved alignment with text prompts.","A key feature of PixArt-\\Sigma is its training efficiency.","Leveraging the foundational pre-training of PixArt-\\alpha, it evolves from the `weaker' baseline to a `stronger' model via incorporating higher quality data, a process we term \"weak-to-strong training\".","The advancements in PixArt-\\Sigma are twofold: (1) High-Quality Training Data: PixArt-\\Sigma incorporates superior-quality image data, paired with more precise and detailed image captions.","(2) Efficient Token Compression: we propose a novel attention module within the DiT framework that compresses both keys and values, significantly improving efficiency and facilitating ultra-high-resolution image generation.","Thanks to these improvements, PixArt-\\Sigma achieves superior image quality and user prompt adherence capabilities with significantly smaller model size (0.6B parameters) than existing text-to-image diffusion models, such as SDXL (2.6B parameters) and SD Cascade (5.1B parameters).","Moreover, PixArt-\\Sigma's capability to generate 4K images supports the creation of high-resolution posters and wallpapers, efficiently bolstering the production of high-quality visual content in industries such as film and gaming."],"url":"http://arxiv.org/abs/2403.04692v1"}
{"created":"2024-03-07 16:52:49","title":"Yi: Open Foundation Models by 01.AI","abstract":"We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities. The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models. Our base models achieve strong performance on a wide range of benchmarks like MMLU, and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena. Building upon our scalable super-computing infrastructure and the classical transformer architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts. For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less than 10K) instruction dataset over multiple iterations such that every single instance has been verified directly by our machine learning engineers. For vision-language, we combine the chat language model with a vision transformer encoder and train the model to align visual representations to the semantic space of the language model. We further extend the context length to 200K through lightweight continual pretraining and demonstrate strong needle-in-a-haystack retrieval performance. We show that extending the depth of the pretrained checkpoint through continual pretraining further improves performance. We believe that given our current results, continuing to scale up model parameters using thoroughly optimized data will lead to even stronger frontier models.","sentences":["We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities.","The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models.","Our base models achieve strong performance on a wide range of benchmarks like MMLU, and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena.","Building upon our scalable super-computing infrastructure and the classical transformer architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts.","For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline.","For finetuning, we polish a small scale (less than 10K) instruction dataset over multiple iterations such that every single instance has been verified directly by our machine learning engineers.","For vision-language, we combine the chat language model with a vision transformer encoder and train the model to align visual representations to the semantic space of the language model.","We further extend the context length to 200K through lightweight continual pretraining and demonstrate strong needle-in-a-haystack retrieval performance.","We show that extending the depth of the pretrained checkpoint through continual pretraining further improves performance.","We believe that given our current results, continuing to scale up model parameters using thoroughly optimized data will lead to even stronger frontier models."],"url":"http://arxiv.org/abs/2403.04652v1"}
{"created":"2024-03-07 16:50:25","title":"Context-Based Multimodal Fusion","abstract":"The fusion models, which effectively combine information from different sources, are widely used in solving multimodal tasks. However, they have significant limitations related to aligning data distributions across different modalities. This challenge can lead to inconsistencies and difficulties in learning robust representations. Alignment models, while specifically addressing this issue, often require training \"from scratch\" with large datasets to achieve optimal results, which can be costly in terms of resources and time. To overcome these limitations, we propose an innovative model called Context-Based Multimodal Fusion (CBMF), which combines both modality fusion and data distribution alignment. In CBMF, each modality is represented by a specific context vector, fused with the embedding of each modality. This enables the use of large pre-trained models that can be frozen, reducing the computational and training data requirements. Additionally, the network learns to differentiate embeddings of different modalities through fusion with context and aligns data distributions using a contrastive approach for self-supervised learning. Thus, CBMF offers an effective and economical solution for solving complex multimodal tasks.","sentences":["The fusion models, which effectively combine information from different sources, are widely used in solving multimodal tasks.","However, they have significant limitations related to aligning data distributions across different modalities.","This challenge can lead to inconsistencies and difficulties in learning robust representations.","Alignment models, while specifically addressing this issue, often require training \"from scratch\" with large datasets to achieve optimal results, which can be costly in terms of resources and time.","To overcome these limitations, we propose an innovative model called Context-Based Multimodal Fusion (CBMF), which combines both modality fusion and data distribution alignment.","In CBMF, each modality is represented by a specific context vector, fused with the embedding of each modality.","This enables the use of large pre-trained models that can be frozen, reducing the computational and training data requirements.","Additionally, the network learns to differentiate embeddings of different modalities through fusion with context and aligns data distributions using a contrastive approach for self-supervised learning.","Thus, CBMF offers an effective and economical solution for solving complex multimodal tasks."],"url":"http://arxiv.org/abs/2403.04650v1"}
{"created":"2024-03-07 16:36:29","title":"Teaching Large Language Models to Reason with Reinforcement Learning","abstract":"Reinforcement Learning from Human Feedback (\\textbf{RLHF}) has emerged as a dominant approach for aligning LLM outputs with human preferences. Inspired by the success of RLHF, we study the performance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Optimization (\\textbf{PPO}), Return-Conditioned RL) on improving LLM reasoning capabilities. We investigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model. We additionally start from multiple model sizes and initializations both with and without supervised fine-tuning (\\textbf{SFT}) data. Overall, we find all algorithms perform comparably, with Expert Iteration performing best in most cases. Surprisingly, we find the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of $10^6$ samples to converge from a pretrained checkpoint. We investigate why this is the case, concluding that during RL training models fail to explore significantly beyond solutions already produced by SFT models. Additionally, we discuss a trade off between maj@1 and pass@96 metric performance during SFT training and how conversely RL training improves both simultaneously. We then conclude by discussing the implications of our findings for RLHF and the future role of RL in LLM fine-tuning.","sentences":["Reinforcement Learning from Human Feedback (\\textbf{RLHF}) has emerged as a dominant approach for aligning LLM outputs with human preferences.","Inspired by the success of RLHF, we study the performance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Optimization (\\textbf{PPO}), Return-Conditioned RL) on improving LLM reasoning capabilities.","We investigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model.","We additionally start from multiple model sizes and initializations both with and without supervised fine-tuning (\\textbf{SFT}) data.","Overall, we find all algorithms perform comparably, with Expert Iteration performing best in most cases.","Surprisingly, we find the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of $10^6","$ samples to converge from a pretrained checkpoint.","We investigate why this is the case, concluding that during RL training models fail to explore significantly beyond solutions already produced by SFT models.","Additionally, we discuss a trade off between maj@1 and pass@96 metric performance during SFT training and how conversely RL training improves both simultaneously.","We then conclude by discussing the implications of our findings for RLHF and the future role of RL in LLM fine-tuning."],"url":"http://arxiv.org/abs/2403.04642v1"}
{"created":"2024-03-07 16:29:19","title":"MaCmS: Magahi Code-mixed Dataset for Sentiment Analysis","abstract":"The present paper introduces new sentiment data, MaCMS, for Magahi-Hindi-English (MHE) code-mixed language, where Magahi is a less-resourced minority language. This dataset is the first Magahi-Hindi-English code-mixed dataset for sentiment analysis tasks. Further, we also provide a linguistics analysis of the dataset to understand the structure of code-mixing and a statistical study to understand the language preferences of speakers with different polarities. With these analyses, we also train baseline models to evaluate the dataset's quality.","sentences":["The present paper introduces new sentiment data, MaCMS, for Magahi-Hindi-English (MHE) code-mixed language, where Magahi is a less-resourced minority language.","This dataset is the first Magahi-Hindi-English code-mixed dataset for sentiment analysis tasks.","Further, we also provide a linguistics analysis of the dataset to understand the structure of code-mixing and a statistical study to understand the language preferences of speakers with different polarities.","With these analyses, we also train baseline models to evaluate the dataset's quality."],"url":"http://arxiv.org/abs/2403.04639v1"}
{"created":"2024-03-07 16:21:02","title":"Virtuoso: An Open-Source, Comprehensive and Modular Simulation Framework for Virtual Memory Research","abstract":"Virtual memory is a cornerstone of modern computing systems.Introduced as one of the earliest instances of hardware-software co-design, VM facilitates programmer-transparent memory man agement, data sharing, process isolation and memory protection. Evaluating the efficiency of various virtual memory (VM) designs is crucial (i) given their significant impact on the system, including the CPU caches, the main memory, and the storage device and (ii) given that different system architectures might benefit from various VM techniques. Such an evaluation is not straightforward, as it heavily hinges on modeling the interplay between different VM techniques and the interactions of VM with the system architecture. Modern simulators, however, struggle to keep up with the rapid VM research developments, lacking the capability to model a wide range of contemporary VM techniques and their interactions. To this end, we present Virtuoso, an open-source, comprehensive and modular simulation framework that models various VM designs to establish a common ground for virtual memory research. We demonstrate the versatility and the potential of Virtuoso with four new case studies. Virtuoso is freely open-source and can be found at https://github.com/CMU-SAFARI/Virtuoso.","sentences":["Virtual memory is a cornerstone of modern computing systems.","Introduced as one of the earliest instances of hardware-software co-design, VM facilitates programmer-transparent memory man agement, data sharing, process isolation and memory protection.","Evaluating the efficiency of various virtual memory (VM) designs is crucial (i) given their significant impact on the system, including the CPU caches, the main memory, and the storage device and (ii) given that different system architectures might benefit from various VM techniques.","Such an evaluation is not straightforward, as it heavily hinges on modeling the interplay between different VM techniques and the interactions of VM with the system architecture.","Modern simulators, however, struggle to keep up with the rapid VM research developments, lacking the capability to model a wide range of contemporary VM techniques and their interactions.","To this end, we present Virtuoso, an open-source, comprehensive and modular simulation framework that models various VM designs to establish a common ground for virtual memory research.","We demonstrate the versatility and the potential of Virtuoso with four new case studies.","Virtuoso is freely open-source and can be found at https://github.com/CMU-SAFARI/Virtuoso."],"url":"http://arxiv.org/abs/2403.04635v1"}
{"created":"2024-03-07 16:18:28","title":"Pix2Gif: Motion-Guided Diffusion for GIF Generation","abstract":"We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video) generation. We tackle this problem differently by formulating the task as an image translation problem steered by text and motion magnitude prompts, as shown in teaser fig. To ensure that the model adheres to motion guidance, we propose a new motion-guided warping module to spatially transform the features of the source image conditioned on the two types of prompts. Furthermore, we introduce a perceptual loss to ensure the transformed feature map remains within the same space as the target image, ensuring content consistency and coherence. In preparation for the model training, we meticulously curated data by extracting coherent image frames from the TGIF video-caption dataset, which provides rich information about the temporal changes of subjects. After pretraining, we apply our model in a zero-shot manner to a number of video datasets. Extensive qualitative and quantitative experiments demonstrate the effectiveness of our model -- it not only captures the semantic prompt from text but also the spatial ones from motion guidance. We train all our models using a single node of 16xV100 GPUs. Code, dataset and models are made public at: https://hiteshk03.github.io/Pix2Gif/.","sentences":["We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video) generation.","We tackle this problem differently by formulating the task as an image translation problem steered by text and motion magnitude prompts, as shown in teaser fig.","To ensure that the model adheres to motion guidance, we propose a new motion-guided warping module to spatially transform the features of the source image conditioned on the two types of prompts.","Furthermore, we introduce a perceptual loss to ensure the transformed feature map remains within the same space as the target image, ensuring content consistency and coherence.","In preparation for the model training, we meticulously curated data by extracting coherent image frames from the TGIF video-caption dataset, which provides rich information about the temporal changes of subjects.","After pretraining, we apply our model in a zero-shot manner to a number of video datasets.","Extensive qualitative and quantitative experiments demonstrate the effectiveness of our model -- it not only captures the semantic prompt from text but also the spatial ones from motion guidance.","We train all our models using a single node of 16xV100 GPUs.","Code, dataset and models are made public at: https://hiteshk03.github.io/Pix2Gif/."],"url":"http://arxiv.org/abs/2403.04634v1"}
{"created":"2024-03-07 16:14:08","title":"Time-Aware Projections: Truly Node-Private Graph Statistics under Continual Observation","abstract":"We describe the first algorithms that satisfy the standard notion of node-differential privacy in the continual release setting (i.e., without an assumed promise on input streams). Previous work addresses node-private continual release by assuming an unenforced promise on the maximum degree in a graph; indeed, the algorithms from these works exhibit blatant privacy violations when the degree bound is not met. Our algorithms are accurate on sparse graphs, for several fundamental graph problems: counting edges, triangles, other subgraphs, and connected components; and releasing degree histograms. Our unconditionally private algorithms generally have optimal error, up to polylogarithmic factors and lower-order terms.   We provide general transformations that take a base algorithm for the continual release setting, which need only be private for streams satisfying a promised degree bound, and produce an algorithm that is unconditionally private yet mimics the base algorithm when the stream meets the degree bound (and adds only linear overhead to the time and space complexity of the base algorithm). To do so, we design new projection algorithms for graph streams, based on the batch-model techniques of Day et al. 2016 and Blocki et al. 2013, which modify the stream to limit its degree. Our main technical innovation is to show that the projections are stable -- meaning that similar input graphs have similar projections -- when the input stream satisfies a privately testable safety condition. Our transformation then follows a novel online variant of the Propose-Test-Release framework (Dwork and Lei, 2009), privately testing the safety condition before releasing output at each step.","sentences":["We describe the first algorithms that satisfy the standard notion of node-differential privacy in the continual release setting (i.e., without an assumed promise on input streams).","Previous work addresses node-private continual release by assuming an unenforced promise on the maximum degree in a graph; indeed, the algorithms from these works exhibit blatant privacy violations when the degree bound is not met.","Our algorithms are accurate on sparse graphs, for several fundamental graph problems: counting edges, triangles, other subgraphs, and connected components; and releasing degree histograms.","Our unconditionally private algorithms generally have optimal error, up to polylogarithmic factors and lower-order terms.   ","We provide general transformations that take a base algorithm for the continual release setting, which need only be private for streams satisfying a promised degree bound, and produce an algorithm that is unconditionally private yet mimics the base algorithm when the stream meets the degree bound (and adds only linear overhead to the time and space complexity of the base algorithm).","To do so, we design new projection algorithms for graph streams, based on the batch-model techniques of Day et al. 2016 and Blocki et al. 2013, which modify the stream to limit its degree.","Our main technical innovation is to show that the projections are stable -- meaning that similar input graphs have similar projections -- when the input stream satisfies a privately testable safety condition.","Our transformation then follows a novel online variant of the Propose-Test-Release framework (Dwork and Lei, 2009), privately testing the safety condition before releasing output at each step."],"url":"http://arxiv.org/abs/2403.04630v1"}
{"created":"2024-03-07 15:54:46","title":"In-n-Out: Calibrating Graph Neural Networks for Link Prediction","abstract":"Deep neural networks are notoriously miscalibrated, i.e., their outputs do not reflect the true probability of the event we aim to predict. While networks for tabular or image data are usually overconfident, recent works have shown that graph neural networks (GNNs) show the opposite behavior for node-level classification. But what happens when we are predicting links? We show that, in this case, GNNs often exhibit a mixed behavior. More specifically, they may be overconfident in negative predictions while being underconfident in positive ones. Based on this observation, we propose IN-N-OUT, the first-ever method to calibrate GNNs for link prediction. IN-N-OUT is based on two simple intuitions: i) attributing true/false labels to an edge while respecting a GNNs prediction should cause but small fluctuations in that edge's embedding; and, conversely, ii) if we label that same edge contradicting our GNN, embeddings should change more substantially. An extensive experimental campaign shows that IN-N-OUT significantly improves the calibration of GNNs in link prediction, consistently outperforming the baselines available -- which are not designed for this specific task.","sentences":["Deep neural networks are notoriously miscalibrated, i.e., their outputs do not reflect the true probability of the event we aim to predict.","While networks for tabular or image data are usually overconfident, recent works have shown that graph neural networks (GNNs) show the opposite behavior for node-level classification.","But what happens when we are predicting links?","We show that, in this case, GNNs often exhibit a mixed behavior.","More specifically, they may be overconfident in negative predictions while being underconfident in positive ones.","Based on this observation, we propose IN-N-OUT, the first-ever method to calibrate GNNs for link prediction.","IN-N-OUT is based on two simple intuitions: i) attributing true/false labels to an edge while respecting a GNNs prediction should cause but small fluctuations in that edge's embedding; and, conversely, ii) if we label that same edge contradicting our GNN, embeddings should change more substantially.","An extensive experimental campaign shows that IN-N-OUT significantly improves the calibration of GNNs in link prediction, consistently outperforming the baselines available -- which are not designed for this specific task."],"url":"http://arxiv.org/abs/2403.04605v1"}
{"created":"2024-03-07 15:47:52","title":"Contrastive Continual Learning with Importance Sampling and Prototype-Instance Relation Distillation","abstract":"Recently, because of the high-quality representations of contrastive learning methods, rehearsal-based contrastive continual learning has been proposed to explore how to continually learn transferable representation embeddings to avoid the catastrophic forgetting issue in traditional continual settings. Based on this framework, we propose Contrastive Continual Learning via Importance Sampling (CCLIS) to preserve knowledge by recovering previous data distributions with a new strategy for Replay Buffer Selection (RBS), which minimize estimated variance to save hard negative samples for representation learning with high quality. Furthermore, we present the Prototype-instance Relation Distillation (PRD) loss, a technique designed to maintain the relationship between prototypes and sample representations using a self-distillation process. Experiments on standard continual learning benchmarks reveal that our method notably outperforms existing baselines in terms of knowledge preservation and thereby effectively counteracts catastrophic forgetting in online contexts. The code is available at https://github.com/lijy373/CCLIS.","sentences":["Recently, because of the high-quality representations of contrastive learning methods, rehearsal-based contrastive continual learning has been proposed to explore how to continually learn transferable representation embeddings to avoid the catastrophic forgetting issue in traditional continual settings.","Based on this framework, we propose Contrastive Continual Learning via Importance Sampling (CCLIS) to preserve knowledge by recovering previous data distributions with a new strategy for Replay Buffer Selection (RBS), which minimize estimated variance to save hard negative samples for representation learning with high quality.","Furthermore, we present the Prototype-instance Relation Distillation (PRD) loss, a technique designed to maintain the relationship between prototypes and sample representations using a self-distillation process.","Experiments on standard continual learning benchmarks reveal that our method notably outperforms existing baselines in terms of knowledge preservation and thereby effectively counteracts catastrophic forgetting in online contexts.","The code is available at https://github.com/lijy373/CCLIS."],"url":"http://arxiv.org/abs/2403.04599v1"}
{"created":"2024-03-07 15:46:19","title":"Optimizing Inventory Placement for a Downstream Online Matching Problem","abstract":"We study the inventory placement problem of splitting $Q$ units of a single item across warehouses, in advance of a downstream online matching problem that represents the dynamic fulfillment decisions of an e-commerce retailer. This is a challenging problem both in theory, because the downstream matching problem itself is computationally hard, and in practice, because the fulfillment team is constantly updating its algorithm and the placement team cannot directly evaluate how a placement decision would perform.   We compare the performance of three placement procedures based on optimizing surrogate functions that have been studied and applied: Offline, Myopic, and Fluid placement. On the theory side, we show that optimizing inventory placement for the Offline surrogate leads to a $(1-(1-1/d)^d)/2$-approximation for the joint placement and fulfillment problem. We assume $d$ is an upper bound on how many warehouses can serve any demand location and that stochastic arrivals satisfy either temporal or spatial independence. The crux of our theoretical contribution is to use randomized rounding to derive a tight $(1-(1-1/d)^d)$-approximation for the integer programming problem of optimizing the Offline surrogate. We use statistical learning to show that rounding after optimizing a sample-average Offline surrogate, which is necessary due to the exponentially-sized support, does indeed have vanishing loss.   On the experimental side, we extract real-world sequences of customer orders from publicly-available JD.com data and evaluate different combinations of placement and fulfillment procedures. Optimizing the Offline surrogate performs best overall, even compared to simulation procedures, corroborating our theory.","sentences":["We study the inventory placement problem of splitting $Q$ units of a single item across warehouses, in advance of a downstream online matching problem that represents the dynamic fulfillment decisions of an e-commerce retailer.","This is a challenging problem both in theory, because the downstream matching problem itself is computationally hard, and in practice, because the fulfillment team is constantly updating its algorithm and the placement team cannot directly evaluate how a placement decision would perform.   ","We compare the performance of three placement procedures based on optimizing surrogate functions that have been studied and applied: Offline, Myopic, and Fluid placement.","On the theory side, we show that optimizing inventory placement for the Offline surrogate leads to a $(1-(1-1/d)^d)/2$-approximation for the joint placement and fulfillment problem.","We assume $d$ is an upper bound on how many warehouses can serve any demand location and that stochastic arrivals satisfy either temporal or spatial independence.","The crux of our theoretical contribution is to use randomized rounding to derive a tight $(1-(1-1/d)^d)$-approximation for the integer programming problem of optimizing the Offline surrogate.","We use statistical learning to show that rounding after optimizing a sample-average Offline surrogate, which is necessary due to the exponentially-sized support, does indeed have vanishing loss.   ","On the experimental side, we extract real-world sequences of customer orders from publicly-available JD.com data and evaluate different combinations of placement and fulfillment procedures.","Optimizing the Offline surrogate performs best overall, even compared to simulation procedures, corroborating our theory."],"url":"http://arxiv.org/abs/2403.04598v1"}
{"created":"2024-03-07 15:40:01","title":"A Detailed Audio-Text Data Simulation Pipeline using Single-Event Sounds","abstract":"Recently, there has been an increasing focus on audio-text cross-modal learning. However, most of the existing audio-text datasets contain only simple descriptions of sound events. Compared with classification labels, the advantages of such descriptions are significantly limited. In this paper, we first analyze the detailed information that human descriptions of audio may contain beyond sound event labels. Based on the analysis, we propose an automatic pipeline for curating audio-text pairs with rich details. Leveraging the property that sounds can be mixed and concatenated in the time domain, we control details in four aspects: temporal relationship, loudness, speaker identity, and occurrence number, in simulating audio mixtures. Corresponding details are transformed into captions by large language models. Audio-text pairs with rich details in text descriptions are thereby obtained. We validate the effectiveness of our pipeline with a small amount of simulated data, demonstrating that the simulated data enables models to learn detailed audio captioning.","sentences":["Recently, there has been an increasing focus on audio-text cross-modal learning.","However, most of the existing audio-text datasets contain only simple descriptions of sound events.","Compared with classification labels, the advantages of such descriptions are significantly limited.","In this paper, we first analyze the detailed information that human descriptions of audio may contain beyond sound event labels.","Based on the analysis, we propose an automatic pipeline for curating audio-text pairs with rich details.","Leveraging the property that sounds can be mixed and concatenated in the time domain, we control details in four aspects: temporal relationship, loudness, speaker identity, and occurrence number, in simulating audio mixtures.","Corresponding details are transformed into captions by large language models.","Audio-text pairs with rich details in text descriptions are thereby obtained.","We validate the effectiveness of our pipeline with a small amount of simulated data, demonstrating that the simulated data enables models to learn detailed audio captioning."],"url":"http://arxiv.org/abs/2403.04594v1"}
{"created":"2024-03-07 15:39:18","title":"Embodied Understanding of Driving Scenarios","abstract":"Embodied scene understanding serves as the cornerstone for autonomous agents to perceive, interpret, and respond to open driving scenarios. Such understanding is typically founded upon Vision-Language Models (VLMs). Nevertheless, existing VLMs are restricted to the 2D domain, devoid of spatial awareness and long-horizon extrapolation proficiencies. We revisit the key aspects of autonomous driving and formulate appropriate rubrics. Hereby, we introduce the Embodied Language Model (ELM), a comprehensive framework tailored for agents' understanding of driving scenes with large spatial and temporal spans. ELM incorporates space-aware pre-training to endow the agent with robust spatial localization capabilities. Besides, the model employs time-aware token selection to accurately inquire about temporal cues. We instantiate ELM on the reformulated multi-faced benchmark, and it surpasses previous state-of-the-art approaches in all aspects. All code, data, and models will be publicly shared.","sentences":["Embodied scene understanding serves as the cornerstone for autonomous agents to perceive, interpret, and respond to open driving scenarios.","Such understanding is typically founded upon Vision-Language Models (VLMs).","Nevertheless, existing VLMs are restricted to the 2D domain, devoid of spatial awareness and long-horizon extrapolation proficiencies.","We revisit the key aspects of autonomous driving and formulate appropriate rubrics.","Hereby, we introduce the Embodied Language Model (ELM), a comprehensive framework tailored for agents' understanding of driving scenes with large spatial and temporal spans.","ELM incorporates space-aware pre-training to endow the agent with robust spatial localization capabilities.","Besides, the model employs time-aware token selection to accurately inquire about temporal cues.","We instantiate ELM on the reformulated multi-faced benchmark, and it surpasses previous state-of-the-art approaches in all aspects.","All code, data, and models will be publicly shared."],"url":"http://arxiv.org/abs/2403.04593v1"}
{"created":"2024-03-07 15:35:36","title":"Algorithms and complexity for path covers of temporal DAGs: when is Dilworth dynamic?","abstract":"In this paper, we study a dynamic analogue of the Path Cover problem, which can be solved in polynomial-time in directed acyclic graphs. A temporal digraph has an arc set that changes over discrete time-steps, if the underlying digraph (the union of all the arc sets) is acyclic, then we have a temporal DAG. A temporal path is a directed path in the underlying digraph, such that the time-steps of arcs are strictly increasing along the path. Two temporal paths are temporally disjoint if they do not occupy any vertex at the same time. A temporal (resp. temporally disjoint) path cover is a collection of (resp. temporally disjoint) temporal paths that covers all vertices. In this paper, we study the computational complexities of the problems of finding a temporal (disjoint) path cover with minimum cardinality, denoted as Temporal Path Cover (TPC) and Temporally Disjoint Path Cover (TD-PC). We show that both problems are NP-hard even when the underlying DAG is planar, bipartite, subcubic, and there are only two arc-disjoint time-steps. Moreover, TD-PC remains NP-hard even on temporal oriented trees. In contrast, we show that TPC is polynomial-time solvable on temporal oriented trees by a reduction to Clique Cover for (static undirected) weakly chordal graphs (a subclass of perfect graphs for which Clique Cover admits an efficient algorithm). This highlights an interesting algorithmic difference between the two problems. Although it is NP-hard on temporal oriented trees, TD-PC becomes polynomial-time solvable on temporal oriented lines and temporal rooted directed trees. We also show that TPC (resp. TD-PC) admits an XP (resp. FPT) time algorithm with respect to parameter tmax + tw, where tmax is the maximum time-step, and tw is the treewidth of the underlying static undirected graph.","sentences":["In this paper, we study a dynamic analogue of the Path Cover problem, which can be solved in polynomial-time in directed acyclic graphs.","A temporal digraph has an arc set that changes over discrete time-steps, if the underlying digraph (the union of all the arc sets) is acyclic, then we have a temporal DAG.","A temporal path is a directed path in the underlying digraph, such that the time-steps of arcs are strictly increasing along the path.","Two temporal paths are temporally disjoint if they do not occupy any vertex at the same time.","A temporal (resp.","temporally disjoint) path cover is a collection of (resp.","temporally disjoint) temporal paths that covers all vertices.","In this paper, we study the computational complexities of the problems of finding a temporal (disjoint) path cover with minimum cardinality, denoted as Temporal Path Cover (TPC) and Temporally Disjoint Path Cover (TD-PC).","We show that both problems are NP-hard even when the underlying DAG is planar, bipartite, subcubic, and there are only two arc-disjoint time-steps.","Moreover, TD-PC remains NP-hard even on temporal oriented trees.","In contrast, we show that TPC is polynomial-time solvable on temporal oriented trees by a reduction to Clique Cover for (static undirected) weakly chordal graphs (a subclass of perfect graphs for which Clique Cover admits an efficient algorithm).","This highlights an interesting algorithmic difference between the two problems.","Although it is NP-hard on temporal oriented trees, TD-PC becomes polynomial-time solvable on temporal oriented lines and temporal rooted directed trees.","We also show that TPC (resp.","TD-PC) admits an XP (resp.","FPT) time algorithm with respect to parameter tmax + tw, where tmax is the maximum time-step, and tw is the treewidth of the underlying static undirected graph."],"url":"http://arxiv.org/abs/2403.04589v1"}
{"created":"2024-03-07 15:29:04","title":"What Cannot be Skipped About the Skiplist: A Survey of Skiplists and Their Applications in Big Data Systems","abstract":"Skiplists have become prevalent in systems. The main advantages of skiplists are their simplicity and ease of implementation, and the ability to support operations in the same asymptotic complexities as their tree-based counterparts. In this survey, we explore skiplists and their many variants. We highlight many scenarios of how skiplists are useful and fit well in these usage scenarios. We study several extensions to skiplists to make them fit for more applications, e.g., their use in the multi-dimensional space, network overlaying algorithms, as well as serving as indexes in database systems. Besides, we also discuss systems that adopt the idea of skiplists and apply the probabilistic skip pattern into their designs.","sentences":["Skiplists have become prevalent in systems.","The main advantages of skiplists are their simplicity and ease of implementation, and the ability to support operations in the same asymptotic complexities as their tree-based counterparts.","In this survey, we explore skiplists and their many variants.","We highlight many scenarios of how skiplists are useful and fit well in these usage scenarios.","We study several extensions to skiplists to make them fit for more applications, e.g., their use in the multi-dimensional space, network overlaying algorithms, as well as serving as indexes in database systems.","Besides, we also discuss systems that adopt the idea of skiplists and apply the probabilistic skip pattern into their designs."],"url":"http://arxiv.org/abs/2403.04582v1"}
{"created":"2024-03-07 15:06:24","title":"ShuffleBench: A Benchmark for Large-Scale Data Shuffling Operations with Distributed Stream Processing Frameworks","abstract":"Distributed stream processing frameworks help building scalable and reliable applications that perform transformations and aggregations on continuous data streams. This paper introduces ShuffleBench, a novel benchmark to evaluate the performance of modern stream processing frameworks. In contrast to other benchmarks, it focuses on use cases where stream processing frameworks are mainly employed for shuffling (i.e., re-distributing) data records to perform state-local aggregations, while the actual aggregation logic is considered as black-box software components. ShuffleBench is inspired by requirements for near real-time analytics of a large cloud observability platform and takes up benchmarking metrics and methods for latency, throughput, and scalability established in the performance engineering research community. Although inspired by a real-world observability use case, it is highly configurable to allow domain-independent evaluations. ShuffleBench comes as a ready-to-use open-source software utilizing existing Kubernetes tooling and providing implementations for four state-of-the-art frameworks. Therefore, we expect ShuffleBench to be a valuable contribution to both industrial practitioners building stream processing applications and researchers working on new stream processing approaches. We complement this paper with an experimental performance evaluation that employs ShuffleBench with various configurations on Flink, Hazelcast, Kafka Streams, and Spark in a cloud-native environment. Our results show that Flink achieves the highest throughput while Hazelcast processes data streams with the lowest latency.","sentences":["Distributed stream processing frameworks help building scalable and reliable applications that perform transformations and aggregations on continuous data streams.","This paper introduces ShuffleBench, a novel benchmark to evaluate the performance of modern stream processing frameworks.","In contrast to other benchmarks, it focuses on use cases where stream processing frameworks are mainly employed for shuffling (i.e., re-distributing) data records to perform state-local aggregations, while the actual aggregation logic is considered as black-box software components.","ShuffleBench is inspired by requirements for near real-time analytics of a large cloud observability platform and takes up benchmarking metrics and methods for latency, throughput, and scalability established in the performance engineering research community.","Although inspired by a real-world observability use case, it is highly configurable to allow domain-independent evaluations.","ShuffleBench comes as a ready-to-use open-source software utilizing existing Kubernetes tooling and providing implementations for four state-of-the-art frameworks.","Therefore, we expect ShuffleBench to be a valuable contribution to both industrial practitioners building stream processing applications and researchers working on new stream processing approaches.","We complement this paper with an experimental performance evaluation that employs ShuffleBench with various configurations on Flink, Hazelcast, Kafka Streams, and Spark in a cloud-native environment.","Our results show that Flink achieves the highest throughput while Hazelcast processes data streams with the lowest latency."],"url":"http://arxiv.org/abs/2403.04570v1"}
{"created":"2024-03-07 14:56:06","title":"Reducing self-supervised learning complexity improves weakly-supervised classification performance in computational pathology","abstract":"Deep Learning models have been successfully utilized to extract clinically actionable insights from routinely available histology data. Generally, these models require annotations performed by clinicians, which are scarce and costly to generate. The emergence of self-supervised learning (SSL) methods remove this barrier, allowing for large-scale analyses on non-annotated data. However, recent SSL approaches apply increasingly expansive model architectures and larger datasets, causing the rapid escalation of data volumes, hardware prerequisites, and overall expenses, limiting access to these resources to few institutions. Therefore, we investigated the complexity of contrastive SSL in computational pathology in relation to classification performance with the utilization of consumer-grade hardware. Specifically, we analyzed the effects of adaptations in data volume, architecture, and algorithms on downstream classification tasks, emphasizing their impact on computational resources. We trained breast cancer foundation models on a large public patient cohort and validated them on various downstream classification tasks in a weakly supervised manner on two external public patient cohorts. Our experiments demonstrate that we can improve downstream classification performance whilst reducing SSL training duration by 90%. In summary, we propose a set of adaptations which enable the utilization of SSL in computational pathology in non-resource abundant environments.","sentences":["Deep Learning models have been successfully utilized to extract clinically actionable insights from routinely available histology data.","Generally, these models require annotations performed by clinicians, which are scarce and costly to generate.","The emergence of self-supervised learning (SSL) methods remove this barrier, allowing for large-scale analyses on non-annotated data.","However, recent SSL approaches apply increasingly expansive model architectures and larger datasets, causing the rapid escalation of data volumes, hardware prerequisites, and overall expenses, limiting access to these resources to few institutions.","Therefore, we investigated the complexity of contrastive SSL in computational pathology in relation to classification performance with the utilization of consumer-grade hardware.","Specifically, we analyzed the effects of adaptations in data volume, architecture, and algorithms on downstream classification tasks, emphasizing their impact on computational resources.","We trained breast cancer foundation models on a large public patient cohort and validated them on various downstream classification tasks in a weakly supervised manner on two external public patient cohorts.","Our experiments demonstrate that we can improve downstream classification performance whilst reducing SSL training duration by 90%.","In summary, we propose a set of adaptations which enable the utilization of SSL in computational pathology in non-resource abundant environments."],"url":"http://arxiv.org/abs/2403.04558v1"}
{"created":"2024-03-07 14:45:03","title":"Dissecting Sample Hardness: A Fine-Grained Analysis of Hardness Characterization Methods for Data-Centric AI","abstract":"Characterizing samples that are difficult to learn from is crucial to developing highly performant ML models. This has led to numerous Hardness Characterization Methods (HCMs) that aim to identify \"hard\" samples. However, there is a lack of consensus regarding the definition and evaluation of \"hardness\". Unfortunately, current HCMs have only been evaluated on specific types of hardness and often only qualitatively or with respect to downstream performance, overlooking the fundamental quantitative identification task. We address this gap by presenting a fine-grained taxonomy of hardness types. Additionally, we propose the Hardness Characterization Analysis Toolkit (H-CAT), which supports comprehensive and quantitative benchmarking of HCMs across the hardness taxonomy and can easily be extended to new HCMs, hardness types, and datasets. We use H-CAT to evaluate 13 different HCMs across 8 hardness types. This comprehensive evaluation encompassing over 14K setups uncovers strengths and weaknesses of different HCMs, leading to practical tips to guide HCM selection and future development. Our findings highlight the need for more comprehensive HCM evaluation, while we hope our hardness taxonomy and toolkit will advance the principled evaluation and uptake of data-centric AI methods.","sentences":["Characterizing samples that are difficult to learn from is crucial to developing highly performant ML models.","This has led to numerous Hardness Characterization Methods (HCMs) that aim to identify \"hard\" samples.","However, there is a lack of consensus regarding the definition and evaluation of \"hardness\".","Unfortunately, current HCMs have only been evaluated on specific types of hardness and often only qualitatively or with respect to downstream performance, overlooking the fundamental quantitative identification task.","We address this gap by presenting a fine-grained taxonomy of hardness types.","Additionally, we propose the Hardness Characterization Analysis Toolkit (H-CAT), which supports comprehensive and quantitative benchmarking of HCMs across the hardness taxonomy and can easily be extended to new HCMs, hardness types, and datasets.","We use H-CAT to evaluate 13 different HCMs across 8 hardness types.","This comprehensive evaluation encompassing over 14K setups uncovers strengths and weaknesses of different HCMs, leading to practical tips to guide HCM selection and future development.","Our findings highlight the need for more comprehensive HCM evaluation, while we hope our hardness taxonomy and toolkit will advance the principled evaluation and uptake of data-centric AI methods."],"url":"http://arxiv.org/abs/2403.04551v1"}
{"created":"2024-03-07 14:43:17","title":"CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?","abstract":"We study the effectiveness of data-balancing for mitigating biases in contrastive language-image pretraining (CLIP), identifying areas of strength and limitation. First, we reaffirm prior conclusions that CLIP models can inadvertently absorb societal stereotypes. To counter this, we present a novel algorithm, called Multi-Modal Moment Matching (M4), designed to reduce both representation and association biases (i.e. in first- and second-order statistics) in multimodal data. We use M4 to conduct an in-depth analysis taking into account various factors, such as the model, representation, and data size. Our study also explores the dynamic nature of how CLIP learns and unlearns biases. In particular, we find that fine-tuning is effective in countering representation biases, though its impact diminishes for association biases. Also, data balancing has a mixed impact on quality: it tends to improve classification but can hurt retrieval. Interestingly, data and architectural improvements seem to mitigate the negative impact of data balancing on performance; e.g. applying M4 to SigLIP-B/16 with data quality filters improves COCO image-to-text retrieval @5 from 86% (without data balancing) to 87% and ImageNet 0-shot classification from 77% to 77.5%! Finally, we conclude with recommendations for improving the efficacy of data balancing in multimodal systems.","sentences":["We study the effectiveness of data-balancing for mitigating biases in contrastive language-image pretraining (CLIP), identifying areas of strength and limitation.","First, we reaffirm prior conclusions that CLIP models can inadvertently absorb societal stereotypes.","To counter this, we present a novel algorithm, called Multi-Modal Moment Matching (M4), designed to reduce both representation and association biases (i.e. in first- and second-order statistics) in multimodal data.","We use M4 to conduct an in-depth analysis taking into account various factors, such as the model, representation, and data size.","Our study also explores the dynamic nature of how CLIP learns and unlearns biases.","In particular, we find that fine-tuning is effective in countering representation biases, though its impact diminishes for association biases.","Also, data balancing has a mixed impact on quality: it tends to improve classification but can hurt retrieval.","Interestingly, data and architectural improvements seem to mitigate the negative impact of data balancing on performance; e.g. applying M4 to SigLIP-B/16 with data quality filters improves COCO image-to-text retrieval @5 from 86% (without data balancing) to 87% and ImageNet 0-shot classification from 77% to 77.5%!","Finally, we conclude with recommendations for improving the efficacy of data balancing in multimodal systems."],"url":"http://arxiv.org/abs/2403.04547v1"}
{"created":"2024-03-07 14:42:33","title":"Architectural Blueprint For Heterogeneity-Resilient Federated Learning","abstract":"This paper proposes a novel three tier architecture for federated learning to optimize edge computing environments. The proposed architecture addresses the challenges associated with client data heterogeneity and computational constraints. It introduces a scalable, privacy preserving framework that enhances the efficiency of distributed machine learning. Through experimentation, the paper demonstrates the architecture capability to manage non IID data sets more effectively than traditional federated learning models. Additionally, the paper highlights the potential of this innovative approach to significantly improve model accuracy, reduce communication overhead, and facilitate broader adoption of federated learning technologies.","sentences":["This paper proposes a novel three tier architecture for federated learning to optimize edge computing environments.","The proposed architecture addresses the challenges associated with client data heterogeneity and computational constraints.","It introduces a scalable, privacy preserving framework that enhances the efficiency of distributed machine learning.","Through experimentation, the paper demonstrates the architecture capability to manage non IID data sets more effectively than traditional federated learning models.","Additionally, the paper highlights the potential of this innovative approach to significantly improve model accuracy, reduce communication overhead, and facilitate broader adoption of federated learning technologies."],"url":"http://arxiv.org/abs/2403.04546v1"}
{"created":"2024-03-07 14:40:53","title":"Improve Generalization Ability of Deep Wide Residual Network with A Suitable Scaling Factor","abstract":"Deep Residual Neural Networks (ResNets) have demonstrated remarkable success across a wide range of real-world applications. In this paper, we identify a suitable scaling factor (denoted by $\\alpha$) on the residual branch of deep wide ResNets to achieve good generalization ability. We show that if $\\alpha$ is a constant, the class of functions induced by Residual Neural Tangent Kernel (RNTK) is asymptotically not learnable, as the depth goes to infinity. We also highlight a surprising phenomenon: even if we allow $\\alpha$ to decrease with increasing depth $L$, the degeneration phenomenon may still occur. However, when $\\alpha$ decreases rapidly with $L$, the kernel regression with deep RNTK with early stopping can achieve the minimax rate provided that the target regression function falls in the reproducing kernel Hilbert space associated with the infinite-depth RNTK. Our simulation studies on synthetic data and real classification tasks such as MNIST, CIFAR10 and CIFAR100 support our theoretical criteria for choosing $\\alpha$.","sentences":["Deep Residual Neural Networks (ResNets) have demonstrated remarkable success across a wide range of real-world applications.","In this paper, we identify a suitable scaling factor (denoted by $\\alpha$) on the residual branch of deep wide ResNets to achieve good generalization ability.","We show that if $\\alpha$ is a constant, the class of functions induced by Residual Neural Tangent Kernel (RNTK) is asymptotically not learnable, as the depth goes to infinity.","We also highlight a surprising phenomenon: even if we allow $\\alpha$ to decrease with increasing depth $L$, the degeneration phenomenon may still occur.","However, when $\\alpha$ decreases rapidly with $L$, the kernel regression with deep RNTK with early stopping can achieve the minimax rate provided that the target regression function falls in the reproducing kernel Hilbert space associated with the infinite-depth RNTK.","Our simulation studies on synthetic data and real classification tasks such as MNIST, CIFAR10 and CIFAR100 support our theoretical criteria for choosing $\\alpha$."],"url":"http://arxiv.org/abs/2403.04545v1"}
{"created":"2024-03-07 14:40:07","title":"A Simple and Near-Optimal Algorithm for Directed Expander Decompositions","abstract":"In this work, we present the first algorithm to compute expander decompositions in an $m$-edge directed graph with near-optimal time $\\tilde{O}(m)$. Further, our algorithm can maintain such a decomposition in a dynamic graph and again obtains near-optimal update times. Our result improves over previous algorithms of Bernstein-Probst Gutenberg-Saranurak (FOCS 2020), Hua-Kyng-Probst Gutenberg-Wu (SODA 2023) that only obtained algorithms optimal up to subpolynomial factors. At the same time, our algorithm is much simpler and more accessible than previous work. In order to obtain our new algorithm, we present a new push-pull-relabel flow framework that generalizes the classic push-relabel flow algorithm of Goldberg-Tarjan (JACM 1988), which was later dynamized for computing expander decompositions in undirected graphs by Henzinger-Rao-Wang (SIAM J. Comput. 2020), Saranurak-Wang (SODA 2019). We then show that the flow problems formulated in recent work of Hua-Kyng-Probst Gutenberg-Wu (SODA 2023) to decompose directed graphs can be solved much more efficiently in the push-pull-relabel flow framework.","sentences":["In this work, we present the first algorithm to compute expander decompositions in an $m$-edge directed graph with near-optimal time $\\tilde{O}(m)$. Further, our algorithm can maintain such a decomposition in a dynamic graph and again obtains near-optimal update times.","Our result improves over previous algorithms of Bernstein-Probst Gutenberg-Saranurak (FOCS 2020), Hua-Kyng-Probst Gutenberg-Wu (SODA 2023) that only obtained algorithms optimal up to subpolynomial factors.","At the same time, our algorithm is much simpler and more accessible than previous work.","In order to obtain our new algorithm, we present a new push-pull-relabel flow framework that generalizes the classic push-relabel flow algorithm of Goldberg-Tarjan (JACM 1988), which was later dynamized for computing expander decompositions in undirected graphs by Henzinger-Rao-Wang (SIAM J. Comput. 2020), Saranurak-Wang (SODA 2019).","We then show that the flow problems formulated in recent work of Hua-Kyng-Probst Gutenberg-Wu (SODA 2023) to decompose directed graphs can be solved much more efficiently in the push-pull-relabel flow framework."],"url":"http://arxiv.org/abs/2403.04542v1"}
{"created":"2024-03-07 14:36:26","title":"PUMA: Efficient and Low-Cost Memory Allocation and Alignment Support for Processing-Using-Memory Architectures","abstract":"Processing-using-DRAM (PUD) architectures impose a restrictive data layout and alignment for their operands, where source and destination operands (i) must reside in the same DRAM subarray (i.e., a group of DRAM rows sharing the same row buffer and row decoder) and (ii) are aligned to the boundaries of a DRAM row. However, standard memory allocation routines (i.e., malloc, posix_memalign, and huge pages-based memory allocation) fail to meet the data layout and alignment requirements for PUD architectures to operate successfully. To allow the memory allocation API to influence the OS memory allocator and ensure that memory objects are placed within specific DRAM subarrays, we propose a new lazy data allocation routine (in the kernel) for PUD memory objects called PUMA. The key idea of PUMA is to use the internal DRAM mapping information together with huge pages and then split huge pages into finer-grained allocation units that are (i) aligned to the page address and size and (ii) virtually contiguous.   We implement PUMA as a kernel module using QEMU and emulate a RISC-V machine running Fedora 33 with v5.9.0 Linux Kernel. We emulate the implementation of a PUD system capable of executing row copy operations (as in RowClone) and Boolean AND/OR/NOT operations (as in Ambit). In our experiments, such an operation is performed in the host CPU if a given operation cannot be executed in our PUD substrate (due to data misalignment). PUMA significantly outperforms the baseline memory allocators for all evaluated microbenchmarks and allocation sizes.","sentences":["Processing-using-DRAM (PUD) architectures impose a restrictive data layout and alignment for their operands, where source and destination operands (i) must reside in the same DRAM subarray (i.e., a group of DRAM rows sharing the same row buffer and row decoder) and (ii) are aligned to the boundaries of a DRAM row.","However, standard memory allocation routines (i.e., malloc, posix_memalign, and huge pages-based memory allocation) fail to meet the data layout and alignment requirements for PUD architectures to operate successfully.","To allow the memory allocation API to influence the OS memory allocator and ensure that memory objects are placed within specific DRAM subarrays, we propose a new lazy data allocation routine (in the kernel) for PUD memory objects called PUMA.","The key idea of PUMA is to use the internal DRAM mapping information together with huge pages and then split huge pages into finer-grained allocation units that are (i) aligned to the page address and size and (ii) virtually contiguous.   ","We implement PUMA as a kernel module using QEMU and emulate a RISC-V machine running Fedora 33 with v5.9.0 Linux Kernel.","We emulate the implementation of a PUD system capable of executing row copy operations (as in RowClone) and Boolean AND/OR/NOT operations (as in Ambit).","In our experiments, such an operation is performed in the host CPU if a given operation cannot be executed in our PUD substrate (due to data misalignment).","PUMA significantly outperforms the baseline memory allocators for all evaluated microbenchmarks and allocation sizes."],"url":"http://arxiv.org/abs/2403.04539v1"}
{"created":"2024-03-07 14:28:04","title":"Enhancing Data Quality in Federated Fine-Tuning of Foundation Models","abstract":"In the current landscape of foundation model training, there is a significant reliance on public domain data, which is nearing exhaustion according to recent research. To further scale up, it is crucial to incorporate collaboration among multiple specialized and high-quality private domain data sources. However, the challenge of training models locally without sharing private data presents numerous obstacles in data quality control. To tackle this issue, we propose a data quality control pipeline for federated fine-tuning of foundation models. This pipeline computes scores reflecting the quality of training data and determines a global threshold for a unified standard, aiming for improved global performance. Our experiments show that the proposed quality control pipeline facilitates the effectiveness and reliability of the model training, leading to better performance.","sentences":["In the current landscape of foundation model training, there is a significant reliance on public domain data, which is nearing exhaustion according to recent research.","To further scale up, it is crucial to incorporate collaboration among multiple specialized and high-quality private domain data sources.","However, the challenge of training models locally without sharing private data presents numerous obstacles in data quality control.","To tackle this issue, we propose a data quality control pipeline for federated fine-tuning of foundation models.","This pipeline computes scores reflecting the quality of training data and determines a global threshold for a unified standard, aiming for improved global performance.","Our experiments show that the proposed quality control pipeline facilitates the effectiveness and reliability of the model training, leading to better performance."],"url":"http://arxiv.org/abs/2403.04529v1"}
{"created":"2024-03-07 14:27:08","title":"Hyperspectral unmixing for Raman spectroscopy via physics-constrained autoencoders","abstract":"Raman spectroscopy is widely used across scientific domains to characterize the chemical composition of samples in a non-destructive, label-free manner. Many applications entail the unmixing of signals from mixtures of molecular species to identify the individual components present and their proportions, yet conventional methods for chemometrics often struggle with complex mixture scenarios encountered in practice. Here, we develop hyperspectral unmixing algorithms based on autoencoder neural networks, and we systematically validate them using both synthetic and experimental benchmark datasets created in-house. Our results demonstrate that unmixing autoencoders provide improved accuracy, robustness and efficiency compared to standard unmixing methods. We also showcase the applicability of autoencoders to complex biological settings by showing improved biochemical characterization of volumetric Raman imaging data from a monocytic cell.","sentences":["Raman spectroscopy is widely used across scientific domains to characterize the chemical composition of samples in a non-destructive, label-free manner.","Many applications entail the unmixing of signals from mixtures of molecular species to identify the individual components present and their proportions, yet conventional methods for chemometrics often struggle with complex mixture scenarios encountered in practice.","Here, we develop hyperspectral unmixing algorithms based on autoencoder neural networks, and we systematically validate them using both synthetic and experimental benchmark datasets created in-house.","Our results demonstrate that unmixing autoencoders provide improved accuracy, robustness and efficiency compared to standard unmixing methods.","We also showcase the applicability of autoencoders to complex biological settings by showing improved biochemical characterization of volumetric Raman imaging data from a monocytic cell."],"url":"http://arxiv.org/abs/2403.04526v1"}
{"created":"2024-03-07 14:23:25","title":"Uncertainty-Aware Relational Graph Neural Network for Few-Shot Knowledge Graph Completion","abstract":"Few-shot knowledge graph completion (FKGC) aims to query the unseen facts of a relation given its few-shot reference entity pairs. The side effect of noises due to the uncertainty of entities and triples may limit the few-shot learning, but existing FKGC works neglect such uncertainty, which leads them more susceptible to limited reference samples with noises. In this paper, we propose a novel uncertainty-aware few-shot KG completion framework (UFKGC) to model uncertainty for a better understanding of the limited data by learning representations under Gaussian distribution. Uncertainty representation is first designed for estimating the uncertainty scope of the entity pairs after transferring feature representations into a Gaussian distribution. Further, to better integrate the neighbors with uncertainty characteristics for entity features, we design an uncertainty-aware relational graph neural network (UR-GNN) to conduct convolution operations between the Gaussian distributions. Then, multiple random samplings are conducted for reference triples within the Gaussian distribution to generate smooth reference representations during the optimization. The final completion score for each query instance is measured by the designed uncertainty optimization to make our approach more robust to the noises in few-shot scenarios. Experimental results show that our approach achieves excellent performance on two benchmark datasets compared to its competitors.","sentences":["Few-shot knowledge graph completion (FKGC) aims to query the unseen facts of a relation given its few-shot reference entity pairs.","The side effect of noises due to the uncertainty of entities and triples may limit the few-shot learning, but existing FKGC works neglect such uncertainty, which leads them more susceptible to limited reference samples with noises.","In this paper, we propose a novel uncertainty-aware few-shot KG completion framework (UFKGC) to model uncertainty for a better understanding of the limited data by learning representations under Gaussian distribution.","Uncertainty representation is first designed for estimating the uncertainty scope of the entity pairs after transferring feature representations into a Gaussian distribution.","Further, to better integrate the neighbors with uncertainty characteristics for entity features, we design an uncertainty-aware relational graph neural network (UR-GNN) to conduct convolution operations between the Gaussian distributions.","Then, multiple random samplings are conducted for reference triples within the Gaussian distribution to generate smooth reference representations during the optimization.","The final completion score for each query instance is measured by the designed uncertainty optimization to make our approach more robust to the noises in few-shot scenarios.","Experimental results show that our approach achieves excellent performance on two benchmark datasets compared to its competitors."],"url":"http://arxiv.org/abs/2403.04521v1"}
{"created":"2024-03-07 14:16:59","title":"A Coreset for Approximate Furthest-Neighbor Queries in a Simple Polygon","abstract":"Let $\\mathcal{P}$ be a simple polygon with $m$ vertices and let $P$ be a set of $n$ points inside $\\mathcal{P}$. We prove that there exists, for any $\\varepsilon>0$, a set $\\mathcal{C} \\subset P$ of size $O(1/\\varepsilon^2)$ such that the following holds: for any query point $q$ inside the polygon $\\mathcal{P}$, the geodesic distance from $q$ to its furthest neighbor in $\\mathcal{C}$ is at least $1-\\varepsilon$ times the geodesic distance to its further neighbor in $P$. Thus the set $\\mathcal{C}$ can be used for answering $\\varepsilon$-approximate furthest-neighbor queries with a data structure whose storage requirement is independent of the size of $P$. The coreset can be constructed in $O\\left(\\frac{1}{\\varepsilon} \\left( n\\log(1/\\varepsilon) + (n+m)\\log(n+m)\\right) \\right)$ time.","sentences":["Let $\\mathcal{P}$ be a simple polygon with $m$ vertices and let $P$ be a set of $n$ points inside $\\mathcal{P}$. We prove that there exists, for any $\\varepsilon>0$, a set $\\mathcal{C} \\subset P$ of size $O(1/\\varepsilon^2)$ such that the following holds: for any query point $q$ inside the polygon $\\mathcal{P}$, the geodesic distance from $q$ to its furthest neighbor in $\\mathcal{C}$ is at least $1-\\varepsilon$ times the geodesic distance to its further neighbor in $P$. Thus the set $\\mathcal{C}$ can be used for answering $\\varepsilon$-approximate furthest-neighbor queries with a data structure whose storage requirement is independent of the size of $P$. The coreset can be constructed in $O\\left(\\frac{1}{\\varepsilon} \\left( n\\log(1/\\varepsilon)","+ (n+m)\\log(n+m)\\right) \\right)$ time."],"url":"http://arxiv.org/abs/2403.04513v1"}
{"created":"2024-03-07 14:14:40","title":"Uncovering the Deep Filter Bubble: Narrow Exposure in Short-Video Recommendation","abstract":"Filter bubbles have been studied extensively within the context of online content platforms due to their potential to cause undesirable outcomes such as user dissatisfaction or polarization. With the rise of short-video platforms, the filter bubble has been given extra attention because these platforms rely on an unprecedented use of the recommender system to provide relevant content. In our work, we investigate the deep filter bubble, which refers to the user being exposed to narrow content within their broad interests. We accomplish this using one-year interaction data from a top short-video platform in China, which includes hierarchical data with three levels of categories for each video. We formalize our definition of a \"deep\" filter bubble within this context, and then explore various correlations within the data: first understanding the evolution of the deep filter bubble over time, and later revealing some of the factors that give rise to this phenomenon, such as specific categories, user demographics, and feedback type. We observe that while the overall proportion of users in a filter bubble remains largely constant over time, the depth composition of their filter bubble changes. In addition, we find that some demographic groups that have a higher likelihood of seeing narrower content and implicit feedback signals can lead to less bubble formation. Finally, we propose some ways in which recommender systems can be designed to reduce the risk of a user getting caught in a bubble.","sentences":["Filter bubbles have been studied extensively within the context of online content platforms due to their potential to cause undesirable outcomes such as user dissatisfaction or polarization.","With the rise of short-video platforms, the filter bubble has been given extra attention because these platforms rely on an unprecedented use of the recommender system to provide relevant content.","In our work, we investigate the deep filter bubble, which refers to the user being exposed to narrow content within their broad interests.","We accomplish this using one-year interaction data from a top short-video platform in China, which includes hierarchical data with three levels of categories for each video.","We formalize our definition of a \"deep\" filter bubble within this context, and then explore various correlations within the data: first understanding the evolution of the deep filter bubble over time, and later revealing some of the factors that give rise to this phenomenon, such as specific categories, user demographics, and feedback type.","We observe that while the overall proportion of users in a filter bubble remains largely constant over time, the depth composition of their filter bubble changes.","In addition, we find that some demographic groups that have a higher likelihood of seeing narrower content and implicit feedback signals can lead to less bubble formation.","Finally, we propose some ways in which recommender systems can be designed to reduce the risk of a user getting caught in a bubble."],"url":"http://arxiv.org/abs/2403.04511v1"}
{"created":"2024-03-07 14:03:31","title":"Ducho 2.0: Towards a More Up-to-Date Feature Extraction and Processing Framework for Multimodal Recommendation","abstract":"In this work, we introduce Ducho 2.0, the latest stable version of our framework. Differently from Ducho, Ducho 2.0 offers a more personalized user experience with the definition and import of custom extraction models fine-tuned on specific tasks and datasets. Moreover, the new version is capable of extracting and processing features through multimodal-by-design large models. Notably, all these new features are supported by optimized data loading and storing to the local memory. To showcase the capabilities of Ducho 2.0, we demonstrate a complete multimodal recommendation pipeline, from the extraction/processing to the final recommendation. The idea is to provide practitioners and experienced scholars with a ready-to-use tool that, put on top of any multimodal recommendation framework, may permit them to run extensive benchmarking analyses. All materials are accessible at: \\url{https://github.com/sisinflab/Ducho}.","sentences":["In this work, we introduce Ducho 2.0, the latest stable version of our framework.","Differently from Ducho, Ducho 2.0 offers a more personalized user experience with the definition and import of custom extraction models fine-tuned on specific tasks and datasets.","Moreover, the new version is capable of extracting and processing features through multimodal-by-design large models.","Notably, all these new features are supported by optimized data loading and storing to the local memory.","To showcase the capabilities of Ducho 2.0, we demonstrate a complete multimodal recommendation pipeline, from the extraction/processing to the final recommendation.","The idea is to provide practitioners and experienced scholars with a ready-to-use tool that, put on top of any multimodal recommendation framework, may permit them to run extensive benchmarking analyses.","All materials are accessible at: \\url{https://github.com/sisinflab/Ducho}."],"url":"http://arxiv.org/abs/2403.04503v1"}
{"created":"2024-03-07 13:49:43","title":"What makes an image realistic?","abstract":"The last decade has seen tremendous progress in our ability to generate realistic-looking data, be it images, text, audio, or video. Here, we discuss the closely related problem of quantifying realism, that is, designing functions that can reliably tell realistic data from unrealistic data. This problem turns out to be significantly harder to solve and remains poorly understood, despite its prevalence in machine learning and recent breakthroughs in generative AI. Drawing on insights from algorithmic information theory, we discuss why this problem is challenging, why a good generative model alone is insufficient to solve it, and what a good solution would look like. In particular, we introduce the notion of a universal critic, which unlike adversarial critics does not require adversarial training. While universal critics are not immediately practical, they can serve both as a North Star for guiding practical implementations and as a tool for analyzing existing attempts to capture realism.","sentences":["The last decade has seen tremendous progress in our ability to generate realistic-looking data, be it images, text, audio, or video.","Here, we discuss the closely related problem of quantifying realism, that is, designing functions that can reliably tell realistic data from unrealistic data.","This problem turns out to be significantly harder to solve and remains poorly understood, despite its prevalence in machine learning and recent breakthroughs in generative AI.","Drawing on insights from algorithmic information theory, we discuss why this problem is challenging, why a good generative model alone is insufficient to solve it, and what a good solution would look like.","In particular, we introduce the notion of a universal critic, which unlike adversarial critics does not require adversarial training.","While universal critics are not immediately practical, they can serve both as a North Star for guiding practical implementations and as a tool for analyzing existing attempts to capture realism."],"url":"http://arxiv.org/abs/2403.04493v1"}
{"created":"2024-03-07 13:38:18","title":"Privacy in Cloud Computing through Immersion-based Coding","abstract":"Cloud computing enables users to process and store data remotely on high-performance computers and servers by sharing data over the Internet. However, transferring data to clouds causes unavoidable privacy concerns. Here, we present a synthesis framework to design coding mechanisms that allow sharing and processing data in a privacy-preserving manner without sacrificing data utility and algorithmic performance. We consider the setup where the user aims to run an algorithm in the cloud using private data. The cloud then returns some data utility back to the user (utility refers to the service that the algorithm provides, e.g., classification, prediction, AI models, etc.). To avoid privacy concerns, the proposed scheme provides tools to co-design: 1) coding mechanisms to distort the original data and guarantee a prescribed differential privacy level; 2) an equivalent-but-different algorithm (referred here to as the target algorithm) that runs on distorted data and produces distorted utility; and 3) a decoding function that extracts the true utility from the distorted one with a negligible error. Then, instead of sharing the original data and algorithm with the cloud, only the distorted data and target algorithm are disclosed, thereby avoiding privacy concerns. The proposed scheme is built on the synergy of differential privacy and system immersion tools from control theory. The key underlying idea is to design a higher-dimensional target algorithm that embeds all trajectories of the original algorithm and works on randomly encoded data to produce randomly encoded utility. We show that the proposed scheme can be designed to offer any level of differential privacy without degrading the algorithm's utility. We present two use cases to illustrate the performance of the developed tools: privacy in optimization/learning algorithms and a nonlinear networked control system.","sentences":["Cloud computing enables users to process and store data remotely on high-performance computers and servers by sharing data over the Internet.","However, transferring data to clouds causes unavoidable privacy concerns.","Here, we present a synthesis framework to design coding mechanisms that allow sharing and processing data in a privacy-preserving manner without sacrificing data utility and algorithmic performance.","We consider the setup where the user aims to run an algorithm in the cloud using private data.","The cloud then returns some data utility back to the user (utility refers to the service that the algorithm provides, e.g., classification, prediction, AI models, etc.).","To avoid privacy concerns, the proposed scheme provides tools to co-design: 1) coding mechanisms to distort the original data and guarantee a prescribed differential privacy level; 2) an equivalent-but-different algorithm (referred here to as the target algorithm) that runs on distorted data and produces distorted utility; and 3) a decoding function that extracts the true utility from the distorted one with a negligible error.","Then, instead of sharing the original data and algorithm with the cloud, only the distorted data and target algorithm are disclosed, thereby avoiding privacy concerns.","The proposed scheme is built on the synergy of differential privacy and system immersion tools from control theory.","The key underlying idea is to design a higher-dimensional target algorithm that embeds all trajectories of the original algorithm and works on randomly encoded data to produce randomly encoded utility.","We show that the proposed scheme can be designed to offer any level of differential privacy without degrading the algorithm's utility.","We present two use cases to illustrate the performance of the developed tools: privacy in optimization/learning algorithms and a nonlinear networked control system."],"url":"http://arxiv.org/abs/2403.04485v1"}
{"created":"2024-03-07 13:36:15","title":"Source Matters: Source Dataset Impact on Model Robustness in Medical Imaging","abstract":"Transfer learning has become an essential part of medical imaging classification algorithms, often leveraging ImageNet weights. However, the domain shift from natural to medical images has prompted alternatives such as RadImageNet, often demonstrating comparable classification performance. However, it remains unclear whether the performance gains from transfer learning stem from improved generalization or shortcut learning. To address this, we investigate potential confounders -- whether synthetic or sampled from the data -- across two publicly available chest X-ray and CT datasets. We show that ImageNet and RadImageNet achieve comparable classification performance, yet ImageNet is much more prone to overfitting to confounders. We recommend that researchers using ImageNet-pretrained models reexamine their model robustness by conducting similar experiments. Our code and experiments are available at https://github.com/DovileDo/source-matters.","sentences":["Transfer learning has become an essential part of medical imaging classification algorithms, often leveraging ImageNet weights.","However, the domain shift from natural to medical images has prompted alternatives such as RadImageNet, often demonstrating comparable classification performance.","However, it remains unclear whether the performance gains from transfer learning stem from improved generalization or shortcut learning.","To address this, we investigate potential confounders -- whether synthetic or sampled from the data -- across two publicly available chest X-ray and CT datasets.","We show that ImageNet and RadImageNet achieve comparable classification performance, yet ImageNet is much more prone to overfitting to confounders.","We recommend that researchers using ImageNet-pretrained models reexamine their model robustness by conducting similar experiments.","Our code and experiments are available at https://github.com/DovileDo/source-matters."],"url":"http://arxiv.org/abs/2403.04484v1"}
{"created":"2024-03-07 13:36:08","title":"GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability","abstract":"Evaluating and enhancing the general capabilities of large language models (LLMs) has been an important research topic. Graph is a common data structure in the real world, and understanding graph data is a crucial part for advancing general intelligence. To evaluate and enhance the graph understanding abilities of LLMs, in this paper, we propose a benchmark named GraphInstruct, which comprehensively includes 21 classical graph reasoning tasks, providing diverse graph generation pipelines and detailed reasoning steps. Based on GraphInstruct, we further construct GraphLM through efficient instruction-tuning, which shows prominent graph understanding capability. In order to enhance the LLM with graph reasoning capability as well, we propose a step mask training strategy, and construct a model named GraphLM+. As one of the pioneering efforts to enhance the graph understanding and reasoning abilities of LLMs, extensive experiments have demonstrated the superiority of GraphLM and GraphLM+ over other LLMs. We look forward to more researchers exploring the potential of LLMs in the graph data mining domain through GraphInstruct. Our code for generating GraphInstruct is released publicly at: https://github.com/CGCL-codes/GraphInstruct.","sentences":["Evaluating and enhancing the general capabilities of large language models (LLMs) has been an important research topic.","Graph is a common data structure in the real world, and understanding graph data is a crucial part for advancing general intelligence.","To evaluate and enhance the graph understanding abilities of LLMs, in this paper, we propose a benchmark named GraphInstruct, which comprehensively includes 21 classical graph reasoning tasks, providing diverse graph generation pipelines and detailed reasoning steps.","Based on GraphInstruct, we further construct GraphLM through efficient instruction-tuning, which shows prominent graph understanding capability.","In order to enhance the LLM with graph reasoning capability as well, we propose a step mask training strategy, and construct a model named GraphLM+.","As one of the pioneering efforts to enhance the graph understanding and reasoning abilities of LLMs, extensive experiments have demonstrated the superiority of GraphLM and GraphLM+ over other LLMs.","We look forward to more researchers exploring the potential of LLMs in the graph data mining domain through GraphInstruct.","Our code for generating GraphInstruct is released publicly at: https://github.com/CGCL-codes/GraphInstruct."],"url":"http://arxiv.org/abs/2403.04483v1"}
{"created":"2024-03-07 13:33:30","title":"On the Topology Awareness and Generalization Performance of Graph Neural Networks","abstract":"Many computer vision and machine learning problems are modelled as learning tasks on graphs, where graph neural networks (GNNs) have emerged as a dominant tool for learning representations of graph-structured data. A key feature of GNNs is their use of graph structures as input, enabling them to exploit the graphs' inherent topological properties-known as the topology awareness of GNNs. Despite the empirical successes of GNNs, the influence of topology awareness on generalization performance remains unexplored, particularly for node-level tasks that diverge from the assumption of data being independent and identically distributed (I.I.D.). The precise definition and characterization of the topology awareness of GNNs, especially concerning different topological features, are still unclear. This paper introduces a comprehensive framework to characterize the topology awareness of GNNs across any topological feature. Using this framework, we investigate the effects of topology awareness on GNN generalization performance. Contrary to the prevailing belief that enhancing the topology awareness of GNNs is always advantageous, our analysis reveals a critical insight: improving the topology awareness of GNNs may inadvertently lead to unfair generalization across structural groups, which might not be desired in some scenarios. Additionally, we conduct a case study using the intrinsic graph metric, the shortest path distance, on various benchmark datasets. The empirical results of this case study confirm our theoretical insights. Moreover, we demonstrate the practical applicability of our framework by using it to tackle the cold start problem in graph active learning.","sentences":["Many computer vision and machine learning problems are modelled as learning tasks on graphs, where graph neural networks (GNNs) have emerged as a dominant tool for learning representations of graph-structured data.","A key feature of GNNs is their use of graph structures as input, enabling them to exploit the graphs' inherent topological properties-known as the topology awareness of GNNs.","Despite the empirical successes of GNNs, the influence of topology awareness on generalization performance remains unexplored, particularly for node-level tasks that diverge from the assumption of data being independent and identically distributed (I.I.D.).","The precise definition and characterization of the topology awareness of GNNs, especially concerning different topological features, are still unclear.","This paper introduces a comprehensive framework to characterize the topology awareness of GNNs across any topological feature.","Using this framework, we investigate the effects of topology awareness on GNN generalization performance.","Contrary to the prevailing belief that enhancing the topology awareness of GNNs is always advantageous, our analysis reveals a critical insight: improving the topology awareness of GNNs may inadvertently lead to unfair generalization across structural groups, which might not be desired in some scenarios.","Additionally, we conduct a case study using the intrinsic graph metric, the shortest path distance, on various benchmark datasets.","The empirical results of this case study confirm our theoretical insights.","Moreover, we demonstrate the practical applicability of our framework by using it to tackle the cold start problem in graph active learning."],"url":"http://arxiv.org/abs/2403.04482v1"}
{"created":"2024-03-07 13:25:12","title":"Modeling Methane Intensity of Oil and Gas Upstream Activities by Production Profile","abstract":"We propose a methodology for modelling methane intensities of Oil and Gas upstream activities for different production profiles with diverse combinations of region of operation and production volumes associated. This methodology leverages different data sources, including satellite measurements and public estimates of methane emissions but also country-level oil and gas production data and company reporting. The obtained methane intensity models are compared to the reference companies' own reporting in order to better understand methane emissions for different types of companies. The results show that regions of operation within the different production profiles have a significant impact on the value of modelled methane intensities, especially for operators located in a single or few countries, such as national and medium-sized international operators. This paper also shows that methane intensities reported by the companies tend to be on average 16.1 times smaller than that obtained using the methodology presented here, and cannot account for total methane emissions that are estimated for upstream operations in the different regions observed.","sentences":["We propose a methodology for modelling methane intensities of Oil and Gas upstream activities for different production profiles with diverse combinations of region of operation and production volumes associated.","This methodology leverages different data sources, including satellite measurements and public estimates of methane emissions but also country-level oil and gas production data and company reporting.","The obtained methane intensity models are compared to the reference companies' own reporting in order to better understand methane emissions for different types of companies.","The results show that regions of operation within the different production profiles have a significant impact on the value of modelled methane intensities, especially for operators located in a single or few countries, such as national and medium-sized international operators.","This paper also shows that methane intensities reported by the companies tend to be on average 16.1 times smaller than that obtained using the methodology presented here, and cannot account for total methane emissions that are estimated for upstream operations in the different regions observed."],"url":"http://arxiv.org/abs/2403.04479v1"}
{"created":"2024-03-07 13:10:37","title":"A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges","abstract":"Graph-structured data exhibits universality and widespread applicability across diverse domains, such as social network analysis, biochemistry, financial fraud detection, and network security. Significant strides have been made in leveraging Graph Neural Networks (GNNs) to achieve remarkable success in these areas. However, in real-world scenarios, the training environment for models is often far from ideal, leading to substantial performance degradation of GNN models due to various unfavorable factors, including imbalance in data distribution, the presence of noise in erroneous data, privacy protection of sensitive information, and generalization capability for out-of-distribution (OOD) scenarios. To tackle these issues, substantial efforts have been devoted to improving the performance of GNN models in practical real-world scenarios, as well as enhancing their reliability and robustness. In this paper, we present a comprehensive survey that systematically reviews existing GNN models, focusing on solutions to the four mentioned real-world challenges including imbalance, noise, privacy, and OOD in practical scenarios that many existing reviews have not considered. Specifically, we first highlight the four key challenges faced by existing GNNs, paving the way for our exploration of real-world GNN models. Subsequently, we provide detailed discussions on these four aspects, dissecting how these solutions contribute to enhancing the reliability and robustness of GNN models. Last but not least, we outline promising directions and offer future perspectives in the field.","sentences":["Graph-structured data exhibits universality and widespread applicability across diverse domains, such as social network analysis, biochemistry, financial fraud detection, and network security.","Significant strides have been made in leveraging Graph Neural Networks (GNNs) to achieve remarkable success in these areas.","However, in real-world scenarios, the training environment for models is often far from ideal, leading to substantial performance degradation of GNN models due to various unfavorable factors, including imbalance in data distribution, the presence of noise in erroneous data, privacy protection of sensitive information, and generalization capability for out-of-distribution (OOD) scenarios.","To tackle these issues, substantial efforts have been devoted to improving the performance of GNN models in practical real-world scenarios, as well as enhancing their reliability and robustness.","In this paper, we present a comprehensive survey that systematically reviews existing GNN models, focusing on solutions to the four mentioned real-world challenges including imbalance, noise, privacy, and OOD in practical scenarios that many existing reviews have not considered.","Specifically, we first highlight the four key challenges faced by existing GNNs, paving the way for our exploration of real-world GNN models.","Subsequently, we provide detailed discussions on these four aspects, dissecting how these solutions contribute to enhancing the reliability and robustness of GNN models.","Last but not least, we outline promising directions and offer future perspectives in the field."],"url":"http://arxiv.org/abs/2403.04468v1"}
{"created":"2024-03-07 12:47:42","title":"Low-Resource Court Judgment Summarization for Common Law Systems","abstract":"Common law courts need to refer to similar precedents' judgments to inform their current decisions. Generating high-quality summaries of court judgment documents can facilitate legal practitioners to efficiently review previous cases and assist the general public in accessing how the courts operate and how the law is applied. Previous court judgment summarization research focuses on civil law or a particular jurisdiction's judgments. However, judges can refer to the judgments from all common law jurisdictions. Current summarization datasets are insufficient to satisfy the demands of summarizing precedents across multiple jurisdictions, especially when labeled data are scarce for many jurisdictions. To address the lack of datasets, we present CLSum, the first dataset for summarizing multi-jurisdictional common law court judgment documents. Besides, this is the first court judgment summarization work adopting large language models (LLMs) in data augmentation, summary generation, and evaluation. Specifically, we design an LLM-based data augmentation method incorporating legal knowledge. We also propose a legal knowledge enhanced evaluation metric based on LLM to assess the quality of generated judgment summaries. Our experimental results verify that the LLM-based summarization methods can perform well in the few-shot and zero-shot settings. Our LLM-based data augmentation method can mitigate the impact of low data resources. Furthermore, we carry out comprehensive comparative experiments to find essential model components and settings that are capable of enhancing summarization performance.","sentences":["Common law courts need to refer to similar precedents' judgments to inform their current decisions.","Generating high-quality summaries of court judgment documents can facilitate legal practitioners to efficiently review previous cases and assist the general public in accessing how the courts operate and how the law is applied.","Previous court judgment summarization research focuses on civil law or a particular jurisdiction's judgments.","However, judges can refer to the judgments from all common law jurisdictions.","Current summarization datasets are insufficient to satisfy the demands of summarizing precedents across multiple jurisdictions, especially when labeled data are scarce for many jurisdictions.","To address the lack of datasets, we present CLSum, the first dataset for summarizing multi-jurisdictional common law court judgment documents.","Besides, this is the first court judgment summarization work adopting large language models (LLMs) in data augmentation, summary generation, and evaluation.","Specifically, we design an LLM-based data augmentation method incorporating legal knowledge.","We also propose a legal knowledge enhanced evaluation metric based on LLM to assess the quality of generated judgment summaries.","Our experimental results verify that the LLM-based summarization methods can perform well in the few-shot and zero-shot settings.","Our LLM-based data augmentation method can mitigate the impact of low data resources.","Furthermore, we carry out comprehensive comparative experiments to find essential model components and settings that are capable of enhancing summarization performance."],"url":"http://arxiv.org/abs/2403.04454v1"}
{"created":"2024-03-07 12:45:51","title":"Vlearn: Off-Policy Learning with Efficient State-Value Function Estimation","abstract":"Existing off-policy reinforcement learning algorithms typically necessitate an explicit state-action-value function representation, which becomes problematic in high-dimensional action spaces. These algorithms often encounter challenges where they struggle with the curse of dimensionality, as maintaining a state-action-value function in such spaces becomes data-inefficient. In this work, we propose a novel off-policy trust region optimization approach, called Vlearn, that eliminates the requirement for an explicit state-action-value function. Instead, we demonstrate how to efficiently leverage just a state-value function as the critic, thus overcoming several limitations of existing methods. By doing so, Vlearn addresses the computational challenges posed by high-dimensional action spaces. Furthermore, Vlearn introduces an efficient approach to address the challenges associated with pure state-value function learning in the off-policy setting. This approach not only simplifies the implementation of off-policy policy gradient algorithms but also leads to consistent and robust performance across various benchmark tasks. Specifically, by removing the need for a state-action-value function Vlearn simplifies the learning process and allows for more efficient exploration and exploitation in complex environments","sentences":["Existing off-policy reinforcement learning algorithms typically necessitate an explicit state-action-value function representation, which becomes problematic in high-dimensional action spaces.","These algorithms often encounter challenges where they struggle with the curse of dimensionality, as maintaining a state-action-value function in such spaces becomes data-inefficient.","In this work, we propose a novel off-policy trust region optimization approach, called Vlearn, that eliminates the requirement for an explicit state-action-value function.","Instead, we demonstrate how to efficiently leverage just a state-value function as the critic, thus overcoming several limitations of existing methods.","By doing so, Vlearn addresses the computational challenges posed by high-dimensional action spaces.","Furthermore, Vlearn introduces an efficient approach to address the challenges associated with pure state-value function learning in the off-policy setting.","This approach not only simplifies the implementation of off-policy policy gradient algorithms but also leads to consistent and robust performance across various benchmark tasks.","Specifically, by removing the need for a state-action-value function Vlearn simplifies the learning process and allows for more efficient exploration and exploitation in complex environments"],"url":"http://arxiv.org/abs/2403.04453v1"}
{"created":"2024-03-07 12:43:42","title":"Membership Inference Attacks and Privacy in Topic Modeling","abstract":"Recent research shows that large language models are susceptible to privacy attacks that infer aspects of the training data. However, it is unclear if simpler generative models, like topic models, share similar vulnerabilities. In this work, we propose an attack against topic models that can confidently identify members of the training data in Latent Dirichlet Allocation. Our results suggest that the privacy risks associated with generative modeling are not restricted to large neural models. Additionally, to mitigate these vulnerabilities, we explore differentially private (DP) topic modeling. We propose a framework for private topic modeling that incorporates DP vocabulary selection as a pre-processing step, and show that it improves privacy while having limited effects on practical utility.","sentences":["Recent research shows that large language models are susceptible to privacy attacks that infer aspects of the training data.","However, it is unclear if simpler generative models, like topic models, share similar vulnerabilities.","In this work, we propose an attack against topic models that can confidently identify members of the training data in Latent Dirichlet Allocation.","Our results suggest that the privacy risks associated with generative modeling are not restricted to large neural models.","Additionally, to mitigate these vulnerabilities, we explore differentially private (DP) topic modeling.","We propose a framework for private topic modeling that incorporates DP vocabulary selection as a pre-processing step, and show that it improves privacy while having limited effects on practical utility."],"url":"http://arxiv.org/abs/2403.04451v1"}
{"created":"2024-03-07 12:10:41","title":"Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation","abstract":"We present Human to Humanoid (H2O), a reinforcement learning (RL) based framework that enables real-time whole-body teleoperation of a full-sized humanoid robot with only an RGB camera. To create a large-scale retargeted motion dataset of human movements for humanoid robots, we propose a scalable \"sim-to-data\" process to filter and pick feasible motions using a privileged motion imitator. Afterwards, we train a robust real-time humanoid motion imitator in simulation using these refined motions and transfer it to the real humanoid robot in a zero-shot manner. We successfully achieve teleoperation of dynamic whole-body motions in real-world scenarios, including walking, back jumping, kicking, turning, waving, pushing, boxing, etc. To the best of our knowledge, this is the first demonstration to achieve learning-based real-time whole-body humanoid teleoperation.","sentences":["We present Human to Humanoid (H2O), a reinforcement learning (RL) based framework that enables real-time whole-body teleoperation of a full-sized humanoid robot with only an RGB camera.","To create a large-scale retargeted motion dataset of human movements for humanoid robots, we propose a scalable \"sim-to-data\" process to filter and pick feasible motions using a privileged motion imitator.","Afterwards, we train a robust real-time humanoid motion imitator in simulation using these refined motions and transfer it to the real humanoid robot in a zero-shot manner.","We successfully achieve teleoperation of dynamic whole-body motions in real-world scenarios, including walking, back jumping, kicking, turning, waving, pushing, boxing, etc.","To the best of our knowledge, this is the first demonstration to achieve learning-based real-time whole-body humanoid teleoperation."],"url":"http://arxiv.org/abs/2403.04436v1"}
{"created":"2024-03-07 12:09:36","title":"Pilot Spoofing Attack on the Downlink of Cell-Free Massive MIMO: From the Perspective of Adversaries","abstract":"The channel hardening effect is less pronounced in the cell-free massive multiple-input multiple-output (mMIMO) system compared to its cellular counterpart, making it necessary to estimate the downlink effective channel gains to ensure decent performance. However, the downlink training inadvertently creates an opportunity for adversarial nodes to launch pilot spoofing attacks (PSAs). First, we demonstrate that adversarial distributed access points (APs) can severely degrade the achievable downlink rate. They achieve this by estimating their channels to users in the uplink training phase and then precoding and sending the same pilot sequences as those used by legitimate APs during the downlink training phase. Then, the impact of the downlink PSA is investigated by rigorously deriving a closed-form expression of the per-user achievable downlink rate. By employing the min-max criterion to optimize the power allocation coefficients, the maximum per-user achievable rate of downlink transmission is minimized from the perspective of adversarial APs. As an alternative to the downlink PSA, adversarial APs may opt to precode random interference during the downlink data transmission phase in order to disrupt legitimate communications. In this scenario, the achievable downlink rate is derived, and then power optimization algorithms are also developed. We present numerical results to showcase the detrimental impact of the downlink PSA and compare the effects of these two types of attacks.","sentences":["The channel hardening effect is less pronounced in the cell-free massive multiple-input multiple-output (mMIMO) system compared to its cellular counterpart, making it necessary to estimate the downlink effective channel gains to ensure decent performance.","However, the downlink training inadvertently creates an opportunity for adversarial nodes to launch pilot spoofing attacks (PSAs).","First, we demonstrate that adversarial distributed access points (APs) can severely degrade the achievable downlink rate.","They achieve this by estimating their channels to users in the uplink training phase and then precoding and sending the same pilot sequences as those used by legitimate APs during the downlink training phase.","Then, the impact of the downlink PSA is investigated by rigorously deriving a closed-form expression of the per-user achievable downlink rate.","By employing the min-max criterion to optimize the power allocation coefficients, the maximum per-user achievable rate of downlink transmission is minimized from the perspective of adversarial APs.","As an alternative to the downlink PSA, adversarial APs may opt to precode random interference during the downlink data transmission phase in order to disrupt legitimate communications.","In this scenario, the achievable downlink rate is derived, and then power optimization algorithms are also developed.","We present numerical results to showcase the detrimental impact of the downlink PSA and compare the effects of these two types of attacks."],"url":"http://arxiv.org/abs/2403.04435v1"}
{"created":"2024-03-07 12:00:33","title":"On-demand Quantization for Green Federated Generative Diffusion in Mobile Edge Networks","abstract":"Generative Artificial Intelligence (GAI) shows remarkable productivity and creativity in Mobile Edge Networks, such as the metaverse and the Industrial Internet of Things. Federated learning is a promising technique for effectively training GAI models in mobile edge networks due to its data distribution. However, there is a notable issue with communication consumption when training large GAI models like generative diffusion models in mobile edge networks. Additionally, the substantial energy consumption associated with training diffusion-based models, along with the limited resources of edge devices and complexities of network environments, pose challenges for improving the training efficiency of GAI models. To address this challenge, we propose an on-demand quantized energy-efficient federated diffusion approach for mobile edge networks. Specifically, we first design a dynamic quantized federated diffusion training scheme considering various demands from the edge devices. Then, we study an energy efficiency problem based on specific quantization requirements. Numerical results show that our proposed method significantly reduces system energy consumption and transmitted model size compared to both baseline federated diffusion and fixed quantized federated diffusion methods while effectively maintaining reasonable quality and diversity of generated data.","sentences":["Generative Artificial Intelligence (GAI) shows remarkable productivity and creativity in Mobile Edge Networks, such as the metaverse and the Industrial Internet of Things.","Federated learning is a promising technique for effectively training GAI models in mobile edge networks due to its data distribution.","However, there is a notable issue with communication consumption when training large GAI models like generative diffusion models in mobile edge networks.","Additionally, the substantial energy consumption associated with training diffusion-based models, along with the limited resources of edge devices and complexities of network environments, pose challenges for improving the training efficiency of GAI models.","To address this challenge, we propose an on-demand quantized energy-efficient federated diffusion approach for mobile edge networks.","Specifically, we first design a dynamic quantized federated diffusion training scheme considering various demands from the edge devices.","Then, we study an energy efficiency problem based on specific quantization requirements.","Numerical results show that our proposed method significantly reduces system energy consumption and transmitted model size compared to both baseline federated diffusion and fixed quantized federated diffusion methods while effectively maintaining reasonable quality and diversity of generated data."],"url":"http://arxiv.org/abs/2403.04430v1"}
{"created":"2024-03-07 11:59:00","title":"Exploring the Influence of Dimensionality Reduction on Anomaly Detection Performance in Multivariate Time Series","abstract":"This paper presents an extensive empirical study on the integration of dimensionality reduction techniques with advanced unsupervised time series anomaly detection models, focusing on the MUTANT and Anomaly-Transformer models. The study involves a comprehensive evaluation across three different datasets: MSL, SMAP, and SWaT. Each dataset poses unique challenges, allowing for a robust assessment of the models' capabilities in varied contexts. The dimensionality reduction techniques examined include PCA, UMAP, Random Projection, and t-SNE, each offering distinct advantages in simplifying high-dimensional data. Our findings reveal that dimensionality reduction not only aids in reducing computational complexity but also significantly enhances anomaly detection performance in certain scenarios. Moreover, a remarkable reduction in training times was observed, with reductions by approximately 300\\% and 650\\% when dimensionality was halved and minimized to the lowest dimensions, respectively. This efficiency gain underscores the dual benefit of dimensionality reduction in both performance enhancement and operational efficiency. The MUTANT model exhibits notable adaptability, especially with UMAP reduction, while the Anomaly-Transformer demonstrates versatility across various reduction techniques. These insights provide a deeper understanding of the synergistic effects of dimensionality reduction and anomaly detection, contributing valuable perspectives to the field of time series analysis. The study underscores the importance of selecting appropriate dimensionality reduction strategies based on specific model requirements and dataset characteristics, paving the way for more efficient, accurate, and scalable solutions in anomaly detection.","sentences":["This paper presents an extensive empirical study on the integration of dimensionality reduction techniques with advanced unsupervised time series anomaly detection models, focusing on the MUTANT and Anomaly-Transformer models.","The study involves a comprehensive evaluation across three different datasets: MSL, SMAP, and SWaT. Each dataset poses unique challenges, allowing for a robust assessment of the models' capabilities in varied contexts.","The dimensionality reduction techniques examined include PCA, UMAP, Random Projection, and t-SNE, each offering distinct advantages in simplifying high-dimensional data.","Our findings reveal that dimensionality reduction not only aids in reducing computational complexity but also significantly enhances anomaly detection performance in certain scenarios.","Moreover, a remarkable reduction in training times was observed, with reductions by approximately 300\\% and 650\\% when dimensionality was halved and minimized to the lowest dimensions, respectively.","This efficiency gain underscores the dual benefit of dimensionality reduction in both performance enhancement and operational efficiency.","The MUTANT model exhibits notable adaptability, especially with UMAP reduction, while the Anomaly-Transformer demonstrates versatility across various reduction techniques.","These insights provide a deeper understanding of the synergistic effects of dimensionality reduction and anomaly detection, contributing valuable perspectives to the field of time series analysis.","The study underscores the importance of selecting appropriate dimensionality reduction strategies based on specific model requirements and dataset characteristics, paving the way for more efficient, accurate, and scalable solutions in anomaly detection."],"url":"http://arxiv.org/abs/2403.04429v1"}
{"created":"2024-03-07 11:56:36","title":"Sentiment-driven prediction of financial returns: a Bayesian-enhanced FinBERT approach","abstract":"Predicting financial returns accurately poses a significant challenge due to the inherent uncertainty in financial time series data. Enhancing prediction models' performance hinges on effectively capturing both social and financial sentiment. In this study, we showcase the efficacy of leveraging sentiment information extracted from tweets using the FinBERT large language model. By meticulously curating an optimal feature set through correlation analysis and employing Bayesian-optimized Recursive Feature Elimination for automatic feature selection, we surpass existing methodologies, achieving an F1-score exceeding 70% on the test set. This success translates into demonstrably higher cumulative profits during backtested trading. Our investigation focuses on real-world SPY ETF data alongside corresponding tweets sourced from the StockTwits platform.","sentences":["Predicting financial returns accurately poses a significant challenge due to the inherent uncertainty in financial time series data.","Enhancing prediction models' performance hinges on effectively capturing both social and financial sentiment.","In this study, we showcase the efficacy of leveraging sentiment information extracted from tweets using the FinBERT large language model.","By meticulously curating an optimal feature set through correlation analysis and employing Bayesian-optimized Recursive Feature Elimination for automatic feature selection, we surpass existing methodologies, achieving an F1-score exceeding 70% on the test set.","This success translates into demonstrably higher cumulative profits during backtested trading.","Our investigation focuses on real-world SPY ETF data alongside corresponding tweets sourced from the StockTwits platform."],"url":"http://arxiv.org/abs/2403.04427v1"}
{"created":"2024-03-07 11:12:35","title":"Collaborative Cybersecurity Using Blockchain: A Survey","abstract":"Collaborative cybersecurity relies on organizations sharing information to boost security, but trust management is a key concern. Decentralized solutions like distributed ledgers, particularly blockchain, are crucial for eliminating single points of failure. However, the existing literature on blockchain-based collaborative cybersecurity is limited, lacking comprehensive insights. This paper addresses this gap by surveying blockchain's role in collaborative cybersecurity from 2016 to 2023. It explores various applications, trends, and the evolution of blockchain technology, focusing on access control, data validation policies, underlying tech, and consensus mechanisms. A key finding is the fragmentation of the field with no dominant research group or venue. Many recent projects poorly select consensus protocols for their blockchain. To aid researchers and practitioners, this paper offers guidelines for choosing the right blockchain for specific purposes and highlights open research areas and lessons learned from past blockchain applications in collaborative cybersecurity, encouraging further exploration in this field.","sentences":["Collaborative cybersecurity relies on organizations sharing information to boost security, but trust management is a key concern.","Decentralized solutions like distributed ledgers, particularly blockchain, are crucial for eliminating single points of failure.","However, the existing literature on blockchain-based collaborative cybersecurity is limited, lacking comprehensive insights.","This paper addresses this gap by surveying blockchain's role in collaborative cybersecurity from 2016 to 2023.","It explores various applications, trends, and the evolution of blockchain technology, focusing on access control, data validation policies, underlying tech, and consensus mechanisms.","A key finding is the fragmentation of the field with no dominant research group or venue.","Many recent projects poorly select consensus protocols for their blockchain.","To aid researchers and practitioners, this paper offers guidelines for choosing the right blockchain for specific purposes and highlights open research areas and lessons learned from past blockchain applications in collaborative cybersecurity, encouraging further exploration in this field."],"url":"http://arxiv.org/abs/2403.04410v1"}
{"created":"2024-03-07 10:55:39","title":"Conjugate operators for transparent, explorable research outputs","abstract":"Charts, figures, and text derived from data play an important role in decision making, from data-driven policy development to day-to-day choices informed by online articles. Making sense of, or fact-checking, outputs means understanding how they relate to the underlying data. Even for domain experts with access to the source code and data sets, this poses a significant challenge. In this paper we introduce a new program analysis framework which supports interactive exploration of fine-grained I/O relationships directly through computed outputs, making use of dynamic dependence graphs. Our main contribution is a novel notion in data provenance which we call related inputs, a relation of mutual relevance or \"cognacy\" which arises between inputs when they contribute to common features of the output. Queries of this form allow readers to ask questions like \"What outputs use this data element, and what other data elements are used along with it?\". We show how Jonsson and Tarski's concept of conjugate operators on Boolean algebras appropriately characterises the notion of cognacy in a dependence graph, and give a procedure for computing related inputs over such a graph.","sentences":["Charts, figures, and text derived from data play an important role in decision making, from data-driven policy development to day-to-day choices informed by online articles.","Making sense of, or fact-checking, outputs means understanding how they relate to the underlying data.","Even for domain experts with access to the source code and data sets, this poses a significant challenge.","In this paper we introduce a new program analysis framework which supports interactive exploration of fine-grained I/O relationships directly through computed outputs, making use of dynamic dependence graphs.","Our main contribution is a novel notion in data provenance which we call related inputs, a relation of mutual relevance or \"cognacy\" which arises between inputs when they contribute to common features of the output.","Queries of this form allow readers to ask questions like \"What outputs use this data element, and what other data elements are used along with it?\".","We show how Jonsson and Tarski's concept of conjugate operators on Boolean algebras appropriately characterises the notion of cognacy in a dependence graph, and give a procedure for computing related inputs over such a graph."],"url":"http://arxiv.org/abs/2403.04403v1"}
{"created":"2024-03-07 10:44:47","title":"MAGR: Manifold-Aligned Graph Regularization for Continual Action Quality Assessment","abstract":"Action Quality Assessment (AQA) evaluates diverse skills but models struggle with non-stationary data. We propose Continual AQA (CAQA) to refine models using sparse new data. Feature replay preserves memory without storing raw inputs. However, the misalignment between static old features and the dynamically changing feature manifold causes severe catastrophic forgetting. To address this novel problem, we propose Manifold-Aligned Graph Regularization (MAGR), which first aligns deviated old features to the current feature manifold, ensuring representation consistency. It then constructs a graph jointly arranging old and new features aligned with quality scores. Experiments show MAGR outperforms recent strong baselines with up to 6.56%, 5.66%, 15.64%, and 9.05% correlation gains on the MTL-AQA, FineDiving, UNLV-Dive, and JDM-MSA split datasets, respectively. This validates MAGR for continual assessment challenges arising from non-stationary skill variations.","sentences":["Action Quality Assessment (AQA) evaluates diverse skills but models struggle with non-stationary data.","We propose Continual AQA (CAQA) to refine models using sparse new data.","Feature replay preserves memory without storing raw inputs.","However, the misalignment between static old features and the dynamically changing feature manifold causes severe catastrophic forgetting.","To address this novel problem, we propose Manifold-Aligned Graph Regularization (MAGR), which first aligns deviated old features to the current feature manifold, ensuring representation consistency.","It then constructs a graph jointly arranging old and new features aligned with quality scores.","Experiments show MAGR outperforms recent strong baselines with up to 6.56%, 5.66%, 15.64%, and 9.05% correlation gains on the MTL-AQA, FineDiving, UNLV-Dive, and JDM-MSA split datasets, respectively.","This validates MAGR for continual assessment challenges arising from non-stationary skill variations."],"url":"http://arxiv.org/abs/2403.04398v1"}
{"created":"2024-03-07 10:29:05","title":"Comparison of Deep Learning Techniques on Human Activity Recognition using Ankle Inertial Signals","abstract":"Human Activity Recognition (HAR) is one of the fundamental building blocks of human assistive devices like orthoses and exoskeletons. There are different approaches to HAR depending on the application. Numerous studies have been focused on improving them by optimising input data or classification algorithms. However, most of these studies have been focused on applications like security and monitoring, smart devices, the internet of things, etc. On the other hand, HAR can help adjust and control wearable assistive devices, yet there has not been enough research facilitating its implementation. In this study, we propose several models to predict four activities from inertial sensors located in the ankle area of a lower-leg assistive device user. This choice is because they do not need to be attached to the user's skin and can be directly implemented inside the control unit of the device. The proposed models are based on Artificial Neural Networks and could achieve up to 92.8% average classification accuracy","sentences":["Human Activity Recognition (HAR) is one of the fundamental building blocks of human assistive devices like orthoses and exoskeletons.","There are different approaches to HAR depending on the application.","Numerous studies have been focused on improving them by optimising input data or classification algorithms.","However, most of these studies have been focused on applications like security and monitoring, smart devices, the internet of things, etc.","On the other hand, HAR can help adjust and control wearable assistive devices, yet there has not been enough research facilitating its implementation.","In this study, we propose several models to predict four activities from inertial sensors located in the ankle area of a lower-leg assistive device user.","This choice is because they do not need to be attached to the user's skin and can be directly implemented inside the control unit of the device.","The proposed models are based on Artificial Neural Networks and could achieve up to 92.8% average classification accuracy"],"url":"http://arxiv.org/abs/2403.04387v1"}
{"created":"2024-03-07 10:25:23","title":"Impacts of Color and Texture Distortions on Earth Observation Data in Deep Learning","abstract":"Land cover classification and change detection are two important applications of remote sensing and Earth observation (EO) that have benefited greatly from the advances of deep learning. Convolutional and transformer-based U-net models are the state-of-the-art architectures for these tasks, and their performances have been boosted by an increased availability of large-scale annotated EO datasets. However, the influence of different visual characteristics of the input EO data on a model's predictions is not well understood. In this work we systematically examine model sensitivities with respect to several color- and texture-based distortions on the input EO data during inference, given models that have been trained without such distortions. We conduct experiments with multiple state-of-the-art segmentation networks for land cover classification and show that they are in general more sensitive to texture than to color distortions. Beyond revealing intriguing characteristics of widely used land cover classification models, our results can also be used to guide the development of more robust models within the EO domain.","sentences":["Land cover classification and change detection are two important applications of remote sensing and Earth observation (EO) that have benefited greatly from the advances of deep learning.","Convolutional and transformer-based U-net models are the state-of-the-art architectures for these tasks, and their performances have been boosted by an increased availability of large-scale annotated EO datasets.","However, the influence of different visual characteristics of the input EO data on a model's predictions is not well understood.","In this work we systematically examine model sensitivities with respect to several color- and texture-based distortions on the input EO data during inference, given models that have been trained without such distortions.","We conduct experiments with multiple state-of-the-art segmentation networks for land cover classification and show that they are in general more sensitive to texture than to color distortions.","Beyond revealing intriguing characteristics of widely used land cover classification models, our results can also be used to guide the development of more robust models within the EO domain."],"url":"http://arxiv.org/abs/2403.04385v1"}
{"created":"2024-03-07 09:56:56","title":"Learning to Remove Wrinkled Transparent Film with Polarized Prior","abstract":"In this paper, we study a new problem, Film Removal (FR), which attempts to remove the interference of wrinkled transparent films and reconstruct the original information under films for industrial recognition systems. We first physically model the imaging of industrial materials covered by the film. Considering the specular highlight from the film can be effectively recorded by the polarized camera, we build a practical dataset with polarization information containing paired data with and without transparent film. We aim to remove interference from the film (specular highlights and other degradations) with an end-to-end framework. To locate the specular highlight, we use an angle estimation network to optimize the polarization angle with the minimized specular highlight. The image with minimized specular highlight is set as a prior for supporting the reconstruction network. Based on the prior and the polarized images, the reconstruction network can decouple all degradations from the film. Extensive experiments show that our framework achieves SOTA performance in both image reconstruction and industrial downstream tasks. Our code will be released at \\url{https://github.com/jqtangust/FilmRemoval}.","sentences":["In this paper, we study a new problem, Film Removal (FR), which attempts to remove the interference of wrinkled transparent films and reconstruct the original information under films for industrial recognition systems.","We first physically model the imaging of industrial materials covered by the film.","Considering the specular highlight from the film can be effectively recorded by the polarized camera, we build a practical dataset with polarization information containing paired data with and without transparent film.","We aim to remove interference from the film (specular highlights and other degradations) with an end-to-end framework.","To locate the specular highlight, we use an angle estimation network to optimize the polarization angle with the minimized specular highlight.","The image with minimized specular highlight is set as a prior for supporting the reconstruction network.","Based on the prior and the polarized images, the reconstruction network can decouple all degradations from the film.","Extensive experiments show that our framework achieves SOTA performance in both image reconstruction and industrial downstream tasks.","Our code will be released at \\url{https://github.com/jqtangust/FilmRemoval}."],"url":"http://arxiv.org/abs/2403.04368v1"}
{"created":"2024-03-07 09:51:11","title":"Enhancing Court View Generation with Knowledge Injection and Guidance","abstract":"Court View Generation (CVG) is a challenging task in the field of Legal Artificial Intelligence (LegalAI), which aims to generate court views based on the plaintiff claims and the fact descriptions. While Pretrained Language Models (PLMs) have showcased their prowess in natural language generation, their application to the complex, knowledge-intensive domain of CVG often reveals inherent limitations. In this paper, we present a novel approach, named Knowledge Injection and Guidance (KIG), designed to bolster CVG using PLMs. To efficiently incorporate domain knowledge during the training stage, we introduce a knowledge-injected prompt encoder for prompt tuning, thereby reducing computational overhead. Moreover, to further enhance the model's ability to utilize domain knowledge, we employ a generating navigator, which dynamically guides the text generation process in the inference stage without altering the model's architecture, making it readily transferable. Comprehensive experiments on real-world data demonstrate the effectiveness of our approach compared to several established baselines, especially in the responsivity of claims, where it outperforms the best baseline by 11.87%.","sentences":["Court View Generation (CVG) is a challenging task in the field of Legal Artificial Intelligence (LegalAI), which aims to generate court views based on the plaintiff claims and the fact descriptions.","While Pretrained Language Models (PLMs) have showcased their prowess in natural language generation, their application to the complex, knowledge-intensive domain of CVG often reveals inherent limitations.","In this paper, we present a novel approach, named Knowledge Injection and Guidance (KIG), designed to bolster CVG using PLMs.","To efficiently incorporate domain knowledge during the training stage, we introduce a knowledge-injected prompt encoder for prompt tuning, thereby reducing computational overhead.","Moreover, to further enhance the model's ability to utilize domain knowledge, we employ a generating navigator, which dynamically guides the text generation process in the inference stage without altering the model's architecture, making it readily transferable.","Comprehensive experiments on real-world data demonstrate the effectiveness of our approach compared to several established baselines, especially in the responsivity of claims, where it outperforms the best baseline by 11.87%."],"url":"http://arxiv.org/abs/2403.04366v1"}
{"created":"2024-03-07 09:41:11","title":"Symmetry Considerations for Learning Task Symmetric Robot Policies","abstract":"Symmetry is a fundamental aspect of many real-world robotic tasks. However, current deep reinforcement learning (DRL) approaches can seldom harness and exploit symmetry effectively. Often, the learned behaviors fail to achieve the desired transformation invariances and suffer from motion artifacts. For instance, a quadruped may exhibit different gaits when commanded to move forward or backward, even though it is symmetrical about its torso. This issue becomes further pronounced in high-dimensional or complex environments, where DRL methods are prone to local optima and fail to explore regions of the state space equally. Past methods on encouraging symmetry for robotic tasks have studied this topic mainly in a single-task setting, where symmetry usually refers to symmetry in the motion, such as the gait patterns. In this paper, we revisit this topic for goal-conditioned tasks in robotics, where symmetry lies mainly in task execution and not necessarily in the learned motions themselves. In particular, we investigate two approaches to incorporate symmetry invariance into DRL -- data augmentation and mirror loss function. We provide a theoretical foundation for using augmented samples in an on-policy setting. Based on this, we show that the corresponding approach achieves faster convergence and improves the learned behaviors in various challenging robotic tasks, from climbing boxes with a quadruped to dexterous manipulation.","sentences":["Symmetry is a fundamental aspect of many real-world robotic tasks.","However, current deep reinforcement learning (DRL) approaches can seldom harness and exploit symmetry effectively.","Often, the learned behaviors fail to achieve the desired transformation invariances and suffer from motion artifacts.","For instance, a quadruped may exhibit different gaits when commanded to move forward or backward, even though it is symmetrical about its torso.","This issue becomes further pronounced in high-dimensional or complex environments, where DRL methods are prone to local optima and fail to explore regions of the state space equally.","Past methods on encouraging symmetry for robotic tasks have studied this topic mainly in a single-task setting, where symmetry usually refers to symmetry in the motion, such as the gait patterns.","In this paper, we revisit this topic for goal-conditioned tasks in robotics, where symmetry lies mainly in task execution and not necessarily in the learned motions themselves.","In particular, we investigate two approaches to incorporate symmetry invariance into DRL -- data augmentation and mirror loss function.","We provide a theoretical foundation for using augmented samples in an on-policy setting.","Based on this, we show that the corresponding approach achieves faster convergence and improves the learned behaviors in various challenging robotic tasks, from climbing boxes with a quadruped to dexterous manipulation."],"url":"http://arxiv.org/abs/2403.04359v1"}
{"created":"2024-03-07 09:11:16","title":"CoTBal: Comprehensive Task Balancing for Multi-Task Visual Instruction Tuning","abstract":"Visual instruction tuning is a key training stage of large multimodal models (LMMs). Nevertheless, the common practice of indiscriminately mixing instruction-following data from various tasks may result in suboptimal overall performance due to different instruction formats and knowledge domains across tasks. To mitigate this issue, we propose a novel Comprehensive Task Balancing (CoTBal) algorithm for multi-task visual instruction tuning of LMMs. To our knowledge, this is the first work that explores multi-task optimization in visual instruction tuning. Specifically, we consider two key dimensions for task balancing: (1) Inter-Task Contribution, the phenomenon where learning one task potentially enhances the performance in other tasks, attributable to the overlapping knowledge domains, and (2) Intra-Task Difficulty, which refers to the learning difficulty within a single task. By quantifying these two dimensions with performance-based metrics, task balancing is thus enabled by assigning more weights to tasks that offer substantial contributions to others, receive minimal contributions from others, and also have great intra-task difficulties. Experiments show that our CoTBal leads to superior overall performance in multi-task visual instruction tuning.","sentences":["Visual instruction tuning is a key training stage of large multimodal models (LMMs).","Nevertheless, the common practice of indiscriminately mixing instruction-following data from various tasks may result in suboptimal overall performance due to different instruction formats and knowledge domains across tasks.","To mitigate this issue, we propose a novel Comprehensive Task Balancing (CoTBal) algorithm for multi-task visual instruction tuning of LMMs.","To our knowledge, this is the first work that explores multi-task optimization in visual instruction tuning.","Specifically, we consider two key dimensions for task balancing: (1) Inter-Task Contribution, the phenomenon where learning one task potentially enhances the performance in other tasks, attributable to the overlapping knowledge domains, and (2) Intra-Task Difficulty, which refers to the learning difficulty within a single task.","By quantifying these two dimensions with performance-based metrics, task balancing is thus enabled by assigning more weights to tasks that offer substantial contributions to others, receive minimal contributions from others, and also have great intra-task difficulties.","Experiments show that our CoTBal leads to superior overall performance in multi-task visual instruction tuning."],"url":"http://arxiv.org/abs/2403.04343v1"}
{"created":"2024-03-07 08:32:17","title":"Can Your Model Tell a Negation from an Implicature? Unravelling Challenges With Intent Encoders","abstract":"Conversational systems often rely on embedding models for intent classification and intent clustering tasks. The advent of Large Language Models (LLMs), which enable instructional embeddings allowing one to adjust semantics over the embedding space using prompts, are being viewed as a panacea for these downstream conversational tasks. However, traditional evaluation benchmarks rely solely on task metrics that don't particularly measure gaps related to semantic understanding. Thus, we propose an intent semantic toolkit that gives a more holistic view of intent embedding models by considering three tasks -- (1) intent classification, (2) intent clustering, and (3) a novel triplet task. The triplet task gauges the model's understanding of two semantic concepts paramount in real-world conversational systems -- negation and implicature. We observe that current embedding models fare poorly in semantic understanding of these concepts. To address this, we propose a pre-training approach to improve the embedding model by leveraging augmentation with data generated by an auto-regressive model and a contrastive loss term. Our approach improves the semantic understanding of the intent embedding model on the aforementioned linguistic dimensions while slightly effecting their performance on downstream task metrics.","sentences":["Conversational systems often rely on embedding models for intent classification and intent clustering tasks.","The advent of Large Language Models (LLMs), which enable instructional embeddings allowing one to adjust semantics over the embedding space using prompts, are being viewed as a panacea for these downstream conversational tasks.","However, traditional evaluation benchmarks rely solely on task metrics that don't particularly measure gaps related to semantic understanding.","Thus, we propose an intent semantic toolkit that gives a more holistic view of intent embedding models by considering three tasks -- (1) intent classification, (2) intent clustering, and (3) a novel triplet task.","The triplet task gauges the model's understanding of two semantic concepts paramount in real-world conversational systems -- negation and implicature.","We observe that current embedding models fare poorly in semantic understanding of these concepts.","To address this, we propose a pre-training approach to improve the embedding model by leveraging augmentation with data generated by an auto-regressive model and a contrastive loss term.","Our approach improves the semantic understanding of the intent embedding model on the aforementioned linguistic dimensions while slightly effecting their performance on downstream task metrics."],"url":"http://arxiv.org/abs/2403.04314v1"}
{"created":"2024-03-07 08:30:26","title":"ALTO: An Efficient Network Orchestrator for Compound AI Systems","abstract":"We present ALTO, a network orchestrator for efficiently serving compound AI systems such as pipelines of language models. ALTO achieves high throughput and low latency by taking advantage of an optimization opportunity specific to generative language models: streaming intermediate outputs. As language models produce outputs token by token, ALTO exposes opportunities to stream intermediate outputs between stages when possible. We highlight two new challenges of correctness and load balancing which emerge when streaming intermediate data across distributed pipeline stage instances. We also motivate the need for an aggregation-aware routing interface and distributed prompt-aware scheduling to address these challenges. We demonstrate the impact of ALTO's partial output streaming on a complex chatbot verification pipeline, increasing throughput by up to 3x for a fixed latency target of 4 seconds / request while also reducing tail latency by 1.8x compared to a baseline serving approach.","sentences":["We present ALTO, a network orchestrator for efficiently serving compound AI systems such as pipelines of language models.","ALTO achieves high throughput and low latency by taking advantage of an optimization opportunity specific to generative language models: streaming intermediate outputs.","As language models produce outputs token by token, ALTO exposes opportunities to stream intermediate outputs between stages when possible.","We highlight two new challenges of correctness and load balancing which emerge when streaming intermediate data across distributed pipeline stage instances.","We also motivate the need for an aggregation-aware routing interface and distributed prompt-aware scheduling to address these challenges.","We demonstrate the impact of ALTO's partial output streaming on a complex chatbot verification pipeline, increasing throughput by up to 3x for a fixed latency target of 4 seconds / request while also reducing tail latency by 1.8x compared to a baseline serving approach."],"url":"http://arxiv.org/abs/2403.04311v1"}
{"created":"2024-03-07 08:26:00","title":"Generating insights about financial asks from Reddit posts and user interactions","abstract":"As an increasingly large number of people turn to platforms like Reddit, YouTube, Twitter, Instagram, etc. for financial advice, generating insights about the content generated and interactions taking place within these platforms have become a key research question. This study proposes content and interaction analysis techniques for a large repository created from social media content, where people interactions are centered around financial information exchange. We propose methods for content analysis that can generate human-interpretable insights using topic-centered clustering and multi-document abstractive summarization. We share details of insights generated from our experiments with a large repository of data gathered from subreddit for personal finance. We have also explored the use of ChatGPT and Vicuna for generating responses to queries and compared them with human responses. The methods proposed in this work are generic and applicable to all large social media platforms.","sentences":["As an increasingly large number of people turn to platforms like Reddit, YouTube, Twitter, Instagram, etc. for financial advice, generating insights about the content generated and interactions taking place within these platforms have become a key research question.","This study proposes content and interaction analysis techniques for a large repository created from social media content, where people interactions are centered around financial information exchange.","We propose methods for content analysis that can generate human-interpretable insights using topic-centered clustering and multi-document abstractive summarization.","We share details of insights generated from our experiments with a large repository of data gathered from subreddit for personal finance.","We have also explored the use of ChatGPT and Vicuna for generating responses to queries and compared them with human responses.","The methods proposed in this work are generic and applicable to all large social media platforms."],"url":"http://arxiv.org/abs/2403.04308v1"}
{"created":"2024-03-07 07:58:58","title":"LitSim: Conflict-aware Policy for Long-term Interactive Traffic Simulation","abstract":"Simulation is pivotal in evaluating the performance of autonomous driving systems due to the advantages in efficiency and cost compared to on-road testing. Realistic multi-agent behavior~(e.g., interactive and long-term) is needed to narrow the gap between the simulation and the reality. The existing work has the following shortcomings in achieving this goal:~(1) log replay offers realistic scenarios but leads to unrealistic collisions due to lacking dynamic interactions, and~(2) model-based and learning-based solutions encourage interactions but often deviate from real-world data in long horizons. In this work, we propose LitSim, a long-term interactive simulation approach that maximizes realism while avoiding unrealistic collisions. Specifically, we replay the log for most scenarios and intervene only when LitSim predicts unrealistic conflicts. We then encourage interactions among the agents and resolve the conflicts, thereby reducing the likelihood of unrealistic collisions. We train and validate our model on the real-world dataset NGSIM, and the experimental results demonstrate that LitSim outperforms the current popular approaches in realism and reactivity.","sentences":["Simulation is pivotal in evaluating the performance of autonomous driving systems due to the advantages in efficiency and cost compared to on-road testing.","Realistic multi-agent behavior~(e.g., interactive and long-term) is needed to narrow the gap between the simulation and the reality.","The existing work has the following shortcomings in achieving this goal:~(1) log replay offers realistic scenarios but leads to unrealistic collisions due to lacking dynamic interactions, and~(2) model-based and learning-based solutions encourage interactions but often deviate from real-world data in long horizons.","In this work, we propose LitSim, a long-term interactive simulation approach that maximizes realism while avoiding unrealistic collisions.","Specifically, we replay the log for most scenarios and intervene only when LitSim predicts unrealistic conflicts.","We then encourage interactions among the agents and resolve the conflicts, thereby reducing the likelihood of unrealistic collisions.","We train and validate our model on the real-world dataset NGSIM, and the experimental results demonstrate that LitSim outperforms the current popular approaches in realism and reactivity."],"url":"http://arxiv.org/abs/2403.04299v1"}
{"created":"2024-03-07 07:47:09","title":"Understanding how social discussion platforms like Reddit are influencing financial behavior","abstract":"This study proposes content and interaction analysis techniques for a large repository created from social media content. Though we have presented our study for a large platform dedicated to discussions around financial topics, the proposed methods are generic and applicable to all platforms. Along with an extension of topic extraction method using Latent Dirichlet Allocation, we propose a few measures to assess user participation, influence and topic affinities specifically. Our study also maps user-generated content to components of behavioral finance. While these types of information are usually gathered through surveys, it is obvious that large scale data analysis from social media can reveal many potentially unknown or rare insights. Characterising users based on their platform behavior to provide critical insights about how communities are formed and trust is established in these platforms using graphical analysis is also studied.","sentences":["This study proposes content and interaction analysis techniques for a large repository created from social media content.","Though we have presented our study for a large platform dedicated to discussions around financial topics, the proposed methods are generic and applicable to all platforms.","Along with an extension of topic extraction method using Latent Dirichlet Allocation, we propose a few measures to assess user participation, influence and topic affinities specifically.","Our study also maps user-generated content to components of behavioral finance.","While these types of information are usually gathered through surveys, it is obvious that large scale data analysis from social media can reveal many potentially unknown or rare insights.","Characterising users based on their platform behavior to provide critical insights about how communities are formed and trust is established in these platforms using graphical analysis is also studied."],"url":"http://arxiv.org/abs/2403.04298v1"}
{"created":"2024-03-07 07:24:11","title":"SSDRec: Self-Augmented Sequence Denoising for Sequential Recommendation","abstract":"Traditional sequential recommendation methods assume that users' sequence data is clean enough to learn accurate sequence representations to reflect user preferences. In practice, users' sequences inevitably contain noise (e.g., accidental interactions), leading to incorrect reflections of user preferences. Consequently, some pioneer studies have explored modeling sequentiality and correlations in sequences to implicitly or explicitly reduce noise's influence. However, relying on only available intra-sequence information (i.e., sequentiality and correlations in a sequence) is insufficient and may result in over-denoising and under-denoising problems (OUPs), especially for short sequences. To improve reliability, we propose to augment sequences by inserting items before denoising. However, due to the data sparsity issue and computational costs, it is challenging to select proper items from the entire item universe to insert into proper positions in a target sequence. Motivated by the above observation, we propose a novel framework--Self-augmented Sequence Denoising for sequential Recommendation (SSDRec) with a three-stage learning paradigm to solve the above challenges. In the first stage, we empower SSDRec by a global relation encoder to learn multi-faceted inter-sequence relations in a data-driven manner. These relations serve as prior knowledge to guide subsequent stages. In the second stage, we devise a self-augmentation module to augment sequences to alleviate OUPs. Finally, we employ a hierarchical denoising module in the third stage to reduce the risk of false augmentations and pinpoint all noise in raw sequences. Extensive experiments on five real-world datasets demonstrate the superiority of \\model over state-of-the-art denoising methods and its flexible applications to mainstream sequential recommendation models. The source code is available at https://github.com/zc-97/SSDRec.","sentences":["Traditional sequential recommendation methods assume that users' sequence data is clean enough to learn accurate sequence representations to reflect user preferences.","In practice, users' sequences inevitably contain noise (e.g., accidental interactions), leading to incorrect reflections of user preferences.","Consequently, some pioneer studies have explored modeling sequentiality and correlations in sequences to implicitly or explicitly reduce noise's influence.","However, relying on only available intra-sequence information (i.e., sequentiality and correlations in a sequence) is insufficient and may result in over-denoising and under-denoising problems (OUPs), especially for short sequences.","To improve reliability, we propose to augment sequences by inserting items before denoising.","However, due to the data sparsity issue and computational costs, it is challenging to select proper items from the entire item universe to insert into proper positions in a target sequence.","Motivated by the above observation, we propose a novel framework--Self-augmented Sequence Denoising for sequential Recommendation (SSDRec) with a three-stage learning paradigm to solve the above challenges.","In the first stage, we empower SSDRec by a global relation encoder to learn multi-faceted inter-sequence relations in a data-driven manner.","These relations serve as prior knowledge to guide subsequent stages.","In the second stage, we devise a self-augmentation module to augment sequences to alleviate OUPs.","Finally, we employ a hierarchical denoising module in the third stage to reduce the risk of false augmentations and pinpoint all noise in raw sequences.","Extensive experiments on five real-world datasets demonstrate the superiority of \\model over state-of-the-art denoising methods and its flexible applications to mainstream sequential recommendation models.","The source code is available at https://github.com/zc-97/SSDRec."],"url":"http://arxiv.org/abs/2403.04278v1"}
{"created":"2024-03-07 07:13:12","title":"GenML: A Python Library to Generate the Mittag-Leffler Correlated Noise","abstract":"Mittag-Leffler correlated noise (M-L noise) plays a crucial role in the dynamics of complex systems, yet the scientific community has lacked tools for its direct generation. Addressing this gap, our work introduces GenML, a Python library specifically designed for generating M-L noise. We detail the architecture and functionalities of GenML and its underlying algorithmic approach, which enables the precise simulation of M-L noise. The effectiveness of GenML is validated through quantitative analyses of autocorrelation functions and diffusion behaviors, showcasing its capability to accurately replicate theoretical noise properties. Our contribution with GenML enables the effective application of M-L noise data in numerical simulation and data-driven methods for describing complex systems, moving beyond mere theoretical modeling.","sentences":["Mittag-Leffler correlated noise (M-L noise) plays a crucial role in the dynamics of complex systems, yet the scientific community has lacked tools for its direct generation.","Addressing this gap, our work introduces GenML, a Python library specifically designed for generating M-L noise.","We detail the architecture and functionalities of GenML and its underlying algorithmic approach, which enables the precise simulation of M-L noise.","The effectiveness of GenML is validated through quantitative analyses of autocorrelation functions and diffusion behaviors, showcasing its capability to accurately replicate theoretical noise properties.","Our contribution with GenML enables the effective application of M-L noise data in numerical simulation and data-driven methods for describing complex systems, moving beyond mere theoretical modeling."],"url":"http://arxiv.org/abs/2403.04273v1"}
{"created":"2024-03-07 07:12:24","title":"Active Generalized Category Discovery","abstract":"Generalized Category Discovery (GCD) is a pragmatic and challenging open-world task, which endeavors to cluster unlabeled samples from both novel and old classes, leveraging some labeled data of old classes. Given that knowledge learned from old classes is not fully transferable to new classes, and that novel categories are fully unlabeled, GCD inherently faces intractable problems, including imbalanced classification performance and inconsistent confidence between old and new classes, especially in the low-labeling regime. Hence, some annotations of new classes are deemed necessary. However, labeling new classes is extremely costly. To address this issue, we take the spirit of active learning and propose a new setting called Active Generalized Category Discovery (AGCD). The goal is to improve the performance of GCD by actively selecting a limited amount of valuable samples for labeling from the oracle. To solve this problem, we devise an adaptive sampling strategy, which jointly considers novelty, informativeness and diversity to adaptively select novel samples with proper uncertainty. However, owing to the varied orderings of label indices caused by the clustering of novel classes, the queried labels are not directly applicable to subsequent training. To overcome this issue, we further propose a stable label mapping algorithm that transforms ground truth labels to the label space of the classifier, thereby ensuring consistent training across different active selection stages. Our method achieves state-of-the-art performance on both generic and fine-grained datasets. Our code is available at https://github.com/mashijie1028/ActiveGCD","sentences":["Generalized Category Discovery (GCD) is a pragmatic and challenging open-world task, which endeavors to cluster unlabeled samples from both novel and old classes, leveraging some labeled data of old classes.","Given that knowledge learned from old classes is not fully transferable to new classes, and that novel categories are fully unlabeled, GCD inherently faces intractable problems, including imbalanced classification performance and inconsistent confidence between old and new classes, especially in the low-labeling regime.","Hence, some annotations of new classes are deemed necessary.","However, labeling new classes is extremely costly.","To address this issue, we take the spirit of active learning and propose a new setting called Active Generalized Category Discovery (AGCD).","The goal is to improve the performance of GCD by actively selecting a limited amount of valuable samples for labeling from the oracle.","To solve this problem, we devise an adaptive sampling strategy, which jointly considers novelty, informativeness and diversity to adaptively select novel samples with proper uncertainty.","However, owing to the varied orderings of label indices caused by the clustering of novel classes, the queried labels are not directly applicable to subsequent training.","To overcome this issue, we further propose a stable label mapping algorithm that transforms ground truth labels to the label space of the classifier, thereby ensuring consistent training across different active selection stages.","Our method achieves state-of-the-art performance on both generic and fine-grained datasets.","Our code is available at https://github.com/mashijie1028/ActiveGCD"],"url":"http://arxiv.org/abs/2403.04272v1"}
{"created":"2024-03-07 06:57:20","title":"Conflict and Fairness in Resource Allocation","abstract":"In the standard model of fair allocation of resources to agents, every agent has some utility for every resource, and the goal is to assign resources to agents so that the agents' welfare is maximized. Motivated by job scheduling, interest in this problem dates back to the work of Deuermeyer et al. [SIAM J. on Algebraic Discrete Methods'82]. Recent works consider the compatibility between resources and assign only mutually compatible resources to an agent. We study a fair allocation problem in which we are given a set of agents, a set of resources, a utility function for every agent over a set of resources, and a {\\it conflict graph} on the set of resources (where an edge denotes incompatibility). The goal is to assign resources to the agents such that $(i)$ the set of resources allocated to an agent are compatible with each other, and $(ii)$ the minimum satisfaction of an agent is maximized, where the satisfaction of an agent is the sum of the utility of the assigned resources. Chiarelli et al. [Algorithmica'22] explore this problem from the classical complexity perspective to draw the boundary between the cases that are polynomial-time solvable and those that are \\NP-hard. In this article, we study the parameterized complexity of the problem (and its variants) by considering several natural and structural parameters.","sentences":["In the standard model of fair allocation of resources to agents, every agent has some utility for every resource, and the goal is to assign resources to agents so that the agents' welfare is maximized.","Motivated by job scheduling, interest in this problem dates back to the work of Deuermeyer et al.","[SIAM J. on Algebraic Discrete Methods'82].","Recent works consider the compatibility between resources and assign only mutually compatible resources to an agent.","We study a fair allocation problem in which we are given a set of agents, a set of resources, a utility function for every agent over a set of resources, and a {\\it conflict graph} on the set of resources (where an edge denotes incompatibility).","The goal is to assign resources to the agents such that $(i)$ the set of resources allocated to an agent are compatible with each other, and $(ii)$ the minimum satisfaction of an agent is maximized, where the satisfaction of an agent is the sum of the utility of the assigned resources.","Chiarelli et al.","[Algorithmica'22] explore this problem from the classical complexity perspective to draw the boundary between the cases that are polynomial-time solvable and those that are \\NP-hard.","In this article, we study the parameterized complexity of the problem (and its variants) by considering several natural and structural parameters."],"url":"http://arxiv.org/abs/2403.04265v1"}
{"created":"2024-03-07 06:55:42","title":"Switching Classes: Characterization and Computation","abstract":"In a graph, the switching operation reverses adjacencies between a subset of vertices and the others. For a hereditary graph class $\\mathcal{G}$, we are concerned with the maximum subclass and the minimum superclass of $\\mathcal{G}$ that are closed under switching. We characterize the maximum subclass for many important classes $\\mathcal{G}$, and prove that it is finite when $\\mathcal{G}$ is minor-closed and omits at least one graph. For several graph classes, we develop polynomial-time algorithms to recognize the minimum superclass. We also show that the recognition of the superclass is NP-complete for $H$-free graphs when $H$ is a sufficiently long path or cycle, and it cannot be solved in subexponential time assuming the Exponential Time Hypothesis.","sentences":["In a graph, the switching operation reverses adjacencies between a subset of vertices and the others.","For a hereditary graph class $\\mathcal{G}$, we are concerned with the maximum subclass and the minimum superclass of $\\mathcal{G}$ that are closed under switching.","We characterize the maximum subclass for many important classes $\\mathcal{G}$, and prove that it is finite when $\\mathcal{G}$ is minor-closed and omits at least one graph.","For several graph classes, we develop polynomial-time algorithms to recognize the minimum superclass.","We also show that the recognition of the superclass is NP-complete for $H$-free graphs when $H$ is a sufficiently long path or cycle, and it cannot be solved in subexponential time assuming the Exponential Time Hypothesis."],"url":"http://arxiv.org/abs/2403.04263v1"}
{"created":"2024-03-07 06:52:51","title":"Advancing Biomedical Text Mining with Community Challenges","abstract":"The field of biomedical research has witnessed a significant increase in the accumulation of vast amounts of textual data from various sources such as scientific literatures, electronic health records, clinical trial reports, and social media. However, manually processing and analyzing these extensive and complex resources is time-consuming and inefficient. To address this challenge, biomedical text mining, also known as biomedical natural language processing, has garnered great attention. Community challenge evaluation competitions have played an important role in promoting technology innovation and interdisciplinary collaboration in biomedical text mining research. These challenges provide platforms for researchers to develop state-of-the-art solutions for data mining and information processing in biomedical research. In this article, we review the recent advances in community challenges specific to Chinese biomedical text mining. Firstly, we collect the information of these evaluation tasks, such as data sources and task types. Secondly, we conduct systematic summary and comparative analysis, including named entity recognition, entity normalization, attribute extraction, relation extraction, event extraction, text classification, text similarity, knowledge graph construction, question answering, text generation, and large language model evaluation. Then, we summarize the potential clinical applications of these community challenge tasks from translational informatics perspective. Finally, we discuss the contributions and limitations of these community challenges, while highlighting future directions in the era of large language models.","sentences":["The field of biomedical research has witnessed a significant increase in the accumulation of vast amounts of textual data from various sources such as scientific literatures, electronic health records, clinical trial reports, and social media.","However, manually processing and analyzing these extensive and complex resources is time-consuming and inefficient.","To address this challenge, biomedical text mining, also known as biomedical natural language processing, has garnered great attention.","Community challenge evaluation competitions have played an important role in promoting technology innovation and interdisciplinary collaboration in biomedical text mining research.","These challenges provide platforms for researchers to develop state-of-the-art solutions for data mining and information processing in biomedical research.","In this article, we review the recent advances in community challenges specific to Chinese biomedical text mining.","Firstly, we collect the information of these evaluation tasks, such as data sources and task types.","Secondly, we conduct systematic summary and comparative analysis, including named entity recognition, entity normalization, attribute extraction, relation extraction, event extraction, text classification, text similarity, knowledge graph construction, question answering, text generation, and large language model evaluation.","Then, we summarize the potential clinical applications of these community challenge tasks from translational informatics perspective.","Finally, we discuss the contributions and limitations of these community challenges, while highlighting future directions in the era of large language models."],"url":"http://arxiv.org/abs/2403.04261v1"}
{"created":"2024-03-07 06:40:53","title":"Depth-aware Test-Time Training for Zero-shot Video Object Segmentation","abstract":"Zero-shot Video Object Segmentation (ZSVOS) aims at segmenting the primary moving object without any human annotations. Mainstream solutions mainly focus on learning a single model on large-scale video datasets, which struggle to generalize to unseen videos. In this work, we introduce a test-time training (TTT) strategy to address the problem. Our key insight is to enforce the model to predict consistent depth during the TTT process. In detail, we first train a single network to perform both segmentation and depth prediction tasks. This can be effectively learned with our specifically designed depth modulation layer. Then, for the TTT process, the model is updated by predicting consistent depth maps for the same frame under different data augmentations. In addition, we explore different TTT weight updating strategies. Our empirical results suggest that the momentum-based weight initialization and looping-based training scheme lead to more stable improvements. Experiments show that the proposed method achieves clear improvements on ZSVOS. Our proposed video TTT strategy provides significant superiority over state-of-the-art TTT methods. Our code is available at: https://nifangbaage.github.io/DATTT.","sentences":["Zero-shot Video Object Segmentation (ZSVOS) aims at segmenting the primary moving object without any human annotations.","Mainstream solutions mainly focus on learning a single model on large-scale video datasets, which struggle to generalize to unseen videos.","In this work, we introduce a test-time training (TTT) strategy to address the problem.","Our key insight is to enforce the model to predict consistent depth during the TTT process.","In detail, we first train a single network to perform both segmentation and depth prediction tasks.","This can be effectively learned with our specifically designed depth modulation layer.","Then, for the TTT process, the model is updated by predicting consistent depth maps for the same frame under different data augmentations.","In addition, we explore different TTT weight updating strategies.","Our empirical results suggest that the momentum-based weight initialization and looping-based training scheme lead to more stable improvements.","Experiments show that the proposed method achieves clear improvements on ZSVOS.","Our proposed video TTT strategy provides significant superiority over state-of-the-art TTT methods.","Our code is available at: https://nifangbaage.github.io/DATTT."],"url":"http://arxiv.org/abs/2403.04258v1"}
{"created":"2024-03-07 06:40:01","title":"Towards Robustness Analysis of E-Commerce Ranking System","abstract":"Information retrieval (IR) is a pivotal component in various applications. Recent advances in machine learning (ML) have enabled the integration of ML algorithms into IR, particularly in ranking systems. While there is a plethora of research on the robustness of ML-based ranking systems, these studies largely neglect commercial e-commerce systems and fail to establish a connection between real-world and manipulated query relevance. In this paper, we present the first systematic measurement study on the robustness of e-commerce ranking systems. We define robustness as the consistency of ranking outcomes for semantically identical queries. To quantitatively analyze robustness, we propose a novel metric that considers both ranking position and item-specific information that are absent in existing metrics. Our large-scale measurement study with real-world data from e-commerce retailers reveals an open opportunity to measure and improve robustness since semantically identical queries often yield inconsistent ranking results. Based on our observations, we propose several solution directions to enhance robustness, such as the use of Large Language Models. Note that the issue of robustness discussed herein does not constitute an error or oversight. Rather, in scenarios where there exists a vast array of choices, it is feasible to present a multitude of products in various permutations, all of which could be equally appealing. However, this extensive selection may lead to customer confusion. As e-commerce retailers use various techniques to improve the quality of search results, we hope that this research offers valuable guidance for measuring the robustness of the ranking systems.","sentences":["Information retrieval (IR) is a pivotal component in various applications.","Recent advances in machine learning (ML) have enabled the integration of ML algorithms into IR, particularly in ranking systems.","While there is a plethora of research on the robustness of ML-based ranking systems, these studies largely neglect commercial e-commerce systems and fail to establish a connection between real-world and manipulated query relevance.","In this paper, we present the first systematic measurement study on the robustness of e-commerce ranking systems.","We define robustness as the consistency of ranking outcomes for semantically identical queries.","To quantitatively analyze robustness, we propose a novel metric that considers both ranking position and item-specific information that are absent in existing metrics.","Our large-scale measurement study with real-world data from e-commerce retailers reveals an open opportunity to measure and improve robustness since semantically identical queries often yield inconsistent ranking results.","Based on our observations, we propose several solution directions to enhance robustness, such as the use of Large Language Models.","Note that the issue of robustness discussed herein does not constitute an error or oversight.","Rather, in scenarios where there exists a vast array of choices, it is feasible to present a multitude of products in various permutations, all of which could be equally appealing.","However, this extensive selection may lead to customer confusion.","As e-commerce retailers use various techniques to improve the quality of search results, we hope that this research offers valuable guidance for measuring the robustness of the ranking systems."],"url":"http://arxiv.org/abs/2403.04257v1"}
{"created":"2024-03-07 06:38:41","title":"Federated Recommendation via Hybrid Retrieval Augmented Generation","abstract":"Federated Recommendation (FR) emerges as a novel paradigm that enables privacy-preserving recommendations. However, traditional FR systems usually represent users/items with discrete identities (IDs), suffering from performance degradation due to the data sparsity and heterogeneity in FR. On the other hand, Large Language Models (LLMs) as recommenders have proven effective across various recommendation scenarios. Yet, LLM-based recommenders encounter challenges such as low inference efficiency and potential hallucination, compromising their performance in real-world scenarios. To this end, we propose GPT-FedRec, a federated recommendation framework leveraging ChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism. GPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval process, mining ID-based user patterns and text-based item features. Next, the retrieved results are converted into text prompts and fed into GPT for re-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aims to extract generalized features from data and exploit pretrained knowledge within LLM, overcoming data sparsity and heterogeneity in FR. In addition, the RAG approach also prevents LLM hallucination, improving the recommendation performance for real-world users. Experimental results on diverse benchmark datasets demonstrate the superior performance of GPT-FedRec against state-of-the-art baseline methods.","sentences":["Federated Recommendation (FR) emerges as a novel paradigm that enables privacy-preserving recommendations.","However, traditional FR systems usually represent users/items with discrete identities (IDs), suffering from performance degradation due to the data sparsity and heterogeneity in FR.","On the other hand, Large Language Models (LLMs) as recommenders have proven effective across various recommendation scenarios.","Yet, LLM-based recommenders encounter challenges such as low inference efficiency and potential hallucination, compromising their performance in real-world scenarios.","To this end, we propose GPT-FedRec, a federated recommendation framework leveraging ChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism.","GPT-FedRec is a two-stage solution.","The first stage is a hybrid retrieval process, mining ID-based user patterns and text-based item features.","Next, the retrieved results are converted into text prompts and fed into GPT for re-ranking.","Our proposed hybrid retrieval mechanism and LLM-based re-rank aims to extract generalized features from data and exploit pretrained knowledge within LLM, overcoming data sparsity and heterogeneity in FR.","In addition, the RAG approach also prevents LLM hallucination, improving the recommendation performance for real-world users.","Experimental results on diverse benchmark datasets demonstrate the superior performance of GPT-FedRec against state-of-the-art baseline methods."],"url":"http://arxiv.org/abs/2403.04256v1"}
{"created":"2024-03-07 06:06:55","title":"A Study of Dropout-Induced Modality Bias on Robustness to Missing Video Frames for Audio-Visual Speech Recognition","abstract":"Advanced Audio-Visual Speech Recognition (AVSR) systems have been observed to be sensitive to missing video frames, performing even worse than single-modality models. While applying the dropout technique to the video modality enhances robustness to missing frames, it simultaneously results in a performance loss when dealing with complete data input. In this paper, we investigate this contrasting phenomenon from the perspective of modality bias and reveal that an excessive modality bias on the audio caused by dropout is the underlying reason. Moreover, we present the Modality Bias Hypothesis (MBH) to systematically describe the relationship between modality bias and robustness against missing modality in multimodal systems. Building on these findings, we propose a novel Multimodal Distribution Approximation with Knowledge Distillation (MDA-KD) framework to reduce over-reliance on the audio modality and to maintain performance and robustness simultaneously. Finally, to address an entirely missing modality, we adopt adapters to dynamically switch decision strategies. The effectiveness of our proposed approach is evaluated and validated through a series of comprehensive experiments using the MISP2021 and MISP2022 datasets. Our code is available at https://github.com/dalision/ModalBiasAVSR","sentences":["Advanced Audio-Visual Speech Recognition (AVSR) systems have been observed to be sensitive to missing video frames, performing even worse than single-modality models.","While applying the dropout technique to the video modality enhances robustness to missing frames, it simultaneously results in a performance loss when dealing with complete data input.","In this paper, we investigate this contrasting phenomenon from the perspective of modality bias and reveal that an excessive modality bias on the audio caused by dropout is the underlying reason.","Moreover, we present the Modality Bias Hypothesis (MBH) to systematically describe the relationship between modality bias and robustness against missing modality in multimodal systems.","Building on these findings, we propose a novel Multimodal Distribution Approximation with Knowledge Distillation (MDA-KD) framework to reduce over-reliance on the audio modality and to maintain performance and robustness simultaneously.","Finally, to address an entirely missing modality, we adopt adapters to dynamically switch decision strategies.","The effectiveness of our proposed approach is evaluated and validated through a series of comprehensive experiments using the MISP2021 and MISP2022 datasets.","Our code is available at https://github.com/dalision/ModalBiasAVSR"],"url":"http://arxiv.org/abs/2403.04245v1"}
{"created":"2024-03-07 05:23:06","title":"Equivalence Testing: The Power of Bounded Adaptivity","abstract":"Equivalence testing, a fundamental problem in the field of distribution testing, seeks to infer if two unknown distributions on $[n]$ are the same or far apart in the total variation distance. Conditional sampling has emerged as a powerful query model and has been investigated by theoreticians and practitioners alike, leading to the design of optimal algorithms albeit in a sequential setting (also referred to as adaptive tester). Given the profound impact of parallel computing over the past decades, there has been a strong desire to design algorithms that enable high parallelization. Despite significant algorithmic advancements over the last decade, parallelizable techniques (also termed non-adaptive testers) have $\\tilde{O}(\\log^{12}n)$ query complexity, a prohibitively large complexity to be of practical usage. Therefore, the primary challenge is whether it is possible to design algorithms that enable high parallelization while achieving efficient query complexity.   Our work provides an affirmative answer to the aforementioned challenge: we present a highly parallelizable tester with a query complexity of $\\tilde{O}(\\log n)$, achieved through a single round of adaptivity, marking a significant stride towards harmonizing parallelizability and efficiency in equivalence testing.","sentences":["Equivalence testing, a fundamental problem in the field of distribution testing, seeks to infer if two unknown distributions on $[n]$ are the same or far apart in the total variation distance.","Conditional sampling has emerged as a powerful query model and has been investigated by theoreticians and practitioners alike, leading to the design of optimal algorithms albeit in a sequential setting (also referred to as adaptive tester).","Given the profound impact of parallel computing over the past decades, there has been a strong desire to design algorithms that enable high parallelization.","Despite significant algorithmic advancements over the last decade, parallelizable techniques (also termed non-adaptive testers) have $\\tilde{O}(\\log^{12}n)$ query complexity, a prohibitively large complexity to be of practical usage.","Therefore, the primary challenge is whether it is possible to design algorithms that enable high parallelization while achieving efficient query complexity.   ","Our work provides an affirmative answer to the aforementioned challenge: we present a highly parallelizable tester with a query complexity of $\\tilde{O}(\\log n)$, achieved through a single round of adaptivity, marking a significant stride towards harmonizing parallelizability and efficiency in equivalence testing."],"url":"http://arxiv.org/abs/2403.04230v1"}
{"created":"2024-03-07 04:54:56","title":"Aligners: Decoupling LLMs and Alignment","abstract":"Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We illustrate our method by training an \"ethical\" aligner and verify its efficacy empirically.","sentences":["Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications.","Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion.","We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance.","Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria.","We illustrate our method by training an \"ethical\" aligner and verify its efficacy empirically."],"url":"http://arxiv.org/abs/2403.04224v1"}
{"created":"2024-03-07 04:23:07","title":"HeteroSwitch: Characterizing and Taming System-Induced Data Heterogeneity in Federated Learning","abstract":"Federated Learning (FL) is a practical approach to train deep learning models collaboratively across user-end devices, protecting user privacy by retaining raw data on-device. In FL, participating user-end devices are highly fragmented in terms of hardware and software configurations. Such fragmentation introduces a new type of data heterogeneity in FL, namely \\textit{system-induced data heterogeneity}, as each device generates distinct data depending on its hardware and software configurations. In this paper, we first characterize the impact of system-induced data heterogeneity on FL model performance. We collect a dataset using heterogeneous devices with variations across vendors and performance tiers. By using this dataset, we demonstrate that \\textit{system-induced data heterogeneity} negatively impacts accuracy, and deteriorates fairness and domain generalization problems in FL. To address these challenges, we propose HeteroSwitch, which adaptively adopts generalization techniques (i.e., ISP transformation and SWAD) depending on the level of bias caused by varying HW and SW configurations. In our evaluation with a realistic FL dataset (FLAIR), HeteroSwitch reduces the variance of averaged precision by 6.3\\% across device types.","sentences":["Federated Learning (FL) is a practical approach to train deep learning models collaboratively across user-end devices, protecting user privacy by retaining raw data on-device.","In FL, participating user-end devices are highly fragmented in terms of hardware and software configurations.","Such fragmentation introduces a new type of data heterogeneity in FL, namely \\textit{system-induced data heterogeneity}, as each device generates distinct data depending on its hardware and software configurations.","In this paper, we first characterize the impact of system-induced data heterogeneity on FL model performance.","We collect a dataset using heterogeneous devices with variations across vendors and performance tiers.","By using this dataset, we demonstrate that \\textit{system-induced data heterogeneity} negatively impacts accuracy, and deteriorates fairness and domain generalization problems in FL.","To address these challenges, we propose HeteroSwitch, which adaptively adopts generalization techniques (i.e., ISP transformation and SWAD) depending on the level of bias caused by varying HW and SW configurations.","In our evaluation with a realistic FL dataset (FLAIR), HeteroSwitch reduces the variance of averaged precision by 6.3\\% across device types."],"url":"http://arxiv.org/abs/2403.04207v1"}
{"created":"2024-03-07 04:19:13","title":"On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models","abstract":"Big models have achieved revolutionary breakthroughs in the field of AI, but they might also pose potential concerns. Addressing such concerns, alignment technologies were introduced to make these models conform to human preferences and values. Despite considerable advancements in the past year, various challenges lie in establishing the optimal alignment strategy, such as data cost and scalable oversight, and how to align remains an open question. In this survey paper, we comprehensively investigate value alignment approaches. We first unpack the historical context of alignment tracing back to the 1920s (where it comes from), then delve into the mathematical essence of alignment (what it is), shedding light on the inherent challenges. Following this foundation, we provide a detailed examination of existing alignment methods, which fall into three categories: Reinforcement Learning, Supervised Fine-Tuning, and In-context Learning, and demonstrate their intrinsic connections, strengths, and limitations, helping readers better understand this research area. In addition, two emerging topics, personal alignment, and multimodal alignment, are also discussed as novel frontiers in this field. Looking forward, we discuss potential alignment paradigms and how they could handle remaining challenges, prospecting where future alignment will go.","sentences":["Big models have achieved revolutionary breakthroughs in the field of AI, but they might also pose potential concerns.","Addressing such concerns, alignment technologies were introduced to make these models conform to human preferences and values.","Despite considerable advancements in the past year, various challenges lie in establishing the optimal alignment strategy, such as data cost and scalable oversight, and how to align remains an open question.","In this survey paper, we comprehensively investigate value alignment approaches.","We first unpack the historical context of alignment tracing back to the 1920s (where it comes from), then delve into the mathematical essence of alignment (what it is), shedding light on the inherent challenges.","Following this foundation, we provide a detailed examination of existing alignment methods, which fall into three categories: Reinforcement Learning, Supervised Fine-Tuning, and In-context Learning, and demonstrate their intrinsic connections, strengths, and limitations, helping readers better understand this research area.","In addition, two emerging topics, personal alignment, and multimodal alignment, are also discussed as novel frontiers in this field.","Looking forward, we discuss potential alignment paradigms and how they could handle remaining challenges, prospecting where future alignment will go."],"url":"http://arxiv.org/abs/2403.04204v1"}
{"created":"2024-03-07 04:08:07","title":"Bi-Static Sensing in OFDM Wireless Systems for Indoor Scenarios","abstract":"The sixth generation (6G) systems will likely employ orthogonal frequency division multiplexing (OFDM) waveform for performing the joint task of sensing and communication. In this paper, we design an OFDM system for integrated sensing and communication (ISAC) and propose a novel approach for passive target detection in an indoor deployment using a data driven AI approach. The delay-Doppler profile (DDP) and power delay profile (PDP) is used to train the proposed AI-based detector. We analyze the detection performance of the proposed methods under line of sight (LOS) and non-line of sight (NLOS) conditions for various training strategies. We show that the proposed method provides 10 dB performance improvement over the baseline for 80% target detection under LOS conditions and the performance drops by 10-20 dB for NLOS depending on the usecase scenarios.","sentences":["The sixth generation (6G) systems will likely employ orthogonal frequency division multiplexing (OFDM) waveform for performing the joint task of sensing and communication.","In this paper, we design an OFDM system for integrated sensing and communication (ISAC) and propose a novel approach for passive target detection in an indoor deployment using a data driven AI approach.","The delay-Doppler profile (DDP) and power delay profile (PDP) is used to train the proposed AI-based detector.","We analyze the detection performance of the proposed methods under line of sight (LOS) and non-line of sight (NLOS) conditions for various training strategies.","We show that the proposed method provides 10 dB performance improvement over the baseline for 80% target detection under LOS conditions and the performance drops by 10-20 dB for NLOS depending on the usecase scenarios."],"url":"http://arxiv.org/abs/2403.04201v1"}
{"created":"2024-03-07 03:38:44","title":"Generative AI for Synthetic Data Generation: Methods, Challenges and the Future","abstract":"The recent surge in research focused on generating synthetic data from large language models (LLMs), especially for scenarios with limited data availability, marks a notable shift in Generative Artificial Intelligence (AI). Their ability to perform comparably to real-world data positions this approach as a compelling solution to low-resource challenges. This paper delves into advanced technologies that leverage these gigantic LLMs for the generation of task-specific training data. We outline methodologies, evaluation techniques, and practical applications, discuss the current limitations, and suggest potential pathways for future research.","sentences":["The recent surge in research focused on generating synthetic data from large language models (LLMs), especially for scenarios with limited data availability, marks a notable shift in Generative Artificial Intelligence (AI).","Their ability to perform comparably to real-world data positions this approach as a compelling solution to low-resource challenges.","This paper delves into advanced technologies that leverage these gigantic LLMs for the generation of task-specific training data.","We outline methodologies, evaluation techniques, and practical applications, discuss the current limitations, and suggest potential pathways for future research."],"url":"http://arxiv.org/abs/2403.04190v1"}
{"created":"2024-03-07 03:26:10","title":"Exploring the Impact of Opinion Polarization on Short Video Consumption","abstract":"Investigating the increasingly popular domain of short video consumption, this study focuses on the impact of Opinion Polarization (OP), a significant factor in the digital landscape influencing public opinions and social interactions. We analyze OP's effect on viewers' perceptions and behaviors, finding that traditional feedback metrics like likes and watch time fail to fully capture and measure OP. Addressing this gap, our research utilizes Electroencephalogram (EEG) signals to introduce a novel, non-invasive approach for evaluating neural responses to OP, affecting perception and cognition. Empirical analysis reveals OP's considerable impact on viewers' emotions, evidenced by changes in brain activity. Our findings also highlight the potential of EEG data in predicting exposure to polarized short video content, offering a new perspective on the dynamics of short video consumption and a unique method for quantifying OP's effects.","sentences":["Investigating the increasingly popular domain of short video consumption, this study focuses on the impact of Opinion Polarization (OP), a significant factor in the digital landscape influencing public opinions and social interactions.","We analyze OP's effect on viewers' perceptions and behaviors, finding that traditional feedback metrics like likes and watch time fail to fully capture and measure OP.","Addressing this gap, our research utilizes Electroencephalogram (EEG) signals to introduce a novel, non-invasive approach for evaluating neural responses to OP, affecting perception and cognition.","Empirical analysis reveals OP's considerable impact on viewers' emotions, evidenced by changes in brain activity.","Our findings also highlight the potential of EEG data in predicting exposure to polarized short video content, offering a new perspective on the dynamics of short video consumption and a unique method for quantifying OP's effects."],"url":"http://arxiv.org/abs/2403.04184v1"}
{"created":"2024-03-07 03:23:13","title":"RATSF: Empowering Customer Service Volume Management through Retrieval-Augmented Time-Series Forecasting","abstract":"An efficient customer service management system hinges on precise forecasting of service volume. In this scenario, where data non-stationarity is pronounced, successful forecasting heavily relies on identifying and leveraging similar historical data rather than merely summarizing periodic patterns. Existing models based on RNN or Transformer architectures often struggle with this flexible and effective utilization. To address this challenge, we propose an efficient and adaptable cross-attention module termed RACA, which effectively leverages historical segments in forecasting task, and we devised a precise representation scheme for querying historical sequences, coupled with the design of a knowledge repository. These critical components collectively form our Retrieval-Augmented Temporal Sequence Forecasting framework (RATSF). RATSF not only significantly enhances performance in the context of Fliggy hotel service volume forecasting but, more crucially, can be seamlessly integrated into other Transformer-based time-series forecasting models across various application scenarios. Extensive experimentation has validated the effectiveness and generalizability of this system design across multiple diverse contexts.","sentences":["An efficient customer service management system hinges on precise forecasting of service volume.","In this scenario, where data non-stationarity is pronounced, successful forecasting heavily relies on identifying and leveraging similar historical data rather than merely summarizing periodic patterns.","Existing models based on RNN or Transformer architectures often struggle with this flexible and effective utilization.","To address this challenge, we propose an efficient and adaptable cross-attention module termed RACA, which effectively leverages historical segments in forecasting task, and we devised a precise representation scheme for querying historical sequences, coupled with the design of a knowledge repository.","These critical components collectively form our Retrieval-Augmented Temporal Sequence Forecasting framework (RATSF).","RATSF not only significantly enhances performance in the context of Fliggy hotel service volume forecasting but, more crucially, can be seamlessly integrated into other Transformer-based time-series forecasting models across various application scenarios.","Extensive experimentation has validated the effectiveness and generalizability of this system design across multiple diverse contexts."],"url":"http://arxiv.org/abs/2403.04180v1"}
{"created":"2024-03-07 03:22:31","title":"Mining Transactional Data To Produce Extended Association Rules Using Collaborative Apriori, Fsa-Red And M5p Predictive Algorithm As A Basis Of Business Actions","abstract":"There are large amounts of transactional data which showed consumer shopping cart at a store that sells more than 150 types of products. In this case, the company is utilizing these data in making business action. In previous studies, the data that has a lot of attributes and record data reduction algorithms handled by the FSA Red (Feature Selection for Association Rules)are then mined using Apriori algorithm. The resulting association rules have high levels of accuracy and excellent test results, which rely more than 90%. In this study, the association rules generated in previous research will be updated by using prediction algorithms M5P, so that the association rules can be used within a period of several months in the future. Furthermore, some data mining technique such as: clustering and time series pattern will be implemented to examine the truth and extend the validity of association rules which were built. It can be concluded that the association rules were established after will generate strong association rules with confidence equal or higher than 70% and the rules established truth can be seen from the time series pattern on each group of goods which are then used as the basis of business actions.","sentences":["There are large amounts of transactional data which showed consumer shopping cart at a store that sells more than 150 types of products.","In this case, the company is utilizing these data in making business action.","In previous studies, the data that has a lot of attributes and record data reduction algorithms handled by the FSA Red (Feature Selection for Association Rules)are then mined using Apriori algorithm.","The resulting association rules have high levels of accuracy and excellent test results, which rely more than 90%.","In this study, the association rules generated in previous research will be updated by using prediction algorithms M5P, so that the association rules can be used within a period of several months in the future.","Furthermore, some data mining technique such as: clustering and time series pattern will be implemented to examine the truth and extend the validity of association rules which were built.","It can be concluded that the association rules were established after will generate strong association rules with confidence equal or higher than 70% and the rules established truth can be seen from the time series pattern on each group of goods which are then used as the basis of business actions."],"url":"http://arxiv.org/abs/2403.04179v1"}
{"created":"2024-03-07 03:07:59","title":"Image Coding for Machines with Edge Information Learning Using Segment Anything","abstract":"Image Coding for Machines (ICM) is an image compression technique for image recognition.   This technique is essential due to the growing demand for image recognition AI.   In this paper, we propose a method for ICM that focuses on encoding and decoding only the edge information of object parts in an image, which we call SA-ICM.   This is an Learned Image Compression (LIC) model trained using edge information created by Segment Anything.   Our method can be used for image recognition models with various tasks.   SA-ICM is also robust to changes in input data, making it effective for a variety of use cases.   Additionally, our method provides benefits from a privacy point of view, as it removes human facial information on the encoder's side, thus protecting one's privacy.   Furthermore, this LIC model training method can be used to train Neural Representations for Videos (NeRV), which is a video compression model.   By training NeRV using edge information created by Segment Anything, it is possible to create a NeRV that is effective for image recognition (SA-NeRV).   Experimental results confirm the advantages of SA-ICM, presenting the best performance in image compression for image recognition.   We also show that SA-NeRV is superior to ordinary NeRV in video compression for machines.","sentences":["Image Coding for Machines (ICM) is an image compression technique for image recognition.   ","This technique is essential due to the growing demand for image recognition AI.   ","In this paper, we propose a method for ICM that focuses on encoding and decoding only the edge information of object parts in an image, which we call SA-ICM.   ","This is an Learned Image Compression (LIC) model trained using edge information created by Segment Anything.   ","Our method can be used for image recognition models with various tasks.   ","SA-ICM is also robust to changes in input data, making it effective for a variety of use cases.   ","Additionally, our method provides benefits from a privacy point of view, as it removes human facial information on the encoder's side, thus protecting one's privacy.   ","Furthermore, this LIC model training method can be used to train Neural Representations for Videos (NeRV), which is a video compression model.   ","By training NeRV using edge information created by Segment Anything, it is possible to create a NeRV that is effective for image recognition (SA-NeRV).   ","Experimental results confirm the advantages of SA-ICM, presenting the best performance in image compression for image recognition.   ","We also show that SA-NeRV is superior to ordinary NeRV in video compression for machines."],"url":"http://arxiv.org/abs/2403.04173v1"}
{"created":"2024-03-07 02:54:51","title":"Super-resolution on network telemetry time series","abstract":"Fine-grained monitoring is crucial for multiple data-driven tasks such as debugging, provisioning, and securing networks. Yet, practical constraints in collecting, extracting, and storing data often force operators to use coarse-grained sampled monitoring, degrading the performance of the various tasks. In this work, we explore the feasibility of leveraging the correlations among coarse-grained time series to impute their fine-grained counterparts in software. We present Zoom2Net, a transformer-based model for network imputation that incorporates domain knowledge through operational and measurement constraints, ensuring that the imputed network telemetry time series are not only realistic but also align with existing measurements and are plausible. This approach enhances the capabilities of current monitoring infrastructures, allowing operators to gain more insights into system behaviors without the need for hardware upgrades. We evaluate Zoom2Net on four diverse datasets (e.g. cloud telemetry and Internet data transfer) and use cases (such as bursts analysis and traffic classification). We demonstrate that Zoom2Net consistently achieves high imputation accuracy with a zoom-in factor of up to 100 and performs better on downstream tasks compared to baselines by an average of 38%.","sentences":["Fine-grained monitoring is crucial for multiple data-driven tasks such as debugging, provisioning, and securing networks.","Yet, practical constraints in collecting, extracting, and storing data often force operators to use coarse-grained sampled monitoring, degrading the performance of the various tasks.","In this work, we explore the feasibility of leveraging the correlations among coarse-grained time series to impute their fine-grained counterparts in software.","We present Zoom2Net, a transformer-based model for network imputation that incorporates domain knowledge through operational and measurement constraints, ensuring that the imputed network telemetry time series are not only realistic but also align with existing measurements and are plausible.","This approach enhances the capabilities of current monitoring infrastructures, allowing operators to gain more insights into system behaviors without the need for hardware upgrades.","We evaluate Zoom2Net on four diverse datasets (e.g. cloud telemetry and Internet data transfer) and use cases (such as bursts analysis and traffic classification).","We demonstrate that Zoom2Net consistently achieves high imputation accuracy with a zoom-in factor of up to 100 and performs better on downstream tasks compared to baselines by an average of 38%."],"url":"http://arxiv.org/abs/2403.04165v1"}
{"created":"2024-03-07 02:24:45","title":"Stabilizing Policy Gradients for Stochastic Differential Equations via Consistency with Perturbation Process","abstract":"Considering generating samples with high rewards, we focus on optimizing deep neural networks parameterized stochastic differential equations (SDEs), the advanced generative models with high expressiveness, with policy gradient, the leading algorithm in reinforcement learning. Nevertheless, when applying policy gradients to SDEs, since the policy gradient is estimated on a finite set of trajectories, it can be ill-defined, and the policy behavior in data-scarce regions may be uncontrolled. This challenge compromises the stability of policy gradients and negatively impacts sample complexity. To address these issues, we propose constraining the SDE to be consistent with its associated perturbation process. Since the perturbation process covers the entire space and is easy to sample, we can mitigate the aforementioned problems. Our framework offers a general approach allowing for a versatile selection of policy gradient methods to effectively and efficiently train SDEs. We evaluate our algorithm on the task of structure-based drug design and optimize the binding affinity of generated ligand molecules. Our method achieves the best Vina score -9.07 on the CrossDocked2020 dataset.","sentences":["Considering generating samples with high rewards, we focus on optimizing deep neural networks parameterized stochastic differential equations (SDEs), the advanced generative models with high expressiveness, with policy gradient, the leading algorithm in reinforcement learning.","Nevertheless, when applying policy gradients to SDEs, since the policy gradient is estimated on a finite set of trajectories, it can be ill-defined, and the policy behavior in data-scarce regions may be uncontrolled.","This challenge compromises the stability of policy gradients and negatively impacts sample complexity.","To address these issues, we propose constraining the SDE to be consistent with its associated perturbation process.","Since the perturbation process covers the entire space and is easy to sample, we can mitigate the aforementioned problems.","Our framework offers a general approach allowing for a versatile selection of policy gradient methods to effectively and efficiently train SDEs.","We evaluate our algorithm on the task of structure-based drug design and optimize the binding affinity of generated ligand molecules.","Our method achieves the best Vina score -9.07 on the CrossDocked2020 dataset."],"url":"http://arxiv.org/abs/2403.04154v1"}
{"created":"2024-03-07 02:10:59","title":"MAP: MAsk-Pruning for Source-Free Model Intellectual Property Protection","abstract":"Deep learning has achieved remarkable progress in various applications, heightening the importance of safeguarding the intellectual property (IP) of well-trained models. It entails not only authorizing usage but also ensuring the deployment of models in authorized data domains, i.e., making models exclusive to certain target domains. Previous methods necessitate concurrent access to source training data and target unauthorized data when performing IP protection, making them risky and inefficient for decentralized private data. In this paper, we target a practical setting where only a well-trained source model is available and investigate how we can realize IP protection. To achieve this, we propose a novel MAsk Pruning (MAP) framework. MAP stems from an intuitive hypothesis, i.e., there are target-related parameters in a well-trained model, locating and pruning them is the key to IP protection. Technically, MAP freezes the source model and learns a target-specific binary mask to prevent unauthorized data usage while minimizing performance degradation on authorized data. Moreover, we introduce a new metric aimed at achieving a better balance between source and target performance degradation. To verify the effectiveness and versatility, we have evaluated MAP in a variety of scenarios, including vanilla source-available, practical source-free, and challenging data-free. Extensive experiments indicate that MAP yields new state-of-the-art performance.","sentences":["Deep learning has achieved remarkable progress in various applications, heightening the importance of safeguarding the intellectual property (IP) of well-trained models.","It entails not only authorizing usage but also ensuring the deployment of models in authorized data domains, i.e., making models exclusive to certain target domains.","Previous methods necessitate concurrent access to source training data and target unauthorized data when performing IP protection, making them risky and inefficient for decentralized private data.","In this paper, we target a practical setting where only a well-trained source model is available and investigate how we can realize IP protection.","To achieve this, we propose a novel MAsk Pruning (MAP) framework.","MAP stems from an intuitive hypothesis, i.e., there are target-related parameters in a well-trained model, locating and pruning them is the key to IP protection.","Technically, MAP freezes the source model and learns a target-specific binary mask to prevent unauthorized data usage while minimizing performance degradation on authorized data.","Moreover, we introduce a new metric aimed at achieving a better balance between source and target performance degradation.","To verify the effectiveness and versatility, we have evaluated MAP in a variety of scenarios, including vanilla source-available, practical source-free, and challenging data-free.","Extensive experiments indicate that MAP yields new state-of-the-art performance."],"url":"http://arxiv.org/abs/2403.04149v1"}
{"created":"2024-03-07 01:52:05","title":"FL-GUARD: A Holistic Framework for Run-Time Detection and Recovery of Negative Federated Learning","abstract":"Federated learning (FL) is a promising approach for learning a model from data distributed on massive clients without exposing data privacy. It works effectively in the ideal federation where clients share homogeneous data distribution and learning behavior. However, FL may fail to function appropriately when the federation is not ideal, amid an unhealthy state called Negative Federated Learning (NFL), in which most clients gain no benefit from participating in FL. Many studies have tried to address NFL. However, their solutions either (1) predetermine to prevent NFL in the entire learning life-cycle or (2) tackle NFL in the aftermath of numerous learning rounds. Thus, they either (1) indiscriminately incur extra costs even if FL can perform well without such costs or (2) waste numerous learning rounds. Additionally, none of the previous work takes into account the clients who may be unwilling/unable to follow the proposed NFL solutions when using those solutions to upgrade an FL system in use. This paper introduces FL-GUARD, a holistic framework that can be employed on any FL system for tackling NFL in a run-time paradigm. That is, to dynamically detect NFL at the early stage (tens of rounds) of learning and then to activate recovery measures when necessary. Specifically, we devise a cost-effective NFL detection mechanism, which relies on an estimation of performance gain on clients. Only when NFL is detected, we activate the NFL recovery process, in which each client learns in parallel an adapted model when training the global model. Extensive experiment results confirm the effectiveness of FL-GUARD in detecting NFL and recovering from NFL to a healthy learning state. We also show that FL-GUARD is compatible with previous NFL solutions and robust against clients unwilling/unable to take any recovery measures.","sentences":["Federated learning (FL) is a promising approach for learning a model from data distributed on massive clients without exposing data privacy.","It works effectively in the ideal federation where clients share homogeneous data distribution and learning behavior.","However, FL may fail to function appropriately when the federation is not ideal, amid an unhealthy state called Negative Federated Learning (NFL), in which most clients gain no benefit from participating in FL.","Many studies have tried to address NFL.","However, their solutions either (1) predetermine to prevent NFL in the entire learning life-cycle or (2) tackle NFL in the aftermath of numerous learning rounds.","Thus, they either (1) indiscriminately incur extra costs even if FL can perform well without such costs or (2) waste numerous learning rounds.","Additionally, none of the previous work takes into account the clients who may be unwilling/unable to follow the proposed NFL solutions when using those solutions to upgrade an FL system in use.","This paper introduces FL-GUARD, a holistic framework that can be employed on any FL system for tackling NFL in a run-time paradigm.","That is, to dynamically detect NFL at the early stage (tens of rounds) of learning and then to activate recovery measures when necessary.","Specifically, we devise a cost-effective NFL detection mechanism, which relies on an estimation of performance gain on clients.","Only when NFL is detected, we activate the NFL recovery process, in which each client learns in parallel an adapted model when training the global model.","Extensive experiment results confirm the effectiveness of FL-GUARD in detecting NFL and recovering from NFL to a healthy learning state.","We also show that FL-GUARD is compatible with previous NFL solutions and robust against clients unwilling/unable to take any recovery measures."],"url":"http://arxiv.org/abs/2403.04146v1"}
{"created":"2024-03-07 01:50:36","title":"FedClust: Optimizing Federated Learning on Non-IID Data through Weight-Driven Client Clustering","abstract":"Federated learning (FL) is an emerging distributed machine learning paradigm enabling collaborative model training on decentralized devices without exposing their local data. A key challenge in FL is the uneven data distribution across client devices, violating the well-known assumption of independent-and-identically-distributed (IID) training samples in conventional machine learning. Clustered federated learning (CFL) addresses this challenge by grouping clients based on the similarity of their data distributions. However, existing CFL approaches require a large number of communication rounds for stable cluster formation and rely on a predefined number of clusters, thus limiting their flexibility and adaptability. This paper proposes FedClust, a novel CFL approach leveraging correlations between local model weights and client data distributions. FedClust groups clients into clusters in a one-shot manner using strategically selected partial model weights and dynamically accommodates newcomers in real-time. Experimental results demonstrate FedClust outperforms baseline approaches in terms of accuracy and communication costs.","sentences":["Federated learning (FL) is an emerging distributed machine learning paradigm enabling collaborative model training on decentralized devices without exposing their local data.","A key challenge in FL is the uneven data distribution across client devices, violating the well-known assumption of independent-and-identically-distributed (IID) training samples in conventional machine learning.","Clustered federated learning (CFL) addresses this challenge by grouping clients based on the similarity of their data distributions.","However, existing CFL approaches require a large number of communication rounds for stable cluster formation and rely on a predefined number of clusters, thus limiting their flexibility and adaptability.","This paper proposes FedClust, a novel CFL approach leveraging correlations between local model weights and client data distributions.","FedClust groups clients into clusters in a one-shot manner using strategically selected partial model weights and dynamically accommodates newcomers in real-time.","Experimental results demonstrate FedClust outperforms baseline approaches in terms of accuracy and communication costs."],"url":"http://arxiv.org/abs/2403.04144v1"}
{"created":"2024-03-07 01:29:48","title":"Unsupervised Learning of Harmonic Analysis Based on Neural HSMM with Code Quality Templates","abstract":"This paper presents a method of unsupervised learning of harmonic analysis based on a hidden semi-Markov model (HSMM). We introduce the chord quality templates, which specify the probability of pitch class emissions given a root note and a chord quality. Other probability distributions that comprise the HSMM are automatically learned via unsupervised learning, which has been a challenge in existing research. The results of the harmonic analysis of the proposed model were evaluated using existing labeled data. While our proposed method has yet to perform as well as existing models that used supervised learning and complex rule design, it has the advantage of not requiring expensive labeled data or rule elaboration. Furthermore, we also show how to recognize the tonic without prior knowledge, based on the transition probabilities of the Markov model.","sentences":["This paper presents a method of unsupervised learning of harmonic analysis based on a hidden semi-Markov model (HSMM).","We introduce the chord quality templates, which specify the probability of pitch class emissions given a root note and a chord quality.","Other probability distributions that comprise the HSMM are automatically learned via unsupervised learning, which has been a challenge in existing research.","The results of the harmonic analysis of the proposed model were evaluated using existing labeled data.","While our proposed method has yet to perform as well as existing models that used supervised learning and complex rule design, it has the advantage of not requiring expensive labeled data or rule elaboration.","Furthermore, we also show how to recognize the tonic without prior knowledge, based on the transition probabilities of the Markov model."],"url":"http://arxiv.org/abs/2403.04135v1"}
{"created":"2024-03-07 01:24:59","title":"Towards learning-based planning:The nuPlan benchmark for real-world autonomous driving","abstract":"Machine Learning (ML) has replaced traditional handcrafted methods for perception and prediction in autonomous vehicles. Yet for the equally important planning task, the adoption of ML-based techniques is slow. We present nuPlan, the world's first real-world autonomous driving dataset, and benchmark. The benchmark is designed to test the ability of ML-based planners to handle diverse driving situations and to make safe and efficient decisions. To that end, we introduce a new large-scale dataset that consists of 1282 hours of diverse driving scenarios from 4 cities (Las Vegas, Boston, Pittsburgh, and Singapore) and includes high-quality auto-labeled object tracks and traffic light data. We exhaustively mine and taxonomize common and rare driving scenarios which are used during evaluation to get fine-grained insights into the performance and characteristics of a planner. Beyond the dataset, we provide a simulation and evaluation framework that enables a planner's actions to be simulated in closed-loop to account for interactions with other traffic participants. We present a detailed analysis of numerous baselines and investigate gaps between ML-based and traditional methods. Find the nuPlan dataset and code at nuplan.org.","sentences":["Machine Learning (ML) has replaced traditional handcrafted methods for perception and prediction in autonomous vehicles.","Yet for the equally important planning task, the adoption of ML-based techniques is slow.","We present nuPlan, the world's first real-world autonomous driving dataset, and benchmark.","The benchmark is designed to test the ability of ML-based planners to handle diverse driving situations and to make safe and efficient decisions.","To that end, we introduce a new large-scale dataset that consists of 1282 hours of diverse driving scenarios from 4 cities (Las Vegas, Boston, Pittsburgh, and Singapore) and includes high-quality auto-labeled object tracks and traffic light data.","We exhaustively mine and taxonomize common and rare driving scenarios which are used during evaluation to get fine-grained insights into the performance and characteristics of a planner.","Beyond the dataset, we provide a simulation and evaluation framework that enables a planner's actions to be simulated in closed-loop to account for interactions with other traffic participants.","We present a detailed analysis of numerous baselines and investigate gaps between ML-based and traditional methods.","Find the nuPlan dataset and code at nuplan.org."],"url":"http://arxiv.org/abs/2403.04133v1"}
{"created":"2024-03-07 01:22:38","title":"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference","abstract":"Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at \\url{https://chat.lmsys.org}.","sentences":["Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges.","To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences.","Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing.","The platform has been operational for several months, amassing over 240K votes.","This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models.","We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters.","These analyses collectively establish a robust foundation for the credibility of Chatbot Arena.","Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies.","Our demo is publicly available at \\url{https://chat.lmsys.org}."],"url":"http://arxiv.org/abs/2403.04132v1"}
{"created":"2024-03-07 01:08:41","title":"An Explainable AI Framework for Artificial Intelligence of Medical Things","abstract":"The healthcare industry has been revolutionized by the convergence of Artificial Intelligence of Medical Things (AIoMT), allowing advanced data-driven solutions to improve healthcare systems. With the increasing complexity of Artificial Intelligence (AI) models, the need for Explainable Artificial Intelligence (XAI) techniques become paramount, particularly in the medical domain, where transparent and interpretable decision-making becomes crucial. Therefore, in this work, we leverage a custom XAI framework, incorporating techniques such as Local Interpretable Model-Agnostic Explanations (LIME), SHapley Additive exPlanations (SHAP), and Gradient-weighted Class Activation Mapping (Grad-Cam), explicitly designed for the domain of AIoMT. The proposed framework enhances the effectiveness of strategic healthcare methods and aims to instill trust and promote understanding in AI-driven medical applications. Moreover, we utilize a majority voting technique that aggregates predictions from multiple convolutional neural networks (CNNs) and leverages their collective intelligence to make robust and accurate decisions in the healthcare system. Building upon this decision-making process, we apply the XAI framework to brain tumor detection as a use case demonstrating accurate and transparent diagnosis. Evaluation results underscore the exceptional performance of the XAI framework, achieving high precision, recall, and F1 scores with a training accuracy of 99% and a validation accuracy of 98%. Combining advanced XAI techniques with ensemble-based deep-learning (DL) methodologies allows for precise and reliable brain tumor diagnoses as an application of AIoMT.","sentences":["The healthcare industry has been revolutionized by the convergence of Artificial Intelligence of Medical Things (AIoMT), allowing advanced data-driven solutions to improve healthcare systems.","With the increasing complexity of Artificial Intelligence (AI) models, the need for Explainable Artificial Intelligence (XAI) techniques become paramount, particularly in the medical domain, where transparent and interpretable decision-making becomes crucial.","Therefore, in this work, we leverage a custom XAI framework, incorporating techniques such as Local Interpretable Model-Agnostic Explanations (LIME), SHapley Additive exPlanations (SHAP), and Gradient-weighted Class Activation Mapping (Grad-Cam), explicitly designed for the domain of AIoMT.","The proposed framework enhances the effectiveness of strategic healthcare methods and aims to instill trust and promote understanding in AI-driven medical applications.","Moreover, we utilize a majority voting technique that aggregates predictions from multiple convolutional neural networks (CNNs) and leverages their collective intelligence to make robust and accurate decisions in the healthcare system.","Building upon this decision-making process, we apply the XAI framework to brain tumor detection as a use case demonstrating accurate and transparent diagnosis.","Evaluation results underscore the exceptional performance of the XAI framework, achieving high precision, recall, and F1 scores with a training accuracy of 99% and a validation accuracy of 98%.","Combining advanced XAI techniques with ensemble-based deep-learning (DL) methodologies allows for precise and reliable brain tumor diagnoses as an application of AIoMT."],"url":"http://arxiv.org/abs/2403.04130v1"}
{"created":"2024-03-07 00:32:47","title":"A data-centric approach to class-specific bias in image data augmentation","abstract":"Data augmentation (DA) enhances model generalization in computer vision but may introduce biases, impacting class accuracy unevenly. Our study extends this inquiry, examining DA's class-specific bias across various datasets, including those distinct from ImageNet, through random cropping. We evaluated this phenomenon with ResNet50, EfficientNetV2S, and SWIN ViT, discovering that while residual models showed similar bias effects, Vision Transformers exhibited greater robustness or altered dynamics. This suggests a nuanced approach to model selection, emphasizing bias mitigation. We also refined a \"data augmentation robustness scouting\" method to manage DA-induced biases more efficiently, reducing computational demands significantly (training 112 models instead of 1860; a reduction of factor 16.2) while still capturing essential bias trends.","sentences":["Data augmentation (DA) enhances model generalization in computer vision but may introduce biases, impacting class accuracy unevenly.","Our study extends this inquiry, examining DA's class-specific bias across various datasets, including those distinct from ImageNet, through random cropping.","We evaluated this phenomenon with ResNet50, EfficientNetV2S, and SWIN ViT, discovering that while residual models showed similar bias effects, Vision Transformers exhibited greater robustness or altered dynamics.","This suggests a nuanced approach to model selection, emphasizing bias mitigation.","We also refined a \"data augmentation robustness scouting\" method to manage DA-induced biases more efficiently, reducing computational demands significantly (training 112 models instead of 1860; a reduction of factor 16.2) while still capturing essential bias trends."],"url":"http://arxiv.org/abs/2403.04120v1"}
{"created":"2024-03-07 00:00:02","title":"Closing the Visual Sim-to-Real Gap with Object-Composable NeRFs","abstract":"Deep learning methods for perception are the cornerstone of many robotic systems. Despite their potential for impressive performance, obtaining real-world training data is expensive, and can be impractically difficult for some tasks. Sim-to-real transfer with domain randomization offers a potential workaround, but often requires extensive manual tuning and results in models that are brittle to distribution shift between sim and real. In this work, we introduce Composable Object Volume NeRF (COV-NeRF), an object-composable NeRF model that is the centerpiece of a real-to-sim pipeline for synthesizing training data targeted to scenes and objects from the real world. COV-NeRF extracts objects from real images and composes them into new scenes, generating photorealistic renderings and many types of 2D and 3D supervision, including depth maps, segmentation masks, and meshes. We show that COV-NeRF matches the rendering quality of modern NeRF methods, and can be used to rapidly close the sim-to-real gap across a variety of perceptual modalities.","sentences":["Deep learning methods for perception are the cornerstone of many robotic systems.","Despite their potential for impressive performance, obtaining real-world training data is expensive, and can be impractically difficult for some tasks.","Sim-to-real transfer with domain randomization offers a potential workaround, but often requires extensive manual tuning and results in models that are brittle to distribution shift between sim and real.","In this work, we introduce Composable Object Volume NeRF (COV-NeRF), an object-composable NeRF model that is the centerpiece of a real-to-sim pipeline for synthesizing training data targeted to scenes and objects from the real world.","COV-NeRF extracts objects from real images and composes them into new scenes, generating photorealistic renderings and many types of 2D and 3D supervision, including depth maps, segmentation masks, and meshes.","We show that COV-NeRF matches the rendering quality of modern NeRF methods, and can be used to rapidly close the sim-to-real gap across a variety of perceptual modalities."],"url":"http://arxiv.org/abs/2403.04114v1"}
{"created":"2024-03-06 23:49:16","title":"Multi-Object Tracking with Camera-LiDAR Fusion for Autonomous Driving","abstract":"This paper presents a novel multi-modal Multi-Object Tracking (MOT) algorithm for self-driving cars that combines camera and LiDAR data. Camera frames are processed with a state-of-the-art 3D object detector, whereas classical clustering techniques are used to process LiDAR observations. The proposed MOT algorithm comprises a three-step association process, an Extended Kalman filter for estimating the motion of each detected dynamic obstacle, and a track management phase. The EKF motion model requires the current measured relative position and orientation of the observed object and the longitudinal and angular velocities of the ego vehicle as inputs. Unlike most state-of-the-art multi-modal MOT approaches, the proposed algorithm does not rely on maps or knowledge of the ego global pose. Moreover, it uses a 3D detector exclusively for cameras and is agnostic to the type of LiDAR sensor used. The algorithm is validated both in simulation and with real-world data, with satisfactory results.","sentences":["This paper presents a novel multi-modal Multi-Object Tracking (MOT) algorithm for self-driving cars that combines camera and LiDAR data.","Camera frames are processed with a state-of-the-art 3D object detector, whereas classical clustering techniques are used to process LiDAR observations.","The proposed MOT algorithm comprises a three-step association process, an Extended Kalman filter for estimating the motion of each detected dynamic obstacle, and a track management phase.","The EKF motion model requires the current measured relative position and orientation of the observed object and the longitudinal and angular velocities of the ego vehicle as inputs.","Unlike most state-of-the-art multi-modal MOT approaches, the proposed algorithm does not rely on maps or knowledge of the ego global pose.","Moreover, it uses a 3D detector exclusively for cameras and is agnostic to the type of LiDAR sensor used.","The algorithm is validated both in simulation and with real-world data, with satisfactory results."],"url":"http://arxiv.org/abs/2403.04112v1"}
{"created":"2024-03-06 23:20:34","title":"Understanding Biology in the Age of Artificial Intelligence","abstract":"Modern life sciences research is increasingly relying on artificial intelligence approaches to model biological systems, primarily centered around the use of machine learning (ML) models. Although ML is undeniably useful for identifying patterns in large, complex data sets, its widespread application in biological sciences represents a significant deviation from traditional methods of scientific inquiry. As such, the interplay between these models and scientific understanding in biology is a topic with important implications for the future of scientific research, yet it is a subject that has received little attention. Here, we draw from an epistemological toolkit to contextualize recent applications of ML in biological sciences under modern philosophical theories of understanding, identifying general principles that can guide the design and application of ML systems to model biological phenomena and advance scientific knowledge. We propose that conceptions of scientific understanding as information compression, qualitative intelligibility, and dependency relation modelling provide a useful framework for interpreting ML-mediated understanding of biological systems. Through a detailed analysis of two key application areas of ML in modern biological research - protein structure prediction and single cell RNA-sequencing - we explore how these features have thus far enabled ML systems to advance scientific understanding of their target phenomena, how they may guide the development of future ML models, and the key obstacles that remain in preventing ML from achieving its potential as a tool for biological discovery. Consideration of the epistemological features of ML applications in biology will improve the prospects of these methods to solve important problems and advance scientific understanding of living systems.","sentences":["Modern life sciences research is increasingly relying on artificial intelligence approaches to model biological systems, primarily centered around the use of machine learning (ML) models.","Although ML is undeniably useful for identifying patterns in large, complex data sets, its widespread application in biological sciences represents a significant deviation from traditional methods of scientific inquiry.","As such, the interplay between these models and scientific understanding in biology is a topic with important implications for the future of scientific research, yet it is a subject that has received little attention.","Here, we draw from an epistemological toolkit to contextualize recent applications of ML in biological sciences under modern philosophical theories of understanding, identifying general principles that can guide the design and application of ML systems to model biological phenomena and advance scientific knowledge.","We propose that conceptions of scientific understanding as information compression, qualitative intelligibility, and dependency relation modelling provide a useful framework for interpreting ML-mediated understanding of biological systems.","Through a detailed analysis of two key application areas of ML in modern biological research - protein structure prediction and single cell RNA-sequencing - we explore how these features have thus far enabled ML systems to advance scientific understanding of their target phenomena, how they may guide the development of future ML models, and the key obstacles that remain in preventing ML from achieving its potential as a tool for biological discovery.","Consideration of the epistemological features of ML applications in biology will improve the prospects of these methods to solve important problems and advance scientific understanding of living systems."],"url":"http://arxiv.org/abs/2403.04106v1"}
{"created":"2024-03-06 23:17:16","title":"Artificial Intelligence Exploring the Patent Field","abstract":"Advanced language-processing and machine-learning techniques promise massive efficiency improvements in the previously widely manual field of patent and technical knowledge management. This field presents large-scale and complex data with very precise contents and language representation of those contents. Particularly, patent texts can differ from mundane texts in various aspects, which entails significant opportunities and challenges. This paper presents a systematic overview of patent-related tasks and popular methodologies with a special focus on evolving and promising techniques. Language processing and particularly large language models as well as the recent boost of general generative methods promise to become game changers in the patent field. The patent literature and the fact-based argumentative procedures around patents appear almost as an ideal use case. However, patents entail a number of difficulties with which existing models struggle. The paper introduces fundamental aspects of patents and patent-related data that affect technology that wants to explore or manage them. It further reviews existing methods and approaches and points out how important reliable and unbiased evaluation metrics become. Although research has made substantial progress on certain tasks, the performance across many others remains suboptimal, sometimes because of either the special nature of patents and their language or inconsistencies between legal terms and the everyday meaning of terms. Moreover, yet few methods have demonstrated the ability to produce satisfactory text for specific sections of patents. By pointing out key developments, opportunities, and gaps, we aim to encourage further research and accelerate the advancement of this field.","sentences":["Advanced language-processing and machine-learning techniques promise massive efficiency improvements in the previously widely manual field of patent and technical knowledge management.","This field presents large-scale and complex data with very precise contents and language representation of those contents.","Particularly, patent texts can differ from mundane texts in various aspects, which entails significant opportunities and challenges.","This paper presents a systematic overview of patent-related tasks and popular methodologies with a special focus on evolving and promising techniques.","Language processing and particularly large language models as well as the recent boost of general generative methods promise to become game changers in the patent field.","The patent literature and the fact-based argumentative procedures around patents appear almost as an ideal use case.","However, patents entail a number of difficulties with which existing models struggle.","The paper introduces fundamental aspects of patents and patent-related data that affect technology that wants to explore or manage them.","It further reviews existing methods and approaches and points out how important reliable and unbiased evaluation metrics become.","Although research has made substantial progress on certain tasks, the performance across many others remains suboptimal, sometimes because of either the special nature of patents and their language or inconsistencies between legal terms and the everyday meaning of terms.","Moreover, yet few methods have demonstrated the ability to produce satisfactory text for specific sections of patents.","By pointing out key developments, opportunities, and gaps, we aim to encourage further research and accelerate the advancement of this field."],"url":"http://arxiv.org/abs/2403.04105v1"}
{"created":"2024-03-06 22:52:15","title":"Assisting International Migrants with Everyday Information Seeking: From the Providers' Lens","abstract":"International migrants face difficulties obtaining information for a quality life and well-being in the host country. Prior research indicates that international migrants often seek information from their co-national cohort or contacts from the same country. The downside of this practice, however, is that people can end up clustering in a small-world environment, hindering the information seekers' social adaptation in the long run. In the current research, we investigated the ongoing practices and future opportunities to connect international migrants with others beyond their co-national contacts. Our work zooms in on the providers' perspectives, which complements previous studies that pay exclusive attention to the information seekers. Specifically, we conducted in-depth interviews with 21 participants assisting the needs of informational migrants in the United States. Some of these people are fellow migrants from a different home country than the information seeker, whereas the rest are domestic residents. Our data revealed how these participants dealt with language barriers, overcame knowledge disparities, and calibrated their effort commitment as information providers. Based on these findings, we discuss directions for future information and communication technologies (ICT) design that can facilitate international migrants' daily information seeking by accounting for the provider's needs and concerns.","sentences":["International migrants face difficulties obtaining information for a quality life and well-being in the host country.","Prior research indicates that international migrants often seek information from their co-national cohort or contacts from the same country.","The downside of this practice, however, is that people can end up clustering in a small-world environment, hindering the information seekers' social adaptation in the long run.","In the current research, we investigated the ongoing practices and future opportunities to connect international migrants with others beyond their co-national contacts.","Our work zooms in on the providers' perspectives, which complements previous studies that pay exclusive attention to the information seekers.","Specifically, we conducted in-depth interviews with 21 participants assisting the needs of informational migrants in the United States.","Some of these people are fellow migrants from a different home country than the information seeker, whereas the rest are domestic residents.","Our data revealed how these participants dealt with language barriers, overcame knowledge disparities, and calibrated their effort commitment as information providers.","Based on these findings, we discuss directions for future information and communication technologies (ICT) design that can facilitate international migrants' daily information seeking by accounting for the provider's needs and concerns."],"url":"http://arxiv.org/abs/2403.04096v1"}
{"created":"2024-03-06 22:32:48","title":"Automated Multi-Task Learning for Joint Disease Prediction on Electronic Health Records","abstract":"In the realm of big data and digital healthcare, Electronic Health Records (EHR) have become a rich source of information with the potential to improve patient care and medical research. In recent years, machine learning models have proliferated for analyzing EHR data to predict patients future health conditions. Among them, some studies advocate for multi-task learning (MTL) to jointly predict multiple target diseases for improving the prediction performance over single task learning. Nevertheless, current MTL frameworks for EHR data have significant limitations due to their heavy reliance on human experts to identify task groups for joint training and design model architectures. To reduce human intervention and improve the framework design, we propose an automated approach named AutoDP, which can search for the optimal configuration of task grouping and architectures simultaneously. To tackle the vast joint search space encompassing task combinations and architectures, we employ surrogate model-based optimization, enabling us to efficiently discover the optimal solution. Experimental results on real-world EHR data demonstrate the efficacy of the proposed AutoDP framework. It achieves significant performance improvements over both hand-crafted and automated state-of-the-art methods, also maintains a feasible search cost at the same time.","sentences":["In the realm of big data and digital healthcare, Electronic Health Records (EHR) have become a rich source of information with the potential to improve patient care and medical research.","In recent years, machine learning models have proliferated for analyzing EHR data to predict patients future health conditions.","Among them, some studies advocate for multi-task learning (MTL) to jointly predict multiple target diseases for improving the prediction performance over single task learning.","Nevertheless, current MTL frameworks for EHR data have significant limitations due to their heavy reliance on human experts to identify task groups for joint training and design model architectures.","To reduce human intervention and improve the framework design, we propose an automated approach named AutoDP, which can search for the optimal configuration of task grouping and architectures simultaneously.","To tackle the vast joint search space encompassing task combinations and architectures, we employ surrogate model-based optimization, enabling us to efficiently discover the optimal solution.","Experimental results on real-world EHR data demonstrate the efficacy of the proposed AutoDP framework.","It achieves significant performance improvements over both hand-crafted and automated state-of-the-art methods, also maintains a feasible search cost at the same time."],"url":"http://arxiv.org/abs/2403.04086v1"}
{"created":"2024-03-06 22:30:04","title":"Don't Blame the Data, Blame the Model: Understanding Noise and Bias When Learning from Subjective Annotations","abstract":"Researchers have raised awareness about the harms of aggregating labels especially in subjective tasks that naturally contain disagreements among human annotators. In this work we show that models that are only provided aggregated labels show low confidence on high-disagreement data instances. While previous studies consider such instances as mislabeled, we argue that the reason the high-disagreement text instances have been hard-to-learn is that the conventional aggregated models underperform in extracting useful signals from subjective tasks. Inspired by recent studies demonstrating the effectiveness of learning from raw annotations, we investigate classifying using Multiple Ground Truth (Multi-GT) approaches. Our experiments show an improvement of confidence for the high-disagreement instances.","sentences":["Researchers have raised awareness about the harms of aggregating labels especially in subjective tasks that naturally contain disagreements among human annotators.","In this work we show that models that are only provided aggregated labels show low confidence on high-disagreement data instances.","While previous studies consider such instances as mislabeled, we argue that the reason the high-disagreement text instances have been hard-to-learn is that the conventional aggregated models underperform in extracting useful signals from subjective tasks.","Inspired by recent studies demonstrating the effectiveness of learning from raw annotations, we investigate classifying using Multiple Ground Truth (Multi-GT) approaches.","Our experiments show an improvement of confidence for the high-disagreement instances."],"url":"http://arxiv.org/abs/2403.04085v1"}
{"created":"2024-03-06 22:27:30","title":"Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference","abstract":"Given time series data, how can we answer questions like \"what will happen in the future?\" and \"how did we get here?\" These sorts of probabilistic inference questions are challenging when observations are high-dimensional. In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of contrastive learning to time series data. Prior work already shows that the representations learned by contrastive learning encode a probability ratio. By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian. Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-dimensional matrix. In one special case, inferring intermediate representations will be equivalent to interpolating between the learned representations. We validate our theory using numerical simulations on tasks up to 46-dimensions.","sentences":["Given time series data, how can we answer questions like \"what will happen in the future?\"","and \"how did we get here?\"","These sorts of probabilistic inference questions are challenging when observations are high-dimensional.","In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations.","The key idea is to apply a variant of contrastive learning to time series data.","Prior work already shows that the representations learned by contrastive learning encode a probability ratio.","By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian.","Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-dimensional matrix.","In one special case, inferring intermediate representations will be equivalent to interpolating between the learned representations.","We validate our theory using numerical simulations on tasks up to 46-dimensions."],"url":"http://arxiv.org/abs/2403.04082v1"}
{"created":"2024-03-06 22:06:23","title":"Semi-Supervised Dialogue Abstractive Summarization via High-Quality Pseudolabel Selection","abstract":"Semi-supervised dialogue summarization (SSDS) leverages model-generated summaries to reduce reliance on human-labeled data and improve the performance of summarization models. While addressing label noise, previous works on semi-supervised learning primarily focus on natural language understanding tasks, assuming each sample has a unique label. However, these methods are not directly applicable to SSDS, as it is a generative task, and each dialogue can be summarized in different ways. In this work, we propose a novel scoring approach, SiCF, which encapsulates three primary dimensions of summarization model quality: Semantic invariance (indicative of model confidence), Coverage (factual recall), and Faithfulness (factual precision). Using the SiCF score, we select unlabeled dialogues with high-quality generated summaries to train summarization models. Comprehensive experiments on three public datasets demonstrate the effectiveness of SiCF scores in uncertainty estimation and semi-supervised learning for dialogue summarization tasks. Our code is available at \\url{https://github.com/amazon-science/summarization-sicf-score}.","sentences":["Semi-supervised dialogue summarization (SSDS) leverages model-generated summaries to reduce reliance on human-labeled data and improve the performance of summarization models.","While addressing label noise, previous works on semi-supervised learning primarily focus on natural language understanding tasks, assuming each sample has a unique label.","However, these methods are not directly applicable to SSDS, as it is a generative task, and each dialogue can be summarized in different ways.","In this work, we propose a novel scoring approach, SiCF, which encapsulates three primary dimensions of summarization model quality: Semantic invariance (indicative of model confidence), Coverage (factual recall), and Faithfulness (factual precision).","Using the SiCF score, we select unlabeled dialogues with high-quality generated summaries to train summarization models.","Comprehensive experiments on three public datasets demonstrate the effectiveness of SiCF scores in uncertainty estimation and semi-supervised learning for dialogue summarization tasks.","Our code is available at \\url{https://github.com/amazon-science/summarization-sicf-score}."],"url":"http://arxiv.org/abs/2403.04073v1"}
{"created":"2024-03-06 22:06:21","title":"Forecasting and Mitigating Disruptions in Public Bus Transit Services","abstract":"Public transportation systems often suffer from unexpected fluctuations in demand and disruptions, such as mechanical failures and medical emergencies. These fluctuations and disruptions lead to delays and overcrowding, which are detrimental to the passengers' experience and to the overall performance of the transit service. To proactively mitigate such events, many transit agencies station substitute (reserve) vehicles throughout their service areas, which they can dispatch to augment or replace vehicles on routes that suffer overcrowding or disruption. However, determining the optimal locations where substitute vehicles should be stationed is a challenging problem due to the inherent randomness of disruptions and due to the combinatorial nature of selecting locations across a city. In collaboration with the transit agency of Nashville, TN, we address this problem by introducing data-driven statistical and machine-learning models for forecasting disruptions and an effective randomized local-search algorithm for selecting locations where substitute vehicles are to be stationed. Our research demonstrates promising results in proactive disruption management, offering a practical and easily implementable solution for transit agencies to enhance the reliability of their services. Our results resonate beyond mere operational efficiency: by advancing proactive strategies, our approach fosters more resilient and accessible public transportation, contributing to equitable urban mobility and ultimately benefiting the communities that rely on public transportation the most.","sentences":["Public transportation systems often suffer from unexpected fluctuations in demand and disruptions, such as mechanical failures and medical emergencies.","These fluctuations and disruptions lead to delays and overcrowding, which are detrimental to the passengers' experience and to the overall performance of the transit service.","To proactively mitigate such events, many transit agencies station substitute (reserve) vehicles throughout their service areas, which they can dispatch to augment or replace vehicles on routes that suffer overcrowding or disruption.","However, determining the optimal locations where substitute vehicles should be stationed is a challenging problem due to the inherent randomness of disruptions and due to the combinatorial nature of selecting locations across a city.","In collaboration with the transit agency of Nashville, TN, we address this problem by introducing data-driven statistical and machine-learning models for forecasting disruptions and an effective randomized local-search algorithm for selecting locations where substitute vehicles are to be stationed.","Our research demonstrates promising results in proactive disruption management, offering a practical and easily implementable solution for transit agencies to enhance the reliability of their services.","Our results resonate beyond mere operational efficiency: by advancing proactive strategies, our approach fosters more resilient and accessible public transportation, contributing to equitable urban mobility and ultimately benefiting the communities that rely on public transportation the most."],"url":"http://arxiv.org/abs/2403.04072v1"}
{"created":"2024-03-06 22:04:14","title":"On-device Self-supervised Learning of Visual Perception Tasks aboard Hardware-limited Nano-quadrotors","abstract":"Sub-\\SI{50}{\\gram} nano-drones are gaining momentum in both academia and industry. Their most compelling applications rely on onboard deep learning models for perception despite severe hardware constraints (\\ie sub-\\SI{100}{\\milli\\watt} processor). When deployed in unknown environments not represented in the training data, these models often underperform due to domain shift. To cope with this fundamental problem, we propose, for the first time, on-device learning aboard nano-drones, where the first part of the in-field mission is dedicated to self-supervised fine-tuning of a pre-trained convolutional neural network (CNN). Leveraging a real-world vision-based regression task, we thoroughly explore performance-cost trade-offs of the fine-tuning phase along three axes: \\textit{i}) dataset size (more data increases the regression performance but requires more memory and longer computation); \\textit{ii}) methodologies (\\eg fine-tuning all model parameters vs. only a subset); and \\textit{iii}) self-supervision strategy. Our approach demonstrates an improvement in mean absolute error up to 30\\% compared to the pre-trained baseline, requiring only \\SI{22}{\\second} fine-tuning on an ultra-low-power GWT GAP9 System-on-Chip. Addressing the domain shift problem via on-device learning aboard nano-drones not only marks a novel result for hardware-limited robots but lays the ground for more general advancements for the entire robotics community.","sentences":["Sub-\\SI{50}{\\gram} nano-drones are gaining momentum in both academia and industry.","Their most compelling applications rely on onboard deep learning models for perception despite severe hardware constraints (\\ie sub-\\SI{100}{\\milli\\watt} processor).","When deployed in unknown environments not represented in the training data, these models often underperform due to domain shift.","To cope with this fundamental problem, we propose, for the first time, on-device learning aboard nano-drones, where the first part of the in-field mission is dedicated to self-supervised fine-tuning of a pre-trained convolutional neural network (CNN).","Leveraging a real-world vision-based regression task, we thoroughly explore performance-cost trade-offs of the fine-tuning phase along three axes: \\textit{i}) dataset size (more data increases the regression performance but requires more memory and longer computation); \\textit{ii}) methodologies (\\eg fine-tuning all model parameters vs. only a subset); and \\textit{iii}) self-supervision strategy.","Our approach demonstrates an improvement in mean absolute error up to 30\\% compared to the pre-trained baseline, requiring only \\SI{22}{\\second} fine-tuning on an ultra-low-power GWT GAP9 System-on-Chip.","Addressing the domain shift problem via on-device learning aboard nano-drones not only marks a novel result for hardware-limited robots but lays the ground for more general advancements for the entire robotics community."],"url":"http://arxiv.org/abs/2403.04071v1"}
{"created":"2024-03-06 21:30:44","title":"Assigning Entities to Teams as a Hypergraph Discovery Problem","abstract":"We propose a team assignment algorithm based on a hypergraph approach focusing on resilience and diffusion optimization. Specifically, our method is based on optimizing the algebraic connectivity of the Laplacian matrix of an edge-dependent vertex-weighted hypergraph. We used constrained simulated annealing, where we constrained the effort agents can exert to perform a task and the minimum effort a task requires to be completed. We evaluated our methods in terms of the number of unsuccessful patches to drive our solution into the feasible region and the cost of patching. We showed that our formulation provides more robust solutions than the original data and the greedy approach. We hope that our methods motivate further research in applying hypergraphs to similar problems in different research areas and in exploring variations of our methods.","sentences":["We propose a team assignment algorithm based on a hypergraph approach focusing on resilience and diffusion optimization.","Specifically, our method is based on optimizing the algebraic connectivity of the Laplacian matrix of an edge-dependent vertex-weighted hypergraph.","We used constrained simulated annealing, where we constrained the effort agents can exert to perform a task and the minimum effort a task requires to be completed.","We evaluated our methods in terms of the number of unsuccessful patches to drive our solution into the feasible region and the cost of patching.","We showed that our formulation provides more robust solutions than the original data and the greedy approach.","We hope that our methods motivate further research in applying hypergraphs to similar problems in different research areas and in exploring variations of our methods."],"url":"http://arxiv.org/abs/2403.04063v1"}
{"created":"2024-03-06 20:52:49","title":"Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations","abstract":"Reinforcement learning (RL) has achieved phenomenal success in various domains. However, its data-driven nature also introduces new vulnerabilities that can be exploited by malicious opponents. Recent work shows that a well-trained RL agent can be easily manipulated by strategically perturbing its state observations at the test stage. Existing solutions either introduce a regularization term to improve the smoothness of the trained policy against perturbations or alternatively train the agent's policy and the attacker's policy. However, the former does not provide sufficient protection against strong attacks, while the latter is computationally prohibitive for large environments. In this work, we propose a new robust RL algorithm for deriving a pessimistic policy to safeguard against an agent's uncertainty about true states. This approach is further enhanced with belief state inference and diffusion-based state purification to reduce uncertainty. Empirical results show that our approach obtains superb performance under strong attacks and has a comparable training overhead with regularization-based methods. Our code is available at https://github.com/SliencerX/Belief-enriched-robust-Q-learning.","sentences":["Reinforcement learning (RL) has achieved phenomenal success in various domains.","However, its data-driven nature also introduces new vulnerabilities that can be exploited by malicious opponents.","Recent work shows that a well-trained RL agent can be easily manipulated by strategically perturbing its state observations at the test stage.","Existing solutions either introduce a regularization term to improve the smoothness of the trained policy against perturbations or alternatively train the agent's policy and the attacker's policy.","However, the former does not provide sufficient protection against strong attacks, while the latter is computationally prohibitive for large environments.","In this work, we propose a new robust RL algorithm for deriving a pessimistic policy to safeguard against an agent's uncertainty about true states.","This approach is further enhanced with belief state inference and diffusion-based state purification to reduce uncertainty.","Empirical results show that our approach obtains superb performance under strong attacks and has a comparable training overhead with regularization-based methods.","Our code is available at https://github.com/SliencerX/Belief-enriched-robust-Q-learning."],"url":"http://arxiv.org/abs/2403.04050v1"}
{"created":"2024-03-06 20:37:29","title":"Sample size planning for conditional counterfactual mean estimation with a K-armed randomized experiment","abstract":"We cover how to determine a sufficiently large sample size for a $K$-armed randomized experiment in order to estimate conditional counterfactual expectations in data-driven subgroups. The sub-groups can be output by any feature space partitioning algorithm, including as defined by binning users having similar predictive scores or as defined by a learned policy tree. After carefully specifying the inference target, a minimum confidence level, and a maximum margin of error, the key is to turn the original goal into a simultaneous inference problem where the recommended sample size to offset an increased possibility of estimation error is directly related to the number of inferences to be conducted. Given a fixed sample size budget, our result allows us to invert the question to one about the feasible number of treatment arms or partition complexity (e.g. number of decision tree leaves). Using policy trees to learn sub-groups, we evaluate our nominal guarantees on a large publicly-available randomized experiment test data set.","sentences":["We cover how to determine a sufficiently large sample size for a $K$-armed randomized experiment in order to estimate conditional counterfactual expectations in data-driven subgroups.","The sub-groups can be output by any feature space partitioning algorithm, including as defined by binning users having similar predictive scores or as defined by a learned policy tree.","After carefully specifying the inference target, a minimum confidence level, and a maximum margin of error, the key is to turn the original goal into a simultaneous inference problem where the recommended sample size to offset an increased possibility of estimation error is directly related to the number of inferences to be conducted.","Given a fixed sample size budget, our result allows us to invert the question to one about the feasible number of treatment arms or partition complexity (e.g. number of decision tree leaves).","Using policy trees to learn sub-groups, we evaluate our nominal guarantees on a large publicly-available randomized experiment test data set."],"url":"http://arxiv.org/abs/2403.04039v1"}
{"created":"2024-03-06 20:34:08","title":"OCD-FL: A Novel Communication-Efficient Peer Selection-based Decentralized Federated Learning","abstract":"The conjunction of edge intelligence and the ever-growing Internet-of-Things (IoT) network heralds a new era of collaborative machine learning, with federated learning (FL) emerging as the most prominent paradigm. With the growing interest in these learning schemes, researchers started addressing some of their most fundamental limitations. Indeed, conventional FL with a central aggregator presents a single point of failure and a network bottleneck. To bypass this issue, decentralized FL where nodes collaborate in a peer-to-peer network has been proposed. Despite the latter's efficiency, communication costs and data heterogeneity remain key challenges in decentralized FL. In this context, we propose a novel scheme, called opportunistic communication-efficient decentralized federated learning, a.k.a., OCD-FL, consisting of a systematic FL peer selection for collaboration, aiming to achieve maximum FL knowledge gain while reducing energy consumption. Experimental results demonstrate the capability of OCD-FL to achieve similar or better performances than the fully collaborative FL, while significantly reducing consumed energy by at least 30% and up to 80%.","sentences":["The conjunction of edge intelligence and the ever-growing Internet-of-Things (IoT) network heralds a new era of collaborative machine learning, with federated learning (FL) emerging as the most prominent paradigm.","With the growing interest in these learning schemes, researchers started addressing some of their most fundamental limitations.","Indeed, conventional FL with a central aggregator presents a single point of failure and a network bottleneck.","To bypass this issue, decentralized FL where nodes collaborate in a peer-to-peer network has been proposed.","Despite the latter's efficiency, communication costs and data heterogeneity remain key challenges in decentralized FL.","In this context, we propose a novel scheme, called opportunistic communication-efficient decentralized federated learning, a.k.a., OCD-FL, consisting of a systematic FL peer selection for collaboration, aiming to achieve maximum FL knowledge gain while reducing energy consumption.","Experimental results demonstrate the capability of OCD-FL to achieve similar or better performances than the fully collaborative FL, while significantly reducing consumed energy by at least 30% and up to 80%."],"url":"http://arxiv.org/abs/2403.04037v1"}
{"created":"2024-03-06 20:33:55","title":"Unsupervised Contrastive Learning for Robust RF Device Fingerprinting Under Time-Domain Shift","abstract":"Radio Frequency (RF) device fingerprinting has been recognized as a potential technology for enabling automated wireless device identification and classification. However, it faces a key challenge due to the domain shift that could arise from variations in the channel conditions and environmental settings, potentially degrading the accuracy of RF-based device classification when testing and training data is collected in different domains. This paper introduces a novel solution that leverages contrastive learning to mitigate this domain shift problem. Contrastive learning, a state-of-the-art self-supervised learning approach from deep learning, learns a distance metric such that positive pairs are closer (i.e. more similar) in the learned metric space than negative pairs. When applied to RF fingerprinting, our model treats RF signals from the same transmission as positive pairs and those from different transmissions as negative pairs. Through experiments on wireless and wired RF datasets collected over several days, we demonstrate that our contrastive learning approach captures domain-invariant features, diminishing the effects of domain-specific variations. Our results show large and consistent improvements in accuracy (10.8\\% to 27.8\\%) over baseline models, thus underscoring the effectiveness of contrastive learning in improving device classification under domain shift.","sentences":["Radio Frequency (RF) device fingerprinting has been recognized as a potential technology for enabling automated wireless device identification and classification.","However, it faces a key challenge due to the domain shift that could arise from variations in the channel conditions and environmental settings, potentially degrading the accuracy of RF-based device classification when testing and training data is collected in different domains.","This paper introduces a novel solution that leverages contrastive learning to mitigate this domain shift problem.","Contrastive learning, a state-of-the-art self-supervised learning approach from deep learning, learns a distance metric such that positive pairs are closer (i.e. more similar) in the learned metric space than negative pairs.","When applied to RF fingerprinting, our model treats RF signals from the same transmission as positive pairs and those from different transmissions as negative pairs.","Through experiments on wireless and wired RF datasets collected over several days, we demonstrate that our contrastive learning approach captures domain-invariant features, diminishing the effects of domain-specific variations.","Our results show large and consistent improvements in accuracy (10.8\\% to 27.8\\%) over baseline models, thus underscoring the effectiveness of contrastive learning in improving device classification under domain shift."],"url":"http://arxiv.org/abs/2403.04036v1"}
{"created":"2024-03-06 20:22:08","title":"Can Large Language Models do Analytical Reasoning?","abstract":"This paper explores the cutting-edge Large Language Model with analytical reasoning on sports. Our analytical reasoning embodies the tasks of letting large language models count how many points each team scores in a quarter in the NBA and NFL games. Our major discoveries are in two folds. Firstly, we find among all the models we employed, GPT-4 stands out in effectiveness, followed by Claude-2.1, with GPT-3.5, Gemini-Pro, and Llama-2-70b lagging behind. Specifically, we compare three different prompting techniques and a divide-and-conquer approach, we find that the latter was the most effective. Our divide-and-conquer approach breaks down play-by-play data into smaller, more manageable segments, solves each piece individually, and then aggregates them together. Besides the divide-and-conquer approach, we also explore the Chain of Thought (CoT) strategy, which markedly improves outcomes for certain models, notably GPT-4 and Claude-2.1, with their accuracy rates increasing significantly. However, the CoT strategy has negligible or even detrimental effects on the performance of other models like GPT-3.5 and Gemini-Pro. Secondly, to our surprise, we observe that most models, including GPT-4, struggle to accurately count the total scores for NBA quarters despite showing strong performance in counting NFL quarter scores. This leads us to further investigate the factors that impact the complexity of analytical reasoning tasks with extensive experiments, through which we conclude that task complexity depends on the length of context, the information density, and the presence of related information. Our research provides valuable insights into the complexity of analytical reasoning tasks and potential directions for developing future large language models.","sentences":["This paper explores the cutting-edge Large Language Model with analytical reasoning on sports.","Our analytical reasoning embodies the tasks of letting large language models count how many points each team scores in a quarter in the NBA and NFL games.","Our major discoveries are in two folds.","Firstly, we find among all the models we employed, GPT-4 stands out in effectiveness, followed by Claude-2.1, with GPT-3.5, Gemini-Pro, and Llama-2-70b lagging behind.","Specifically, we compare three different prompting techniques and a divide-and-conquer approach, we find that the latter was the most effective.","Our divide-and-conquer approach breaks down play-by-play data into smaller, more manageable segments, solves each piece individually, and then aggregates them together.","Besides the divide-and-conquer approach, we also explore the Chain of Thought (CoT) strategy, which markedly improves outcomes for certain models, notably GPT-4 and Claude-2.1, with their accuracy rates increasing significantly.","However, the CoT strategy has negligible or even detrimental effects on the performance of other models like GPT-3.5 and Gemini-Pro.","Secondly, to our surprise, we observe that most models, including GPT-4, struggle to accurately count the total scores for NBA quarters despite showing strong performance in counting NFL quarter scores.","This leads us to further investigate the factors that impact the complexity of analytical reasoning tasks with extensive experiments, through which we conclude that task complexity depends on the length of context, the information density, and the presence of related information.","Our research provides valuable insights into the complexity of analytical reasoning tasks and potential directions for developing future large language models."],"url":"http://arxiv.org/abs/2403.04031v1"}
{"created":"2024-03-06 20:15:25","title":"Spanning Tree-based Query Plan Enumeration","abstract":"In this work, we define the problem of finding an optimal query plan as finding spanning trees with low costs. This approach empowers the utilization of a series of spanning tree algorithms, thereby enabling systematic exploration of the plan search space over a join graph. Capitalizing on the polynomial time complexity of spanning tree algorithms, we present the Ensemble Spanning Tree Enumeration (ESTE) strategy. ESTE employs two conventional spanning tree algorithms, Prim's and Kruskal's, together to enhance the robustness of the query optimizer. In ESTE, multiple query plans are enumerated exploring different areas of the search space. This positions ESTE as an intermediate strategy between exhaustive and heuristic enumeration strategies. We show that ESTE is more robust in identifying efficient query plans for large queries. In the case of data modifications and workload demand increase, we believe that our approach can be a cheaper alternative to maintain optimizer robustness by integrating additional spanning tree algorithms rather than completely changing the optimizer to another plan enumeration algorithm. The experimental evaluation shows that ESTE achieves better consistency in plan quality and optimization time than existing solutions while identifying similarly optimal plans.","sentences":["In this work, we define the problem of finding an optimal query plan as finding spanning trees with low costs.","This approach empowers the utilization of a series of spanning tree algorithms, thereby enabling systematic exploration of the plan search space over a join graph.","Capitalizing on the polynomial time complexity of spanning tree algorithms, we present the Ensemble Spanning Tree Enumeration (ESTE) strategy.","ESTE employs two conventional spanning tree algorithms, Prim's and Kruskal's, together to enhance the robustness of the query optimizer.","In ESTE, multiple query plans are enumerated exploring different areas of the search space.","This positions ESTE as an intermediate strategy between exhaustive and heuristic enumeration strategies.","We show that ESTE is more robust in identifying efficient query plans for large queries.","In the case of data modifications and workload demand increase, we believe that our approach can be a cheaper alternative to maintain optimizer robustness by integrating additional spanning tree algorithms rather than completely changing the optimizer to another plan enumeration algorithm.","The experimental evaluation shows that ESTE achieves better consistency in plan quality and optimization time than existing solutions while identifying similarly optimal plans."],"url":"http://arxiv.org/abs/2403.04026v1"}
{"created":"2024-03-06 19:58:19","title":"Knockoff-Guided Feature Selection via A Single Pre-trained Reinforced Agent","abstract":"Feature selection prepares the AI-readiness of data by eliminating redundant features. Prior research falls into two primary categories: i) Supervised Feature Selection, which identifies the optimal feature subset based on their relevance to the target variable; ii) Unsupervised Feature Selection, which reduces the feature space dimensionality by capturing the essential information within the feature set instead of using target variable. However, SFS approaches suffer from time-consuming processes and limited generalizability due to the dependence on the target variable and downstream ML tasks. UFS methods are constrained by the deducted feature space is latent and untraceable. To address these challenges, we introduce an innovative framework for feature selection, which is guided by knockoff features and optimized through reinforcement learning, to identify the optimal and effective feature subset. In detail, our method involves generating \"knockoff\" features that replicate the distribution and characteristics of the original features but are independent of the target variable. Each feature is then assigned a pseudo label based on its correlation with all the knockoff features, serving as a novel metric for feature evaluation. Our approach utilizes these pseudo labels to guide the feature selection process in 3 novel ways, optimized by a single reinforced agent: 1). A deep Q-network, pre-trained with the original features and their corresponding pseudo labels, is employed to improve the efficacy of the exploration process in feature selection. 2). We introduce unsupervised rewards to evaluate the feature subset quality based on the pseudo labels and the feature space reconstruction loss to reduce dependencies on the target variable. 3). A new {\\epsilon}-greedy strategy is used, incorporating insights from the pseudo labels to make the feature selection process more effective.","sentences":["Feature selection prepares the AI-readiness of data by eliminating redundant features.","Prior research falls into two primary categories: i) Supervised Feature Selection, which identifies the optimal feature subset based on their relevance to the target variable; ii) Unsupervised Feature Selection, which reduces the feature space dimensionality by capturing the essential information within the feature set instead of using target variable.","However, SFS approaches suffer from time-consuming processes and limited generalizability due to the dependence on the target variable and downstream ML tasks.","UFS methods are constrained by the deducted feature space is latent and untraceable.","To address these challenges, we introduce an innovative framework for feature selection, which is guided by knockoff features and optimized through reinforcement learning, to identify the optimal and effective feature subset.","In detail, our method involves generating \"knockoff\" features that replicate the distribution and characteristics of the original features but are independent of the target variable.","Each feature is then assigned a pseudo label based on its correlation with all the knockoff features, serving as a novel metric for feature evaluation.","Our approach utilizes these pseudo labels to guide the feature selection process in 3 novel ways, optimized by a single reinforced agent: 1).","A deep Q-network, pre-trained with the original features and their corresponding pseudo labels, is employed to improve the efficacy of the exploration process in feature selection.","2).","We introduce unsupervised rewards to evaluate the feature subset quality based on the pseudo labels and the feature space reconstruction loss to reduce dependencies on the target variable.","3).","A new {\\epsilon}-greedy strategy is used, incorporating insights from the pseudo labels to make the feature selection process more effective."],"url":"http://arxiv.org/abs/2403.04015v1"}
{"created":"2024-03-06 19:46:44","title":"Temporal Cross-Attention for Dynamic Embedding and Tokenization of Multimodal Electronic Health Records","abstract":"The breadth, scale, and temporal granularity of modern electronic health records (EHR) systems offers great potential for estimating personalized and contextual patient health trajectories using sequential deep learning. However, learning useful representations of EHR data is challenging due to its high dimensionality, sparsity, multimodality, irregular and variable-specific recording frequency, and timestamp duplication when multiple measurements are recorded simultaneously. Although recent efforts to fuse structured EHR and unstructured clinical notes suggest the potential for more accurate prediction of clinical outcomes, less focus has been placed on EHR embedding approaches that directly address temporal EHR challenges by learning time-aware representations from multimodal patient time series. In this paper, we introduce a dynamic embedding and tokenization framework for precise representation of multimodal clinical time series that combines novel methods for encoding time and sequential position with temporal cross-attention. Our embedding and tokenization framework, when integrated into a multitask transformer classifier with sliding window attention, outperformed baseline approaches on the exemplar task of predicting the occurrence of nine postoperative complications of more than 120,000 major inpatient surgeries using multimodal data from three hospitals and two academic health centers in the United States.","sentences":["The breadth, scale, and temporal granularity of modern electronic health records (EHR) systems offers great potential for estimating personalized and contextual patient health trajectories using sequential deep learning.","However, learning useful representations of EHR data is challenging due to its high dimensionality, sparsity, multimodality, irregular and variable-specific recording frequency, and timestamp duplication when multiple measurements are recorded simultaneously.","Although recent efforts to fuse structured EHR and unstructured clinical notes suggest the potential for more accurate prediction of clinical outcomes, less focus has been placed on EHR embedding approaches that directly address temporal EHR challenges by learning time-aware representations from multimodal patient time series.","In this paper, we introduce a dynamic embedding and tokenization framework for precise representation of multimodal clinical time series that combines novel methods for encoding time and sequential position with temporal cross-attention.","Our embedding and tokenization framework, when integrated into a multitask transformer classifier with sliding window attention, outperformed baseline approaches on the exemplar task of predicting the occurrence of nine postoperative complications of more than 120,000 major inpatient surgeries using multimodal data from three hospitals and two academic health centers in the United States."],"url":"http://arxiv.org/abs/2403.04012v1"}
{"created":"2024-03-06 19:08:28","title":"Personalized Negative Reservoir for Incremental Learning in Recommender Systems","abstract":"Recommender systems have become an integral part of online platforms. Every day the volume of training data is expanding and the number of user interactions is constantly increasing. The exploration of larger and more expressive models has become a necessary pursuit to improve user experience. However, this progression carries with it an increased computational burden. In commercial settings, once a recommendation system model has been trained and deployed it typically needs to be updated frequently as new client data arrive. Cumulatively, the mounting volume of data is guaranteed to eventually make full batch retraining of the model from scratch computationally infeasible. Naively fine-tuning solely on the new data runs into the well-documented problem of catastrophic forgetting. Despite the fact that negative sampling is a crucial part of training with implicit feedback, no specialized technique exists that is tailored to the incremental learning framework. In this work, we take the first step to propose, a personalized negative reservoir strategy which is used to obtain negative samples for the standard triplet loss. This technique balances alleviation of forgetting with plasticity by encouraging the model to remember stable user preferences and selectively forget when user interests change. We derive the mathematical formulation of a negative sampler to populate and update the reservoir. We integrate our design in three SOTA and commonly used incremental recommendation models. We show that these concrete realizations of our negative reservoir framework achieve state-of-the-art results in standard benchmarks, on multiple standard top-k evaluation metrics.","sentences":["Recommender systems have become an integral part of online platforms.","Every day the volume of training data is expanding and the number of user interactions is constantly increasing.","The exploration of larger and more expressive models has become a necessary pursuit to improve user experience.","However, this progression carries with it an increased computational burden.","In commercial settings, once a recommendation system model has been trained and deployed it typically needs to be updated frequently as new client data arrive.","Cumulatively, the mounting volume of data is guaranteed to eventually make full batch retraining of the model from scratch computationally infeasible.","Naively fine-tuning solely on the new data runs into the well-documented problem of catastrophic forgetting.","Despite the fact that negative sampling is a crucial part of training with implicit feedback, no specialized technique exists that is tailored to the incremental learning framework.","In this work, we take the first step to propose, a personalized negative reservoir strategy which is used to obtain negative samples for the standard triplet loss.","This technique balances alleviation of forgetting with plasticity by encouraging the model to remember stable user preferences and selectively forget when user interests change.","We derive the mathematical formulation of a negative sampler to populate and update the reservoir.","We integrate our design in three SOTA and commonly used incremental recommendation models.","We show that these concrete realizations of our negative reservoir framework achieve state-of-the-art results in standard benchmarks, on multiple standard top-k evaluation metrics."],"url":"http://arxiv.org/abs/2403.03993v1"}
{"created":"2024-03-06 19:01:04","title":"A Sierpinski Triangle Data Structure for Efficient Array Value Update and Prefix Sum Calculation","abstract":"The binary indexed tree, or Fenwick tree, is a data structure that can efficiently update values and calculate prefix sums in an array. It allows both of these operations to be performed in $O(\\log_2 N)$ time. Here we present a novel data structure resembling the Sierpinski triangle, which accomplishes these operations with the same memory usage in $O(\\log_3 N)$ time instead. We show this order to be optimal by making use of a connection to quantum computing.","sentences":["The binary indexed tree, or Fenwick tree, is a data structure that can efficiently update values and calculate prefix sums in an array.","It allows both of these operations to be performed in $O(\\log_2","N)$ time.","Here we present a novel data structure resembling the Sierpinski triangle, which accomplishes these operations with the same memory usage in $O(\\log_3 N)$ time instead.","We show this order to be optimal by making use of a connection to quantum computing."],"url":"http://arxiv.org/abs/2403.03990v1"}
