{"created":"2024-08-05 17:59:51","title":"Latent-INR: A Flexible Framework for Implicit Representations of Videos with Discriminative Semantics","abstract":"Implicit Neural Networks (INRs) have emerged as powerful representations to encode all forms of data, including images, videos, audios, and scenes. With video, many INRs for video have been proposed for the compression task, and recent methods feature significant improvements with respect to encoding time, storage, and reconstruction quality. However, these encoded representations lack semantic meaning, so they cannot be used for any downstream tasks that require such properties, such as retrieval. This can act as a barrier for adoption of video INRs over traditional codecs as they do not offer any significant edge apart from compression. To alleviate this, we propose a flexible framework that decouples the spatial and temporal aspects of the video INR. We accomplish this with a dictionary of per-frame latents that are learned jointly with a set of video specific hypernetworks, such that given a latent, these hypernetworks can predict the INR weights to reconstruct the given frame. This framework not only retains the compression efficiency, but the learned latents can be aligned with features from large vision models, which grants them discriminative properties. We align these latents with CLIP and show good performance for both compression and video retrieval tasks. By aligning with VideoLlama, we are able to perform open-ended chat with our learned latents as the visual inputs. Additionally, the learned latents serve as a proxy for the underlying weights, allowing us perform tasks like video interpolation. These semantic properties and applications, existing simultaneously with ability to perform compression, interpolation, and superresolution properties, are a first in this field of work.","sentences":["Implicit Neural Networks (INRs) have emerged as powerful representations to encode all forms of data, including images, videos, audios, and scenes.","With video, many INRs for video have been proposed for the compression task, and recent methods feature significant improvements with respect to encoding time, storage, and reconstruction quality.","However, these encoded representations lack semantic meaning, so they cannot be used for any downstream tasks that require such properties, such as retrieval.","This can act as a barrier for adoption of video INRs over traditional codecs as they do not offer any significant edge apart from compression.","To alleviate this, we propose a flexible framework that decouples the spatial and temporal aspects of the video INR.","We accomplish this with a dictionary of per-frame latents that are learned jointly with a set of video specific hypernetworks, such that given a latent, these hypernetworks can predict the INR weights to reconstruct the given frame.","This framework not only retains the compression efficiency, but the learned latents can be aligned with features from large vision models, which grants them discriminative properties.","We align these latents with CLIP and show good performance for both compression and video retrieval tasks.","By aligning with VideoLlama, we are able to perform open-ended chat with our learned latents as the visual inputs.","Additionally, the learned latents serve as a proxy for the underlying weights, allowing us perform tasks like video interpolation.","These semantic properties and applications, existing simultaneously with ability to perform compression, interpolation, and superresolution properties, are a first in this field of work."],"url":"http://arxiv.org/abs/2408.02672v1"}
{"created":"2024-08-05 17:57:02","title":"Self-Taught Evaluators","abstract":"Model-based evaluation is at the heart of successful model development -- as a reward model for training, and as a replacement for human evaluation. To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve. In this work, we present an approach that aims to im-prove evaluators without human annotations, using synthetic training data only. Starting from unlabeled instructions, our iterative self-improvement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions. Without any labeled preference data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms commonly used LLM judges such as GPT-4 and matches the performance of the top-performing reward models trained with labeled examples.","sentences":["Model-based evaluation is at the heart of successful model development -- as a reward model for training, and as a replacement for human evaluation.","To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve.","In this work, we present an approach that aims to im-prove evaluators without human annotations, using synthetic training data only.","Starting from unlabeled instructions, our iterative self-improvement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions.","Without any labeled preference data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench.","This outperforms commonly used LLM judges such as GPT-4 and matches the performance of the top-performing reward models trained with labeled examples."],"url":"http://arxiv.org/abs/2408.02666v1"}
{"created":"2024-08-05 17:14:35","title":"Detection of Compromised Functions in a Serverless Cloud Environment","abstract":"Serverless computing is an emerging cloud paradigm with serverless functions at its core. While serverless environments enable software developers to focus on developing applications without the need to actively manage the underlying runtime infrastructure, they open the door to a wide variety of security threats that can be challenging to mitigate with existing methods. Existing security solutions do not apply to all serverless architectures, since they require significant modifications to the serverless infrastructure or rely on third-party services for the collection of more detailed data. In this paper, we present an extendable serverless security threat detection model that leverages cloud providers' native monitoring tools to detect anomalous behavior in serverless applications. Our model aims to detect compromised serverless functions by identifying post-exploitation abnormal behavior related to different types of attacks on serverless functions, and therefore, it is a last line of defense. Our approach is not tied to any specific serverless application, is agnostic to the type of threats, and is adaptable through model adjustments. To evaluate our model's performance, we developed a serverless cybersecurity testbed in an AWS cloud environment, which includes two different serverless applications and simulates a variety of attack scenarios that cover the main security threats faced by serverless functions. Our evaluation demonstrates our model's ability to detect all implemented attacks while maintaining a negligible false alarm rate.","sentences":["Serverless computing is an emerging cloud paradigm with serverless functions at its core.","While serverless environments enable software developers to focus on developing applications without the need to actively manage the underlying runtime infrastructure, they open the door to a wide variety of security threats that can be challenging to mitigate with existing methods.","Existing security solutions do not apply to all serverless architectures, since they require significant modifications to the serverless infrastructure or rely on third-party services for the collection of more detailed data.","In this paper, we present an extendable serverless security threat detection model that leverages cloud providers' native monitoring tools to detect anomalous behavior in serverless applications.","Our model aims to detect compromised serverless functions by identifying post-exploitation abnormal behavior related to different types of attacks on serverless functions, and therefore, it is a last line of defense.","Our approach is not tied to any specific serverless application, is agnostic to the type of threats, and is adaptable through model adjustments.","To evaluate our model's performance, we developed a serverless cybersecurity testbed in an AWS cloud environment, which includes two different serverless applications and simulates a variety of attack scenarios that cover the main security threats faced by serverless functions.","Our evaluation demonstrates our model's ability to detect all implemented attacks while maintaining a negligible false alarm rate."],"url":"http://arxiv.org/abs/2408.02641v1"}
{"created":"2024-08-05 16:58:56","title":"Interactive 3D Medical Image Segmentation with SAM 2","abstract":"Interactive medical image segmentation (IMIS) has shown significant potential in enhancing segmentation accuracy by integrating iterative feedback from medical professionals. However, the limited availability of enough 3D medical data restricts the generalization and robustness of most IMIS methods. The Segment Anything Model (SAM), though effective for 2D images, requires expensive semi-auto slice-by-slice annotations for 3D medical images. In this paper, we explore the zero-shot capabilities of SAM 2, the next-generation Meta SAM model trained on videos, for 3D medical image segmentation. By treating sequential 2D slices of 3D images as video frames, SAM 2 can fully automatically propagate annotations from a single frame to the entire 3D volume. We propose a practical pipeline for using SAM 2 in 3D medical image segmentation and present key findings highlighting its efficiency and potential for further optimization. Concretely, numerical experiments on the BraTS2020 and the medical segmentation decathlon datasets demonstrate that SAM 2 still has a gap with supervised methods but can narrow the gap in specific settings and organ types, significantly reducing the annotation burden on medical professionals. Our code will be open-sourced and available at https://github.com/Chuyun-Shen/SAM_2_Medical_3D.","sentences":["Interactive medical image segmentation (IMIS) has shown significant potential in enhancing segmentation accuracy by integrating iterative feedback from medical professionals.","However, the limited availability of enough 3D medical data restricts the generalization and robustness of most IMIS methods.","The Segment Anything Model (SAM), though effective for 2D images, requires expensive semi-auto slice-by-slice annotations for 3D medical images.","In this paper, we explore the zero-shot capabilities of SAM 2, the next-generation Meta SAM model trained on videos, for 3D medical image segmentation.","By treating sequential 2D slices of 3D images as video frames, SAM 2 can fully automatically propagate annotations from a single frame to the entire 3D volume.","We propose a practical pipeline for using SAM 2 in 3D medical image segmentation and present key findings highlighting its efficiency and potential for further optimization.","Concretely, numerical experiments on the BraTS2020 and the medical segmentation decathlon datasets demonstrate that SAM 2 still has a gap with supervised methods but can narrow the gap in specific settings and organ types, significantly reducing the annotation burden on medical professionals.","Our code will be open-sourced and available at https://github.com/Chuyun-Shen/SAM_2_Medical_3D."],"url":"http://arxiv.org/abs/2408.02635v1"}
{"created":"2024-08-05 16:55:06","title":"SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models","abstract":"As large language models (LLMs) continue to advance in capability and influence, ensuring their security and preventing harmful outputs has become crucial. A promising approach to address these concerns involves training models to automatically generate adversarial prompts for red teaming. However, the evolving subtlety of vulnerabilities in LLMs challenges the effectiveness of current adversarial methods, which struggle to specifically target and explore the weaknesses of these models. To tackle these challenges, we introduce the $\\mathbf{S}\\text{elf-}\\mathbf{E}\\text{volving }\\mathbf{A}\\text{dversarial }\\mathbf{S}\\text{afety }\\mathbf{(SEAS)}$ optimization framework, which enhances security by leveraging data generated by the model itself. SEAS operates through three iterative stages: Initialization, Attack, and Adversarial Optimization, refining both the Red Team and Target models to improve robustness and safety. This framework reduces reliance on manual testing and significantly enhances the security capabilities of LLMs. Our contributions include a novel adversarial framework, a comprehensive safety dataset, and after three iterations, the Target model achieves a security level comparable to GPT-4, while the Red Team model shows a marked increase in attack success rate (ASR) against advanced models.","sentences":["As large language models (LLMs) continue to advance in capability and influence, ensuring their security and preventing harmful outputs has become crucial.","A promising approach to address these concerns involves training models to automatically generate adversarial prompts for red teaming.","However, the evolving subtlety of vulnerabilities in LLMs challenges the effectiveness of current adversarial methods, which struggle to specifically target and explore the weaknesses of these models.","To tackle these challenges, we introduce the $\\mathbf{S}\\text{elf-}\\mathbf{E}\\text{volving }\\mathbf{A}\\text{dversarial }\\mathbf{S}\\text{afety }\\mathbf{(SEAS)}$ optimization framework, which enhances security by leveraging data generated by the model itself.","SEAS operates through three iterative stages: Initialization, Attack, and Adversarial Optimization, refining both the Red Team and Target models to improve robustness and safety.","This framework reduces reliance on manual testing and significantly enhances the security capabilities of LLMs.","Our contributions include a novel adversarial framework, a comprehensive safety dataset, and after three iterations, the Target model achieves a security level comparable to GPT-4, while the Red Team model shows a marked increase in attack success rate (ASR) against advanced models."],"url":"http://arxiv.org/abs/2408.02632v1"}
{"created":"2024-08-05 16:53:23","title":"VidGen-1M: A Large-Scale Dataset for Text-to-video Generation","abstract":"The quality of video-text pairs fundamentally determines the upper bound of text-to-video models. Currently, the datasets used for training these models suffer from significant shortcomings, including low temporal consistency, poor-quality captions, substandard video quality, and imbalanced data distribution. The prevailing video curation process, which depends on image models for tagging and manual rule-based curation, leads to a high computational load and leaves behind unclean data. As a result, there is a lack of appropriate training datasets for text-to-video models. To address this problem, we present VidGen-1M, a superior training dataset for text-to-video models. Produced through a coarse-to-fine curation strategy, this dataset guarantees high-quality videos and detailed captions with excellent temporal consistency. When used to train the video generation model, this dataset has led to experimental results that surpass those obtained with other models.","sentences":["The quality of video-text pairs fundamentally determines the upper bound of text-to-video models.","Currently, the datasets used for training these models suffer from significant shortcomings, including low temporal consistency, poor-quality captions, substandard video quality, and imbalanced data distribution.","The prevailing video curation process, which depends on image models for tagging and manual rule-based curation, leads to a high computational load and leaves behind unclean data.","As a result, there is a lack of appropriate training datasets for text-to-video models.","To address this problem, we present VidGen-1M, a superior training dataset for text-to-video models.","Produced through a coarse-to-fine curation strategy, this dataset guarantees high-quality videos and detailed captions with excellent temporal consistency.","When used to train the video generation model, this dataset has led to experimental results that surpass those obtained with other models."],"url":"http://arxiv.org/abs/2408.02629v1"}
{"created":"2024-08-05 16:39:04","title":"Comparison of Code Quality and Best Practices in IoT and non-IoT Software","abstract":"Context: IoT systems, networks of connected devices powered by software, require studying software quality for maintenance. Despite extensive studies on non-IoT software quality, research on IoT software quality is lacking. It is uncertain if IoT and non-IoT systems software are comparable, hindering the confident application of results and best practices gained on non-IoT systems.   Objective: Therefore, we compare the code quality of two equivalent sets of IoT and non-IoT systems to determine whether there are similarities and differences. We also collect and revisit software-engineering best practices in non-IoT contexts to apply them to IoT.   Method: We design and apply a systematic method to select two sets of 94 non-IoT and IoT systems software from GitHub with comparable characteristics. We compute quality metrics on the systems in these two sets and then analyse and compare the metric values. We analyse in depth and provide specific examples of IoT system's complexity and how it manifests in the codebases. After the comparison, We systematically select and present a list of best practices to address the observed difference between IoT and non-IoT code.   Results: Through a comparison of metrics, we conclude that software for IoT systems is more complex, coupled, larger, less maintainable, and cohesive than non-IoT systems. Several factors, such as integrating multiple hardware and software components and managing data communication between them, contribute to these differences. Considering these differences, we present a revisited best practices list with approaches, tools, or techniques for developing IoT systems. As example, applying modularity, and refactoring are best practices for lowering the complexity.   Conclusion: Based on our work, researchers can now make an informed decision using existing studies on the quality of non-IoT systems for IoT systems.","sentences":["Context: IoT systems, networks of connected devices powered by software, require studying software quality for maintenance.","Despite extensive studies on non-IoT software quality, research on IoT software quality is lacking.","It is uncertain if IoT and non-IoT systems software are comparable, hindering the confident application of results and best practices gained on non-IoT systems.   ","Objective:","Therefore, we compare the code quality of two equivalent sets of IoT and non-IoT systems to determine whether there are similarities and differences.","We also collect and revisit software-engineering best practices in non-IoT contexts to apply them to IoT.   Method: We design and apply a systematic method to select two sets of 94 non-IoT and IoT systems software from GitHub with comparable characteristics.","We compute quality metrics on the systems in these two sets and then analyse and compare the metric values.","We analyse in depth and provide specific examples of IoT system's complexity and how it manifests in the codebases.","After the comparison, We systematically select and present a list of best practices to address the observed difference between IoT and non-IoT code.   ","Results: Through a comparison of metrics, we conclude that software for IoT systems is more complex, coupled, larger, less maintainable, and cohesive than non-IoT systems.","Several factors, such as integrating multiple hardware and software components and managing data communication between them, contribute to these differences.","Considering these differences, we present a revisited best practices list with approaches, tools, or techniques for developing IoT systems.","As example, applying modularity, and refactoring are best practices for lowering the complexity.   ","Conclusion: Based on our work, researchers can now make an informed decision using existing studies on the quality of non-IoT systems for IoT systems."],"url":"http://arxiv.org/abs/2408.02614v1"}
{"created":"2024-08-05 16:31:28","title":"Trade-offs of Dynamic Control Structure in Human-swarm Systems","abstract":"Swarm robotics is a study of simple robots that exhibit complex behaviour only by interacting locally with other robots and their environment. The control in swarm robotics is mainly distributed whereas centralised control is widely used in other fields of robotics. Centralised and decentralised control strategies both pose a unique set of benefits and drawbacks for the control of multi-robot systems. While decentralised systems are more scalable and resilient, they are less efficient compared to the centralised systems and they lead to excessive data transmissions to the human operators causing cognitive overload. We examine the trade-offs of each of these approaches in a human-swarm system to perform an environmental monitoring task and propose a flexible hybrid approach, which combines elements of hierarchical and decentralised systems. We find that a flexible hybrid system can outperform a centralised system (in our environmental monitoring task by 19.2%) while reducing the number of messages sent to a human operator (here by 23.1%). We conclude that establishing centralisation for a system is not always optimal for performance and that utilising aspects of centralised and decentralised systems can keep the swarm from hindering its performance.","sentences":["Swarm robotics is a study of simple robots that exhibit complex behaviour only by interacting locally with other robots and their environment.","The control in swarm robotics is mainly distributed whereas centralised control is widely used in other fields of robotics.","Centralised and decentralised control strategies both pose a unique set of benefits and drawbacks for the control of multi-robot systems.","While decentralised systems are more scalable and resilient, they are less efficient compared to the centralised systems and they lead to excessive data transmissions to the human operators causing cognitive overload.","We examine the trade-offs of each of these approaches in a human-swarm system to perform an environmental monitoring task and propose a flexible hybrid approach, which combines elements of hierarchical and decentralised systems.","We find that a flexible hybrid system can outperform a centralised system (in our environmental monitoring task by 19.2%) while reducing the number of messages sent to a human operator (here by 23.1%).","We conclude that establishing centralisation for a system is not always optimal for performance and that utilising aspects of centralised and decentralised systems can keep the swarm from hindering its performance."],"url":"http://arxiv.org/abs/2408.02605v1"}
{"created":"2024-08-05 16:21:17","title":"Progressively Selective Label Enhancement for Language Model Alignment","abstract":"Large Language Models have demonstrated impressive capabilities in various language tasks but may produce content that misaligns with human expectations, raising ethical and legal concerns. Therefore, it is important to explore the limitations and implement restrictions on the models to ensure safety and compliance, with Reinforcement Learning from Human Feedback (RLHF) being the primary method. Due to challenges in stability and scalability with the RLHF stages, researchers are exploring alternative methods to achieve effects comparable to those of RLHF. However, these methods often depend on large high-quality datasets and inefficiently utilize generated data. To deal with this problem, we propose PSLE, i.e., Progressively Selective Label Enhancement for Language Model Alignment, a framework that fully utilizes all generated data by guiding the model with principles to align outputs with human expectations. Using a dynamically updated threshold, our approach ensures efficient data utilization by incorporating all generated responses and weighting them based on their corresponding reward scores. Experimental results on multiple datasets demonstrate the effectiveness of PSLE compared to existing language model alignment methods.","sentences":["Large Language Models have demonstrated impressive capabilities in various language tasks but may produce content that misaligns with human expectations, raising ethical and legal concerns.","Therefore, it is important to explore the limitations and implement restrictions on the models to ensure safety and compliance, with Reinforcement Learning from Human Feedback (RLHF) being the primary method.","Due to challenges in stability and scalability with the RLHF stages, researchers are exploring alternative methods to achieve effects comparable to those of RLHF.","However, these methods often depend on large high-quality datasets and inefficiently utilize generated data.","To deal with this problem, we propose PSLE, i.e., Progressively Selective Label Enhancement for Language Model Alignment, a framework that fully utilizes all generated data by guiding the model with principles to align outputs with human expectations.","Using a dynamically updated threshold, our approach ensures efficient data utilization by incorporating all generated responses and weighting them based on their corresponding reward scores.","Experimental results on multiple datasets demonstrate the effectiveness of PSLE compared to existing language model alignment methods."],"url":"http://arxiv.org/abs/2408.02599v1"}
{"created":"2024-08-05 16:07:31","title":"Modelling Visual Semantics via Image Captioning to extract Enhanced Multi-Level Cross-Modal Semantic Incongruity Representation with Attention for Multimodal Sarcasm Detection","abstract":"Sarcasm is a type of irony, characterized by an inherent mismatch between the literal interpretation and the intended connotation. Though sarcasm detection in text has been extensively studied, there are situations in which textual input alone might be insufficient to perceive sarcasm. The inclusion of additional contextual cues, such as images, is essential to recognize sarcasm in social media data effectively. This study presents a novel framework for multimodal sarcasm detection that can process input triplets. Two components of these triplets comprise the input text and its associated image, as provided in the datasets. Additionally, a supplementary modality is introduced in the form of descriptive image captions. The motivation behind incorporating this visual semantic representation is to more accurately capture the discrepancies between the textual and visual content, which are fundamental to the sarcasm detection task. The primary contributions of this study are: (1) a robust textual feature extraction branch that utilizes a cross-lingual language model; (2) a visual feature extraction branch that incorporates a self-regulated residual ConvNet integrated with a lightweight spatially aware attention module; (3) an additional modality in the form of image captions generated using an encoder-decoder architecture capable of reading text embedded in images; (4) distinct attention modules to effectively identify the incongruities between the text and two levels of image representations; (5) multi-level cross-domain semantic incongruity representation achieved through feature fusion. Compared with cutting-edge baselines, the proposed model achieves the best accuracy of 92.89% and 64.48%, respectively, on the Twitter multimodal sarcasm and MultiBully datasets.","sentences":["Sarcasm is a type of irony, characterized by an inherent mismatch between the literal interpretation and the intended connotation.","Though sarcasm detection in text has been extensively studied, there are situations in which textual input alone might be insufficient to perceive sarcasm.","The inclusion of additional contextual cues, such as images, is essential to recognize sarcasm in social media data effectively.","This study presents a novel framework for multimodal sarcasm detection that can process input triplets.","Two components of these triplets comprise the input text and its associated image, as provided in the datasets.","Additionally, a supplementary modality is introduced in the form of descriptive image captions.","The motivation behind incorporating this visual semantic representation is to more accurately capture the discrepancies between the textual and visual content, which are fundamental to the sarcasm detection task.","The primary contributions of this study are: (1) a robust textual feature extraction branch that utilizes a cross-lingual language model; (2) a visual feature extraction branch that incorporates a self-regulated residual ConvNet integrated with a lightweight spatially aware attention module; (3) an additional modality in the form of image captions generated using an encoder-decoder architecture capable of reading text embedded in images; (4) distinct attention modules to effectively identify the incongruities between the text and two levels of image representations; (5) multi-level cross-domain semantic incongruity representation achieved through feature fusion.","Compared with cutting-edge baselines, the proposed model achieves the best accuracy of 92.89% and 64.48%, respectively, on the Twitter multimodal sarcasm and MultiBully datasets."],"url":"http://arxiv.org/abs/2408.02595v1"}
{"created":"2024-08-05 16:00:36","title":"Massive MIMO-OTFS-Based Random Access for Cooperative LEO Satellite Constellations","abstract":"This paper investigates joint device identification, channel estimation, and symbol detection for cooperative multi-satellite-enhanced random access, where orthogonal time-frequency space modulation with the large antenna array is utilized to combat the dynamics of the terrestrial-satellite links (TSLs). We introduce the generalized complex exponential basis expansion model to parameterize TSLs, thereby reducing the pilot overhead. By exploiting the block sparsity of the TSLs in the angular domain, a message passing algorithm is designed for initial channel estimation. Subsequently, we examine two cooperative modes to leverage the spatial diversity within satellite constellations: the centralized mode, where computations are performed at a high-power central server, and the distributed mode, where computations are offloaded to edge satellites with minimal signaling overhead. Specifically, in the centralized mode, device identification is achieved by aggregating backhaul information from edge satellites, and channel estimation and symbol detection are jointly enhanced through a structured approximate expectation propagation (AEP) algorithm. In the distributed mode, edge satellites share channel information and exchange soft information about data symbols, leading to a distributed version of AEP. The introduced basis expansion model for TSLs enables the efficient implementation of both centralized and distributed algorithms via fast Fourier transform. Simulation results demonstrate that proposed schemes significantly outperform conventional algorithms in terms of the activity error rate, the normalized mean squared error, and the symbol error rate. Notably, the distributed mode achieves performance comparable to the centralized mode with only two exchanges of soft information about data symbols within the constellation.","sentences":["This paper investigates joint device identification, channel estimation, and symbol detection for cooperative multi-satellite-enhanced random access, where orthogonal time-frequency space modulation with the large antenna array is utilized to combat the dynamics of the terrestrial-satellite links (TSLs).","We introduce the generalized complex exponential basis expansion model to parameterize TSLs, thereby reducing the pilot overhead.","By exploiting the block sparsity of the TSLs in the angular domain, a message passing algorithm is designed for initial channel estimation.","Subsequently, we examine two cooperative modes to leverage the spatial diversity within satellite constellations: the centralized mode, where computations are performed at a high-power central server, and the distributed mode, where computations are offloaded to edge satellites with minimal signaling overhead.","Specifically, in the centralized mode, device identification is achieved by aggregating backhaul information from edge satellites, and channel estimation and symbol detection are jointly enhanced through a structured approximate expectation propagation (AEP) algorithm.","In the distributed mode, edge satellites share channel information and exchange soft information about data symbols, leading to a distributed version of AEP.","The introduced basis expansion model for TSLs enables the efficient implementation of both centralized and distributed algorithms via fast Fourier transform.","Simulation results demonstrate that proposed schemes significantly outperform conventional algorithms in terms of the activity error rate, the normalized mean squared error, and the symbol error rate.","Notably, the distributed mode achieves performance comparable to the centralized mode with only two exchanges of soft information about data symbols within the constellation."],"url":"http://arxiv.org/abs/2408.02586v1"}
{"created":"2024-08-05 16:00:07","title":"Clustering and Mining Accented Speech for Inclusive and Fair Speech Recognition","abstract":"Modern automatic speech recognition (ASR) systems are typically trained on more than tens of thousands hours of speech data, which is one of the main factors for their great success. However, the distribution of such data is typically biased towards common accents or typical speech patterns. As a result, those systems often poorly perform on atypical accented speech. In this paper, we present accent clustering and mining schemes for fair speech recognition systems which can perform equally well on under-represented accented speech. For accent recognition, we applied three schemes to overcome limited size of supervised accent data: supervised or unsupervised pre-training, distributionally robust optimization (DRO) and unsupervised clustering. Three schemes can significantly improve the accent recognition model especially for unbalanced and small accented speech. Fine-tuning ASR on the mined Indian accent speech using the proposed supervised or unsupervised clustering schemes showed 10.0% and 5.3% relative improvements compared to fine-tuning on the randomly sampled speech, respectively.","sentences":["Modern automatic speech recognition (ASR) systems are typically trained on more than tens of thousands hours of speech data, which is one of the main factors for their great success.","However, the distribution of such data is typically biased towards common accents or typical speech patterns.","As a result, those systems often poorly perform on atypical accented speech.","In this paper, we present accent clustering and mining schemes for fair speech recognition systems which can perform equally well on under-represented accented speech.","For accent recognition, we applied three schemes to overcome limited size of supervised accent data: supervised or unsupervised pre-training, distributionally robust optimization (DRO) and unsupervised clustering.","Three schemes can significantly improve the accent recognition model especially for unbalanced and small accented speech.","Fine-tuning ASR on the mined Indian accent speech using the proposed supervised or unsupervised clustering schemes showed 10.0% and 5.3% relative improvements compared to fine-tuning on the randomly sampled speech, respectively."],"url":"http://arxiv.org/abs/2408.02582v1"}
{"created":"2024-08-05 15:59:36","title":"Operational range bounding of spectroscopy models with anomaly detection","abstract":"Safe operation of machine learning models requires architectures that explicitly delimit their operational ranges. We evaluate the ability of anomaly detection algorithms to provide indicators correlated with degraded model performance. By placing acceptance thresholds over such indicators, hard boundaries are formed that define the model's coverage. As a use case, we consider the extraction of exoplanetary spectra from transit light curves, specifically within the context of ESA's upcoming Ariel mission. Isolation Forests are shown to effectively identify contexts where prediction models are likely to fail. Coverage/error trade-offs are evaluated under conditions of data and concept drift. The best performance is seen when Isolation Forests model projections of the prediction model's explainability SHAP values.","sentences":["Safe operation of machine learning models requires architectures that explicitly delimit their operational ranges.","We evaluate the ability of anomaly detection algorithms to provide indicators correlated with degraded model performance.","By placing acceptance thresholds over such indicators, hard boundaries are formed that define the model's coverage.","As a use case, we consider the extraction of exoplanetary spectra from transit light curves, specifically within the context of ESA's upcoming Ariel mission.","Isolation Forests are shown to effectively identify contexts where prediction models are likely to fail.","Coverage/error trade-offs are evaluated under conditions of data and concept drift.","The best performance is seen when Isolation Forests model projections of the prediction model's explainability SHAP values."],"url":"http://arxiv.org/abs/2408.02581v1"}
{"created":"2024-08-05 15:43:56","title":"Cross-Modality Clustering-based Self-Labeling for Multimodal Data Classification","abstract":"Technological advances facilitate the ability to acquire multimodal data, posing a challenge for recognition systems while also providing an opportunity to use the heterogeneous nature of the information to increase the generalization capability of models. An often overlooked issue is the cost of the labeling process, which is typically high due to the need for a significant investment in time and money associated with human experts. Existing semi-supervised learning methods often focus on operating in the feature space created by the fusion of available modalities, neglecting the potential for cross-utilizing complementary information available in each modality. To address this problem, we propose Cross-Modality Clustering-based Self-Labeling (CMCSL). Based on a small set of pre-labeled data, CMCSL groups instances belonging to each modality in the deep feature space and then propagates known labels within the resulting clusters. Next, information about the instances' class membership in each modality is exchanged based on the Euclidean distance to ensure more accurate labeling. Experimental evaluation conducted on 20 datasets derived from the MM-IMDb dataset indicates that cross-propagation of labels between modalities -- especially when the number of pre-labeled instances is small -- can allow for more reliable labeling and thus increase the classification performance in each modality.","sentences":["Technological advances facilitate the ability to acquire multimodal data, posing a challenge for recognition systems while also providing an opportunity to use the heterogeneous nature of the information to increase the generalization capability of models.","An often overlooked issue is the cost of the labeling process, which is typically high due to the need for a significant investment in time and money associated with human experts.","Existing semi-supervised learning methods often focus on operating in the feature space created by the fusion of available modalities, neglecting the potential for cross-utilizing complementary information available in each modality.","To address this problem, we propose Cross-Modality Clustering-based Self-Labeling (CMCSL).","Based on a small set of pre-labeled data, CMCSL groups instances belonging to each modality in the deep feature space and then propagates known labels within the resulting clusters.","Next, information about the instances' class membership in each modality is exchanged based on the Euclidean distance to ensure more accurate labeling.","Experimental evaluation conducted on 20 datasets derived from the MM-IMDb dataset indicates that cross-propagation of labels between modalities -- especially when the number of pre-labeled instances is small -- can allow for more reliable labeling and thus increase the classification performance in each modality."],"url":"http://arxiv.org/abs/2408.02568v1"}
{"created":"2024-08-05 15:26:39","title":"Process-constrained batch Bayesian approaches for yield optimization in multi-reactor systems","abstract":"The optimization of yields in multi-reactor systems, which are advanced tools in heterogeneous catalysis research, presents a significant challenge due to hierarchical technical constraints. To this respect, this work introduces a novel approach called process-constrained batch Bayesian optimization via Thompson sampling (pc-BO-TS) and its generalized hierarchical extension (hpc-BO-TS). This method, tailored for the efficiency demands in multi-reactor systems, integrates experimental constraints and balances between exploration and exploitation in a sequential batch optimization strategy. It offers an improvement over other Bayesian optimization methods. The performance of pc-BO-TS and hpc-BO-TS is validated in synthetic cases as well as in a realistic scenario based on data obtained from high-throughput experiments done on a multi-reactor system available in the REALCAT platform. The proposed methods often outperform other sequential Bayesian optimizations and existing process-constrained batch Bayesian optimization methods. This work proposes a novel approach to optimize the yield of a reaction in a multi-reactor system, marking a significant step forward in digital catalysis and generally in optimization methods for chemical engineering.","sentences":["The optimization of yields in multi-reactor systems, which are advanced tools in heterogeneous catalysis research, presents a significant challenge due to hierarchical technical constraints.","To this respect, this work introduces a novel approach called process-constrained batch Bayesian optimization via Thompson sampling (pc-BO-TS) and its generalized hierarchical extension (hpc-BO-TS).","This method, tailored for the efficiency demands in multi-reactor systems, integrates experimental constraints and balances between exploration and exploitation in a sequential batch optimization strategy.","It offers an improvement over other Bayesian optimization methods.","The performance of pc-BO-TS and hpc-BO-TS is validated in synthetic cases as well as in a realistic scenario based on data obtained from high-throughput experiments done on a multi-reactor system available in the REALCAT platform.","The proposed methods often outperform other sequential Bayesian optimizations and existing process-constrained batch Bayesian optimization methods.","This work proposes a novel approach to optimize the yield of a reaction in a multi-reactor system, marking a significant step forward in digital catalysis and generally in optimization methods for chemical engineering."],"url":"http://arxiv.org/abs/2408.02551v1"}
{"created":"2024-08-05 15:16:24","title":"RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation","abstract":"Implementing Retrieval-Augmented Generation (RAG) systems is inherently complex, requiring deep understanding of data, use cases, and intricate design decisions. Additionally, evaluating these systems presents significant challenges, necessitating assessment of both retrieval accuracy and generative quality through a multi-faceted approach. We introduce RAG Foundry, an open-source framework for augmenting large language models for RAG use cases. RAG Foundry integrates data creation, training, inference and evaluation into a single workflow, facilitating the creation of data-augmented datasets for training and evaluating large language models in RAG settings. This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources. We demonstrate the framework effectiveness by augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive datasets. Code is released as open-source in https://github.com/IntelLabs/RAGFoundry.","sentences":["Implementing Retrieval-Augmented Generation (RAG) systems is inherently complex, requiring deep understanding of data, use cases, and intricate design decisions.","Additionally, evaluating these systems presents significant challenges, necessitating assessment of both retrieval accuracy and generative quality through a multi-faceted approach.","We introduce RAG Foundry, an open-source framework for augmenting large language models for RAG use cases.","RAG Foundry integrates data creation, training, inference and evaluation into a single workflow, facilitating the creation of data-augmented datasets for training and evaluating large language models in RAG settings.","This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources.","We demonstrate the framework effectiveness by augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive datasets.","Code is released as open-source in https://github.com/IntelLabs/RAGFoundry."],"url":"http://arxiv.org/abs/2408.02545v1"}
{"created":"2024-08-05 15:03:19","title":"LMEMs for post-hoc analysis of HPO Benchmarking","abstract":"The importance of tuning hyperparameters in Machine Learning (ML) and Deep Learning (DL) is established through empirical research and applications, evident from the increase in new hyperparameter optimization (HPO) algorithms and benchmarks steadily added by the community. However, current benchmarking practices using averaged performance across many datasets may obscure key differences between HPO methods, especially for pairwise comparisons. In this work, we apply Linear Mixed-Effect Models-based (LMEMs) significance testing for post-hoc analysis of HPO benchmarking runs. LMEMs allow flexible and expressive modeling on the entire experiment data, including information such as benchmark meta-features, offering deeper insights than current analysis practices. We demonstrate this through a case study on the PriorBand paper's experiment data to find insights not reported in the original work.","sentences":["The importance of tuning hyperparameters in Machine Learning (ML) and Deep Learning (DL) is established through empirical research and applications, evident from the increase in new hyperparameter optimization (HPO) algorithms and benchmarks steadily added by the community.","However, current benchmarking practices using averaged performance across many datasets may obscure key differences between HPO methods, especially for pairwise comparisons.","In this work, we apply Linear Mixed-Effect Models-based (LMEMs) significance testing for post-hoc analysis of HPO benchmarking runs.","LMEMs allow flexible and expressive modeling on the entire experiment data, including information such as benchmark meta-features, offering deeper insights than current analysis practices.","We demonstrate this through a case study on the PriorBand paper's experiment data to find insights not reported in the original work."],"url":"http://arxiv.org/abs/2408.02533v1"}
{"created":"2024-08-05 14:46:35","title":"Online Deterministic Minimum Cost Bipartite Matching with Delays on a Line","abstract":"We study the online minimum cost bipartite perfect matching with delays problem. In this problem, $m$ servers and $m$ requests arrive over time, and an online algorithm can delay the matching between servers and requests by paying the delay cost. The objective is to minimize the total distance and delay cost. When servers and requests lie in a known metric space, there is a randomized $O(\\log n)$-competitive algorithm, where $n$ is the size of the metric space. When the metric space is unknown a priori, Azar and Jacob-Fanani proposed a deterministic $O\\left(\\frac{1}{\\epsilon}m^{\\log\\left(\\frac{3+\\epsilon}{2}\\right)}\\right)$-competitive algorithm for any fixed $\\epsilon > 0$. This competitive ratio is tight when $n = 1$ and becomes $O(m^{0.59})$ for sufficiently small $\\epsilon$.   In this paper, we improve upon the result of Azar and Jacob-Fanani for the case where servers and requests are on the real line, providing a deterministic $\\tilde{O}(m^{0.5})$-competitive algorithm. Our algorithm is based on the Robust Matching (RM) algorithm proposed by Raghvendra for the minimum cost bipartite perfect matching problem. In this problem, delay is not allowed, and all servers arrive in the beginning. When a request arrives, the RM algorithm immediately matches the request to a free server based on the request's minimum $t$-net-cost augmenting path, where $t > 1$ is a constant. In our algorithm, we delay the matching of a request until its waiting time exceeds its minimum $t$-net-cost divided by $t$.","sentences":["We study the online minimum cost bipartite perfect matching with delays problem.","In this problem, $m$ servers and $m$ requests arrive over time, and an online algorithm can delay the matching between servers and requests by paying the delay cost.","The objective is to minimize the total distance and delay cost.","When servers and requests lie in a known metric space, there is a randomized $O(\\log n)$-competitive algorithm, where $n$ is the size of the metric space.","When the metric space is unknown a priori, Azar and Jacob-Fanani proposed a deterministic $O\\left(\\frac{1}{\\epsilon}m^{\\log\\left(\\frac{3+\\epsilon}{2}\\right)}\\right)$-competitive algorithm for any fixed $\\epsilon > 0$.","This competitive ratio is tight when $n = 1$ and becomes $O(m^{0.59})$ for sufficiently small $\\epsilon$.   In this paper, we improve upon the result of Azar and Jacob-Fanani for the case where servers and requests are on the real line, providing a deterministic $\\tilde{O}(m^{0.5})$-competitive algorithm.","Our algorithm is based on the Robust Matching (RM) algorithm proposed by Raghvendra for the minimum cost bipartite perfect matching problem.","In this problem, delay is not allowed, and all servers arrive in the beginning.","When a request arrives, the RM algorithm immediately matches the request to a free server based on the request's minimum $t$-net-cost augmenting path, where $t > 1$ is a constant.","In our algorithm, we delay the matching of a request until its waiting time exceeds its minimum $t$-net-cost divided by $t$."],"url":"http://arxiv.org/abs/2408.02526v1"}
