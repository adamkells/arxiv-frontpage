{"created":"2024-10-16 14:42:23","title":"From Measurement Instruments to Data: Leveraging Theory-Driven Synthetic Training Data for Classifying Social Constructs","abstract":"Computational text classification is a challenging task, especially for multi-dimensional social constructs. Recently, there has been increasing discussion that synthetic training data could enhance classification by offering examples of how these constructs are represented in texts. In this paper, we systematically examine the potential of theory-driven synthetic training data for improving the measurement of social constructs. In particular, we explore how researchers can transfer established knowledge from measurement instruments in the social sciences, such as survey scales or annotation codebooks, into theory-driven generation of synthetic data. Using two studies on measuring sexism and political topics, we assess the added value of synthetic training data for fine-tuning text classification models. Although the results of the sexism study were less promising, our findings demonstrate that synthetic data can be highly effective in reducing the need for labeled data in political topic classification. With only a minimal drop in performance, synthetic data allows for substituting large amounts of labeled data. Furthermore, theory-driven synthetic data performed markedly better than data generated without conceptual information in mind.","sentences":["Computational text classification is a challenging task, especially for multi-dimensional social constructs.","Recently, there has been increasing discussion that synthetic training data could enhance classification by offering examples of how these constructs are represented in texts.","In this paper, we systematically examine the potential of theory-driven synthetic training data for improving the measurement of social constructs.","In particular, we explore how researchers can transfer established knowledge from measurement instruments in the social sciences, such as survey scales or annotation codebooks, into theory-driven generation of synthetic data.","Using two studies on measuring sexism and political topics, we assess the added value of synthetic training data for fine-tuning text classification models.","Although the results of the sexism study were less promising, our findings demonstrate that synthetic data can be highly effective in reducing the need for labeled data in political topic classification.","With only a minimal drop in performance, synthetic data allows for substituting large amounts of labeled data.","Furthermore, theory-driven synthetic data performed markedly better than data generated without conceptual information in mind."],"url":"http://arxiv.org/abs/2410.12622v2"}
{"created":"2024-10-16 06:51:09","title":"LLM-based Cognitive Models of Students with Misconceptions","abstract":"Accurately modeling student cognition is crucial for developing effective AI-driven educational technologies. A key challenge is creating realistic student models that satisfy two essential properties: (1) accurately replicating specific misconceptions, and (2) correctly solving problems where these misconceptions are not applicable. This dual requirement reflects the complex nature of student understanding, where misconceptions coexist with correct knowledge. This paper investigates whether Large Language Models (LLMs) can be instruction-tuned to meet this dual requirement and effectively simulate student thinking in algebra. We introduce MalAlgoPy, a novel Python library that generates datasets reflecting authentic student solution patterns through a graph-based representation of algebraic problem-solving. Utilizing MalAlgoPy, we define and examine Cognitive Student Models (CSMs) - LLMs instruction tuned to faithfully emulate realistic student behavior. Our findings reveal that LLMs trained on misconception examples can efficiently learn to replicate errors. However, the training diminishes the model's ability to solve problems correctly, particularly for problem types where the misconceptions are not applicable, thus failing to satisfy second property of CSMs. We demonstrate that by carefully calibrating the ratio of correct to misconception examples in the training data - sometimes as low as 0.25 - it is possible to develop CSMs that satisfy both properties. Our insights enhance our understanding of AI-based student models and pave the way for effective adaptive learning systems.","sentences":["Accurately modeling student cognition is crucial for developing effective AI-driven educational technologies.","A key challenge is creating realistic student models that satisfy two essential properties: (1) accurately replicating specific misconceptions, and (2) correctly solving problems where these misconceptions are not applicable.","This dual requirement reflects the complex nature of student understanding, where misconceptions coexist with correct knowledge.","This paper investigates whether Large Language Models (LLMs) can be instruction-tuned to meet this dual requirement and effectively simulate student thinking in algebra.","We introduce MalAlgoPy, a novel Python library that generates datasets reflecting authentic student solution patterns through a graph-based representation of algebraic problem-solving.","Utilizing MalAlgoPy, we define and examine Cognitive Student Models (CSMs) -","LLMs instruction tuned to faithfully emulate realistic student behavior.","Our findings reveal that LLMs trained on misconception examples can efficiently learn to replicate errors.","However, the training diminishes the model's ability to solve problems correctly, particularly for problem types where the misconceptions are not applicable, thus failing to satisfy second property of CSMs.","We demonstrate that by carefully calibrating the ratio of correct to misconception examples in the training data - sometimes as low as 0.25 - it is possible to develop CSMs that satisfy both properties.","Our insights enhance our understanding of AI-based student models and pave the way for effective adaptive learning systems."],"url":"http://arxiv.org/abs/2410.12294v2"}
