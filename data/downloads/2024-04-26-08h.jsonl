{"created":"2024-04-25 17:59:41","title":"Learning Visuotactile Skills with Two Multifingered Hands","abstract":"Aiming to replicate human-like dexterity, perceptual experiences, and motion patterns, we explore learning from human demonstrations using a bimanual system with multifingered hands and visuotactile data. Two significant challenges exist: the lack of an affordable and accessible teleoperation system suitable for a dual-arm setup with multifingered hands, and the scarcity of multifingered hand hardware equipped with touch sensing. To tackle the first challenge, we develop HATO, a low-cost hands-arms teleoperation system that leverages off-the-shelf electronics, complemented with a software suite that enables efficient data collection; the comprehensive software suite also supports multimodal data processing, scalable policy learning, and smooth policy deployment. To tackle the latter challenge, we introduce a novel hardware adaptation by repurposing two prosthetic hands equipped with touch sensors for research. Using visuotactile data collected from our system, we learn skills to complete long-horizon, high-precision tasks which are difficult to achieve without multifingered dexterity and touch feedback. Furthermore, we empirically investigate the effects of dataset size, sensing modality, and visual input preprocessing on policy learning. Our results mark a promising step forward in bimanual multifingered manipulation from visuotactile data. Videos, code, and datasets can be found at https://toruowo.github.io/hato/ .","sentences":["Aiming to replicate human-like dexterity, perceptual experiences, and motion patterns, we explore learning from human demonstrations using a bimanual system with multifingered hands and visuotactile data.","Two significant challenges exist: the lack of an affordable and accessible teleoperation system suitable for a dual-arm setup with multifingered hands, and the scarcity of multifingered hand hardware equipped with touch sensing.","To tackle the first challenge, we develop HATO, a low-cost hands-arms teleoperation system that leverages off-the-shelf electronics, complemented with a software suite that enables efficient data collection; the comprehensive software suite also supports multimodal data processing, scalable policy learning, and smooth policy deployment.","To tackle the latter challenge, we introduce a novel hardware adaptation by repurposing two prosthetic hands equipped with touch sensors for research.","Using visuotactile data collected from our system, we learn skills to complete long-horizon, high-precision tasks which are difficult to achieve without multifingered dexterity and touch feedback.","Furthermore, we empirically investigate the effects of dataset size, sensing modality, and visual input preprocessing on policy learning.","Our results mark a promising step forward in bimanual multifingered manipulation from visuotactile data.","Videos, code, and datasets can be found at https://toruowo.github.io/hato/ ."],"url":"http://arxiv.org/abs/2404.16823v1"}
{"created":"2024-04-25 17:58:43","title":"Revisiting Text-to-Image Evaluation with Gecko: On Metrics, Prompts, and Human Ratings","abstract":"While text-to-image (T2I) generative models have become ubiquitous, they do not necessarily generate images that align with a given prompt. While previous work has evaluated T2I alignment by proposing metrics, benchmarks, and templates for collecting human judgements, the quality of these components is not systematically measured. Human-rated prompt sets are generally small and the reliability of the ratings -- and thereby the prompt set used to compare models -- is not evaluated. We address this gap by performing an extensive study evaluating auto-eval metrics and human templates. We provide three main contributions: (1) We introduce a comprehensive skills-based benchmark that can discriminate models across different human templates. This skills-based benchmark categorises prompts into sub-skills, allowing a practitioner to pinpoint not only which skills are challenging, but at what level of complexity a skill becomes challenging. (2) We gather human ratings across four templates and four T2I models for a total of >100K annotations. This allows us to understand where differences arise due to inherent ambiguity in the prompt and where they arise due to differences in metric and model quality. (3) Finally, we introduce a new QA-based auto-eval metric that is better correlated with human ratings than existing metrics for our new dataset, across different human templates, and on TIFA160.","sentences":["While text-to-image (T2I) generative models have become ubiquitous, they do not necessarily generate images that align with a given prompt.","While previous work has evaluated T2I alignment by proposing metrics, benchmarks, and templates for collecting human judgements, the quality of these components is not systematically measured.","Human-rated prompt sets are generally small and the reliability of the ratings -- and thereby the prompt set used to compare models -- is not evaluated.","We address this gap by performing an extensive study evaluating auto-eval metrics and human templates.","We provide three main contributions: (1) We introduce a comprehensive skills-based benchmark that can discriminate models across different human templates.","This skills-based benchmark categorises prompts into sub-skills, allowing a practitioner to pinpoint not only which skills are challenging, but at what level of complexity a skill becomes challenging.","(2) We gather human ratings across four templates and four T2I models for a total of >100K annotations.","This allows us to understand where differences arise due to inherent ambiguity in the prompt and where they arise due to differences in metric and model quality.","(3) Finally, we introduce a new QA-based auto-eval metric that is better correlated with human ratings than existing metrics for our new dataset, across different human templates, and on TIFA160."],"url":"http://arxiv.org/abs/2404.16820v1"}
{"created":"2024-04-25 17:57:36","title":"IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages","abstract":"As large language models (LLMs) see increasing adoption across the globe, it is imperative for LLMs to be representative of the linguistic diversity of the world. India is a linguistically diverse country of 1.4 Billion people. To facilitate research on multilingual LLM evaluation, we release IndicGenBench - the largest benchmark for evaluating LLMs on user-facing generation tasks across a diverse set 29 of Indic languages covering 13 scripts and 4 language families. IndicGenBench is composed of diverse generation tasks like cross-lingual summarization, machine translation, and cross-lingual question answering. IndicGenBench extends existing benchmarks to many Indic languages through human curation providing multi-way parallel evaluation data for many under-represented Indic languages for the first time. We evaluate a wide range of proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5, Gemma, BLOOM and LLaMA on IndicGenBench in a variety of settings. The largest PaLM-2 models performs the best on most tasks, however, there is a significant performance gap in all languages compared to English showing that further research is needed for the development of more inclusive multilingual language models. IndicGenBench is released at www.github.com/google-research-datasets/indic-gen-bench","sentences":["As large language models (LLMs) see increasing adoption across the globe, it is imperative for LLMs to be representative of the linguistic diversity of the world.","India is a linguistically diverse country of 1.4 Billion people.","To facilitate research on multilingual LLM evaluation, we release IndicGenBench - the largest benchmark for evaluating LLMs on user-facing generation tasks across a diverse set 29 of Indic languages covering 13 scripts and 4 language families.","IndicGenBench is composed of diverse generation tasks like cross-lingual summarization, machine translation, and cross-lingual question answering.","IndicGenBench extends existing benchmarks to many Indic languages through human curation providing multi-way parallel evaluation data for many under-represented Indic languages for the first time.","We evaluate a wide range of proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5, Gemma, BLOOM and LLaMA on IndicGenBench in a variety of settings.","The largest PaLM-2 models performs the best on most tasks, however, there is a significant performance gap in all languages compared to English showing that further research is needed for the development of more inclusive multilingual language models.","IndicGenBench is released at www.github.com/google-research-datasets/indic-gen-bench"],"url":"http://arxiv.org/abs/2404.16816v1"}
{"created":"2024-04-25 17:56:45","title":"Meta-Transfer Derm-Diagnosis: Exploring Few-Shot Learning and Transfer Learning for Skin Disease Classification in Long-Tail Distribution","abstract":"Addressing the challenges of rare diseases is difficult, especially with the limited number of reference images and a small patient population. This is more evident in rare skin diseases, where we encounter long-tailed data distributions that make it difficult to develop unbiased and broadly effective models. The diverse ways in which image datasets are gathered and their distinct purposes also add to these challenges. Our study conducts a detailed examination of the benefits and drawbacks of episodic and conventional training methodologies, adopting a few-shot learning approach alongside transfer learning. We evaluated our models using the ISIC2018, Derm7pt, and SD-198 datasets. With minimal labeled examples, our models showed substantial information gains and better performance compared to previously trained models. Our research emphasizes the improved ability to represent features in DenseNet121 and MobileNetV2 models, achieved by using pre-trained models on ImageNet to increase similarities within classes. Moreover, our experiments, ranging from 2-way to 5-way classifications with up to 10 examples, showed a growing success rate for traditional transfer learning methods as the number of examples increased. The addition of data augmentation techniques significantly improved our transfer learning based model performance, leading to higher performances than existing methods, especially in the SD-198 and ISIC2018 datasets. All source code related to this work will be made publicly available soon at the provided URL.","sentences":["Addressing the challenges of rare diseases is difficult, especially with the limited number of reference images and a small patient population.","This is more evident in rare skin diseases, where we encounter long-tailed data distributions that make it difficult to develop unbiased and broadly effective models.","The diverse ways in which image datasets are gathered and their distinct purposes also add to these challenges.","Our study conducts a detailed examination of the benefits and drawbacks of episodic and conventional training methodologies, adopting a few-shot learning approach alongside transfer learning.","We evaluated our models using the ISIC2018, Derm7pt, and SD-198 datasets.","With minimal labeled examples, our models showed substantial information gains and better performance compared to previously trained models.","Our research emphasizes the improved ability to represent features in DenseNet121 and MobileNetV2 models, achieved by using pre-trained models on ImageNet to increase similarities within classes.","Moreover, our experiments, ranging from 2-way to 5-way classifications with up to 10 examples, showed a growing success rate for traditional transfer learning methods as the number of examples increased.","The addition of data augmentation techniques significantly improved our transfer learning based model performance, leading to higher performances than existing methods, especially in the SD-198 and ISIC2018 datasets.","All source code related to this work will be made publicly available soon at the provided URL."],"url":"http://arxiv.org/abs/2404.16814v1"}
{"created":"2024-04-25 17:55:14","title":"Make Your LLM Fully Utilize the Context","abstract":"While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the lost-in-the-middle challenge. We hypothesize that it stems from insufficient explicit supervision during the long-context training, which fails to emphasize that any position in a long context can hold crucial information. Based on this intuition, our study presents information-intensive (IN2) training, a purely data-driven solution to overcome lost-in-the-middle. Specifically, IN2 training leverages a synthesized long-context question-answer dataset, where the answer requires (1) fine-grained information awareness on a short segment (~128 tokens) within a synthesized long context (4K-32K tokens), and (2) the integration and reasoning of information from two or more short segments. Through applying this information-intensive training on Mistral-7B, we present FILM-7B (FILl-in-the-Middle). To thoroughly assess the ability of FILM-7B for utilizing long contexts, we design three probing tasks that encompass various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval). The probing results demonstrate that FILM-7B can robustly retrieve information from different positions in its 32K context window. Beyond these probing tasks, FILM-7B significantly improves the performance on real-world long-context tasks (e.g., 23.5->26.9 F1 score on NarrativeQA), while maintaining a comparable performance on short-context tasks (e.g., 59.3->59.2 accuracy on MMLU). Github Link: https://github.com/microsoft/FILM.","sentences":["While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the lost-in-the-middle challenge.","We hypothesize that it stems from insufficient explicit supervision during the long-context training, which fails to emphasize that any position in a long context can hold crucial information.","Based on this intuition, our study presents information-intensive (IN2) training, a purely data-driven solution to overcome lost-in-the-middle.","Specifically, IN2 training leverages a synthesized long-context question-answer dataset, where the answer requires (1) fine-grained information awareness on a short segment (~128 tokens) within a synthesized long context (4K-32K tokens), and (2) the integration and reasoning of information from two or more short segments.","Through applying this information-intensive training on Mistral-7B, we present FILM-7B (FILl-in-the-Middle).","To thoroughly assess the ability of FILM-7B for utilizing long contexts, we design three probing tasks that encompass various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval).","The probing results demonstrate that FILM-7B can robustly retrieve information from different positions in its 32K context window.","Beyond these probing tasks, FILM-7B significantly improves the performance on real-world long-context tasks (e.g., 23.5->26.9 F1 score on NarrativeQA), while maintaining a comparable performance on short-context tasks (e.g., 59.3->59.2 accuracy on MMLU).","Github Link: https://github.com/microsoft/FILM."],"url":"http://arxiv.org/abs/2404.16811v1"}
{"created":"2024-04-25 17:52:39","title":"Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning","abstract":"Generative Commonsense Reasoning (GCR) requires a model to reason about a situation using commonsense knowledge, while generating coherent sentences. Although the quality of the generated sentences is crucial, the diversity of the generation is equally important because it reflects the model's ability to use a range of commonsense knowledge facts. Large Language Models (LLMs) have shown proficiency in enhancing the generation quality across various tasks through in-context learning (ICL) using given examples without the need for any fine-tuning. However, the diversity aspect in LLM outputs has not been systematically studied before. To address this, we propose a simple method that diversifies the LLM generations, while preserving their quality. Experimental results on three benchmark GCR datasets show that our method achieves an ideal balance between the quality and diversity. Moreover, the sentences generated by our proposed method can be used as training data to improve diversity in existing commonsense generators.","sentences":["Generative Commonsense Reasoning (GCR) requires a model to reason about a situation using commonsense knowledge, while generating coherent sentences.","Although the quality of the generated sentences is crucial, the diversity of the generation is equally important because it reflects the model's ability to use a range of commonsense knowledge facts.","Large Language Models (LLMs) have shown proficiency in enhancing the generation quality across various tasks through in-context learning (ICL) using given examples without the need for any fine-tuning.","However, the diversity aspect in LLM outputs has not been systematically studied before.","To address this, we propose a simple method that diversifies the LLM generations, while preserving their quality.","Experimental results on three benchmark GCR datasets show that our method achieves an ideal balance between the quality and diversity.","Moreover, the sentences generated by our proposed method can be used as training data to improve diversity in existing commonsense generators."],"url":"http://arxiv.org/abs/2404.16807v1"}
{"created":"2024-04-25 17:51:10","title":"AAPL: Adding Attributes to Prompt Learning for Vision-Language Models","abstract":"Recent advances in large pre-trained vision-language models have demonstrated remarkable performance on zero-shot downstream tasks. Building upon this, recent studies, such as CoOp and CoCoOp, have proposed the use of prompt learning, where context within a prompt is replaced with learnable vectors, leading to significant improvements over manually crafted prompts. However, the performance improvement for unseen classes is still marginal, and to tackle this problem, data augmentation has been frequently used in traditional zero-shot learning techniques. Through our experiments, we have identified important issues in CoOp and CoCoOp: the context learned through traditional image augmentation is biased toward seen classes, negatively impacting generalization to unseen classes. To address this problem, we propose adversarial token embedding to disentangle low-level visual augmentation features from high-level class information when inducing bias in learnable prompts. Through our novel mechanism called \"Adding Attributes to Prompt Learning\", AAPL, we guide the learnable context to effectively extract text features by focusing on high-level features for unseen classes. We have conducted experiments across 11 datasets, and overall, AAPL shows favorable performances compared to the existing methods in few-shot learning, zero-shot learning, cross-dataset, and domain generalization tasks.","sentences":["Recent advances in large pre-trained vision-language models have demonstrated remarkable performance on zero-shot downstream tasks.","Building upon this, recent studies, such as CoOp and CoCoOp, have proposed the use of prompt learning, where context within a prompt is replaced with learnable vectors, leading to significant improvements over manually crafted prompts.","However, the performance improvement for unseen classes is still marginal, and to tackle this problem, data augmentation has been frequently used in traditional zero-shot learning techniques.","Through our experiments, we have identified important issues in CoOp and CoCoOp: the context learned through traditional image augmentation is biased toward seen classes, negatively impacting generalization to unseen classes.","To address this problem, we propose adversarial token embedding to disentangle low-level visual augmentation features from high-level class information when inducing bias in learnable prompts.","Through our novel mechanism called \"Adding Attributes to Prompt Learning\", AAPL, we guide the learnable context to effectively extract text features by focusing on high-level features for unseen classes.","We have conducted experiments across 11 datasets, and overall, AAPL shows favorable performances compared to the existing methods in few-shot learning, zero-shot learning, cross-dataset, and domain generalization tasks."],"url":"http://arxiv.org/abs/2404.16804v1"}
{"created":"2024-04-25 17:40:52","title":"In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter Optimization","abstract":"With the increasing computational costs associated with deep learning, automated hyperparameter optimization methods, strongly relying on black-box Bayesian optimization (BO), face limitations. Freeze-thaw BO offers a promising grey-box alternative, strategically allocating scarce resources incrementally to different configurations. However, the frequent surrogate model updates inherent to this approach pose challenges for existing methods, requiring retraining or fine-tuning their neural network surrogates online, introducing overhead, instability, and hyper-hyperparameters. In this work, we propose FT-PFN, a novel surrogate for Freeze-thaw style BO. FT-PFN is a prior-data fitted network (PFN) that leverages the transformers' in-context learning ability to efficiently and reliably do Bayesian learning curve extrapolation in a single forward pass. Our empirical analysis across three benchmark suites shows that the predictions made by FT-PFN are more accurate and 10-100 times faster than those of the deep Gaussian process and deep ensemble surrogates used in previous work. Furthermore, we show that, when combined with our novel acquisition mechanism (MFPI-random), the resulting in-context freeze-thaw BO method (ifBO), yields new state-of-the-art performance in the same three families of deep learning HPO benchmarks considered in prior work.","sentences":["With the increasing computational costs associated with deep learning, automated hyperparameter optimization methods, strongly relying on black-box Bayesian optimization (BO), face limitations.","Freeze-thaw BO offers a promising grey-box alternative, strategically allocating scarce resources incrementally to different configurations.","However, the frequent surrogate model updates inherent to this approach pose challenges for existing methods, requiring retraining or fine-tuning their neural network surrogates online, introducing overhead, instability, and hyper-hyperparameters.","In this work, we propose FT-PFN, a novel surrogate for Freeze-thaw style BO.","FT-PFN is a prior-data fitted network (PFN) that leverages the transformers' in-context learning ability to efficiently and reliably do Bayesian learning curve extrapolation in a single forward pass.","Our empirical analysis across three benchmark suites shows that the predictions made by FT-PFN are more accurate and 10-100 times faster than those of the deep Gaussian process and deep ensemble surrogates used in previous work.","Furthermore, we show that, when combined with our novel acquisition mechanism (MFPI-random), the resulting in-context freeze-thaw BO method (ifBO), yields new state-of-the-art performance in the same three families of deep learning HPO benchmarks considered in prior work."],"url":"http://arxiv.org/abs/2404.16795v1"}
{"created":"2024-04-25 17:39:52","title":"A Communication- and Memory-Aware Model for Load Balancing Tasks","abstract":"While load balancing in distributed-memory computing has been well-studied, we present an innovative approach to this problem: a unified, reduced-order model that combines three key components to describe \"work\" in a distributed system: computation, communication, and memory. Our model enables an optimizer to explore complex tradeoffs in task placement, such as increased parallelism at the expense of data replication, which increases memory usage. We propose a fully distributed, heuristic-based load balancing optimization algorithm, and demonstrate that it quickly finds close-to-optimal solutions. We formalize the complex optimization problem as a mixed-integer linear program, and compare it to our strategy. Finally, we show that when applied to an electromagnetics code, our approach obtains up to 2.3x speedups for the imbalanced execution.","sentences":["While load balancing in distributed-memory computing has been well-studied, we present an innovative approach to this problem: a unified, reduced-order model that combines three key components to describe \"work\" in a distributed system: computation, communication, and memory.","Our model enables an optimizer to explore complex tradeoffs in task placement, such as increased parallelism at the expense of data replication, which increases memory usage.","We propose a fully distributed, heuristic-based load balancing optimization algorithm, and demonstrate that it quickly finds close-to-optimal solutions.","We formalize the complex optimization problem as a mixed-integer linear program, and compare it to our strategy.","Finally, we show that when applied to an electromagnetics code, our approach obtains up to 2.3x speedups for the imbalanced execution."],"url":"http://arxiv.org/abs/2404.16793v1"}
{"created":"2024-04-25 17:39:50","title":"Weak-to-Strong Extrapolation Expedites Alignment","abstract":"Although the capabilities of large language models (LLMs) ideally scale up with increasing data and compute, they are inevitably constrained by limited resources in reality. Suppose we have a moderately trained LLM (e.g., trained to align with human preference) in hand, can we further exploit its potential and cheaply acquire a stronger model? In this paper, we propose a simple method called ExPO to boost LLMs' alignment with human preference. ExPO assumes that a medium-aligned model can be interpolated between a less-aligned (weaker) model, e.g., the initial SFT model, and a better-aligned (stronger) one, thereby directly obtaining this stronger model by extrapolating from the weights of the former two relatively weaker models. On the AlpacaEval 2.0 benchmark, we show that ExPO pushes models trained with less preference data (e.g., 10% or 20%) to reach and even surpass the fully-trained one, without any additional training. Furthermore, ExPO also significantly improves off-the-shelf DPO/RLHF models and exhibits decent scalability across model sizes from 7B to 70B. Our work demonstrates the efficacy of model extrapolation in exploiting LLMs' capabilities, suggesting a promising direction that deserves future exploration.","sentences":["Although the capabilities of large language models (LLMs) ideally scale up with increasing data and compute, they are inevitably constrained by limited resources in reality.","Suppose we have a moderately trained LLM (e.g., trained to align with human preference) in hand, can we further exploit its potential and cheaply acquire a stronger model?","In this paper, we propose a simple method called ExPO to boost LLMs' alignment with human preference.","ExPO assumes that a medium-aligned model can be interpolated between a less-aligned (weaker) model, e.g., the initial SFT model, and a better-aligned (stronger) one, thereby directly obtaining this stronger model by extrapolating from the weights of the former two relatively weaker models.","On the AlpacaEval 2.0 benchmark, we show that ExPO pushes models trained with less preference data (e.g., 10% or 20%) to reach and even surpass the fully-trained one, without any additional training.","Furthermore, ExPO also significantly improves off-the-shelf DPO/RLHF models and exhibits decent scalability across model sizes from 7B to 70B.","Our work demonstrates the efficacy of model extrapolation in exploiting LLMs' capabilities, suggesting a promising direction that deserves future exploration."],"url":"http://arxiv.org/abs/2404.16792v1"}
{"created":"2024-04-25 17:38:57","title":"Continual Learning of Large Language Models: A Comprehensive Survey","abstract":"The recent success of large language models (LLMs) trained on static, pre-collected, general datasets has sparked numerous research directions and applications. One such direction addresses the non-trivial challenge of integrating pre-trained LLMs into dynamic data distributions, task structures, and user preferences. Pre-trained LLMs, when tailored for specific needs, often experience significant performance degradation in previous knowledge domains -- a phenomenon known as \"catastrophic forgetting\". While extensively studied in the continual learning (CL) community, it presents new manifestations in the realm of LLMs. In this survey, we provide a comprehensive overview of the current research progress on LLMs within the context of CL. This survey is structured into four main sections: we first describe an overview of continually learning LLMs, consisting of two directions of continuity: vertical continuity (or vertical continual learning), i.e., continual adaptation from general to specific capabilities, and horizontal continuity (or horizontal continual learning), i.e., continual adaptation across time and domains (Section 3). We then summarize three stages of learning LLMs in the context of modern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP), and Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of evaluation protocols for continual learning with LLMs, along with the current available data sources (Section 5). Finally, we discuss intriguing questions pertaining to continual learning for LLMs (Section 6). The full list of papers examined in this survey is available at https://github.com/Wang-ML-Lab/llm-continual-learning-survey.","sentences":["The recent success of large language models (LLMs) trained on static, pre-collected, general datasets has sparked numerous research directions and applications.","One such direction addresses the non-trivial challenge of integrating pre-trained LLMs into dynamic data distributions, task structures, and user preferences.","Pre-trained LLMs, when tailored for specific needs, often experience significant performance degradation in previous knowledge domains -- a phenomenon known as \"catastrophic forgetting\".","While extensively studied in the continual learning (CL) community, it presents new manifestations in the realm of LLMs.","In this survey, we provide a comprehensive overview of the current research progress on LLMs within the context of CL.","This survey is structured into four main sections: we first describe an overview of continually learning LLMs, consisting of two directions of continuity: vertical continuity (or vertical continual learning), i.e., continual adaptation from general to specific capabilities, and horizontal continuity (or horizontal continual learning), i.e., continual adaptation across time and domains (Section 3).","We then summarize three stages of learning LLMs in the context of modern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP), and Continual Fine-Tuning (CFT) (Section 4).","Then we provide an overview of evaluation protocols for continual learning with LLMs, along with the current available data sources (Section 5).","Finally, we discuss intriguing questions pertaining to continual learning for LLMs (Section 6).","The full list of papers examined in this survey is available at https://github.com/Wang-ML-Lab/llm-continual-learning-survey."],"url":"http://arxiv.org/abs/2404.16789v1"}
{"created":"2024-04-25 17:30:38","title":"Registration by Regression (RbR): a framework for interpretable and flexible atlas registration","abstract":"In human neuroimaging studies, atlas registration enables mapping MRI scans to a common coordinate frame, which is necessary to aggregate data from multiple subjects. Machine learning registration methods have achieved excellent speed and accuracy but lack interpretability. More recently, keypoint-based methods have been proposed to tackle this issue, but their accuracy is still subpar, particularly when fitting nonlinear transforms. Here we propose Registration by Regression (RbR), a novel atlas registration framework that is highly robust and flexible, conceptually simple, and can be trained with cheaply obtained data. RbR predicts the (x,y,z) atlas coordinates for every voxel of the input scan (i.e., every voxel is a keypoint), and then uses closed-form expressions to quickly fit transforms using a wide array of possible deformation models, including affine and nonlinear (e.g., Bspline, Demons, invertible diffeomorphic models, etc.). Robustness is provided by the large number of voxels informing the registration and can be further increased by robust estimators like RANSAC. Experiments on independent public datasets show that RbR yields more accurate registration than competing keypoint approaches, while providing full control of the deformation model.","sentences":["In human neuroimaging studies, atlas registration enables mapping MRI scans to a common coordinate frame, which is necessary to aggregate data from multiple subjects.","Machine learning registration methods have achieved excellent speed and accuracy but lack interpretability.","More recently, keypoint-based methods have been proposed to tackle this issue, but their accuracy is still subpar, particularly when fitting nonlinear transforms.","Here we propose Registration by Regression (RbR), a novel atlas registration framework that is highly robust and flexible, conceptually simple, and can be trained with cheaply obtained data.","RbR predicts the (x,y,z) atlas coordinates for every voxel of the input scan (i.e., every voxel is a keypoint), and then uses closed-form expressions to quickly fit transforms using a wide array of possible deformation models, including affine and nonlinear (e.g., Bspline, Demons, invertible diffeomorphic models, etc.).","Robustness is provided by the large number of voxels informing the registration and can be further increased by robust estimators like RANSAC.","Experiments on independent public datasets show that RbR yields more accurate registration than competing keypoint approaches, while providing full control of the deformation model."],"url":"http://arxiv.org/abs/2404.16781v1"}
{"created":"2024-04-25 17:28:33","title":"DrS: Learning Reusable Dense Rewards for Multi-Stage Tasks","abstract":"The success of many RL techniques heavily relies on human-engineered dense rewards, which typically demand substantial domain expertise and extensive trial and error. In our work, we propose DrS (Dense reward learning from Stages), a novel approach for learning reusable dense rewards for multi-stage tasks in a data-driven manner. By leveraging the stage structures of the task, DrS learns a high-quality dense reward from sparse rewards and demonstrations if given. The learned rewards can be \\textit{reused} in unseen tasks, thus reducing the human effort for reward engineering. Extensive experiments on three physical robot manipulation task families with 1000+ task variants demonstrate that our learned rewards can be reused in unseen tasks, resulting in improved performance and sample efficiency of RL algorithms. The learned rewards even achieve comparable performance to human-engineered rewards on some tasks. See our project page (https://sites.google.com/view/iclr24drs) for more details.","sentences":["The success of many RL techniques heavily relies on human-engineered dense rewards, which typically demand substantial domain expertise and extensive trial and error.","In our work, we propose DrS (Dense reward learning from Stages), a novel approach for learning reusable dense rewards for multi-stage tasks in a data-driven manner.","By leveraging the stage structures of the task, DrS learns a high-quality dense reward from sparse rewards and demonstrations if given.","The learned rewards can be \\textit{reused} in unseen tasks, thus reducing the human effort for reward engineering.","Extensive experiments on three physical robot manipulation task families with 1000+ task variants demonstrate that our learned rewards can be reused in unseen tasks, resulting in improved performance and sample efficiency of RL algorithms.","The learned rewards even achieve comparable performance to human-engineered rewards on some tasks.","See our project page (https://sites.google.com/view/iclr24drs) for more details."],"url":"http://arxiv.org/abs/2404.16779v1"}
{"created":"2024-04-25 17:24:35","title":"ConKeD++ -- Improving descriptor learning for retinal image registration: A comprehensive study of contrastive losses","abstract":"Self-supervised contrastive learning has emerged as one of the most successful deep learning paradigms. In this regard, it has seen extensive use in image registration and, more recently, in the particular field of medical image registration. In this work, we propose to test and extend and improve a state-of-the-art framework for color fundus image registration, ConKeD. Using the ConKeD framework we test multiple loss functions, adapting them to the framework and the application domain. Furthermore, we evaluate our models using the standarized benchmark dataset FIRE as well as several datasets that have never been used before for color fundus registration, for which we are releasing the pairing data as well as a standardized evaluation approach. Our work demonstrates state-of-the-art performance across all datasets and metrics demonstrating several advantages over current SOTA color fundus registration methods","sentences":["Self-supervised contrastive learning has emerged as one of the most successful deep learning paradigms.","In this regard, it has seen extensive use in image registration and, more recently, in the particular field of medical image registration.","In this work, we propose to test and extend and improve a state-of-the-art framework for color fundus image registration, ConKeD. Using the ConKeD framework we test multiple loss functions, adapting them to the framework and the application domain.","Furthermore, we evaluate our models using the standarized benchmark dataset FIRE as well as several datasets that have never been used before for color fundus registration, for which we are releasing the pairing data as well as a standardized evaluation approach.","Our work demonstrates state-of-the-art performance across all datasets and metrics demonstrating several advantages over current SOTA color fundus registration methods"],"url":"http://arxiv.org/abs/2404.16773v1"}
{"created":"2024-04-25 17:20:45","title":"REBEL: Reinforcement Learning via Regressing Relative Rewards","abstract":"While originally developed for continuous control problems, Proximal Policy Optimization (PPO) has emerged as the work-horse of a variety of reinforcement learning (RL) applications including the fine-tuning of generative models. Unfortunately, PPO requires multiple heuristics to enable stable convergence (e.g. value networks, clipping) and is notorious for its sensitivity to the precise implementation of these components. In response, we take a step back and ask what a minimalist RL algorithm for the era of generative models would look like. We propose REBEL, an algorithm that cleanly reduces the problem of policy optimization to regressing the relative rewards via a direct policy parameterization between two completions to a prompt, enabling strikingly lightweight implementation. In theory, we prove that fundamental RL algorithms like Natural Policy Gradient can be seen as variants of REBEL, which allows us to match the strongest known theoretical guarantees in terms of convergence and sample complexity in the RL literature. REBEL can also cleanly incorporate offline data and handle the intransitive preferences we frequently see in practice. Empirically, we find that REBEL provides a unified approach to language modeling and image generation with stronger or similar performance as PPO and DPO, all while being simpler to implement and more computationally tractable than PPO.","sentences":["While originally developed for continuous control problems, Proximal Policy Optimization (PPO) has emerged as the work-horse of a variety of reinforcement learning (RL) applications including the fine-tuning of generative models.","Unfortunately, PPO requires multiple heuristics to enable stable convergence (e.g. value networks, clipping) and is notorious for its sensitivity to the precise implementation of these components.","In response, we take a step back and ask what a minimalist RL algorithm for the era of generative models would look like.","We propose REBEL, an algorithm that cleanly reduces the problem of policy optimization to regressing the relative rewards via a direct policy parameterization between two completions to a prompt, enabling strikingly lightweight implementation.","In theory, we prove that fundamental RL algorithms like Natural Policy Gradient can be seen as variants of REBEL, which allows us to match the strongest known theoretical guarantees in terms of convergence and sample complexity in the RL literature.","REBEL can also cleanly incorporate offline data and handle the intransitive preferences we frequently see in practice.","Empirically, we find that REBEL provides a unified approach to language modeling and image generation with stronger or similar performance as PPO and DPO, all while being simpler to implement and more computationally tractable than PPO."],"url":"http://arxiv.org/abs/2404.16767v1"}
{"created":"2024-04-25 17:19:36","title":"Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation Language Model","abstract":"While supervised fine-tuning (SFT) has been a straightforward approach for tailoring the output of foundation large language model (LLM) to specific preferences, concerns have been raised about the depth of this alignment, with some critiques suggesting it is merely \"superficial\". We critically examine this hypothesis within the scope of cross-lingual generation tasks, proposing that the effectiveness of SFT may be constrained by its reliance on prior tokens to guide cross-lingual generation. Based on this crucial insight, and in response to the challenges posed by the costly and limited availability of non-English data for SFT, we introduce a novel training-free alignment method named PreTTY, which employs minimal task-related prior tokens to bridge the foundation LLM and the SFT LLM, achieving comparable performance without training. Experiments on machine translation and part-of-speech tagging across eight languages demonstrate the efficacy of PreTTY in cross-lingual settings. Remarkably, by initiating the decoding process with only one or two prior tokens, foundation LLMs can achieve performance comparable to their SFT counterparts. This method presents a cost-effective alternative to SFT and advances the democratization of multilingual LLMs.","sentences":["While supervised fine-tuning (SFT) has been a straightforward approach for tailoring the output of foundation large language model (LLM) to specific preferences, concerns have been raised about the depth of this alignment, with some critiques suggesting it is merely \"superficial\".","We critically examine this hypothesis within the scope of cross-lingual generation tasks, proposing that the effectiveness of SFT may be constrained by its reliance on prior tokens to guide cross-lingual generation.","Based on this crucial insight, and in response to the challenges posed by the costly and limited availability of non-English data for SFT, we introduce a novel training-free alignment method named PreTTY, which employs minimal task-related prior tokens to bridge the foundation LLM and the SFT LLM, achieving comparable performance without training.","Experiments on machine translation and part-of-speech tagging across eight languages demonstrate the efficacy of PreTTY in cross-lingual settings.","Remarkably, by initiating the decoding process with only one or two prior tokens, foundation LLMs can achieve performance comparable to their SFT counterparts.","This method presents a cost-effective alternative to SFT and advances the democratization of multilingual LLMs."],"url":"http://arxiv.org/abs/2404.16766v1"}
{"created":"2024-04-25 17:19:13","title":"Dataset of Quotation Attribution in German News Articles","abstract":"Extracting who says what to whom is a crucial part in analyzing human communication in today's abundance of data such as online news articles. Yet, the lack of annotated data for this task in German news articles severely limits the quality and usability of possible systems. To remedy this, we present a new, freely available, creative-commons-licensed dataset for quotation attribution in German news articles based on WIKINEWS. The dataset provides curated, high-quality annotations across 1000 documents (250,000 tokens) in a fine-grained annotation schema enabling various downstream uses for the dataset. The annotations not only specify who said what but also how, in which context, to whom and define the type of quotation. We specify our annotation schema, describe the creation of the dataset and provide a quantitative analysis. Further, we describe suitable evaluation metrics, apply two existing systems for quotation attribution, discuss their results to evaluate the utility of our dataset and outline use cases of our dataset in downstream tasks.","sentences":["Extracting who says what to whom is a crucial part in analyzing human communication in today's abundance of data such as online news articles.","Yet, the lack of annotated data for this task in German news articles severely limits the quality and usability of possible systems.","To remedy this, we present a new, freely available, creative-commons-licensed dataset for quotation attribution in German news articles based on WIKINEWS.","The dataset provides curated, high-quality annotations across 1000 documents (250,000 tokens) in a fine-grained annotation schema enabling various downstream uses for the dataset.","The annotations not only specify who said what but also how, in which context, to whom and define the type of quotation.","We specify our annotation schema, describe the creation of the dataset and provide a quantitative analysis.","Further, we describe suitable evaluation metrics, apply two existing systems for quotation attribution, discuss their results to evaluate the utility of our dataset and outline use cases of our dataset in downstream tasks."],"url":"http://arxiv.org/abs/2404.16764v1"}
{"created":"2024-04-25 17:09:14","title":"TokenHMR: Advancing Human Mesh Recovery with a Tokenized Pose Representation","abstract":"We address the problem of regressing 3D human pose and shape from a single image, with a focus on 3D accuracy. The current best methods leverage large datasets of 3D pseudo-ground-truth (p-GT) and 2D keypoints, leading to robust performance. With such methods, we observe a paradoxical decline in 3D pose accuracy with increasing 2D accuracy. This is caused by biases in the p-GT and the use of an approximate camera projection model. We quantify the error induced by current camera models and show that fitting 2D keypoints and p-GT accurately causes incorrect 3D poses. Our analysis defines the invalid distances within which minimizing 2D and p-GT losses is detrimental. We use this to formulate a new loss Threshold-Adaptive Loss Scaling (TALS) that penalizes gross 2D and p-GT losses but not smaller ones. With such a loss, there are many 3D poses that could equally explain the 2D evidence. To reduce this ambiguity we need a prior over valid human poses but such priors can introduce unwanted bias. To address this, we exploit a tokenized representation of human pose and reformulate the problem as token prediction. This restricts the estimated poses to the space of valid poses, effectively providing a uniform prior. Extensive experiments on the EMDB and 3DPW datasets show that our reformulated keypoint loss and tokenization allows us to train on in-the-wild data while improving 3D accuracy over the state-of-the-art. Our models and code are available for research at https://tokenhmr.is.tue.mpg.de.","sentences":["We address the problem of regressing 3D human pose and shape from a single image, with a focus on 3D accuracy.","The current best methods leverage large datasets of 3D pseudo-ground-truth (p-GT) and 2D keypoints, leading to robust performance.","With such methods, we observe a paradoxical decline in 3D pose accuracy with increasing 2D accuracy.","This is caused by biases in the p-GT and the use of an approximate camera projection model.","We quantify the error induced by current camera models and show that fitting 2D keypoints and p-GT accurately causes incorrect 3D poses.","Our analysis defines the invalid distances within which minimizing 2D and p-GT losses is detrimental.","We use this to formulate a new loss Threshold-Adaptive Loss Scaling (TALS) that penalizes gross 2D and p-GT losses but not smaller ones.","With such a loss, there are many 3D poses that could equally explain the 2D evidence.","To reduce this ambiguity we need a prior over valid human poses but such priors can introduce unwanted bias.","To address this, we exploit a tokenized representation of human pose and reformulate the problem as token prediction.","This restricts the estimated poses to the space of valid poses, effectively providing a uniform prior.","Extensive experiments on the EMDB and 3DPW datasets show that our reformulated keypoint loss and tokenization allows us to train on in-the-wild data while improving 3D accuracy over the state-of-the-art.","Our models and code are available for research at https://tokenhmr.is.tue.mpg.de."],"url":"http://arxiv.org/abs/2404.16752v1"}
{"created":"2024-04-25 16:57:05","title":"Automatic Speech Recognition System-Independent Word Error Rate Estimatio","abstract":"Word error rate (WER) is a metric used to evaluate the quality of transcriptions produced by Automatic Speech Recognition (ASR) systems. In many applications, it is of interest to estimate WER given a pair of a speech utterance and a transcript. Previous work on WER estimation focused on building models that are trained with a specific ASR system in mind (referred to as ASR system-dependent). These are also domain-dependent and inflexible in real-world applications. In this paper, a hypothesis generation method for ASR System-Independent WER estimation (SIWE) is proposed. In contrast to prior work, the WER estimators are trained using data that simulates ASR system output. Hypotheses are generated using phonetically similar or linguistically more likely alternative words. In WER estimation experiments, the proposed method reaches a similar performance to ASR system-dependent WER estimators on in-domain data and achieves state-of-the-art performance on out-of-domain data. On the out-of-domain data, the SIWE model outperformed the baseline estimators in root mean square error and Pearson correlation coefficient by relative 17.58% and 18.21%, respectively, on Switchboard and CALLHOME. The performance was further improved when the WER of the training set was close to the WER of the evaluation dataset.","sentences":["Word error rate (WER) is a metric used to evaluate the quality of transcriptions produced by Automatic Speech Recognition (ASR) systems.","In many applications, it is of interest to estimate WER given a pair of a speech utterance and a transcript.","Previous work on WER estimation focused on building models that are trained with a specific ASR system in mind (referred to as ASR system-dependent).","These are also domain-dependent and inflexible in real-world applications.","In this paper, a hypothesis generation method for ASR System-Independent WER estimation (SIWE) is proposed.","In contrast to prior work, the WER estimators are trained using data that simulates ASR system output.","Hypotheses are generated using phonetically similar or linguistically more likely alternative words.","In WER estimation experiments, the proposed method reaches a similar performance to ASR system-dependent WER estimators on in-domain data and achieves state-of-the-art performance on out-of-domain data.","On the out-of-domain data, the SIWE model outperformed the baseline estimators in root mean square error and Pearson correlation coefficient by relative 17.58% and 18.21%, respectively, on Switchboard and CALLHOME.","The performance was further improved when the WER of the training set was close to the WER of the evaluation dataset."],"url":"http://arxiv.org/abs/2404.16743v1"}
{"created":"2024-04-25 16:50:54","title":"Parameterized Complexity of Efficient Sortation","abstract":"A crucial challenge arising in the design of large-scale logistical networks is to optimize parcel sortation for routing. We study this problem under the recent graph-theoretic formalization of Van Dyk, Klause, Koenemann and Megow (IPCO 2024). The problem asks - given an input digraph D (the fulfillment network) together with a set of commodities represented as source-sink tuples - for a minimum-outdegree subgraph H of the transitive closure of D that contains a source-sink route for each of the commodities. Given the underlying motivation, we study two variants of the problem which differ in whether the routes for the commodities are assumed to be given, or can be chosen arbitrarily.   We perform a thorough parameterized analysis of the complexity of both problems. Our results concentrate on three fundamental parameterizations of the problem: (1) When attempting to parameterize by the target outdegree of H, we show that the problems are paraNP-hard even in highly restricted cases; (2) When parameterizing by the number of commodities, we utilize Ramsey-type arguments, kernelization and treewidth reduction techniques to obtain parameterized algorithms for both problems; (3) When parameterizing by the structure of D, we establish fixed-parameter tractability for both problems w.r.t. treewidth, maximum degree and the maximum routing length. We combine this with lower bounds which show that omitting any of the three parameters results in paraNP-hardness.","sentences":["A crucial challenge arising in the design of large-scale logistical networks is to optimize parcel sortation for routing.","We study this problem under the recent graph-theoretic formalization of Van Dyk, Klause, Koenemann and Megow (IPCO 2024).","The problem asks - given an input digraph D (the fulfillment network) together with a set of commodities represented as source-sink tuples - for a minimum-outdegree subgraph H of the transitive closure of D that contains a source-sink route for each of the commodities.","Given the underlying motivation, we study two variants of the problem which differ in whether the routes for the commodities are assumed to be given, or can be chosen arbitrarily.   ","We perform a thorough parameterized analysis of the complexity of both problems.","Our results concentrate on three fundamental parameterizations of the problem: (1) When attempting to parameterize by the target outdegree of H, we show that the problems are paraNP-hard even in highly restricted cases; (2) When parameterizing by the number of commodities, we utilize Ramsey-type arguments, kernelization and treewidth reduction techniques to obtain parameterized algorithms for both problems; (3) When parameterizing by the structure of D, we establish fixed-parameter tractability for both problems w.r.t",". treewidth, maximum degree and the maximum routing length.","We combine this with lower bounds which show that omitting any of the three parameters results in paraNP-hardness."],"url":"http://arxiv.org/abs/2404.16741v1"}
{"created":"2024-04-25 16:41:12","title":"Finch: Sparse and Structured Array Programming with Control Flow","abstract":"From FORTRAN to NumPy, arrays have revolutionized how we express computation. However, arrays in these, and almost all prominent systems, can only handle dense rectilinear integer grids. Real world arrays often contain underlying structure, such as sparsity, runs of repeated values, or symmetry. Support for structured data is fragmented and incomplete. Existing frameworks limit the array structures and program control flow they support to better simplify the problem.   In this work, we propose a new programming language, Finch, which supports both flexible control flow and diverse data structures. Finch facilitates a programming model which resolves the challenges of computing over structured arrays by combining control flow and data structures into a common representation where they can be co-optimized. Finch automatically specializes control flow to data so that performance engineers can focus on experimenting with many algorithms. Finch supports a familiar programming language of loops, statements, ifs, breaks, etc., over a wide variety of array structures, such as sparsity, run-length-encoding, symmetry, triangles, padding, or blocks. Finch reliably utilizes the key properties of structure, such as structural zeros, repeated values, or clustered non-zeros. We show that this leads to dramatic speedups in operations such as SpMV and SpGEMM, image processing, graph analytics, and a high-level tensor operator fusion interface.","sentences":["From FORTRAN to NumPy, arrays have revolutionized how we express computation.","However, arrays in these, and almost all prominent systems, can only handle dense rectilinear integer grids.","Real world arrays often contain underlying structure, such as sparsity, runs of repeated values, or symmetry.","Support for structured data is fragmented and incomplete.","Existing frameworks limit the array structures and program control flow they support to better simplify the problem.   ","In this work, we propose a new programming language, Finch, which supports both flexible control flow and diverse data structures.","Finch facilitates a programming model which resolves the challenges of computing over structured arrays by combining control flow and data structures into a common representation where they can be co-optimized.","Finch automatically specializes control flow to data so that performance engineers can focus on experimenting with many algorithms.","Finch supports a familiar programming language of loops, statements, ifs, breaks, etc., over a wide variety of array structures, such as sparsity, run-length-encoding, symmetry, triangles, padding, or blocks.","Finch reliably utilizes the key properties of structure, such as structural zeros, repeated values, or clustered non-zeros.","We show that this leads to dramatic speedups in operations such as SpMV and SpGEMM, image processing, graph analytics, and a high-level tensor operator fusion interface."],"url":"http://arxiv.org/abs/2404.16730v1"}
{"created":"2024-04-25 16:38:52","title":"Approximation Algorithms for Hop Constrained and Buy-at-Bulk Network Design via Hop Constrained Oblivious Routing","abstract":"We consider two-cost network design models in which edges of the input graph have an associated cost and length. We build upon recent advances in hop-constrained oblivious routing to obtain two sets of results.   We address multicommodity buy-at-bulk network design in the nonuniform setting. Existing poly-logarithmic approximations are based on the junction tree approach [CHKS09,KN11]. We obtain a new polylogarithmic approximation via a natural LP relaxation. This establishes an upper bound on its integrality gap and affirmatively answers an open question raised in [CHKS09]. The rounding is based on recent results in hop-constrained oblivious routing [GHZ21], and this technique yields a polylogarithmic approximation in more general settings such as set connectivity. Our algorithm for buy-at-bulk network design is based on an LP-based reduction to hop constrained network design for which we obtain LP-based bicriteria approximation algorithms.   We also consider a fault-tolerant version of hop constrained network design where one wants to design a low-cost network to guarantee short paths between a given set of source-sink pairs even when k-1 edges can fail. This model has been considered in network design [GL17,GML18,AJL20] but no approximation algorithms were known. We obtain polylogarithmic bicriteria approximation algorithms for the single-source setting for any fixed k. We build upon the single-source algorithm and the junction-tree approach to obtain an approximation algorithm for the multicommodity setting when at most one edge can fail.","sentences":["We consider two-cost network design models in which edges of the input graph have an associated cost and length.","We build upon recent advances in hop-constrained oblivious routing to obtain two sets of results.   ","We address multicommodity buy-at-bulk network design in the nonuniform setting.","Existing poly-logarithmic approximations are based on the junction tree approach [CHKS09,KN11].","We obtain a new polylogarithmic approximation via a natural LP relaxation.","This establishes an upper bound on its integrality gap and affirmatively answers an open question raised in [CHKS09].","The rounding is based on recent results in hop-constrained oblivious routing [GHZ21], and this technique yields a polylogarithmic approximation in more general settings such as set connectivity.","Our algorithm for buy-at-bulk network design is based on an LP-based reduction to hop constrained network design for which we obtain LP-based bicriteria approximation algorithms.   ","We also consider a fault-tolerant version of hop constrained network design where one wants to design a low-cost network to guarantee short paths between a given set of source-sink pairs even when k-1 edges can fail.","This model has been considered in network design [GL17,GML18,AJL20] but no approximation algorithms were known.","We obtain polylogarithmic bicriteria approximation algorithms for the single-source setting for any fixed k. We build upon the single-source algorithm and the junction-tree approach to obtain an approximation algorithm for the multicommodity setting when at most one edge can fail."],"url":"http://arxiv.org/abs/2404.16725v1"}
{"created":"2024-04-25 16:36:54","title":"Constrained Level Planarity is FPT with Respect to the Vertex Cover Number","abstract":"The problem Level Planarity asks for a crossing-free drawing of a graph in the plane such that vertices are placed at prescribed y-coordinates (called levels) and such that every edge is realized as a y-monotone curve. In the variant Constrained Level Planarity, each level y is equipped with a partial order <_y on its vertices and in the desired drawing the left-to-right order of vertices on level y has to be a linear extension of <_y. Constrained Level Planarity is known to be a remarkably difficult problem: previous results by Klemz and Rote [ACM Trans. Alg. 2019] and by Br\\\"uckner and Rutter [SODA 2017] imply that it remains NP-hard even when restricted to graphs whose tree-depth and feedback vertex set number are bounded by a constant and even when the instances are additionally required to be either proper, meaning that each edge spans two consecutive levels, or ordered, meaning that all given partial orders are total orders. In particular, these results rule out the existence of FPT-time (even XP-time) algorithms with respect to these and related graph parameters (unless P=NP). However, the parameterized complexity of Constrained Level Planarity with respect to the vertex cover number of the input graph remained open.   In this paper, we show that Constrained Level Planarity can be solved in FPT-time when parameterized by the vertex cover number. In view of the previous intractability statements, our result is best-possible in several regards: a speed-up to polynomial time or a generalization to the aforementioned smaller graph parameters is not possible, even if restricting to proper or ordered instances.","sentences":["The problem Level Planarity asks for a crossing-free drawing of a graph in the plane such that vertices are placed at prescribed y-coordinates (called levels) and such that every edge is realized as a y-monotone curve.","In the variant Constrained Level Planarity, each level y is equipped with a partial order <_y on its vertices and in the desired drawing the left-to-right order of vertices on level y has to be a linear extension of <_y. Constrained Level Planarity is known to be a remarkably difficult problem: previous results by Klemz and Rote [ACM Trans.","Alg. 2019]","and by Br\\\"uckner and Rutter [SODA 2017] imply that it remains NP-hard even when restricted to graphs whose tree-depth and feedback vertex set number are bounded by a constant and even when the instances are additionally required to be either proper, meaning that each edge spans two consecutive levels, or ordered, meaning that all given partial orders are total orders.","In particular, these results rule out the existence of FPT-time (even XP-time) algorithms with respect to these and related graph parameters (unless P=NP).","However, the parameterized complexity of Constrained Level Planarity with respect to the vertex cover number of the input graph remained open.   ","In this paper, we show that Constrained Level Planarity can be solved in FPT-time when parameterized by the vertex cover number.","In view of the previous intractability statements, our result is best-possible in several regards: a speed-up to polynomial time or a generalization to the aforementioned smaller graph parameters is not possible, even if restricting to proper or ordered instances."],"url":"http://arxiv.org/abs/2404.16723v1"}
{"created":"2024-04-25 16:33:19","title":"Distilling Privileged Information for Dubins Traveling Salesman Problems with Neighborhoods","abstract":"This paper presents a novel learning approach for Dubins Traveling Salesman Problems(DTSP) with Neighborhood (DTSPN) to quickly produce a tour of a non-holonomic vehicle passing through neighborhoods of given task points. The method involves two learning phases: initially, a model-free reinforcement learning approach leverages privileged information to distill knowledge from expert trajectories generated by the LinKernighan heuristic (LKH) algorithm. Subsequently, a supervised learning phase trains an adaptation network to solve problems independently of privileged information. Before the first learning phase, a parameter initialization technique using the demonstration data was also devised to enhance training efficiency. The proposed learning method produces a solution about 50 times faster than LKH and substantially outperforms other imitation learning and RL with demonstration schemes, most of which fail to sense all the task points.","sentences":["This paper presents a novel learning approach for Dubins Traveling Salesman Problems(DTSP) with Neighborhood (DTSPN) to quickly produce a tour of a non-holonomic vehicle passing through neighborhoods of given task points.","The method involves two learning phases: initially, a model-free reinforcement learning approach leverages privileged information to distill knowledge from expert trajectories generated by the LinKernighan heuristic (LKH) algorithm.","Subsequently, a supervised learning phase trains an adaptation network to solve problems independently of privileged information.","Before the first learning phase, a parameter initialization technique using the demonstration data was also devised to enhance training efficiency.","The proposed learning method produces a solution about 50 times faster than LKH and substantially outperforms other imitation learning and RL with demonstration schemes, most of which fail to sense all the task points."],"url":"http://arxiv.org/abs/2404.16721v1"}
{"created":"2024-04-25 16:20:23","title":"Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding","abstract":"We present LayerSkip, an end-to-end solution to speed-up inference of large language models (LLMs). First, during training we apply layer dropout, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit. Second, during inference, we show that this training recipe increases the accuracy of early exit at earlier layers, without adding any auxiliary layers or modules to the model. Third, we present a novel self-speculative decoding solution where we exit at early layers and verify and correct with remaining layers of the model. Our proposed self-speculative decoding approach has less memory footprint than other speculative decoding approaches and benefits from shared compute and activations of the draft and verification stages. We run experiments on different Llama model sizes on different types of training: pretraining from scratch, continual pretraining, finetuning on specific data domain, and finetuning on specific task. We implement our inference solution and show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x on coding, and 2.0x on TOPv2 semantic parsing task.","sentences":["We present LayerSkip, an end-to-end solution to speed-up inference of large language models (LLMs).","First, during training we apply layer dropout, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit.","Second, during inference, we show that this training recipe increases the accuracy of early exit at earlier layers, without adding any auxiliary layers or modules to the model.","Third, we present a novel self-speculative decoding solution where we exit at early layers and verify and correct with remaining layers of the model.","Our proposed self-speculative decoding approach has less memory footprint than other speculative decoding approaches and benefits from shared compute and activations of the draft and verification stages.","We run experiments on different Llama model sizes on different types of training: pretraining from scratch, continual pretraining, finetuning on specific data domain, and finetuning on specific task.","We implement our inference solution and show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x on coding, and 2.0x on TOPv2 semantic parsing task."],"url":"http://arxiv.org/abs/2404.16710v1"}
{"created":"2024-04-25 16:11:46","title":"Efficient and Near-Optimal Noise Generation for Streaming Differential Privacy","abstract":"In the task of differentially private (DP) continual counting, we receive a stream of increments and our goal is to output an approximate running total of these increments, without revealing too much about any specific increment. Despite its simplicity, differentially private continual counting has attracted significant attention both in theory and in practice. Existing algorithms for differentially private continual counting are either inefficient in terms of their space usage or add an excessive amount of noise, inducing suboptimal utility.   The most practical DP continual counting algorithms add carefully correlated Gaussian noise to the values. The task of choosing the covariance for this noise can be expressed in terms of factoring the lower-triangular matrix of ones (which computes prefix sums). We present two approaches from this class (for different parameter regimes) that achieve near-optimal utility for DP continual counting and only require logarithmic or polylogarithmic space (and time).   Our first approach is based on a space-efficient streaming matrix multiplication algorithm for a class of Toeplitz matrices. We show that to instantiate this algorithm for DP continual counting, it is sufficient to find a low-degree rational function that approximates the square root on a circle in the complex plane. We then apply and extend tools from approximation theory to achieve this. We also derive efficient closed-forms for the objective function for arbitrarily many steps, and show direct numerical optimization yields a highly practical solution to the problem. Our second approach combines our first approach with a recursive construction similar to the binary tree mechanism.","sentences":["In the task of differentially private (DP) continual counting, we receive a stream of increments and our goal is to output an approximate running total of these increments, without revealing too much about any specific increment.","Despite its simplicity, differentially private continual counting has attracted significant attention both in theory and in practice.","Existing algorithms for differentially private continual counting are either inefficient in terms of their space usage or add an excessive amount of noise, inducing suboptimal utility.   ","The most practical DP continual counting algorithms add carefully correlated Gaussian noise to the values.","The task of choosing the covariance for this noise can be expressed in terms of factoring the lower-triangular matrix of ones (which computes prefix sums).","We present two approaches from this class (for different parameter regimes) that achieve near-optimal utility for DP continual counting and only require logarithmic or polylogarithmic space (and time).   ","Our first approach is based on a space-efficient streaming matrix multiplication algorithm for a class of Toeplitz matrices.","We show that to instantiate this algorithm for DP continual counting, it is sufficient to find a low-degree rational function that approximates the square root on a circle in the complex plane.","We then apply and extend tools from approximation theory to achieve this.","We also derive efficient closed-forms for the objective function for arbitrarily many steps, and show direct numerical optimization yields a highly practical solution to the problem.","Our second approach combines our first approach with a recursive construction similar to the binary tree mechanism."],"url":"http://arxiv.org/abs/2404.16706v1"}
{"created":"2024-04-25 16:09:46","title":"SHINE: Social Homology Identification for Navigation in Crowded Environments","abstract":"Navigating mobile robots in social environments remains a challenging task due to the intricacies of human-robot interactions. Most of the motion planners designed for crowded and dynamic environments focus on choosing the best velocity to reach the goal while avoiding collisions, but do not explicitly consider the high-level navigation behavior (avoiding through the left or right side, letting others pass or passing before others, etc.). In this work, we present a novel motion planner that incorporates topology distinct paths representing diverse navigation strategies around humans. The planner selects the topology class that imitates human behavior the best using a deep neural network model trained on real-world human motion data, ensuring socially intelligent and contextually aware navigation. Our system refines the chosen path through an optimization-based local planner in real time, ensuring seamless adherence to desired social behaviors. In this way, we decouple perception and local planning from the decision-making process. We evaluate the prediction accuracy of the network with real-world data. In addition, we assess the navigation capabilities in both simulation and a real-world platform, comparing it with other state-of-the-art planners. We demonstrate that our planner exhibits socially desirable behaviors and shows a smooth and remarkable performance.","sentences":["Navigating mobile robots in social environments remains a challenging task due to the intricacies of human-robot interactions.","Most of the motion planners designed for crowded and dynamic environments focus on choosing the best velocity to reach the goal while avoiding collisions, but do not explicitly consider the high-level navigation behavior (avoiding through the left or right side, letting others pass or passing before others, etc.).","In this work, we present a novel motion planner that incorporates topology distinct paths representing diverse navigation strategies around humans.","The planner selects the topology class that imitates human behavior the best using a deep neural network model trained on real-world human motion data, ensuring socially intelligent and contextually aware navigation.","Our system refines the chosen path through an optimization-based local planner in real time, ensuring seamless adherence to desired social behaviors.","In this way, we decouple perception and local planning from the decision-making process.","We evaluate the prediction accuracy of the network with real-world data.","In addition, we assess the navigation capabilities in both simulation and a real-world platform, comparing it with other state-of-the-art planners.","We demonstrate that our planner exhibits socially desirable behaviors and shows a smooth and remarkable performance."],"url":"http://arxiv.org/abs/2404.16705v1"}
{"created":"2024-04-25 16:02:44","title":"On the Streaming Complexity of Expander Decomposition","abstract":"In this paper we study the problem of finding $(\\epsilon, \\phi)$-expander decompositions of a graph in the streaming model, in particular for dynamic streams of edge insertions and deletions. The goal is to partition the vertex set so that every component induces a $\\phi$-expander, while the number of inter-cluster edges is only an $\\epsilon$ fraction of the total volume. It was recently shown that there exists a simple algorithm to construct a $(O(\\phi \\log n), \\phi)$-expander decomposition of an $n$-vertex graph using $\\widetilde{O}(n/\\phi^2)$ bits of space [Filtser, Kapralov, Makarov, ITCS'23]. This result calls for understanding the extent to which a dependence in space on the sparsity parameter $\\phi$ is inherent. We move towards answering this question on two fronts. We prove that a $(O(\\phi \\log n), \\phi)$-expander decomposition can be found using $\\widetilde{O}(n)$ space, for every $\\phi$. At the core of our result is the first streaming algorithm for computing boundary-linked expander decompositions, a recently introduced strengthening of the classical notion [Goranci et al., SODA'21]. The key advantage is that a classical sparsifier [Fung et al., STOC'11], with size independent of $\\phi$, preserves the cuts inside the clusters of a boundary-linked expander decomposition within a multiplicative error. Notable algorithmic applications use sequences of expander decompositions, in particular one often repeatedly computes a decomposition of the subgraph induced by the inter-cluster edges (e.g., the seminal work of Spielman and Teng on spectral sparsifiers [Spielman, Teng, SIAM Journal of Computing 40(4)], or the recent maximum flow breakthrough [Chen et al., FOCS'22], among others). We prove that any streaming algorithm that computes a sequence of $(O(\\phi \\log n), \\phi)$-expander decompositions requires ${\\widetilde{\\Omega}}(n/\\phi)$ bits of space, even in insertion only streams.","sentences":["In this paper we study the problem of finding $(\\epsilon, \\phi)$-expander decompositions of a graph in the streaming model, in particular for dynamic streams of edge insertions and deletions.","The goal is to partition the vertex set so that every component induces a $\\phi$-expander, while the number of inter-cluster edges is only an $\\epsilon$ fraction of the total volume.","It was recently shown that there exists a simple algorithm to construct a $(O(\\phi \\log n), \\phi)$-expander decomposition of an $n$-vertex graph using $\\widetilde{O}(n/\\phi^2)$ bits of space [Filtser, Kapralov, Makarov, ITCS'23].","This result calls for understanding the extent to which a dependence in space on the sparsity parameter $\\phi$ is inherent.","We move towards answering this question on two fronts.","We prove that a $(O(\\phi \\log n), \\phi)$-expander decomposition can be found using $\\widetilde{O}(n)$ space, for every $\\phi$. At the core of our result is the first streaming algorithm for computing boundary-linked expander decompositions, a recently introduced strengthening of the classical notion","[Goranci et al., SODA'21].","The key advantage is that a classical sparsifier","[Fung et al., STOC'11], with size independent of $\\phi$, preserves the cuts inside the clusters of a boundary-linked expander decomposition within a multiplicative error.","Notable algorithmic applications use sequences of expander decompositions, in particular one often repeatedly computes a decomposition of the subgraph induced by the inter-cluster edges (e.g., the seminal work of Spielman and Teng on spectral sparsifiers [Spielman, Teng, SIAM Journal of Computing 40(4)], or the recent maximum flow breakthrough [Chen et al., FOCS'22], among others).","We prove that any streaming algorithm that computes a sequence of $(O(\\phi \\log n), \\phi)$-expander decompositions requires ${\\widetilde{\\Omega}}(n/\\phi)$ bits of space, even in insertion only streams."],"url":"http://arxiv.org/abs/2404.16701v1"}
{"created":"2024-04-25 15:58:06","title":"Kernelization Dichotomies for Hitting Subgraphs under Structural Parameterizations","abstract":"For a fixed graph $H$, the $H$-SUBGRAPH HITTING problem consists in deleting the minimum number of vertices from an input graph to obtain a graph without any occurrence of $H$ as a subgraph. This problem can be seen as a generalization of VERTEX COVER, which corresponds to the case $H = K_2$. We initiate a study of $H$-SUBGRAPH HITTING from the point of view of characterizing structural parameterizations that allow for polynomial kernels, within the recently active framework of taking as the parameter the number of vertex deletions to obtain a graph in a \"simple\" class $C$. Our main contribution is to identify graph parameters that, when $H$-SUBGRAPH HITTING is parameterized by the vertex-deletion distance to a class $C$ where any of these parameters is bounded, and assuming standard complexity assumptions and that $H$ is biconnected, allow us to prove the following sharp dichotomy: the problem admits a polynomial kernel if and only if $H$ is a clique. These new graph parameters are inspired by the notion of $C$-elimination distance introduced by Bulian and Dawar [Algorithmica 2016], and generalize it in two directions. Our results also apply to the version of the problem where one wants to hit $H$ as an induced subgraph, and imply in particular, that the problems of hitting minors and hitting (induced) subgraphs have a substantially different behavior with respect to the existence of polynomial kernels under structural parameterizations.","sentences":["For a fixed graph $H$, the $H$-SUBGRAPH HITTING problem consists in deleting the minimum number of vertices from an input graph to obtain a graph without any occurrence of $H$ as a subgraph.","This problem can be seen as a generalization of VERTEX COVER, which corresponds to the case $H = K_2$.","We initiate a study of $H$-SUBGRAPH HITTING from the point of view of characterizing structural parameterizations that allow for polynomial kernels, within the recently active framework of taking as the parameter the number of vertex deletions to obtain a graph in a \"simple\" class $C$.","Our main contribution is to identify graph parameters that, when $H$-SUBGRAPH HITTING is parameterized by the vertex-deletion distance to a class $C$ where any of these parameters is bounded, and assuming standard complexity assumptions and that $H$ is biconnected, allow us to prove the following sharp dichotomy: the problem admits a polynomial kernel if and only if $H$ is a clique.","These new graph parameters are inspired by the notion of $C$-elimination distance introduced by Bulian and Dawar","[Algorithmica 2016], and generalize it in two directions.","Our results also apply to the version of the problem where one wants to hit $H$ as an induced subgraph, and imply in particular, that the problems of hitting minors and hitting (induced) subgraphs have a substantially different behavior with respect to the existence of polynomial kernels under structural parameterizations."],"url":"http://arxiv.org/abs/2404.16695v1"}
{"created":"2024-04-25 15:25:30","title":"Multilayer Correlation Clustering","abstract":"In this paper, we establish Multilayer Correlation Clustering, a novel generalization of Correlation Clustering (Bansal et al., FOCS '02) to the multilayer setting. In this model, we are given a series of inputs of Correlation Clustering (called layers) over the common set $V$. The goal is then to find a clustering of $V$ that minimizes the $\\ell_p$-norm ($p\\geq 1$) of the disagreements vector, which is defined as the vector (with dimension equal to the number of layers), each element of which represents the disagreements of the clustering on the corresponding layer. For this generalization, we first design an $O(L\\log n)$-approximation algorithm, where $L$ is the number of layers, based on the well-known region growing technique. We then study an important special case of our problem, namely the problem with the probability constraint. For this case, we first give an $(\\alpha+2)$-approximation algorithm, where $\\alpha$ is any possible approximation ratio for the single-layer counterpart. For instance, we can take $\\alpha=2.5$ in general (Ailon et al., JACM '08) and $\\alpha=1.73+\\epsilon$ for the unweighted case (Cohen-Addad et al., FOCS '23). Furthermore, we design a $4$-approximation algorithm, which improves the above approximation ratio of $\\alpha+2=4.5$ for the general probability-constraint case. Computational experiments using real-world datasets demonstrate the effectiveness of our proposed algorithms.","sentences":["In this paper, we establish Multilayer Correlation Clustering, a novel generalization of Correlation Clustering (Bansal et al., FOCS '02) to the multilayer setting.","In this model, we are given a series of inputs of Correlation Clustering (called layers) over the common set $V$. The goal is then to find a clustering of $V$ that minimizes the $\\ell_p$-norm ($p\\geq 1$) of the disagreements vector, which is defined as the vector (with dimension equal to the number of layers), each element of which represents the disagreements of the clustering on the corresponding layer.","For this generalization, we first design an $O(L\\log n)$-approximation algorithm, where $L$ is the number of layers, based on the well-known region growing technique.","We then study an important special case of our problem, namely the problem with the probability constraint.","For this case, we first give an $(\\alpha+2)$-approximation algorithm, where $\\alpha$ is any possible approximation ratio for the single-layer counterpart.","For instance, we can take $\\alpha=2.5$ in general (Ailon et al., JACM '08) and $\\alpha=1.73+\\epsilon$ for the unweighted case (Cohen-Addad et al., FOCS '23).","Furthermore, we design a $4$-approximation algorithm, which improves the above approximation ratio of $\\alpha+2=4.5$ for the general probability-constraint case.","Computational experiments using real-world datasets demonstrate the effectiveness of our proposed algorithms."],"url":"http://arxiv.org/abs/2404.16676v1"}
{"created":"2024-04-25 15:15:36","title":"EmoVIT: Revolutionizing Emotion Insights with Visual Instruction Tuning","abstract":"Visual Instruction Tuning represents a novel learning paradigm involving the fine-tuning of pre-trained language models using task-specific instructions. This paradigm shows promising zero-shot results in various natural language processing tasks but is still unexplored in vision emotion understanding. In this work, we focus on enhancing the model's proficiency in understanding and adhering to instructions related to emotional contexts. Initially, we identify key visual clues critical to visual emotion recognition. Subsequently, we introduce a novel GPT-assisted pipeline for generating emotion visual instruction data, effectively addressing the scarcity of annotated instruction data in this domain. Expanding on the groundwork established by InstructBLIP, our proposed EmoVIT architecture incorporates emotion-specific instruction data, leveraging the powerful capabilities of Large Language Models to enhance performance. Through extensive experiments, our model showcases its proficiency in emotion classification, adeptness in affective reasoning, and competence in comprehending humor. The comparative analysis provides a robust benchmark for Emotion Visual Instruction Tuning in the era of LLMs, providing valuable insights and opening avenues for future exploration in this domain. Our code is available at \\url{https://github.com/aimmemotion/EmoVIT}.","sentences":["Visual Instruction Tuning represents a novel learning paradigm involving the fine-tuning of pre-trained language models using task-specific instructions.","This paradigm shows promising zero-shot results in various natural language processing tasks but is still unexplored in vision emotion understanding.","In this work, we focus on enhancing the model's proficiency in understanding and adhering to instructions related to emotional contexts.","Initially, we identify key visual clues critical to visual emotion recognition.","Subsequently, we introduce a novel GPT-assisted pipeline for generating emotion visual instruction data, effectively addressing the scarcity of annotated instruction data in this domain.","Expanding on the groundwork established by InstructBLIP, our proposed EmoVIT architecture incorporates emotion-specific instruction data, leveraging the powerful capabilities of Large Language Models to enhance performance.","Through extensive experiments, our model showcases its proficiency in emotion classification, adeptness in affective reasoning, and competence in comprehending humor.","The comparative analysis provides a robust benchmark for Emotion Visual Instruction Tuning in the era of LLMs, providing valuable insights and opening avenues for future exploration in this domain.","Our code is available at \\url{https://github.com/aimmemotion/EmoVIT}."],"url":"http://arxiv.org/abs/2404.16670v1"}
{"created":"2024-04-25 15:00:34","title":"Computing Hamiltonian Paths with Partial Order Restrictions","abstract":"When solving the Hamiltonian path problem it seems natural to be given additional precedence constraints for the order in which the vertices are visited. For example one could decide whether a Hamiltonian path exists for a fixed starting point, or that some vertices are visited before another vertex. We consider the problem of finding a Hamiltonian path that observes all precedence constraints given in a partial order on the vertex set. We show that this problem is $\\mathsf{NP}$-complete even if restricted to complete bipartite graphs and posets of height 2. In contrast, for posets of width $k$ there is an $\\mathcal{O}(k^2 n^k)$ algorithm for arbitrary graphs with $n$ vertices. We show that it is unlikely that the running time of this algorithm can be improved significantly, i.e., there is no $f(k) n^{o(k)}$ time algorithm under the assumption of the Exponential Time Hypothesis. Furthermore, for the class of outerplanar graphs, we give an $\\mathcal{O}(n^2)$ algorithm for arbitrary posets.","sentences":["When solving the Hamiltonian path problem it seems natural to be given additional precedence constraints for the order in which the vertices are visited.","For example one could decide whether a Hamiltonian path exists for a fixed starting point, or that some vertices are visited before another vertex.","We consider the problem of finding a Hamiltonian path that observes all precedence constraints given in a partial order on the vertex set.","We show that this problem is $\\mathsf{NP}$-complete even if restricted to complete bipartite graphs and posets of height 2.","In contrast, for posets of width $k$ there is an $\\mathcal{O}(k^2 n^k)$ algorithm for arbitrary graphs with $n$ vertices.","We show that it is unlikely that the running time of this algorithm can be improved significantly, i.e., there is no $f(k) n^{o(k)}$ time algorithm under the assumption of the Exponential Time Hypothesis.","Furthermore, for the class of outerplanar graphs, we give an $\\mathcal{O}(n^2)$ algorithm for arbitrary posets."],"url":"http://arxiv.org/abs/2404.16662v1"}
{"created":"2024-04-25 14:48:29","title":"A Self-Organizing Clustering System for Unsupervised Distribution Shift Detection","abstract":"Modeling non-stationary data is a challenging problem in the field of continual learning, and data distribution shifts may result in negative consequences on the performance of a machine learning model. Classic learning tools are often vulnerable to perturbations of the input covariates, and are sensitive to outliers and noise, and some tools are based on rigid algebraic assumptions. Distribution shifts are frequently occurring due to changes in raw materials for production, seasonality, a different user base, or even adversarial attacks. Therefore, there is a need for more effective distribution shift detection techniques.   In this work, we propose a continual learning framework for monitoring and detecting distribution changes. We explore the problem in a latent space generated by a bio-inspired self-organizing clustering and statistical aspects of the latent space. In particular, we investigate the projections made by two topology-preserving maps: the Self-Organizing Map and the Scale Invariant Map. Our method can be applied in both a supervised and an unsupervised context. We construct the assessment of changes in the data distribution as a comparison of Gaussian signals, making the proposed method fast and robust. We compare it to other unsupervised techniques, specifically Principal Component Analysis (PCA) and Kernel-PCA. Our comparison involves conducting experiments using sequences of images (based on MNIST and injected shifts with adversarial samples), chemical sensor measurements, and the environmental variable related to ozone levels. The empirical study reveals the potential of the proposed approach.","sentences":["Modeling non-stationary data is a challenging problem in the field of continual learning, and data distribution shifts may result in negative consequences on the performance of a machine learning model.","Classic learning tools are often vulnerable to perturbations of the input covariates, and are sensitive to outliers and noise, and some tools are based on rigid algebraic assumptions.","Distribution shifts are frequently occurring due to changes in raw materials for production, seasonality, a different user base, or even adversarial attacks.","Therefore, there is a need for more effective distribution shift detection techniques.   ","In this work, we propose a continual learning framework for monitoring and detecting distribution changes.","We explore the problem in a latent space generated by a bio-inspired self-organizing clustering and statistical aspects of the latent space.","In particular, we investigate the projections made by two topology-preserving maps: the Self-Organizing Map and the Scale Invariant Map.","Our method can be applied in both a supervised and an unsupervised context.","We construct the assessment of changes in the data distribution as a comparison of Gaussian signals, making the proposed method fast and robust.","We compare it to other unsupervised techniques, specifically Principal Component Analysis (PCA) and Kernel-PCA.","Our comparison involves conducting experiments using sequences of images (based on MNIST and injected shifts with adversarial samples), chemical sensor measurements, and the environmental variable related to ozone levels.","The empirical study reveals the potential of the proposed approach."],"url":"http://arxiv.org/abs/2404.16656v1"}
{"created":"2024-04-25 14:26:53","title":"Privacy-Preserving Statistical Data Generation: Application to Sepsis Detection","abstract":"The biomedical field is among the sectors most impacted by the increasing regulation of Artificial Intelligence (AI) and data protection legislation, given the sensitivity of patient information. However, the rise of synthetic data generation methods offers a promising opportunity for data-driven technologies. In this study, we propose a statistical approach for synthetic data generation applicable in classification problems. We assess the utility and privacy implications of synthetic data generated by Kernel Density Estimator and K-Nearest Neighbors sampling (KDE-KNN) within a real-world context, specifically focusing on its application in sepsis detection. The detection of sepsis is a critical challenge in clinical practice due to its rapid progression and potentially life-threatening consequences. Moreover, we emphasize the benefits of KDE-KNN compared to current synthetic data generation methodologies. Additionally, our study examines the effects of incorporating synthetic data into model training procedures. This investigation provides valuable insights into the effectiveness of synthetic data generation techniques in mitigating regulatory constraints within the biomedical field.","sentences":["The biomedical field is among the sectors most impacted by the increasing regulation of Artificial Intelligence (AI) and data protection legislation, given the sensitivity of patient information.","However, the rise of synthetic data generation methods offers a promising opportunity for data-driven technologies.","In this study, we propose a statistical approach for synthetic data generation applicable in classification problems.","We assess the utility and privacy implications of synthetic data generated by Kernel Density Estimator and K-Nearest Neighbors sampling (KDE-KNN) within a real-world context, specifically focusing on its application in sepsis detection.","The detection of sepsis is a critical challenge in clinical practice due to its rapid progression and potentially life-threatening consequences.","Moreover, we emphasize the benefits of KDE-KNN compared to current synthetic data generation methodologies.","Additionally, our study examines the effects of incorporating synthetic data into model training procedures.","This investigation provides valuable insights into the effectiveness of synthetic data generation techniques in mitigating regulatory constraints within the biomedical field."],"url":"http://arxiv.org/abs/2404.16638v1"}
{"created":"2024-04-25 14:24:41","title":"Zero-Shot Distillation for Image Encoders: How to Make Effective Use of Synthetic Data","abstract":"Multi-modal foundation models such as CLIP have showcased impressive zero-shot capabilities. However, their applicability in resource-constrained environments is limited due to their large number of parameters and high inference time. While existing approaches have scaled down the entire CLIP architecture, we focus on training smaller variants of the image encoder, which suffices for efficient zero-shot classification. The use of synthetic data has shown promise in distilling representations from larger teachers, resulting in strong few-shot and linear probe performance. However, we find that this approach surprisingly fails in true zero-shot settings when using contrastive losses. We identify the exploitation of spurious features as being responsible for poor generalization between synthetic and real data. However, by using the image feature-based L2 distillation loss, we mitigate these problems and train students that achieve zero-shot performance which on four domain-specific datasets is on-par with a ViT-B/32 teacher model trained on DataCompXL, while featuring up to 92% fewer parameters.","sentences":["Multi-modal foundation models such as CLIP have showcased impressive zero-shot capabilities.","However, their applicability in resource-constrained environments is limited due to their large number of parameters and high inference time.","While existing approaches have scaled down the entire CLIP architecture, we focus on training smaller variants of the image encoder, which suffices for efficient zero-shot classification.","The use of synthetic data has shown promise in distilling representations from larger teachers, resulting in strong few-shot and linear probe performance.","However, we find that this approach surprisingly fails in true zero-shot settings when using contrastive losses.","We identify the exploitation of spurious features as being responsible for poor generalization between synthetic and real data.","However, by using the image feature-based L2 distillation loss, we mitigate these problems and train students that achieve zero-shot performance which on four domain-specific datasets is on-par with a ViT-B/32 teacher model trained on DataCompXL, while featuring up to 92% fewer parameters."],"url":"http://arxiv.org/abs/2404.16637v1"}
{"created":"2024-04-25 14:23:24","title":"TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning","abstract":"Charts are important for presenting and explaining complex data relationships. Recently, multimodal large language models (MLLMs) have shown remarkable capabilities in various chart understanding tasks. However, the sheer size of these models in terms of parameters and computational requirements limits their use in resource-constrained environments. In this paper, we present TinyChart, an efficient MLLM for chart understanding with only 3B parameters. TinyChart overcomes two key challenges in efficient chart understanding: (1) reduce the burden of learning numerical computations through a Program-of-Thoughts (PoT) learning strategy, which trains the model to generate Python programs for numerical calculations, and (2) reduce lengthy vision feature sequences produced by the vision transformer for high-resolution images through a Vision Token Merging module, which gradually merges most similar vision tokens. Extensive experiments demonstrate that our 3B TinyChart achieves SOTA performance on a variety of chart understanding benchmarks including ChartQA, Chart-to-Text, Chart-to-Table, OpenCQA, and ChartX. It outperforms several chart understanding MLLM with up to 13B parameters such as ChartLlama and ChartAst, and close-sourced general-purpose MLLM GPT-4V on ChartQA. It also demonstrates its superior efficiency with higher throughput during inference due to a smaller model scale and more efficient vision encoding. Our code and model are available at https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/TinyChart.","sentences":["Charts are important for presenting and explaining complex data relationships.","Recently, multimodal large language models (MLLMs) have shown remarkable capabilities in various chart understanding tasks.","However, the sheer size of these models in terms of parameters and computational requirements limits their use in resource-constrained environments.","In this paper, we present TinyChart, an efficient MLLM for chart understanding with only 3B parameters.","TinyChart overcomes two key challenges in efficient chart understanding: (1) reduce the burden of learning numerical computations through a Program-of-Thoughts (PoT) learning strategy, which trains the model to generate Python programs for numerical calculations, and (2) reduce lengthy vision feature sequences produced by the vision transformer for high-resolution images through a Vision Token Merging module, which gradually merges most similar vision tokens.","Extensive experiments demonstrate that our 3B TinyChart achieves SOTA performance on a variety of chart understanding benchmarks including ChartQA, Chart-to-Text, Chart-to-Table, OpenCQA, and ChartX. It outperforms several chart understanding MLLM with up to 13B parameters such as ChartLlama and ChartAst, and close-sourced general-purpose MLLM GPT-4V on ChartQA.","It also demonstrates its superior efficiency with higher throughput during inference due to a smaller model scale and more efficient vision encoding.","Our code and model are available at https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/TinyChart."],"url":"http://arxiv.org/abs/2404.16635v1"}
{"created":"2024-04-25 14:17:34","title":"Legal Aspects for Software Developers Interested in Generative AI Applications","abstract":"Recent successes in Generative Artificial Intelligence (GenAI) have led to new technologies capable of generating high-quality code, natural language, and images. The next step is to integrate GenAI technology into products, a task typically conducted by software developers. Such product development always comes with a certain risk of liability. Within this article, we want to shed light on the current state of two such risks: data protection and copyright. Both aspects are crucial for GenAI. This technology deals with data for both model training and generated output. We summarize key aspects regarding our current knowledge that every software developer involved in product development using GenAI should be aware of to avoid critical mistakes that may expose them to liability claims.","sentences":["Recent successes in Generative Artificial Intelligence (GenAI) have led to new technologies capable of generating high-quality code, natural language, and images.","The next step is to integrate GenAI technology into products, a task typically conducted by software developers.","Such product development always comes with a certain risk of liability.","Within this article, we want to shed light on the current state of two such risks: data protection and copyright.","Both aspects are crucial for GenAI.","This technology deals with data for both model training and generated output.","We summarize key aspects regarding our current knowledge that every software developer involved in product development using GenAI should be aware of to avoid critical mistakes that may expose them to liability claims."],"url":"http://arxiv.org/abs/2404.16630v1"}
{"created":"2024-04-25 14:09:22","title":"Development of parallel programs on shared data-structures -- Revised version","abstract":"A syntax-directed formal system for the development of totally correct programs with respect to an unfair shared-state parallel while-language is proposed. The system can be understood as a compositional reformulation of the Owicki/Gries method for verification of parallel programs. Auxiliary variables are used both as a specification tool to eliminate undesirable implementations, and as a verification tool to make it possible to prove that an already finished program satisfies a particular specification. Auxiliary variables may be of any sort, and it is up to the user to define the auxiliary structure he prefers. Moreover, the auxiliary structure is only a part of the logic. This means that auxiliary variables do not have to be implemented as if they were ordinary programming variables. The system is proved sound and relatively complete with respect to an operational semantics and employed to develop three nontrivial algorithms: the Dining-Philosophers, the Bubble-Lattice-Sort and the Set-Partition algorithms. Finally, a related method for the development of (possibly nonterminating) programs with respect to four properties is described. This approach is then used to develop Dekker's algorithm.","sentences":["A syntax-directed formal system for the development of totally correct programs with respect to an unfair shared-state parallel while-language is proposed.","The system can be understood as a compositional reformulation of the Owicki/Gries method for verification of parallel programs.","Auxiliary variables are used both as a specification tool to eliminate undesirable implementations, and as a verification tool to make it possible to prove that an already finished program satisfies a particular specification.","Auxiliary variables may be of any sort, and it is up to the user to define the auxiliary structure he prefers.","Moreover, the auxiliary structure is only a part of the logic.","This means that auxiliary variables do not have to be implemented as if they were ordinary programming variables.","The system is proved sound and relatively complete with respect to an operational semantics and employed to develop three nontrivial algorithms: the Dining-Philosophers, the Bubble-Lattice-Sort and the Set-Partition algorithms.","Finally, a related method for the development of (possibly nonterminating) programs with respect to four properties is described.","This approach is then used to develop Dekker's algorithm."],"url":"http://arxiv.org/abs/2404.16624v1"}
{"created":"2024-04-25 14:08:02","title":"Layered List Labeling","abstract":"The list-labeling problem is one of the most basic and well-studied algorithmic primitives in data structures, with an extensive literature spanning upper bounds, lower bounds, and data management applications. The classical algorithm for this problem, dating back to 1981, has amortized cost $O(\\log^2 n)$. Subsequent work has led to improvements in three directions: \\emph{low-latency} (worst-case) bounds; \\emph{high-throughput} (expected) bounds; and (adaptive) bounds for \\emph{important workloads}.   Perhaps surprisingly, these three directions of research have remained almost entirely disjoint -- this is because, so far, the techniques that allow for progress in one direction have forced worsening bounds in the others. Thus there would appear to be a tension between worst-case, adaptive, and expected bounds. List labeling has been proposed for use in databases at least as early as PODS'99, but a database needs good throughput, response time, and needs to adapt to common workloads (e.g., bulk loads), and no current list-labeling algorithm achieve good bounds for all three.   We show that this tension is not fundamental. In fact, with the help of new data-structural techniques, one can actually \\emph{combine} any three list-labeling solutions in order to cherry-pick the best worst-case, adaptive, and expected bounds from each of them.","sentences":["The list-labeling problem is one of the most basic and well-studied algorithmic primitives in data structures, with an extensive literature spanning upper bounds, lower bounds, and data management applications.","The classical algorithm for this problem, dating back to 1981, has amortized cost $O(\\log^2 n)$.","Subsequent work has led to improvements in three directions: \\emph{low-latency} (worst-case) bounds; \\emph{high-throughput} (expected) bounds; and (adaptive) bounds for \\emph{important workloads}.   ","Perhaps surprisingly, these three directions of research have remained almost entirely disjoint -- this is because, so far, the techniques that allow for progress in one direction have forced worsening bounds in the others.","Thus there would appear to be a tension between worst-case, adaptive, and expected bounds.","List labeling has been proposed for use in databases at least as early as PODS'99, but a database needs good throughput, response time, and needs to adapt to common workloads (e.g., bulk loads), and no current list-labeling algorithm achieve good bounds for all three.   ","We show that this tension is not fundamental.","In fact, with the help of new data-structural techniques, one can actually \\emph{combine} any three list-labeling solutions in order to cherry-pick the best worst-case, adaptive, and expected bounds from each of them."],"url":"http://arxiv.org/abs/2404.16623v1"}
{"created":"2024-04-25 14:02:25","title":"The THU-HCSI Multi-Speaker Multi-Lingual Few-Shot Voice Cloning System for LIMMITS'24 Challenge","abstract":"This paper presents the multi-speaker multi-lingual few-shot voice cloning system developed by THU-HCSI team for LIMMITS'24 Challenge. To achieve high speaker similarity and naturalness in both mono-lingual and cross-lingual scenarios, we build the system upon YourTTS and add several enhancements. For further improving speaker similarity and speech quality, we introduce speaker-aware text encoder and flow-based decoder with Transformer blocks. In addition, we denoise the few-shot data, mix up them with pre-training data, and adopt a speaker-balanced sampling strategy to guarantee effective fine-tuning for target speakers. The official evaluations in track 1 show that our system achieves the best speaker similarity MOS of 4.25 and obtains considerable naturalness MOS of 3.97.","sentences":["This paper presents the multi-speaker multi-lingual few-shot voice cloning system developed by THU-HCSI team for LIMMITS'24 Challenge.","To achieve high speaker similarity and naturalness in both mono-lingual and cross-lingual scenarios, we build the system upon YourTTS and add several enhancements.","For further improving speaker similarity and speech quality, we introduce speaker-aware text encoder and flow-based decoder with Transformer blocks.","In addition, we denoise the few-shot data, mix up them with pre-training data, and adopt a speaker-balanced sampling strategy to guarantee effective fine-tuning for target speakers.","The official evaluations in track 1 show that our system achieves the best speaker similarity MOS of 4.25 and obtains considerable","naturalness MOS of 3.97."],"url":"http://arxiv.org/abs/2404.16619v1"}
{"created":"2024-04-25 13:56:05","title":"Robust Capped lp-Norm Support Vector Ordinal Regression","abstract":"Ordinal regression is a specialized supervised problem where the labels show an inherent order. The order distinguishes it from normal multi-class problem. Support Vector Ordinal Regression, as an outstanding ordinal regression model, is widely used in many ordinal regression tasks. However, like most supervised learning algorithms, the design of SVOR is based on the assumption that the training data are real and reliable, which is difficult to satisfy in real-world data. In many practical applications, outliers are frequently present in the training set, potentially leading to misguide the learning process, such that the performance is non-optimal. In this paper, we propose a novel capped $\\ell_{p}$-norm loss function that is theoretically robust to both light and heavy outliers. The capped $\\ell_{p}$-norm loss can help the model detect and eliminate outliers during training process. Adhering to this concept, we introduce a new model, Capped $\\ell_{p}$-Norm Support Vector Ordinal Regression(CSVOR), that is robust to outliers. CSVOR uses a weight matrix to detect and eliminate outliers during the training process to improve the robustness to outliers. Moreover, a Re-Weighted algorithm algorithm which is illustrated convergence by our theoretical results is proposed to effectively minimize the corresponding problem. Extensive experimental results demonstrate that our model outperforms state-of-the-art(SOTA) methods, particularly in the presence of outliers.","sentences":["Ordinal regression is a specialized supervised problem where the labels show an inherent order.","The order distinguishes it from normal multi-class problem.","Support Vector Ordinal Regression, as an outstanding ordinal regression model, is widely used in many ordinal regression tasks.","However, like most supervised learning algorithms, the design of SVOR is based on the assumption that the training data are real and reliable, which is difficult to satisfy in real-world data.","In many practical applications, outliers are frequently present in the training set, potentially leading to misguide the learning process, such that the performance is non-optimal.","In this paper, we propose a novel capped $\\ell_{p}$-norm loss function that is theoretically robust to both light and heavy outliers.","The capped $\\ell_{p}$-norm loss can help the model detect and eliminate outliers during training process.","Adhering to this concept, we introduce a new model, Capped $\\ell_{p}$-Norm Support Vector Ordinal Regression(CSVOR), that is robust to outliers.","CSVOR uses a weight matrix to detect and eliminate outliers during the training process to improve the robustness to outliers.","Moreover, a Re-Weighted algorithm algorithm which is illustrated convergence by our theoretical results is proposed to effectively minimize the corresponding problem.","Extensive experimental results demonstrate that our model outperforms state-of-the-art(SOTA) methods, particularly in the presence of outliers."],"url":"http://arxiv.org/abs/2404.16616v1"}
{"created":"2024-04-25 12:36:19","title":"Exploring Internal Numeracy in Language Models: A Case Study on ALBERT","abstract":"It has been found that Transformer-based language models have the ability to perform basic quantitative reasoning. In this paper, we propose a method for studying how these models internally represent numerical data, and use our proposal to analyze the ALBERT family of language models. Specifically, we extract the learned embeddings these models use to represent tokens that correspond to numbers and ordinals, and subject these embeddings to Principal Component Analysis (PCA). PCA results reveal that ALBERT models of different sizes, trained and initialized separately, consistently learn to use the axes of greatest variation to represent the approximate ordering of various numerical concepts. Numerals and their textual counterparts are represented in separate clusters, but increase along the same direction in 2D space. Our findings illustrate that language models, trained purely to model text, can intuit basic mathematical concepts, opening avenues for NLP applications that intersect with quantitative reasoning.","sentences":["It has been found that Transformer-based language models have the ability to perform basic quantitative reasoning.","In this paper, we propose a method for studying how these models internally represent numerical data, and use our proposal to analyze the ALBERT family of language models.","Specifically, we extract the learned embeddings these models use to represent tokens that correspond to numbers and ordinals, and subject these embeddings to Principal Component Analysis (PCA).","PCA results reveal that ALBERT models of different sizes, trained and initialized separately, consistently learn to use the axes of greatest variation to represent the approximate ordering of various numerical concepts.","Numerals and their textual counterparts are represented in separate clusters, but increase along the same direction in 2D space.","Our findings illustrate that language models, trained purely to model text, can intuit basic mathematical concepts, opening avenues for NLP applications that intersect with quantitative reasoning."],"url":"http://arxiv.org/abs/2404.16574v1"}
{"created":"2024-04-25 12:24:37","title":"Evaluating Large Language Models on Time Series Feature Understanding: A Comprehensive Taxonomy and Benchmark","abstract":"Large Language Models (LLMs) offer the potential for automatic time series analysis and reporting, which is a critical task across many domains, spanning healthcare, finance, climate, energy, and many more. In this paper, we propose a framework for rigorously evaluating the capabilities of LLMs on time series understanding, encompassing both univariate and multivariate forms. We introduce a comprehensive taxonomy of time series features, a critical framework that delineates various characteristics inherent in time series data. Leveraging this taxonomy, we have systematically designed and synthesized a diverse dataset of time series, embodying the different outlined features. This dataset acts as a solid foundation for assessing the proficiency of LLMs in comprehending time series. Our experiments shed light on the strengths and limitations of state-of-the-art LLMs in time series understanding, revealing which features these models readily comprehend effectively and where they falter. In addition, we uncover the sensitivity of LLMs to factors including the formatting of the data, the position of points queried within a series and the overall time series length.","sentences":["Large Language Models (LLMs) offer the potential for automatic time series analysis and reporting, which is a critical task across many domains, spanning healthcare, finance, climate, energy, and many more.","In this paper, we propose a framework for rigorously evaluating the capabilities of LLMs on time series understanding, encompassing both univariate and multivariate forms.","We introduce a comprehensive taxonomy of time series features, a critical framework that delineates various characteristics inherent in time series data.","Leveraging this taxonomy, we have systematically designed and synthesized a diverse dataset of time series, embodying the different outlined features.","This dataset acts as a solid foundation for assessing the proficiency of LLMs in comprehending time series.","Our experiments shed light on the strengths and limitations of state-of-the-art LLMs in time series understanding, revealing which features these models readily comprehend effectively and where they falter.","In addition, we uncover the sensitivity of LLMs to factors including the formatting of the data, the position of points queried within a series and the overall time series length."],"url":"http://arxiv.org/abs/2404.16563v1"}
{"created":"2024-04-25 12:18:04","title":"Research on geometric figure classification algorithm based on Deep Learning","abstract":"In recent years, with the rapid development of computer information technology, the development of artificial intelligence has been accelerating. The traditional geometry recognition technology is relatively backward and the recognition rate is low. In the face of massive information database, the traditional algorithm model inevitably has the problems of low recognition accuracy and poor performance. Deep learning theory has gradually become a very important part of machine learning. The implementation of convolutional neural network (CNN) reduces the difficulty of graphics generation algorithm. In this paper, using the advantages of lenet-5 architecture sharing weights and feature extraction and classification, the proposed geometric pattern recognition algorithm model is faster in the training data set. By constructing the shared feature parameters of the algorithm model, the cross-entropy loss function is used in the recognition process to improve the generalization of the model and improve the average recognition accuracy of the test data set.","sentences":["In recent years, with the rapid development of computer information technology, the development of artificial intelligence has been accelerating.","The traditional geometry recognition technology is relatively backward and the recognition rate is low.","In the face of massive information database, the traditional algorithm model inevitably has the problems of low recognition accuracy and poor performance.","Deep learning theory has gradually become a very important part of machine learning.","The implementation of convolutional neural network (CNN) reduces the difficulty of graphics generation algorithm.","In this paper, using the advantages of lenet-5 architecture sharing weights and feature extraction and classification, the proposed geometric pattern recognition algorithm model is faster in the training data set.","By constructing the shared feature parameters of the algorithm model, the cross-entropy loss function is used in the recognition process to improve the generalization of the model and improve the average recognition accuracy of the test data set."],"url":"http://arxiv.org/abs/2404.16561v1"}
{"created":"2024-04-25 12:11:28","title":"Conditional Distribution Modelling for Few-Shot Image Synthesis with Diffusion Models","abstract":"Few-shot image synthesis entails generating diverse and realistic images of novel categories using only a few example images. While multiple recent efforts in this direction have achieved impressive results, the existing approaches are dependent only upon the few novel samples available at test time in order to generate new images, which restricts the diversity of the generated images. To overcome this limitation, we propose Conditional Distribution Modelling (CDM) -- a framework which effectively utilizes Diffusion models for few-shot image generation. By modelling the distribution of the latent space used to condition a Diffusion process, CDM leverages the learnt statistics of the training data to get a better approximation of the unseen class distribution, thereby removing the bias arising due to limited number of few shot samples. Simultaneously, we devise a novel inversion based optimization strategy that further improves the approximated unseen class distribution, and ensures the fidelity of the generated samples to the unseen class. The experimental results on four benchmark datasets demonstrate the effectiveness of our proposed CDM for few-shot generation.","sentences":["Few-shot image synthesis entails generating diverse and realistic images of novel categories using only a few example images.","While multiple recent efforts in this direction have achieved impressive results, the existing approaches are dependent only upon the few novel samples available at test time in order to generate new images, which restricts the diversity of the generated images.","To overcome this limitation, we propose Conditional Distribution Modelling (CDM) -- a framework which effectively utilizes Diffusion models for few-shot image generation.","By modelling the distribution of the latent space used to condition a Diffusion process, CDM leverages the learnt statistics of the training data to get a better approximation of the unseen class distribution, thereby removing the bias arising due to limited number of few shot samples.","Simultaneously, we devise a novel inversion based optimization strategy that further improves the approximated unseen class distribution, and ensures the fidelity of the generated samples to the unseen class.","The experimental results on four benchmark datasets demonstrate the effectiveness of our proposed CDM for few-shot generation."],"url":"http://arxiv.org/abs/2404.16556v1"}
{"created":"2024-04-25 12:09:17","title":"RE-RecSys: An End-to-End system for recommending properties in Real-Estate domain","abstract":"We propose an end-to-end real-estate recommendation system, RE-RecSys, which has been productionized in real-world industry setting. We categorize any user into 4 categories based on available historical data: i) cold-start users; ii) short-term users; iii) long-term users; and iv) short-long term users. For cold-start users, we propose a novel rule-based engine that is based on the popularity of locality and user preferences. For short-term users, we propose to use content-filtering model which recommends properties based on recent interactions of users. For long-term and short-long term users, we propose a novel combination of content and collaborative filtering based approach which can be easily productionized in the real-world scenario. Moreover, based on the conversion rate, we have designed a novel weighing scheme for different impressions done by users on the platform for the training of content and collaborative models. Finally, we show the efficiency of the proposed pipeline, RE-RecSys, on a real-world property and clickstream dataset collected from leading real-estate platform in India. We show that the proposed pipeline is deployable in real-world scenario with an average latency of <40 ms serving 1000 rpm.","sentences":["We propose an end-to-end real-estate recommendation system, RE-RecSys, which has been productionized in real-world industry setting.","We categorize any user into 4 categories based on available historical data: i) cold-start users; ii) short-term users; iii) long-term users; and iv) short-long term users.","For cold-start users, we propose a novel rule-based engine that is based on the popularity of locality and user preferences.","For short-term users, we propose to use content-filtering model which recommends properties based on recent interactions of users.","For long-term and short-long term users, we propose a novel combination of content and collaborative filtering based approach which can be easily productionized in the real-world scenario.","Moreover, based on the conversion rate, we have designed a novel weighing scheme for different impressions done by users on the platform for the training of content and collaborative models.","Finally, we show the efficiency of the proposed pipeline, RE-RecSys, on a real-world property and clickstream dataset collected from leading real-estate platform in India.","We show that the proposed pipeline is deployable in real-world scenario with an average latency of <40 ms serving 1000 rpm."],"url":"http://arxiv.org/abs/2404.16553v1"}
{"created":"2024-04-25 12:07:41","title":"Surprisingly Strong Performance Prediction with Neural Graph Features","abstract":"Performance prediction has been a key part of the neural architecture search (NAS) process, allowing to speed up NAS algorithms by avoiding resource-consuming network training. Although many performance predictors correlate well with ground truth performance, they require training data in the form of trained networks. Recently, zero-cost proxies have been proposed as an efficient method to estimate network performance without any training. However, they are still poorly understood, exhibit biases with network properties, and their performance is limited. Inspired by the drawbacks of zero-cost proxies, we propose neural graph features (GRAF), simple to compute properties of architectural graphs. GRAF offers fast and interpretable performance prediction while outperforming zero-cost proxies and other common encodings. In combination with other zero-cost proxies, GRAF outperforms most existing performance predictors at a fraction of the cost.","sentences":["Performance prediction has been a key part of the neural architecture search (NAS) process, allowing to speed up NAS algorithms by avoiding resource-consuming network training.","Although many performance predictors correlate well with ground truth performance, they require training data in the form of trained networks.","Recently, zero-cost proxies have been proposed as an efficient method to estimate network performance without any training.","However, they are still poorly understood, exhibit biases with network properties, and their performance is limited.","Inspired by the drawbacks of zero-cost proxies, we propose neural graph features (GRAF), simple to compute properties of architectural graphs.","GRAF offers fast and interpretable performance prediction while outperforming zero-cost proxies and other common encodings.","In combination with other zero-cost proxies, GRAF outperforms most existing performance predictors at a fraction of the cost."],"url":"http://arxiv.org/abs/2404.16551v1"}
{"created":"2024-04-25 12:04:36","title":"Application of Long-Short Term Memory and Convolutional Neural Networks for Real-Time Bridge Scour Forecast","abstract":"Scour around bridge piers is a critical challenge for infrastructures around the world. In the absence of analytical models and due to the complexity of the scour process, it is difficult for current empirical methods to achieve accurate predictions. In this paper, we exploit the power of deep learning algorithms to forecast the scour depth variations around bridge piers based on historical sensor monitoring data, including riverbed elevation, flow elevation, and flow velocity. We investigated the performance of Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN) models for real-time scour forecasting using data collected from bridges in Alaska and Oregon from 2006 to 2021. The LSTM models achieved mean absolute error (MAE) ranging from 0.1m to 0.5m for predicting bed level variations a week in advance, showing a reasonable performance. The Fully Convolutional Network (FCN) variant of CNN outperformed other CNN configurations, showing a comparable performance to LSTMs with significantly lower computational costs. We explored various innovative random-search heuristics for hyperparameter tuning and model optimisation which resulted in reduced computational cost compared to grid-search method. The impact of different combinations of sensor features on scour prediction showed the significance of the historical time series of scour for predicting upcoming events. Overall, this study provides a greater understanding of the potential of Deep Learning (DL) for real-time scour forecasting and early warning in bridges with diverse scour and flow characteristics including riverine and tidal/coastal bridges.","sentences":["Scour around bridge piers is a critical challenge for infrastructures around the world.","In the absence of analytical models and due to the complexity of the scour process, it is difficult for current empirical methods to achieve accurate predictions.","In this paper, we exploit the power of deep learning algorithms to forecast the scour depth variations around bridge piers based on historical sensor monitoring data, including riverbed elevation, flow elevation, and flow velocity.","We investigated the performance of Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN) models for real-time scour forecasting using data collected from bridges in Alaska and Oregon from 2006 to 2021.","The LSTM models achieved mean absolute error (MAE) ranging from 0.1m to 0.5m for predicting bed level variations a week in advance, showing a reasonable performance.","The Fully Convolutional Network (FCN) variant of CNN outperformed other CNN configurations, showing a comparable performance to LSTMs with significantly lower computational costs.","We explored various innovative random-search heuristics for hyperparameter tuning and model optimisation which resulted in reduced computational cost compared to grid-search method.","The impact of different combinations of sensor features on scour prediction showed the significance of the historical time series of scour for predicting upcoming events.","Overall, this study provides a greater understanding of the potential of Deep Learning (DL) for real-time scour forecasting and early warning in bridges with diverse scour and flow characteristics including riverine and tidal/coastal bridges."],"url":"http://arxiv.org/abs/2404.16549v1"}
{"created":"2024-04-25 12:04:31","title":"Cross-Domain Spatial Matching for Camera and Radar Sensor Data Fusion in Autonomous Vehicle Perception System","abstract":"In this paper, we propose a novel approach to address the problem of camera and radar sensor fusion for 3D object detection in autonomous vehicle perception systems. Our approach builds on recent advances in deep learning and leverages the strengths of both sensors to improve object detection performance. Precisely, we extract 2D features from camera images using a state-of-the-art deep learning architecture and then apply a novel Cross-Domain Spatial Matching (CDSM) transformation method to convert these features into 3D space. We then fuse them with extracted radar data using a complementary fusion strategy to produce a final 3D object representation. To demonstrate the effectiveness of our approach, we evaluate it on the NuScenes dataset. We compare our approach to both single-sensor performance and current state-of-the-art fusion methods. Our results show that the proposed approach achieves superior performance over single-sensor solutions and could directly compete with other top-level fusion methods.","sentences":["In this paper, we propose a novel approach to address the problem of camera and radar sensor fusion for 3D object detection in autonomous vehicle perception systems.","Our approach builds on recent advances in deep learning and leverages the strengths of both sensors to improve object detection performance.","Precisely, we extract 2D features from camera images using a state-of-the-art deep learning architecture and then apply a novel Cross-Domain Spatial Matching (CDSM) transformation method to convert these features into 3D space.","We then fuse them with extracted radar data using a complementary fusion strategy to produce a final 3D object representation.","To demonstrate the effectiveness of our approach, we evaluate it on the NuScenes dataset.","We compare our approach to both single-sensor performance and current state-of-the-art fusion methods.","Our results show that the proposed approach achieves superior performance over single-sensor solutions and could directly compete with other top-level fusion methods."],"url":"http://arxiv.org/abs/2404.16548v1"}
{"created":"2024-04-25 11:55:29","title":"Approximation Algorithm of Minimum All-Ones Problem for Arbitrary Graphs","abstract":"Let $G=(V, E)$ be a graph and let each vertex of $G$ has a lamp and a button. Each button can be of $\\sigma^+$-type or $\\sigma$-type.   Assume that initially some lamps are on and others are off. The button on vertex $x$ is of $\\sigma^+$-type ($\\sigma$-type, respectively) if pressing the button changes the lamp states on $x$ and on its neighbors in $G$ (the lamp states on the neighbors of $x$ only, respectively). Assume that there is a set $X\\subseteq V$ such that pressing buttons on vertices of $X$ lights all lamps on vertices of $G$. In particular, it is known to hold when initially all lamps are off and all buttons are of $\\sigma^+$-type.   Finding such a set $X$ of the smallest size is NP-hard even if initially all lamps are off and all buttons are of $\\sigma^+$-type. Using a linear algebraic approach we design a polynomial-time approximation algorithm for the problem such that for the set $X$ constructed by the algorithm, we have $|X|\\le \\min\\{r,(|V|+{\\rm opt})/2\\},$ where $r$ is the rank of a (modified) adjacent matrix of $G$ and ${\\rm opt}$ is the size of an optimal solution to the problem.   To the best of our knowledge, this is the first polynomial-time approximation algorithm for the problem with a nontrivial approximation guarantee.","sentences":["Let $G=(V, E)$ be a graph and let each vertex of $G$ has a lamp and a button.","Each button can be of $\\sigma^+$-type or $\\sigma$-type.   ","Assume that initially some lamps are on and others are off.","The button on vertex $x$ is of $\\sigma^+$-type ($\\sigma$-type, respectively) if pressing the button changes the lamp states on $x$ and on its neighbors in $G$ (the lamp states on the neighbors of $x$ only, respectively).","Assume that there is a set $X\\subseteq V$ such that pressing buttons on vertices of $X$ lights all lamps on vertices of $G$. In particular, it is known to hold when initially all lamps are off and all buttons are of $\\sigma^+$-type.   ","Finding such a set $X$ of the smallest size is NP-hard even if initially all lamps are off and all buttons are of $\\sigma^+$-type.","Using a linear algebraic approach we design a polynomial-time approximation algorithm for the problem such that for the set $X$ constructed by the algorithm, we have $|X|\\le \\min\\{r,(|V|+{\\rm opt})/2\\},$ where $r$ is the rank of a (modified) adjacent matrix of $G$ and ${\\rm opt}$ is the size of an optimal solution to the problem.   ","To the best of our knowledge, this is the first polynomial-time approximation algorithm for the problem with a nontrivial approximation guarantee."],"url":"http://arxiv.org/abs/2404.16540v1"}
{"created":"2024-04-25 11:53:36","title":"OpenDlign: Enhancing Open-World 3D Learning with Depth-Aligned Images","abstract":"Recent advances in Vision and Language Models (VLMs) have improved open-world 3D representation, facilitating 3D zero-shot capability in unseen categories. Existing open-world methods pre-train an extra 3D encoder to align features from 3D data (e.g., depth maps or point clouds) with CAD-rendered images and corresponding texts. However, the limited color and texture variations in CAD images can compromise the alignment robustness. Furthermore, the volume discrepancy between pre-training datasets of the 3D encoder and VLM leads to sub-optimal 2D to 3D knowledge transfer. To overcome these issues, we propose OpenDlign, a novel framework for learning open-world 3D representations, that leverages depth-aligned images generated from point cloud-projected depth maps. Unlike CAD-rendered images, our generated images provide rich, realistic color and texture diversity while preserving geometric and semantic consistency with the depth maps. OpenDlign also optimizes depth map projection and integrates depth-specific text prompts, improving 2D VLM knowledge adaptation for 3D learning efficient fine-tuning. Experimental results show that OpenDlign significantly outperforms existing benchmarks in zero-shot and few-shot 3D tasks, exceeding prior scores by 8.0% on ModelNet40 and 16.4% on OmniObject3D with just 6 million tuned parameters. Moreover, integrating generated depth-aligned images into existing 3D learning pipelines consistently improves their performance.","sentences":["Recent advances in Vision and Language Models (VLMs) have improved open-world 3D representation, facilitating 3D zero-shot capability in unseen categories.","Existing open-world methods pre-train an extra 3D encoder to align features from 3D data (e.g., depth maps or point clouds) with CAD-rendered images and corresponding texts.","However, the limited color and texture variations in CAD images can compromise the alignment robustness.","Furthermore, the volume discrepancy between pre-training datasets of the 3D encoder and VLM leads to sub-optimal 2D to 3D knowledge transfer.","To overcome these issues, we propose OpenDlign, a novel framework for learning open-world 3D representations, that leverages depth-aligned images generated from point cloud-projected depth maps.","Unlike CAD-rendered images, our generated images provide rich, realistic color and texture diversity while preserving geometric and semantic consistency with the depth maps.","OpenDlign also optimizes depth map projection and integrates depth-specific text prompts, improving 2D VLM knowledge adaptation for 3D learning efficient fine-tuning.","Experimental results show that OpenDlign significantly outperforms existing benchmarks in zero-shot and few-shot 3D tasks, exceeding prior scores by 8.0% on ModelNet40 and 16.4% on OmniObject3D with just 6 million tuned parameters.","Moreover, integrating generated depth-aligned images into existing 3D learning pipelines consistently improves their performance."],"url":"http://arxiv.org/abs/2404.16538v1"}
{"created":"2024-04-25 11:42:43","title":"On the Political Economy of Link-based Web Search","abstract":"Web search engines arguably form the most popular data-driven systems in contemporary society. They wield a considerable power by functioning as gatekeepers of the Web, with most user journeys on the Web beginning with them. Starting from the late 1990s, search engines have been dominated by the paradigm of link-based web search. In this paper, we critically analyze the political economy of the paradigm of link-based web search, drawing upon insights and methodologies from critical political economy. We draw several insights on how link-based web search has led to phenomena that favor capital through long-term structural changes on the Web, and how it has led to accentuating unpaid digital labor and ecologically unsustainable practices, among several others. We show how contemporary observations on the degrading quality of link-based web search can be traced back to the internal contradictions with the paradigm, and how such socio-technical phenomena may lead to a disutility of the link-based web search model. Our contribution is primarily on enhancing the understanding of the political economy of link-based web search, and laying bare the phenomena at work, and implicitly catalyze the search for alternative models.","sentences":["Web search engines arguably form the most popular data-driven systems in contemporary society.","They wield a considerable power by functioning as gatekeepers of the Web, with most user journeys on the Web beginning with them.","Starting from the late 1990s, search engines have been dominated by the paradigm of link-based web search.","In this paper, we critically analyze the political economy of the paradigm of link-based web search, drawing upon insights and methodologies from critical political economy.","We draw several insights on how link-based web search has led to phenomena that favor capital through long-term structural changes on the Web, and how it has led to accentuating unpaid digital labor and ecologically unsustainable practices, among several others.","We show how contemporary observations on the degrading quality of link-based web search can be traced back to the internal contradictions with the paradigm, and how such socio-technical phenomena may lead to a disutility of the link-based web search model.","Our contribution is primarily on enhancing the understanding of the political economy of link-based web search, and laying bare the phenomena at work, and implicitly catalyze the search for alternative models."],"url":"http://arxiv.org/abs/2404.16530v1"}
{"created":"2024-04-25 11:38:54","title":"Energy Efficient Service Placement for IoT Networks","abstract":"In recent years, there has been a significant expansion in the Internet of Things (IoT), with a growing number of devices being connected to the internet. This has led to an increase in data collection and analysis as well as the development of new technologies and applications. The rise of IoT has also brought about new challenges, such as security concerns and energy efficiency. This study investigates a layered IoT architecture that combines fog and cloud computing, aiming to assess the impact of service placement on energy efficiency. Through simulations, we analyse energy use across Access Fog, Metro Fog, and Cloud Data Centre layers for different IoT request volumes. Findings indicate that Access Fog is optimal for single requests, while Metro Fog efficiently manages higher demands from multiple devices. The study emphasizes the need for adaptive service deployment, responsive to network load variations, to improve energy efficiency. Hence, we propose the implementation of dynamic service placement strategies within Internet of Things (IoT) environments.","sentences":["In recent years, there has been a significant expansion in the Internet of Things (IoT), with a growing number of devices being connected to the internet.","This has led to an increase in data collection and analysis as well as the development of new technologies and applications.","The rise of IoT has also brought about new challenges, such as security concerns and energy efficiency.","This study investigates a layered IoT architecture that combines fog and cloud computing, aiming to assess the impact of service placement on energy efficiency.","Through simulations, we analyse energy use across Access Fog, Metro Fog, and Cloud Data Centre layers for different IoT request volumes.","Findings indicate that Access Fog is optimal for single requests, while Metro Fog efficiently manages higher demands from multiple devices.","The study emphasizes the need for adaptive service deployment, responsive to network load variations, to improve energy efficiency.","Hence, we propose the implementation of dynamic service placement strategies within Internet of Things (IoT) environments."],"url":"http://arxiv.org/abs/2404.16527v1"}
{"created":"2024-04-25 11:20:34","title":"Scalable Distributed String Sorting","abstract":"String sorting is an important part of tasks such as building index data structures. Unfortunately, current string sorting algorithms do not scale to massively parallel distributed-memory machines since they either have latency (at least) proportional to the number of processors $p$ or communicate the data a large number of times (at least logarithmic). We present practical and efficient algorithms for distributed-memory string sorting that scale to large $p$. Similar to state-of-the-art sorters for atomic objects, the algorithms have latency of about $p^{1/k}$ when allowing the data to be communicated $k$ times. Experiments indicate good scaling behavior on a wide range of inputs on up to 49152 cores. Overall, we achieve speedups of up to 5 over the current state-of-the-art distributed string sorting algorithms.","sentences":["String sorting is an important part of tasks such as building index data structures.","Unfortunately, current string sorting algorithms do not scale to massively parallel distributed-memory machines since they either have latency (at least) proportional to the number of processors $p$ or communicate the data a large number of times (at least logarithmic).","We present practical and efficient algorithms for distributed-memory string sorting that scale to large $p$. Similar to state-of-the-art sorters for atomic objects, the algorithms have latency of about $p^{1/k}$ when allowing the data to be communicated $k$ times.","Experiments indicate good scaling behavior on a wide range of inputs on up to 49152 cores.","Overall, we achieve speedups of up to 5 over the current state-of-the-art distributed string sorting algorithms."],"url":"http://arxiv.org/abs/2404.16517v1"}
{"created":"2024-04-25 11:02:54","title":"Exploring the Dynamics of Data Transmission in 5G Networks: A Conceptual Analysis","abstract":"This conceptual analysis examines the dynamics of data transmission in 5G networks. It addresses various aspects of sending data from cameras and LiDARs installed on a remote-controlled ferry to a land-based control center. The range of topics includes all stages of video and LiDAR data processing from acquisition and encoding to final decoding, all aspects of their transmission and reception via the WebRTC protocol, and all possible types of network problems such as handovers or congestion that could affect the quality of experience for end-users. A series of experiments were conducted to evaluate the key aspects of the data transmission. These include simulation-based reproducible runs and real-world experiments conducted using open-source solutions we developed: \"Gymir5G\" - an OMNeT++-based 5G simulation and \"GstWebRTCApp\" - a GStreamer-based application for adaptive control of media streams over the WebRTC protocol. One of the goals of this study is to formulate the bandwidth and latency requirements for reliable real-time communication and to estimate their approximate values. This goal was achieved through simulation-based experiments involving docking maneuvers in the Bay of Kiel, Germany. The final latency for the entire data processing pipeline was also estimated during the real tests. In addition, a series of simulation-based experiments showed the impact of key WebRTC features and demonstrated the effectiveness of the WebRTC protocol, while the conducted video codec comparison showed that the hardware-accelerated H.264 codec is the best. Finally, the research addresses the topic of adaptive communication, where the traditional congestion avoidance and deep reinforcement learning approaches were analyzed. The comparison in a sandbox scenario shows that the AI-based solution outperforms the WebRTC baseline GCC algorithm in terms of data rates, latency, and packet loss.","sentences":["This conceptual analysis examines the dynamics of data transmission in 5G networks.","It addresses various aspects of sending data from cameras and LiDARs installed on a remote-controlled ferry to a land-based control center.","The range of topics includes all stages of video and LiDAR data processing from acquisition and encoding to final decoding, all aspects of their transmission and reception via the WebRTC protocol, and all possible types of network problems such as handovers or congestion that could affect the quality of experience for end-users.","A series of experiments were conducted to evaluate the key aspects of the data transmission.","These include simulation-based reproducible runs and real-world experiments conducted using open-source solutions we developed: \"Gymir5G\" - an OMNeT++-based 5G simulation and \"GstWebRTCApp\" - a GStreamer-based application for adaptive control of media streams over the WebRTC protocol.","One of the goals of this study is to formulate the bandwidth and latency requirements for reliable real-time communication and to estimate their approximate values.","This goal was achieved through simulation-based experiments involving docking maneuvers in the Bay of Kiel, Germany.","The final latency for the entire data processing pipeline was also estimated during the real tests.","In addition, a series of simulation-based experiments showed the impact of key WebRTC features and demonstrated the effectiveness of the WebRTC protocol, while the conducted video codec comparison showed that the hardware-accelerated H.264 codec is the best.","Finally, the research addresses the topic of adaptive communication, where the traditional congestion avoidance and deep reinforcement learning approaches were analyzed.","The comparison in a sandbox scenario shows that the AI-based solution outperforms the WebRTC baseline GCC algorithm in terms of data rates, latency, and packet loss."],"url":"http://arxiv.org/abs/2404.16508v1"}
{"created":"2024-04-25 10:54:47","title":"Hardware Implementation of Double Pendulum Pseudo Random Number Generator","abstract":"The objective of this project is to utilize an FPGA board which is the CMOD A7 35t to obtain a pseudo random number which can be used for encryption. We aim to achieve this by leveraging the inherent randomness present in environmental data captured by sensors. This data will be used as a seed to initialize an algorithm implemented on the CMOD A7 35t FPGA board. The project will focus on interfacing the sensors with the FPGA and developing suitable algorithms to ensure the generated numbers exhibit strong randomness properties.","sentences":["The objective of this project is to utilize an FPGA board which is the CMOD A7 35t to obtain a pseudo random number which can be used for encryption.","We aim to achieve this by leveraging the inherent randomness present in environmental data captured by sensors.","This data will be used as a seed to initialize an algorithm implemented on the CMOD A7 35t FPGA board.","The project will focus on interfacing the sensors with the FPGA and developing suitable algorithms to ensure the generated numbers exhibit strong randomness properties."],"url":"http://arxiv.org/abs/2404.16504v1"}
{"created":"2024-04-25 10:49:14","title":"Conformal Prediction of Motion Control Performance for an Automated Vehicle in Presence of Actuator Degradations and Failures","abstract":"Automated driving systems require monitoring mechanisms to ensure safe operation, especially if system components degrade or fail. Their runtime self-representation plays a key role as it provides a-priori knowledge about the system's capabilities and limitations. In this paper, we propose a data-driven approach for deriving such a self-representation model for the motion controller of an automated vehicle. A conformalized prediction model is learned and allows estimating how operational conditions as well as potential degradations and failures of the vehicle's actuators impact motion control performance. During runtime behavior generation, our predictor can provide a heuristic for determining the admissible action space.","sentences":["Automated driving systems require monitoring mechanisms to ensure safe operation, especially if system components degrade or fail.","Their runtime self-representation plays a key role as it provides a-priori knowledge about the system's capabilities and limitations.","In this paper, we propose a data-driven approach for deriving such a self-representation model for the motion controller of an automated vehicle.","A conformalized prediction model is learned and allows estimating how operational conditions as well as potential degradations and failures of the vehicle's actuators impact motion control performance.","During runtime behavior generation, our predictor can provide a heuristic for determining the admissible action space."],"url":"http://arxiv.org/abs/2404.16500v1"}
{"created":"2024-04-25 10:41:12","title":"Probabilistic Multi-Layer Perceptrons for Wind Farm Condition Monitoring","abstract":"We provide a condition monitoring system for wind farms, based on normal behaviour modelling using a probabilistic multi-layer perceptron with transfer learning via fine-tuning. The model predicts the output power of the wind turbine under normal behaviour based on features retrieved from supervisory control and data acquisition (SCADA) systems. Its advantages are that (i) it can be trained with SCADA data of at least a few years, (ii) it can incorporate all SCADA data of all wind turbines in a wind farm as features, (iii) it assumes that the output power follows a normal density with heteroscedastic variance and (iv) it can predict the output of one wind turbine by borrowing strength from the data of all other wind turbines in a farm. Probabilistic guidelines for condition monitoring are given via a CUSUM control chart. We illustrate the performance of our model in a real SCADA data example which provides evidence that it outperforms other probabilistic prediction models.","sentences":["We provide a condition monitoring system for wind farms, based on normal behaviour modelling using a probabilistic multi-layer perceptron with transfer learning via fine-tuning.","The model predicts the output power of the wind turbine under normal behaviour based on features retrieved from supervisory control and data acquisition (SCADA) systems.","Its advantages are that (i) it can be trained with SCADA data of at least a few years, (ii) it can incorporate all SCADA data of all wind turbines in a wind farm as features, (iii) it assumes that the output power follows a normal density with heteroscedastic variance and (iv) it can predict the output of one wind turbine by borrowing strength from the data of all other wind turbines in a farm.","Probabilistic guidelines for condition monitoring are given via a CUSUM control chart.","We illustrate the performance of our model in a real SCADA data example which provides evidence that it outperforms other probabilistic prediction models."],"url":"http://arxiv.org/abs/2404.16496v1"}
{"created":"2024-04-25 10:26:18","title":"Cost-Driven Data Replication with Predictions","abstract":"This paper studies an online replication problem for distributed data access. The goal is to dynamically create and delete data copies in a multi-server system as time passes to minimize the total storage and network cost of serving access requests. We study the problem in the emergent learning-augmented setting, assuming simple binary predictions about inter-request times at individual servers. We develop an online algorithm and prove that it is ($\\frac{5+\\alpha}{3}$)-consistent (competitiveness under perfect predictions) and ($1 + \\frac{1}{\\alpha}$)-robust (competitiveness under terrible predictions), where $\\alpha \\in (0, 1]$ is a hyper-parameter representing the level of distrust in the predictions. We also study the impact of mispredictions on the competitive ratio of the proposed algorithm and adapt it to achieve a bounded robustness while retaining its consistency. We further establish a lower bound of $\\frac{3}{2}$ on the consistency of any deterministic learning-augmented algorithm. Experimental evaluations are carried out to evaluate our algorithms using real data access traces.","sentences":["This paper studies an online replication problem for distributed data access.","The goal is to dynamically create and delete data copies in a multi-server system as time passes to minimize the total storage and network cost of serving access requests.","We study the problem in the emergent learning-augmented setting, assuming simple binary predictions about inter-request times at individual servers.","We develop an online algorithm and prove that it is ($\\frac{5+\\alpha}{3}$)-consistent (competitiveness under perfect predictions) and ($1 + \\frac{1}{\\alpha}$)-robust (competitiveness under terrible predictions), where $\\alpha \\in (0, 1]$ is a hyper-parameter representing the level of distrust in the predictions.","We also study the impact of mispredictions on the competitive ratio of the proposed algorithm and adapt it to achieve a bounded robustness while retaining its consistency.","We further establish a lower bound of $\\frac{3}{2}$ on the consistency of any deterministic learning-augmented algorithm.","Experimental evaluations are carried out to evaluate our algorithms using real data access traces."],"url":"http://arxiv.org/abs/2404.16489v1"}
{"created":"2024-04-25 10:12:31","title":"Leveraging Pretrained Latent Representations for Few-Shot Imitation Learning on a Dexterous Robotic Hand","abstract":"In the context of imitation learning applied to dexterous robotic hands, the high complexity of the systems makes learning complex manipulation tasks challenging. However, the numerous datasets depicting human hands in various different tasks could provide us with better knowledge regarding human hand motion. We propose a method to leverage multiple large-scale task-agnostic datasets to obtain latent representations that effectively encode motion subtrajectories that we included in a transformer-based behavior cloning method. Our results demonstrate that employing latent representations yields enhanced performance compared to conventional behavior cloning methods, particularly regarding resilience to errors and noise in perception and proprioception. Furthermore, the proposed approach solely relies on human demonstrations, eliminating the need for teleoperation and, therefore, accelerating the data acquisition process. Accurate inverse kinematics for fingertip retargeting ensures precise transfer from human hand data to the robot, facilitating effective learning and deployment of manipulation policies. Finally, the trained policies have been successfully transferred to a real-world 23Dof robotic system.","sentences":["In the context of imitation learning applied to dexterous robotic hands, the high complexity of the systems makes learning complex manipulation tasks challenging.","However, the numerous datasets depicting human hands in various different tasks could provide us with better knowledge regarding human hand motion.","We propose a method to leverage multiple large-scale task-agnostic datasets to obtain latent representations that effectively encode motion subtrajectories that we included in a transformer-based behavior cloning method.","Our results demonstrate that employing latent representations yields enhanced performance compared to conventional behavior cloning methods, particularly regarding resilience to errors and noise in perception and proprioception.","Furthermore, the proposed approach solely relies on human demonstrations, eliminating the need for teleoperation and, therefore, accelerating the data acquisition process.","Accurate inverse kinematics for fingertip retargeting ensures precise transfer from human hand data to the robot, facilitating effective learning and deployment of manipulation policies.","Finally, the trained policies have been successfully transferred to a real-world 23Dof robotic system."],"url":"http://arxiv.org/abs/2404.16483v1"}
{"created":"2024-04-25 09:47:53","title":"Sublinear-Time Opinion Estimation in the Friedkin--Johnsen Model","abstract":"Online social networks are ubiquitous parts of modern societies and the discussions that take place in these networks impact people's opinions on diverse topics, such as politics or vaccination. One of the most popular models to formally describe this opinion formation process is the Friedkin--Johnsen (FJ) model, which allows to define measures, such as the polarization and the disagreement of a network. Recently, Xu, Bao and Zhang (WebConf'21) showed that all opinions and relevant measures in the FJ model can be approximated in near-linear time. However, their algorithm requires the entire network and the opinions of all nodes as input. Given the sheer size of online social networks and increasing data-access limitations, obtaining the entirety of this data might, however, be unrealistic in practice. In this paper, we show that node opinions and all relevant measures, like polarization and disagreement, can be efficiently approximated in time that is sublinear in the size of the network. Particularly, our algorithms only require query-access to the network and do not have to preprocess the graph. Furthermore, we use a connection between FJ opinion dynamics and personalized PageRank, and show that in $d$-regular graphs, we can deterministically approximate each node's opinion by only looking at a constant-size neighborhood, independently of the network size. We also experimentally validate that our estimation algorithms perform well in practice.","sentences":["Online social networks are ubiquitous parts of modern societies and the discussions that take place in these networks impact people's opinions on diverse topics, such as politics or vaccination.","One of the most popular models to formally describe this opinion formation process is the Friedkin--Johnsen (FJ) model, which allows to define measures, such as the polarization and the disagreement of a network.","Recently, Xu, Bao and Zhang (WebConf'21) showed that all opinions and relevant measures in the FJ model can be approximated in near-linear time.","However, their algorithm requires the entire network and the opinions of all nodes as input.","Given the sheer size of online social networks and increasing data-access limitations, obtaining the entirety of this data might, however, be unrealistic in practice.","In this paper, we show that node opinions and all relevant measures, like polarization and disagreement, can be efficiently approximated in time that is sublinear in the size of the network.","Particularly, our algorithms only require query-access to the network and do not have to preprocess the graph.","Furthermore, we use a connection between FJ opinion dynamics and personalized PageRank, and show that in $d$-regular graphs, we can deterministically approximate each node's opinion by only looking at a constant-size neighborhood, independently of the network size.","We also experimentally validate that our estimation algorithms perform well in practice."],"url":"http://arxiv.org/abs/2404.16464v1"}
{"created":"2024-04-25 09:42:50","title":"Large Language Models Perform on Par with Experts Identifying Mental Health Factors in Adolescent Online Forums","abstract":"Mental health in children and adolescents has been steadily deteriorating over the past few years [ 1 ]. The recent advent of Large Language Models (LLMs) offers much hope for cost and time efficient scaling of monitoring and intervention, yet despite specifically prevalent issues such as school bullying and eating disorders, previous studies on have not investigated performance in this domain or for open information extraction where the set of answers is not predetermined. We create a new dataset of Reddit posts from adolescents aged 12-19 annotated by expert psychiatrists for the following categories: TRAUMA, PRECARITY, CONDITION, SYMPTOMS, SUICIDALITY and TREATMENT and compare expert labels to annotations from two top performing LLMs (GPT3.5 and GPT4). In addition, we create two synthetic datasets to assess whether LLMs perform better when annotating data as they generate it. We find GPT4 to be on par with human inter-annotator agreement and performance on synthetic data to be substantially higher, however we find the model still occasionally errs on issues of negation and factuality and higher performance on synthetic data is driven by greater complexity of real data rather than inherent advantage.","sentences":["Mental health in children and adolescents has been steadily deteriorating over the past few years [ 1 ].","The recent advent of Large Language Models (LLMs) offers much hope for cost and time efficient scaling of monitoring and intervention, yet despite specifically prevalent issues such as school bullying and eating disorders, previous studies on have not investigated performance in this domain or for open information extraction where the set of answers is not predetermined.","We create a new dataset of Reddit posts from adolescents aged 12-19 annotated by expert psychiatrists for the following categories: TRAUMA, PRECARITY, CONDITION, SYMPTOMS, SUICIDALITY and TREATMENT and compare expert labels to annotations from two top performing LLMs (GPT3.5 and GPT4).","In addition, we create two synthetic datasets to assess whether LLMs perform better when annotating data as they generate it.","We find GPT4 to be on par with human inter-annotator agreement and performance on synthetic data to be substantially higher, however we find the model still occasionally errs on issues of negation and factuality and higher performance on synthetic data is driven by greater complexity of real data rather than inherent advantage."],"url":"http://arxiv.org/abs/2404.16461v1"}
{"created":"2024-04-25 09:37:44","title":"Towards Precise Observations of Neural Model Robustness in Classification","abstract":"In deep learning applications, robustness measures the ability of neural models that handle slight changes in input data, which could lead to potential safety hazards, especially in safety-critical applications. Pre-deployment assessment of model robustness is essential, but existing methods often suffer from either high costs or imprecise results. To enhance safety in real-world scenarios, metrics that effectively capture the model's robustness are needed. To address this issue, we compare the rigour and usage conditions of various assessment methods based on different definitions. Then, we propose a straightforward and practical metric utilizing hypothesis testing for probabilistic robustness and have integrated it into the TorchAttacks library. Through a comparative analysis of diverse robustness assessment methods, our approach contributes to a deeper understanding of model robustness in safety-critical applications.","sentences":["In deep learning applications, robustness measures the ability of neural models that handle slight changes in input data, which could lead to potential safety hazards, especially in safety-critical applications.","Pre-deployment assessment of model robustness is essential, but existing methods often suffer from either high costs or imprecise results.","To enhance safety in real-world scenarios, metrics that effectively capture the model's robustness are needed.","To address this issue, we compare the rigour and usage conditions of various assessment methods based on different definitions.","Then, we propose a straightforward and practical metric utilizing hypothesis testing for probabilistic robustness and have integrated it into the TorchAttacks library.","Through a comparative analysis of diverse robustness assessment methods, our approach contributes to a deeper understanding of model robustness in safety-critical applications."],"url":"http://arxiv.org/abs/2404.16457v1"}
{"created":"2024-04-25 09:35:09","title":"Correlation-Decoupled Knowledge Distillation for Multimodal Sentiment Analysis with Incomplete Modalities","abstract":"Multimodal sentiment analysis (MSA) aims to understand human sentiment through multimodal data. Most MSA efforts are based on the assumption of modality completeness. However, in real-world applications, some practical factors cause uncertain modality missingness, which drastically degrades the model's performance. To this end, we propose a Correlation-decoupled Knowledge Distillation (CorrKD) framework for the MSA task under uncertain missing modalities. Specifically, we present a sample-level contrastive distillation mechanism that transfers comprehensive knowledge containing cross-sample correlations to reconstruct missing semantics. Moreover, a category-guided prototype distillation mechanism is introduced to capture cross-category correlations using category prototypes to align feature distributions and generate favorable joint representations. Eventually, we design a response-disentangled consistency distillation strategy to optimize the sentiment decision boundaries of the student network through response disentanglement and mutual information maximization. Comprehensive experiments on three datasets indicate that our framework can achieve favorable improvements compared with several baselines.","sentences":["Multimodal sentiment analysis (MSA) aims to understand human sentiment through multimodal data.","Most MSA efforts are based on the assumption of modality completeness.","However, in real-world applications, some practical factors cause uncertain modality missingness, which drastically degrades the model's performance.","To this end, we propose a Correlation-decoupled Knowledge Distillation (CorrKD) framework for the MSA task under uncertain missing modalities.","Specifically, we present a sample-level contrastive distillation mechanism that transfers comprehensive knowledge containing cross-sample correlations to reconstruct missing semantics.","Moreover, a category-guided prototype distillation mechanism is introduced to capture cross-category correlations using category prototypes to align feature distributions and generate favorable joint representations.","Eventually, we design a response-disentangled consistency distillation strategy to optimize the sentiment decision boundaries of the student network through response disentanglement and mutual information maximization.","Comprehensive experiments on three datasets indicate that our framework can achieve favorable improvements compared with several baselines."],"url":"http://arxiv.org/abs/2404.16456v1"}
{"created":"2024-04-25 09:32:34","title":"PAD: Patch-Agnostic Defense against Adversarial Patch Attacks","abstract":"Adversarial patch attacks present a significant threat to real-world object detectors due to their practical feasibility. Existing defense methods, which rely on attack data or prior knowledge, struggle to effectively address a wide range of adversarial patches. In this paper, we show two inherent characteristics of adversarial patches, semantic independence and spatial heterogeneity, independent of their appearance, shape, size, quantity, and location. Semantic independence indicates that adversarial patches operate autonomously within their semantic context, while spatial heterogeneity manifests as distinct image quality of the patch area that differs from original clean image due to the independent generation process. Based on these observations, we propose PAD, a novel adversarial patch localization and removal method that does not require prior knowledge or additional training. PAD offers patch-agnostic defense against various adversarial patches, compatible with any pre-trained object detectors. Our comprehensive digital and physical experiments involving diverse patch types, such as localized noise, printable, and naturalistic patches, exhibit notable improvements over state-of-the-art works. Our code is available at https://github.com/Lihua-Jing/PAD.","sentences":["Adversarial patch attacks present a significant threat to real-world object detectors due to their practical feasibility.","Existing defense methods, which rely on attack data or prior knowledge, struggle to effectively address a wide range of adversarial patches.","In this paper, we show two inherent characteristics of adversarial patches, semantic independence and spatial heterogeneity, independent of their appearance, shape, size, quantity, and location.","Semantic independence indicates that adversarial patches operate autonomously within their semantic context, while spatial heterogeneity manifests as distinct image quality of the patch area that differs from original clean image due to the independent generation process.","Based on these observations, we propose PAD, a novel adversarial patch localization and removal method that does not require prior knowledge or additional training.","PAD offers patch-agnostic defense against various adversarial patches, compatible with any pre-trained object detectors.","Our comprehensive digital and physical experiments involving diverse patch types, such as localized noise, printable, and naturalistic patches, exhibit notable improvements over state-of-the-art works.","Our code is available at https://github.com/Lihua-Jing/PAD."],"url":"http://arxiv.org/abs/2404.16452v1"}
{"created":"2024-04-25 09:27:35","title":"On Software Ageing Indicators in OpenStack","abstract":"Distributed systems in general and cloud systems in particular, are susceptible to failures that can lead to substantial economic and data losses, security breaches, and even potential threats to human safety. Software ageing is an example of one such vulnerability. It emerges due to routine re-usage of computational systems units which induce fatigue within the components, resulting in an increased failure rate and potential system breakdown. Due to its stochastic nature, ageing cannot be directly measured, instead ageing indicators as proxies are used. While there are dozens of studies on different ageing indicators, their comprehensive comparison in different settings remains underexplored. In this paper, we compare two ageing indicators in OpenStack as a use case. Specifically, our evaluation compares memory usage (including swap memory) and request response time, as readily available indicators. By executing multiple OpenStack deployments with varying configurations, we conduct a series of experiments and analyze the ageing indicators. Comparative analysis through statistical tests provides valuable insights into the strengths and weaknesses of the utilised ageing indicators. Finally, through an in-depth analysis of other OpenStack failures, we identify underlying failure patterns and their impact on the studied ageing indicators.","sentences":["Distributed systems in general and cloud systems in particular, are susceptible to failures that can lead to substantial economic and data losses, security breaches, and even potential threats to human safety.","Software ageing is an example of one such vulnerability.","It emerges due to routine re-usage of computational systems units which induce fatigue within the components, resulting in an increased failure rate and potential system breakdown.","Due to its stochastic nature, ageing cannot be directly measured, instead ageing indicators as proxies are used.","While there are dozens of studies on different ageing indicators, their comprehensive comparison in different settings remains underexplored.","In this paper, we compare two ageing indicators in OpenStack as a use case.","Specifically, our evaluation compares memory usage (including swap memory) and request response time, as readily available indicators.","By executing multiple OpenStack deployments with varying configurations, we conduct a series of experiments and analyze the ageing indicators.","Comparative analysis through statistical tests provides valuable insights into the strengths and weaknesses of the utilised ageing indicators.","Finally, through an in-depth analysis of other OpenStack failures, we identify underlying failure patterns and their impact on the studied ageing indicators."],"url":"http://arxiv.org/abs/2404.16446v1"}
{"created":"2024-04-25 09:23:03","title":"Automating the Discovery of Partial Differential Equations in Dynamical Systems","abstract":"Identifying partial differential equations (PDEs) from data is crucial for understanding the governing mechanisms of natural phenomena, yet it remains a challenging task. We present an extension to the ARGOS framework, ARGOS-RAL, which leverages sparse regression with the recurrent adaptive lasso to identify PDEs from limited prior knowledge automatically. Our method automates calculating partial derivatives, constructing a candidate library, and estimating a sparse model. We rigorously evaluate the performance of ARGOS-RAL in identifying canonical PDEs under various noise levels and sample sizes, demonstrating its robustness in handling noisy and non-uniformly distributed data. We also test the algorithm's performance on datasets consisting solely of random noise to simulate scenarios with severely compromised data quality. Our results show that ARGOS-RAL effectively and reliably identifies the underlying PDEs from data, outperforming the sequential threshold ridge regression method in most cases. We highlight the potential of combining statistical methods, machine learning, and dynamical systems theory to automatically discover governing equations from collected data, streamlining the scientific modeling process.","sentences":["Identifying partial differential equations (PDEs) from data is crucial for understanding the governing mechanisms of natural phenomena, yet it remains a challenging task.","We present an extension to the ARGOS framework, ARGOS-RAL, which leverages sparse regression with the recurrent adaptive lasso to identify PDEs from limited prior knowledge automatically.","Our method automates calculating partial derivatives, constructing a candidate library, and estimating a sparse model.","We rigorously evaluate the performance of ARGOS-RAL in identifying canonical PDEs under various noise levels and sample sizes, demonstrating its robustness in handling noisy and non-uniformly distributed data.","We also test the algorithm's performance on datasets consisting solely of random noise to simulate scenarios with severely compromised data quality.","Our results show that ARGOS-RAL effectively and reliably identifies the underlying PDEs from data, outperforming the sequential threshold ridge regression method in most cases.","We highlight the potential of combining statistical methods, machine learning, and dynamical systems theory to automatically discover governing equations from collected data, streamlining the scientific modeling process."],"url":"http://arxiv.org/abs/2404.16444v1"}
{"created":"2024-04-25 09:22:31","title":"Tightening I/O Lower Bounds through the Hourglass Dependency Pattern","abstract":"When designing an algorithm, one cares about arithmetic/computational complexity, but data movement (I/O) complexity plays an increasingly important role that highly impacts performance and energy consumption. For a given algorithm and a given I/O model, scheduling strategies such as loop tiling can reduce the required I/O down to a limit, called the I/O complexity, inherent to the algorithm itself. The objective of I/O complexity analysis is to compute, for a given program, its minimal I/O requirement among all valid schedules. We consider a sequential execution model with two memories, an infinite one, and a small one of size S on which the computations retrieve and produce data. The I/O is the number of reads and writes between the two memories. We identify a common \"hourglass pattern\" in the dependency graphs of several common linear algebra kernels. Using the properties of this pattern, we mathematically prove tighter lower bounds on their I/O complexity, which improves the previous state-of-the-art bound by a parametric ratio. This proof was integrated inside the IOLB automatic lower bound derivation tool.","sentences":["When designing an algorithm, one cares about arithmetic/computational complexity, but data movement (I/O) complexity plays an increasingly important role that highly impacts performance and energy consumption.","For a given algorithm and a given I/O model, scheduling strategies such as loop tiling can reduce the required I/O down to a limit, called the I/O complexity, inherent to the algorithm itself.","The objective of I/O complexity analysis is to compute, for a given program, its minimal I/O requirement among all valid schedules.","We consider a sequential execution model with two memories, an infinite one, and a small one of size S on which the computations retrieve and produce data.","The I/O is the number of reads and writes between the two memories.","We identify a common \"hourglass pattern\" in the dependency graphs of several common linear algebra kernels.","Using the properties of this pattern, we mathematically prove tighter lower bounds on their I/O complexity, which improves the previous state-of-the-art bound by a parametric ratio.","This proof was integrated inside the IOLB automatic lower bound derivation tool."],"url":"http://arxiv.org/abs/2404.16443v1"}
{"created":"2024-04-25 09:20:51","title":"Contextual Categorization Enhancement through LLMs Latent-Space","abstract":"Managing the semantic quality of the categorization in large textual datasets, such as Wikipedia, presents significant challenges in terms of complexity and cost. In this paper, we propose leveraging transformer models to distill semantic information from texts in the Wikipedia dataset and its associated categories into a latent space. We then explore different approaches based on these encodings to assess and enhance the semantic identity of the categories. Our graphical approach is powered by Convex Hull, while we utilize Hierarchical Navigable Small Worlds (HNSWs) for the hierarchical approach. As a solution to the information loss caused by the dimensionality reduction, we modulate the following mathematical solution: an exponential decay function driven by the Euclidean distances between the high-dimensional encodings of the textual categories. This function represents a filter built around a contextual category and retrieves items with a certain Reconsideration Probability (RP). Retrieving high-RP items serves as a tool for database administrators to improve data groupings by providing recommendations and identifying outliers within a contextual framework.","sentences":["Managing the semantic quality of the categorization in large textual datasets, such as Wikipedia, presents significant challenges in terms of complexity and cost.","In this paper, we propose leveraging transformer models to distill semantic information from texts in the Wikipedia dataset and its associated categories into a latent space.","We then explore different approaches based on these encodings to assess and enhance the semantic identity of the categories.","Our graphical approach is powered by Convex Hull, while we utilize Hierarchical Navigable Small Worlds (HNSWs) for the hierarchical approach.","As a solution to the information loss caused by the dimensionality reduction, we modulate the following mathematical solution: an exponential decay function driven by the Euclidean distances between the high-dimensional encodings of the textual categories.","This function represents a filter built around a contextual category and retrieves items with a certain Reconsideration Probability (RP).","Retrieving high-RP items serves as a tool for database administrators to improve data groupings by providing recommendations and identifying outliers within a contextual framework."],"url":"http://arxiv.org/abs/2404.16442v1"}
{"created":"2024-04-25 09:12:35","title":"Leveraging tropical reef, bird and unrelated sounds for superior transfer learning in marine bioacoustics","abstract":"Machine learning has the potential to revolutionize passive acoustic monitoring (PAM) for ecological assessments. However, high annotation and compute costs limit the field's efficacy. Generalizable pretrained networks can overcome these costs, but high-quality pretraining requires vast annotated libraries, limiting its current applicability primarily to bird taxa. Here, we identify the optimum pretraining strategy for a data-deficient domain using coral reef bioacoustics. We assemble ReefSet, a large annotated library of reef sounds, though modest compared to bird libraries at 2% of the sample count. Through testing few-shot transfer learning performance, we observe that pretraining on bird audio provides notably superior generalizability compared to pretraining on ReefSet or unrelated audio alone. However, our key findings show that cross-domain mixing which leverages bird, reef and unrelated audio during pretraining maximizes reef generalizability. SurfPerch, our pretrained network, provides a strong foundation for automated analysis of marine PAM data with minimal annotation and compute costs.","sentences":["Machine learning has the potential to revolutionize passive acoustic monitoring (PAM) for ecological assessments.","However, high annotation and compute costs limit the field's efficacy.","Generalizable pretrained networks can overcome these costs, but high-quality pretraining requires vast annotated libraries, limiting its current applicability primarily to bird taxa.","Here, we identify the optimum pretraining strategy for a data-deficient domain using coral reef bioacoustics.","We assemble ReefSet, a large annotated library of reef sounds, though modest compared to bird libraries at 2% of the sample count.","Through testing few-shot transfer learning performance, we observe that pretraining on bird audio provides notably superior generalizability compared to pretraining on ReefSet or unrelated audio alone.","However, our key findings show that cross-domain mixing which leverages bird, reef and unrelated audio during pretraining maximizes reef generalizability.","SurfPerch, our pretrained network, provides a strong foundation for automated analysis of marine PAM data with minimal annotation and compute costs."],"url":"http://arxiv.org/abs/2404.16436v1"}
{"created":"2024-04-25 09:07:19","title":"Point-JEPA: A Joint Embedding Predictive Architecture for Self-Supervised Learning on Point Cloud","abstract":"Recent advancements in self-supervised learning in the point cloud domain have demonstrated significant potential. However, these methods often suffer from drawbacks, including lengthy pre-training time, the necessity of reconstruction in the input space, or the necessity of additional modalities. In order to address these issues, we introduce Point-JEPA, a joint embedding predictive architecture designed specifically for point cloud data. To this end, we introduce a sequencer that orders point cloud tokens to efficiently compute and utilize tokens proximity based on their indices during target and context selection. The sequencer also allows shared computations of the tokens proximity between context and target selection, further improving the efficiency. Experimentally, our method achieves competitive results with state-of-the-art methods while avoiding the reconstruction in the input space or additional modality.","sentences":["Recent advancements in self-supervised learning in the point cloud domain have demonstrated significant potential.","However, these methods often suffer from drawbacks, including lengthy pre-training time, the necessity of reconstruction in the input space, or the necessity of additional modalities.","In order to address these issues, we introduce Point-JEPA, a joint embedding predictive architecture designed specifically for point cloud data.","To this end, we introduce a sequencer that orders point cloud tokens to efficiently compute and utilize tokens proximity based on their indices during target and context selection.","The sequencer also allows shared computations of the tokens proximity between context and target selection, further improving the efficiency.","Experimentally, our method achieves competitive results with state-of-the-art methods while avoiding the reconstruction in the input space or additional modality."],"url":"http://arxiv.org/abs/2404.16432v1"}
{"created":"2024-04-25 09:05:39","title":"Secure Coded Distributed Computing","abstract":"In this paper, we consider two critical aspects of security in the \\textit{distributed computing (DC)} model: \\textit{secure data shuffling} and \\textit{secure coded computing}. It is imperative that any external entity overhearing the transmissions does not gain any information about the \\textit{intermediate values (IVs)} exchanged during the shuffling phase of the DC model. Our approach ensures IV confidentiality during data shuffling. Moreover, each node in the system must be able to recover the IVs necessary for computing its output functions but must also remain oblivious to the IVs associated with output functions not assigned to it. We design secure DC methods and establish achievable limits on the tradeoffs between the communication and computation loads to contribute to the advancement of secure data processing in distributed systems.","sentences":["In this paper, we consider two critical aspects of security in the \\textit{distributed computing (DC)} model: \\textit{secure data shuffling} and \\textit{secure coded computing}.","It is imperative that any external entity overhearing the transmissions does not gain any information about the \\textit{intermediate values (IVs)} exchanged during the shuffling phase of the DC model.","Our approach ensures IV confidentiality during data shuffling.","Moreover, each node in the system must be able to recover the IVs necessary for computing its output functions but must also remain oblivious to the IVs associated with output functions not assigned to it.","We design secure DC methods and establish achievable limits on the tradeoffs between the communication and computation loads to contribute to the advancement of secure data processing in distributed systems."],"url":"http://arxiv.org/abs/2404.16431v1"}
{"created":"2024-04-25 09:02:11","title":"Depth Supervised Neural Surface Reconstruction from Airborne Imagery","abstract":"While originally developed for novel view synthesis, Neural Radiance Fields (NeRFs) have recently emerged as an alternative to multi-view stereo (MVS). Triggered by a manifold of research activities, promising results have been gained especially for texture-less, transparent, and reflecting surfaces, while such scenarios remain challenging for traditional MVS-based approaches. However, most of these investigations focus on close-range scenarios, with studies for airborne scenarios still missing. For this task, NeRFs face potential difficulties at areas of low image redundancy and weak data evidence, as often found in street canyons, facades or building shadows. Furthermore, training such networks is computationally expensive. Thus, the aim of our work is twofold: First, we investigate the applicability of NeRFs for aerial image blocks representing different characteristics like nadir-only, oblique and high-resolution imagery. Second, during these investigations we demonstrate the benefit of integrating depth priors from tie-point measures, which are provided during presupposed Bundle Block Adjustment. Our work is based on the state-of-the-art framework VolSDF, which models 3D scenes by signed distance functions (SDFs), since this is more applicable for surface reconstruction compared to the standard volumetric representation in vanilla NeRFs. For evaluation, the NeRF-based reconstructions are compared to results of a publicly available benchmark dataset for airborne images.","sentences":["While originally developed for novel view synthesis, Neural Radiance Fields (NeRFs) have recently emerged as an alternative to multi-view stereo (MVS).","Triggered by a manifold of research activities, promising results have been gained especially for texture-less, transparent, and reflecting surfaces, while such scenarios remain challenging for traditional MVS-based approaches.","However, most of these investigations focus on close-range scenarios, with studies for airborne scenarios still missing.","For this task, NeRFs face potential difficulties at areas of low image redundancy and weak data evidence, as often found in street canyons, facades or building shadows.","Furthermore, training such networks is computationally expensive.","Thus, the aim of our work is twofold:","First, we investigate the applicability of NeRFs for aerial image blocks representing different characteristics like nadir-only, oblique and high-resolution imagery.","Second, during these investigations we demonstrate the benefit of integrating depth priors from tie-point measures, which are provided during presupposed Bundle Block Adjustment.","Our work is based on the state-of-the-art framework VolSDF, which models 3D scenes by signed distance functions (SDFs), since this is more applicable for surface reconstruction compared to the standard volumetric representation in vanilla NeRFs.","For evaluation, the NeRF-based reconstructions are compared to results of a publicly available benchmark dataset for airborne images."],"url":"http://arxiv.org/abs/2404.16429v1"}
{"created":"2024-04-25 08:51:59","title":"SynCellFactory: Generative Data Augmentation for Cell Tracking","abstract":"Cell tracking remains a pivotal yet challenging task in biomedical research. The full potential of deep learning for this purpose is often untapped due to the limited availability of comprehensive and varied training data sets. In this paper, we present SynCellFactory, a generative cell video augmentation. At the heart of SynCellFactory lies the ControlNet architecture, which has been fine-tuned to synthesize cell imagery with photorealistic accuracy in style and motion patterns. This technique enables the creation of synthetic yet realistic cell videos that mirror the complexity of authentic microscopy time-lapses. Our experiments demonstrate that SynCellFactory boosts the performance of well-established deep learning models for cell tracking, particularly when original training data is sparse.","sentences":["Cell tracking remains a pivotal yet challenging task in biomedical research.","The full potential of deep learning for this purpose is often untapped due to the limited availability of comprehensive and varied training data sets.","In this paper, we present SynCellFactory, a generative cell video augmentation.","At the heart of SynCellFactory lies the ControlNet architecture, which has been fine-tuned to synthesize cell imagery with photorealistic accuracy in style and motion patterns.","This technique enables the creation of synthetic yet realistic cell videos that mirror the complexity of authentic microscopy time-lapses.","Our experiments demonstrate that SynCellFactory boosts the performance of well-established deep learning models for cell tracking, particularly when original training data is sparse."],"url":"http://arxiv.org/abs/2404.16421v1"}
{"created":"2024-04-25 08:49:47","title":"Instruction Matters, a Simple yet Effective Task Selection Approach in Instruction Tuning for Specific Tasks","abstract":"Instruction tuning has shown its ability to not only enhance zero-shot generalization across various tasks but also its effectiveness in improving the performance of specific tasks. A crucial aspect in instruction tuning for a particular task is a strategic selection of related tasks that offer meaningful supervision, thereby enhancing efficiency and preventing performance degradation from irrelevant tasks. Our research reveals that leveraging instruction information \\textit{alone} enables the identification of pertinent tasks for instruction tuning. This approach is notably simpler compared to traditional methods that necessitate complex measurements of pairwise transferability between tasks or the creation of data samples for the target task. Furthermore, by additionally learning the unique instructional template style of the meta-dataset, we observe an improvement in task selection accuracy, which contributes to enhanced overall performance. Experimental results demonstrate that training on a small set of tasks, chosen solely based on the instructions, leads to substantial performance improvements on benchmarks like P3, Big-Bench, NIV2, and Big-Bench Hard. Significantly, these improvements exceed those achieved by prior task selection methods, highlighting the efficacy of our approach.","sentences":["Instruction tuning has shown its ability to not only enhance zero-shot generalization across various tasks but also its effectiveness in improving the performance of specific tasks.","A crucial aspect in instruction tuning for a particular task is a strategic selection of related tasks that offer meaningful supervision, thereby enhancing efficiency and preventing performance degradation from irrelevant tasks.","Our research reveals that leveraging instruction information \\textit{alone} enables the identification of pertinent tasks for instruction tuning.","This approach is notably simpler compared to traditional methods that necessitate complex measurements of pairwise transferability between tasks or the creation of data samples for the target task.","Furthermore, by additionally learning the unique instructional template style of the meta-dataset, we observe an improvement in task selection accuracy, which contributes to enhanced overall performance.","Experimental results demonstrate that training on a small set of tasks, chosen solely based on the instructions, leads to substantial performance improvements on benchmarks like P3, Big-Bench, NIV2, and Big-Bench Hard.","Significantly, these improvements exceed those achieved by prior task selection methods, highlighting the efficacy of our approach."],"url":"http://arxiv.org/abs/2404.16418v1"}
{"created":"2024-04-25 08:49:08","title":"Learning Discriminative Spatio-temporal Representations for Semi-supervised Action Recognition","abstract":"Semi-supervised action recognition aims to improve spatio-temporal reasoning ability with a few labeled data in conjunction with a large amount of unlabeled data. Albeit recent advancements, existing powerful methods are still prone to making ambiguous predictions under scarce labeled data, embodied as the limitation of distinguishing different actions with similar spatio-temporal information. In this paper, we approach this problem by empowering the model two aspects of capability, namely discriminative spatial modeling and temporal structure modeling for learning discriminative spatio-temporal representations. Specifically, we propose an Adaptive Contrastive Learning~(ACL) strategy. It assesses the confidence of all unlabeled samples by the class prototypes of the labeled data, and adaptively selects positive-negative samples from a pseudo-labeled sample bank to construct contrastive learning. Additionally, we introduce a Multi-scale Temporal Learning~(MTL) strategy. It could highlight informative semantics from long-term clips and integrate them into the short-term clip while suppressing noisy information. Afterwards, both of these two new techniques are integrated in a unified framework to encourage the model to make accurate predictions. Extensive experiments on UCF101, HMDB51 and Kinetics400 show the superiority of our method over prior state-of-the-art approaches.","sentences":["Semi-supervised action recognition aims to improve spatio-temporal reasoning ability with a few labeled data in conjunction with a large amount of unlabeled data.","Albeit recent advancements, existing powerful methods are still prone to making ambiguous predictions under scarce labeled data, embodied as the limitation of distinguishing different actions with similar spatio-temporal information.","In this paper, we approach this problem by empowering the model two aspects of capability, namely discriminative spatial modeling and temporal structure modeling for learning discriminative spatio-temporal representations.","Specifically, we propose an Adaptive Contrastive Learning~(ACL) strategy.","It assesses the confidence of all unlabeled samples by the class prototypes of the labeled data, and adaptively selects positive-negative samples from a pseudo-labeled sample bank to construct contrastive learning.","Additionally, we introduce a Multi-scale Temporal Learning~(MTL) strategy.","It could highlight informative semantics from long-term clips and integrate them into the short-term clip while suppressing noisy information.","Afterwards, both of these two new techniques are integrated in a unified framework to encourage the model to make accurate predictions.","Extensive experiments on UCF101, HMDB51 and Kinetics400 show the superiority of our method over prior state-of-the-art approaches."],"url":"http://arxiv.org/abs/2404.16416v1"}
{"created":"2024-04-25 08:43:06","title":"Asking and Answering Questions to Extract Event-Argument Structures","abstract":"This paper presents a question-answering approach to extract document-level event-argument structures. We automatically ask and answer questions for each argument type an event may have. Questions are generated using manually defined templates and generative transformers. Template-based questions are generated using predefined role-specific wh-words and event triggers from the context document. Transformer-based questions are generated using large language models trained to formulate questions based on a passage and the expected answer. Additionally, we develop novel data augmentation strategies specialized in inter-sentential event-argument relations. We use a simple span-swapping technique, coreference resolution, and large language models to augment the training instances. Our approach enables transfer learning without any corpora-specific modifications and yields competitive results with the RAMS dataset. It outperforms previous work, and it is especially beneficial to extract arguments that appear in different sentences than the event trigger. We also present detailed quantitative and qualitative analyses shedding light on the most common errors made by our best model.","sentences":["This paper presents a question-answering approach to extract document-level event-argument structures.","We automatically ask and answer questions for each argument type an event may have.","Questions are generated using manually defined templates and generative transformers.","Template-based questions are generated using predefined role-specific wh-words and event triggers from the context document.","Transformer-based questions are generated using large language models trained to formulate questions based on a passage and the expected answer.","Additionally, we develop novel data augmentation strategies specialized in inter-sentential event-argument relations.","We use a simple span-swapping technique, coreference resolution, and large language models to augment the training instances.","Our approach enables transfer learning without any corpora-specific modifications and yields competitive results with the RAMS dataset.","It outperforms previous work, and it is especially beneficial to extract arguments that appear in different sentences than the event trigger.","We also present detailed quantitative and qualitative analyses shedding light on the most common errors made by our best model."],"url":"http://arxiv.org/abs/2404.16413v1"}
{"created":"2024-04-25 08:39:10","title":"Label-Free Topic-Focused Summarization Using Query Augmentation","abstract":"In today's data and information-rich world, summarization techniques are essential in harnessing vast text to extract key information and enhance decision-making and efficiency. In particular, topic-focused summarization is important due to its ability to tailor content to specific aspects of an extended text. However, this usually requires extensive labelled datasets and considerable computational power. This study introduces a novel method, Augmented-Query Summarization (AQS), for topic-focused summarization without the need for extensive labelled datasets, leveraging query augmentation and hierarchical clustering. This approach facilitates the transferability of machine learning models to the task of summarization, circumventing the need for topic-specific training. Through real-world tests, our method demonstrates the ability to generate relevant and accurate summaries, showing its potential as a cost-effective solution in data-rich environments. This innovation paves the way for broader application and accessibility in the field of topic-focused summarization technology, offering a scalable, efficient method for personalized content extraction.","sentences":["In today's data and information-rich world, summarization techniques are essential in harnessing vast text to extract key information and enhance decision-making and efficiency.","In particular, topic-focused summarization is important due to its ability to tailor content to specific aspects of an extended text.","However, this usually requires extensive labelled datasets and considerable computational power.","This study introduces a novel method, Augmented-Query Summarization (AQS), for topic-focused summarization without the need for extensive labelled datasets, leveraging query augmentation and hierarchical clustering.","This approach facilitates the transferability of machine learning models to the task of summarization, circumventing the need for topic-specific training.","Through real-world tests, our method demonstrates the ability to generate relevant and accurate summaries, showing its potential as a cost-effective solution in data-rich environments.","This innovation paves the way for broader application and accessibility in the field of topic-focused summarization technology, offering a scalable, efficient method for personalized content extraction."],"url":"http://arxiv.org/abs/2404.16411v1"}
{"created":"2024-04-25 08:36:09","title":"Cross-sensor super-resolution of irregularly sampled Sentinel-2 time series","abstract":"Satellite imaging generally presents a trade-off between the frequency of acquisitions and the spatial resolution of the images. Super-resolution is often advanced as a way to get the best of both worlds. In this work, we investigate multi-image super-resolution of satellite image time series, i.e. how multiple images of the same area acquired at different dates can help reconstruct a higher resolution observation. In particular, we extend state-of-the-art deep single and multi-image super-resolution algorithms, such as SRDiff and HighRes-net, to deal with irregularly sampled Sentinel-2 time series. We introduce BreizhSR, a new dataset for 4x super-resolution of Sentinel-2 time series using very high-resolution SPOT-6 imagery of Brittany, a French region. We show that using multiple images significantly improves super-resolution performance, and that a well-designed temporal positional encoding allows us to perform super-resolution for different times of the series. In addition, we observe a trade-off between spectral fidelity and perceptual quality of the reconstructed HR images, questioning future directions for super-resolution of Earth Observation data.","sentences":["Satellite imaging generally presents a trade-off between the frequency of acquisitions and the spatial resolution of the images.","Super-resolution is often advanced as a way to get the best of both worlds.","In this work, we investigate multi-image super-resolution of satellite image time series, i.e. how multiple images of the same area acquired at different dates can help reconstruct a higher resolution observation.","In particular, we extend state-of-the-art deep single and multi-image super-resolution algorithms, such as SRDiff and HighRes-net, to deal with irregularly sampled Sentinel-2 time series.","We introduce BreizhSR, a new dataset for 4x super-resolution of Sentinel-2 time series using very high-resolution SPOT-6 imagery of Brittany, a French region.","We show that using multiple images significantly improves super-resolution performance, and that a well-designed temporal positional encoding allows us to perform super-resolution for different times of the series.","In addition, we observe a trade-off between spectral fidelity and perceptual quality of the reconstructed HR images, questioning future directions for super-resolution of Earth Observation data."],"url":"http://arxiv.org/abs/2404.16409v1"}
{"created":"2024-04-25 08:33:32","title":"Regular Typed Unification","abstract":"Here we define a new unification algorithm for terms interpreted in semantic domains denoted by a subclass of regular types here called deterministic regular types. This reflects our intention not to handle the semantic universe as a homogeneous collection of values, but instead, to partition it in a way that is similar to data types in programming languages. We first define the new unification algorithm which is based on constraint generation and constraint solving, and then prove its main properties: termination, soundness, and completeness with respect to the semantics. Finally, we discuss how to apply this algorithm to a dynamically typed version of Prolog.","sentences":["Here we define a new unification algorithm for terms interpreted in semantic domains denoted by a subclass of regular types here called deterministic regular types.","This reflects our intention not to handle the semantic universe as a homogeneous collection of values, but instead, to partition it in a way that is similar to data types in programming languages.","We first define the new unification algorithm which is based on constraint generation and constraint solving, and then prove its main properties: termination, soundness, and completeness with respect to the semantics.","Finally, we discuss how to apply this algorithm to a dynamically typed version of Prolog."],"url":"http://arxiv.org/abs/2404.16406v1"}
{"created":"2024-04-25 08:18:18","title":"Revisiting Relevance Feedback for CLIP-based Interactive Image Retrieval","abstract":"Many image retrieval studies use metric learning to train an image encoder. However, metric learning cannot handle differences in users' preferences, and requires data to train an image encoder. To overcome these limitations, we revisit relevance feedback, a classic technique for interactive retrieval systems, and propose an interactive CLIP-based image retrieval system with relevance feedback. Our retrieval system first executes the retrieval, collects each user's unique preferences through binary feedback, and returns images the user prefers. Even when users have various preferences, our retrieval system learns each user's preference through the feedback and adapts to the preference. Moreover, our retrieval system leverages CLIP's zero-shot transferability and achieves high accuracy without training. We empirically show that our retrieval system competes well with state-of-the-art metric learning in category-based image retrieval, despite not training image encoders specifically for each dataset. Furthermore, we set up two additional experimental settings where users have various preferences: one-label-based image retrieval and conditioned image retrieval. In both cases, our retrieval system effectively adapts to each user's preferences, resulting in improved accuracy compared to image retrieval without feedback. Overall, our work highlights the potential benefits of integrating CLIP with classic relevance feedback techniques to enhance image retrieval.","sentences":["Many image retrieval studies use metric learning to train an image encoder.","However, metric learning cannot handle differences in users' preferences, and requires data to train an image encoder.","To overcome these limitations, we revisit relevance feedback, a classic technique for interactive retrieval systems, and propose an interactive CLIP-based image retrieval system with relevance feedback.","Our retrieval system first executes the retrieval, collects each user's unique preferences through binary feedback, and returns images the user prefers.","Even when users have various preferences, our retrieval system learns each user's preference through the feedback and adapts to the preference.","Moreover, our retrieval system leverages CLIP's zero-shot transferability and achieves high accuracy without training.","We empirically show that our retrieval system competes well with state-of-the-art metric learning in category-based image retrieval, despite not training image encoders specifically for each dataset.","Furthermore, we set up two additional experimental settings where users have various preferences: one-label-based image retrieval and conditioned image retrieval.","In both cases, our retrieval system effectively adapts to each user's preferences, resulting in improved accuracy compared to image retrieval without feedback.","Overall, our work highlights the potential benefits of integrating CLIP with classic relevance feedback techniques to enhance image retrieval."],"url":"http://arxiv.org/abs/2404.16398v1"}
{"created":"2024-04-25 08:01:11","title":"Dirigent: Lightweight Serverless Orchestration","abstract":"While Function as a Service (FaaS) platforms can initialize function sandboxes on worker nodes in 10-100s of milliseconds, the latency to schedule functions in real FaaS clusters can be orders of magnitude higher. We find that the current approach of building FaaS cluster managers on top of legacy orchestration systems like Kubernetes leads to high scheduling delay at high sandbox churn, which is typical in FaaS clusters. While generic cluster managers use hierarchical abstractions and multiple internal components to manage and reconcile state with frequent persistent updates, this becomes a bottleneck for FaaS, where cluster state frequently changes as sandboxes are created on the critical path of requests. Based on our root cause analysis of performance issues in existing FaaS cluster managers, we propose Dirigent, a clean-slate system architecture for FaaS orchestration with three key principles. First, Dirigent optimizes internal cluster manager abstractions to simplify state management. Second, it eliminates persistent state updates on the critical path of function invocations, leveraging the fact that FaaS abstracts sandboxes from users to relax exact state reconstruction guarantees. Finally, Dirigent runs monolithic control and data planes to minimize internal communication overheads and maximize throughput. We compare Dirigent to state-of-the-art FaaS platforms and show that Dirigent reduces 99th percentile per-function scheduling latency for a production workload by 2.79x compared to AWS Lambda and can spin up 2500 sandboxes per second at low latency, which is 1250x more than with Knative.","sentences":["While Function as a Service (FaaS) platforms can initialize function sandboxes on worker nodes in 10-100s of milliseconds, the latency to schedule functions in real FaaS clusters can be orders of magnitude higher.","We find that the current approach of building FaaS cluster managers on top of legacy orchestration systems like Kubernetes leads to high scheduling delay at high sandbox churn, which is typical in FaaS clusters.","While generic cluster managers use hierarchical abstractions and multiple internal components to manage and reconcile state with frequent persistent updates, this becomes a bottleneck for FaaS, where cluster state frequently changes as sandboxes are created on the critical path of requests.","Based on our root cause analysis of performance issues in existing FaaS cluster managers, we propose Dirigent, a clean-slate system architecture for FaaS orchestration with three key principles.","First, Dirigent optimizes internal cluster manager abstractions to simplify state management.","Second, it eliminates persistent state updates on the critical path of function invocations, leveraging the fact that FaaS abstracts sandboxes from users to relax exact state reconstruction guarantees.","Finally, Dirigent runs monolithic control and data planes to minimize internal communication overheads and maximize throughput.","We compare Dirigent to state-of-the-art FaaS platforms and show that Dirigent reduces 99th percentile per-function scheduling latency for a production workload by 2.79x compared to AWS Lambda and can spin up 2500 sandboxes per second at low latency, which is 1250x more than with Knative."],"url":"http://arxiv.org/abs/2404.16393v1"}
{"created":"2024-04-25 07:51:26","title":"Efficiency in Focus: LayerNorm as a Catalyst for Fine-tuning Medical Visual Language Pre-trained Models","abstract":"In the realm of Medical Visual Language Models (Med-VLMs), the quest for universal efficient fine-tuning mechanisms remains paramount, especially given researchers in interdisciplinary fields are often extremely short of training resources, yet largely unexplored. Given the unique challenges in the medical domain, such as limited data scope and significant domain-specific requirements, evaluating and adapting Parameter-Efficient Fine-Tuning (PEFT) methods specifically for Med-VLMs is essential. Most of the current PEFT methods on Med-VLMs have yet to be comprehensively investigated but mainly focus on adding some components to the model's structure or input. However, fine-tuning intrinsic model components often yields better generality and consistency, and its impact on the ultimate performance of Med-VLMs has been widely overlooked and remains understudied. In this paper, we endeavour to explore an alternative to traditional PEFT methods, especially the impact of fine-tuning LayerNorm layers, FFNs and Attention layers on the Med-VLMs. Our comprehensive studies span both small-scale and large-scale Med-VLMs, evaluating their performance under various fine-tuning paradigms across tasks such as Medical Visual Question Answering and Medical Imaging Report Generation. The findings reveal unique insights into the effects of intrinsic parameter fine-tuning methods on fine-tuning Med-VLMs to downstream tasks and expose fine-tuning solely the LayerNorm layers not only surpasses the efficiency of traditional PEFT methods but also retains the model's accuracy and generalization capabilities across a spectrum of medical downstream tasks. The experiments show LayerNorm fine-tuning's superior adaptability and scalability, particularly in the context of large-scale Med-VLMs.","sentences":["In the realm of Medical Visual Language Models (Med-VLMs), the quest for universal efficient fine-tuning mechanisms remains paramount, especially given researchers in interdisciplinary fields are often extremely short of training resources, yet largely unexplored.","Given the unique challenges in the medical domain, such as limited data scope and significant domain-specific requirements, evaluating and adapting Parameter-Efficient Fine-Tuning (PEFT) methods specifically for Med-VLMs is essential.","Most of the current PEFT methods on Med-VLMs have yet to be comprehensively investigated but mainly focus on adding some components to the model's structure or input.","However, fine-tuning intrinsic model components often yields better generality and consistency, and its impact on the ultimate performance of Med-VLMs has been widely overlooked and remains understudied.","In this paper, we endeavour to explore an alternative to traditional PEFT methods, especially the impact of fine-tuning LayerNorm layers, FFNs and Attention layers on the Med-VLMs.","Our comprehensive studies span both small-scale and large-scale Med-VLMs, evaluating their performance under various fine-tuning paradigms across tasks such as Medical Visual Question Answering and Medical Imaging Report Generation.","The findings reveal unique insights into the effects of intrinsic parameter fine-tuning methods on fine-tuning Med-VLMs to downstream tasks and expose fine-tuning solely the LayerNorm layers not only surpasses the efficiency of traditional PEFT methods but also retains the model's accuracy and generalization capabilities across a spectrum of medical downstream tasks.","The experiments show LayerNorm fine-tuning's superior adaptability and scalability, particularly in the context of large-scale Med-VLMs."],"url":"http://arxiv.org/abs/2404.16385v1"}
{"created":"2024-04-25 07:31:42","title":"A Hypergraph Approach to Distributed Broadcast","abstract":"This paper explores the distributed broadcast problem within the context of network communications, a critical challenge in decentralized information dissemination. We put forth a novel hypergraph-based approach to address this issue, focusing on minimizing the number of broadcasts to ensure comprehensive data sharing among all network users. A key contribution of our work is the establishment of a general lower bound for the problem using the min-cut capacity of hypergraphs. Additionally, we present the distributed broadcast for quasi-trees (DBQT) algorithm tailored for the unique structure of quasi-trees, which is proven to be optimal. This paper advances both network communication strategies and hypergraph theory, with implications for a wide range of real-world applications, from vehicular and sensor networks to distributed storage systems.","sentences":["This paper explores the distributed broadcast problem within the context of network communications, a critical challenge in decentralized information dissemination.","We put forth a novel hypergraph-based approach to address this issue, focusing on minimizing the number of broadcasts to ensure comprehensive data sharing among all network users.","A key contribution of our work is the establishment of a general lower bound for the problem using the min-cut capacity of hypergraphs.","Additionally, we present the distributed broadcast for quasi-trees (DBQT) algorithm tailored for the unique structure of quasi-trees, which is proven to be optimal.","This paper advances both network communication strategies and hypergraph theory, with implications for a wide range of real-world applications, from vehicular and sensor networks to distributed storage systems."],"url":"http://arxiv.org/abs/2404.16376v1"}
{"created":"2024-04-25 07:29:17","title":"List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs","abstract":"Set-of-Mark (SoM) Prompting unleashes the visual grounding capability of GPT-4V, by enabling the model to associate visual objects with tags inserted on the image. These tags, marked with alphanumerics, can be indexed via text tokens for easy reference. Despite the extraordinary performance from GPT-4V, we observe that other Multimodal Large Language Models (MLLMs) struggle to understand these visual tags. To promote the learning of SoM prompting for open-source models, we propose a new learning paradigm: \"list items one by one,\" which asks the model to enumerate and describe all visual tags placed on the image following the alphanumeric orders of tags. By integrating our curated dataset with other visual instruction tuning datasets, we are able to equip existing MLLMs with the SoM prompting ability. Furthermore, we evaluate our finetuned SoM models on five MLLM benchmarks. We find that this new dataset, even in a relatively small size (10k-30k images with tags), significantly enhances visual reasoning capabilities and reduces hallucinations for MLLMs. Perhaps surprisingly, these improvements persist even when the visual tags are omitted from input images during inference. This suggests the potential of \"list items one by one\" as a new paradigm for training MLLMs, which strengthens the object-text alignment through the use of visual tags in the training stage. Finally, we conduct analyses by probing trained models to understand the working mechanism of SoM. Our code and data are available at \\url{https://github.com/zzxslp/SoM-LLaVA}.","sentences":["Set-of-Mark (SoM)","Prompting unleashes the visual grounding capability of GPT-4V, by enabling the model to associate visual objects with tags inserted on the image.","These tags, marked with alphanumerics, can be indexed via text tokens for easy reference.","Despite the extraordinary performance from GPT-4V, we observe that other Multimodal Large Language Models (MLLMs) struggle to understand these visual tags.","To promote the learning of SoM prompting for open-source models, we propose a new learning paradigm: \"list items one by one,\" which asks the model to enumerate and describe all visual tags placed on the image following the alphanumeric orders of tags.","By integrating our curated dataset with other visual instruction tuning datasets, we are able to equip existing MLLMs with the SoM prompting ability.","Furthermore, we evaluate our finetuned SoM models on five MLLM benchmarks.","We find that this new dataset, even in a relatively small size (10k-30k images with tags), significantly enhances visual reasoning capabilities and reduces hallucinations for MLLMs.","Perhaps surprisingly, these improvements persist even when the visual tags are omitted from input images during inference.","This suggests the potential of \"list items one by one\" as a new paradigm for training MLLMs, which strengthens the object-text alignment through the use of visual tags in the training stage.","Finally, we conduct analyses by probing trained models to understand the working mechanism of SoM.","Our code and data are available at \\url{https://github.com/zzxslp/SoM-LLaVA}."],"url":"http://arxiv.org/abs/2404.16375v1"}
{"created":"2024-04-25 07:21:14","title":"Multimodal Information Interaction for Medical Image Segmentation","abstract":"The use of multimodal data in assisted diagnosis and segmentation has emerged as a prominent area of interest in current research. However, one of the primary challenges is how to effectively fuse multimodal features. Most of the current approaches focus on the integration of multimodal features while ignoring the correlation and consistency between different modal features, leading to the inclusion of potentially irrelevant information. To address this issue, we introduce an innovative Multimodal Information Cross Transformer (MicFormer), which employs a dual-stream architecture to simultaneously extract features from each modality. Leveraging the Cross Transformer, it queries features from one modality and retrieves corresponding responses from another, facilitating effective communication between bimodal features. Additionally, we incorporate a deformable Transformer architecture to expand the search space. We conducted experiments on the MM-WHS dataset, and in the CT-MRI multimodal image segmentation task, we successfully improved the whole-heart segmentation DICE score to 85.57 and MIoU to 75.51. Compared to other multimodal segmentation techniques, our method outperforms by margins of 2.83 and 4.23, respectively. This demonstrates the efficacy of MicFormer in integrating relevant information between different modalities in multimodal tasks. These findings hold significant implications for multimodal image tasks, and we believe that MicFormer possesses extensive potential for broader applications across various domains. Access to our method is available at https://github.com/fxxJuses/MICFormer","sentences":["The use of multimodal data in assisted diagnosis and segmentation has emerged as a prominent area of interest in current research.","However, one of the primary challenges is how to effectively fuse multimodal features.","Most of the current approaches focus on the integration of multimodal features while ignoring the correlation and consistency between different modal features, leading to the inclusion of potentially irrelevant information.","To address this issue, we introduce an innovative Multimodal Information Cross Transformer (MicFormer), which employs a dual-stream architecture to simultaneously extract features from each modality.","Leveraging the Cross Transformer, it queries features from one modality and retrieves corresponding responses from another, facilitating effective communication between bimodal features.","Additionally, we incorporate a deformable Transformer architecture to expand the search space.","We conducted experiments on the MM-WHS dataset, and in the CT-MRI multimodal image segmentation task, we successfully improved the whole-heart segmentation DICE score to 85.57 and MIoU to 75.51.","Compared to other multimodal segmentation techniques, our method outperforms by margins of 2.83 and 4.23, respectively.","This demonstrates the efficacy of MicFormer in integrating relevant information between different modalities in multimodal tasks.","These findings hold significant implications for multimodal image tasks, and we believe that MicFormer possesses extensive potential for broader applications across various domains.","Access to our method is available at https://github.com/fxxJuses/MICFormer"],"url":"http://arxiv.org/abs/2404.16371v1"}
{"created":"2024-04-25 07:10:29","title":"Learning Syntax Without Planting Trees: Understanding When and Why Transformers Generalize Hierarchically","abstract":"Transformers trained on natural language data have been shown to learn its hierarchical structure and generalize to sentences with unseen syntactic structures without explicitly encoding any structural bias. In this work, we investigate sources of inductive bias in transformer models and their training that could cause such generalization behavior to emerge. We extensively experiment with transformer models trained on multiple synthetic datasets and with different training objectives and show that while other objectives e.g. sequence-to-sequence modeling, prefix language modeling, often failed to lead to hierarchical generalization, models trained with the language modeling objective consistently learned to generalize hierarchically. We then conduct pruning experiments to study how transformers trained with the language modeling objective encode hierarchical structure. When pruned, we find joint existence of subnetworks within the model with different generalization behaviors (subnetworks corresponding to hierarchical structure and linear order). Finally, we take a Bayesian perspective to further uncover transformers' preference for hierarchical generalization: We establish a correlation between whether transformers generalize hierarchically on a dataset and whether the simplest explanation of that dataset is provided by a hierarchical grammar compared to regular grammars exhibiting linear generalization.","sentences":["Transformers trained on natural language data have been shown to learn its hierarchical structure and generalize to sentences with unseen syntactic structures without explicitly encoding any structural bias.","In this work, we investigate sources of inductive bias in transformer models and their training that could cause such generalization behavior to emerge.","We extensively experiment with transformer models trained on multiple synthetic datasets and with different training objectives and show that while other objectives e.g. sequence-to-sequence modeling, prefix language modeling, often failed to lead to hierarchical generalization, models trained with the language modeling objective consistently learned to generalize hierarchically.","We then conduct pruning experiments to study how transformers trained with the language modeling objective encode hierarchical structure.","When pruned, we find joint existence of subnetworks within the model with different generalization behaviors (subnetworks corresponding to hierarchical structure and linear order).","Finally, we take a Bayesian perspective to further uncover transformers' preference for hierarchical generalization: We establish a correlation between whether transformers generalize hierarchically on a dataset and whether the simplest explanation of that dataset is provided by a hierarchical grammar compared to regular grammars exhibiting linear generalization."],"url":"http://arxiv.org/abs/2404.16367v1"}
{"created":"2024-04-25 07:09:05","title":"Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection","abstract":"Unsupervised graph anomaly detection aims at identifying rare patterns that deviate from the majority in a graph without the aid of labels, which is important for a variety of real-world applications. Recent advances have utilized Graph Neural Networks (GNNs) to learn effective node representations by aggregating information from neighborhoods. This is motivated by the hypothesis that nodes in the graph tend to exhibit consistent behaviors with their neighborhoods. However, such consistency can be disrupted by graph anomalies in multiple ways. Most existing methods directly employ GNNs to learn representations, disregarding the negative impact of graph anomalies on GNNs, resulting in sub-optimal node representations and anomaly detection performance. While a few recent approaches have redesigned GNNs for graph anomaly detection under semi-supervised label guidance, how to address the adverse effects of graph anomalies on GNNs in unsupervised scenarios and learn effective representations for anomaly detection are still under-explored. To bridge this gap, in this paper, we propose a simple yet effective framework for Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection (G3AD). Specifically, G3AD introduces two auxiliary networks along with correlation constraints to guard the GNNs from inconsistent information encoding. Furthermore, G3AD introduces an adaptive caching module to guard the GNNs from solely reconstructing the observed data that contains anomalies. Extensive experiments demonstrate that our proposed G3AD can outperform seventeen state-of-the-art methods on both synthetic and real-world datasets.","sentences":["Unsupervised graph anomaly detection aims at identifying rare patterns that deviate from the majority in a graph without the aid of labels, which is important for a variety of real-world applications.","Recent advances have utilized Graph Neural Networks (GNNs) to learn effective node representations by aggregating information from neighborhoods.","This is motivated by the hypothesis that nodes in the graph tend to exhibit consistent behaviors with their neighborhoods.","However, such consistency can be disrupted by graph anomalies in multiple ways.","Most existing methods directly employ GNNs to learn representations, disregarding the negative impact of graph anomalies on GNNs, resulting in sub-optimal node representations and anomaly detection performance.","While a few recent approaches have redesigned GNNs for graph anomaly detection under semi-supervised label guidance, how to address the adverse effects of graph anomalies on GNNs in unsupervised scenarios and learn effective representations for anomaly detection are still under-explored.","To bridge this gap, in this paper, we propose a simple yet effective framework for Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection (G3AD).","Specifically, G3AD introduces two auxiliary networks along with correlation constraints to guard the GNNs from inconsistent information encoding.","Furthermore, G3AD introduces an adaptive caching module to guard the GNNs from solely reconstructing the observed data that contains anomalies.","Extensive experiments demonstrate that our proposed G3AD can outperform seventeen state-of-the-art methods on both synthetic and real-world datasets."],"url":"http://arxiv.org/abs/2404.16366v1"}
{"created":"2024-04-25 07:08:00","title":"VISLA Benchmark: Evaluating Embedding Sensitivity to Semantic and Lexical Alterations","abstract":"Despite their remarkable successes, state-of-the-art language models face challenges in grasping certain important semantic details. This paper introduces the VISLA (Variance and Invariance to Semantic and Lexical Alterations) benchmark, designed to evaluate the semantic and lexical understanding of language models. VISLA presents a 3-way semantic (in)equivalence task with a triplet of sentences associated with an image, to evaluate both vision-language models (VLMs) and unimodal language models (ULMs). An evaluation involving 34 VLMs and 20 ULMs reveals surprising difficulties in distinguishing between lexical and semantic variations. Spatial semantics encoded by language models also appear to be highly sensitive to lexical information. Notably, text encoders of VLMs demonstrate greater sensitivity to semantic and lexical variations than unimodal text encoders. Our contributions include the unification of image-to-text and text-to-text retrieval tasks, an off-the-shelf evaluation without fine-tuning, and assessing LMs' semantic (in)variance in the presence of lexical alterations. The results highlight strengths and weaknesses across diverse vision and unimodal language models, contributing to a deeper understanding of their capabilities. % VISLA enables a rigorous evaluation, shedding light on language models' capabilities in handling semantic and lexical nuances. Data and code will be made available at https://github.com/Sri-Harsha/visla_benchmark.","sentences":["Despite their remarkable successes, state-of-the-art language models face challenges in grasping certain important semantic details.","This paper introduces the VISLA (Variance and Invariance to Semantic and Lexical Alterations) benchmark, designed to evaluate the semantic and lexical understanding of language models.","VISLA presents a 3-way semantic (in)equivalence task with a triplet of sentences associated with an image, to evaluate both vision-language models (VLMs) and unimodal language models (ULMs).","An evaluation involving 34 VLMs and 20 ULMs reveals surprising difficulties in distinguishing between lexical and semantic variations.","Spatial semantics encoded by language models also appear to be highly sensitive to lexical information.","Notably, text encoders of VLMs demonstrate greater sensitivity to semantic and lexical variations than unimodal text encoders.","Our contributions include the unification of image-to-text and text-to-text retrieval tasks, an off-the-shelf evaluation without fine-tuning, and assessing LMs' semantic (in)variance in the presence of lexical alterations.","The results highlight strengths and weaknesses across diverse vision and unimodal language models, contributing to a deeper understanding of their capabilities.","% VISLA enables a rigorous evaluation, shedding light on language models' capabilities in handling semantic and lexical nuances.","Data and code will be made available at https://github.com/Sri-Harsha/visla_benchmark."],"url":"http://arxiv.org/abs/2404.16365v1"}
{"created":"2024-04-25 07:02:07","title":"ReZero: Boosting MCTS-based Algorithms by Just-in-Time and Speedy Reanalyze","abstract":"MCTS-based algorithms, such as MuZero and its derivatives, have achieved widespread success in various decision-making domains. These algorithms employ the reanalyze process to enhance sample efficiency, albeit at the expense of significant wall-clock time consumption. To address this issue, we propose a general approach named ReZero to boost MCTS-based algorithms. Specifically, we propose a new scheme that simplifies data collecting and reanalyzing, which significantly reduces the search cost while guarantees the performance as well. Furthermore, to accelerate each search process, we conceive a method to reuse the subsequent information in the trajectory. The corresponding analysis conducted on the bandit model also provides auxiliary theoretical substantiation for our design. Experiments conducted on Atari environments and board games demonstrates that ReZero substantially improves training speed while maintaining high sample efficiency. The code is available as part of the LightZero benchmark at https://github.com/opendilab/LightZero.","sentences":["MCTS-based algorithms, such as MuZero and its derivatives, have achieved widespread success in various decision-making domains.","These algorithms employ the reanalyze process to enhance sample efficiency, albeit at the expense of significant wall-clock time consumption.","To address this issue, we propose a general approach named ReZero to boost MCTS-based algorithms.","Specifically, we propose a new scheme that simplifies data collecting and reanalyzing, which significantly reduces the search cost while guarantees the performance as well.","Furthermore, to accelerate each search process, we conceive a method to reuse the subsequent information in the trajectory.","The corresponding analysis conducted on the bandit model also provides auxiliary theoretical substantiation for our design.","Experiments conducted on Atari environments and board games demonstrates that ReZero substantially improves training speed while maintaining high sample efficiency.","The code is available as part of the LightZero benchmark at https://github.com/opendilab/LightZero."],"url":"http://arxiv.org/abs/2404.16364v1"}
{"created":"2024-04-25 06:42:32","title":"Evolutionary Causal Discovery with Relative Impact Stratification for Interpretable Data Analysis","abstract":"This study proposes Evolutionary Causal Discovery (ECD) for causal discovery that tailors response variables, predictor variables, and corresponding operators to research datasets. Utilizing genetic programming for variable relationship parsing, the method proceeds with the Relative Impact Stratification (RIS) algorithm to assess the relative impact of predictor variables on the response variable, facilitating expression simplification and enhancing the interpretability of variable relationships. ECD proposes an expression tree to visualize the RIS results, offering a differentiated depiction of unknown causal relationships compared to conventional causal discovery. The ECD method represents an evolution and augmentation of existing causal discovery methods, providing an interpretable approach for analyzing variable relationships in complex systems, particularly in healthcare settings with Electronic Health Record (EHR) data. Experiments on both synthetic and real-world EHR datasets demonstrate the efficacy of ECD in uncovering patterns and mechanisms among variables, maintaining high accuracy and stability across different noise levels. On the real-world EHR dataset, ECD reveals the intricate relationships between the response variable and other predictive variables, aligning with the results of structural equation modeling and shapley additive explanations analyses.","sentences":["This study proposes Evolutionary Causal Discovery (ECD) for causal discovery that tailors response variables, predictor variables, and corresponding operators to research datasets.","Utilizing genetic programming for variable relationship parsing, the method proceeds with the Relative Impact Stratification (RIS) algorithm to assess the relative impact of predictor variables on the response variable, facilitating expression simplification and enhancing the interpretability of variable relationships.","ECD proposes an expression tree to visualize the RIS results, offering a differentiated depiction of unknown causal relationships compared to conventional causal discovery.","The ECD method represents an evolution and augmentation of existing causal discovery methods, providing an interpretable approach for analyzing variable relationships in complex systems, particularly in healthcare settings with Electronic Health Record (EHR) data.","Experiments on both synthetic and real-world EHR datasets demonstrate the efficacy of ECD in uncovering patterns and mechanisms among variables, maintaining high accuracy and stability across different noise levels.","On the real-world EHR dataset, ECD reveals the intricate relationships between the response variable and other predictive variables, aligning with the results of structural equation modeling and shapley additive explanations analyses."],"url":"http://arxiv.org/abs/2404.16361v1"}
{"created":"2024-04-25 06:22:21","title":"Integration of Mixture of Experts and Multimodal Generative AI in Internet of Vehicles: A Survey","abstract":"Generative AI (GAI) can enhance the cognitive, reasoning, and planning capabilities of intelligent modules in the Internet of Vehicles (IoV) by synthesizing augmented datasets, completing sensor data, and making sequential decisions. In addition, the mixture of experts (MoE) can enable the distributed and collaborative execution of AI models without performance degradation between connected vehicles. In this survey, we explore the integration of MoE and GAI to enable Artificial General Intelligence in IoV, which can enable the realization of full autonomy for IoV with minimal human supervision and applicability in a wide range of mobility scenarios, including environment monitoring, traffic management, and autonomous driving. In particular, we present the fundamentals of GAI, MoE, and their interplay applications in IoV. Furthermore, we discuss the potential integration of MoE and GAI in IoV, including distributed perception and monitoring, collaborative decision-making and planning, and generative modeling and simulation. Finally, we present several potential research directions for facilitating the integration.","sentences":["Generative AI (GAI) can enhance the cognitive, reasoning, and planning capabilities of intelligent modules in the Internet of Vehicles (IoV) by synthesizing augmented datasets, completing sensor data, and making sequential decisions.","In addition, the mixture of experts (MoE) can enable the distributed and collaborative execution of AI models without performance degradation between connected vehicles.","In this survey, we explore the integration of MoE and GAI to enable Artificial General Intelligence in IoV, which can enable the realization of full autonomy for IoV with minimal human supervision and applicability in a wide range of mobility scenarios, including environment monitoring, traffic management, and autonomous driving.","In particular, we present the fundamentals of GAI, MoE, and their interplay applications in IoV.","Furthermore, we discuss the potential integration of MoE and GAI in IoV, including distributed perception and monitoring, collaborative decision-making and planning, and generative modeling and simulation.","Finally, we present several potential research directions for facilitating the integration."],"url":"http://arxiv.org/abs/2404.16356v1"}
{"created":"2024-04-25 05:59:44","title":"More Asymmetry Yields Faster Matrix Multiplication","abstract":"We present a new improvement on the laser method for designing fast matrix multiplication algorithms. The new method further develops the recent advances by [Duan, Wu, Zhou FOCS 2023] and [Vassilevska Williams, Xu, Xu, Zhou SODA 2024]. Surprisingly the new improvement is achieved by incorporating more asymmetry in the analysis, circumventing a fundamental tool of prior work that requires two of the three dimensions to be treated identically. The method yields a new bound on the square matrix multiplication exponent $$\\omega<2.371339,$$ improved from the previous bound of $\\omega<2.371552$. We also improve the bounds of the exponents for multiplying rectangular matrices of various shapes.","sentences":["We present a new improvement on the laser method for designing fast matrix multiplication algorithms.","The new method further develops the recent advances by [Duan, Wu, Zhou FOCS 2023] and [Vassilevska Williams, Xu, Xu, Zhou SODA 2024].","Surprisingly the new improvement is achieved by incorporating more asymmetry in the analysis, circumventing a fundamental tool of prior work that requires two of the three dimensions to be treated identically.","The method yields a new bound on the square matrix multiplication exponent $$\\omega<2.371339,$$ improved from the previous bound of $\\omega<2.371552$. We also improve the bounds of the exponents for multiplying rectangular matrices of various shapes."],"url":"http://arxiv.org/abs/2404.16349v1"}
{"created":"2024-04-25 05:07:50","title":"Training-Free Unsupervised Prompt for Vision-Language Models","abstract":"Prompt learning has become the most effective paradigm for adapting large pre-trained vision-language models (VLMs) to downstream tasks. Recently, unsupervised prompt tuning methods, such as UPL and POUF, directly leverage pseudo-labels as supervisory information to fine-tune additional adaptation modules on unlabeled data. However, inaccurate pseudo labels easily misguide the tuning process and result in poor representation capabilities. In light of this, we propose Training-Free Unsupervised Prompts (TFUP), which maximally preserves the inherent representation capabilities and enhances them with a residual connection to similarity-based prediction probabilities in a training-free and labeling-free manner. Specifically, we integrate both instance confidence and prototype scores to select representative samples, which are used to customize a reliable Feature Cache Model (FCM) for training-free inference. Then, we design a Multi-level Similarity Measure (MSM) that considers both feature-level and semantic-level similarities to calculate the distance between each test image and the cached sample as the weight of the corresponding cached label to generate similarity-based prediction probabilities. In this way, TFUP achieves surprising performance, even surpassing the training-base method on multiple classification datasets. Based on our TFUP, we propose a training-based approach (TFUP-T) to further boost the adaptation performance. In addition to the standard cross-entropy loss, TFUP-T adopts an additional marginal distribution entropy loss to constrain the model from a global perspective. Our TFUP-T achieves new state-of-the-art classification performance compared to unsupervised and few-shot adaptation approaches on multiple benchmarks. In particular, TFUP-T improves the classification accuracy of POUF by 3.3% on the most challenging Domain-Net dataset.","sentences":["Prompt learning has become the most effective paradigm for adapting large pre-trained vision-language models (VLMs) to downstream tasks.","Recently, unsupervised prompt tuning methods, such as UPL and POUF, directly leverage pseudo-labels as supervisory information to fine-tune additional adaptation modules on unlabeled data.","However, inaccurate pseudo labels easily misguide the tuning process and result in poor representation capabilities.","In light of this, we propose Training-Free Unsupervised Prompts (TFUP), which maximally preserves the inherent representation capabilities and enhances them with a residual connection to similarity-based prediction probabilities in a training-free and labeling-free manner.","Specifically, we integrate both instance confidence and prototype scores to select representative samples, which are used to customize a reliable Feature Cache Model (FCM) for training-free inference.","Then, we design a Multi-level Similarity Measure (MSM) that considers both feature-level and semantic-level similarities to calculate the distance between each test image and the cached sample as the weight of the corresponding cached label to generate similarity-based prediction probabilities.","In this way, TFUP achieves surprising performance, even surpassing the training-base method on multiple classification datasets.","Based on our TFUP, we propose a training-based approach (TFUP-T) to further boost the adaptation performance.","In addition to the standard cross-entropy loss, TFUP-T adopts an additional marginal distribution entropy loss to constrain the model from a global perspective.","Our TFUP-T achieves new state-of-the-art classification performance compared to unsupervised and few-shot adaptation approaches on multiple benchmarks.","In particular, TFUP-T improves the classification accuracy of POUF by 3.3% on the most challenging Domain-Net dataset."],"url":"http://arxiv.org/abs/2404.16339v1"}
{"created":"2024-04-25 04:53:43","title":"FedStyle: Style-Based Federated Learning Crowdsourcing Framework for Art Commissions","abstract":"The unique artistic style is crucial to artists' occupational competitiveness, yet prevailing Art Commission Platforms rarely support style-based retrieval. Meanwhile, the fast-growing generative AI techniques aggravate artists' concerns about releasing personal artworks to public platforms. To achieve artistic style-based retrieval without exposing personal artworks, we propose FedStyle, a style-based federated learning crowdsourcing framework. It allows artists to train local style models and share model parameters rather than artworks for collaboration. However, most artists possess a unique artistic style, resulting in severe model drift among them. FedStyle addresses such extreme data heterogeneity by having artists learn their abstract style representations and align with the server, rather than merely aggregating model parameters lacking semantics. Besides, we introduce contrastive learning to meticulously construct the style representation space, pulling artworks with similar styles closer and keeping different ones apart in the embedding space. Extensive experiments on the proposed datasets demonstrate the superiority of FedStyle.","sentences":["The unique artistic style is crucial to artists' occupational competitiveness, yet prevailing Art Commission Platforms rarely support style-based retrieval.","Meanwhile, the fast-growing generative AI techniques aggravate artists' concerns about releasing personal artworks to public platforms.","To achieve artistic style-based retrieval without exposing personal artworks, we propose FedStyle, a style-based federated learning crowdsourcing framework.","It allows artists to train local style models and share model parameters rather than artworks for collaboration.","However, most artists possess a unique artistic style, resulting in severe model drift among them.","FedStyle addresses such extreme data heterogeneity by having artists learn their abstract style representations and align with the server, rather than merely aggregating model parameters lacking semantics.","Besides, we introduce contrastive learning to meticulously construct the style representation space, pulling artworks with similar styles closer and keeping different ones apart in the embedding space.","Extensive experiments on the proposed datasets demonstrate the superiority of FedStyle."],"url":"http://arxiv.org/abs/2404.16336v1"}
{"created":"2024-04-25 04:31:41","title":"On Approximating the Dynamic and Discrete Network Flow Problem","abstract":"We examine the dynamic network flow problem under the assumption that the flow consists of discrete units. The dynamic network flow problem is commonly addressed in the context of developing evacuation plans, where the flow is typically treated as a continuous quantity. However, real-world scenarios often involve moving groups, such as families, as single units. We demonstrate that solving the dynamic flow problem with this consideration is APX-hard. Conversely, we present a PTAS for instances where the base graph is a path with a constant number of nodes. We introduce a `ready time' constraint to the minsum bin packing problem, meaning certain items cannot be placed in specific bins, develop a PTAS for this modified problem, and apply our algorithms to the discrete and dynamic flow problem.","sentences":["We examine the dynamic network flow problem under the assumption that the flow consists of discrete units.","The dynamic network flow problem is commonly addressed in the context of developing evacuation plans, where the flow is typically treated as a continuous quantity.","However, real-world scenarios often involve moving groups, such as families, as single units.","We demonstrate that solving the dynamic flow problem with this consideration is APX-hard.","Conversely, we present a PTAS for instances where the base graph is a path with a constant number of nodes.","We introduce a `ready time' constraint to the minsum bin packing problem, meaning certain items cannot be placed in specific bins, develop a PTAS for this modified problem, and apply our algorithms to the discrete and dynamic flow problem."],"url":"http://arxiv.org/abs/2404.16329v1"}
{"created":"2024-04-25 04:23:40","title":"NeuroKoopman Dynamic Causal Discovery","abstract":"In many real-world applications where the system dynamics has an underlying interdependency among its variables (such as power grid, economics, neuroscience, omics networks, environmental ecosystems, and others), one is often interested in knowing whether the past values of one time series influences the future of another, known as Granger causality, and the associated underlying dynamics. This paper introduces a Koopman-inspired framework that leverages neural networks for data-driven learning of the Koopman bases, termed NeuroKoopman Dynamic Causal Discovery (NKDCD), for reliably inferring the Granger causality along with the underlying nonlinear dynamics. NKDCD employs an autoencoder architecture that lifts the nonlinear dynamics to a higher dimension using data-learned bases, where the lifted time series can be reliably modeled linearly. The lifting function, the linear Granger causality lag matrices, and the projection function (from lifted space to base space) are all represented as multilayer perceptrons and are all learned simultaneously in one go. NKDCD also utilizes sparsity-inducing penalties on the weights of the lag matrices, encouraging the model to select only the needed causal dependencies within the data. Through extensive testing on practically applicable datasets, it is shown that the NKDCD outperforms the existing nonlinear Granger causality discovery approaches.","sentences":["In many real-world applications where the system dynamics has an underlying interdependency among its variables (such as power grid, economics, neuroscience, omics networks, environmental ecosystems, and others), one is often interested in knowing whether the past values of one time series influences the future of another, known as Granger causality, and the associated underlying dynamics.","This paper introduces a Koopman-inspired framework that leverages neural networks for data-driven learning of the Koopman bases, termed NeuroKoopman Dynamic Causal Discovery (NKDCD), for reliably inferring the Granger causality along with the underlying nonlinear dynamics.","NKDCD employs an autoencoder architecture that lifts the nonlinear dynamics to a higher dimension using data-learned bases, where the lifted time series can be reliably modeled linearly.","The lifting function, the linear Granger causality lag matrices, and the projection function (from lifted space to base space) are all represented as multilayer perceptrons and are all learned simultaneously in one go.","NKDCD also utilizes sparsity-inducing penalties on the weights of the lag matrices, encouraging the model to select only the needed causal dependencies within the data.","Through extensive testing on practically applicable datasets, it is shown that the NKDCD outperforms the existing nonlinear Granger causality discovery approaches."],"url":"http://arxiv.org/abs/2404.16326v1"}
{"created":"2024-04-25 04:21:57","title":"Semantic Segmentation Refiner for Ultrasound Applications with Zero-Shot Foundation Models","abstract":"Despite the remarkable success of deep learning in medical imaging analysis, medical image segmentation remains challenging due to the scarcity of high-quality labeled images for supervision. Further, the significant domain gap between natural and medical images in general and ultrasound images in particular hinders fine-tuning models trained on natural images to the task at hand. In this work, we address the performance degradation of segmentation models in low-data regimes and propose a prompt-less segmentation method harnessing the ability of segmentation foundation models to segment abstract shapes. We do that via our novel prompt point generation algorithm which uses coarse semantic segmentation masks as input and a zero-shot prompt-able foundation model as an optimization target. We demonstrate our method on a segmentation findings task (pathologic anomalies) in ultrasound images. Our method's advantages are brought to light in varying degrees of low-data regime experiments on a small-scale musculoskeletal ultrasound images dataset, yielding a larger performance gain as the training set size decreases.","sentences":["Despite the remarkable success of deep learning in medical imaging analysis, medical image segmentation remains challenging due to the scarcity of high-quality labeled images for supervision.","Further, the significant domain gap between natural and medical images in general and ultrasound images in particular hinders fine-tuning models trained on natural images to the task at hand.","In this work, we address the performance degradation of segmentation models in low-data regimes and propose a prompt-less segmentation method harnessing the ability of segmentation foundation models to segment abstract shapes.","We do that via our novel prompt point generation algorithm which uses coarse semantic segmentation masks as input and a zero-shot prompt-able foundation model as an optimization target.","We demonstrate our method on a segmentation findings task (pathologic anomalies) in ultrasound images.","Our method's advantages are brought to light in varying degrees of low-data regime experiments on a small-scale musculoskeletal ultrasound images dataset, yielding a larger performance gain as the training set size decreases."],"url":"http://arxiv.org/abs/2404.16325v1"}
{"created":"2024-04-25 03:46:53","title":"FLAASH: Flexible Accelerator Architecture for Sparse High-Order Tensor Contraction","abstract":"Tensors play a vital role in machine learning (ML) and often exhibit properties best explored while maintaining high-order. Efficiently performing ML computations requires taking advantage of sparsity, but generalized hardware support is challenging. This paper introduces FLAASH, a flexible and modular accelerator design for sparse tensor contraction that achieves over 25x speedup for a deep learning workload. Our architecture performs sparse high-order tensor contraction by distributing sparse dot products, or portions thereof, to numerous Sparse Dot Product Engines (SDPEs). Memory structure and job distribution can be customized, and we demonstrate a simple approach as a proof of concept. We address the challenges associated with control flow to navigate data structures, high-order representation, and high-sparsity handling. The effectiveness of our approach is demonstrated through various evaluations, showcasing significant speedup as sparsity and order increase.","sentences":["Tensors play a vital role in machine learning (ML) and often exhibit properties best explored while maintaining high-order.","Efficiently performing ML computations requires taking advantage of sparsity, but generalized hardware support is challenging.","This paper introduces FLAASH, a flexible and modular accelerator design for sparse tensor contraction that achieves over 25x speedup for a deep learning workload.","Our architecture performs sparse high-order tensor contraction by distributing sparse dot products, or portions thereof, to numerous Sparse Dot Product Engines (SDPEs).","Memory structure and job distribution can be customized, and we demonstrate a simple approach as a proof of concept.","We address the challenges associated with control flow to navigate data structures, high-order representation, and high-sparsity handling.","The effectiveness of our approach is demonstrated through various evaluations, showcasing significant speedup as sparsity and order increase."],"url":"http://arxiv.org/abs/2404.16317v1"}
{"created":"2024-04-25 03:44:45","title":"Parallel and (Nearly) Work-Efficient Dynamic Programming","abstract":"The idea of dynamic programming (DP), proposed by Bellman in the 1950s, is one of the most important algorithmic techniques. However, in parallel, many fundamental and sequentially simple problems become more challenging, and open to a (nearly) work-efficient solution (i.e., the work is off by at most a polylogarithmic factor over the best sequential solution). In fact, sequential DP algorithms employ many advanced optimizations such as decision monotonicity or special data structures, and achieve better work than straightforward solutions. Many such optimizations are inherently sequential, which creates extra challenges for a parallel algorithm to achieve the same work bound.   The goal of this paper is to achieve (nearly) work-efficient parallel DP algorithms by parallelizing classic, highly-optimized and practical sequential algorithms. We show a general framework called the Cordon Algorithm for parallel DP algorithms, and use it to solve several classic problems. Our selection of problems includes Longest Increasing Subsequence (LIS), sparse Longest Common Subsequence (LCS), convex/concave generalized Least Weight Subsequence (LWS), Optimal Alphabetic Tree (OAT), and more. We show how the Cordon Algorithm can be used to achieve the same level of optimization as the sequential algorithms, and achieve good parallelism. Many of our algorithms are conceptually simple, and we show some experimental results as proofs-of-concept.","sentences":["The idea of dynamic programming (DP), proposed by Bellman in the 1950s, is one of the most important algorithmic techniques.","However, in parallel, many fundamental and sequentially simple problems become more challenging, and open to a (nearly) work-efficient solution (i.e., the work is off by at most a polylogarithmic factor over the best sequential solution).","In fact, sequential DP algorithms employ many advanced optimizations such as decision monotonicity or special data structures, and achieve better work than straightforward solutions.","Many such optimizations are inherently sequential, which creates extra challenges for a parallel algorithm to achieve the same work bound.   ","The goal of this paper is to achieve (nearly) work-efficient parallel DP algorithms by parallelizing classic, highly-optimized and practical sequential algorithms.","We show a general framework called the Cordon Algorithm for parallel DP algorithms, and use it to solve several classic problems.","Our selection of problems includes Longest Increasing Subsequence (LIS), sparse Longest Common Subsequence (LCS), convex/concave generalized Least Weight Subsequence (LWS), Optimal Alphabetic Tree (OAT), and more.","We show how the Cordon Algorithm can be used to achieve the same level of optimization as the sequential algorithms, and achieve good parallelism.","Many of our algorithms are conceptually simple, and we show some experimental results as proofs-of-concept."],"url":"http://arxiv.org/abs/2404.16314v1"}
{"created":"2024-04-25 03:23:28","title":"WorldValuesBench: A Large-Scale Benchmark Dataset for Multi-Cultural Value Awareness of Language Models","abstract":"The awareness of multi-cultural human values is critical to the ability of language models (LMs) to generate safe and personalized responses. However, this awareness of LMs has been insufficiently studied, since the computer science community lacks access to the large-scale real-world data about multi-cultural values. In this paper, we present WorldValuesBench, a globally diverse, large-scale benchmark dataset for the multi-cultural value prediction task, which requires a model to generate a rating response to a value question based on demographic contexts. Our dataset is derived from an influential social science project, World Values Survey (WVS), that has collected answers to hundreds of value questions (e.g., social, economic, ethical) from 94,728 participants worldwide. We have constructed more than 20 million examples of the type \"(demographic attributes, value question) $\\rightarrow$ answer\" from the WVS responses. We perform a case study using our dataset and show that the task is challenging for strong open and closed-source models. On merely $11.1\\%$, $25.0\\%$, $72.2\\%$, and $75.0\\%$ of the questions, Alpaca-7B, Vicuna-7B-v1.5, Mixtral-8x7B-Instruct-v0.1, and GPT-3.5 Turbo can respectively achieve $<0.2$ Wasserstein 1-distance from the human normalized answer distributions. WorldValuesBench opens up new research avenues in studying limitations and opportunities in multi-cultural value awareness of LMs.","sentences":["The awareness of multi-cultural human values is critical to the ability of language models (LMs) to generate safe and personalized responses.","However, this awareness of LMs has been insufficiently studied, since the computer science community lacks access to the large-scale real-world data about multi-cultural values.","In this paper, we present WorldValuesBench, a globally diverse, large-scale benchmark dataset for the multi-cultural value prediction task, which requires a model to generate a rating response to a value question based on demographic contexts.","Our dataset is derived from an influential social science project, World Values Survey (WVS), that has collected answers to hundreds of value questions (e.g., social, economic, ethical) from 94,728 participants worldwide.","We have constructed more than 20 million examples of the type \"(demographic attributes, value question) $\\rightarrow$ answer\" from the WVS responses.","We perform a case study using our dataset and show that the task is challenging for strong open and closed-source models.","On merely $11.1\\%$, $25.0\\%$, $72.2\\%$, and $75.0\\%$ of the questions, Alpaca-7B, Vicuna-7B-v1.5, Mixtral-8x7B-Instruct-v0.1, and GPT-3.5","Turbo can respectively achieve $<0.2$ Wasserstein 1-distance from the human normalized answer distributions.","WorldValuesBench opens up new research avenues in studying limitations and opportunities in multi-cultural value awareness of LMs."],"url":"http://arxiv.org/abs/2404.16308v1"}
{"created":"2024-04-25 03:22:48","title":"Boosting Model Resilience via Implicit Adversarial Data Augmentation","abstract":"Data augmentation plays a pivotal role in enhancing and diversifying training data. Nonetheless, consistently improving model performance in varied learning scenarios, especially those with inherent data biases, remains challenging. To address this, we propose to augment the deep features of samples by incorporating their adversarial and anti-adversarial perturbation distributions, enabling adaptive adjustment in the learning difficulty tailored to each sample's specific characteristics. We then theoretically reveal that our augmentation process approximates the optimization of a surrogate loss function as the number of augmented copies increases indefinitely. This insight leads us to develop a meta-learning-based framework for optimizing classifiers with this novel loss, introducing the effects of augmentation while bypassing the explicit augmentation process. We conduct extensive experiments across four common biased learning scenarios: long-tail learning, generalized long-tail learning, noisy label learning, and subpopulation shift learning. The empirical results demonstrate that our method consistently achieves state-of-the-art performance, highlighting its broad adaptability.","sentences":["Data augmentation plays a pivotal role in enhancing and diversifying training data.","Nonetheless, consistently improving model performance in varied learning scenarios, especially those with inherent data biases, remains challenging.","To address this, we propose to augment the deep features of samples by incorporating their adversarial and anti-adversarial perturbation distributions, enabling adaptive adjustment in the learning difficulty tailored to each sample's specific characteristics.","We then theoretically reveal that our augmentation process approximates the optimization of a surrogate loss function as the number of augmented copies increases indefinitely.","This insight leads us to develop a meta-learning-based framework for optimizing classifiers with this novel loss, introducing the effects of augmentation while bypassing the explicit augmentation process.","We conduct extensive experiments across four common biased learning scenarios: long-tail learning, generalized long-tail learning, noisy label learning, and subpopulation shift learning.","The empirical results demonstrate that our method consistently achieves state-of-the-art performance, highlighting its broad adaptability."],"url":"http://arxiv.org/abs/2404.16307v1"}
{"created":"2024-04-25 02:51:55","title":"Style Adaptation for Domain-adaptive Semantic Segmentation","abstract":"Unsupervised Domain Adaptation (UDA) refers to the method that utilizes annotated source domain data and unlabeled target domain data to train a model capable of generalizing to the target domain data. Domain discrepancy leads to a significant decrease in the performance of general network models trained on the source domain data when applied to the target domain. We introduce a straightforward approach to mitigate the domain discrepancy, which necessitates no additional parameter calculations and seamlessly integrates with self-training-based UDA methods. Through the transfer of the target domain style to the source domain in the latent feature space, the model is trained to prioritize the target domain style during the decision-making process. We tackle the problem at both the image-level and shallow feature map level by transferring the style information from the target domain to the source domain data. As a result, we obtain a model that exhibits superior performance on the target domain. Our method yields remarkable enhancements in the state-of-the-art performance for synthetic-to-real UDA tasks. For example, our proposed method attains a noteworthy UDA performance of 76.93 mIoU on the GTA->Cityscapes dataset, representing a notable improvement of +1.03 percentage points over the previous state-of-the-art results.","sentences":["Unsupervised Domain Adaptation (UDA) refers to the method that utilizes annotated source domain data and unlabeled target domain data to train a model capable of generalizing to the target domain data.","Domain discrepancy leads to a significant decrease in the performance of general network models trained on the source domain data when applied to the target domain.","We introduce a straightforward approach to mitigate the domain discrepancy, which necessitates no additional parameter calculations and seamlessly integrates with self-training-based UDA methods.","Through the transfer of the target domain style to the source domain in the latent feature space, the model is trained to prioritize the target domain style during the decision-making process.","We tackle the problem at both the image-level and shallow feature map level by transferring the style information from the target domain to the source domain data.","As a result, we obtain a model that exhibits superior performance on the target domain.","Our method yields remarkable enhancements in the state-of-the-art performance for synthetic-to-real UDA tasks.","For example, our proposed method attains a noteworthy UDA performance of 76.93 mIoU on the GTA->Cityscapes dataset, representing a notable improvement of +1.03 percentage points over the previous state-of-the-art results."],"url":"http://arxiv.org/abs/2404.16301v1"}
{"created":"2024-04-25 02:48:16","title":"Reinforcement Learning with Generative Models for Compact Support Sets","abstract":"Foundation models contain a wealth of information from their vast number of training samples. However, most prior arts fail to extract this information in a precise and efficient way for small sample sizes. In this work, we propose a framework utilizing reinforcement learning as a control for foundation models, allowing for the granular generation of small, focused synthetic support sets to augment the performance of neural network models on real data classification tasks. We first allow a reinforcement learning agent access to a novel context based dictionary; the agent then uses this dictionary with a novel prompt structure to form and optimize prompts as inputs to generative models, receiving feedback based on a reward function combining the change in validation accuracy and entropy. A support set is formed this way over several exploration steps. Our framework produced excellent results, increasing classification accuracy by significant margins for no additional labelling or data cost.","sentences":["Foundation models contain a wealth of information from their vast number of training samples.","However, most prior arts fail to extract this information in a precise and efficient way for small sample sizes.","In this work, we propose a framework utilizing reinforcement learning as a control for foundation models, allowing for the granular generation of small, focused synthetic support sets to augment the performance of neural network models on real data classification tasks.","We first allow a reinforcement learning agent access to a novel context based dictionary; the agent then uses this dictionary with a novel prompt structure to form and optimize prompts as inputs to generative models, receiving feedback based on a reward function combining the change in validation accuracy and entropy.","A support set is formed this way over several exploration steps.","Our framework produced excellent results, increasing classification accuracy by significant margins for no additional labelling or data cost."],"url":"http://arxiv.org/abs/2404.16300v1"}
{"created":"2024-04-25 02:25:35","title":"LLM-Based Section Identifiers Excel on Open Source but Stumble in Real World Applications","abstract":"Electronic health records (EHR) even though a boon for healthcare practitioners, are growing convoluted and longer every day. Sifting around these lengthy EHRs is taxing and becomes a cumbersome part of physician-patient interaction. Several approaches have been proposed to help alleviate this prevalent issue either via summarization or sectioning, however, only a few approaches have truly been helpful in the past. With the rise of automated methods, machine learning (ML) has shown promise in solving the task of identifying relevant sections in EHR. However, most ML methods rely on labeled data which is difficult to get in healthcare. Large language models (LLMs) on the other hand, have performed impressive feats in natural language processing (NLP), that too in a zero-shot manner, i.e. without any labeled data. To that end, we propose using LLMs to identify relevant section headers. We find that GPT-4 can effectively solve the task on both zero and few-shot settings as well as segment dramatically better than state-of-the-art methods. Additionally, we also annotate a much harder real world dataset and find that GPT-4 struggles to perform well, alluding to further research and harder benchmarks.","sentences":["Electronic health records (EHR) even though a boon for healthcare practitioners, are growing convoluted and longer every day.","Sifting around these lengthy EHRs is taxing and becomes a cumbersome part of physician-patient interaction.","Several approaches have been proposed to help alleviate this prevalent issue either via summarization or sectioning, however, only a few approaches have truly been helpful in the past.","With the rise of automated methods, machine learning (ML) has shown promise in solving the task of identifying relevant sections in EHR.","However, most ML methods rely on labeled data which is difficult to get in healthcare.","Large language models (LLMs) on the other hand, have performed impressive feats in natural language processing (NLP), that too in a zero-shot manner, i.e. without any labeled data.","To that end, we propose using LLMs to identify relevant section headers.","We find that GPT-4 can effectively solve the task on both zero and few-shot settings as well as segment dramatically better than state-of-the-art methods.","Additionally, we also annotate a much harder real world dataset and find that GPT-4 struggles to perform well, alluding to further research and harder benchmarks."],"url":"http://arxiv.org/abs/2404.16294v1"}
{"created":"2024-04-25 02:23:11","title":"One Noise to Rule Them All: Learning a Unified Model of Spatially-Varying Noise Patterns","abstract":"Procedural noise is a fundamental component of computer graphics pipelines, offering a flexible way to generate textures that exhibit \"natural\" random variation. Many different types of noise exist, each produced by a separate algorithm. In this paper, we present a single generative model which can learn to generate multiple types of noise as well as blend between them. In addition, it is capable of producing spatially-varying noise blends despite not having access to such data for training. These features are enabled by training a denoising diffusion model using a novel combination of data augmentation and network conditioning techniques. Like procedural noise generators, the model's behavior is controllable via interpretable parameters and a source of randomness. We use our model to produce a variety of visually compelling noise textures. We also present an application of our model to improving inverse procedural material design; using our model in place of fixed-type noise nodes in a procedural material graph results in higher-fidelity material reconstructions without needing to know the type of noise in advance.","sentences":["Procedural noise is a fundamental component of computer graphics pipelines, offering a flexible way to generate textures that exhibit \"natural\" random variation.","Many different types of noise exist, each produced by a separate algorithm.","In this paper, we present a single generative model which can learn to generate multiple types of noise as well as blend between them.","In addition, it is capable of producing spatially-varying noise blends despite not having access to such data for training.","These features are enabled by training a denoising diffusion model using a novel combination of data augmentation and network conditioning techniques.","Like procedural noise generators, the model's behavior is controllable via interpretable parameters and a source of randomness.","We use our model to produce a variety of visually compelling noise textures.","We also present an application of our model to improving inverse procedural material design; using our model in place of fixed-type noise nodes in a procedural material graph results in higher-fidelity material reconstructions without needing to know the type of noise in advance."],"url":"http://arxiv.org/abs/2404.16292v1"}
{"created":"2024-04-25 01:53:21","title":"Timely Communications for Remote Inference","abstract":"In this paper, we analyze the impact of data freshness on remote inference systems, where a pre-trained neural network infers a time-varying target (e.g., the locations of vehicles and pedestrians) based on features (e.g., video frames) observed at a sensing node (e.g., a camera). One might expect that the performance of a remote inference system degrades monotonically as the feature becomes stale. Using an information-theoretic analysis, we show that this is true if the feature and target data sequence can be closely approximated as a Markov chain, whereas it is not true if the data sequence is far from Markovian. Hence, the inference error is a function of Age of Information (AoI), where the function could be non-monotonic. To minimize the inference error in real-time, we propose a new \"selection-from-buffer\" model for sending the features, which is more general than the \"generate-at-will\" model used in earlier studies. In addition, we design low-complexity scheduling policies to improve inference performance. For single-source, single-channel systems, we provide an optimal scheduling policy. In multi-source, multi-channel systems, the scheduling problem becomes a multi-action restless multi-armed bandit problem. For this setting, we design a new scheduling policy by integrating Whittle index-based source selection and duality-based feature selection-from-buffer algorithms. This new scheduling policy is proven to be asymptotically optimal. These scheduling results hold for minimizing general AoI functions (monotonic or non-monotonic). Data-driven evaluations demonstrate the significant advantages of our proposed scheduling policies.","sentences":["In this paper, we analyze the impact of data freshness on remote inference systems, where a pre-trained neural network infers a time-varying target (e.g., the locations of vehicles and pedestrians) based on features (e.g., video frames) observed at a sensing node (e.g., a camera).","One might expect that the performance of a remote inference system degrades monotonically as the feature becomes stale.","Using an information-theoretic analysis, we show that this is true if the feature and target data sequence can be closely approximated as a Markov chain, whereas it is not true if the data sequence is far from Markovian.","Hence, the inference error is a function of Age of Information (AoI), where the function could be non-monotonic.","To minimize the inference error in real-time, we propose a new \"selection-from-buffer\" model for sending the features, which is more general than the \"generate-at-will\" model used in earlier studies.","In addition, we design low-complexity scheduling policies to improve inference performance.","For single-source, single-channel systems, we provide an optimal scheduling policy.","In multi-source, multi-channel systems, the scheduling problem becomes a multi-action restless multi-armed bandit problem.","For this setting, we design a new scheduling policy by integrating Whittle index-based source selection and duality-based feature selection-from-buffer algorithms.","This new scheduling policy is proven to be asymptotically optimal.","These scheduling results hold for minimizing general AoI functions (monotonic or non-monotonic).","Data-driven evaluations demonstrate the significant advantages of our proposed scheduling policies."],"url":"http://arxiv.org/abs/2404.16281v1"}
{"created":"2024-04-25 01:33:55","title":"Causally Inspired Regularization Enables Domain General Representations","abstract":"Given a causal graph representing the data-generating process shared across different domains/distributions, enforcing sufficient graph-implied conditional independencies can identify domain-general (non-spurious) feature representations. For the standard input-output predictive setting, we categorize the set of graphs considered in the literature into two distinct groups: (i) those in which the empirical risk minimizer across training domains gives domain-general representations and (ii) those where it does not. For the latter case (ii), we propose a novel framework with regularizations, which we demonstrate are sufficient for identifying domain-general feature representations without a priori knowledge (or proxies) of the spurious features. Empirically, our proposed method is effective for both (semi) synthetic and real-world data, outperforming other state-of-the-art methods in average and worst-domain transfer accuracy.","sentences":["Given a causal graph representing the data-generating process shared across different domains/distributions, enforcing sufficient graph-implied conditional independencies can identify domain-general (non-spurious) feature representations.","For the standard input-output predictive setting, we categorize the set of graphs considered in the literature into two distinct groups: (i) those in which the empirical risk minimizer across training domains gives domain-general representations and (ii) those where it does not.","For the latter case (ii), we propose a novel framework with regularizations, which we demonstrate are sufficient for identifying domain-general feature representations without a priori knowledge (or proxies) of the spurious features.","Empirically, our proposed method is effective for both (semi) synthetic and real-world data, outperforming other state-of-the-art methods in average and worst-domain transfer accuracy."],"url":"http://arxiv.org/abs/2404.16277v1"}
{"created":"2024-04-25 01:01:06","title":"True random number generation using metastable 1T' molybdenum ditelluride","abstract":"True random numbers play a critical role in secure cryptography. The generation relies on a stable and readily extractable entropy source. Here, from solution-processed structurally metastable 1T' MoTe2, we prove stable output of featureless, stochastic, and yet stable conductance noise at a broad temperature (down to 15 K) with minimal power consumption (down to 0.05 micro-W). Our characterizations and statistical analysis of the characteristics of the conductance noise suggest that the noise arises from the volatility of the stochastic polarization of the underlying ferroelectric dipoles in the 1T' MoTe2. Further, as proved in our experiments and indicated by our Monte Carlo simulation, the ferroelectric dipole polarization is a reliable entropy source with the stochastic polarization persistent and stable over time. Exploiting the conductance noise, we achieve the generation of true random numbers and demonstrate their use in common cryptographic applications, for example, password generation and data encryption. Besides, particularly, we show a privacy safeguarding approach to sensitive data that can be critical for the cryptography of neural networks. We believe our work will bring insights into the understanding of the metastable 1T' MoTe2 and, more importantly, underpin its great potential in secure cryptography.","sentences":["True random numbers play a critical role in secure cryptography.","The generation relies on a stable and readily extractable entropy source.","Here, from solution-processed structurally metastable 1T' MoTe2, we prove stable output of featureless, stochastic, and yet stable conductance noise at a broad temperature (down to 15 K) with minimal power consumption (down to 0.05 micro-W).","Our characterizations and statistical analysis of the characteristics of the conductance noise suggest that the noise arises from the volatility of the stochastic polarization of the underlying ferroelectric dipoles in the 1T' MoTe2.","Further, as proved in our experiments and indicated by our Monte Carlo simulation, the ferroelectric dipole polarization is a reliable entropy source with the stochastic polarization persistent and stable over time.","Exploiting the conductance noise, we achieve the generation of true random numbers and demonstrate their use in common cryptographic applications, for example, password generation and data encryption.","Besides, particularly, we show a privacy safeguarding approach to sensitive data that can be critical for the cryptography of neural networks.","We believe our work will bring insights into the understanding of the metastable 1T' MoTe2 and, more importantly, underpin its great potential in secure cryptography."],"url":"http://arxiv.org/abs/2404.16271v1"}
{"created":"2024-04-25 00:30:28","title":"Dynamic PageRank: Algorithms and Lower Bounds","abstract":"We consider the PageRank problem in the dynamic setting, where the goal is to explicitly maintain an approximate PageRank vector $\\pi \\in \\mathbb{R}^n$ for a graph under a sequence of edge insertions and deletions. Our main result is a complete characterization of the complexity of dynamic PageRank maintenance for both multiplicative and additive ($L_1$) approximations.   First, we establish matching lower and upper bounds for maintaining additive approximate PageRank in both incremental and decremental settings. In particular, we demonstrate that in the worst-case $(1/\\alpha)^{\\Theta(\\log \\log n)}$ update time is necessary and sufficient for this problem, where $\\alpha$ is the desired additive approximation. On the other hand, we demonstrate that the commonly employed ForwardPush approach performs substantially worse than this optimal runtime. Specifically, we show that ForwardPush requires $\\Omega(n^{1-\\delta})$ time per update on average, for any $\\delta > 0$, even in the incremental setting.   For multiplicative approximations, however, we demonstrate that the situation is significantly more challenging. Specifically, we prove that any algorithm that explicitly maintains a constant factor multiplicative approximation of the PageRank vector of a directed graph must have amortized update time $\\Omega(n^{1-\\delta})$, for any $\\delta > 0$, even in the incremental setting, thereby resolving a 13-year old open question of Bahmani et al.~(VLDB 2010). This sharply contrasts with the undirected setting, where we show that $\\rm{poly}\\ \\log n$ update time is feasible, even in the fully dynamic setting under oblivious adversary.","sentences":["We consider the PageRank problem in the dynamic setting, where the goal is to explicitly maintain an approximate PageRank vector $\\pi \\in \\mathbb{R}^n$ for a graph under a sequence of edge insertions and deletions.","Our main result is a complete characterization of the complexity of dynamic PageRank maintenance for both multiplicative and additive ($L_1$) approximations.   ","First, we establish matching lower and upper bounds for maintaining additive approximate PageRank in both incremental and decremental settings.","In particular, we demonstrate that in the worst-case $(1/\\alpha)^{\\Theta(\\log \\log n)}$ update time is necessary and sufficient for this problem, where $\\alpha$ is the desired additive approximation.","On the other hand, we demonstrate that the commonly employed ForwardPush approach performs substantially worse than this optimal runtime.","Specifically, we show that ForwardPush requires $\\Omega(n^{1-\\delta})$ time per update on average, for any $\\delta > 0$, even in the incremental setting.   ","For multiplicative approximations, however, we demonstrate that the situation is significantly more challenging.","Specifically, we prove that any algorithm that explicitly maintains a constant factor multiplicative approximation of the PageRank vector of a directed graph must have amortized update time $\\Omega(n^{1-\\delta})$, for any $\\delta > 0$, even in the incremental setting, thereby resolving a 13-year old open question of Bahmani et al.~(VLDB 2010).","This sharply contrasts with the undirected setting, where we show that $\\rm{poly}\\ \\log n$ update time is feasible, even in the fully dynamic setting under oblivious adversary."],"url":"http://arxiv.org/abs/2404.16267v1"}
{"created":"2024-04-25 00:05:19","title":"Translation of Multifaceted Data without Re-Training of Machine Translation Systems","abstract":"Translating major language resources to build minor language resources becomes a widely-used approach. Particularly in translating complex data points composed of multiple components, it is common to translate each component separately. However, we argue that this practice often overlooks the interrelation between components within the same data point. To address this limitation, we propose a novel MT pipeline that considers the intra-data relation in implementing MT for training data. In our MT pipeline, all the components in a data point are concatenated to form a single translation sequence and subsequently reconstructed to the data components after translation. We introduce a Catalyst Statement (CS) to enhance the intra-data relation, and Indicator Token (IT) to assist the decomposition of a translated sequence into its respective data components. Through our approach, we have achieved a considerable improvement in translation quality itself, along with its effectiveness as training data. Compared with the conventional approach that translates each data component separately, our method yields better training data that enhances the performance of the trained model by 2.690 points for the web page ranking (WPR) task, and 0.845 for the question generation (QG) task in the XGLUE benchmark.","sentences":["Translating major language resources to build minor language resources becomes a widely-used approach.","Particularly in translating complex data points composed of multiple components, it is common to translate each component separately.","However, we argue that this practice often overlooks the interrelation between components within the same data point.","To address this limitation, we propose a novel MT pipeline that considers the intra-data relation in implementing MT for training data.","In our MT pipeline, all the components in a data point are concatenated to form a single translation sequence and subsequently reconstructed to the data components after translation.","We introduce a Catalyst Statement (CS) to enhance the intra-data relation, and Indicator Token (IT) to assist the decomposition of a translated sequence into its respective data components.","Through our approach, we have achieved a considerable improvement in translation quality itself, along with its effectiveness as training data.","Compared with the conventional approach that translates each data component separately, our method yields better training data that enhances the performance of the trained model by 2.690 points for the web page ranking (WPR) task, and 0.845 for the question generation (QG) task in the XGLUE benchmark."],"url":"http://arxiv.org/abs/2404.16257v1"}
{"created":"2024-04-24 23:56:03","title":"Enhancing Privacy in Face Analytics Using Fully Homomorphic Encryption","abstract":"Modern face recognition systems utilize deep neural networks to extract salient features from a face. These features denote embeddings in latent space and are often stored as templates in a face recognition system. These embeddings are susceptible to data leakage and, in some cases, can even be used to reconstruct the original face image. To prevent compromising identities, template protection schemes are commonly employed. However, these schemes may still not prevent the leakage of soft biometric information such as age, gender and race. To alleviate this issue, we propose a novel technique that combines Fully Homomorphic Encryption (FHE) with an existing template protection scheme known as PolyProtect. We show that the embeddings can be compressed and encrypted using FHE and transformed into a secure PolyProtect template using polynomial transformation, for additional protection. We demonstrate the efficacy of the proposed approach through extensive experiments on multiple datasets. Our proposed approach ensures irreversibility and unlinkability, effectively preventing the leakage of soft biometric attributes from face embeddings without compromising recognition accuracy.","sentences":["Modern face recognition systems utilize deep neural networks to extract salient features from a face.","These features denote embeddings in latent space and are often stored as templates in a face recognition system.","These embeddings are susceptible to data leakage and, in some cases, can even be used to reconstruct the original face image.","To prevent compromising identities, template protection schemes are commonly employed.","However, these schemes may still not prevent the leakage of soft biometric information such as age, gender and race.","To alleviate this issue, we propose a novel technique that combines Fully Homomorphic Encryption (FHE) with an existing template protection scheme known as PolyProtect.","We show that the embeddings can be compressed and encrypted using FHE and transformed into a secure PolyProtect template using polynomial transformation, for additional protection.","We demonstrate the efficacy of the proposed approach through extensive experiments on multiple datasets.","Our proposed approach ensures irreversibility and unlinkability, effectively preventing the leakage of soft biometric attributes from face embeddings without compromising recognition accuracy."],"url":"http://arxiv.org/abs/2404.16255v1"}
{"created":"2024-04-24 22:58:42","title":"Synergizing Privacy and Utility in Data Analytics Through Advanced Information Theorization","abstract":"This study develops a novel framework for privacy-preserving data analytics, addressing the critical challenge of balancing data utility with privacy concerns. We introduce three sophisticated algorithms: a Noise-Infusion Technique tailored for high-dimensional image data, a Variational Autoencoder (VAE) for robust feature extraction while masking sensitive attributes and an Expectation Maximization (EM) approach optimized for structured data privacy. Applied to datasets such as Modified MNIST and CelebrityA, our methods significantly reduce mutual information between sensitive attributes and transformed data, thereby enhancing privacy. Our experimental results confirm that these approaches achieve superior privacy protection and retain high utility, making them viable for practical applications where both aspects are crucial. The research contributes to the field by providing a flexible and effective strategy for deploying privacy-preserving algorithms across various data types and establishing new benchmarks for utility and confidentiality in data analytics.","sentences":["This study develops a novel framework for privacy-preserving data analytics, addressing the critical challenge of balancing data utility with privacy concerns.","We introduce three sophisticated algorithms: a Noise-Infusion Technique tailored for high-dimensional image data, a Variational Autoencoder (VAE) for robust feature extraction while masking sensitive attributes and an Expectation Maximization (EM) approach optimized for structured data privacy.","Applied to datasets such as Modified MNIST and CelebrityA, our methods significantly reduce mutual information between sensitive attributes and transformed data, thereby enhancing privacy.","Our experimental results confirm that these approaches achieve superior privacy protection and retain high utility, making them viable for practical applications where both aspects are crucial.","The research contributes to the field by providing a flexible and effective strategy for deploying privacy-preserving algorithms across various data types and establishing new benchmarks for utility and confidentiality in data analytics."],"url":"http://arxiv.org/abs/2404.16241v1"}
{"created":"2024-04-24 22:28:12","title":"AutoGluon-Multimodal (AutoMM): Supercharging Multimodal AutoML with Foundation Models","abstract":"AutoGluon-Multimodal (AutoMM) is introduced as an open-source AutoML library designed specifically for multimodal learning. Distinguished by its exceptional ease of use, AutoMM enables fine-tuning of foundational models with just three lines of code. Supporting various modalities including image, text, and tabular data, both independently and in combination, the library offers a comprehensive suite of functionalities spanning classification, regression, object detection, semantic matching, and image segmentation. Experiments across diverse datasets and tasks showcases AutoMM's superior performance in basic classification and regression tasks compared to existing AutoML tools, while also demonstrating competitive results in advanced tasks, aligning with specialized toolboxes designed for such purposes.","sentences":["AutoGluon-Multimodal (AutoMM) is introduced as an open-source AutoML library designed specifically for multimodal learning.","Distinguished by its exceptional ease of use, AutoMM enables fine-tuning of foundational models with just three lines of code.","Supporting various modalities including image, text, and tabular data, both independently and in combination, the library offers a comprehensive suite of functionalities spanning classification, regression, object detection, semantic matching, and image segmentation.","Experiments across diverse datasets and tasks showcases AutoMM's superior performance in basic classification and regression tasks compared to existing AutoML tools, while also demonstrating competitive results in advanced tasks, aligning with specialized toolboxes designed for such purposes."],"url":"http://arxiv.org/abs/2404.16233v1"}
{"created":"2024-04-24 22:24:52","title":"SECO: Secure Inference With Model Splitting Across Multi-Server Hierarchy","abstract":"In the context of prediction-as-a-service, concerns about the privacy of the data and the model have been brought up and tackled via secure inference protocols. These protocols are built up by using single or multiple cryptographic tools designed under a variety of different security assumptions.   In this paper, we introduce SECO, a secure inference protocol that enables a user holding an input data vector and multiple server nodes deployed with a split neural network model to collaboratively compute the prediction, without compromising either party's data privacy. We extend prior work on secure inference that requires the entire neural network model to be located on a single server node, to a multi-server hierarchy, where the user communicates to a gateway server node, which in turn communicates to remote server nodes. The inference task is split across the server nodes and must be performed over an encrypted copy of the data vector.   We adopt multiparty homomorphic encryption and multiparty garbled circuit schemes, making the system secure against dishonest majority of semi-honest servers as well as protecting the partial model structure from the user. We evaluate SECO on multiple models, achieving the reduction of computation and communication cost for the user, making the protocol applicable to user's devices with limited resources.","sentences":["In the context of prediction-as-a-service, concerns about the privacy of the data and the model have been brought up and tackled via secure inference protocols.","These protocols are built up by using single or multiple cryptographic tools designed under a variety of different security assumptions.   ","In this paper, we introduce SECO, a secure inference protocol that enables a user holding an input data vector and multiple server nodes deployed with a split neural network model to collaboratively compute the prediction, without compromising either party's data privacy.","We extend prior work on secure inference that requires the entire neural network model to be located on a single server node, to a multi-server hierarchy, where the user communicates to a gateway server node, which in turn communicates to remote server nodes.","The inference task is split across the server nodes and must be performed over an encrypted copy of the data vector.   ","We adopt multiparty homomorphic encryption and multiparty garbled circuit schemes, making the system secure against dishonest majority of semi-honest servers as well as protecting the partial model structure from the user.","We evaluate SECO on multiple models, achieving the reduction of computation and communication cost for the user, making the protocol applicable to user's devices with limited resources."],"url":"http://arxiv.org/abs/2404.16232v1"}
{"created":"2024-04-24 21:59:40","title":"Computational analysis of the language of pain: a systematic review","abstract":"Objectives: This study aims to systematically review the literature on the computational processing of the language of pain, whether generated by patients or physicians, identifying current trends and challenges. Methods: Following the PRISMA guidelines, a comprehensive literature search was conducted to select relevant studies on the computational processing of the language of pain and answer pre-defined research questions. Data extraction and synthesis were performed to categorize selected studies according to their primary purpose and outcome, patient and pain population, textual data, computational methodology, and outcome targets. Results: Physician-generated language of pain, specifically from clinical notes, was the most used data. Tasks included patient diagnosis and triaging, identification of pain mentions, treatment response prediction, biomedical entity extraction, correlation of linguistic features with clinical states, and lexico-semantic analysis of pain narratives. Only one study included previous linguistic knowledge on pain utterances in their experimental setup. Most studies targeted their outcomes for physicians, either directly as clinical tools or as indirect knowledge. The least targeted stage of clinical pain care was self-management, in which patients are most involved. The least studied dimensions of pain were affective and sociocultural. Only two studies measured how physician performance on clinical tasks improved with the inclusion of the proposed algorithm. Discussion: This study found that future research should focus on analyzing patient-generated language of pain, developing patient-centered resources for self-management and patient-empowerment, exploring affective and sociocultural aspects of pain, and measuring improvements in physician performance when aided by the proposed tools.","sentences":["Objectives:","This study aims to systematically review the literature on the computational processing of the language of pain, whether generated by patients or physicians, identifying current trends and challenges.","Methods: Following the PRISMA guidelines, a comprehensive literature search was conducted to select relevant studies on the computational processing of the language of pain and answer pre-defined research questions.","Data extraction and synthesis were performed to categorize selected studies according to their primary purpose and outcome, patient and pain population, textual data, computational methodology, and outcome targets.","Results: Physician-generated language of pain, specifically from clinical notes, was the most used data.","Tasks included patient diagnosis and triaging, identification of pain mentions, treatment response prediction, biomedical entity extraction, correlation of linguistic features with clinical states, and lexico-semantic analysis of pain narratives.","Only one study included previous linguistic knowledge on pain utterances in their experimental setup.","Most studies targeted their outcomes for physicians, either directly as clinical tools or as indirect knowledge.","The least targeted stage of clinical pain care was self-management, in which patients are most involved.","The least studied dimensions of pain were affective and sociocultural.","Only two studies measured how physician performance on clinical tasks improved with the inclusion of the proposed algorithm.","Discussion:","This study found that future research should focus on analyzing patient-generated language of pain, developing patient-centered resources for self-management and patient-empowerment, exploring affective and sociocultural aspects of pain, and measuring improvements in physician performance when aided by the proposed tools."],"url":"http://arxiv.org/abs/2404.16226v1"}
{"created":"2024-04-24 21:56:06","title":"Tractable Conjunctive Queries over Static and Dynamic Relations","abstract":"We investigate the evaluation of conjunctive queries over static and dynamic relations. While static relations are given as input and do not change, dynamic relations are subject to inserts and deletes.   We characterise syntactically three classes of queries that admit constant update time and constant enumeration delay. We call such queries tractable. Depending on the class, the preprocessing time is linear, polynomial, or exponential (under data complexity, so the query size is constant).   To decide whether a query is tractable, it does not suffice to analyse separately the sub-query over the static relations and the sub-query over the dynamic relations. Instead, we need to take the interaction between the static and the dynamic relations into account. Even when the sub-query over the dynamic relations is not tractable, the overall query can become tractable if the dynamic relations are sufficiently constrained by the static ones.","sentences":["We investigate the evaluation of conjunctive queries over static and dynamic relations.","While static relations are given as input and do not change, dynamic relations are subject to inserts and deletes.   ","We characterise syntactically three classes of queries that admit constant update time and constant enumeration delay.","We call such queries tractable.","Depending on the class, the preprocessing time is linear, polynomial, or exponential (under data complexity, so the query size is constant).   ","To decide whether a query is tractable, it does not suffice to analyse separately the sub-query over the static relations and the sub-query over the dynamic relations.","Instead, we need to take the interaction between the static and the dynamic relations into account.","Even when the sub-query over the dynamic relations is not tractable, the overall query can become tractable if the dynamic relations are sufficiently constrained by the static ones."],"url":"http://arxiv.org/abs/2404.16224v1"}
{"created":"2024-04-24 21:49:59","title":"Step Differences in Instructional Video","abstract":"Comparing a user video to a reference how-to video is a key requirement for AR/VR technology delivering personalized assistance tailored to the user's progress. However, current approaches for language-based assistance can only answer questions about a single video. We propose an approach that first automatically generates large amounts of visual instruction tuning data involving pairs of videos from HowTo100M by leveraging existing step annotations and accompanying narrations, and then trains a video-conditioned language model to jointly reason across multiple raw videos. Our model achieves state-of-the-art performance at identifying differences between video pairs and ranking videos based on the severity of these differences, and shows promising ability to perform general reasoning over multiple videos.","sentences":["Comparing a user video to a reference how-to video is a key requirement for AR/VR technology delivering personalized assistance tailored to the user's progress.","However, current approaches for language-based assistance can only answer questions about a single video.","We propose an approach that first automatically generates large amounts of visual instruction tuning data involving pairs of videos from HowTo100M by leveraging existing step annotations and accompanying narrations, and then trains a video-conditioned language model to jointly reason across multiple raw videos.","Our model achieves state-of-the-art performance at identifying differences between video pairs and ranking videos based on the severity of these differences, and shows promising ability to perform general reasoning over multiple videos."],"url":"http://arxiv.org/abs/2404.16222v1"}
{"created":"2024-04-24 21:33:17","title":"Efficient NAS with FaDE on Hierarchical Spaces","abstract":"Neural architecture search (NAS) is a challenging problem. Hierarchical search spaces allow for cheap evaluations of neural network sub modules to serve as surrogate for architecture evaluations. Yet, sometimes the hierarchy is too restrictive or the surrogate fails to generalize. We present FaDE which uses differentiable architecture search to obtain relative performance predictions on finite regions of a hierarchical NAS space. The relative nature of these ranks calls for a memory-less, batch-wise outer search algorithm for which we use an evolutionary algorithm with pseudo-gradient descent. FaDE is especially suited on deep hierarchical, respectively multi-cell search spaces, which it can explore by linear instead of exponential cost and therefore eliminates the need for a proxy search space.   Our experiments show that firstly, FaDE-ranks on finite regions of the search space correlate with corresponding architecture performances and secondly, the ranks can empower a pseudo-gradient evolutionary search on the complete neural architecture search space.","sentences":["Neural architecture search (NAS) is a challenging problem.","Hierarchical search spaces allow for cheap evaluations of neural network sub modules to serve as surrogate for architecture evaluations.","Yet, sometimes the hierarchy is too restrictive or the surrogate fails to generalize.","We present FaDE which uses differentiable architecture search to obtain relative performance predictions on finite regions of a hierarchical NAS space.","The relative nature of these ranks calls for a memory-less, batch-wise outer search algorithm for which we use an evolutionary algorithm with pseudo-gradient descent.","FaDE is especially suited on deep hierarchical, respectively multi-cell search spaces, which it can explore by linear instead of exponential cost and therefore eliminates the need for a proxy search space.   ","Our experiments show that firstly, FaDE-ranks on finite regions of the search space correlate with corresponding architecture performances and secondly, the ranks can empower a pseudo-gradient evolutionary search on the complete neural architecture search space."],"url":"http://arxiv.org/abs/2404.16218v1"}
{"created":"2024-04-24 21:32:59","title":"Fault-Tolerant Bounded Flow Preservers","abstract":"Given a directed graph $G = (V, E)$ with $n$ vertices, $m$ edges and a designated source vertex $s\\in V$, we consider the question of finding a sparse subgraph $H$ of $G$ that preserves the flow from $s$ up to a given threshold $\\lambda$ even after failure of $k$ edges. We refer to such subgraphs as $(\\lambda,k)$-fault-tolerant bounded-flow-preserver ($(\\lambda,k)$-FT-BFP). Formally, for any $F \\subseteq E$ of at most $k$ edges and any $v\\in V$, the $(s, v)$-max-flow in $H \\setminus F$ is equal to $(s, v)$-max-flow in $G \\setminus F$, if the latter is bounded by $\\lambda$, and at least $\\lambda$ otherwise. Our contributions are summarized as follows:   1. We provide a polynomial time algorithm that given any graph $G$ constructs a $(\\lambda,k)$-FT-BFP of $G$ with at most $\\lambda 2^kn$ edges.   2. We also prove a matching lower bound of $\\Omega(\\lambda 2^kn)$ on the size of $(\\lambda,k)$-FT-BFP. In particular, we show that for every $\\lambda,k,n\\geq 1$, there exists an $n$-vertex directed graph whose optimal $(\\lambda,k)$-FT-BFP contains $\\Omega(\\min\\{2^k\\lambda n,n^2\\})$ edges.   3. Furthermore, we show that the problem of computing approximate $(\\lambda,k)$-FT-BFP is NP-hard for any approximation ratio that is better than $O(\\log(\\lambda^{-1} n))$.","sentences":["Given a directed graph $G = (V, E)$ with $n$ vertices, $m$ edges and a designated source vertex $s\\in V$, we consider the question of finding a sparse subgraph $H$ of $G$ that preserves the flow from $s$ up to a given threshold $\\lambda$ even after failure of $k$ edges.","We refer to such subgraphs as $(\\lambda,k)$-fault-tolerant bounded-flow-preserver ($(\\lambda,k)$-FT-BFP).","Formally, for any $F \\subseteq E$ of at most $k$ edges and any $v\\in V$, the $(s, v)$-max-flow in $H \\setminus F$ is equal to $(s, v)$-max-flow in $G \\setminus F$, if the latter is bounded by $\\lambda$, and at least $\\lambda$ otherwise.","Our contributions are summarized as follows:   1.","We provide a polynomial time algorithm that given any graph $G$ constructs a $(\\lambda,k)$-FT-BFP of $G$ with at most $\\lambda 2^kn$ edges.   ","2.","We also prove a matching lower bound of $\\Omega(\\lambda 2^kn)$ on the size of $(\\lambda,k)$-FT-BFP.","In particular, we show that for every $\\lambda,k,n\\geq 1$, there exists an $n$-vertex directed graph whose optimal $(\\lambda,k)$-FT-BFP contains $\\Omega(\\min\\{2^k\\lambda n,n^2\\})$ edges.   ","3.","Furthermore, we show that the problem of computing approximate $(\\lambda,k)$-FT-BFP is NP-hard for any approximation ratio that is better than $O(\\log(\\lambda^{-1} n))$."],"url":"http://arxiv.org/abs/2404.16217v1"}
{"created":"2024-04-24 21:30:01","title":"ActiveRIR: Active Audio-Visual Exploration for Acoustic Environment Modeling","abstract":"An environment acoustic model represents how sound is transformed by the physical characteristics of an indoor environment, for any given source/receiver location. Traditional methods for constructing acoustic models involve expensive and time-consuming collection of large quantities of acoustic data at dense spatial locations in the space, or rely on privileged knowledge of scene geometry to intelligently select acoustic data sampling locations. We propose active acoustic sampling, a new task for efficiently building an environment acoustic model of an unmapped environment in which a mobile agent equipped with visual and acoustic sensors jointly constructs the environment acoustic model and the occupancy map on-the-fly. We introduce ActiveRIR, a reinforcement learning (RL) policy that leverages information from audio-visual sensor streams to guide agent navigation and determine optimal acoustic data sampling positions, yielding a high quality acoustic model of the environment from a minimal set of acoustic samples. We train our policy with a novel RL reward based on information gain in the environment acoustic model. Evaluating on diverse unseen indoor environments from a state-of-the-art acoustic simulation platform, ActiveRIR outperforms an array of methods--both traditional navigation agents based on spatial novelty and visual exploration as well as existing state-of-the-art methods.","sentences":["An environment acoustic model represents how sound is transformed by the physical characteristics of an indoor environment, for any given source/receiver location.","Traditional methods for constructing acoustic models involve expensive and time-consuming collection of large quantities of acoustic data at dense spatial locations in the space, or rely on privileged knowledge of scene geometry to intelligently select acoustic data sampling locations.","We propose active acoustic sampling, a new task for efficiently building an environment acoustic model of an unmapped environment in which a mobile agent equipped with visual and acoustic sensors jointly constructs the environment acoustic model and the occupancy map on-the-fly.","We introduce ActiveRIR, a reinforcement learning (RL) policy that leverages information from audio-visual sensor streams to guide agent navigation and determine optimal acoustic data sampling positions, yielding a high quality acoustic model of the environment from a minimal set of acoustic samples.","We train our policy with a novel RL reward based on information gain in the environment acoustic model.","Evaluating on diverse unseen indoor environments from a state-of-the-art acoustic simulation platform, ActiveRIR outperforms an array of methods--both traditional navigation agents based on spatial novelty and visual exploration as well as existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.16216v1"}
{"created":"2024-04-24 21:21:50","title":"An Analysis of Recent Advances in Deepfake Image Detection in an Evolving Threat Landscape","abstract":"Deepfake or synthetic images produced using deep generative models pose serious risks to online platforms. This has triggered several research efforts to accurately detect deepfake images, achieving excellent performance on publicly available deepfake datasets. In this work, we study 8 state-of-the-art detectors and argue that they are far from being ready for deployment due to two recent developments. First, the emergence of lightweight methods to customize large generative models, can enable an attacker to create many customized generators (to create deepfakes), thereby substantially increasing the threat surface. We show that existing defenses fail to generalize well to such \\emph{user-customized generative models} that are publicly available today. We discuss new machine learning approaches based on content-agnostic features, and ensemble modeling to improve generalization performance against user-customized models. Second, the emergence of \\textit{vision foundation models} -- machine learning models trained on broad data that can be easily adapted to several downstream tasks -- can be misused by attackers to craft adversarial deepfakes that can evade existing defenses. We propose a simple adversarial attack that leverages existing foundation models to craft adversarial samples \\textit{without adding any adversarial noise}, through careful semantic manipulation of the image content. We highlight the vulnerabilities of several defenses against our attack, and explore directions leveraging advanced foundation models and adversarial training to defend against this new threat.","sentences":["Deepfake or synthetic images produced using deep generative models pose serious risks to online platforms.","This has triggered several research efforts to accurately detect deepfake images, achieving excellent performance on publicly available deepfake datasets.","In this work, we study 8 state-of-the-art detectors and argue that they are far from being ready for deployment due to two recent developments.","First, the emergence of lightweight methods to customize large generative models, can enable an attacker to create many customized generators (to create deepfakes), thereby substantially increasing the threat surface.","We show that existing defenses fail to generalize well to such \\emph{user-customized generative models} that are publicly available today.","We discuss new machine learning approaches based on content-agnostic features, and ensemble modeling to improve generalization performance against user-customized models.","Second, the emergence of \\textit{vision foundation models} -- machine learning models trained on broad data that can be easily adapted to several downstream tasks -- can be misused by attackers to craft adversarial deepfakes that can evade existing defenses.","We propose a simple adversarial attack that leverages existing foundation models to craft adversarial samples \\textit{without adding any adversarial noise}, through careful semantic manipulation of the image content.","We highlight the vulnerabilities of several defenses against our attack, and explore directions leveraging advanced foundation models and adversarial training to defend against this new threat."],"url":"http://arxiv.org/abs/2404.16212v1"}
{"created":"2024-04-24 21:09:56","title":"Efficient Data Management for IPFS dApps","abstract":"Inefficient data management has been the Achilles heel of blockchain-based decentralized applications (dApps). An off-chain storage layer, which lies between the application and the blockchain layers, can improve space efficiency and data availability with erasure codes and decentralized maintenance. This paper presents two fundamental components of such storage layer designed and implemented for the IPFS network. The IPFS Community is a component built on top of the IPFS network that encodes and decodes data before uploading to the network. Since data is encoded with alpha entanglement codes, the solution requires less storage space than the native IPFS solution which replicates data by pinning content with the IPFS Cluster. To detect and repair failures in a timely manner, we introduce the monitoring and repair component. This novel component is activated by any node and distributes the load of repairs among various nodes. These two components are implemented as pluggable modules, and can, therefore, be easily migrated to other distributed file systems by adjusting the connector component.","sentences":["Inefficient data management has been the Achilles heel of blockchain-based decentralized applications (dApps).","An off-chain storage layer, which lies between the application and the blockchain layers, can improve space efficiency and data availability with erasure codes and decentralized maintenance.","This paper presents two fundamental components of such storage layer designed and implemented for the IPFS network.","The IPFS Community is a component built on top of the IPFS network that encodes and decodes data before uploading to the network.","Since data is encoded with alpha entanglement codes, the solution requires less storage space than the native IPFS solution which replicates data by pinning content with the IPFS Cluster.","To detect and repair failures in a timely manner, we introduce the monitoring and repair component.","This novel component is activated by any node and distributes the load of repairs among various nodes.","These two components are implemented as pluggable modules, and can, therefore, be easily migrated to other distributed file systems by adjusting the connector component."],"url":"http://arxiv.org/abs/2404.16210v1"}
{"created":"2024-04-24 20:33:25","title":"Improving Multi-label Recognition using Class Co-Occurrence Probabilities","abstract":"Multi-label Recognition (MLR) involves the identification of multiple objects within an image. To address the additional complexity of this problem, recent works have leveraged information from vision-language models (VLMs) trained on large text-images datasets for the task. These methods learn an independent classifier for each object (class), overlooking correlations in their occurrences. Such co-occurrences can be captured from the training data as conditional probabilities between a pair of classes. We propose a framework to extend the independent classifiers by incorporating the co-occurrence information for object pairs to improve the performance of independent classifiers. We use a Graph Convolutional Network (GCN) to enforce the conditional probabilities between classes, by refining the initial estimates derived from image and text sources obtained using VLMs. We validate our method on four MLR datasets, where our approach outperforms all state-of-the-art methods.","sentences":["Multi-label Recognition (MLR) involves the identification of multiple objects within an image.","To address the additional complexity of this problem, recent works have leveraged information from vision-language models (VLMs) trained on large text-images datasets for the task.","These methods learn an independent classifier for each object (class), overlooking correlations in their occurrences.","Such co-occurrences can be captured from the training data as conditional probabilities between a pair of classes.","We propose a framework to extend the independent classifiers by incorporating the co-occurrence information for object pairs to improve the performance of independent classifiers.","We use a Graph Convolutional Network (GCN) to enforce the conditional probabilities between classes, by refining the initial estimates derived from image and text sources obtained using VLMs.","We validate our method on four MLR datasets, where our approach outperforms all state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.16193v1"}
{"created":"2024-04-24 20:22:48","title":"Pearls from Pebbles: Improved Confidence Functions for Auto-labeling","abstract":"Auto-labeling is an important family of techniques that produce labeled training sets with minimum manual labeling. A prominent variant, threshold-based auto-labeling (TBAL), works by finding a threshold on a model's confidence scores above which it can accurately label unlabeled data points. However, many models are known to produce overconfident scores, leading to poor TBAL performance. While a natural idea is to apply off-the-shelf calibration methods to alleviate the overconfidence issue, such methods still fall short. Rather than experimenting with ad-hoc choices of confidence functions, we propose a framework for studying the \\emph{optimal} TBAL confidence function. We develop a tractable version of the framework to obtain \\texttt{Colander} (Confidence functions for Efficient and Reliable Auto-labeling), a new post-hoc method specifically designed to maximize performance in TBAL systems. We perform an extensive empirical evaluation of our method \\texttt{Colander} and compare it against methods designed for calibration. \\texttt{Colander} achieves up to 60\\% improvements on coverage over the baselines while maintaining auto-labeling error below $5\\%$ and using the same amount of labeled data as the baselines.","sentences":["Auto-labeling is an important family of techniques that produce labeled training sets with minimum manual labeling.","A prominent variant, threshold-based auto-labeling (TBAL), works by finding a threshold on a model's confidence scores above which it can accurately label unlabeled data points.","However, many models are known to produce overconfident scores, leading to poor TBAL performance.","While a natural idea is to apply off-the-shelf calibration methods to alleviate the overconfidence issue, such methods still fall short.","Rather than experimenting with ad-hoc choices of confidence functions, we propose a framework for studying the \\emph{optimal} TBAL confidence function.","We develop a tractable version of the framework to obtain \\texttt{Colander} (Confidence functions for Efficient and Reliable Auto-labeling), a new post-hoc method specifically designed to maximize performance in TBAL systems.","We perform an extensive empirical evaluation of our method \\texttt{Colander} and compare it against methods designed for calibration.","\\texttt{Colander} achieves up to 60\\% improvements on coverage over the baselines while maintaining auto-labeling error below $5\\%$ and using the same amount of labeled data as the baselines."],"url":"http://arxiv.org/abs/2404.16188v1"}
{"created":"2024-04-24 20:15:57","title":"ABCD: Trust enhanced Attention based Convolutional Autoencoder for Risk Assessment","abstract":"Anomaly detection in industrial systems is crucial for preventing equipment failures, ensuring risk identification, and maintaining overall system efficiency. Traditional monitoring methods often rely on fixed thresholds and empirical rules, which may not be sensitive enough to detect subtle changes in system health and predict impending failures. To address this limitation, this paper proposes, a novel Attention-based convolutional autoencoder (ABCD) for risk detection and map the risk value derive to the maintenance planning. ABCD learns the normal behavior of conductivity from historical data of a real-world industrial cooling system and reconstructs the input data, identifying anomalies that deviate from the expected patterns. The framework also employs calibration techniques to ensure the reliability of its predictions. Evaluation results demonstrate that with the attention mechanism in ABCD a 57.4% increase in performance and a reduction of false alarms by 9.37% is seen compared to without attention. The approach can effectively detect risks, the risk priority rank mapped to maintenance, providing valuable insights for cooling system designers and service personnel. Calibration error of 0.03% indicates that the model is well-calibrated and enhances model's trustworthiness, enabling informed decisions about maintenance strategies","sentences":["Anomaly detection in industrial systems is crucial for preventing equipment failures, ensuring risk identification, and maintaining overall system efficiency.","Traditional monitoring methods often rely on fixed thresholds and empirical rules, which may not be sensitive enough to detect subtle changes in system health and predict impending failures.","To address this limitation, this paper proposes, a novel Attention-based convolutional autoencoder (ABCD) for risk detection and map the risk value derive to the maintenance planning.","ABCD learns the normal behavior of conductivity from historical data of a real-world industrial cooling system and reconstructs the input data, identifying anomalies that deviate from the expected patterns.","The framework also employs calibration techniques to ensure the reliability of its predictions.","Evaluation results demonstrate that with the attention mechanism in ABCD a 57.4% increase in performance and a reduction of false alarms by 9.37% is seen compared to without attention.","The approach can effectively detect risks, the risk priority rank mapped to maintenance, providing valuable insights for cooling system designers and service personnel.","Calibration error of 0.03% indicates that the model is well-calibrated and enhances model's trustworthiness, enabling informed decisions about maintenance strategies"],"url":"http://arxiv.org/abs/2404.16183v1"}
{"created":"2024-04-24 20:10:10","title":"Blind Federated Learning without initial model","abstract":"Federated learning is an emerging machine learning approach that allows the construction of a model between several participants who hold their own private data. This method is secure and privacy-preserving, suitable for training a machine learning model using sensitive data from different sources, such as hospitals. In this paper, the authors propose two innovative methodologies for Particle Swarm Optimisation-based federated learning of Fuzzy Cognitive Maps in a privacy-preserving way. In addition, one relevant contribution this research includes is the lack of an initial model in the federated learning process, making it effectively blind. This proposal is tested with several open datasets, improving both accuracy and precision.","sentences":["Federated learning is an emerging machine learning approach that allows the construction of a model between several participants who hold their own private data.","This method is secure and privacy-preserving, suitable for training a machine learning model using sensitive data from different sources, such as hospitals.","In this paper, the authors propose two innovative methodologies for Particle Swarm Optimisation-based federated learning of Fuzzy Cognitive Maps in a privacy-preserving way.","In addition, one relevant contribution this research includes is the lack of an initial model in the federated learning process, making it effectively blind.","This proposal is tested with several open datasets, improving both accuracy and precision."],"url":"http://arxiv.org/abs/2404.16180v1"}
{"created":"2024-04-24 20:09:21","title":"S2DEVFMAP: Self-Supervised Learning Framework with Dual Ensemble Voting Fusion for Maximizing Anomaly Prediction in Timeseries","abstract":"Anomaly detection plays a crucial role in industrial settings, particularly in maintaining the reliability and optimal performance of cooling systems. Traditional anomaly detection methods often face challenges in handling diverse data characteristics and variations in noise levels, resulting in limited effectiveness. And yet traditional anomaly detection often relies on application of single models. This work proposes a novel, robust approach using five heterogeneous independent models combined with a dual ensemble fusion of voting techniques. Diverse models capture various system behaviors, while the fusion strategy maximizes detection effectiveness and minimizes false alarms. Each base autoencoder model learns a unique representation of the data, leveraging their complementary strengths to improve anomaly detection performance. To increase the effectiveness and reliability of final anomaly prediction, dual ensemble technique is applied. This approach outperforms in maximizing the coverage of identifying anomalies. Experimental results on a real-world dataset of industrial cooling system data demonstrate the effectiveness of the proposed approach. This approach can be extended to other industrial applications where anomaly detection is critical for ensuring system reliability and preventing potential malfunctions.","sentences":["Anomaly detection plays a crucial role in industrial settings, particularly in maintaining the reliability and optimal performance of cooling systems.","Traditional anomaly detection methods often face challenges in handling diverse data characteristics and variations in noise levels, resulting in limited effectiveness.","And yet traditional anomaly detection often relies on application of single models.","This work proposes a novel, robust approach using five heterogeneous independent models combined with a dual ensemble fusion of voting techniques.","Diverse models capture various system behaviors, while the fusion strategy maximizes detection effectiveness and minimizes false alarms.","Each base autoencoder model learns a unique representation of the data, leveraging their complementary strengths to improve anomaly detection performance.","To increase the effectiveness and reliability of final anomaly prediction, dual ensemble technique is applied.","This approach outperforms in maximizing the coverage of identifying anomalies.","Experimental results on a real-world dataset of industrial cooling system data demonstrate the effectiveness of the proposed approach.","This approach can be extended to other industrial applications where anomaly detection is critical for ensuring system reliability and preventing potential malfunctions."],"url":"http://arxiv.org/abs/2404.16179v1"}
{"created":"2024-04-24 20:05:39","title":"Advancing Recommender Systems by mitigating Shilling attacks","abstract":"Considering the premise that the number of products offered grow in an exponential fashion and the amount of data that a user can assimilate before making a decision is relatively small, recommender systems help in categorizing content according to user preferences. Collaborative filtering is a widely used method for computing recommendations due to its good performance. But, this method makes the system vulnerable to attacks which try to bias the recommendations. These attacks, known as 'shilling attacks' are performed to push an item or nuke an item in the system. This paper proposes an algorithm to detect such shilling profiles in the system accurately and also study the effects of such profiles on the recommendations.","sentences":["Considering the premise that the number of products offered grow in an exponential fashion and the amount of data that a user can assimilate before making a decision is relatively small, recommender systems help in categorizing content according to user preferences.","Collaborative filtering is a widely used method for computing recommendations due to its good performance.","But, this method makes the system vulnerable to attacks which try to bias the recommendations.","These attacks, known as 'shilling attacks' are performed to push an item or nuke an item in the system.","This paper proposes an algorithm to detect such shilling profiles in the system accurately and also study the effects of such profiles on the recommendations."],"url":"http://arxiv.org/abs/2404.16177v1"}
{"created":"2024-04-24 20:05:12","title":"Unweighted Layered Graph Traversal","abstract":"Introduced by Papadimitriou and Yannakakis in 1989, layered graph traversal is an important problem in online algorithms and mobile computing that has been studied for several decades, and which now is essentially resolved in its original formulation. In this paper, we demonstrate that what appears to be an innocuous modification of the problem actually leads to a drastic (exponential) reduction of the competitive ratio. Specifically, we present an algorithm that is $O(\\log^2 w)$-competitive for traversing unweighted layered graphs of width $w$. Our technique is based on a simple entropic regularizer, which evolves as the agent progresses in the layered graph. Our algorithm is randomized and simply maintains that at all layers, the probability distribution of the position of the mobile agent maximizes the entropic regularizer.","sentences":["Introduced by Papadimitriou and Yannakakis in 1989, layered graph traversal is an important problem in online algorithms and mobile computing that has been studied for several decades, and which now is essentially resolved in its original formulation.","In this paper, we demonstrate that what appears to be an innocuous modification of the problem actually leads to a drastic (exponential) reduction of the competitive ratio.","Specifically, we present an algorithm that is $O(\\log^2 w)$-competitive for traversing unweighted layered graphs of width $w$. Our technique is based on a simple entropic regularizer, which evolves as the agent progresses in the layered graph.","Our algorithm is randomized and simply maintains that at all layers, the probability distribution of the position of the mobile agent maximizes the entropic regularizer."],"url":"http://arxiv.org/abs/2404.16176v1"}
{"created":"2024-04-24 20:03:01","title":"An analysis of the effects of sharing research data, code, and preprints on citations","abstract":"Calls to make scientific research more open have gained traction with a range of societal stakeholders. Open Science practices include but are not limited to the early sharing of results via preprints and openly sharing outputs such as data and code to make research more reproducible and extensible. Existing evidence shows that adopting Open Science practices has effects in several domains. In this study, we investigate whether adopting one or more Open Science practices leads to significantly higher citations for an associated publication, which is one form of academic impact. We use a novel dataset known as Open Science Indicators, produced by PLOS and DataSeer, which includes all PLOS publications from 2018 to 2023 as well as a comparison group sampled from the PMC Open Access Subset. In total, we analyze circa 122'000 publications. We calculate publication and author-level citation indicators and use a broad set of control variables to isolate the effect of Open Science Indicators on received citations. We show that Open Science practices are adopted to different degrees across scientific disciplines. We find that the early release of a publication as a preprint correlates with a significant positive citation advantage of about 20.2% on average. We also find that sharing data in an online repository correlates with a smaller yet still positive citation advantage of 4.3% on average. However, we do not find a significant citation advantage for sharing code. Further research is needed on additional or alternative measures of impact beyond citations. Our results are likely to be of interest to researchers, as well as publishers, research funders, and policymakers.","sentences":["Calls to make scientific research more open have gained traction with a range of societal stakeholders.","Open Science practices include but are not limited to the early sharing of results via preprints and openly sharing outputs such as data and code to make research more reproducible and extensible.","Existing evidence shows that adopting Open Science practices has effects in several domains.","In this study, we investigate whether adopting one or more Open Science practices leads to significantly higher citations for an associated publication, which is one form of academic impact.","We use a novel dataset known as Open Science Indicators, produced by PLOS and DataSeer, which includes all PLOS publications from 2018 to 2023 as well as a comparison group sampled from the PMC Open Access Subset.","In total, we analyze circa 122'000 publications.","We calculate publication and author-level citation indicators and use a broad set of control variables to isolate the effect of Open Science Indicators on received citations.","We show that Open Science practices are adopted to different degrees across scientific disciplines.","We find that the early release of a publication as a preprint correlates with a significant positive citation advantage of about 20.2% on average.","We also find that sharing data in an online repository correlates with a smaller yet still positive citation advantage of 4.3% on average.","However, we do not find a significant citation advantage for sharing code.","Further research is needed on additional or alternative measures of impact beyond citations.","Our results are likely to be of interest to researchers, as well as publishers, research funders, and policymakers."],"url":"http://arxiv.org/abs/2404.16171v1"}
{"created":"2024-04-24 19:57:09","title":"Interpretable Machine Learning Models for Predicting the Next Targets of Activist Funds","abstract":"This work develops a predictive model to identify potential targets of activist investment funds, which strategically acquire significant corporate stakes to drive operational and strategic improvements and enhance shareholder value. Predicting these targets is crucial for companies to mitigate intervention risks, for activists to select optimal targets, and for investors to capitalize on associated stock price gains. Our analysis utilizes data from the Russell 3000 index from 2016 to 2022. We tested 123 variations of models using different data imputation, oversampling, and machine learning methods, achieving a top AUC-ROC of 0.782. This demonstrates the model's effectiveness in identifying likely targets of activist funds. We applied the Shapley value method to determine the most influential factors in a company's susceptibility to activist investment. This interpretative approach provides clear insights into the driving forces behind activist targeting. Our model offers stakeholders a strategic tool for proactive corporate governance and investment strategy, enhancing understanding of the dynamics of activist investing.","sentences":["This work develops a predictive model to identify potential targets of activist investment funds, which strategically acquire significant corporate stakes to drive operational and strategic improvements and enhance shareholder value.","Predicting these targets is crucial for companies to mitigate intervention risks, for activists to select optimal targets, and for investors to capitalize on associated stock price gains.","Our analysis utilizes data from the Russell 3000 index from 2016 to 2022.","We tested 123 variations of models using different data imputation, oversampling, and machine learning methods, achieving a top AUC-ROC of 0.782.","This demonstrates the model's effectiveness in identifying likely targets of activist funds.","We applied the Shapley value method to determine the most influential factors in a company's susceptibility to activist investment.","This interpretative approach provides clear insights into the driving forces behind activist targeting.","Our model offers stakeholders a strategic tool for proactive corporate governance and investment strategy, enhancing understanding of the dynamics of activist investing."],"url":"http://arxiv.org/abs/2404.16169v1"}
{"created":"2024-04-24 19:55:50","title":"The Over-Certainty Phenomenon in Modern UDA Algorithms","abstract":"When neural networks are confronted with unfamiliar data that deviate from their training set, this signifies a domain shift. While these networks output predictions on their inputs, they typically fail to account for their level of familiarity with these novel observations. This challenge becomes even more pronounced in resource-constrained settings, such as embedded systems or edge devices. To address such challenges, we aim to recalibrate a neural network's decision boundaries in relation to its cognizance of the data it observes, introducing an approach we coin as certainty distillation. While prevailing works navigate unsupervised domain adaptation (UDA) with the goal of curtailing model entropy, they unintentionally birth models that grapple with calibration inaccuracies - a dilemma we term the over-certainty phenomenon. In this paper, we probe the drawbacks of this traditional learning model. As a solution to the issue, we propose a UDA algorithm that not only augments accuracy but also assures model calibration, all while maintaining suitability for environments with limited computational resources.","sentences":["When neural networks are confronted with unfamiliar data that deviate from their training set, this signifies a domain shift.","While these networks output predictions on their inputs, they typically fail to account for their level of familiarity with these novel observations.","This challenge becomes even more pronounced in resource-constrained settings, such as embedded systems or edge devices.","To address such challenges, we aim to recalibrate a neural network's decision boundaries in relation to its cognizance of the data it observes, introducing an approach we coin as certainty distillation.","While prevailing works navigate unsupervised domain adaptation (UDA) with the goal of curtailing model entropy, they unintentionally birth models that grapple with calibration inaccuracies - a dilemma we term the over-certainty phenomenon.","In this paper, we probe the drawbacks of this traditional learning model.","As a solution to the issue, we propose a UDA algorithm that not only augments accuracy but also assures model calibration, all while maintaining suitability for environments with limited computational resources."],"url":"http://arxiv.org/abs/2404.16168v1"}
{"created":"2024-04-24 19:30:18","title":"Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant","abstract":"Large language models (LLMs) have demonstrated impressive generalization capabilities on specific tasks with human-written instruction data. However, the limited quantity, diversity, and professional expertise of such instruction data raise concerns about the performance of LLMs in psychotherapy tasks when provided with domain-specific instructions. To address this, we firstly propose Domain-Specific Assistant Instructions based on AlexanderStreet therapy, and secondly, we use an adaption fine-tuning method and retrieval augmented generation method to improve pre-trained LLMs. Through quantitative evaluation of linguistic quality using automatic and human evaluation, we observe that pre-trained LLMs on Psychotherapy Assistant Instructions outperform state-of-the-art LLMs response baselines. Our Assistant-Instruction approach offers a half-annotation method to align pre-trained LLMs with instructions and provide pre-trained LLMs with more psychotherapy knowledge.","sentences":["Large language models (LLMs) have demonstrated impressive generalization capabilities on specific tasks with human-written instruction data.","However, the limited quantity, diversity, and professional expertise of such instruction data raise concerns about the performance of LLMs in psychotherapy tasks when provided with domain-specific instructions.","To address this, we firstly propose Domain-Specific Assistant Instructions based on AlexanderStreet therapy, and secondly, we use an adaption fine-tuning method and retrieval augmented generation method to improve pre-trained LLMs.","Through quantitative evaluation of linguistic quality using automatic and human evaluation, we observe that pre-trained LLMs on Psychotherapy Assistant Instructions outperform state-of-the-art LLMs response baselines.","Our Assistant-Instruction approach offers a half-annotation method to align pre-trained LLMs with instructions and provide pre-trained LLMs with more psychotherapy knowledge."],"url":"http://arxiv.org/abs/2404.16160v1"}
{"created":"2024-04-24 19:25:58","title":"The Feasibility of Implementing Large-Scale Transformers on Multi-FPGA Platforms","abstract":"FPGAs are rarely mentioned when discussing the implementation of large machine learning applications, such as Large Language Models (LLMs), in the data center. There has been much evidence showing that single FPGAs can be competitive with GPUs in performance for some computations, especially for low latency, and often much more efficient when power is considered. This suggests that there is merit to exploring the use of multiple FPGAs for large machine learning applications. The challenge with using multiple FPGAs is that there is no commonly-accepted flow for developing and deploying multi-FPGA applications, i.e., there are no tools to describe a large application, map it to multiple FPGAs and then deploy the application on a multi-FPGA platform. In this paper, we explore the feasibility of implementing large transformers using multiple FPGAs by developing a scalable multi-FPGA platform and some tools to map large applications to the platform. We validate our approach by designing an efficient multi-FPGA version of the I-BERT transformer and implement one encoder using six FPGAs as a working proof-of-concept to show that our platform and tools work. Based on our proof-of-concept prototype and the estimations of performance using the latest FPGAs compared to GPUs, we conclude that there can be a place for FPGAs in the world of large machine learning applications. We demonstrate a promising first step that shows that with the right infrastructure and tools it is reasonable to continue to explore the possible benefits of using FPGAs for applications such as LLMs.","sentences":["FPGAs are rarely mentioned when discussing the implementation of large machine learning applications, such as Large Language Models (LLMs), in the data center.","There has been much evidence showing that single FPGAs can be competitive with GPUs in performance for some computations, especially for low latency, and often much more efficient when power is considered.","This suggests that there is merit to exploring the use of multiple FPGAs for large machine learning applications.","The challenge with using multiple FPGAs is that there is no commonly-accepted flow for developing and deploying multi-FPGA applications, i.e., there are no tools to describe a large application, map it to multiple FPGAs and then deploy the application on a multi-FPGA platform.","In this paper, we explore the feasibility of implementing large transformers using multiple FPGAs by developing a scalable multi-FPGA platform and some tools to map large applications to the platform.","We validate our approach by designing an efficient multi-FPGA version of the I-BERT transformer and implement one encoder using six FPGAs as a working proof-of-concept to show that our platform and tools work.","Based on our proof-of-concept prototype and the estimations of performance using the latest FPGAs compared to GPUs, we conclude that there can be a place for FPGAs in the world of large machine learning applications.","We demonstrate a promising first step that shows that with the right infrastructure and tools it is reasonable to continue to explore the possible benefits of using FPGAs for applications such as LLMs."],"url":"http://arxiv.org/abs/2404.16158v1"}
{"created":"2024-04-24 19:10:59","title":"A Rollup Comparison Framework","abstract":"Rollups are a popular blockchain paradigm where one blockchain network is anchored to a different blockchain network, typically though smart contracts and data commitments. The rollup executes transactions on its own network and periodically publishes them along with the state root of the rollup network. The state root is determined to be final by a protocol, often enforced by smart contracts on the anchoring blockchain, which may let the state roots be challenged or verify an accompanying validity proof. While this core functionality is universal to existing rollups, these systems have introduced unique features as they vie for users and market dominance. In this paper, we aim to classify ways in which these rollups differ in order to establish a common ground of understanding. We explore various dimensions in which these system can differ: familiarity, finality time, modularity, and maturity. The result is a framework that can be used to understand and compare the properties of rollups.","sentences":["Rollups are a popular blockchain paradigm where one blockchain network is anchored to a different blockchain network, typically though smart contracts and data commitments.","The rollup executes transactions on its own network and periodically publishes them along with the state root of the rollup network.","The state root is determined to be final by a protocol, often enforced by smart contracts on the anchoring blockchain, which may let the state roots be challenged or verify an accompanying validity proof.","While this core functionality is universal to existing rollups, these systems have introduced unique features as they vie for users and market dominance.","In this paper, we aim to classify ways in which these rollups differ in order to establish a common ground of understanding.","We explore various dimensions in which these system can differ: familiarity, finality time, modularity, and maturity.","The result is a framework that can be used to understand and compare the properties of rollups."],"url":"http://arxiv.org/abs/2404.16150v1"}
{"created":"2024-04-24 18:49:37","title":"3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement","abstract":"In the field of 3D Human Pose Estimation (HPE), accurately estimating human pose, especially in scenarios with occlusions, is a significant challenge. This work identifies and addresses a gap in the current state of the art in 3D HPE concerning the scarcity of data and strategies for handling occlusions. We introduce our novel BlendMimic3D dataset, designed to mimic real-world situations where occlusions occur for seamless integration in 3D HPE algorithms. Additionally, we propose a 3D pose refinement block, employing a Graph Convolutional Network (GCN) to enhance pose representation through a graph model. This GCN block acts as a plug-and-play solution, adaptable to various 3D HPE frameworks without requiring retraining them. By training the GCN with occluded data from BlendMimic3D, we demonstrate significant improvements in resolving occluded poses, with comparable results for non-occluded ones. Project web page is available at https://blendmimic3d.github.io/BlendMimic3D/.","sentences":["In the field of 3D Human Pose Estimation (HPE), accurately estimating human pose, especially in scenarios with occlusions, is a significant challenge.","This work identifies and addresses a gap in the current state of the art in 3D HPE concerning the scarcity of data and strategies for handling occlusions.","We introduce our novel BlendMimic3D dataset, designed to mimic real-world situations where occlusions occur for seamless integration in 3D HPE algorithms.","Additionally, we propose a 3D pose refinement block, employing a Graph Convolutional Network (GCN) to enhance pose representation through a graph model.","This GCN block acts as a plug-and-play solution, adaptable to various 3D HPE frameworks without requiring retraining them.","By training the GCN with occluded data from BlendMimic3D, we demonstrate significant improvements in resolving occluded poses, with comparable results for non-occluded ones.","Project web page is available at https://blendmimic3d.github.io/BlendMimic3D/."],"url":"http://arxiv.org/abs/2404.16136v1"}
