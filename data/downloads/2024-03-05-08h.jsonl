{"created":"2024-03-03 00:14:12","title":"Improving Uncertainty Sampling with Bell Curve Weight Function","abstract":"Typically, a supervised learning model is trained using passive learning by randomly selecting unlabelled instances to annotate. This approach is effective for learning a model, but can be costly in cases where acquiring labelled instances is expensive. For example, it can be time-consuming to manually identify spam mails (labelled instances) from thousands of emails (unlabelled instances) flooding an inbox during initial data collection. Generally, we answer the above scenario with uncertainty sampling, an active learning method that improves the efficiency of supervised learning by using fewer labelled instances than passive learning. Given an unlabelled data pool, uncertainty sampling queries the labels of instances where the predicted probabilities, p, fall into the uncertainty region, i.e., $p \\approx 0.5$. The newly acquired labels are then added to the existing labelled data pool to learn a new model. Nonetheless, the performance of uncertainty sampling is susceptible to the area of unpredictable responses (AUR) and the nature of the dataset. It is difficult to determine whether to use passive learning or uncertainty sampling without prior knowledge of a new dataset. To address this issue, we propose bell curve sampling, which employs a bell curve weight function to acquire new labels. With the bell curve centred at p=0.5, bell curve sampling selects instances whose predicted values are in the uncertainty area most of the time without neglecting the rest. Simulation results show that, most of the time bell curve sampling outperforms uncertainty sampling and passive learning in datasets of different natures and with AUR.","sentences":["Typically, a supervised learning model is trained using passive learning by randomly selecting unlabelled instances to annotate.","This approach is effective for learning a model, but can be costly in cases where acquiring labelled instances is expensive.","For example, it can be time-consuming to manually identify spam mails (labelled instances) from thousands of emails (unlabelled instances) flooding an inbox during initial data collection.","Generally, we answer the above scenario with uncertainty sampling, an active learning method that improves the efficiency of supervised learning by using fewer labelled instances than passive learning.","Given an unlabelled data pool, uncertainty sampling queries the labels of instances where the predicted probabilities, p, fall into the uncertainty region, i.e., $p \\approx 0.5$. The newly acquired labels are then added to the existing labelled data pool to learn a new model.","Nonetheless, the performance of uncertainty sampling is susceptible to the area of unpredictable responses (AUR) and the nature of the dataset.","It is difficult to determine whether to use passive learning or uncertainty sampling without prior knowledge of a new dataset.","To address this issue, we propose bell curve sampling, which employs a bell curve weight function to acquire new labels.","With the bell curve centred at p=0.5, bell curve sampling selects instances whose predicted values are in the uncertainty area most of the time without neglecting the rest.","Simulation results show that, most of the time bell curve sampling outperforms uncertainty sampling and passive learning in datasets of different natures and with AUR."],"url":"http://arxiv.org/abs/2403.01352v1"}
{"created":"2024-03-02 23:58:33","title":"The Repercussions of the COVID-19 Pandemic on Higher Education and its implications for Syrian Refugees Students (An Analytical Descriptive Study)","abstract":"This study aims to reveal the most important challenges and difficulties that refugee students faced in Jordanian universities (e.g., Yarmouk University, AL Al-Bayt, and the Private Zarqa University) due to the COVID-19 pandemic through measuring a different of indicators that are related, in addition, to identify some of the independent variables on e-educational challenges. In the study, the analytical description approach was used. The data collection tool is a questionnaire, which was distributed to a random sample of students electronically. Results show that the necessity to implement educational and psychological counseling programs and economic support programs to support the e-Learning costs. The study confirmed that refugees are the most affected students with the pandemic compared to the host community.   Keywords: Syrian refugees, COVID-19, e-learning","sentences":["This study aims to reveal the most important challenges and difficulties that refugee students faced in Jordanian universities (e.g., Yarmouk University, AL Al-Bayt, and the Private Zarqa University) due to the COVID-19 pandemic through measuring a different of indicators that are related, in addition, to identify some of the independent variables on e-educational challenges.","In the study, the analytical description approach was used.","The data collection tool is a questionnaire, which was distributed to a random sample of students electronically.","Results show that the necessity to implement educational and psychological counseling programs and economic support programs to support the e-Learning costs.","The study confirmed that refugees are the most affected students with the pandemic compared to the host community.   ","Keywords: Syrian refugees, COVID-19, e-learning"],"url":"http://arxiv.org/abs/2403.01347v1"}
{"created":"2024-03-02 23:53:24","title":"Improve Cost Efficiency of Active Learning over Noisy Dataset","abstract":"Active learning is a learning strategy whereby the machine learning algorithm actively identifies and labels data points to optimize its learning. This strategy is particularly effective in domains where an abundance of unlabeled data exists, but the cost of labeling these data points is prohibitively expensive. In this paper, we consider cases of binary classification, where acquiring a positive instance incurs a significantly higher cost compared to that of negative instances. For example, in the financial industry, such as in money-lending businesses, a defaulted loan constitutes a positive event leading to substantial financial loss. To address this issue, we propose a shifted normal distribution sampling function that samples from a wider range than typical uncertainty sampling. Our simulation underscores that our proposed sampling function limits both noisy and positive label selection, delivering between 20% and 32% improved cost efficiency over different test datasets.","sentences":["Active learning is a learning strategy whereby the machine learning algorithm actively identifies and labels data points to optimize its learning.","This strategy is particularly effective in domains where an abundance of unlabeled data exists, but the cost of labeling these data points is prohibitively expensive.","In this paper, we consider cases of binary classification, where acquiring a positive instance incurs a significantly higher cost compared to that of negative instances.","For example, in the financial industry, such as in money-lending businesses, a defaulted loan constitutes a positive event leading to substantial financial loss.","To address this issue, we propose a shifted normal distribution sampling function that samples from a wider range than typical uncertainty sampling.","Our simulation underscores that our proposed sampling function limits both noisy and positive label selection, delivering between 20% and 32% improved cost efficiency over different test datasets."],"url":"http://arxiv.org/abs/2403.01346v1"}
{"created":"2024-03-02 23:40:23","title":"ShapeBoost: Boosting Human Shape Estimation with Part-Based Parameterization and Clothing-Preserving Augmentation","abstract":"Accurate human shape recovery from a monocular RGB image is a challenging task because humans come in different shapes and sizes and wear different clothes. In this paper, we propose ShapeBoost, a new human shape recovery framework that achieves pixel-level alignment even for rare body shapes and high accuracy for people wearing different types of clothes. Unlike previous approaches that rely on the use of PCA-based shape coefficients, we adopt a new human shape parameterization that decomposes the human shape into bone lengths and the mean width of each part slice. This part-based parameterization technique achieves a balance between flexibility and validity using a semi-analytical shape reconstruction algorithm. Based on this new parameterization, a clothing-preserving data augmentation module is proposed to generate realistic images with diverse body shapes and accurate annotations. Experimental results show that our method outperforms other state-of-the-art methods in diverse body shape situations as well as in varied clothing situations.","sentences":["Accurate human shape recovery from a monocular RGB image is a challenging task because humans come in different shapes and sizes and wear different clothes.","In this paper, we propose ShapeBoost, a new human shape recovery framework that achieves pixel-level alignment even for rare body shapes and high accuracy for people wearing different types of clothes.","Unlike previous approaches that rely on the use of PCA-based shape coefficients, we adopt a new human shape parameterization that decomposes the human shape into bone lengths and the mean width of each part slice.","This part-based parameterization technique achieves a balance between flexibility and validity using a semi-analytical shape reconstruction algorithm.","Based on this new parameterization, a clothing-preserving data augmentation module is proposed to generate realistic images with diverse body shapes and accurate annotations.","Experimental results show that our method outperforms other state-of-the-art methods in diverse body shape situations as well as in varied clothing situations."],"url":"http://arxiv.org/abs/2403.01345v1"}
{"created":"2024-03-02 23:37:16","title":"Mitigating the Bias in the Model for Continual Test-Time Adaptation","abstract":"Continual Test-Time Adaptation (CTA) is a challenging task that aims to adapt a source pre-trained model to continually changing target domains. In the CTA setting, a model does not know when the target domain changes, thus facing a drastic change in the distribution of streaming inputs during the test-time. The key challenge is to keep adapting the model to the continually changing target domains in an online manner. We find that a model shows highly biased predictions as it constantly adapts to the chaining distribution of the target data. It predicts certain classes more often than other classes, making inaccurate over-confident predictions. This paper mitigates this issue to improve performance in the CTA scenario. To alleviate the bias issue, we make class-wise exponential moving average target prototypes with reliable target samples and exploit them to cluster the target features class-wisely. Moreover, we aim to align the target distributions to the source distribution by anchoring the target feature to its corresponding source prototype. With extensive experiments, our proposed method achieves noteworthy performance gain when applied on top of existing CTA methods without substantial adaptation time overhead.","sentences":["Continual Test-Time Adaptation (CTA) is a challenging task that aims to adapt a source pre-trained model to continually changing target domains.","In the CTA setting, a model does not know when the target domain changes, thus facing a drastic change in the distribution of streaming inputs during the test-time.","The key challenge is to keep adapting the model to the continually changing target domains in an online manner.","We find that a model shows highly biased predictions as it constantly adapts to the chaining distribution of the target data.","It predicts certain classes more often than other classes, making inaccurate over-confident predictions.","This paper mitigates this issue to improve performance in the CTA scenario.","To alleviate the bias issue, we make class-wise exponential moving average target prototypes with reliable target samples and exploit them to cluster the target features class-wisely.","Moreover, we aim to align the target distributions to the source distribution by anchoring the target feature to its corresponding source prototype.","With extensive experiments, our proposed method achieves noteworthy performance gain when applied on top of existing CTA methods without substantial adaptation time overhead."],"url":"http://arxiv.org/abs/2403.01344v1"}
{"created":"2024-03-02 23:33:44","title":"The Effectiveness of a Training Program Based on Health Education to Improve Health Empowerment Level among Refugees in Jordan","abstract":"Objectives: The study aimed to evaluate the effectiveness of a health education-based training program in enhancing the level of health empowerment among refugees in Jordan. Health empowerment is a key component to promote health as it enables individuals to control and manage their health outcomes and improve them. Refugees are a vulnerable population group with limited access to healthcare.   Methodology: The study sample consisted of 38 refugees in Irbid governorate, Jordan, who were conveniently selected in coordination with some organizations working in the field of asylum in the governorate. They were randomly divided into two groups: an experimental group (n = 19) that received the health education training program, and a control group (n = 19) that did not receive any health education training. The Health Empowerment Scale (HES), a validated tool, was used to collect data from both groups in the pre and post-tests, and a follow-up test was conducted for members of the experimental group only. Results: The results showed a statistically significant increase in the health empowerment scores for the experimental group that received the training program compared to the control group. The mean of the pre-test for the experimental group was (1.97 - 0.27), and for the control group, it was (1.84 - 0.21). The post-test mean for the experimental group became (3.88 - 0.13), while for the control group, it was (1.85 - 0.20). The follow-up test also demonstrated that the enhanced health empowerment levels were maintained in the experimental group, with no significant difference between the post-test and follow-up scores, indicating the effectiveness of the health education training program in enhancing health empowerment for refugees in Jordan.","sentences":["Objectives: The study aimed to evaluate the effectiveness of a health education-based training program in enhancing the level of health empowerment among refugees in Jordan.","Health empowerment is a key component to promote health as it enables individuals to control and manage their health outcomes and improve them.","Refugees are a vulnerable population group with limited access to healthcare.   ","Methodology:","The study sample consisted of 38 refugees in Irbid governorate, Jordan, who were conveniently selected in coordination with some organizations working in the field of asylum in the governorate.","They were randomly divided into two groups: an experimental group (n = 19) that received the health education training program, and a control group (n = 19) that did not receive any health education training.","The Health Empowerment Scale (HES), a validated tool, was used to collect data from both groups in the pre and post-tests, and a follow-up test was conducted for members of the experimental group only.","Results:","The results showed a statistically significant increase in the health empowerment scores for the experimental group that received the training program compared to the control group.","The mean of the pre-test for the experimental group was (1.97 - 0.27), and for the control group, it was (1.84 - 0.21).","The post-test mean for the experimental group became (3.88 - 0.13), while for the control group, it was (1.85 - 0.20).","The follow-up test also demonstrated that the enhanced health empowerment levels were maintained in the experimental group, with no significant difference between the post-test and follow-up scores, indicating the effectiveness of the health education training program in enhancing health empowerment for refugees in Jordan."],"url":"http://arxiv.org/abs/2403.01343v1"}
{"created":"2024-03-02 22:24:31","title":"Euclidean distance compression via deep random features","abstract":"Motivated by the problem of compressing point sets into as few bits as possible while maintaining information about approximate distances between points, we construct random nonlinear maps $\\varphi_\\ell$ that compress point sets in the following way. For a point set $S$, the map $\\varphi_\\ell:\\mathbb{R}^d \\to N^{-1/2}\\{-1,1\\}^N$ has the property that storing $\\varphi_\\ell(S)$ (a \\emph{sketch} of $S$) allows one to report pairwise squared distances between points in $S$ up to some multiplicative $(1\\pm \\epsilon)$ error with high probability as long as the minimum distance is not too small compared to $\\epsilon$. The maps $\\varphi_\\ell$ are the $\\ell$-fold composition of a certain type of random feature mapping. Moreover, we determine how large $N$ needs to be as a function of $\\epsilon$ and other parameters of the point set.   Compared to existing techniques, our maps offer several advantages. The standard method for compressing point sets by random mappings relies on the Johnson-Lindenstrauss lemma which implies that if a set of $n$ points is mapped by a Gaussian random matrix to $\\mathbb{R}^k$ with $k =\\Theta(\\epsilon^{-2}\\log n)$, then pairwise distances between points are preserved up to a multiplicative $(1\\pm \\epsilon)$ error with high probability. The main advantage of our maps $\\varphi_\\ell$ over random linear maps is that ours map point sets directly into the discrete cube $N^{-1/2}\\{-1,1\\}^N$ and so there is no additional step needed to convert the sketch to bits. For some range of parameters, our maps $\\varphi_\\ell$ produce sketches which require fewer bits of storage space.","sentences":["Motivated by the problem of compressing point sets into as few bits as possible while maintaining information about approximate distances between points, we construct random nonlinear maps $\\varphi_\\ell$ that compress point sets in the following way.","For a point set $S$, the map $\\varphi_\\ell:\\mathbb{R}^d \\to N^{-1/2}\\{-1,1\\}^N$ has the property that storing $\\varphi_\\ell(S)$ (a \\emph{sketch} of $S$) allows one to report pairwise squared distances between points in $S$ up to some multiplicative $(1\\pm \\epsilon)$ error with high probability as long as the minimum distance is not too small compared to $\\epsilon$. The maps $\\varphi_\\ell$ are the $\\ell$-fold composition of a certain type of random feature mapping.","Moreover, we determine how large $N$ needs to be as a function of $\\epsilon$ and other parameters of the point set.   ","Compared to existing techniques, our maps offer several advantages.","The standard method for compressing point sets by random mappings relies on the Johnson-Lindenstrauss lemma which implies that if a set of $n$ points is mapped by a Gaussian random matrix to $\\mathbb{R}^k$ with $k =\\Theta(\\epsilon^{-2}\\log n)$, then pairwise distances between points are preserved up to a multiplicative $(1\\pm \\epsilon)$ error with high probability.","The main advantage of our maps $\\varphi_\\ell$ over random linear maps is that ours map point sets directly into the discrete cube $N^{-1/2}\\{-1,1\\}^N$ and so there is no additional step needed to convert the sketch to bits.","For some range of parameters, our maps $\\varphi_\\ell$ produce sketches which require fewer bits of storage space."],"url":"http://arxiv.org/abs/2403.01327v1"}
{"created":"2024-03-02 22:08:10","title":"NeRF-VPT: Learning Novel View Representations with Neural Radiance Fields via View Prompt Tuning","abstract":"Neural Radiance Fields (NeRF) have garnered remarkable success in novel view synthesis. Nonetheless, the task of generating high-quality images for novel views persists as a critical challenge. While the existing efforts have exhibited commendable progress, capturing intricate details, enhancing textures, and achieving superior Peak Signal-to-Noise Ratio (PSNR) metrics warrant further focused attention and advancement. In this work, we propose NeRF-VPT, an innovative method for novel view synthesis to address these challenges. Our proposed NeRF-VPT employs a cascading view prompt tuning paradigm, wherein RGB information gained from preceding rendering outcomes serves as instructive visual prompts for subsequent rendering stages, with the aspiration that the prior knowledge embedded in the prompts can facilitate the gradual enhancement of rendered image quality. NeRF-VPT only requires sampling RGB data from previous stage renderings as priors at each training stage, without relying on extra guidance or complex techniques. Thus, our NeRF-VPT is plug-and-play and can be readily integrated into existing methods. By conducting comparative analyses of our NeRF-VPT against several NeRF-based approaches on demanding real-scene benchmarks, such as Realistic Synthetic 360, Real Forward-Facing, Replica dataset, and a user-captured dataset, we substantiate that our NeRF-VPT significantly elevates baseline performance and proficiently generates more high-quality novel view images than all the compared state-of-the-art methods. Furthermore, the cascading learning of NeRF-VPT introduces adaptability to scenarios with sparse inputs, resulting in a significant enhancement of accuracy for sparse-view novel view synthesis. The source code and dataset are available at \\url{https://github.com/Freedomcls/NeRF-VPT}.","sentences":["Neural Radiance Fields (NeRF) have garnered remarkable success in novel view synthesis.","Nonetheless, the task of generating high-quality images for novel views persists as a critical challenge.","While the existing efforts have exhibited commendable progress, capturing intricate details, enhancing textures, and achieving superior Peak Signal-to-Noise Ratio (PSNR) metrics warrant further focused attention and advancement.","In this work, we propose NeRF-VPT, an innovative method for novel view synthesis to address these challenges.","Our proposed NeRF-VPT employs a cascading view prompt tuning paradigm, wherein RGB information gained from preceding rendering outcomes serves as instructive visual prompts for subsequent rendering stages, with the aspiration that the prior knowledge embedded in the prompts can facilitate the gradual enhancement of rendered image quality.","NeRF-VPT only requires sampling RGB data from previous stage renderings as priors at each training stage, without relying on extra guidance or complex techniques.","Thus, our NeRF-VPT is plug-and-play and can be readily integrated into existing methods.","By conducting comparative analyses of our NeRF-VPT against several NeRF-based approaches on demanding real-scene benchmarks, such as Realistic Synthetic 360, Real Forward-Facing, Replica dataset, and a user-captured dataset, we substantiate that our NeRF-VPT significantly elevates baseline performance and proficiently generates more high-quality novel view images than all the compared state-of-the-art methods.","Furthermore, the cascading learning of NeRF-VPT introduces adaptability to scenarios with sparse inputs, resulting in a significant enhancement of accuracy for sparse-view novel view synthesis.","The source code and dataset are available at \\url{https://github.com/Freedomcls/NeRF-VPT}."],"url":"http://arxiv.org/abs/2403.01325v1"}
{"created":"2024-03-02 21:29:04","title":"TUMTraf V2X Cooperative Perception Dataset","abstract":"Cooperative perception offers several benefits for enhancing the capabilities of autonomous vehicles and improving road safety. Using roadside sensors in addition to onboard sensors increases reliability and extends the sensor range. External sensors offer higher situational awareness for automated vehicles and prevent occlusions. We propose CoopDet3D, a cooperative multi-modal fusion model, and TUMTraf-V2X, a perception dataset, for the cooperative 3D object detection and tracking task. Our dataset contains 2,000 labeled point clouds and 5,000 labeled images from five roadside and four onboard sensors. It includes 30k 3D boxes with track IDs and precise GPS and IMU data. We labeled eight categories and covered occlusion scenarios with challenging driving maneuvers, like traffic violations, near-miss events, overtaking, and U-turns. Through multiple experiments, we show that our CoopDet3D camera-LiDAR fusion model achieves an increase of +14.36 3D mAP compared to a vehicle camera-LiDAR fusion model. Finally, we make our dataset, model, labeling tool, and dev-kit publicly available on our website: https://tum-traffic-dataset.github.io/tumtraf-v2x.","sentences":["Cooperative perception offers several benefits for enhancing the capabilities of autonomous vehicles and improving road safety.","Using roadside sensors in addition to onboard sensors increases reliability and extends the sensor range.","External sensors offer higher situational awareness for automated vehicles and prevent occlusions.","We propose CoopDet3D, a cooperative multi-modal fusion model, and TUMTraf-V2X, a perception dataset, for the cooperative 3D object detection and tracking task.","Our dataset contains 2,000 labeled point clouds and 5,000 labeled images from five roadside and four onboard sensors.","It includes 30k 3D boxes with track IDs and precise GPS and IMU data.","We labeled eight categories and covered occlusion scenarios with challenging driving maneuvers, like traffic violations, near-miss events, overtaking, and U-turns.","Through multiple experiments, we show that our CoopDet3D camera-LiDAR fusion model achieves an increase of +14.36 3D mAP compared to a vehicle camera-LiDAR fusion model.","Finally, we make our dataset, model, labeling tool, and dev-kit publicly available on our website: https://tum-traffic-dataset.github.io/tumtraf-v2x."],"url":"http://arxiv.org/abs/2403.01316v1"}
{"created":"2024-03-02 21:22:36","title":"Superflows: A New Tool for Forensic Network Flow Analysis","abstract":"Network security analysts gather data from diverse sources, from high-level summaries of network flow and traffic volumes to low-level details such as service logs from servers and the contents of individual packets. They validate and check this data against traffic patterns and historical indicators of compromise. Based on the results of this analysis, a decision is made to either automatically manage the traffic or report it to an analyst for further investigation. Unfortunately, due rapidly increasing traffic volumes, there are far more events to check than operational teams can handle for effective forensic analysis. However, just as packets are grouped into flows that share a commonality, we argue that a high-level construct for grouping network flows into a set a flows that share a hypothesis is needed to significantly improve the quality of operational network response by increasing Events Per Analysts Hour (EPAH).   In this paper, we propose a formalism for describing a superflow construct, which we characterize as an aggregation of one or more flows based on an analyst-specific hypothesis about traffic behavior. We demonstrate simple superflow constructions and representations, and perform a case study to explain how the formalism can be used to reduce the volume of data for forensic analysis.","sentences":["Network security analysts gather data from diverse sources, from high-level summaries of network flow and traffic volumes to low-level details such as service logs from servers and the contents of individual packets.","They validate and check this data against traffic patterns and historical indicators of compromise.","Based on the results of this analysis, a decision is made to either automatically manage the traffic or report it to an analyst for further investigation.","Unfortunately, due rapidly increasing traffic volumes, there are far more events to check than operational teams can handle for effective forensic analysis.","However, just as packets are grouped into flows that share a commonality, we argue that a high-level construct for grouping network flows into a set a flows that share a hypothesis is needed to significantly improve the quality of operational network response by increasing Events Per Analysts Hour (EPAH).   ","In this paper, we propose a formalism for describing a superflow construct, which we characterize as an aggregation of one or more flows based on an analyst-specific hypothesis about traffic behavior.","We demonstrate simple superflow constructions and representations, and perform a case study to explain how the formalism can be used to reduce the volume of data for forensic analysis."],"url":"http://arxiv.org/abs/2403.01314v1"}
{"created":"2024-03-02 20:36:10","title":"ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation","abstract":"Web-scale training on paired text-image data is becoming increasingly central to multimodal learning, but is challenged by the highly noisy nature of datasets in the wild. Standard data filtering approaches succeed in removing mismatched text-image pairs, but permit semantically related but highly abstract or subjective text. These approaches lack the fine-grained ability to isolate the most concrete samples that provide the strongest signal for learning in a noisy dataset. In this work, we propose a new metric, image caption concreteness, that evaluates caption text without an image reference to measure its concreteness and relevancy for use in multimodal learning. Our approach leverages strong foundation models for measuring visual-semantic information loss in multimodal representations. We demonstrate that this strongly correlates with human evaluation of concreteness in both single-word and sentence-level texts. Moreover, we show that curation using ICC complements existing approaches: It succeeds in selecting the highest quality samples from multimodal web-scale datasets to allow for efficient training in resource-constrained settings.","sentences":["Web-scale training on paired text-image data is becoming increasingly central to multimodal learning, but is challenged by the highly noisy nature of datasets in the wild.","Standard data filtering approaches succeed in removing mismatched text-image pairs, but permit semantically related but highly abstract or subjective text.","These approaches lack the fine-grained ability to isolate the most concrete samples that provide the strongest signal for learning in a noisy dataset.","In this work, we propose a new metric, image caption concreteness, that evaluates caption text without an image reference to measure its concreteness and relevancy for use in multimodal learning.","Our approach leverages strong foundation models for measuring visual-semantic information loss in multimodal representations.","We demonstrate that this strongly correlates with human evaluation of concreteness in both single-word and sentence-level texts.","Moreover, we show that curation using ICC complements existing approaches: It succeeds in selecting the highest quality samples from multimodal web-scale datasets to allow for efficient training in resource-constrained settings."],"url":"http://arxiv.org/abs/2403.01306v1"}
