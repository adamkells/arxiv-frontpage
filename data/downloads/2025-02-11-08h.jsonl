{"created":"2025-02-10 18:59:58","title":"EVEv2: Improved Baselines for Encoder-Free Vision-Language Models","abstract":"Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient deployment. We systematically clarify the performance gap between VLMs using pre-trained vision encoders, discrete tokenizers, and minimalist visual layers from scratch, deeply excavating the under-examined characteristics of encoder-free VLMs. We develop efficient strategies for encoder-free VLMs that rival mainstream encoder-based ones. After an in-depth investigation, we launch EVEv2.0, a new and improved family of encoder-free VLMs. We show that: (i) Properly decomposing and hierarchically associating vision and language within a unified model reduces interference between modalities. (ii) A well-designed training strategy enables effective optimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0 represents a thorough study for developing a decoder-only architecture across modalities, demonstrating superior data efficiency and strong vision-reasoning capability. Code is publicly available at: https://github.com/baaivision/EVE.","sentences":["Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient deployment.","We systematically clarify the performance gap between VLMs using pre-trained vision encoders, discrete tokenizers, and minimalist visual layers from scratch, deeply excavating the under-examined characteristics of encoder-free VLMs.","We develop efficient strategies for encoder-free VLMs that rival mainstream encoder-based ones.","After an in-depth investigation, we launch EVEv2.0, a new and improved family of encoder-free VLMs.","We show that: (i) Properly decomposing and hierarchically associating vision and language within a unified model reduces interference between modalities.","(ii) A well-designed training strategy enables effective optimization for encoder-free VLMs.","Through extensive evaluation, our EVEv2.0 represents a thorough study for developing a decoder-only architecture across modalities, demonstrating superior data efficiency and strong vision-reasoning capability.","Code is publicly available at: https://github.com/baaivision/EVE."],"url":"http://arxiv.org/abs/2502.06788v1"}
{"created":"2025-02-10 18:59:10","title":"Matryoshka Quantization","abstract":"Quantizing model weights is critical for reducing the communication and inference costs of large models. However, quantizing models -- especially to low precisions like int4 or int2 -- requires a trade-off in model quality; int2, in particular, is known to severely degrade model quality. Consequently, practitioners are often forced to maintain multiple models with different quantization levels or serve a single model that best satisfies the quality-latency trade-off. On the other hand, integer data types, such as int8, inherently possess a nested (Matryoshka) structure where smaller bit-width integers, like int4 or int2, are nested within the most significant bits. This paper proposes Matryoshka Quantization (MatQuant), a novel multi-scale quantization technique that addresses the challenge of needing multiple quantized models. It allows training and maintaining just one model, which can then be served at different precision levels. Furthermore, due to the co-training and co-distillation regularization provided by MatQuant, the int2 precision models extracted by MatQuant can be up to $10\\%$ more accurate than standard int2 quantization (using techniques like QAT or OmniQuant). This represents significant progress in model quantization, demonstrated by the fact that, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more accurate than an int8 FFN-quantized Gemma-2 2B model.","sentences":["Quantizing model weights is critical for reducing the communication and inference costs of large models.","However, quantizing models -- especially to low precisions like int4 or int2 -- requires a trade-off in model quality; int2, in particular, is known to severely degrade model quality.","Consequently, practitioners are often forced to maintain multiple models with different quantization levels or serve a single model that best satisfies the quality-latency trade-off.","On the other hand, integer data types, such as int8, inherently possess a nested (Matryoshka) structure where smaller bit-width integers, like int4 or int2, are nested within the most significant bits.","This paper proposes Matryoshka Quantization (MatQuant), a novel multi-scale quantization technique that addresses the challenge of needing multiple quantized models.","It allows training and maintaining just one model, which can then be served at different precision levels.","Furthermore, due to the co-training and co-distillation regularization provided by MatQuant, the int2 precision models extracted by MatQuant can be up to $10\\%$ more accurate than standard int2 quantization (using techniques like QAT or OmniQuant).","This represents significant progress in model quantization, demonstrated by the fact that, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more accurate than an int8 FFN-quantized Gemma-2 2B model."],"url":"http://arxiv.org/abs/2502.06786v1"}
{"created":"2025-02-10 18:58:40","title":"RelGNN: Composite Message Passing for Relational Deep Learning","abstract":"Predictive tasks on relational databases are critical in real-world applications spanning e-commerce, healthcare, and social media. To address these tasks effectively, Relational Deep Learning (RDL) encodes relational data as graphs, enabling Graph Neural Networks (GNNs) to exploit relational structures for improved predictions. However, existing heterogeneous GNNs often overlook the intrinsic structural properties of relational databases, leading to modeling inefficiencies. Here we introduce RelGNN, a novel GNN framework specifically designed to capture the unique characteristics of relational databases. At the core of our approach is the introduction of atomic routes, which are sequences of nodes forming high-order tripartite structures. Building upon these atomic routes, RelGNN designs new composite message passing mechanisms between heterogeneous nodes, allowing direct single-hop interactions between them. This approach avoids redundant aggregations and mitigates information entanglement, ultimately leading to more efficient and accurate predictive modeling. RelGNN is evaluated on 30 diverse real-world tasks from RelBench (Fey et al., 2024), and consistently achieves state-of-the-art accuracy with up to 25% improvement.","sentences":["Predictive tasks on relational databases are critical in real-world applications spanning e-commerce, healthcare, and social media.","To address these tasks effectively, Relational Deep Learning (RDL) encodes relational data as graphs, enabling Graph Neural Networks (GNNs) to exploit relational structures for improved predictions.","However, existing heterogeneous GNNs often overlook the intrinsic structural properties of relational databases, leading to modeling inefficiencies.","Here we introduce RelGNN, a novel GNN framework specifically designed to capture the unique characteristics of relational databases.","At the core of our approach is the introduction of atomic routes, which are sequences of nodes forming high-order tripartite structures.","Building upon these atomic routes, RelGNN designs new composite message passing mechanisms between heterogeneous nodes, allowing direct single-hop interactions between them.","This approach avoids redundant aggregations and mitigates information entanglement, ultimately leading to more efficient and accurate predictive modeling.","RelGNN is evaluated on 30 diverse real-world tasks from RelBench (Fey et al., 2024), and consistently achieves state-of-the-art accuracy with up to 25% improvement."],"url":"http://arxiv.org/abs/2502.06784v1"}
{"created":"2025-02-10 18:58:11","title":"Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT","abstract":"Recent advancements have established Diffusion Transformers (DiTs) as a dominant framework in generative modeling. Building on this success, Lumina-Next achieves exceptional performance in the generation of photorealistic images with Next-DiT. However, its potential for video generation remains largely untapped, with significant challenges in modeling the spatiotemporal complexity inherent to video data. To address this, we introduce Lumina-Video, a framework that leverages the strengths of Next-DiT while introducing tailored solutions for video synthesis. Lumina-Video incorporates a Multi-scale Next-DiT architecture, which jointly learns multiple patchifications to enhance both efficiency and flexibility. By incorporating the motion score as an explicit condition, Lumina-Video also enables direct control of generated videos' dynamic degree. Combined with a progressive training scheme with increasingly higher resolution and FPS, and a multi-source training scheme with mixed natural and synthetic data, Lumina-Video achieves remarkable aesthetic quality and motion smoothness at high training and inference efficiency. We additionally propose Lumina-V2A, a video-to-audio model based on Next-DiT, to create synchronized sounds for generated videos. Codes are released at https://www.github.com/Alpha-VLLM/Lumina-Video.","sentences":["Recent advancements have established Diffusion Transformers (DiTs) as a dominant framework in generative modeling.","Building on this success, Lumina-Next achieves exceptional performance in the generation of photorealistic images with Next-DiT.","However, its potential for video generation remains largely untapped, with significant challenges in modeling the spatiotemporal complexity inherent to video data.","To address this, we introduce Lumina-Video, a framework that leverages the strengths of Next-DiT while introducing tailored solutions for video synthesis.","Lumina-Video incorporates a Multi-scale Next-DiT architecture, which jointly learns multiple patchifications to enhance both efficiency and flexibility.","By incorporating the motion score as an explicit condition, Lumina-Video also enables direct control of generated videos' dynamic degree.","Combined with a progressive training scheme with increasingly higher resolution and FPS, and a multi-source training scheme with mixed natural and synthetic data, Lumina-Video achieves remarkable aesthetic quality and motion smoothness at high training and inference efficiency.","We additionally propose Lumina-V2A, a video-to-audio model based on Next-DiT, to create synchronized sounds for generated videos.","Codes are released at https://www.github.com/Alpha-VLLM/Lumina-Video."],"url":"http://arxiv.org/abs/2502.06782v1"}
{"created":"2025-02-10 18:57:29","title":"Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning","abstract":"Reasoning abilities, especially those for solving complex math problems, are crucial components of general intelligence. Recent advances by proprietary companies, such as o-series models of OpenAI, have made remarkable progress on reasoning tasks. However, the complete technical details remain unrevealed, and the techniques that are believed certainly to be adopted are only reinforcement learning (RL) and the long chain of thoughts. This paper proposes a new RL framework, termed OREAL, to pursue the performance limit that can be achieved through \\textbf{O}utcome \\textbf{RE}w\\textbf{A}rd-based reinforcement \\textbf{L}earning for mathematical reasoning tasks, where only binary outcome rewards are easily accessible. We theoretically prove that behavior cloning on positive trajectories from best-of-N (BoN) sampling is sufficient to learn the KL-regularized optimal policy in binary feedback environments. This formulation further implies that the rewards of negative samples should be reshaped to ensure the gradient consistency between positive and negative samples. To alleviate the long-existing difficulties brought by sparse rewards in RL, which are even exacerbated by the partial correctness of the long chain of thought for reasoning tasks, we further apply a token-level reward model to sample important tokens in reasoning trajectories for learning. With OREAL, for the first time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL, being on par with 32B models. OREAL-32B also surpasses previous 32B models trained by distillation with 95.0 pass@1 accuracy on MATH-500. Our investigation also indicates the importance of initial policy models and training queries for RL. Code, models, and data will be released to benefit future research\\footnote{https://github.com/InternLM/OREAL}.","sentences":["Reasoning abilities, especially those for solving complex math problems, are crucial components of general intelligence.","Recent advances by proprietary companies, such as o-series models of OpenAI, have made remarkable progress on reasoning tasks.","However, the complete technical details remain unrevealed, and the techniques that are believed certainly to be adopted are only reinforcement learning (RL) and the long chain of thoughts.","This paper proposes a new RL framework, termed OREAL, to pursue the performance limit that can be achieved through \\textbf{O}utcome \\textbf{RE}w\\textbf{A}rd-based reinforcement \\textbf{L}earning for mathematical reasoning tasks, where only binary outcome rewards are easily accessible.","We theoretically prove that behavior cloning on positive trajectories from best-of-N (BoN) sampling is sufficient to learn the KL-regularized optimal policy in binary feedback environments.","This formulation further implies that the rewards of negative samples should be reshaped to ensure the gradient consistency between positive and negative samples.","To alleviate the long-existing difficulties brought by sparse rewards in RL, which are even exacerbated by the partial correctness of the long chain of thought for reasoning tasks, we further apply a token-level reward model to sample important tokens in reasoning trajectories for learning.","With OREAL, for the first time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL, being on par with 32B models.","OREAL-32B also surpasses previous 32B models trained by distillation with 95.0 pass@1 accuracy on MATH-500.","Our investigation also indicates the importance of initial policy models and training queries for RL. Code, models, and data will be released to benefit future research\\footnote{https://github.com/InternLM/OREAL}."],"url":"http://arxiv.org/abs/2502.06781v1"}
{"created":"2025-02-10 18:54:05","title":"Towards Internet-Scale Training For Agents","abstract":"The predominant approach for training web navigation agents gathers human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data are an inefficient resource. We develop a pipeline to facilitate Internet-scale training for agents without laborious human annotations. In the first stage, an LLM generates tasks for 150k diverse websites. In the next stage, LLM agents complete tasks and produce trajectories. In the final stage, an LLM reviews the trajectories and judges their success. Language models are competitive with human annotators, detecting and filtering out harmful content with an accuracy of 97%, generating feasible tasks with an 89% rate, and judging successful trajectories with an 82.6% accuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of tasks for 150k sites. Training on the data generated by our pipeline is competitive with training on human demonstrations. In data-limited settings derived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and +122.1% respectively for agents trained on mixtures of data from our pipeline, and human data. When training agents with all available human data from these benchmarks, agents fail to generalize to diverse real sites, and adding our data improves their generalization by +149.0% for WebLINX and +156.3% for Mind2Web. Code will be available at: data-for-agents.github.io.","sentences":["The predominant approach for training web navigation agents gathers human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data are an inefficient resource.","We develop a pipeline to facilitate Internet-scale training for agents without laborious human annotations.","In the first stage, an LLM generates tasks for 150k diverse websites.","In the next stage, LLM agents complete tasks and produce trajectories.","In the final stage, an LLM reviews the trajectories and judges their success.","Language models are competitive with human annotators, detecting and filtering out harmful content with an accuracy of 97%, generating feasible tasks with an 89% rate, and judging successful trajectories with an 82.6% accuracy.","Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of tasks for 150k sites.","Training on the data generated by our pipeline is competitive with training on human demonstrations.","In data-limited settings derived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and +122.1% respectively for agents trained on mixtures of data from our pipeline, and human data.","When training agents with all available human data from these benchmarks, agents fail to generalize to diverse real sites, and adding our data improves their generalization by +149.0% for WebLINX and +156.3% for Mind2Web.","Code will be available at: data-for-agents.github.io."],"url":"http://arxiv.org/abs/2502.06776v1"}
{"created":"2025-02-10 18:47:04","title":"Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs","abstract":"There is growing demand for performing inference with hundreds of thousands of input tokens on trained transformer models. Inference at this extreme scale demands significant computational resources, hindering the application of transformers at long contexts on commodity (i.e not data center scale) hardware. To address the inference time costs associated with running self-attention based transformer language models on long contexts and enable their adoption on widely available hardware, we propose a tunable mechanism that reduces the cost of the forward pass by attending to only the most relevant tokens at every generation step using a top-k selection mechanism. We showcase the efficiency gains afforded by our method by performing inference on context windows up to 1M tokens using approximately 16GB of GPU RAM. Our experiments reveal that models are capable of handling the sparsity induced by the reduced number of keys and values. By attending to less than 2% of input tokens, we achieve over 95% of model performance on common long context benchmarks (LM-Eval, AlpacaEval, and RULER).","sentences":["There is growing demand for performing inference with hundreds of thousands of input tokens on trained transformer models.","Inference at this extreme scale demands significant computational resources, hindering the application of transformers at long contexts on commodity (i.e not data center scale) hardware.","To address the inference time costs associated with running self-attention based transformer language models on long contexts and enable their adoption on widely available hardware, we propose a tunable mechanism that reduces the cost of the forward pass by attending to only the most relevant tokens at every generation step using a top-k selection mechanism.","We showcase the efficiency gains afforded by our method by performing inference on context windows up to 1M tokens using approximately 16GB of GPU RAM.","Our experiments reveal that models are capable of handling the sparsity induced by the reduced number of keys and values.","By attending to less than 2% of input tokens, we achieve over 95% of model performance on common long context benchmarks (LM-Eval, AlpacaEval, and RULER)."],"url":"http://arxiv.org/abs/2502.06766v1"}
{"created":"2025-02-10 18:33:15","title":"SAMRefiner: Taming Segment Anything Model for Universal Mask Refinement","abstract":"In this paper, we explore a principal way to enhance the quality of widely pre-existing coarse masks, enabling them to serve as reliable training data for segmentation models to reduce the annotation cost. In contrast to prior refinement techniques that are tailored to specific models or tasks in a close-world manner, we propose SAMRefiner, a universal and efficient approach by adapting SAM to the mask refinement task. The core technique of our model is the noise-tolerant prompting scheme. Specifically, we introduce a multi-prompt excavation strategy to mine diverse input prompts for SAM (i.e., distance-guided points, context-aware elastic bounding boxes, and Gaussian-style masks) from initial coarse masks. These prompts can collaborate with each other to mitigate the effect of defects in coarse masks. In particular, considering the difficulty of SAM to handle the multi-object case in semantic segmentation, we introduce a split-then-merge (STM) pipeline. Additionally, we extend our method to SAMRefiner++ by introducing an additional IoU adaption step to further boost the performance of the generic SAMRefiner on the target dataset. This step is self-boosted and requires no additional annotation. The proposed framework is versatile and can flexibly cooperate with existing segmentation methods. We evaluate our mask framework on a wide range of benchmarks under different settings, demonstrating better accuracy and efficiency. SAMRefiner holds significant potential to expedite the evolution of refinement tools. Our code is available at https://github.com/linyq2117/SAMRefiner.","sentences":["In this paper, we explore a principal way to enhance the quality of widely pre-existing coarse masks, enabling them to serve as reliable training data for segmentation models to reduce the annotation cost.","In contrast to prior refinement techniques that are tailored to specific models or tasks in a close-world manner, we propose SAMRefiner, a universal and efficient approach by adapting SAM to the mask refinement task.","The core technique of our model is the noise-tolerant prompting scheme.","Specifically, we introduce a multi-prompt excavation strategy to mine diverse input prompts for SAM (i.e., distance-guided points, context-aware elastic bounding boxes, and Gaussian-style masks) from initial coarse masks.","These prompts can collaborate with each other to mitigate the effect of defects in coarse masks.","In particular, considering the difficulty of SAM to handle the multi-object case in semantic segmentation, we introduce a split-then-merge (STM) pipeline.","Additionally, we extend our method to SAMRefiner++ by introducing an additional IoU adaption step to further boost the performance of the generic SAMRefiner on the target dataset.","This step is self-boosted and requires no additional annotation.","The proposed framework is versatile and can flexibly cooperate with existing segmentation methods.","We evaluate our mask framework on a wide range of benchmarks under different settings, demonstrating better accuracy and efficiency.","SAMRefiner holds significant potential to expedite the evolution of refinement tools.","Our code is available at https://github.com/linyq2117/SAMRefiner."],"url":"http://arxiv.org/abs/2502.06756v1"}
{"created":"2025-02-10 18:28:21","title":"Blockchain-Powered Asset Tokenization Platform","abstract":"Blockchain Technology has revolutionized Finance and Technology with its secure, decentralized, and trust-less methodologies of data management. In a world where asset value fluctuations are unprecedented, it has become increasingly important to secure one's stake on their valuable assets and streamline the process of acquiring and transferring that stake over a trust-less environment. Tokenization proves to be unbeaten when it comes to giving the ownership of one's asset, an immutable, liquid, and irrefutable identity, as of the likes of cryptocurrency. It enables users to store and maintain records of their assets and even transfer fractions of these assets to other investors and stakeholders in the form of these tokens. However, like cryptocurrency, it too has witnessed attacks by malicious users that have compromised on their very foundation of security.These attacks have inflicted more damage since they represent real-world assets that have physical importance. This project aims to assist users to secure their valuable assets by providing a highly secure user-friendly platform to manage, create and deploy asset-tokens, and facilitate open and transparent communication between stakeholders, thereby upholding the decentralized nature of blockchain and offering the financial freedom of asset ownership, with an added market value of a cryptocurrency-backed tokens.","sentences":["Blockchain Technology has revolutionized Finance and Technology with its secure, decentralized, and trust-less methodologies of data management.","In a world where asset value fluctuations are unprecedented, it has become increasingly important to secure one's stake on their valuable assets and streamline the process of acquiring and transferring that stake over a trust-less environment.","Tokenization proves to be unbeaten when it comes to giving the ownership of one's asset, an immutable, liquid, and irrefutable identity, as of the likes of cryptocurrency.","It enables users to store and maintain records of their assets and even transfer fractions of these assets to other investors and stakeholders in the form of these tokens.","However, like cryptocurrency, it too has witnessed attacks by malicious users that have compromised on their very foundation of security.","These attacks have inflicted more damage since they represent real-world assets that have physical importance.","This project aims to assist users to secure their valuable assets by providing a highly secure user-friendly platform to manage, create and deploy asset-tokens, and facilitate open and transparent communication between stakeholders, thereby upholding the decentralized nature of blockchain and offering the financial freedom of asset ownership, with an added market value of a cryptocurrency-backed tokens."],"url":"http://arxiv.org/abs/2502.06752v1"}
{"created":"2025-02-10 18:23:55","title":"Accelerating Data Processing and Benchmarking of AI Models for Pathology","abstract":"Advances in foundation modeling have reshaped computational pathology. However, the increasing number of available models and lack of standardized benchmarks make it increasingly complex to assess their strengths, limitations, and potential for further development. To address these challenges, we introduce a new suite of software tools for whole-slide image processing, foundation model benchmarking, and curated publicly available tasks. We anticipate that these resources will promote transparency, reproducibility, and continued progress in the field.","sentences":["Advances in foundation modeling have reshaped computational pathology.","However, the increasing number of available models and lack of standardized benchmarks make it increasingly complex to assess their strengths, limitations, and potential for further development.","To address these challenges, we introduce a new suite of software tools for whole-slide image processing, foundation model benchmarking, and curated publicly available tasks.","We anticipate that these resources will promote transparency, reproducibility, and continued progress in the field."],"url":"http://arxiv.org/abs/2502.06750v1"}
{"created":"2025-02-10 18:10:09","title":"A Fair Federated Learning Framework for Collaborative Network Traffic Prediction and Resource Allocation","abstract":"In the beyond 5G era, AI/ML empowered realworld digital twins (DTs) will enable diverse network operators to collaboratively optimize their networks, ultimately improving end-user experience. Although centralized AI-based learning techniques have been shown to achieve significant network traffic accuracy, resulting in efficient network operations, they require sharing of sensitive data among operators, leading to privacy and security concerns. Distributed learning, and specifically federated learning (FL), that keeps data isolated at local clients, has emerged as an effective and promising solution for mitigating such concerns. Federated learning poses, however, new challenges in ensuring fairness both in terms of collaborative training contributions from heterogeneous data and in mitigating bias in model predictions with respect to sensitive attributes. To address these challenges, a fair FL framework is proposed for collaborative network traffic prediction and resource allocation. To demonstrate the effectiveness of the proposed approach, noniid and imbalanced federated datasets based on real-word traffic traces are utilized for an elastic optical network. The assumption is that different optical nodes may be managed by different operators. Fairness is evaluated according to the coefficient of variations measure in terms of accuracy across the operators and in terms of quality-of-service across the connections (i.e., reflecting end-user experience). It is shown that fair traffic prediction across the operators result in fairer resource allocations across the connections.","sentences":["In the beyond 5G era, AI/ML empowered realworld digital twins (DTs) will enable diverse network operators to collaboratively optimize their networks, ultimately improving end-user experience.","Although centralized AI-based learning techniques have been shown to achieve significant network traffic accuracy, resulting in efficient network operations, they require sharing of sensitive data among operators, leading to privacy and security concerns.","Distributed learning, and specifically federated learning (FL), that keeps data isolated at local clients, has emerged as an effective and promising solution for mitigating such concerns.","Federated learning poses, however, new challenges in ensuring fairness both in terms of collaborative training contributions from heterogeneous data and in mitigating bias in model predictions with respect to sensitive attributes.","To address these challenges, a fair FL framework is proposed for collaborative network traffic prediction and resource allocation.","To demonstrate the effectiveness of the proposed approach, noniid and imbalanced federated datasets based on real-word traffic traces are utilized for an elastic optical network.","The assumption is that different optical nodes may be managed by different operators.","Fairness is evaluated according to the coefficient of variations measure in terms of accuracy across the operators and in terms of quality-of-service across the connections (i.e., reflecting end-user experience).","It is shown that fair traffic prediction across the operators result in fairer resource allocations across the connections."],"url":"http://arxiv.org/abs/2502.06743v1"}
{"created":"2025-02-10 18:09:45","title":"ViSIR: Vision Transformer Single Image Reconstruction Method for Earth System Models","abstract":"Purpose: Earth system models (ESMs) integrate the interactions of the atmosphere, ocean, land, ice, and biosphere to estimate the state of regional and global climate under a wide variety of conditions. The ESMs are highly complex, and thus, deep neural network architectures are used to model the complexity and store the down-sampled data. In this paper, we propose the Vision Transformer Sinusoidal Representation Networks (ViSIR) to improve the single image SR (SR) reconstruction task for the ESM data.   Methods: ViSIR combines the SR capability of Vision Transformers (ViT) with the high-frequency detail preservation of the Sinusoidal Representation Network (SIREN) to address the spectral bias observed in SR tasks.   Results: The ViSIR outperforms ViT by 4.1 dB, SIREN by 7.5 dB, and SR-Generative Adversarial (SR-GANs) by 7.1dB PSNR on average for three different measurements.   Conclusion: The proposed ViSIR is evaluated and compared with state-of-the-art methods. The results show that the proposed algorithm is outperforming other methods in terms of Mean Square Error(MSE), Peak-Signal-to-Noise-Ratio(PSNR), and Structural Similarity Index Measure(SSIM).","sentences":["Purpose: Earth system models (ESMs) integrate the interactions of the atmosphere, ocean, land, ice, and biosphere to estimate the state of regional and global climate under a wide variety of conditions.","The ESMs are highly complex, and thus, deep neural network architectures are used to model the complexity and store the down-sampled data.","In this paper, we propose the Vision Transformer Sinusoidal Representation Networks (ViSIR) to improve the single image SR (SR) reconstruction task for the ESM data.   ","Methods: ViSIR combines the SR capability of Vision Transformers (ViT) with the high-frequency detail preservation of the Sinusoidal Representation Network (SIREN) to address the spectral bias observed in SR tasks.   ","Results:","The ViSIR outperforms ViT by 4.1 dB, SIREN by 7.5 dB, and SR-Generative Adversarial (SR-GANs) by 7.1dB PSNR on average for three different measurements.   ","Conclusion: The proposed ViSIR is evaluated and compared with state-of-the-art methods.","The results show that the proposed algorithm is outperforming other methods in terms of Mean Square Error(MSE), Peak-Signal-to-Noise-Ratio(PSNR), and Structural Similarity Index Measure(SSIM)."],"url":"http://arxiv.org/abs/2502.06741v1"}
{"created":"2025-02-10 18:03:36","title":"VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data","abstract":"Process Reward Models (PRMs) have proven effective at enhancing mathematical reasoning for Large Language Models (LLMs) by leveraging increased inference-time computation. However, they are predominantly trained on mathematical data and their generalizability to non-mathematical domains has not been rigorously studied. In response, this work first shows that current PRMs have poor performance in other domains. To address this limitation, we introduce VersaPRM, a multi-domain PRM trained on synthetic reasoning data generated using our novel data generation and annotation method. VersaPRM achieves consistent performance gains across diverse domains. For instance, in the MMLU-Pro category of Law, VersaPRM via weighted majority voting, achieves a 7.9% performance gain over the majority voting baseline -- surpassing Qwen2.5-Math-PRM's gain of 1.3%. We further contribute to the community by open-sourcing all data, code and models for VersaPRM.","sentences":["Process Reward Models (PRMs) have proven effective at enhancing mathematical reasoning for Large Language Models (LLMs) by leveraging increased inference-time computation.","However, they are predominantly trained on mathematical data and their generalizability to non-mathematical domains has not been rigorously studied.","In response, this work first shows that current PRMs have poor performance in other domains.","To address this limitation, we introduce VersaPRM, a multi-domain PRM trained on synthetic reasoning data generated using our novel data generation and annotation method.","VersaPRM achieves consistent performance gains across diverse domains.","For instance, in the MMLU-Pro category of Law, VersaPRM via weighted majority voting, achieves a 7.9% performance gain over the majority voting baseline -- surpassing Qwen2.5-Math-PRM's gain of 1.3%.","We further contribute to the community by open-sourcing all data, code and models for VersaPRM."],"url":"http://arxiv.org/abs/2502.06737v1"}
{"created":"2025-02-10 17:57:15","title":"Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining","abstract":"Pretraining large language models (LLMs) on vast and heterogeneous datasets is crucial for achieving state-of-the-art performance across diverse downstream tasks. However, current training paradigms treat all samples equally, overlooking the importance or relevance of individual samples throughout the training process. Existing reweighting strategies, which primarily focus on group-level data importance, fail to leverage fine-grained instance-level information and do not adapt dynamically to individual sample importance as training progresses. In this paper, we introduce novel algorithms for dynamic, instance-level data reweighting aimed at improving both the efficiency and effectiveness of LLM pretraining. Our methods adjust the weight of each training sample based on its loss value in an online fashion, allowing the model to dynamically focus on more informative or important samples at the current training stage. In particular, our framework allows us to systematically devise reweighting strategies deprioritizing redundant or uninformative data, which we find tend to work best. Furthermore, we develop a new theoretical framework for analyzing the impact of loss-based reweighting on the convergence of gradient-based optimization, providing the first formal characterization of how these strategies affect convergence bounds. We empirically validate our approach across a spectrum of tasks, from pretraining 7B and 1.4B parameter LLMs to smaller-scale language models and linear regression problems, demonstrating that our loss-based reweighting approach can lead to faster convergence and significantly improved performance.","sentences":["Pretraining large language models (LLMs) on vast and heterogeneous datasets is crucial for achieving state-of-the-art performance across diverse downstream tasks.","However, current training paradigms treat all samples equally, overlooking the importance or relevance of individual samples throughout the training process.","Existing reweighting strategies, which primarily focus on group-level data importance, fail to leverage fine-grained instance-level information and do not adapt dynamically to individual sample importance as training progresses.","In this paper, we introduce novel algorithms for dynamic, instance-level data reweighting aimed at improving both the efficiency and effectiveness of LLM pretraining.","Our methods adjust the weight of each training sample based on its loss value in an online fashion, allowing the model to dynamically focus on more informative or important samples at the current training stage.","In particular, our framework allows us to systematically devise reweighting strategies deprioritizing redundant or uninformative data, which we find tend to work best.","Furthermore, we develop a new theoretical framework for analyzing the impact of loss-based reweighting on the convergence of gradient-based optimization, providing the first formal characterization of how these strategies affect convergence bounds.","We empirically validate our approach across a spectrum of tasks, from pretraining 7B and 1.4B parameter LLMs to smaller-scale language models and linear regression problems, demonstrating that our loss-based reweighting approach can lead to faster convergence and significantly improved performance."],"url":"http://arxiv.org/abs/2502.06733v1"}
{"created":"2025-02-10 17:56:30","title":"Engineering Insights into Biclique Partitions and Fractional Binary Ranks of Matrices","abstract":"We investigate structural properties of the binary rank of Kronecker powers of binary matrices, equivalently, the biclique partition numbers of the corresponding bipartite graphs. To this end, we engineer a Column Generation approach to solve linear optimization problems for the fractional biclique partition number of bipartite graphs, specifically examining the Domino graph and its Kronecker powers. We address the challenges posed by the double exponential growth of the number of bicliques in increasing Kronecker powers. We discuss various strategies to generate suitable initial sets of bicliques, including an inductive method for increasing Kronecker powers. We show how to manage the number of active bicliques to improve running time and to stay within memory limits. Our computational results reveal that the fractional binary rank is not multiplicative with respect to the Kronecker product. Hence, there are binary matrices, and bipartite graphs, respectively, such as the Domino, where the asymptotic fractional binary rank is strictly smaller than the fractional binary rank. While we used our algorithm to reduce the upper bound, we formally prove that the fractional biclique cover number is a lower bound, which is at least as good as the widely used isolating (or fooling set) bound. For the Domino, we obtain that the asymptotic fractional binary rank lies in the interval $[2,2.373]$. Since our computational resources are not sufficient to further reduce the upper bound, we encourage further exploration using more substantial computing resources or further mathematical engineering techniques to narrow the gap and advance our understanding of biclique partitions, particularly, to settle the open question whether binary rank and biclique partition number are multiplicative with respect to the Kronecker product.","sentences":["We investigate structural properties of the binary rank of Kronecker powers of binary matrices, equivalently, the biclique partition numbers of the corresponding bipartite graphs.","To this end, we engineer a Column Generation approach to solve linear optimization problems for the fractional biclique partition number of bipartite graphs, specifically examining the Domino graph and its Kronecker powers.","We address the challenges posed by the double exponential growth of the number of bicliques in increasing Kronecker powers.","We discuss various strategies to generate suitable initial sets of bicliques, including an inductive method for increasing Kronecker powers.","We show how to manage the number of active bicliques to improve running time and to stay within memory limits.","Our computational results reveal that the fractional binary rank is not multiplicative with respect to the Kronecker product.","Hence, there are binary matrices, and bipartite graphs, respectively, such as the Domino, where the asymptotic fractional binary rank is strictly smaller than the fractional binary rank.","While we used our algorithm to reduce the upper bound, we formally prove that the fractional biclique cover number is a lower bound, which is at least as good as the widely used isolating (or fooling set) bound.","For the Domino, we obtain that the asymptotic fractional binary rank lies in the interval $[2,2.373]$. Since our computational resources are not sufficient to further reduce the upper bound, we encourage further exploration using more substantial computing resources or further mathematical engineering techniques to narrow the gap and advance our understanding of biclique partitions, particularly, to settle the open question whether binary rank and biclique partition number are multiplicative with respect to the Kronecker product."],"url":"http://arxiv.org/abs/2502.06730v1"}
{"created":"2025-02-10 17:55:52","title":"Application of Artificial Intelligence (AI) in Civil Engineering","abstract":"Hard computing generally deals with precise data, which provides ideal solutions to problems. However, in the civil engineering field, amongst other disciplines, that is not always the case as real-world systems are continuously changing. Here lies the need to explore soft computing methods and artificial intelligence to solve civil engineering shortcomings. The integration of advanced computational models, including Artificial Neural Networks (ANNs), Fuzzy Logic, Genetic Algorithms (GAs), and Probabilistic Reasoning, has revolutionized the domain of civil engineering. These models have significantly advanced diverse sub-fields by offering innovative solutions and improved analysis capabilities. Sub-fields such as: slope stability analysis, bearing capacity, water quality and treatment, transportation systems, air quality, structural materials, etc. ANNs predict non-linearities and provide accurate estimates. Fuzzy logic uses an efficient decision-making process to provide a more precise assessment of systems. Lastly, while GAs optimizes models (based on evolutionary processes) for better outcomes, probabilistic reasoning lowers their statistical uncertainties.","sentences":["Hard computing generally deals with precise data, which provides ideal solutions to problems.","However, in the civil engineering field, amongst other disciplines, that is not always the case as real-world systems are continuously changing.","Here lies the need to explore soft computing methods and artificial intelligence to solve civil engineering shortcomings.","The integration of advanced computational models, including Artificial Neural Networks (ANNs), Fuzzy Logic, Genetic Algorithms (GAs), and Probabilistic Reasoning, has revolutionized the domain of civil engineering.","These models have significantly advanced diverse sub-fields by offering innovative solutions and improved analysis capabilities.","Sub-fields such as: slope stability analysis, bearing capacity, water quality and treatment, transportation systems, air quality, structural materials, etc. ANNs predict non-linearities and provide accurate estimates.","Fuzzy logic uses an efficient decision-making process to provide a more precise assessment of systems.","Lastly, while GAs optimizes models (based on evolutionary processes) for better outcomes, probabilistic reasoning lowers their statistical uncertainties."],"url":"http://arxiv.org/abs/2502.06727v1"}
{"created":"2025-02-10 17:41:57","title":"Learning Musical Representations for Music Performance Question Answering","abstract":"Music performances are representative scenarios for audio-visual modeling. Unlike common scenarios with sparse audio, music performances continuously involve dense audio signals throughout. While existing multimodal learning methods on the audio-video QA demonstrate impressive capabilities in general scenarios, they are incapable of dealing with fundamental problems within the music performances: they underexplore the interaction between the multimodal signals in performance and fail to consider the distinctive characteristics of instruments and music. Therefore, existing methods tend to answer questions regarding musical performances inaccurately. To bridge the above research gaps, (i) given the intricate multimodal interconnectivity inherent to music data, our primary backbone is designed to incorporate multimodal interactions within the context of music; (ii) to enable the model to learn music characteristics, we annotate and release rhythmic and music sources in the current music datasets; (iii) for time-aware audio-visual modeling, we align the model's music predictions with the temporal dimension. Our experiments show state-of-the-art effects on the Music AVQA datasets. Our code is available at https://github.com/xid32/Amuse.","sentences":["Music performances are representative scenarios for audio-visual modeling.","Unlike common scenarios with sparse audio, music performances continuously involve dense audio signals throughout.","While existing multimodal learning methods on the audio-video QA demonstrate impressive capabilities in general scenarios, they are incapable of dealing with fundamental problems within the music performances: they underexplore the interaction between the multimodal signals in performance and fail to consider the distinctive characteristics of instruments and music.","Therefore, existing methods tend to answer questions regarding musical performances inaccurately.","To bridge the above research gaps, (i) given the intricate multimodal interconnectivity inherent to music data, our primary backbone is designed to incorporate multimodal interactions within the context of music; (ii) to enable the model to learn music characteristics, we annotate and release rhythmic and music sources in the current music datasets; (iii) for time-aware audio-visual modeling, we align the model's music predictions with the temporal dimension.","Our experiments show state-of-the-art effects on the Music AVQA datasets.","Our code is available at https://github.com/xid32/Amuse."],"url":"http://arxiv.org/abs/2502.06710v1"}
{"created":"2025-02-10 17:37:34","title":"TEMSET-24K: Densely Annotated Dataset for Indexing Multipart Endoscopic Videos using Surgical Timeline Segmentation","abstract":"Indexing endoscopic surgical videos is vital in surgical data science, forming the basis for systematic retrospective analysis and clinical performance evaluation. Despite its significance, current video analytics rely on manual indexing, a time-consuming process. Advances in computer vision, particularly deep learning, offer automation potential, yet progress is limited by the lack of publicly available, densely annotated surgical datasets. To address this, we present TEMSET-24K, an open-source dataset comprising 24,306 trans-anal endoscopic microsurgery (TEMS) video micro-clips. Each clip is meticulously annotated by clinical experts using a novel hierarchical labeling taxonomy encompassing phase, task, and action triplets, capturing intricate surgical workflows. To validate this dataset, we benchmarked deep learning models, including transformer-based architectures. Our in silico evaluation demonstrates high accuracy (up to 0.99) and F1 scores (up to 0.99) for key phases like Setup and Suturing. The STALNet model, tested with ConvNeXt, ViT, and SWIN V2 encoders, consistently segmented well-represented phases. TEMSET-24K provides a critical benchmark, propelling state-of-the-art solutions in surgical data science.","sentences":["Indexing endoscopic surgical videos is vital in surgical data science, forming the basis for systematic retrospective analysis and clinical performance evaluation.","Despite its significance, current video analytics rely on manual indexing, a time-consuming process.","Advances in computer vision, particularly deep learning, offer automation potential, yet progress is limited by the lack of publicly available, densely annotated surgical datasets.","To address this, we present TEMSET-24K, an open-source dataset comprising 24,306 trans-anal endoscopic microsurgery (TEMS) video micro-clips.","Each clip is meticulously annotated by clinical experts using a novel hierarchical labeling taxonomy encompassing phase, task, and action triplets, capturing intricate surgical workflows.","To validate this dataset, we benchmarked deep learning models, including transformer-based architectures.","Our in silico evaluation demonstrates high accuracy (up to 0.99) and F1 scores (up to 0.99) for key phases like Setup and Suturing.","The STALNet model, tested with ConvNeXt, ViT, and SWIN V2 encoders, consistently segmented well-represented phases.","TEMSET-24K provides a critical benchmark, propelling state-of-the-art solutions in surgical data science."],"url":"http://arxiv.org/abs/2502.06708v1"}
{"created":"2025-02-10 17:37:26","title":"FinMamba: Market-Aware Graph Enhanced Multi-Level Mamba for Stock Movement Prediction","abstract":"Recently, combining stock features with inter-stock correlations has become a common and effective approach for stock movement prediction. However, financial data presents significant challenges due to its low signal-to-noise ratio and the dynamic complexity of the market, which give rise to two key limitations in existing methods. First, the relationships between stocks are highly influenced by multifaceted factors including macroeconomic market dynamics, and current models fail to adaptively capture these evolving interactions under specific market conditions. Second, for the accuracy and timeliness required by real-world trading, existing financial data mining methods struggle to extract beneficial pattern-oriented dependencies from long historical data while maintaining high efficiency and low memory consumption. To address the limitations, we propose FinMamba, a Mamba-GNN-based framework for market-aware and multi-level hybrid stock movement prediction. Specifically, we devise a dynamic graph to learn the changing representations of inter-stock relationships by integrating a pruning module that adapts to market trends. Afterward, with a selective mechanism, the multi-level Mamba discards irrelevant information and resets states to skillfully recall historical patterns across multiple time scales with linear time costs, which are then jointly optimized for reliable prediction. Extensive experiments on U.S. and Chinese stock markets demonstrate the effectiveness of our proposed FinMamba, achieving state-of-the-art prediction accuracy and trading profitability, while maintaining low computational complexity. The code is available at https://github.com/TROUBADOUR000/FinMamba.","sentences":["Recently, combining stock features with inter-stock correlations has become a common and effective approach for stock movement prediction.","However, financial data presents significant challenges due to its low signal-to-noise ratio and the dynamic complexity of the market, which give rise to two key limitations in existing methods.","First, the relationships between stocks are highly influenced by multifaceted factors including macroeconomic market dynamics, and current models fail to adaptively capture these evolving interactions under specific market conditions.","Second, for the accuracy and timeliness required by real-world trading, existing financial data mining methods struggle to extract beneficial pattern-oriented dependencies from long historical data while maintaining high efficiency and low memory consumption.","To address the limitations, we propose FinMamba, a Mamba-GNN-based framework for market-aware and multi-level hybrid stock movement prediction.","Specifically, we devise a dynamic graph to learn the changing representations of inter-stock relationships by integrating a pruning module that adapts to market trends.","Afterward, with a selective mechanism, the multi-level Mamba discards irrelevant information and resets states to skillfully recall historical patterns across multiple time scales with linear time costs, which are then jointly optimized for reliable prediction.","Extensive experiments on U.S. and Chinese stock markets demonstrate the effectiveness of our proposed FinMamba, achieving state-of-the-art prediction accuracy and trading profitability, while maintaining low computational complexity.","The code is available at https://github.com/TROUBADOUR000/FinMamba."],"url":"http://arxiv.org/abs/2502.06707v1"}
{"created":"2025-02-10 17:36:46","title":"A Case Study in Gamification for a Cybersecurity Education Program: A Game for Cryptography","abstract":"Advances in technology, a growing pool of sensitive data, and heightened global tensions has increased the demand for skilled cybersecurity professionals. Despite the recent increase in attention given to cybersecurity education, traditional approaches have continue in failing to keep pace with the rapidly evolving cyber threat landscape. Challenges such as a shortage of qualified educators and resource-intensive practical training exacerbate these issues.   Gamification offers an innovative approach to provide practical hands-on experiences, and equip educators with up-to-date and accessible teaching tools that are targeted to industry-specific concepts. The paper begins with a review of the literature on existing challenges in cybersecurity education and gamification methods already employed in the field, before presenting a real-world case study of a gamified cryptography teaching tool. The paper discusses the design, development process, and intended use cases for this tool. This research highlights and provides an example of how integrating gamification into curricula can address key educational gaps, ensuring a more robust and effective pipeline of cybersecurity talent for the future.","sentences":["Advances in technology, a growing pool of sensitive data, and heightened global tensions has increased the demand for skilled cybersecurity professionals.","Despite the recent increase in attention given to cybersecurity education, traditional approaches have continue in failing to keep pace with the rapidly evolving cyber threat landscape.","Challenges such as a shortage of qualified educators and resource-intensive practical training exacerbate these issues.   ","Gamification offers an innovative approach to provide practical hands-on experiences, and equip educators with up-to-date and accessible teaching tools that are targeted to industry-specific concepts.","The paper begins with a review of the literature on existing challenges in cybersecurity education and gamification methods already employed in the field, before presenting a real-world case study of a gamified cryptography teaching tool.","The paper discusses the design, development process, and intended use cases for this tool.","This research highlights and provides an example of how integrating gamification into curricula can address key educational gaps, ensuring a more robust and effective pipeline of cybersecurity talent for the future."],"url":"http://arxiv.org/abs/2502.06706v1"}
{"created":"2025-02-10 17:28:09","title":"Performance Analysis of Pinching-Antenna Systems","abstract":"The sixth generation of wireless networks envisions intelligent and adaptive environments capable of meeting the demands of emerging applications such as immersive extended reality, advanced healthcare, and the metaverse. However, this vision requires overcoming critical challenges, including the limitations of conventional wireless technologies in mitigating path loss and dynamically adapting to diverse user needs. Among the proposed reconfigurable technologies, pinching antenna systems (PASs) offer a novel way to turn path loss into a programmable parameter by using dielectric waveguides to minimize propagation losses at high frequencies. In this paper, we develop a comprehensive analytical framework that derives closed-form expressions for the outage probability and average rate of PASs while incorporating both free-space path loss and waveguide attenuation under realistic conditions. In addition, we characterize the optimal placement of pinching antennas to maximize performance under waveguide losses. Numerical results show the significant impact of waveguide losses on system performance, especially for longer waveguides, emphasizing the importance of accurate loss modeling. Despite these challenges, PASs consistently outperform conventional systems in terms of reliability and data rate, underscoring their potential to enable high-performance programmable wireless environments.","sentences":["The sixth generation of wireless networks envisions intelligent and adaptive environments capable of meeting the demands of emerging applications such as immersive extended reality, advanced healthcare, and the metaverse.","However, this vision requires overcoming critical challenges, including the limitations of conventional wireless technologies in mitigating path loss and dynamically adapting to diverse user needs.","Among the proposed reconfigurable technologies, pinching antenna systems (PASs) offer a novel way to turn path loss into a programmable parameter by using dielectric waveguides to minimize propagation losses at high frequencies.","In this paper, we develop a comprehensive analytical framework that derives closed-form expressions for the outage probability and average rate of PASs while incorporating both free-space path loss and waveguide attenuation under realistic conditions.","In addition, we characterize the optimal placement of pinching antennas to maximize performance under waveguide losses.","Numerical results show the significant impact of waveguide losses on system performance, especially for longer waveguides, emphasizing the importance of accurate loss modeling.","Despite these challenges, PASs consistently outperform conventional systems in terms of reliability and data rate, underscoring their potential to enable high-performance programmable wireless environments."],"url":"http://arxiv.org/abs/2502.06701v1"}
{"created":"2025-02-10 17:18:54","title":"FairDropout: Using Example-Tied Dropout to Enhance Generalization of Minority Groups","abstract":"Deep learning models frequently exploit spurious features in training data to achieve low training error, often resulting in poor generalization when faced with shifted testing distributions. To address this issue, various methods from imbalanced learning, representation learning, and classifier recalibration have been proposed to enhance the robustness of deep neural networks against spurious correlations. In this paper, we observe that models trained with empirical risk minimization tend to generalize well for examples from the majority groups while memorizing instances from minority groups. Building on recent findings that show memorization can be localized to a limited number of neurons, we apply example-tied dropout as a method we term FairDropout, aimed at redirecting this memorization to specific neurons that we subsequently drop out during inference. We empirically evaluate FairDropout using the subpopulation benchmark suite encompassing vision, language, and healthcare tasks, demonstrating that it significantly reduces reliance on spurious correlations, and outperforms state-of-the-art methods.","sentences":["Deep learning models frequently exploit spurious features in training data to achieve low training error, often resulting in poor generalization when faced with shifted testing distributions.","To address this issue, various methods from imbalanced learning, representation learning, and classifier recalibration have been proposed to enhance the robustness of deep neural networks against spurious correlations.","In this paper, we observe that models trained with empirical risk minimization tend to generalize well for examples from the majority groups while memorizing instances from minority groups.","Building on recent findings that show memorization can be localized to a limited number of neurons, we apply example-tied dropout as a method we term FairDropout, aimed at redirecting this memorization to specific neurons that we subsequently drop out during inference.","We empirically evaluate FairDropout using the subpopulation benchmark suite encompassing vision, language, and healthcare tasks, demonstrating that it significantly reduces reliance on spurious correlations, and outperforms state-of-the-art methods."],"url":"http://arxiv.org/abs/2502.06695v1"}
{"created":"2025-02-10 17:14:37","title":"Network Intrusion Datasets: A Survey, Limitations, and Recommendations","abstract":"Data-driven cyberthreat detection has become a crucial defense technique in modern cybersecurity. Network defense, supported by Network Intrusion Detection Systems (NIDSs), has also increasingly adopted data-driven approaches, leading to greater reliance on data. Despite its importance, data scarcity has long been recognized as a major obstacle in NIDS research. In response, the community has published many new datasets recently. However, many of them remain largely unknown and unanalyzed, leaving researchers uncertain about their suitability for specific use cases.   In this paper, we aim to address this knowledge gap by performing a systematic literature review (SLR) of 89 public datasets for NIDS research. Each dataset is comparatively analyzed across 13 key properties, and its potential applications are outlined. Beyond the review, we also discuss domain-specific challenges and common data limitations to facilitate a critical view on data quality. To aid in data selection, we conduct a dataset popularity analysis in contemporary state-of-the-art NIDS research. Furthermore, the paper presents best practices for dataset selection, generation, and usage. By providing a comprehensive overview of the domain and its data, this work aims to guide future research toward improving data quality and the robustness of NIDS solutions.","sentences":["Data-driven cyberthreat detection has become a crucial defense technique in modern cybersecurity.","Network defense, supported by Network Intrusion Detection Systems (NIDSs), has also increasingly adopted data-driven approaches, leading to greater reliance on data.","Despite its importance, data scarcity has long been recognized as a major obstacle in NIDS research.","In response, the community has published many new datasets recently.","However, many of them remain largely unknown and unanalyzed, leaving researchers uncertain about their suitability for specific use cases.   ","In this paper, we aim to address this knowledge gap by performing a systematic literature review (SLR) of 89 public datasets for NIDS research.","Each dataset is comparatively analyzed across 13 key properties, and its potential applications are outlined.","Beyond the review, we also discuss domain-specific challenges and common data limitations to facilitate a critical view on data quality.","To aid in data selection, we conduct a dataset popularity analysis in contemporary state-of-the-art NIDS research.","Furthermore, the paper presents best practices for dataset selection, generation, and usage.","By providing a comprehensive overview of the domain and its data, this work aims to guide future research toward improving data quality and the robustness of NIDS solutions."],"url":"http://arxiv.org/abs/2502.06688v1"}
{"created":"2025-02-10 17:13:11","title":"No Trick, No Treat: Pursuits and Challenges Towards Simulation-free Training of Neural Samplers","abstract":"We consider the sampling problem, where the aim is to draw samples from a distribution whose density is known only up to a normalization constant. Recent breakthroughs in generative modeling to approximate a high-dimensional data distribution have sparked significant interest in developing neural network-based methods for this challenging problem. However, neural samplers typically incur heavy computational overhead due to simulating trajectories during training. This motivates the pursuit of simulation-free training procedures of neural samplers. In this work, we propose an elegant modification to previous methods, which allows simulation-free training with the help of a time-dependent normalizing flow. However, it ultimately suffers from severe mode collapse. On closer inspection, we find that nearly all successful neural samplers rely on Langevin preconditioning to avoid mode collapsing. We systematically analyze several popular methods with various objective functions and demonstrate that, in the absence of Langevin preconditioning, most of them fail to adequately cover even a simple target. Finally, we draw attention to a strong baseline by combining the state-of-the-art MCMC method, Parallel Tempering (PT), with an additional generative model to shed light on future explorations of neural samplers.","sentences":["We consider the sampling problem, where the aim is to draw samples from a distribution whose density is known only up to a normalization constant.","Recent breakthroughs in generative modeling to approximate a high-dimensional data distribution have sparked significant interest in developing neural network-based methods for this challenging problem.","However, neural samplers typically incur heavy computational overhead due to simulating trajectories during training.","This motivates the pursuit of simulation-free training procedures of neural samplers.","In this work, we propose an elegant modification to previous methods, which allows simulation-free training with the help of a time-dependent normalizing flow.","However, it ultimately suffers from severe mode collapse.","On closer inspection, we find that nearly all successful neural samplers rely on Langevin preconditioning to avoid mode collapsing.","We systematically analyze several popular methods with various objective functions and demonstrate that, in the absence of Langevin preconditioning, most of them fail to adequately cover even a simple target.","Finally, we draw attention to a strong baseline by combining the state-of-the-art MCMC method, Parallel Tempering (PT), with an additional generative model to shed light on future explorations of neural samplers."],"url":"http://arxiv.org/abs/2502.06685v1"}
{"created":"2025-02-10 17:11:20","title":"EquiTabPFN: A Target-Permutation Equivariant Prior Fitted Networks","abstract":"Recent foundational models for tabular data, such as TabPFN, have demonstrated remarkable effectiveness in adapting to new tasks through in-context learning. However, these models overlook a crucial equivariance property: the arbitrary ordering of target dimensions should not influence model predictions. In this study, we identify this oversight as a source of incompressible error, termed the equivariance gap, which introduces instability in predictions. To mitigate these issues, we propose a novel model designed to preserve equivariance across output dimensions. Our experimental results indicate that our proposed model not only addresses these pitfalls effectively but also achieves competitive benchmark performance.","sentences":["Recent foundational models for tabular data, such as TabPFN, have demonstrated remarkable effectiveness in adapting to new tasks through in-context learning.","However, these models overlook a crucial equivariance property: the arbitrary ordering of target dimensions should not influence model predictions.","In this study, we identify this oversight as a source of incompressible error, termed the equivariance gap, which introduces instability in predictions.","To mitigate these issues, we propose a novel model designed to preserve equivariance across output dimensions.","Our experimental results indicate that our proposed model not only addresses these pitfalls effectively but also achieves competitive benchmark performance."],"url":"http://arxiv.org/abs/2502.06684v1"}
{"created":"2025-02-10 17:07:53","title":"Transfer Your Perspective: Controllable 3D Generation from Any Viewpoint in a Driving Scene","abstract":"Self-driving cars relying solely on ego-centric perception face limitations in sensing, often failing to detect occluded, faraway objects. Collaborative autonomous driving (CAV) seems like a promising direction, but collecting data for development is non-trivial. It requires placing multiple sensor-equipped agents in a real-world driving scene, simultaneously! As such, existing datasets are limited in locations and agents. We introduce a novel surrogate to the rescue, which is to generate realistic perception from different viewpoints in a driving scene, conditioned on a real-world sample - the ego-car's sensory data. This surrogate has huge potential: it could potentially turn any ego-car dataset into a collaborative driving one to scale up the development of CAV. We present the very first solution, using a combination of simulated collaborative data and real ego-car data. Our method, Transfer Your Perspective (TYP), learns a conditioned diffusion model whose output samples are not only realistic but also consistent in both semantics and layouts with the given ego-car data. Empirical results demonstrate TYP's effectiveness in aiding in a CAV setting. In particular, TYP enables us to (pre-)train collaborative perception algorithms like early and late fusion with little or no real-world collaborative data, greatly facilitating downstream CAV applications.","sentences":["Self-driving cars relying solely on ego-centric perception face limitations in sensing, often failing to detect occluded, faraway objects.","Collaborative autonomous driving (CAV) seems like a promising direction, but collecting data for development is non-trivial.","It requires placing multiple sensor-equipped agents in a real-world driving scene, simultaneously!","As such, existing datasets are limited in locations and agents.","We introduce a novel surrogate to the rescue, which is to generate realistic perception from different viewpoints in a driving scene, conditioned on a real-world sample - the ego-car's sensory data.","This surrogate has huge potential: it could potentially turn any ego-car dataset into a collaborative driving one to scale up the development of CAV.","We present the very first solution, using a combination of simulated collaborative data and real ego-car data.","Our method, Transfer Your Perspective (TYP), learns a conditioned diffusion model whose output samples are not only realistic but also consistent in both semantics and layouts with the given ego-car data.","Empirical results demonstrate TYP's effectiveness in aiding in a CAV setting.","In particular, TYP enables us to (pre-)train collaborative perception algorithms like early and late fusion with little or no real-world collaborative data, greatly facilitating downstream CAV applications."],"url":"http://arxiv.org/abs/2502.06682v1"}
{"created":"2025-02-10 16:51:51","title":"On the Limitations of Combining Sentiment Analysis Tools in a Cross-Platform Setting","abstract":"A positive working climate is essential in modern software development. It enhances productivity since a satisfied developer tends to deliver better results. Sentiment analysis tools are a means to analyze and classify textual communication between developers according to the polarity of the statements. Most of these tools deliver promising results when used with test data from the domain they are developed for (e.g., GitHub). But the tools' outcomes lack reliability when used in a different domain (e.g., Stack Overflow). One possible way to mitigate this problem is to combine different tools trained in different domains. In this paper, we analyze a combination of three sentiment analysis tools in a voting classifier according to their reliability and performance. The tools are trained and evaluated using five already existing polarity data sets (e.g. from GitHub). The results indicate that this kind of combination of tools is a good choice in the within-platform setting. However, a majority vote does not necessarily lead to better results when applying in cross-platform domains. In most cases, the best individual tool in the ensemble is preferable. This is mainly due to the often large difference in performance of the individual tools, even on the same data set. However, this may also be due to the different annotated data sets.","sentences":["A positive working climate is essential in modern software development.","It enhances productivity since a satisfied developer tends to deliver better results.","Sentiment analysis tools are a means to analyze and classify textual communication between developers according to the polarity of the statements.","Most of these tools deliver promising results when used with test data from the domain they are developed for (e.g., GitHub).","But the tools' outcomes lack reliability when used in a different domain (e.g., Stack Overflow).","One possible way to mitigate this problem is to combine different tools trained in different domains.","In this paper, we analyze a combination of three sentiment analysis tools in a voting classifier according to their reliability and performance.","The tools are trained and evaluated using five already existing polarity data sets (e.g. from GitHub).","The results indicate that this kind of combination of tools is a good choice in the within-platform setting.","However, a majority vote does not necessarily lead to better results when applying in cross-platform domains.","In most cases, the best individual tool in the ensemble is preferable.","This is mainly due to the often large difference in performance of the individual tools, even on the same data set.","However, this may also be due to the different annotated data sets."],"url":"http://arxiv.org/abs/2502.06665v1"}
{"created":"2025-02-10 16:48:48","title":"Generating Samples to Question Trained Models","abstract":"There is a growing need for investigating how machine learning models operate. With this work, we aim to understand trained machine learning models by questioning their data preferences. We propose a mathematical framework that allows us to probe trained models and identify their preferred samples in various scenarios including prediction-risky, parameter-sensitive, or model-contrastive samples. To showcase our framework, we pose these queries to a range of models trained on a range of classification and regression tasks, and receive answers in the form of generated data.","sentences":["There is a growing need for investigating how machine learning models operate.","With this work, we aim to understand trained machine learning models by questioning their data preferences.","We propose a mathematical framework that allows us to probe trained models and identify their preferred samples in various scenarios including prediction-risky, parameter-sensitive, or model-contrastive samples.","To showcase our framework, we pose these queries to a range of models trained on a range of classification and regression tasks, and receive answers in the form of generated data."],"url":"http://arxiv.org/abs/2502.06658v1"}
{"created":"2025-02-10 16:47:42","title":"Onion Routing Key Distribution for QKDN","abstract":"The advance of quantum computing poses a significant threat to classical cryptography, compromising the security of current encryption schemes such as RSA and ECC. In response to this challenge, two main approaches have emerged: quantum cryptography and post-quantum cryptography (PQC). However, both have implementation and security limitations. In this paper, we propose a secure key distribution protocol for Quantum Key Distribution Networks (QKDN), which incorporates encapsulation techniques in the key-relay model for QKDN inspired by onion routing and combined with PQC to guarantee confidentiality, integrity, authenticity and anonymity in communication. The proposed protocol optimizes security by using post-quantum public key encryption to protect the shared secrets from intermediate nodes in the QKDN, thereby reducing the risk of attacks by malicious intermediaries. Finally, relevant use cases are presented, such as critical infrastructure networks, interconnection of data centers and digital money, demonstrating the applicability of the proposal in critical high-security environments.","sentences":["The advance of quantum computing poses a significant threat to classical cryptography, compromising the security of current encryption schemes such as RSA and ECC.","In response to this challenge, two main approaches have emerged: quantum cryptography and post-quantum cryptography (PQC).","However, both have implementation and security limitations.","In this paper, we propose a secure key distribution protocol for Quantum Key Distribution Networks (QKDN), which incorporates encapsulation techniques in the key-relay model for QKDN inspired by onion routing and combined with PQC to guarantee confidentiality, integrity, authenticity and anonymity in communication.","The proposed protocol optimizes security by using post-quantum public key encryption to protect the shared secrets from intermediate nodes in the QKDN, thereby reducing the risk of attacks by malicious intermediaries.","Finally, relevant use cases are presented, such as critical infrastructure networks, interconnection of data centers and digital money, demonstrating the applicability of the proposal in critical high-security environments."],"url":"http://arxiv.org/abs/2502.06657v1"}
{"created":"2025-02-10 16:43:32","title":"In-Context Learning (and Unlearning) of Length Biases","abstract":"Large language models have demonstrated strong capabilities to learn in-context, where exemplar input-output pairings are appended to the prompt for demonstration. However, existing work has demonstrated the ability of models to learn lexical and label biases in-context, which negatively impacts both performance and robustness of models. The impact of other statistical data biases remains under-explored, which this work aims to address. We specifically investigate the impact of length biases on in-context learning. We demonstrate that models do learn length biases in the context window for their predictions, and further empirically analyze the factors that modulate the level of bias exhibited by the model. In addition, we show that learning length information in-context can be used to counter the length bias that has been encoded in models (e.g., via fine-tuning). This reveals the power of in-context learning in debiasing model prediction behaviors without the need for costly parameter updates.","sentences":["Large language models have demonstrated strong capabilities to learn in-context, where exemplar input-output pairings are appended to the prompt for demonstration.","However, existing work has demonstrated the ability of models to learn lexical and label biases in-context, which negatively impacts both performance and robustness of models.","The impact of other statistical data biases remains under-explored, which this work aims to address.","We specifically investigate the impact of length biases on in-context learning.","We demonstrate that models do learn length biases in the context window for their predictions, and further empirically analyze the factors that modulate the level of bias exhibited by the model.","In addition, we show that learning length information in-context can be used to counter the length bias that has been encoded in models (e.g., via fine-tuning).","This reveals the power of in-context learning in debiasing model prediction behaviors without the need for costly parameter updates."],"url":"http://arxiv.org/abs/2502.06653v1"}
{"created":"2025-02-10 16:42:00","title":"Transparent NLP: Using RAG and LLM Alignment for Privacy Q&A","abstract":"The transparency principle of the General Data Protection Regulation (GDPR) requires data processing information to be clear, precise, and accessible. While language models show promise in this context, their probabilistic nature complicates truthfulness and comprehensibility.   This paper examines state-of-the-art Retrieval Augmented Generation (RAG) systems enhanced with alignment techniques to fulfill GDPR obligations. We evaluate RAG systems incorporating an alignment module like Rewindable Auto-regressive Inference (RAIN) and our proposed multidimensional extension, MultiRAIN, using a Privacy Q&A dataset. Responses are optimized for preciseness and comprehensibility and are assessed through 21 metrics, including deterministic and large language model-based evaluations.   Our results show that RAG systems with an alignment module outperform baseline RAG systems on most metrics, though none fully match human answers. Principal component analysis of the results reveals complex interactions between metrics, highlighting the need to refine metrics. This study provides a foundation for integrating advanced natural language processing systems into legal compliance frameworks.","sentences":["The transparency principle of the General Data Protection Regulation (GDPR) requires data processing information to be clear, precise, and accessible.","While language models show promise in this context, their probabilistic nature complicates truthfulness and comprehensibility.   ","This paper examines state-of-the-art Retrieval Augmented Generation (RAG) systems enhanced with alignment techniques to fulfill GDPR obligations.","We evaluate RAG systems incorporating an alignment module like Rewindable Auto-regressive Inference (RAIN) and our proposed multidimensional extension, MultiRAIN, using a Privacy Q&A dataset.","Responses are optimized for preciseness and comprehensibility and are assessed through 21 metrics, including deterministic and large language model-based evaluations.   ","Our results show that RAG systems with an alignment module outperform baseline RAG systems on most metrics, though none fully match human answers.","Principal component analysis of the results reveals complex interactions between metrics, highlighting the need to refine metrics.","This study provides a foundation for integrating advanced natural language processing systems into legal compliance frameworks."],"url":"http://arxiv.org/abs/2502.06652v1"}
{"created":"2025-02-10 16:41:49","title":"Differentially Private Empirical Cumulative Distribution Functions","abstract":"In order to both learn and protect sensitive training data, there has been a growing interest in privacy preserving machine learning methods. Differential privacy has emerged as an important measure of privacy. We are interested in the federated setting where a group of parties each have one or more training instances and want to learn collaboratively without revealing their data.   In this paper, we propose strategies to compute differentially private empirical distribution functions. While revealing complete functions is more expensive from the point of view of privacy budget, it may also provide richer and more valuable information to the learner. We prove privacy guarantees and discuss the computational cost, both for a generic strategy fitting any security model and a special-purpose strategy based on secret sharing. We survey a number of applications and present experiments.","sentences":["In order to both learn and protect sensitive training data, there has been a growing interest in privacy preserving machine learning methods.","Differential privacy has emerged as an important measure of privacy.","We are interested in the federated setting where a group of parties each have one or more training instances and want to learn collaboratively without revealing their data.   ","In this paper, we propose strategies to compute differentially private empirical distribution functions.","While revealing complete functions is more expensive from the point of view of privacy budget, it may also provide richer and more valuable information to the learner.","We prove privacy guarantees and discuss the computational cost, both for a generic strategy fitting any security model and a special-purpose strategy based on secret sharing.","We survey a number of applications and present experiments."],"url":"http://arxiv.org/abs/2502.06651v1"}
{"created":"2025-02-10 16:40:26","title":"Prototype Contrastive Consistency Learning for Semi-Supervised Medical Image Segmentation","abstract":"Medical image segmentation is a crucial task in medical image analysis, but it can be very challenging especially when there are less labeled data but with large unlabeled data. Contrastive learning has proven to be effective for medical image segmentation in semi-supervised learning by constructing contrastive samples from partial pixels. However, although previous contrastive learning methods can mine semantic information from partial pixels within images, they ignore the whole context information of unlabeled images, which is very important to precise segmentation. In order to solve this problem, we propose a novel prototype contrastive learning method called Prototype Contrastive Consistency Segmentation (PCCS) for semi-supervised medical image segmentation. The core idea is to enforce the prototypes of the same semantic class to be closer and push the prototypes in different semantic classes far away from each other. Specifically, we construct a signed distance map and an uncertainty map from unlabeled images. The signed distance map is used to construct prototypes for contrastive learning, and then we estimate the prototype uncertainty from the uncertainty map as trade-off among prototypes. In order to obtain better prototypes, based on the student-teacher architecture, a new mechanism named prototype updating prototype is designed to assist in updating the prototypes for contrastive learning. In addition, we propose an uncertainty-consistency loss to mine more reliable information from unlabeled data. Extensive experiments on medical image segmentation demonstrate that PCCS achieves better segmentation performance than the state-of-the-art methods. The code is available at https://github.com/comphsh/PCCS.","sentences":["Medical image segmentation is a crucial task in medical image analysis, but it can be very challenging especially when there are less labeled data but with large unlabeled data.","Contrastive learning has proven to be effective for medical image segmentation in semi-supervised learning by constructing contrastive samples from partial pixels.","However, although previous contrastive learning methods can mine semantic information from partial pixels within images, they ignore the whole context information of unlabeled images, which is very important to precise segmentation.","In order to solve this problem, we propose a novel prototype contrastive learning method called Prototype Contrastive Consistency Segmentation (PCCS) for semi-supervised medical image segmentation.","The core idea is to enforce the prototypes of the same semantic class to be closer and push the prototypes in different semantic classes far away from each other.","Specifically, we construct a signed distance map and an uncertainty map from unlabeled images.","The signed distance map is used to construct prototypes for contrastive learning, and then we estimate the prototype uncertainty from the uncertainty map as trade-off among prototypes.","In order to obtain better prototypes, based on the student-teacher architecture, a new mechanism named prototype updating prototype is designed to assist in updating the prototypes for contrastive learning.","In addition, we propose an uncertainty-consistency loss to mine more reliable information from unlabeled data.","Extensive experiments on medical image segmentation demonstrate that PCCS achieves better segmentation performance than the state-of-the-art methods.","The code is available at https://github.com/comphsh/PCCS."],"url":"http://arxiv.org/abs/2502.06650v1"}
{"created":"2025-02-10 16:34:36","title":"MoETuner: Optimized Mixture of Expert Serving with Balanced Expert Placement and Token Routing","abstract":"Mixture-of-Experts (MoE) model architecture has emerged as a promising solution for scaling transformer models efficiently, offering sparse activation that reduces computational costs while increasing model capacity. However, as MoE models scale, they need to be distributed across GPU devices, thus face critical performance bottlenecks due to their large memory footprint. Expert parallelism distributes experts across GPUs, however, faces key challenges including an unbalanced token routing and expert activation, resulting in communication tail latency and processing inefficiencies. While existing solutions address some of these issues, they fail to resolve the dual challenges of load imbalance and communication skew. The imbalance in token processing load across experts causes uneven processing times on different GPUs, while communication skew between GPUs leads to unbalanced inter-GPU data transfers. These factors degrade the performance of MoE models by increasing tail latency and reducing overall throughput. To address these limitations, we propose an Integer Linear Programming (ILP) formulation to optimize expert placement by jointly considering token load, communication, and computation costs. We exploit the property that there is a token routing dependency across layers, where tokens routed to a specific expert in one layer are likely to be routed to a limited set of experts in the subsequent layer. Our solution, MoETuner, offers an optimal expert-to-GPU assignment that minimizes inter-GPU token routing costs and balances token processing across devices, thereby reducing tail latency and end-to-end execution time. Experimental results demonstrate 9.3% and 17.5% of end-to-end speedups for single-node and multi-node inference respectively, showcasing the potential of our ILP-based optimization for offering expert parallel solutions for next-generation MoEs.","sentences":["Mixture-of-Experts (MoE) model architecture has emerged as a promising solution for scaling transformer models efficiently, offering sparse activation that reduces computational costs while increasing model capacity.","However, as MoE models scale, they need to be distributed across GPU devices, thus face critical performance bottlenecks due to their large memory footprint.","Expert parallelism distributes experts across GPUs, however, faces key challenges including an unbalanced token routing and expert activation, resulting in communication tail latency and processing inefficiencies.","While existing solutions address some of these issues, they fail to resolve the dual challenges of load imbalance and communication skew.","The imbalance in token processing load across experts causes uneven processing times on different GPUs, while communication skew between GPUs leads to unbalanced inter-GPU data transfers.","These factors degrade the performance of MoE models by increasing tail latency and reducing overall throughput.","To address these limitations, we propose an Integer Linear Programming (ILP) formulation to optimize expert placement by jointly considering token load, communication, and computation costs.","We exploit the property that there is a token routing dependency across layers, where tokens routed to a specific expert in one layer are likely to be routed to a limited set of experts in the subsequent layer.","Our solution, MoETuner, offers an optimal expert-to-GPU assignment that minimizes inter-GPU token routing costs and balances token processing across devices, thereby reducing tail latency and end-to-end execution time.","Experimental results demonstrate 9.3% and 17.5% of end-to-end speedups for single-node and multi-node inference respectively, showcasing the potential of our ILP-based optimization for offering expert parallel solutions for next-generation MoEs."],"url":"http://arxiv.org/abs/2502.06643v1"}
{"created":"2025-02-10 16:31:37","title":"Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM","abstract":"Steel-LLM is a Chinese-centric language model developed from scratch with the goal of creating a high-quality, open-source model despite limited computational resources. Launched in March 2024, the project aimed to train a 1-billion-parameter model on a large-scale dataset, prioritizing transparency and the sharing of practical insights to assist others in the community. The training process primarily focused on Chinese data, with a small proportion of English data included, addressing gaps in existing open-source LLMs by providing a more detailed and practical account of the model-building journey. Steel-LLM has demonstrated competitive performance on benchmarks such as CEVAL and CMMLU, outperforming early models from larger institutions. This paper provides a comprehensive summary of the project's key contributions, including data collection, model design, training methodologies, and the challenges encountered along the way, offering a valuable resource for researchers and practitioners looking to develop their own LLMs. The model checkpoints and training script are available at https://github.com/zhanshijinwat/Steel-LLM.","sentences":["Steel-LLM is a Chinese-centric language model developed from scratch with the goal of creating a high-quality, open-source model despite limited computational resources.","Launched in March 2024, the project aimed to train a 1-billion-parameter model on a large-scale dataset, prioritizing transparency and the sharing of practical insights to assist others in the community.","The training process primarily focused on Chinese data, with a small proportion of English data included, addressing gaps in existing open-source LLMs by providing a more detailed and practical account of the model-building journey.","Steel-LLM has demonstrated competitive performance on benchmarks such as CEVAL and CMMLU, outperforming early models from larger institutions.","This paper provides a comprehensive summary of the project's key contributions, including data collection, model design, training methodologies, and the challenges encountered along the way, offering a valuable resource for researchers and practitioners looking to develop their own LLMs.","The model checkpoints and training script are available at https://github.com/zhanshijinwat/Steel-LLM."],"url":"http://arxiv.org/abs/2502.06635v1"}
{"created":"2025-02-10 16:29:21","title":"Automatic Annotation Augmentation Boosts Translation between Molecules and Natural Language","abstract":"Recent advancements in AI for biological research focus on integrating molecular data with natural language to accelerate drug discovery. However, the scarcity of high-quality annotations limits progress in this area. This paper introduces LA$^3$, a Language-based Automatic Annotation Augmentation framework that leverages large language models to augment existing datasets, thereby improving AI training. We demonstrate the effectiveness of LA$^3$ by creating an enhanced dataset, LaChEBI-20, where we systematically rewrite the annotations of molecules from an established dataset. These rewritten annotations preserve essential molecular information while providing more varied sentence structures and vocabulary. Using LaChEBI-20, we train LaMolT5 based on a benchmark architecture to learn the mapping between molecular representations and augmented annotations.   Experimental results on text-based *de novo* molecule generation and molecule captioning demonstrate that LaMolT5 outperforms state-of-the-art models. Notably, incorporating LA$^3$ leads to improvements of up to 301% over the benchmark architecture. Furthermore, we validate the effectiveness of LA$^3$ notable applications in *image*, *text* and *graph* tasks, affirming its versatility and utility.","sentences":["Recent advancements in AI for biological research focus on integrating molecular data with natural language to accelerate drug discovery.","However, the scarcity of high-quality annotations limits progress in this area.","This paper introduces LA$^3$, a Language-based Automatic Annotation Augmentation framework that leverages large language models to augment existing datasets, thereby improving AI training.","We demonstrate the effectiveness of LA$^3$ by creating an enhanced dataset, LaChEBI-20, where we systematically rewrite the annotations of molecules from an established dataset.","These rewritten annotations preserve essential molecular information while providing more varied sentence structures and vocabulary.","Using LaChEBI-20, we train LaMolT5 based on a benchmark architecture to learn the mapping between molecular representations and augmented annotations.   ","Experimental results on text-based *de novo* molecule generation and molecule captioning demonstrate that LaMolT5 outperforms state-of-the-art models.","Notably, incorporating LA$^3$ leads to improvements of up to 301% over the benchmark architecture.","Furthermore, we validate the effectiveness of LA$^3$ notable applications in *image*, *text* and *graph* tasks, affirming its versatility and utility."],"url":"http://arxiv.org/abs/2502.06634v1"}
{"created":"2025-02-10 16:29:12","title":"Combining Large Language Models with Static Analyzers for Code Review Generation","abstract":"Code review is a crucial but often complex, subjective, and time-consuming activity in software development. Over the past decades, significant efforts have been made to automate this process. Early approaches focused on knowledge-based systems (KBS) that apply rule-based mechanisms to detect code issues, providing precise feedback but struggling with complex, context-dependent cases. More recent work has shifted toward fine-tuning pre-trained language models for code review, enabling broader issue coverage but often at the expense of precision. In this paper, we propose a hybrid approach that combines the strengths of KBS and learning-based systems (LBS) to generate high-quality, comprehensive code reviews. Our method integrates knowledge at three distinct stages of the language model pipeline: during data preparation (Data-Augmented Training, DAT), at inference (Retrieval-Augmented Generation, RAG), and after inference (Naive Concatenation of Outputs, NCO). We empirically evaluate our combination strategies against standalone KBS and LBS fine-tuned on a real-world dataset. Our results show that these hybrid strategies enhance the relevance, completeness, and overall quality of review comments, effectively bridging the gap between rule-based tools and deep learning models.","sentences":["Code review is a crucial but often complex, subjective, and time-consuming activity in software development.","Over the past decades, significant efforts have been made to automate this process.","Early approaches focused on knowledge-based systems (KBS) that apply rule-based mechanisms to detect code issues, providing precise feedback but struggling with complex, context-dependent cases.","More recent work has shifted toward fine-tuning pre-trained language models for code review, enabling broader issue coverage but often at the expense of precision.","In this paper, we propose a hybrid approach that combines the strengths of KBS and learning-based systems (LBS) to generate high-quality, comprehensive code reviews.","Our method integrates knowledge at three distinct stages of the language model pipeline: during data preparation (Data-Augmented Training, DAT), at inference (Retrieval-Augmented Generation, RAG), and after inference (Naive Concatenation of Outputs, NCO).","We empirically evaluate our combination strategies against standalone KBS and LBS fine-tuned on a real-world dataset.","Our results show that these hybrid strategies enhance the relevance, completeness, and overall quality of review comments, effectively bridging the gap between rule-based tools and deep learning models."],"url":"http://arxiv.org/abs/2502.06633v1"}
{"created":"2025-02-10 16:28:35","title":"Few-Shot Classification and Anatomical Localization of Tissues in SPECT Imaging","abstract":"Accurate classification and anatomical localization are essential for effective medical diagnostics and research, which may be efficiently performed using deep learning techniques. However, availability of limited labeled data poses a significant challenge. To address this, we adapted Prototypical Networks and the Propagation-Reconstruction Network (PRNet) for few-shot classification and localization, respectively, in Single Photon Emission Computed Tomography (SPECT) images. For the proof of concept we used a 2D-sliced image cropped around heart. The Prototypical Network, with a pre-trained ResNet-18 backbone, classified ventricles, myocardium, and liver tissues with 96.67% training and 93.33% validation accuracy. PRNet, adapted for 2D imaging with an encoder-decoder architecture and skip connections, achieved a training loss of 1.395, accurately reconstructing patches and capturing spatial relationships. These results highlight the potential of Prototypical Networks for tissue classification with limited labeled data and PRNet for anatomical landmark localization, paving the way for improved performance in deep learning frameworks.","sentences":["Accurate classification and anatomical localization are essential for effective medical diagnostics and research, which may be efficiently performed using deep learning techniques.","However, availability of limited labeled data poses a significant challenge.","To address this, we adapted Prototypical Networks and the Propagation-Reconstruction Network (PRNet) for few-shot classification and localization, respectively, in Single Photon Emission Computed Tomography (SPECT) images.","For the proof of concept we used a 2D-sliced image cropped around heart.","The Prototypical Network, with a pre-trained ResNet-18 backbone, classified ventricles, myocardium, and liver tissues with 96.67% training and 93.33% validation accuracy.","PRNet, adapted for 2D imaging with an encoder-decoder architecture and skip connections, achieved a training loss of 1.395, accurately reconstructing patches and capturing spatial relationships.","These results highlight the potential of Prototypical Networks for tissue classification with limited labeled data and PRNet for anatomical landmark localization, paving the way for improved performance in deep learning frameworks."],"url":"http://arxiv.org/abs/2502.06632v1"}
{"created":"2025-02-10 16:27:20","title":"Conformal Predictions for Human Action Recognition with Vision-Language Models","abstract":"Human-In-The-Loop (HITL) frameworks are integral to many real-world computer vision systems, enabling human operators to make informed decisions with AI assistance. Conformal Predictions (CP), which provide label sets with rigorous guarantees on ground truth inclusion probabilities, have recently gained traction as a valuable tool in HITL settings. One key application area is video surveillance, closely associated with Human Action Recognition (HAR). This study explores the application of CP on top of state-of-the-art HAR methods that utilize extensively pre-trained Vision-Language Models (VLMs). Our findings reveal that CP can significantly reduce the average number of candidate classes without modifying the underlying VLM. However, these reductions often result in distributions with long tails. To address this, we introduce a method based on tuning the temperature parameter of the VLMs to minimize these tails without requiring additional calibration data. Our code is made available on GitHub at the address https://github.com/tbary/CP4VLM.","sentences":["Human-In-The-Loop (HITL) frameworks are integral to many real-world computer vision systems, enabling human operators to make informed decisions with AI assistance.","Conformal Predictions (CP), which provide label sets with rigorous guarantees on ground truth inclusion probabilities, have recently gained traction as a valuable tool in HITL settings.","One key application area is video surveillance, closely associated with Human Action Recognition (HAR).","This study explores the application of CP on top of state-of-the-art HAR methods that utilize extensively pre-trained Vision-Language Models (VLMs).","Our findings reveal that CP can significantly reduce the average number of candidate classes without modifying the underlying VLM.","However, these reductions often result in distributions with long tails.","To address this, we introduce a method based on tuning the temperature parameter of the VLMs to minimize these tails without requiring additional calibration data.","Our code is made available on GitHub at the address https://github.com/tbary/CP4VLM."],"url":"http://arxiv.org/abs/2502.06631v1"}
{"created":"2025-02-10 16:15:29","title":"On the Reliability of Information Retrieval From MDS Coded Data in DNA Storage","abstract":"This work presents a theoretical analysis of the probability of successfully retrieving data encoded with MDS codes (e.g., Reed-Solomon codes) in DNA storage systems. We study this probability under independent and identically distributed (i.i.d.) substitution errors, focusing on a common code design strategy that combines inner and outer MDS codes. Our analysis demonstrates how this probability depends on factors such as the total number of sequencing reads, their distribution across strands, the rates of the inner and outer codes, and the substitution error probabilities. These results provide actionable insights into optimizing DNA storage systems under reliability constraints, including determining the minimum number of sequencing reads needed for reliable data retrieval and identifying the optimal balance between the rates of inner and outer MDS codes.","sentences":["This work presents a theoretical analysis of the probability of successfully retrieving data encoded with MDS codes (e.g., Reed-Solomon codes) in DNA storage systems.","We study this probability under independent and identically distributed (i.i.d.)","substitution errors, focusing on a common code design strategy that combines inner and outer MDS codes.","Our analysis demonstrates how this probability depends on factors such as the total number of sequencing reads, their distribution across strands, the rates of the inner and outer codes, and the substitution error probabilities.","These results provide actionable insights into optimizing DNA storage systems under reliability constraints, including determining the minimum number of sequencing reads needed for reliable data retrieval and identifying the optimal balance between the rates of inner and outer MDS codes."],"url":"http://arxiv.org/abs/2502.06618v1"}
{"created":"2025-02-10 16:07:55","title":"Automatic ISA analysis for Secure Context Switching","abstract":"Instruction set architectures are complex, with hundreds of registers and instructions that can modify dozens of them during execution, variably on each instance. Prose-style ISA specifications struggle to capture these intricacies of the ISAs, where often the important details about a single register are spread out across hundreds of pages of documentation. Ensuring that all ISA-state is swapped in context switch implementations of privileged software requires meticulous examination of these pages. This manual process is tedious and error-prone.   We propose a tool called Sailor that leverages machine-readable ISA specifications written in Sail to automate this task. Sailor determines the ISA-state necessary to swap during the context switch using the data collected from Sail and a novel algorithm to classify ISA-state as security-sensitive. Using Sailor's output, we identify three different classes of mishandled ISA-state across four open-source confidential computing systems. We further reveal five distinct security vulnerabilities that can be exploited using the mishandled ISA-state. This research exposes an often overlooked attack surface that stems from mishandled ISA-state, enabling unprivileged adversaries to exploit system vulnerabilities.","sentences":["Instruction set architectures are complex, with hundreds of registers and instructions that can modify dozens of them during execution, variably on each instance.","Prose-style ISA specifications struggle to capture these intricacies of the ISAs, where often the important details about a single register are spread out across hundreds of pages of documentation.","Ensuring that all ISA-state is swapped in context switch implementations of privileged software requires meticulous examination of these pages.","This manual process is tedious and error-prone.   ","We propose a tool called Sailor that leverages machine-readable ISA specifications written in Sail to automate this task.","Sailor determines the ISA-state necessary to swap during the context switch using the data collected from Sail and a novel algorithm to classify ISA-state as security-sensitive.","Using Sailor's output, we identify three different classes of mishandled ISA-state across four open-source confidential computing systems.","We further reveal five distinct security vulnerabilities that can be exploited using the mishandled ISA-state.","This research exposes an often overlooked attack surface that stems from mishandled ISA-state, enabling unprivileged adversaries to exploit system vulnerabilities."],"url":"http://arxiv.org/abs/2502.06609v1"}
{"created":"2025-02-10 16:07:54","title":"TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models","abstract":"Recent advancements in diffusion techniques have propelled image and video generation to unprece- dented levels of quality, significantly accelerating the deployment and application of generative AI. However, 3D shape generation technology has so far lagged behind, constrained by limitations in 3D data scale, complexity of 3D data process- ing, and insufficient exploration of advanced tech- niques in the 3D domain. Current approaches to 3D shape generation face substantial challenges in terms of output quality, generalization capa- bility, and alignment with input conditions. We present TripoSG, a new streamlined shape diffu- sion paradigm capable of generating high-fidelity 3D meshes with precise correspondence to input images. Specifically, we propose: 1) A large-scale rectified flow transformer for 3D shape generation, achieving state-of-the-art fidelity through training on extensive, high-quality data. 2) A hybrid supervised training strategy combining SDF, normal, and eikonal losses for 3D VAE, achieving high- quality 3D reconstruction performance. 3) A data processing pipeline to generate 2 million high- quality 3D samples, highlighting the crucial rules for data quality and quantity in training 3D gen- erative models. Through comprehensive experi- ments, we have validated the effectiveness of each component in our new framework. The seamless integration of these parts has enabled TripoSG to achieve state-of-the-art performance in 3D shape generation. The resulting 3D shapes exhibit en- hanced detail due to high-resolution capabilities and demonstrate exceptional fidelity to input im- ages. Moreover, TripoSG demonstrates improved versatility in generating 3D models from diverse image styles and contents, showcasing strong gen- eralization capabilities. To foster progress and innovation in the field of 3D generation, we will make our model publicly available.","sentences":["Recent advancements in diffusion techniques have propelled image and video generation to unprece- dented levels of quality, significantly accelerating the deployment and application of generative AI.","However, 3D shape generation technology has so far lagged behind, constrained by limitations in 3D data scale, complexity of 3D data process-","ing, and insufficient exploration of advanced tech- niques in the 3D domain.","Current approaches to 3D shape generation face substantial challenges in terms of output quality, generalization capa- bility, and alignment with input conditions.","We present TripoSG, a new streamlined shape diffu- sion paradigm capable of generating high-fidelity 3D meshes with precise correspondence to input images.","Specifically, we propose: 1) A large-scale rectified flow transformer for 3D shape generation, achieving state-of-the-art fidelity through training on extensive, high-quality data.","2) A hybrid supervised training strategy combining SDF, normal, and eikonal losses for 3D VAE, achieving high- quality 3D reconstruction performance.","3) A data processing pipeline to generate 2 million high- quality 3D samples, highlighting the crucial rules for data quality and quantity in training 3D gen-","erative models.","Through comprehensive experi- ments, we have validated the effectiveness of each component in our new framework.","The seamless integration of these parts has enabled TripoSG to achieve state-of-the-art performance in 3D shape generation.","The resulting 3D shapes exhibit en- hanced detail due to high-resolution capabilities and demonstrate exceptional fidelity to input im- ages.","Moreover, TripoSG demonstrates improved versatility in generating 3D models from diverse image styles and contents, showcasing strong gen- eralization capabilities.","To foster progress and innovation in the field of 3D generation, we will make our model publicly available."],"url":"http://arxiv.org/abs/2502.06608v1"}
{"created":"2025-02-10 16:01:55","title":"Do we really have to filter out random noise in pre-training data for language models?","abstract":"Web-scale pre-training datasets are the cornerstone of LLMs' success. However, text data curated from the internet inevitably contains random noise caused by decoding errors or unregulated web content. In contrast to previous works that focus on low quality or synthetic data, our study \\textbf{provides the first systematic investigation into such random noise through a cohesive ``What-Why-How'' framework.} Surprisingly, we observed that the resulting increase in next-token prediction (NTP) loss was significantly lower than the proportion of random noise. We provide a theoretical justification for this phenomenon, which also elucidates the success of multilingual models. On the other hand, experiments show that the model's performance in downstream tasks is not based solely on the NTP loss, which means that random noise may result in degraded downstream performance. To address the potential adverse effects, we introduce a novel plug-and-play Local Gradient Matching loss, which explicitly enhances the denoising capability of the downstream task head by aligning the gradient of normal and perturbed features without requiring knowledge of the model's parameters. Additional experiments on 8 language and 14 vision benchmarks further validate its effectiveness.","sentences":["Web-scale pre-training datasets are the cornerstone of LLMs' success.","However, text data curated from the internet inevitably contains random noise caused by decoding errors or unregulated web content.","In contrast to previous works that focus on low quality or synthetic data, our study \\textbf{provides the first systematic investigation into such random noise through a cohesive ``What-Why-How'' framework.}","Surprisingly, we observed that the resulting increase in next-token prediction (NTP) loss was significantly lower than the proportion of random noise.","We provide a theoretical justification for this phenomenon, which also elucidates the success of multilingual models.","On the other hand, experiments show that the model's performance in downstream tasks is not based solely on the NTP loss, which means that random noise may result in degraded downstream performance.","To address the potential adverse effects, we introduce a novel plug-and-play Local Gradient Matching loss, which explicitly enhances the denoising capability of the downstream task head by aligning the gradient of normal and perturbed features without requiring knowledge of the model's parameters.","Additional experiments on 8 language and 14 vision benchmarks further validate its effectiveness."],"url":"http://arxiv.org/abs/2502.06604v1"}
{"created":"2025-02-10 16:00:48","title":"Amortized In-Context Bayesian Posterior Estimation","abstract":"Bayesian inference provides a natural way of incorporating prior beliefs and assigning a probability measure to the space of hypotheses. Current solutions rely on iterative routines like Markov Chain Monte Carlo (MCMC) sampling and Variational Inference (VI), which need to be re-run whenever new observations are available. Amortization, through conditional estimation, is a viable strategy to alleviate such difficulties and has been the guiding principle behind simulation-based inference, neural processes and in-context methods using pre-trained models. In this work, we conduct a thorough comparative analysis of amortized in-context Bayesian posterior estimation methods from the lens of different optimization objectives and architectural choices. Such methods train an amortized estimator to perform posterior parameter inference by conditioning on a set of data examples passed as context to a sequence model such as a transformer. In contrast to language models, we leverage permutation invariant architectures as the true posterior is invariant to the ordering of context examples. Our empirical study includes generalization to out-of-distribution tasks, cases where the assumed underlying model is misspecified, and transfer from simulated to real problems. Subsequently, it highlights the superiority of the reverse KL estimator for predictive problems, especially when combined with the transformer architecture and normalizing flows.","sentences":["Bayesian inference provides a natural way of incorporating prior beliefs and assigning a probability measure to the space of hypotheses.","Current solutions rely on iterative routines like Markov Chain Monte Carlo (MCMC) sampling and Variational Inference (VI), which need to be re-run whenever new observations are available.","Amortization, through conditional estimation, is a viable strategy to alleviate such difficulties and has been the guiding principle behind simulation-based inference, neural processes and in-context methods using pre-trained models.","In this work, we conduct a thorough comparative analysis of amortized in-context Bayesian posterior estimation methods from the lens of different optimization objectives and architectural choices.","Such methods train an amortized estimator to perform posterior parameter inference by conditioning on a set of data examples passed as context to a sequence model such as a transformer.","In contrast to language models, we leverage permutation invariant architectures as the true posterior is invariant to the ordering of context examples.","Our empirical study includes generalization to out-of-distribution tasks, cases where the assumed underlying model is misspecified, and transfer from simulated to real problems.","Subsequently, it highlights the superiority of the reverse KL estimator for predictive problems, especially when combined with the transformer architecture and normalizing flows."],"url":"http://arxiv.org/abs/2502.06601v1"}
{"created":"2025-02-10 16:00:00","title":"Evaluation of Multilingual Image Captioning: How far can we get with CLIP models?","abstract":"The evaluation of image captions, looking at both linguistic fluency and semantic correspondence to visual contents, has witnessed a significant effort. Still, despite advancements such as the CLIPScore metric, multilingual captioning evaluation has remained relatively unexplored. This work presents several strategies, and extensive experiments, related to evaluating CLIPScore variants in multilingual settings. To address the lack of multilingual test data, we consider two different strategies: (1) using quality aware machine-translated datasets with human judgements, and (2) re-purposing multilingual datasets that target semantic inference and reasoning. Our results highlight the potential of finetuned multilingual models to generalize across languages and to handle complex linguistic challenges. Tests with machine-translated data show that multilingual CLIPScore models can maintain a high correlation with human judgements across different languages, and additional tests with natively multilingual and multicultural data further attest to the high-quality assessments.","sentences":["The evaluation of image captions, looking at both linguistic fluency and semantic correspondence to visual contents, has witnessed a significant effort.","Still, despite advancements such as the CLIPScore metric, multilingual captioning evaluation has remained relatively unexplored.","This work presents several strategies, and extensive experiments, related to evaluating CLIPScore variants in multilingual settings.","To address the lack of multilingual test data, we consider two different strategies: (1) using quality aware machine-translated datasets with human judgements, and (2) re-purposing multilingual datasets that target semantic inference and reasoning.","Our results highlight the potential of finetuned multilingual models to generalize across languages and to handle complex linguistic challenges.","Tests with machine-translated data show that multilingual CLIPScore models can maintain a high correlation with human judgements across different languages, and additional tests with natively multilingual and multicultural data further attest to the high-quality assessments."],"url":"http://arxiv.org/abs/2502.06600v1"}
{"created":"2025-02-10 15:58:26","title":"Continual Release Moment Estimation with Differential Privacy","abstract":"We propose Joint Moment Estimation (JME), a method for continually and privately estimating both the first and second moments of data with reduced noise compared to naive approaches. JME uses the matrix mechanism and a joint sensitivity analysis to allow the second moment estimation with no additional privacy cost, thereby improving accuracy while maintaining privacy. We demonstrate JME's effectiveness in two applications: estimating the running mean and covariance matrix for Gaussian density estimation, and model training with DP-Adam on CIFAR-10.","sentences":["We propose Joint Moment Estimation (JME), a method for continually and privately estimating both the first and second moments of data with reduced noise compared to naive approaches.","JME uses the matrix mechanism and a joint sensitivity analysis to allow the second moment estimation with no additional privacy cost, thereby improving accuracy while maintaining privacy.","We demonstrate JME's effectiveness in two applications: estimating the running mean and covariance matrix for Gaussian density estimation, and model training with DP-Adam on CIFAR-10."],"url":"http://arxiv.org/abs/2502.06597v1"}
{"created":"2025-02-10 15:55:08","title":"Diffeomorphic Temporal Alignment Nets for Time-series Joint Alignment and Averaging","abstract":"In time-series analysis, nonlinear temporal misalignment remains a pivotal challenge that forestalls even simple averaging. Since its introduction, the Diffeomorphic Temporal Alignment Net (DTAN), which we first introduced (Weber et al., 2019) and further developed in (Weber & Freifeld, 2023), has proven itself as an effective solution for this problem (these conference papers are earlier partial versions of the current manuscript). DTAN predicts and applies diffeomorphic transformations in an input-dependent manner, thus facilitating the joint alignment (JA) and averaging of time-series ensembles in an unsupervised or a weakly-supervised manner. The inherent challenges of the weakly/unsupervised setting, particularly the risk of trivial solutions through excessive signal distortion, are mitigated using either one of two distinct strategies: 1) a regularization term for warps; 2) using the Inverse Consistency Averaging Error (ICAE). The latter is a novel, regularization-free approach which also facilitates the JA of variable-length signals. We also further extend our framework to incorporate multi-task learning (MT-DTAN), enabling simultaneous time-series alignment and classification. Additionally, we conduct a comprehensive evaluation of different backbone architectures, demonstrating their efficacy in time-series alignment tasks. Finally, we showcase the utility of our approach in enabling Principal Component Analysis (PCA) for misaligned time-series data. Extensive experiments across 128 UCR datasets validate the superiority of our approach over contemporary averaging methods, including both traditional and learning-based approaches, marking a significant advancement in the field of time-series analysis.","sentences":["In time-series analysis, nonlinear temporal misalignment remains a pivotal challenge that forestalls even simple averaging.","Since its introduction, the Diffeomorphic Temporal Alignment Net (DTAN), which we first introduced (Weber et al., 2019) and further developed in (Weber & Freifeld, 2023), has proven itself as an effective solution for this problem (these conference papers are earlier partial versions of the current manuscript).","DTAN predicts and applies diffeomorphic transformations in an input-dependent manner, thus facilitating the joint alignment (JA) and averaging of time-series ensembles in an unsupervised or a weakly-supervised manner.","The inherent challenges of the weakly/unsupervised setting, particularly the risk of trivial solutions through excessive signal distortion, are mitigated using either one of two distinct strategies: 1) a regularization term for warps; 2) using the Inverse Consistency Averaging Error (ICAE).","The latter is a novel, regularization-free approach which also facilitates the JA of variable-length signals.","We also further extend our framework to incorporate multi-task learning (MT-DTAN), enabling simultaneous time-series alignment and classification.","Additionally, we conduct a comprehensive evaluation of different backbone architectures, demonstrating their efficacy in time-series alignment tasks.","Finally, we showcase the utility of our approach in enabling Principal Component Analysis (PCA) for misaligned time-series data.","Extensive experiments across 128 UCR datasets validate the superiority of our approach over contemporary averaging methods, including both traditional and learning-based approaches, marking a significant advancement in the field of time-series analysis."],"url":"http://arxiv.org/abs/2502.06591v1"}
{"created":"2025-02-10 15:54:34","title":"Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training","abstract":"Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability. We introduce Hephaestus-Forge, the first large-scale pre-training corpus designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adapting to environmental feedback. Hephaestus-Forge comprises 103B agent-specific data encompassing 76,537 APIs, including both tool documentation to introduce knowledge of API functions and function calling trajectories to strengthen intrinsic reasoning. To explore effective training protocols, we investigate scaling laws to identify the optimal recipe in data mixing ratios. By continual pre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale open-source LLMs and rivals commercial LLMs on three agent benchmarks, demonstrating the effectiveness of our pre-training corpus in enhancing fundamental agentic capabilities and generalization of LLMs to new tasks or environments.","sentences":["Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability.","We introduce Hephaestus-Forge, the first large-scale pre-training corpus designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adapting to environmental feedback.","Hephaestus-Forge comprises 103B agent-specific data encompassing 76,537 APIs, including both tool documentation to introduce knowledge of API functions and function calling trajectories to strengthen intrinsic reasoning.","To explore effective training protocols, we investigate scaling laws to identify the optimal recipe in data mixing ratios.","By continual pre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale open-source LLMs and rivals commercial LLMs on three agent benchmarks, demonstrating the effectiveness of our pre-training corpus in enhancing fundamental agentic capabilities and generalization of LLMs to new tasks or environments."],"url":"http://arxiv.org/abs/2502.06589v1"}
{"created":"2025-02-10 15:53:26","title":"evclust: Python library for evidential clustering","abstract":"A recent developing trend in clustering is the advancement of algorithms that not only identify clusters within data, but also express and capture the uncertainty of cluster membership. Evidential clustering addresses this by using the Dempster-Shafer theory of belief functions, a framework designed to manage and represent uncertainty. This approach results in a credal partition, a structured set of mass functions that quantify the uncertain assignment of each object to potential groups. The Python framework evclust, presented in this paper, offers a suite of efficient evidence clustering algorithms as well as tools for visualizing, evaluating and analyzing credal partitions.","sentences":["A recent developing trend in clustering is the advancement of algorithms that not only identify clusters within data, but also express and capture the uncertainty of cluster membership.","Evidential clustering addresses this by using the Dempster-Shafer theory of belief functions, a framework designed to manage and represent uncertainty.","This approach results in a credal partition, a structured set of mass functions that quantify the uncertain assignment of each object to potential groups.","The Python framework evclust, presented in this paper, offers a suite of efficient evidence clustering algorithms as well as tools for visualizing, evaluating and analyzing credal partitions."],"url":"http://arxiv.org/abs/2502.06587v1"}
{"created":"2025-02-10 15:53:25","title":"Decay of correlation for edge colorings when $q>3\u0394$","abstract":"We examine various perspectives on the decay of correlation for the uniform distribution over proper $q$-edge colorings of graphs with maximum degree $\\Delta$.   First, we establish the coupling independence property when $q\\ge 3\\Delta$ for general graphs. Together with the work of Chen et al. (2024), this result implies a fully polynomial-time approximation scheme (FPTAS) for counting the number of proper $q$-edge colorings.   Next, we prove the strong spatial mixing property on trees, provided that $q> (3+o(1))\\Delta$. The strong spatial mixing property is derived from the spectral independence property of a version of the weighted edge coloring distribution, which is established using the matrix trickle-down method developed in Abdolazimi, Liu and Oveis Gharan (FOCS, 2021) and Wang, Zhang and Zhang (STOC, 2024).   Finally, we show that the weak spatial mixing property holds on trees with maximum degree $\\Delta$ if and only if $q\\ge 2\\Delta-1$.","sentences":["We examine various perspectives on the decay of correlation for the uniform distribution over proper $q$-edge colorings of graphs with maximum degree $\\Delta$.   First, we establish the coupling independence property when $q\\ge 3\\Delta$ for general graphs.","Together with the work of Chen et al. (2024), this result implies a fully polynomial-time approximation scheme (FPTAS) for counting the number of proper $q$-edge colorings.   ","Next, we prove the strong spatial mixing property on trees, provided that $q> (3+o(1))\\Delta$. The strong spatial mixing property is derived from the spectral independence property of a version of the weighted edge coloring distribution, which is established using the matrix trickle-down method developed in Abdolazimi, Liu and Oveis Gharan (FOCS, 2021) and Wang, Zhang and Zhang (STOC, 2024).   ","Finally, we show that the weak spatial mixing property holds on trees with maximum degree $\\Delta$ if and only if $q\\ge 2\\Delta-1$."],"url":"http://arxiv.org/abs/2502.06586v1"}
{"created":"2025-02-10 15:52:55","title":"Deep Reinforcement Learning based Triggering Function for Early Classifiers of Time Series","abstract":"Early Classification of Time Series (ECTS) has been recognized as an important problem in many areas where decisions have to be taken as soon as possible, before the full data availability, while time pressure increases. Numerous ECTS approaches have been proposed, based on different triggering functions, each taking into account various pieces of information related to the incoming time series and/or the output of a classifier. Although their performances have been empirically compared in the literature, no studies have been carried out on the optimality of these triggering functions that involve ``man-tailored'' decision rules. Based on the same information, could there be better triggering functions? This paper presents one way to investigate this question by showing first how to translate ECTS problems into Reinforcement Learning (RL) ones, where the very same information is used in the state space. A thorough comparison of the performance obtained by ``handmade'' approaches and their ``RL-based'' counterparts has been carried out. A second question investigated in this paper is whether a different combination of information, defining the state space in RL systems, can achieve even better performance. Experiments show that the system we describe, called \\textsc{Alert}, significantly outperforms its state-of-the-art competitors on a large number of datasets.","sentences":["Early Classification of Time Series (ECTS) has been recognized as an important problem in many areas where decisions have to be taken as soon as possible, before the full data availability, while time pressure increases.","Numerous ECTS approaches have been proposed, based on different triggering functions, each taking into account various pieces of information related to the incoming time series and/or the output of a classifier.","Although their performances have been empirically compared in the literature, no studies have been carried out on the optimality of these triggering functions that involve ``man-tailored'' decision rules.","Based on the same information, could there be better triggering functions?","This paper presents one way to investigate this question by showing first how to translate ECTS problems into Reinforcement Learning (RL) ones, where the very same information is used in the state space.","A thorough comparison of the performance obtained by ``handmade'' approaches and their ``RL-based'' counterparts has been carried out.","A second question investigated in this paper is whether a different combination of information, defining the state space in RL systems, can achieve even better performance.","Experiments show that the system we describe, called \\textsc{Alert}, significantly outperforms its state-of-the-art competitors on a large number of datasets."],"url":"http://arxiv.org/abs/2502.06584v1"}
{"created":"2025-02-10 15:48:11","title":"A Survey on Video Analytics in Cloud-Edge-Terminal Collaborative Systems","abstract":"The explosive growth of video data has driven the development of distributed video analytics in cloud-edge-terminal collaborative (CETC) systems, enabling efficient video processing, real-time inference, and privacy-preserving analysis. Among multiple advantages, CETC systems can distribute video processing tasks and enable adaptive analytics across cloud, edge, and terminal devices, leading to breakthroughs in video surveillance, autonomous driving, and smart cities. In this survey, we first analyze fundamental architectural components, including hierarchical, distributed, and hybrid frameworks, alongside edge computing platforms and resource management mechanisms. Building upon these foundations, edge-centric approaches emphasize on-device processing, edge-assisted offloading, and edge intelligence, while cloud-centric methods leverage powerful computational capabilities for complex video understanding and model training. Our investigation also covers hybrid video analytics incorporating adaptive task offloading and resource-aware scheduling techniques that optimize performance across the entire system. Beyond conventional approaches, recent advances in large language models and multimodal integration reveal both opportunities and challenges in platform scalability, data protection, and system reliability. Future directions also encompass explainable systems, efficient processing mechanisms, and advanced video analytics, offering valuable insights for researchers and practitioners in this dynamic field.","sentences":["The explosive growth of video data has driven the development of distributed video analytics in cloud-edge-terminal collaborative (CETC) systems, enabling efficient video processing, real-time inference, and privacy-preserving analysis.","Among multiple advantages, CETC systems can distribute video processing tasks and enable adaptive analytics across cloud, edge, and terminal devices, leading to breakthroughs in video surveillance, autonomous driving, and smart cities.","In this survey, we first analyze fundamental architectural components, including hierarchical, distributed, and hybrid frameworks, alongside edge computing platforms and resource management mechanisms.","Building upon these foundations, edge-centric approaches emphasize on-device processing, edge-assisted offloading, and edge intelligence, while cloud-centric methods leverage powerful computational capabilities for complex video understanding and model training.","Our investigation also covers hybrid video analytics incorporating adaptive task offloading and resource-aware scheduling techniques that optimize performance across the entire system.","Beyond conventional approaches, recent advances in large language models and multimodal integration reveal both opportunities and challenges in platform scalability, data protection, and system reliability.","Future directions also encompass explainable systems, efficient processing mechanisms, and advanced video analytics, offering valuable insights for researchers and practitioners in this dynamic field."],"url":"http://arxiv.org/abs/2502.06581v1"}
{"created":"2025-02-10 15:44:34","title":"Predictive Red Teaming: Breaking Policies Without Breaking Robots","abstract":"Visuomotor policies trained via imitation learning are capable of performing challenging manipulation tasks, but are often extremely brittle to lighting, visual distractors, and object locations. These vulnerabilities can depend unpredictably on the specifics of training, and are challenging to expose without time-consuming and expensive hardware evaluations. We propose the problem of predictive red teaming: discovering vulnerabilities of a policy with respect to environmental factors, and predicting the corresponding performance degradation without hardware evaluations in off-nominal scenarios. In order to achieve this, we develop RoboART: an automated red teaming (ART) pipeline that (1) modifies nominal observations using generative image editing to vary different environmental factors, and (2) predicts performance under each variation using a policy-specific anomaly detector executed on edited observations. Experiments across 500+ hardware trials in twelve off-nominal conditions for visuomotor diffusion policies demonstrate that RoboART predicts performance degradation with high accuracy (less than 0.19 average difference between predicted and real success rates). We also demonstrate how predictive red teaming enables targeted data collection: fine-tuning with data collected under conditions predicted to be adverse boosts baseline performance by 2-7x.","sentences":["Visuomotor policies trained via imitation learning are capable of performing challenging manipulation tasks, but are often extremely brittle to lighting, visual distractors, and object locations.","These vulnerabilities can depend unpredictably on the specifics of training, and are challenging to expose without time-consuming and expensive hardware evaluations.","We propose the problem of predictive red teaming: discovering vulnerabilities of a policy with respect to environmental factors, and predicting the corresponding performance degradation without hardware evaluations in off-nominal scenarios.","In order to achieve this, we develop RoboART: an automated red teaming (ART) pipeline that (1) modifies nominal observations using generative image editing to vary different environmental factors, and (2) predicts performance under each variation using a policy-specific anomaly detector executed on edited observations.","Experiments across 500+ hardware trials in twelve off-nominal conditions for visuomotor diffusion policies demonstrate that RoboART predicts performance degradation with high accuracy (less than 0.19 average difference between predicted and real success rates).","We also demonstrate how predictive red teaming enables targeted data collection: fine-tuning with data collected under conditions predicted to be adverse boosts baseline performance by 2-7x."],"url":"http://arxiv.org/abs/2502.06575v1"}
{"created":"2025-02-10 15:42:38","title":"On the Impact of the Utility in Semivalue-based Data Valuation","abstract":"Semivalue-based data valuation in machine learning (ML) quantifies the contribution of individual data points to a downstream ML task by leveraging principles from cooperative game theory and the notion of utility. While this framework has been used in practice for assessing data quality, our experiments reveal inconsistent valuation outcomes across different utilities, albeit all related to ML performance. Beyond raising concerns about the reliability of data valuation, this inconsistency is challenging to interpret, as it stems from the complex interaction of the utility with data points and semivalue weights, which has barely been studied in prior work. In this paper, we take a first step toward clarifying the utility impact on semivalue-based data valuation. Specifically, we provide geometric interpretations of this impact for a broad family of classification utilities, which includes the accuracy and the arithmetic mean. We introduce the notion of spatial signatures: given a semivalue, data points can be embedded into a two-dimensional space, and utility functions map to the dual of this space. This geometric perspective separates the influence of the dataset and semivalue from that of the utility, providing a theoretical explanation for the experimentally observed sensitivity of valuation outcomes to the utility choice.","sentences":["Semivalue-based data valuation in machine learning (ML) quantifies the contribution of individual data points to a downstream ML task by leveraging principles from cooperative game theory and the notion of utility.","While this framework has been used in practice for assessing data quality, our experiments reveal inconsistent valuation outcomes across different utilities, albeit all related to ML performance.","Beyond raising concerns about the reliability of data valuation, this inconsistency is challenging to interpret, as it stems from the complex interaction of the utility with data points and semivalue weights, which has barely been studied in prior work.","In this paper, we take a first step toward clarifying the utility impact on semivalue-based data valuation.","Specifically, we provide geometric interpretations of this impact for a broad family of classification utilities, which includes the accuracy and the arithmetic mean.","We introduce the notion of spatial signatures: given a semivalue, data points can be embedded into a two-dimensional space, and utility functions map to the dual of this space.","This geometric perspective separates the influence of the dataset and semivalue from that of the utility, providing a theoretical explanation for the experimentally observed sensitivity of valuation outcomes to the utility choice."],"url":"http://arxiv.org/abs/2502.06574v1"}
{"created":"2025-02-10 15:40:35","title":"LawGPT: Knowledge-Guided Data Generation and Its Application to Legal LLM","abstract":"Large language models (LLMs), both proprietary and open-source, have demonstrated remarkable capabilities across various natural language processing tasks. However, they face significant limitations in legal reasoning tasks. Proprietary models introduce data privacy risks and high inference costs, while open-source models underperform due to insufficient legal domain training data. To address these limitations, we study data generation for legal reasoning to improve the legal reasoning performance of open-source LLMs with the help of proprietary LLMs. This is challenging due to the lack of legal knowledge in proprietary LLMs and the difficulty in verifying the generated data. We propose KgDG, a knowledge-guided data generation framework for legal reasoning. Our framework enables leveraging legal knowledge to enhance generation diversity and introduces a refinement and verification process to ensure the quality of generated data. Moreover, we expand the generated dataset to further enhance the LLM reasoning capabilities. Using KgDG, we create a synthetic legal reasoning dataset containing 50K high-quality examples. Our trained model LawGPT outperforms existing legal-specific LLMs and achieves performance comparable to proprietary LLMs, demonstrating the effectiveness of KgDG and LawGPT. Our code and resources is publicly available at https://anonymous.4open.science/r/KgDG-45F5 .","sentences":["Large language models (LLMs), both proprietary and open-source, have demonstrated remarkable capabilities across various natural language processing tasks.","However, they face significant limitations in legal reasoning tasks.","Proprietary models introduce data privacy risks and high inference costs, while open-source models underperform due to insufficient legal domain training data.","To address these limitations, we study data generation for legal reasoning to improve the legal reasoning performance of open-source LLMs with the help of proprietary LLMs.","This is challenging due to the lack of legal knowledge in proprietary LLMs and the difficulty in verifying the generated data.","We propose KgDG, a knowledge-guided data generation framework for legal reasoning.","Our framework enables leveraging legal knowledge to enhance generation diversity and introduces a refinement and verification process to ensure the quality of generated data.","Moreover, we expand the generated dataset to further enhance the LLM reasoning capabilities.","Using KgDG, we create a synthetic legal reasoning dataset containing 50K high-quality examples.","Our trained model LawGPT outperforms existing legal-specific LLMs and achieves performance comparable to proprietary LLMs, demonstrating the effectiveness of KgDG and LawGPT.","Our code and resources is publicly available at https://anonymous.4open.science/r/KgDG-45F5 ."],"url":"http://arxiv.org/abs/2502.06572v1"}
{"created":"2025-02-10 15:31:57","title":"Robust Scatter Matrix Estimation for Elliptical Distributions in Polynomial Time","abstract":"We study the problem of computationally efficient robust estimation of scatter matrices of elliptical distributions under the strong contamination model. We design polynomial time algorithms that achieve dimension-independent error in Frobenius norm.   Our first result is a sequence of efficient algorithms that approaches nearly optimal error. Specifically, under a mild assumption on the eigenvalues of the scatter matrix $\\Sigma$, for every $t \\in \\mathbb{N}$, we design an estimator that, given $n = d^{O(t)}$ samples, in time $n^{O(t)}$ finds $\\hat{\\Sigma}$ such that $ \\Vert{\\Sigma^{-1/2}\\, ({\\hat{\\Sigma} - \\Sigma})\\, \\Sigma^{-1/2}}\\Vert_{\\text{F}} \\le O(t \\cdot \\varepsilon^{1-\\frac{1}{t}})$, where $\\varepsilon$ is the fraction of corruption. We do not require any assumptions on the moments of the distribution, while all previously known computationally efficient algorithms for robust covariance/scatter estimation with dimension-independent error rely on strong assumptions on the moments, such as sub-Gaussianity or (certifiable) hypercontractivity.   Furthermore, under a stronger assumption on the eigenvalues of $\\Sigma$ (that, in particular, is satisfied by all matrices with constant condition number),   we provide a fast (sub-quadratic in the input size) algorithm that, given nearly optimal number of samples $n = \\tilde{O}(d^2/\\varepsilon)$, in time $\\tilde{O}({nd^2 poly(1/\\varepsilon)})$ finds $\\hat{\\Sigma}$ such that $\\Vert\\hat{\\Sigma} - \\Sigma\\Vert_{\\text{F}} \\le O(\\Vert{\\Sigma}\\Vert \\cdot \\sqrt{\\varepsilon})$.   Our approach is based on robust covariance estimation of the spatial sign (the projection onto the sphere of radius $\\sqrt{d}$) of elliptical distributions.","sentences":["We study the problem of computationally efficient robust estimation of scatter matrices of elliptical distributions under the strong contamination model.","We design polynomial time algorithms that achieve dimension-independent error in Frobenius norm.   ","Our first result is a sequence of efficient algorithms that approaches nearly optimal error.","Specifically, under a mild assumption on the eigenvalues of the scatter matrix $\\Sigma$, for every $t \\in \\mathbb{N}$, we design an estimator that, given $n = d^{O(t)}$ samples, in time $n^{O(t)}$ finds $\\hat{\\Sigma}$ such that $ \\Vert{\\Sigma^{-1/2}\\, ({\\hat{\\Sigma} - \\Sigma})\\, \\Sigma^{-1/2}}\\Vert_{\\text{F}} \\le O(t \\cdot \\varepsilon^{1-\\frac{1}{t}})$, where $\\varepsilon$ is the fraction of corruption.","We do not require any assumptions on the moments of the distribution, while all previously known computationally efficient algorithms for robust covariance/scatter estimation with dimension-independent error rely on strong assumptions on the moments, such as sub-Gaussianity or (certifiable) hypercontractivity.   ","Furthermore, under a stronger assumption on the eigenvalues of $\\Sigma$ (that, in particular, is satisfied by all matrices with constant condition number),   we provide a fast (sub-quadratic in the input size) algorithm that, given nearly optimal number of samples $n = \\tilde{O}(d^2/\\varepsilon)$, in time $\\tilde{O}({nd^2 poly(1/\\varepsilon)})$ finds $\\hat{\\Sigma}$ such that $\\Vert\\hat{\\Sigma} - \\Sigma\\Vert_{\\text{F}} \\le O(\\Vert{\\Sigma}\\Vert","\\cdot \\sqrt{\\varepsilon})$.   Our approach is based on robust covariance estimation of the spatial sign (the projection onto the sphere of radius $\\sqrt{d}$) of elliptical distributions."],"url":"http://arxiv.org/abs/2502.06564v1"}
{"created":"2025-02-10 15:31:54","title":"Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation","abstract":"First-order logic (FOL) reasoning, which involves sequential deduction, is pivotal for intelligent systems and serves as a valuable task for evaluating reasoning capabilities, particularly in chain-of-thought (CoT) contexts. Existing benchmarks often rely on extensive human annotation or handcrafted templates, making it difficult to achieve the necessary complexity, scalability, and diversity for robust evaluation. To address these limitations, we propose a novel framework called ProverGen that synergizes the generative strengths of Large Language Models (LLMs) with the rigor and precision of symbolic provers, enabling the creation of a scalable, diverse, and high-quality FOL reasoning dataset, ProverQA. ProverQA is also distinguished by its inclusion of accessible and logically coherent intermediate reasoning steps for each problem. Our evaluation shows that state-of-the-art LLMs struggle to solve ProverQA problems, even with CoT prompting, highlighting the dataset's challenging nature. We also finetune Llama3.1-8B-Instruct on a separate training set generated by our framework. The finetuned model demonstrates consistent improvements on both in-distribution and out-of-distribution test sets, suggesting the value of our proposed data generation framework. Code available at: https://github.com/opendatalab/ProverGen","sentences":["First-order logic (FOL) reasoning, which involves sequential deduction, is pivotal for intelligent systems and serves as a valuable task for evaluating reasoning capabilities, particularly in chain-of-thought (CoT) contexts.","Existing benchmarks often rely on extensive human annotation or handcrafted templates, making it difficult to achieve the necessary complexity, scalability, and diversity for robust evaluation.","To address these limitations, we propose a novel framework called ProverGen that synergizes the generative strengths of Large Language Models (LLMs) with the rigor and precision of symbolic provers, enabling the creation of a scalable, diverse, and high-quality FOL reasoning dataset, ProverQA.","ProverQA is also distinguished by its inclusion of accessible and logically coherent intermediate reasoning steps for each problem.","Our evaluation shows that state-of-the-art LLMs struggle to solve ProverQA problems, even with CoT prompting, highlighting the dataset's challenging nature.","We also finetune Llama3.1-8B-Instruct on a separate training set generated by our framework.","The finetuned model demonstrates consistent improvements on both in-distribution and out-of-distribution test sets, suggesting the value of our proposed data generation framework.","Code available at: https://github.com/opendatalab/ProverGen"],"url":"http://arxiv.org/abs/2502.06563v1"}
{"created":"2025-02-10 15:25:11","title":"Position: It's Time to Act on the Risk of Efficient Personalized Text Generation","abstract":"The recent surge in high-quality open-sourced Generative AI text models (colloquially: LLMs), as well as efficient finetuning techniques, has opened the possibility of creating high-quality personalized models, i.e., models generating text attuned to a specific individual's needs and capable of credibly imitating their writing style by leveraging that person's own data to refine an open-source model. The technology to create such models is accessible to private individuals, and training and running such models can be done cheaply on consumer-grade hardware. These advancements are a huge gain for usability and privacy. This position paper argues, however, that these advancements also introduce new safety risks by making it practically feasible for malicious actors to impersonate specific individuals at scale, for instance for the purpose of phishing emails, based on small amounts of publicly available text. We further argue that these risks are complementary to - and distinct from - the much-discussed risks of other impersonation attacks such as image, voice, or video deepfakes, and are not adequately addressed by the larger research community, or the current generation of open - and closed-source models.","sentences":["The recent surge in high-quality open-sourced Generative AI text models (colloquially: LLMs), as well as efficient finetuning techniques, has opened the possibility of creating high-quality personalized models, i.e., models generating text attuned to a specific individual's needs and capable of credibly imitating their writing style by leveraging that person's own data to refine an open-source model.","The technology to create such models is accessible to private individuals, and training and running such models can be done cheaply on consumer-grade hardware.","These advancements are a huge gain for usability and privacy.","This position paper argues, however, that these advancements also introduce new safety risks by making it practically feasible for malicious actors to impersonate specific individuals at scale, for instance for the purpose of phishing emails, based on small amounts of publicly available text.","We further argue that these risks are complementary to - and distinct from - the much-discussed risks of other impersonation attacks such as image, voice, or video deepfakes, and are not adequately addressed by the larger research community, or the current generation of open - and closed-source models."],"url":"http://arxiv.org/abs/2502.06560v1"}
{"created":"2025-02-10 15:25:06","title":"Can We Trust AI Benchmarks? An Interdisciplinary Review of Current Issues in AI Evaluation","abstract":"Quantitative Artificial Intelligence (AI) Benchmarks have emerged as fundamental tools for evaluating the performance, capability, and safety of AI models and systems. Currently, they shape the direction of AI development and are playing an increasingly prominent role in regulatory frameworks. As their influence grows, however, so too does concerns about how and with what effects they evaluate highly sensitive topics such as capabilities, including high-impact capabilities, safety and systemic risks. This paper presents an interdisciplinary meta-review of about 100 studies that discuss shortcomings in quantitative benchmarking practices, published in the last 10 years. It brings together many fine-grained issues in the design and application of benchmarks (such as biases in dataset creation, inadequate documentation, data contamination, and failures to distinguish signal from noise) with broader sociotechnical issues (such as an over-focus on evaluating text-based AI models according to one-time testing logic that fails to account for how AI models are increasingly multimodal and interact with humans and other technical systems). Our review also highlights a series of systemic flaws in current benchmarking practices, such as misaligned incentives, construct validity issues, unknown unknowns, and problems with the gaming of benchmark results. Furthermore, it underscores how benchmark practices are fundamentally shaped by cultural, commercial and competitive dynamics that often prioritise state-of-the-art performance at the expense of broader societal concerns. By providing an overview of risks associated with existing benchmarking procedures, we problematise disproportionate trust placed in benchmarks and contribute to ongoing efforts to improve the accountability and relevance of quantitative AI benchmarks within the complexities of real-world scenarios.","sentences":["Quantitative Artificial Intelligence (AI) Benchmarks have emerged as fundamental tools for evaluating the performance, capability, and safety of AI models and systems.","Currently, they shape the direction of AI development and are playing an increasingly prominent role in regulatory frameworks.","As their influence grows, however, so too does concerns about how and with what effects they evaluate highly sensitive topics such as capabilities, including high-impact capabilities, safety and systemic risks.","This paper presents an interdisciplinary meta-review of about 100 studies that discuss shortcomings in quantitative benchmarking practices, published in the last 10 years.","It brings together many fine-grained issues in the design and application of benchmarks (such as biases in dataset creation, inadequate documentation, data contamination, and failures to distinguish signal from noise) with broader sociotechnical issues (such as an over-focus on evaluating text-based AI models according to one-time testing logic that fails to account for how AI models are increasingly multimodal and interact with humans and other technical systems).","Our review also highlights a series of systemic flaws in current benchmarking practices, such as misaligned incentives, construct validity issues, unknown unknowns, and problems with the gaming of benchmark results.","Furthermore, it underscores how benchmark practices are fundamentally shaped by cultural, commercial and competitive dynamics that often prioritise state-of-the-art performance at the expense of broader societal concerns.","By providing an overview of risks associated with existing benchmarking procedures, we problematise disproportionate trust placed in benchmarks and contribute to ongoing efforts to improve the accountability and relevance of quantitative AI benchmarks within the complexities of real-world scenarios."],"url":"http://arxiv.org/abs/2502.06559v1"}
{"created":"2025-02-10 15:24:56","title":"On the FirstFit Algorithm for Online Unit-Interval Coloring","abstract":"In this paper, we study the performance of the FirstFit algorithm for the online unit-length intervals coloring problem where the intervals can be either open or closed, which serves a further investigation towards the actual performance of FirstFit. We develop a sophisticated counting method by generalizing the classic neighborhood bound, which limits the color FirstFit can assign an interval by counting the potential intersections. In the generalization, we show that for any interval, there is a critical interval intersecting it that can help reduce the overestimation of the number of intersections, and it further helps bound the color an interval can be assigned. The technical challenge then falls on identifying these critical intervals that guarantee the effectiveness of counting. Using this new mechanism for bounding the color that FirstFit can assign an interval, we provide a tight analysis of $2\\omega$ colors when all intervals have integral endpoints and an upper bound of $\\lceil\\frac{7}{3}\\omega\\rceil-2$ colors for the general case, where $\\omega$ is the optimal number of colors needed for the input set of intervals.","sentences":["In this paper, we study the performance of the FirstFit algorithm for the online unit-length intervals coloring problem where the intervals can be either open or closed, which serves a further investigation towards the actual performance of FirstFit.","We develop a sophisticated counting method by generalizing the classic neighborhood bound, which limits the color FirstFit can assign an interval by counting the potential intersections.","In the generalization, we show that for any interval, there is a critical interval intersecting it that can help reduce the overestimation of the number of intersections, and it further helps bound the color an interval can be assigned.","The technical challenge then falls on identifying these critical intervals that guarantee the effectiveness of counting.","Using this new mechanism for bounding the color that FirstFit can assign an interval, we provide a tight analysis of $2\\omega$ colors when all intervals have integral endpoints and an upper bound of $\\lceil\\frac{7}{3}\\omega\\rceil-2$ colors for the general case, where $\\omega$ is the optimal number of colors needed for the input set of intervals."],"url":"http://arxiv.org/abs/2502.06558v1"}
{"created":"2025-02-10 15:23:52","title":"Is API Access to LLMs Useful for Generating Private Synthetic Tabular Data?","abstract":"Differentially private (DP) synthetic data is a versatile tool for enabling the analysis of private data. Recent advancements in large language models (LLMs) have inspired a number of algorithm techniques for improving DP synthetic data generation. One family of approaches uses DP finetuning on the foundation model weights; however, the model weights for state-of-the-art models may not be public. In this work we propose two DP synthetic tabular data algorithms that only require API access to the foundation model. We adapt the Private Evolution algorithm (Lin et al., 2023; Xie et al., 2024) -- which was designed for image and text data -- to the tabular data domain. In our extension of Private Evolution, we define a query workload-based distance measure, which may be of independent interest. We propose a family of algorithms that use one-shot API access to LLMs, rather than adaptive queries to the LLM. Our findings reveal that API-access to powerful LLMs does not always improve the quality of DP synthetic data compared to established baselines that operate without such access. We provide insights into the underlying reasons and propose improvements to LLMs that could make them more effective for this application.","sentences":["Differentially private (DP) synthetic data is a versatile tool for enabling the analysis of private data.","Recent advancements in large language models (LLMs) have inspired a number of algorithm techniques for improving DP synthetic data generation.","One family of approaches uses DP finetuning on the foundation model weights; however, the model weights for state-of-the-art models may not be public.","In this work we propose two DP synthetic tabular data algorithms that only require API access to the foundation model.","We adapt the Private Evolution algorithm (Lin et al., 2023; Xie et al., 2024) -- which was designed for image and text data -- to the tabular data domain.","In our extension of Private Evolution, we define a query workload-based distance measure, which may be of independent interest.","We propose a family of algorithms that use one-shot API access to LLMs, rather than adaptive queries to the LLM.","Our findings reveal that API-access to powerful LLMs does not always improve the quality of DP synthetic data compared to established baselines that operate without such access.","We provide insights into the underlying reasons and propose improvements to LLMs that could make them more effective for this application."],"url":"http://arxiv.org/abs/2502.06555v1"}
{"created":"2025-02-10 15:20:07","title":"Diffusion Models for Computational Neuroimaging: A Survey","abstract":"Computational neuroimaging involves analyzing brain images or signals to provide mechanistic insights and predictive tools for human cognition and behavior. While diffusion models have shown stability and high-quality generation in natural images, there is increasing interest in adapting them to analyze brain data for various neurological tasks such as data enhancement, disease diagnosis and brain decoding. This survey provides an overview of recent efforts to integrate diffusion models into computational neuroimaging. We begin by introducing the common neuroimaging data modalities, follow with the diffusion formulations and conditioning mechanisms. Then we discuss how the variations of the denoising starting point, condition input and generation target of diffusion models are developed and enhance specific neuroimaging tasks. For a comprehensive overview of the ongoing research, we provide a publicly available repository at https://github.com/JoeZhao527/dm4neuro.","sentences":["Computational neuroimaging involves analyzing brain images or signals to provide mechanistic insights and predictive tools for human cognition and behavior.","While diffusion models have shown stability and high-quality generation in natural images, there is increasing interest in adapting them to analyze brain data for various neurological tasks such as data enhancement, disease diagnosis and brain decoding.","This survey provides an overview of recent efforts to integrate diffusion models into computational neuroimaging.","We begin by introducing the common neuroimaging data modalities, follow with the diffusion formulations and conditioning mechanisms.","Then we discuss how the variations of the denoising starting point, condition input and generation target of diffusion models are developed and enhance specific neuroimaging tasks.","For a comprehensive overview of the ongoing research, we provide a publicly available repository at https://github.com/JoeZhao527/dm4neuro."],"url":"http://arxiv.org/abs/2502.06552v1"}
{"created":"2025-02-10 15:09:29","title":"Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos","abstract":"Zebrafish are widely used in biomedical research and developmental stages of their embryos often need to be synchronized for further analysis. We present an unsupervised approach to extract descriptive features from 3D+t point clouds of zebrafish embryos and subsequently use those features to temporally align corresponding developmental stages. An autoencoder architecture is proposed to learn a descriptive representation of the point clouds and we designed a deep regression network for their temporal alignment. We achieve a high alignment accuracy with an average mismatch of only 3.83 minutes over an experimental duration of 5.3 hours. As a fully-unsupervised approach, there is no manual labeling effort required and unlike manual analyses the method easily scales. Besides, the alignment without human annotation of the data also avoids any influence caused by subjective bias.","sentences":["Zebrafish are widely used in biomedical research and developmental stages of their embryos often need to be synchronized for further analysis.","We present an unsupervised approach to extract descriptive features from 3D+t point clouds of zebrafish embryos and subsequently use those features to temporally align corresponding developmental stages.","An autoencoder architecture is proposed to learn a descriptive representation of the point clouds and we designed a deep regression network for their temporal alignment.","We achieve a high alignment accuracy with an average mismatch of only 3.83 minutes over an experimental duration of 5.3 hours.","As a fully-unsupervised approach, there is no manual labeling effort required and unlike manual analyses the method easily scales.","Besides, the alignment without human annotation of the data also avoids any influence caused by subjective bias."],"url":"http://arxiv.org/abs/2502.06543v1"}
{"created":"2025-02-10 15:07:35","title":"Decentralizing Trust: Consortium Blockchains and Hyperledger Fabric Explained","abstract":"Trust models are essential components of networks of any nature, as they refer to confidence frameworks to evaluate and verify if their participants act reliably and fairly. They are necessary to any social, organizational, or computer network model to ensure truthful interactions, data integrity, and overall system resilience. Trust models can be centralized or distributed, each providing a good fair of benefits and challenges. Blockchain is a special case of distributed trust models that utilize advanced cryptographic techniques and decentralized consensus mechanisms to enforce confidence among participants within a network. In this piece, we provide an overview of blockchain networks from the trust model perspective, with a special focus on the Hyperledger Fabric framework, a widespread blockchain implementation with a consortium architecture. We explore Fabric in detail, including its trust model, components, overall architecture, and a general implementation blueprint for the platform. We intend to offer readers with technical backgrounds but not necessarily experts in the blockchain field a friendly review of these topics to spark their curiosity to continue expanding their knowledge on these increasingly popular technologies.","sentences":["Trust models are essential components of networks of any nature, as they refer to confidence frameworks to evaluate and verify if their participants act reliably and fairly.","They are necessary to any social, organizational, or computer network model to ensure truthful interactions, data integrity, and overall system resilience.","Trust models can be centralized or distributed, each providing a good fair of benefits and challenges.","Blockchain is a special case of distributed trust models that utilize advanced cryptographic techniques and decentralized consensus mechanisms to enforce confidence among participants within a network.","In this piece, we provide an overview of blockchain networks from the trust model perspective, with a special focus on the Hyperledger Fabric framework, a widespread blockchain implementation with a consortium architecture.","We explore Fabric in detail, including its trust model, components, overall architecture, and a general implementation blueprint for the platform.","We intend to offer readers with technical backgrounds but not necessarily experts in the blockchain field a friendly review of these topics to spark their curiosity to continue expanding their knowledge on these increasingly popular technologies."],"url":"http://arxiv.org/abs/2502.06540v1"}
{"created":"2025-02-10 14:47:41","title":"Approximation Algorithms for Optimal Hopsets","abstract":"For a given graph $G$, a \"hopset\" $H$ with hopbound $\\beta$ and stretch $\\alpha$ is a set of edges such that between every pair of vertices $u$ and $v$, there is a path with at most $\\beta$ hops in $G \\cup H$ that approximates the distance between $u$ and $v$ up to a multiplicative stretch of $\\alpha$. Hopsets have found a wide range of applications for distance-based problems in various computational models since the 90s. More recently, there has been significant interest in understanding these fundamental objects from an existential and structural perspective. But all of this work takes a worst-case (or existential) point of view: How many edges do we need to add to satisfy a given hopbound and stretch requirement for any input graph?   We initiate the study of the natural optimization variant of this problem: given a specific graph instance, what is the minimum number of edges that satisfy the hopbound and stretch requirements? We give approximation algorithms for a generalized hopset problem which, when combined with known existential bounds, lead to different approximation guarantees for various regimes depending on hopbound, stretch, and directed vs. undirected inputs. We complement our upper bounds with a lower bound that implies Label Cover hardness for directed hopsets and shortcut sets with hopbound at least $3$.","sentences":["For a given graph $G$, a \"hopset\" $H$ with hopbound $\\beta$ and stretch $\\alpha$ is a set of edges such that between every pair of vertices $u$ and $v$, there is a path with at most $\\beta$ hops in $G \\cup H$ that approximates the distance between $u$ and $v$ up to a multiplicative stretch of $\\alpha$. Hopsets have found a wide range of applications for distance-based problems in various computational models since the 90s.","More recently, there has been significant interest in understanding these fundamental objects from an existential and structural perspective.","But all of this work takes a worst-case (or existential) point of view: How many edges do we need to add to satisfy a given hopbound and stretch requirement for any input graph?   ","We initiate the study of the natural optimization variant of this problem: given a specific graph instance, what is the minimum number of edges that satisfy the hopbound and stretch requirements?","We give approximation algorithms for a generalized hopset problem which, when combined with known existential bounds, lead to different approximation guarantees for various regimes depending on hopbound, stretch, and directed vs. undirected inputs.","We complement our upper bounds with a lower bound that implies Label Cover hardness for directed hopsets and shortcut sets with hopbound at least $3$."],"url":"http://arxiv.org/abs/2502.06522v1"}
{"created":"2025-02-10 14:43:15","title":"Sentient: Multi-Scenario Behavioral Intent Analysis for Advanced Persistent Threat Detection","abstract":"Advanced Persistent Threats (APTs) are challenging to detect due to their complexity and stealth. To mitigate such attacks, many approaches utilize provenance graphs to model entities and their dependencies, detecting the covert and persistent nature of APTs. However, existing methods face several challenges: 1) Environmental noise hinders precise detection; 2) Reliance on hard-to-obtain labeled data and prior knowledge of APTs limits their ability to detect unknown threats; 3) The difficulty in capturing long-range interaction dependencies, leading to the loss of critical context. We propose Sentient, a threat detection system based on behavioral intent analysis that detects node-level threats from audit logs. Sentient constructs a provenance graph from the audit logs and uses this graph to build multiple scenarios. By combining graph comprehension with multiple scenario comprehension, Sentient learns normal interaction behaviors. Sentient detects anomalies by identifying interactions that deviate from the established behavior patterns. We evaluated Sentient on three widely used datasets covering both real-world and simulated attacks. The results confirm that Sentient consistently delivers strong detection performance. Notably, Sentient achieves entity-level APT detection with a precision of 96% and a recall of 99%.","sentences":["Advanced Persistent Threats (APTs) are challenging to detect due to their complexity and stealth.","To mitigate such attacks, many approaches utilize provenance graphs to model entities and their dependencies, detecting the covert and persistent nature of APTs.","However, existing methods face several challenges: 1) Environmental noise hinders precise detection; 2) Reliance on hard-to-obtain labeled data and prior knowledge of APTs limits their ability to detect unknown threats; 3) The difficulty in capturing long-range interaction dependencies, leading to the loss of critical context.","We propose Sentient, a threat detection system based on behavioral intent analysis that detects node-level threats from audit logs.","Sentient constructs a provenance graph from the audit logs and uses this graph to build multiple scenarios.","By combining graph comprehension with multiple scenario comprehension, Sentient learns normal interaction behaviors.","Sentient detects anomalies by identifying interactions that deviate from the established behavior patterns.","We evaluated Sentient on three widely used datasets covering both real-world and simulated attacks.","The results confirm that Sentient consistently delivers strong detection performance.","Notably, Sentient achieves entity-level APT detection with a precision of 96% and a recall of 99%."],"url":"http://arxiv.org/abs/2502.06521v1"}
{"created":"2025-02-10 14:37:26","title":"Boost-and-Skip: A Simple Guidance-Free Diffusion for Minority Generation","abstract":"Minority samples are underrepresented instances located in low-density regions of a data manifold, and are valuable in many generative AI applications, such as data augmentation, creative content generation, etc. Unfortunately, existing diffusion-based minority generators often rely on computationally expensive guidance dedicated for minority generation. To address this, here we present a simple yet powerful guidance-free approach called Boost-and-Skip for generating minority samples using diffusion models. The key advantage of our framework requires only two minimal changes to standard generative processes: (i) variance-boosted initialization and (ii) timestep skipping. We highlight that these seemingly-trivial modifications are supported by solid theoretical and empirical evidence, thereby effectively promoting emergence of underrepresented minority features. Our comprehensive experiments demonstrate that Boost-and-Skip greatly enhances the capability of generating minority samples, even rivaling guidance-based state-of-the-art approaches while requiring significantly fewer computations.","sentences":["Minority samples are underrepresented instances located in low-density regions of a data manifold, and are valuable in many generative AI applications, such as data augmentation, creative content generation, etc.","Unfortunately, existing diffusion-based minority generators often rely on computationally expensive guidance dedicated for minority generation.","To address this, here we present a simple yet powerful guidance-free approach called Boost-and-Skip for generating minority samples using diffusion models.","The key advantage of our framework requires only two minimal changes to standard generative processes: (i) variance-boosted initialization and (ii) timestep skipping.","We highlight that these seemingly-trivial modifications are supported by solid theoretical and empirical evidence, thereby effectively promoting emergence of underrepresented minority features.","Our comprehensive experiments demonstrate that Boost-and-Skip greatly enhances the capability of generating minority samples, even rivaling guidance-based state-of-the-art approaches while requiring significantly fewer computations."],"url":"http://arxiv.org/abs/2502.06516v1"}
{"created":"2025-02-10 14:25:37","title":"A Resilient and Energy-Efficient Smart Metering Infrastructure Utilizing a Self-Organizing UAV Swarm","abstract":"The smart metering infrastructure may become one of the key elements in efficiently managing energy in smart cities. At the same time, traditional measurement record collection is performed by manual methods, which raises cost, safety, and accuracy issues. This paper proposes an innovative SMI architecture based on an unmanned aerial vehicle swarm organizing itself for the autonomous data collection in smart metering infrastructure with scalability and cost-effectiveness while minimizing risks. We design an architecture-based comprehensive system with various phases of operation, communication protocols, and robust failure-handling mechanisms to ensure reliable operations. We further perform extensive simulations in maintenance of precise formations during flight, efficient data collection from smart meters, and adaptation to various failure scenarios. Importantly, we analyze the energy consumption of the proposed system in both drone flight operations and network communication. We now propose a battery sizing strategy and provide an estimate of the operational lifetime of the swarm, underlining the feasibility and practicality of our approach. Our results show that UAV swarms have great potential to revolutionize smart metering and to bring a further brick to greener and more resilient smart cities.","sentences":["The smart metering infrastructure may become one of the key elements in efficiently managing energy in smart cities.","At the same time, traditional measurement record collection is performed by manual methods, which raises cost, safety, and accuracy issues.","This paper proposes an innovative SMI architecture based on an unmanned aerial vehicle swarm organizing itself for the autonomous data collection in smart metering infrastructure with scalability and cost-effectiveness while minimizing risks.","We design an architecture-based comprehensive system with various phases of operation, communication protocols, and robust failure-handling mechanisms to ensure reliable operations.","We further perform extensive simulations in maintenance of precise formations during flight, efficient data collection from smart meters, and adaptation to various failure scenarios.","Importantly, we analyze the energy consumption of the proposed system in both drone flight operations and network communication.","We now propose a battery sizing strategy and provide an estimate of the operational lifetime of the swarm, underlining the feasibility and practicality of our approach.","Our results show that UAV swarms have great potential to revolutionize smart metering and to bring a further brick to greener and more resilient smart cities."],"url":"http://arxiv.org/abs/2502.06508v1"}
{"created":"2025-02-10 14:20:13","title":"An Efficient Security Model for Industrial Internet of Things (IIoT) System Based on Machine Learning Principles","abstract":"This paper presents a security paradigm for edge devices to defend against various internal and external threats. The first section of the manuscript proposes employing machine learning models to identify MQTT-based (Message Queue Telemetry Transport) attacks using the Intrusion Detection and Prevention System (IDPS) for edge nodes. Because the Machine Learning (ML) model cannot be trained directly on low-performance platforms (such as edge devices),a new methodology for updating ML models is proposed to provide a tradeoff between the model performance and the computational complexity. The proposed methodology involves training the model on a high-performance computing platform and then installing the trained model as a detection engine on low-performance platforms (such as the edge node of the edge layer) to identify new attacks. Multiple security techniques have been employed in the second half of the manuscript to verify that the exchanged trained model and the exchanged data files are valid and undiscoverable (information authenticity and privacy) and that the source (such as a fog node or edge device) is indeed what it it claimed to be (source authentication and message integrity). Finally, the proposed security paradigm is found to be effective against various internal and external threats and can be applied to a low-cost single-board computer (SBC).","sentences":["This paper presents a security paradigm for edge devices to defend against various internal and external threats.","The first section of the manuscript proposes employing machine learning models to identify MQTT-based (Message Queue Telemetry Transport) attacks using the Intrusion Detection and Prevention System (IDPS) for edge nodes.","Because the Machine Learning (ML) model cannot be trained directly on low-performance platforms (such as edge devices),a new methodology for updating ML models is proposed to provide a tradeoff between the model performance and the computational complexity.","The proposed methodology involves training the model on a high-performance computing platform and then installing the trained model as a detection engine on low-performance platforms (such as the edge node of the edge layer) to identify new attacks.","Multiple security techniques have been employed in the second half of the manuscript to verify that the exchanged trained model and the exchanged data files are valid and undiscoverable (information authenticity and privacy) and that the source (such as a fog node or edge device) is indeed what it it claimed to be (source authentication and message integrity).","Finally, the proposed security paradigm is found to be effective against various internal and external threats and can be applied to a low-cost single-board computer (SBC)."],"url":"http://arxiv.org/abs/2502.06502v1"}
{"created":"2025-02-10 14:20:01","title":"Learning Clustering-based Prototypes for Compositional Zero-shot Learning","abstract":"Learning primitive (i.e., attribute and object) concepts from seen compositions is the primary challenge of Compositional Zero-Shot Learning (CZSL). Existing CZSL solutions typically rely on oversimplified data assumptions, e.g., modeling each primitive with a single centroid primitive representation, ignoring the natural diversities of the attribute (resp. object) when coupled with different objects (resp. attribute). In this work, we develop ClusPro, a robust clustering-based prototype mining framework for CZSL that defines the conceptual boundaries of primitives through a set of diversified prototypes. Specifically, ClusPro conducts within-primitive clustering on the embedding space for automatically discovering and dynamically updating prototypes. These representative prototypes are subsequently used to repaint a well-structured and independent primitive embedding space, ensuring intra-primitive separation and inter-primitive decorrelation through prototype-based contrastive learning and decorrelation learning. Moreover, ClusPro efficiently performs prototype clustering in a non-parametric fashion without the introduction of additional learnable parameters or computational budget during testing. Experiments on three benchmarks demonstrate ClusPro outperforms various top-leading CZSL solutions under both closed-world and open-world settings.","sentences":["Learning primitive (i.e., attribute and object) concepts from seen compositions is the primary challenge of Compositional Zero-Shot Learning (CZSL).","Existing CZSL solutions typically rely on oversimplified data assumptions, e.g., modeling each primitive with a single centroid primitive representation, ignoring the natural diversities of the attribute (resp.","object) when coupled with different objects (resp. attribute).","In this work, we develop ClusPro, a robust clustering-based prototype mining framework for CZSL that defines the conceptual boundaries of primitives through a set of diversified prototypes.","Specifically, ClusPro conducts within-primitive clustering on the embedding space for automatically discovering and dynamically updating prototypes.","These representative prototypes are subsequently used to repaint a well-structured and independent primitive embedding space, ensuring intra-primitive separation and inter-primitive decorrelation through prototype-based contrastive learning and decorrelation learning.","Moreover, ClusPro efficiently performs prototype clustering in a non-parametric fashion without the introduction of additional learnable parameters or computational budget during testing.","Experiments on three benchmarks demonstrate ClusPro outperforms various top-leading CZSL solutions under both closed-world and open-world settings."],"url":"http://arxiv.org/abs/2502.06501v1"}
{"created":"2025-02-10 14:16:51","title":"Decision Boundary Optimization-Informed Domain Adaptation","abstract":"Maximum Mean Discrepancy (MMD) is widely used in a number of domain adaptation (DA) methods and shows its effectiveness in aligning data distributions across domains. However, in previous DA research, MMD-based DA methods focus mostly on distribution alignment, and ignore to optimize the decision boundary for classification-aware DA, thereby falling short in reducing the DA upper error bound. In this paper, we propose a strengthened MMD measurement, namely, Decision Boundary optimization-informed MMD (DB-MMD), which enables MMD to carefully take into account the decision boundaries, thereby simultaneously optimizing the distribution alignment and cross-domain classifier within a hybrid framework, and leading to a theoretical bound guided DA. We further seamlessly embed the proposed DB-MMD measurement into several popular DA methods, e.g., MEDA, DGA-DA, to demonstrate its effectiveness w.r.t different experimental settings. We carry out comprehensive experiments using 8 standard DA datasets. The experimental results show that the DB-MMD enforced DA methods improve their baseline models using plain vanilla MMD, with a margin that can be as high as 9.5.","sentences":["Maximum Mean Discrepancy (MMD) is widely used in a number of domain adaptation (DA) methods and shows its effectiveness in aligning data distributions across domains.","However, in previous DA research, MMD-based DA methods focus mostly on distribution alignment, and ignore to optimize the decision boundary for classification-aware DA, thereby falling short in reducing the DA upper error bound.","In this paper, we propose a strengthened MMD measurement, namely, Decision Boundary optimization-informed MMD (DB-MMD), which enables MMD to carefully take into account the decision boundaries, thereby simultaneously optimizing the distribution alignment and cross-domain classifier within a hybrid framework, and leading to a theoretical bound guided DA.","We further seamlessly embed the proposed DB-MMD measurement into several popular DA methods, e.g., MEDA, DGA-DA, to demonstrate its effectiveness w.r.t different experimental settings.","We carry out comprehensive experiments using 8 standard DA datasets.","The experimental results show that the DB-MMD enforced DA methods improve their baseline models using plain vanilla MMD, with a margin that can be as high as 9.5."],"url":"http://arxiv.org/abs/2502.06498v1"}
{"created":"2025-02-10 14:11:29","title":"EdgeMLBalancer: A Self-Adaptive Approach for Dynamic Model Switching on Resource-Constrained Edge Devices","abstract":"The widespread adoption of machine learning on edge devices, such as mobile phones, laptops, IoT devices, etc., has enabled real-time AI applications in resource-constrained environments. Existing solutions for managing computational resources often focus narrowly on accuracy or energy efficiency, failing to adapt dynamically to varying workloads. Furthermore, the existing system lack robust mechanisms to adaptively balance CPU utilization, leading to inefficiencies in resource-constrained scenarios like real-time traffic monitoring. To address these limitations, we propose a self-adaptive approach that optimizes CPU utilization and resource management on edge devices. Our approach, EdgeMLBalancer balances between models through dynamic switching, guided by real-time CPU usage monitoring across processor cores. Tested on real-time traffic data, the approach adapts object detection models based on CPU usage, ensuring efficient resource utilization. The approach leverages epsilon-greedy strategy which promotes fairness and prevents resource starvation, maintaining system robustness. The results of our evaluation demonstrate significant improvements by balancing computational efficiency and accuracy, highlighting the approach's ability to adapt seamlessly to varying workloads. This work lays the groundwork for further advancements in self-adaptation for resource-constrained environments.","sentences":["The widespread adoption of machine learning on edge devices, such as mobile phones, laptops, IoT devices, etc., has enabled real-time AI applications in resource-constrained environments.","Existing solutions for managing computational resources often focus narrowly on accuracy or energy efficiency, failing to adapt dynamically to varying workloads.","Furthermore, the existing system lack robust mechanisms to adaptively balance CPU utilization, leading to inefficiencies in resource-constrained scenarios like real-time traffic monitoring.","To address these limitations, we propose a self-adaptive approach that optimizes CPU utilization and resource management on edge devices.","Our approach, EdgeMLBalancer balances between models through dynamic switching, guided by real-time CPU usage monitoring across processor cores.","Tested on real-time traffic data, the approach adapts object detection models based on CPU usage, ensuring efficient resource utilization.","The approach leverages epsilon-greedy strategy which promotes fairness and prevents resource starvation, maintaining system robustness.","The results of our evaluation demonstrate significant improvements by balancing computational efficiency and accuracy, highlighting the approach's ability to adapt seamlessly to varying workloads.","This work lays the groundwork for further advancements in self-adaptation for resource-constrained environments."],"url":"http://arxiv.org/abs/2502.06493v1"}
{"created":"2025-02-10 14:08:55","title":"Model-Based Offline Reinforcement Learning with Reliability-Guaranteed Sequence Modeling","abstract":"Model-based offline reinforcement learning (MORL) aims to learn a policy by exploiting a dynamics model derived from an existing dataset. Applying conservative quantification to the dynamics model, most existing works on MORL generate trajectories that approximate the real data distribution to facilitate policy learning by using current information (e.g., the state and action at time step $t$). However, these works neglect the impact of historical information on environmental dynamics, leading to the generation of unreliable trajectories that may not align with the real data distribution. In this paper, we propose a new MORL algorithm \\textbf{R}eliability-guaranteed \\textbf{T}ransformer (RT), which can eliminate unreliable trajectories by calculating the cumulative reliability of the generated trajectory (i.e., using a weighted variational distance away from the real data). Moreover, by sampling candidate actions with high rewards, RT can efficiently generate high-return trajectories from the existing offline data. We theoretically prove the performance guarantees of RT in policy learning, and empirically demonstrate its effectiveness against state-of-the-art model-based methods on several benchmark tasks.","sentences":["Model-based offline reinforcement learning (MORL) aims to learn a policy by exploiting a dynamics model derived from an existing dataset.","Applying conservative quantification to the dynamics model, most existing works on MORL generate trajectories that approximate the real data distribution to facilitate policy learning by using current information (e.g., the state and action at time step $t$).","However, these works neglect the impact of historical information on environmental dynamics, leading to the generation of unreliable trajectories that may not align with the real data distribution.","In this paper, we propose a new MORL algorithm \\textbf{R}eliability-guaranteed \\textbf{T}ransformer (RT), which can eliminate unreliable trajectories by calculating the cumulative reliability of the generated trajectory (i.e., using a weighted variational distance away from the real data).","Moreover, by sampling candidate actions with high rewards, RT can efficiently generate high-return trajectories from the existing offline data.","We theoretically prove the performance guarantees of RT in policy learning, and empirically demonstrate its effectiveness against state-of-the-art model-based methods on several benchmark tasks."],"url":"http://arxiv.org/abs/2502.06491v1"}
{"created":"2025-02-10 13:55:59","title":"ARRIVAL: Recursive Framework & $\\ell_1$-Contraction","abstract":"ARRIVAL is the problem of deciding which out of two possible destinations will be reached first by a token that moves deterministically along the edges of a directed graph, according to so-called switching rules. It is known to lie in NP $\\cap$ CoNP, but not known to lie in P. The state-of-the-art algorithm due to G\\\"artner et al. (ICALP `21) runs in time $2^{\\mathcal{O}(\\sqrt{n} \\log n)}$ on an $n$-vertex graph.   We prove that ARRIVAL can be solved in time $2^{\\mathcal{O}(k \\log^2 n)}$ on $n$-vertex graphs of treewidth $k$. Our algorithm is derived by adapting a simple recursive algorithm for a generalization of ARRIVAL called G-ARRIVAL. This simple recursive algorithm acts as a framework from which we can also rederive the subexponential upper bound of G\\\"artner et al.   Our second result is a reduction from G-ARRIVAL to the problem of finding an approximate fixed point of an $\\ell_1$-contracting function $f : [0, 1]^n \\rightarrow [0, 1]^n$. Finding such fixed points is a well-studied problem in the case of the $\\ell_2$-metric and the $\\ell_\\infty$-metric, but little is known about the $\\ell_1$-case.   Both of our results highlight parallels between ARRIVAL and the Simple Stochastic Games (SSG) problem. Concretely, Chatterjee et al. (SODA `23) gave an algorithm for SSG parameterized by treewidth that achieves a similar bound as we do for ARRIVAL, and SSG is known to reduce to $\\ell_\\infty$-contraction.","sentences":["ARRIVAL is the problem of deciding which out of two possible destinations will be reached first by a token that moves deterministically along the edges of a directed graph, according to so-called switching rules.","It is known to lie in NP $\\cap$ CoNP, but not known to lie in P. The state-of-the-art algorithm due to G\\\"artner et al.","(ICALP `21) runs in time $2^{\\mathcal{O}(\\sqrt{n} \\log n)}$ on an $n$-vertex graph.   ","We prove that ARRIVAL can be solved in time $2^{\\mathcal{O}(k \\log^2 n)}$ on $n$-vertex graphs of treewidth $k$.","Our algorithm is derived by adapting a simple recursive algorithm for a generalization of ARRIVAL called G-ARRIVAL.","This simple recursive algorithm acts as a framework from which we can also rederive the subexponential upper bound of G\\\"artner et al.   ","Our second result is a reduction from G-ARRIVAL to the problem of finding an approximate fixed point of an $\\ell_1$-contracting function $f :","[0, 1]^n \\rightarrow [0, 1]^n$. Finding such fixed points is a well-studied problem in the case of the $\\ell_2$-metric and the $\\ell_\\infty$-metric, but little is known about the $\\ell_1$-case.   ","Both of our results highlight parallels between ARRIVAL and the Simple Stochastic Games (SSG) problem.","Concretely, Chatterjee et al. (SODA `23) gave an algorithm for SSG parameterized by treewidth that achieves a similar bound as we do for ARRIVAL, and SSG is known to reduce to $\\ell_\\infty$-contraction."],"url":"http://arxiv.org/abs/2502.06477v1"}
{"created":"2025-02-10 13:50:12","title":"Beyond Literal Token Overlap: Token Alignability for Multilinguality","abstract":"Previous work has considered token overlap, or even similarity of token distributions, as predictors for multilinguality and cross-lingual knowledge transfer in language models. However, these very literal metrics assign large distances to language pairs with different scripts, which can nevertheless show good cross-linguality. This limits the explanatory strength of token overlap for knowledge transfer between language pairs that use distinct scripts or follow different orthographic conventions. In this paper, we propose subword token alignability as a new way to understand the impact and quality of multilingual tokenisation. In particular, this metric predicts multilinguality much better when scripts are disparate and the overlap of literal tokens is low. We analyse this metric in the context of both encoder and decoder models, look at data size as a potential distractor, and discuss how this insight may be applied to multilingual tokenisation in future work. We recommend our subword token alignability metric for identifying optimal language pairs for cross-lingual transfer, as well as to guide the construction of better multilingual tokenisers in the future. We publish our code and reproducibility details.","sentences":["Previous work has considered token overlap, or even similarity of token distributions, as predictors for multilinguality and cross-lingual knowledge transfer in language models.","However, these very literal metrics assign large distances to language pairs with different scripts, which can nevertheless show good cross-linguality.","This limits the explanatory strength of token overlap for knowledge transfer between language pairs that use distinct scripts or follow different orthographic conventions.","In this paper, we propose subword token alignability as a new way to understand the impact and quality of multilingual tokenisation.","In particular, this metric predicts multilinguality much better when scripts are disparate and the overlap of literal tokens is low.","We analyse this metric in the context of both encoder and decoder models, look at data size as a potential distractor, and discuss how this insight may be applied to multilingual tokenisation in future work.","We recommend our subword token alignability metric for identifying optimal language pairs for cross-lingual transfer, as well as to guide the construction of better multilingual tokenisers in the future.","We publish our code and reproducibility details."],"url":"http://arxiv.org/abs/2502.06468v1"}
{"created":"2025-02-10 13:45:55","title":"A Quadratic Lower Bound for Stable Roommates Solvability","abstract":"In their seminal work on the Stable Marriage Problem (SM), Gale and Shapley introduced a generalization of SM referred to as the Stable Roommates Problem (SR). An instance of SR consists of a set of $2n$ agents, and each agent has preferences in the form of a ranked list of all other agents. The goal is to find a one-to-one matching between the agents that is stable in the sense that no pair of agents have a mutual incentive to deviate from the matching. Unlike the (bipartite) stable marriage problem, in SR, stable matchings need not exist. Irving devised an algorithm that finds a stable matching or reports that none exists in $O(n^2)$ time. In their influential 1989 text, Gusfield and Irving posed the question of whether $\\Omega(n^2)$ time is required for SR solvability -- the task of determining if an SR instance admits a stable matching.   In this paper we provide an affirmative answer to Gusfield and Irving's question. We show that any (randomized) algorithm that determines SR solvability requires $\\Omega(n^2)$ adaptive Boolean queries to the agents' preferences (in expectation). Our argument follows from a reduction from the communication complexity of the set disjointness function. The query lower bound implies quadratic time lower bounds for Turing machines, and memory access lower bounds for random access machines. Thus, we establish that Irving's algorithm is optimal (up to a logarithmic factor) in a very strong sense.","sentences":["In their seminal work on the Stable Marriage Problem (SM), Gale and Shapley introduced a generalization of SM referred to as the Stable Roommates Problem (SR).","An instance of SR consists of a set of $2n$ agents, and each agent has preferences in the form of a ranked list of all other agents.","The goal is to find a one-to-one matching between the agents that is stable in the sense that no pair of agents have a mutual incentive to deviate from the matching.","Unlike the (bipartite) stable marriage problem, in SR, stable matchings need not exist.","Irving devised an algorithm that finds a stable matching or reports that none exists in $O(n^2)$ time.","In their influential 1989 text, Gusfield and Irving posed the question of whether $\\Omega(n^2)$ time is required for SR solvability -- the task of determining if an SR instance admits a stable matching.   ","In this paper we provide an affirmative answer to Gusfield and Irving's question.","We show that any (randomized) algorithm that determines SR solvability requires $\\Omega(n^2)$ adaptive Boolean queries to the agents' preferences (in expectation).","Our argument follows from a reduction from the communication complexity of the set disjointness function.","The query lower bound implies quadratic time lower bounds for Turing machines, and memory access lower bounds for random access machines.","Thus, we establish that Irving's algorithm is optimal (up to a logarithmic factor) in a very strong sense."],"url":"http://arxiv.org/abs/2502.06464v1"}
{"created":"2025-02-10 13:42:50","title":"New simple and fastest quicksort algorithm for equal keys","abstract":"This paper introduces a novel and efficient partitioning technique for quicksort, specifically designed for real-world data with duplicate elements (50-year-old problem). The method is referred to as \"equal quicksort\" or \"eqsort\". Based on the experimental findings, it has been determined that the newly developed algorithm, eqsort, is competitive with the best current implementations,such as fat partitioning algorithms and dual-pivot quicksort. This method offers several advantages over the commonly used dual-pivot method and pdqsort partitioning, making it a potential replacement.","sentences":["This paper introduces a novel and efficient partitioning technique for quicksort, specifically designed for real-world data with duplicate elements (50-year-old problem).","The method is referred to as \"equal quicksort\" or \"eqsort\".","Based on the experimental findings, it has been determined that the newly developed algorithm, eqsort, is competitive with the best current implementations,such as fat partitioning algorithms and dual-pivot quicksort.","This method offers several advantages over the commonly used dual-pivot method and pdqsort partitioning, making it a potential replacement."],"url":"http://arxiv.org/abs/2502.06461v1"}
{"created":"2025-02-10 13:41:11","title":"Maximum Coverage $k$-Antichains and Chains: A Greedy Approach","abstract":"Given an input acyclic digraph $G = (V,E)$ and a positive integer $k$, the problem of Maximum Coverage $k$-Antichains (resp., Chains) denoted as MA-$k$ (resp., MC-$k$) asks to find $k$ sets of pairwise unreachable vertices, known as antichains (resp., $k$ subsequences of paths, known as chains), maximizing the number of vertices covered by these antichains (resp. chains). While MC-$k$ has been recently solved in (almost) optimal $O(|E|^{1+o(1)})$ time [Kogan and Parter, ICALP 2022], the fastest known algorithm for MA-$k$ is a recent $(k|E|)^{1+o(1)}$-time solution [Kogan and Parter, ESA 2024] as well as a $1/2$ approximation running in $|E|^{1+o(1)}$ time in the same paper. In this paper, we leverage a paths-based proof of the Greene-Kleitmann (GK) theorem with the help of the greedy algorithm for set cover and recent advances on fast algorithms for flows and shortest paths to obtain the following results for MA-$k$:   - The first (exact) algorithm running in $|E|^{1+o(1)}$ time, hence independent in $k$.   - A randomized algorithm running in $\\tilde{O}(\\alpha_k|E|)$ time, where $\\alpha_k$ is the size of the optimal solution. That is, a near-linear parameterized running time, generalizing the result of [M\\\"akinen et al., ACM TALG] obtained for $k=1$.   - An approximation algorithm running in time $O(\\alpha_1^2|V| + (\\alpha_1+k)|E|)$ with approximation ratio of $(1-1/e) > 0.63 > 1/2$.   Our last two solutions rely on the use of greedy set cover, first exploited in [Felsner et al., Order 2003] for chains, which we now apply to antichains. We complement these results with two examples (one for chains and one for antichains) showing that, for every $k \\ge 2$, greedy misses a $1/4$ portion of the optimal coverage. We also show that greedy is a $\\Omega(\\log{|V|})$ factor away from minimality when required to cover all vertices: previously unknown for sets of chains or antichains.","sentences":["Given an input acyclic digraph $G = (V,E)$ and a positive integer $k$, the problem of Maximum Coverage $k$-Antichains (resp., Chains) denoted as MA-$k$ (resp., MC-$k$) asks to find $k$ sets of pairwise unreachable vertices, known as antichains (resp., $k$ subsequences of paths, known as chains), maximizing the number of vertices covered by these antichains (resp. chains).","While MC-$k$ has been recently solved in (almost) optimal $O(|E|^{1+o(1)})$ time [Kogan and Parter, ICALP 2022], the fastest known algorithm for MA-$k$ is a recent $(k|E|)^{1+o(1)}$-time solution [Kogan and Parter, ESA 2024] as well as a $1/2$ approximation running in $|E|^{1+o(1)}$ time in the same paper.","In this paper, we leverage a paths-based proof of the Greene-Kleitmann (GK) theorem with the help of the greedy algorithm for set cover and recent advances on fast algorithms for flows and shortest paths to obtain the following results for MA-$k$:   - The first (exact) algorithm running in $|E|^{1+o(1)}$ time, hence independent in $k$.   - A randomized algorithm running in $\\tilde{O}(\\alpha_k|E|)$ time, where $\\alpha_k$ is the size of the optimal solution.","That is, a near-linear parameterized running time, generalizing the result of [M\\\"akinen et al., ACM TALG] obtained for $k=1$.   - An approximation algorithm running in time $O(\\alpha_1^2|V|","+ (\\alpha_1+k)|E|)$ with approximation ratio of $(1-1/e) >","0.63 >","1/2$.   Our last two solutions rely on the use of greedy set cover, first exploited in [Felsner et al., Order 2003] for chains, which we now apply to antichains.","We complement these results with two examples (one for chains and one for antichains) showing that, for every $k \\ge 2$, greedy misses a $1/4$ portion of the optimal coverage.","We also show that greedy is a $\\Omega(\\log{|V|})$ factor away from minimality when required to cover all vertices: previously unknown for sets of chains or antichains."],"url":"http://arxiv.org/abs/2502.06459v1"}
{"created":"2025-02-10 13:19:30","title":"Low-dimensional Functions are Efficiently Learnable under Randomly Biased Distributions","abstract":"The problem of learning single index and multi index models has gained significant interest as a fundamental task in high-dimensional statistics. Many recent works have analysed gradient-based methods, particularly in the setting of isotropic data distributions, often in the context of neural network training. Such studies have uncovered precise characterisations of algorithmic sample complexity in terms of certain analytic properties of the target function, such as the leap, information, and generative exponents. These properties establish a quantitative separation between low and high complexity learning tasks. In this work, we show that high complexity cases are rare. Specifically, we prove that introducing a small random perturbation to the data distribution--via a random shift in the first moment--renders any Gaussian single index model as easy to learn as a linear function. We further extend this result to a class of multi index models, namely sparse Boolean functions, also known as Juntas.","sentences":["The problem of learning single index and multi index models has gained significant interest as a fundamental task in high-dimensional statistics.","Many recent works have analysed gradient-based methods, particularly in the setting of isotropic data distributions, often in the context of neural network training.","Such studies have uncovered precise characterisations of algorithmic sample complexity in terms of certain analytic properties of the target function, such as the leap, information, and generative exponents.","These properties establish a quantitative separation between low and high complexity learning tasks.","In this work, we show that high complexity cases are rare.","Specifically, we prove that introducing a small random perturbation to the data distribution--via a random shift in the first moment--renders any Gaussian single index model as easy to learn as a linear function.","We further extend this result to a class of multi index models, namely sparse Boolean functions, also known as Juntas."],"url":"http://arxiv.org/abs/2502.06443v1"}
{"created":"2025-02-10 13:11:40","title":"Rethinking Large-scale Dataset Compression: Shifting Focus From Labels to Images","abstract":"Dataset distillation and dataset pruning are two prominent techniques for compressing datasets to improve computational and storage efficiency. Despite their overlapping objectives, these approaches are rarely compared directly. Even within each field, the evaluation protocols are inconsistent across various methods, which complicates fair comparisons and hinders reproducibility. Considering these limitations, we introduce in this paper a benchmark that equitably evaluates methodologies across both distillation and pruning literatures. Notably, our benchmark reveals that in the mainstream dataset distillation setting for large-scale datasets, which heavily rely on soft labels from pre-trained models, even randomly selected subsets can achieve surprisingly competitive performance. This finding suggests that an overemphasis on soft labels may be diverting attention from the intrinsic value of the image data, while also imposing additional burdens in terms of generation, storage, and application. To address these issues, we propose a new framework for dataset compression, termed Prune, Combine, and Augment (PCA), which focuses on leveraging image data exclusively, relies solely on hard labels for evaluation, and achieves state-of-the-art performance in this setup. By shifting the emphasis back to the images, our benchmark and PCA framework pave the way for more balanced and accessible techniques in dataset compression research. Our code is available at: https://github.com/ArmandXiao/Rethinking-Dataset-Compression","sentences":["Dataset distillation and dataset pruning are two prominent techniques for compressing datasets to improve computational and storage efficiency.","Despite their overlapping objectives, these approaches are rarely compared directly.","Even within each field, the evaluation protocols are inconsistent across various methods, which complicates fair comparisons and hinders reproducibility.","Considering these limitations, we introduce in this paper a benchmark that equitably evaluates methodologies across both distillation and pruning literatures.","Notably, our benchmark reveals that in the mainstream dataset distillation setting for large-scale datasets, which heavily rely on soft labels from pre-trained models, even randomly selected subsets can achieve surprisingly competitive performance.","This finding suggests that an overemphasis on soft labels may be diverting attention from the intrinsic value of the image data, while also imposing additional burdens in terms of generation, storage, and application.","To address these issues, we propose a new framework for dataset compression, termed Prune, Combine, and Augment (PCA), which focuses on leveraging image data exclusively, relies solely on hard labels for evaluation, and achieves state-of-the-art performance in this setup.","By shifting the emphasis back to the images, our benchmark and PCA framework pave the way for more balanced and accessible techniques in dataset compression research.","Our code is available at: https://github.com/ArmandXiao/Rethinking-Dataset-Compression"],"url":"http://arxiv.org/abs/2502.06434v1"}
{"created":"2025-02-10 13:02:19","title":"Hybrid State-Space and GRU-based Graph Tokenization Mamba for Hyperspectral Image Classification","abstract":"Hyperspectral image (HSI) classification plays a pivotal role in domains such as environmental monitoring, agriculture, and urban planning. However, it faces significant challenges due to the high-dimensional nature of the data and the complex spectral-spatial relationships inherent in HSI. Traditional methods, including conventional machine learning and convolutional neural networks (CNNs), often struggle to effectively capture these intricate spectral-spatial features and global contextual information. Transformer-based models, while powerful in capturing long-range dependencies, often demand substantial computational resources, posing challenges in scenarios where labeled datasets are limited, as is commonly seen in HSI applications. To overcome these challenges, this work proposes GraphMamba, a hybrid model that combines spectral-spatial token generation, graph-based token prioritization, and cross-attention mechanisms. The model introduces a novel hybridization of state-space modeling and Gated Recurrent Units (GRU), capturing both linear and nonlinear spatial-spectral dynamics. GraphMamba enhances the ability to model complex spatial-spectral relationships while maintaining scalability and computational efficiency across diverse HSI datasets. Through comprehensive experiments, we demonstrate that GraphMamba outperforms existing state-of-the-art models, offering a scalable and robust solution for complex HSI classification tasks.","sentences":["Hyperspectral image (HSI) classification plays a pivotal role in domains such as environmental monitoring, agriculture, and urban planning.","However, it faces significant challenges due to the high-dimensional nature of the data and the complex spectral-spatial relationships inherent in HSI.","Traditional methods, including conventional machine learning and convolutional neural networks (CNNs), often struggle to effectively capture these intricate spectral-spatial features and global contextual information.","Transformer-based models, while powerful in capturing long-range dependencies, often demand substantial computational resources, posing challenges in scenarios where labeled datasets are limited, as is commonly seen in HSI applications.","To overcome these challenges, this work proposes GraphMamba, a hybrid model that combines spectral-spatial token generation, graph-based token prioritization, and cross-attention mechanisms.","The model introduces a novel hybridization of state-space modeling and Gated Recurrent Units (GRU), capturing both linear and nonlinear spatial-spectral dynamics.","GraphMamba enhances the ability to model complex spatial-spectral relationships while maintaining scalability and computational efficiency across diverse HSI datasets.","Through comprehensive experiments, we demonstrate that GraphMamba outperforms existing state-of-the-art models, offering a scalable and robust solution for complex HSI classification tasks."],"url":"http://arxiv.org/abs/2502.06427v1"}
{"created":"2025-02-10 13:02:00","title":"Generating Privacy-Preserving Personalized Advice with Zero-Knowledge Proofs and LLMs","abstract":"Large language models (LLMs) are increasingly utilized in domains such as finance, healthcare, and interpersonal relationships to provide advice tailored to user traits and contexts. However, this personalization often relies on sensitive data, raising critical privacy concerns and necessitating data minimization. To address these challenges, we propose a framework that integrates zero-knowledge proof (ZKP) technology, specifically zkVM, with LLM-based chatbots. This integration enables privacy-preserving data sharing by verifying user traits without disclosing sensitive information. Our research introduces both an architecture and a prompting strategy for this approach. Through empirical evaluation, we clarify the current constraints and performance limitations of both zkVM and the proposed prompting strategy, thereby demonstrating their practical feasibility in real-world scenarios.","sentences":["Large language models (LLMs) are increasingly utilized in domains such as finance, healthcare, and interpersonal relationships to provide advice tailored to user traits and contexts.","However, this personalization often relies on sensitive data, raising critical privacy concerns and necessitating data minimization.","To address these challenges, we propose a framework that integrates zero-knowledge proof (ZKP) technology, specifically zkVM, with LLM-based chatbots.","This integration enables privacy-preserving data sharing by verifying user traits without disclosing sensitive information.","Our research introduces both an architecture and a prompting strategy for this approach.","Through empirical evaluation, we clarify the current constraints and performance limitations of both zkVM and the proposed prompting strategy, thereby demonstrating their practical feasibility in real-world scenarios."],"url":"http://arxiv.org/abs/2502.06425v1"}
{"created":"2025-02-10 12:55:08","title":"Robust Watermarks Leak: Channel-Aware Feature Extraction Enables Adversarial Watermark Manipulation","abstract":"Watermarking plays a key role in the provenance and detection of AI-generated content. While existing methods prioritize robustness against real-world distortions (e.g., JPEG compression and noise addition), we reveal a fundamental tradeoff: such robust watermarks inherently improve the redundancy of detectable patterns encoded into images, creating exploitable information leakage. To leverage this, we propose an attack framework that extracts leakage of watermark patterns through multi-channel feature learning using a pre-trained vision model. Unlike prior works requiring massive data or detector access, our method achieves both forgery and detection evasion with a single watermarked image. Extensive experiments demonstrate that our method achieves a 60\\% success rate gain in detection evasion and 51\\% improvement in forgery accuracy compared to state-of-the-art methods while maintaining visual fidelity. Our work exposes the robustness-stealthiness paradox: current \"robust\" watermarks sacrifice security for distortion resistance, providing insights for future watermark design.","sentences":["Watermarking plays a key role in the provenance and detection of AI-generated content.","While existing methods prioritize robustness against real-world distortions (e.g., JPEG compression and noise addition), we reveal a fundamental tradeoff: such robust watermarks inherently improve the redundancy of detectable patterns encoded into images, creating exploitable information leakage.","To leverage this, we propose an attack framework that extracts leakage of watermark patterns through multi-channel feature learning using a pre-trained vision model.","Unlike prior works requiring massive data or detector access, our method achieves both forgery and detection evasion with a single watermarked image.","Extensive experiments demonstrate that our method achieves a 60\\% success rate gain in detection evasion and 51\\% improvement in forgery accuracy compared to state-of-the-art methods while maintaining visual fidelity.","Our work exposes the robustness-stealthiness paradox: current \"robust\" watermarks sacrifice security for distortion resistance, providing insights for future watermark design."],"url":"http://arxiv.org/abs/2502.06418v1"}
{"created":"2025-02-10 12:47:36","title":"An Automated Machine Learning Framework for Surgical Suturing Action Detection under Class Imbalance","abstract":"In laparoscopy surgical training and evaluation, real-time detection of surgical actions with interpretable outputs is crucial for automated and real-time instructional feedback and skill development. Such capability would enable development of machine guided training systems. This paper presents a rapid deployment approach utilizing automated machine learning methods, based on surgical action data collected from both experienced and trainee surgeons. The proposed approach effectively tackles the challenge of highly imbalanced class distributions, ensuring robust predictions across varying skill levels of surgeons. Additionally, our method partially incorporates model transparency, addressing the reliability requirements in medical applications. Compared to deep learning approaches, traditional machine learning models not only facilitate efficient rapid deployment but also offer significant advantages in interpretability. Through experiments, this study demonstrates the potential of this approach to provide quick, reliable and effective real-time detection in surgical training environments","sentences":["In laparoscopy surgical training and evaluation, real-time detection of surgical actions with interpretable outputs is crucial for automated and real-time instructional feedback and skill development.","Such capability would enable development of machine guided training systems.","This paper presents a rapid deployment approach utilizing automated machine learning methods, based on surgical action data collected from both experienced and trainee surgeons.","The proposed approach effectively tackles the challenge of highly imbalanced class distributions, ensuring robust predictions across varying skill levels of surgeons.","Additionally, our method partially incorporates model transparency, addressing the reliability requirements in medical applications.","Compared to deep learning approaches, traditional machine learning models not only facilitate efficient rapid deployment but also offer significant advantages in interpretability.","Through experiments, this study demonstrates the potential of this approach to provide quick, reliable and effective real-time detection in surgical training environments"],"url":"http://arxiv.org/abs/2502.06407v1"}
{"created":"2025-02-10 12:32:21","title":"AppVLM: A Lightweight Vision Language Model for Online App Control","abstract":"The utilisation of foundation models as smartphone assistants, termed app agents, is a critical research challenge. These agents aim to execute human instructions on smartphones by interpreting textual instructions and performing actions via the device's interface. While promising, current approaches face significant limitations. Methods that use large proprietary models, such as GPT-4o, are computationally expensive, while those that use smaller fine-tuned models often lack adaptability to out-of-distribution tasks. In this work, we introduce AppVLM, a lightweight Vision-Language Model (VLM). First, we fine-tune it offline on the AndroidControl dataset. Then, we refine its policy by collecting data from the AndroidWorld environment and performing further training iterations. Our results indicate that AppVLM achieves the highest action prediction accuracy in offline evaluation on the AndroidControl dataset, compared to all evaluated baselines, and matches GPT-4o in online task completion success rate in the AndroidWorld environment, while being up to ten times faster. This makes AppVLM a practical and efficient solution for real-world deployment.","sentences":["The utilisation of foundation models as smartphone assistants, termed app agents, is a critical research challenge.","These agents aim to execute human instructions on smartphones by interpreting textual instructions and performing actions via the device's interface.","While promising, current approaches face significant limitations.","Methods that use large proprietary models, such as GPT-4o, are computationally expensive, while those that use smaller fine-tuned models often lack adaptability to out-of-distribution tasks.","In this work, we introduce AppVLM, a lightweight Vision-Language Model (VLM).","First, we fine-tune it offline on the AndroidControl dataset.","Then, we refine its policy by collecting data from the AndroidWorld environment and performing further training iterations.","Our results indicate that AppVLM achieves the highest action prediction accuracy in offline evaluation on the AndroidControl dataset, compared to all evaluated baselines, and matches GPT-4o in online task completion success rate in the AndroidWorld environment, while being up to ten times faster.","This makes AppVLM a practical and efficient solution for real-world deployment."],"url":"http://arxiv.org/abs/2502.06395v1"}
{"created":"2025-02-10 12:30:25","title":"SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators","abstract":"Existing approaches to multilingual text detoxification are hampered by the scarcity of parallel multilingual datasets. In this work, we introduce a pipeline for the generation of multilingual parallel detoxification data. We also introduce SynthDetoxM, a manually collected and synthetically generated multilingual parallel text detoxification dataset comprising 16,000 high-quality detoxification sentence pairs across German, French, Spanish and Russian. The data was sourced from different toxicity evaluation datasets and then rewritten with nine modern open-source LLMs in few-shot setting. Our experiments demonstrate that models trained on the produced synthetic datasets have superior performance to those trained on the human-annotated MultiParaDetox dataset even in data limited setting. Models trained on SynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our dataset and code to help further research in multilingual text detoxification.","sentences":["Existing approaches to multilingual text detoxification are hampered by the scarcity of parallel multilingual datasets.","In this work, we introduce a pipeline for the generation of multilingual parallel detoxification data.","We also introduce SynthDetoxM, a manually collected and synthetically generated multilingual parallel text detoxification dataset comprising 16,000 high-quality detoxification sentence pairs across German, French, Spanish and Russian.","The data was sourced from different toxicity evaluation datasets and then rewritten with nine modern open-source LLMs in few-shot setting.","Our experiments demonstrate that models trained on the produced synthetic datasets have superior performance to those trained on the human-annotated MultiParaDetox dataset even in data limited setting.","Models trained on SynthDetoxM outperform all evaluated LLMs in few-shot setting.","We release our dataset and code to help further research in multilingual text detoxification."],"url":"http://arxiv.org/abs/2502.06394v1"}
{"created":"2025-02-10 12:20:08","title":"When Data Manipulation Meets Attack Goals: An In-depth Survey of Attacks for VLMs","abstract":"Vision-Language Models (VLMs) have gained considerable prominence in recent years due to their remarkable capability to effectively integrate and process both textual and visual information. This integration has significantly enhanced performance across a diverse spectrum of applications, such as scene perception and robotics. However, the deployment of VLMs has also given rise to critical safety and security concerns, necessitating extensive research to assess the potential vulnerabilities these VLM systems may harbor. In this work, we present an in-depth survey of the attack strategies tailored for VLMs. We categorize these attacks based on their underlying objectives - namely jailbreak, camouflage, and exploitation - while also detailing the various methodologies employed for data manipulation of VLMs. Meanwhile, we outline corresponding defense mechanisms that have been proposed to mitigate these vulnerabilities. By discerning key connections and distinctions among the diverse types of attacks, we propose a compelling taxonomy for VLM attacks. Moreover, we summarize the evaluation metrics that comprehensively describe the characteristics and impact of different attacks on VLMs. Finally, we conclude with a discussion of promising future research directions that could further enhance the robustness and safety of VLMs, emphasizing the importance of ongoing exploration in this critical area of study. To facilitate community engagement, we maintain an up-to-date project page, accessible at: https://github.com/AobtDai/VLM_Attack_Paper_List.","sentences":["Vision-Language Models (VLMs) have gained considerable prominence in recent years due to their remarkable capability to effectively integrate and process both textual and visual information.","This integration has significantly enhanced performance across a diverse spectrum of applications, such as scene perception and robotics.","However, the deployment of VLMs has also given rise to critical safety and security concerns, necessitating extensive research to assess the potential vulnerabilities these VLM systems may harbor.","In this work, we present an in-depth survey of the attack strategies tailored for VLMs.","We categorize these attacks based on their underlying objectives - namely jailbreak, camouflage, and exploitation - while also detailing the various methodologies employed for data manipulation of VLMs.","Meanwhile, we outline corresponding defense mechanisms that have been proposed to mitigate these vulnerabilities.","By discerning key connections and distinctions among the diverse types of attacks, we propose a compelling taxonomy for VLM attacks.","Moreover, we summarize the evaluation metrics that comprehensively describe the characteristics and impact of different attacks on VLMs.","Finally, we conclude with a discussion of promising future research directions that could further enhance the robustness and safety of VLMs, emphasizing the importance of ongoing exploration in this critical area of study.","To facilitate community engagement, we maintain an up-to-date project page, accessible at: https://github.com/AobtDai/VLM_Attack_Paper_List."],"url":"http://arxiv.org/abs/2502.06390v1"}
{"created":"2025-02-10 12:15:27","title":"How Humans Help LLMs: Assessing and Incentivizing Human Preference Annotators","abstract":"Human-annotated preference data play an important role in aligning large language models (LLMs). In this paper, we investigate the questions of assessing the performance of human annotators and incentivizing them to provide high-quality annotations. The quality assessment of language/text annotation faces two challenges: (i) the intrinsic heterogeneity among annotators, which prevents the classic methods that assume the underlying existence of a true label; and (ii) the unclear relationship between the annotation quality and the performance of downstream tasks, which excludes the possibility of inferring the annotators' behavior based on the model performance trained from the annotation data. Then we formulate a principal-agent model to characterize the behaviors of and the interactions between the company and the human annotators. The model rationalizes a practical mechanism of a bonus scheme to incentivize annotators which benefits both parties and it underscores the importance of the joint presence of an assessment system and a proper contract scheme. From a technical perspective, our analysis extends the existing literature on the principal-agent model by considering a continuous action space for the agent. We show the gap between the first-best and the second-best solutions (under the continuous action space) is of $\\Theta(1/\\sqrt{n \\log n})$ for the binary contracts and $\\Theta(1/n)$ for the linear contracts, where $n$ is the number of samples used for performance assessment; this contrasts with the known result of $\\exp(-\\Theta(n))$ for the binary contracts when the action space is discrete. Throughout the paper, we use real preference annotation data to accompany our discussions.","sentences":["Human-annotated preference data play an important role in aligning large language models (LLMs).","In this paper, we investigate the questions of assessing the performance of human annotators and incentivizing them to provide high-quality annotations.","The quality assessment of language/text annotation faces two challenges: (i) the intrinsic heterogeneity among annotators, which prevents the classic methods that assume the underlying existence of a true label; and (ii) the unclear relationship between the annotation quality and the performance of downstream tasks, which excludes the possibility of inferring the annotators' behavior based on the model performance trained from the annotation data.","Then we formulate a principal-agent model to characterize the behaviors of and the interactions between the company and the human annotators.","The model rationalizes a practical mechanism of a bonus scheme to incentivize annotators which benefits both parties and it underscores the importance of the joint presence of an assessment system and a proper contract scheme.","From a technical perspective, our analysis extends the existing literature on the principal-agent model by considering a continuous action space for the agent.","We show the gap between the first-best and the second-best solutions (under the continuous action space) is of $\\Theta(1/\\sqrt{n \\log n})$ for the binary contracts and $\\Theta(1/n)$ for the linear contracts, where $n$ is the number of samples used for performance assessment; this contrasts with the known result of $\\exp(-\\Theta(n))$ for the binary contracts when the action space is discrete.","Throughout the paper, we use real preference annotation data to accompany our discussions."],"url":"http://arxiv.org/abs/2502.06387v1"}
{"created":"2025-02-10 12:01:05","title":"Structure-preserving contrastive learning for spatial time series","abstract":"Informative representations enhance model performance and generalisability in downstream tasks. However, learning self-supervised representations for spatially characterised time series, like traffic interactions, poses challenges as it requires maintaining fine-grained similarity relations in the latent space. In this study, we incorporate two structure-preserving regularisers for the contrastive learning of spatial time series: one regulariser preserves the topology of similarities between instances, and the other preserves the graph geometry of similarities across spatial and temporal dimensions. To balance contrastive learning and structure preservation, we propose a dynamic mechanism that adaptively weighs the trade-off and stabilises training. We conduct experiments on multivariate time series classification, as well as macroscopic and microscopic traffic prediction. For all three tasks, our approach preserves the structures of similarity relations more effectively and improves state-of-the-art task performances. The proposed approach can be applied to an arbitrary encoder and is particularly beneficial for time series with spatial or geographical features. Furthermore, this study suggests that higher similarity structure preservation indicates more informative and useful representations. This may help to understand the contribution of representation learning in pattern recognition with neural networks. Our code is made openly accessible with all resulting data at https://github.com/yiru-jiao/spclt.","sentences":["Informative representations enhance model performance and generalisability in downstream tasks.","However, learning self-supervised representations for spatially characterised time series, like traffic interactions, poses challenges as it requires maintaining fine-grained similarity relations in the latent space.","In this study, we incorporate two structure-preserving regularisers for the contrastive learning of spatial time series: one regulariser preserves the topology of similarities between instances, and the other preserves the graph geometry of similarities across spatial and temporal dimensions.","To balance contrastive learning and structure preservation, we propose a dynamic mechanism that adaptively weighs the trade-off and stabilises training.","We conduct experiments on multivariate time series classification, as well as macroscopic and microscopic traffic prediction.","For all three tasks, our approach preserves the structures of similarity relations more effectively and improves state-of-the-art task performances.","The proposed approach can be applied to an arbitrary encoder and is particularly beneficial for time series with spatial or geographical features.","Furthermore, this study suggests that higher similarity structure preservation indicates more informative and useful representations.","This may help to understand the contribution of representation learning in pattern recognition with neural networks.","Our code is made openly accessible with all resulting data at https://github.com/yiru-jiao/spclt."],"url":"http://arxiv.org/abs/2502.06380v1"}
{"created":"2025-02-10 11:59:02","title":"Solving Linear-Gaussian Bayesian Inverse Problems with Decoupled Diffusion Sequential Monte Carlo","abstract":"A recent line of research has exploited pre-trained generative diffusion models as priors for solving Bayesian inverse problems. We contribute to this research direction by designing a sequential Monte Carlo method for linear-Gaussian inverse problems which builds on ``decoupled diffusion\", where the generative process is designed such that larger updates to the sample are possible. The method is asymptotically exact and we demonstrate the effectiveness of our Decoupled Diffusion Sequential Monte Carlo (DDSMC) algorithm on both synthetic data and image reconstruction tasks. Further, we demonstrate how the approach can be extended to discrete data.","sentences":["A recent line of research has exploited pre-trained generative diffusion models as priors for solving Bayesian inverse problems.","We contribute to this research direction by designing a sequential Monte Carlo method for linear-Gaussian inverse problems which builds on ``decoupled diffusion\", where the generative process is designed such that larger updates to the sample are possible.","The method is asymptotically exact and we demonstrate the effectiveness of our Decoupled Diffusion Sequential Monte Carlo (DDSMC) algorithm on both synthetic data and image reconstruction tasks.","Further, we demonstrate how the approach can be extended to discrete data."],"url":"http://arxiv.org/abs/2502.06379v1"}
{"created":"2025-02-10 11:44:46","title":"Hyperparameters in Score-Based Membership Inference Attacks","abstract":"Membership Inference Attacks (MIAs) have emerged as a valuable framework for evaluating privacy leakage by machine learning models. Score-based MIAs are distinguished, in particular, by their ability to exploit the confidence scores that the model generates for particular inputs. Existing score-based MIAs implicitly assume that the adversary has access to the target model's hyperparameters, which can be used to train the shadow models for the attack. In this work, we demonstrate that the knowledge of target hyperparameters is not a prerequisite for MIA in the transfer learning setting. Based on this, we propose a novel approach to select the hyperparameters for training the shadow models for MIA when the attacker has no prior knowledge about them by matching the output distributions of target and shadow models. We demonstrate that using the new approach yields hyperparameters that lead to an attack near indistinguishable in performance from an attack that uses target hyperparameters to train the shadow models. Furthermore, we study the empirical privacy risk of unaccounted use of training data for hyperparameter optimization (HPO) in differentially private (DP) transfer learning. We find no statistically significant evidence that performing HPO using training data would increase vulnerability to MIA.","sentences":["Membership Inference Attacks (MIAs) have emerged as a valuable framework for evaluating privacy leakage by machine learning models.","Score-based MIAs are distinguished, in particular, by their ability to exploit the confidence scores that the model generates for particular inputs.","Existing score-based MIAs implicitly assume that the adversary has access to the target model's hyperparameters, which can be used to train the shadow models for the attack.","In this work, we demonstrate that the knowledge of target hyperparameters is not a prerequisite for MIA in the transfer learning setting.","Based on this, we propose a novel approach to select the hyperparameters for training the shadow models for MIA when the attacker has no prior knowledge about them by matching the output distributions of target and shadow models.","We demonstrate that using the new approach yields hyperparameters that lead to an attack near indistinguishable in performance from an attack that uses target hyperparameters to train the shadow models.","Furthermore, we study the empirical privacy risk of unaccounted use of training data for hyperparameter optimization (HPO) in differentially private (DP) transfer learning.","We find no statistically significant evidence that performing HPO using training data would increase vulnerability to MIA."],"url":"http://arxiv.org/abs/2502.06374v1"}
{"created":"2025-02-10 11:40:11","title":"Simulation as Reality? The Effectiveness of LLM-Generated Data in Open-ended Question Assessment","abstract":"The advancement of Artificial Intelligence (AI) has created opportunities for e-learning, particularly in automated assessment systems that reduce educators' workload and provide timely feedback to students. However, developing effective AI-based assessment tools remains challenging due to the substantial resources required for collecting and annotating real student data. This study investigates the potential and gap of simulative data to address this limitation. Through a two-phase experimental study, we examined the effectiveness and gap of Large Language Model generated synthetic data in training educational assessment systems. Our findings reveal that while simulative data demonstrates promising results in training automated assessment models, outperforming state-of-the-art GPT-4o in most question types, its effectiveness has notable limitations. Specifically, models trained on synthetic data show excellent performance in simulated environment but need progress when applied to real-world scenarios. This performance gap highlights the limitations of only using synthetic data in controlled experimental settings for AI training. The absence of real-world noise and biases, which are also present in over-processed real-world data, contributes to this limitation. We recommend that future development of automated assessment agents and other AI tools should incorporate a mixture of synthetic and real-world data, or introduce more realistic noise and biases patterns, rather than relying solely on synthetic or over-processed data.","sentences":["The advancement of Artificial Intelligence (AI) has created opportunities for e-learning, particularly in automated assessment systems that reduce educators' workload and provide timely feedback to students.","However, developing effective AI-based assessment tools remains challenging due to the substantial resources required for collecting and annotating real student data.","This study investigates the potential and gap of simulative data to address this limitation.","Through a two-phase experimental study, we examined the effectiveness and gap of Large Language Model generated synthetic data in training educational assessment systems.","Our findings reveal that while simulative data demonstrates promising results in training automated assessment models, outperforming state-of-the-art GPT-4o in most question types, its effectiveness has notable limitations.","Specifically, models trained on synthetic data show excellent performance in simulated environment but need progress when applied to real-world scenarios.","This performance gap highlights the limitations of only using synthetic data in controlled experimental settings for AI training.","The absence of real-world noise and biases, which are also present in over-processed real-world data, contributes to this limitation.","We recommend that future development of automated assessment agents and other AI tools should incorporate a mixture of synthetic and real-world data, or introduce more realistic noise and biases patterns, rather than relying solely on synthetic or over-processed data."],"url":"http://arxiv.org/abs/2502.06371v1"}
{"created":"2025-02-10 11:36:45","title":"FOCUS - Multi-View Foot Reconstruction From Synthetically Trained Dense Correspondences","abstract":"Surface reconstruction from multiple, calibrated images is a challenging task - often requiring a large number of collected images with significant overlap. We look at the specific case of human foot reconstruction. As with previous successful foot reconstruction work, we seek to extract rich per-pixel geometry cues from multi-view RGB images, and fuse these into a final 3D object. Our method, FOCUS, tackles this problem with 3 main contributions: (i) SynFoot2, an extension of an existing synthetic foot dataset to include a new data type: dense correspondence with the parameterized foot model FIND; (ii) an uncertainty-aware dense correspondence predictor trained on our synthetic dataset; (iii) two methods for reconstructing a 3D surface from dense correspondence predictions: one inspired by Structure-from-Motion, and one optimization-based using the FIND model. We show that our reconstruction achieves state-of-the-art reconstruction quality in a few-view setting, performing comparably to state-of-the-art when many views are available, and runs substantially faster. We release our synthetic dataset to the research community. Code is available at: https://github.com/OllieBoyne/FOCUS","sentences":["Surface reconstruction from multiple, calibrated images is a challenging task - often requiring a large number of collected images with significant overlap.","We look at the specific case of human foot reconstruction.","As with previous successful foot reconstruction work, we seek to extract rich per-pixel geometry cues from multi-view RGB images, and fuse these into a final 3D object.","Our method, FOCUS, tackles this problem with 3 main contributions: (i) SynFoot2, an extension of an existing synthetic foot dataset to include a new data type: dense correspondence with the parameterized foot model FIND; (ii) an uncertainty-aware dense correspondence predictor trained on our synthetic dataset; (iii) two methods for reconstructing a 3D surface from dense correspondence predictions: one inspired by Structure-from-Motion, and one optimization-based using the FIND model.","We show that our reconstruction achieves state-of-the-art reconstruction quality in a few-view setting, performing comparably to state-of-the-art when many views are available, and runs substantially faster.","We release our synthetic dataset to the research community.","Code is available at: https://github.com/OllieBoyne/FOCUS"],"url":"http://arxiv.org/abs/2502.06367v1"}
{"created":"2025-02-10 11:30:35","title":"Automatic Identification of Samples in Hip-Hop Music via Multi-Loss Training and an Artificial Dataset","abstract":"Sampling, the practice of reusing recorded music or sounds from another source in a new work, is common in popular music genres like hip-hop and rap. Numerous services have emerged that allow users to identify connections between samples and the songs that incorporate them, with the goal of enhancing music discovery. Designing a system that can perform the same task automatically is challenging, as samples are commonly altered with audio effects like pitch- and time-stretching and may only be seconds long. Progress on this task has been minimal and is further blocked by the limited availability of training data. Here, we show that a convolutional neural network trained on an artificial dataset can identify real-world samples in commercial hip-hop music. We extract vocal, harmonic, and percussive elements from several databases of non-commercial music recordings using audio source separation, and train the model to fingerprint a subset of these elements in transformed versions of the original audio. We optimize the model using a joint classification and metric learning loss and show that it achieves 13% greater precision on real-world instances of sampling than a fingerprinting system using acoustic landmarks, and that it can recognize samples that have been both pitch shifted and time stretched. We also show that, for half of the commercial music recordings we tested, our model is capable of locating the position of a sample to within five seconds.","sentences":["Sampling, the practice of reusing recorded music or sounds from another source in a new work, is common in popular music genres like hip-hop and rap.","Numerous services have emerged that allow users to identify connections between samples and the songs that incorporate them, with the goal of enhancing music discovery.","Designing a system that can perform the same task automatically is challenging, as samples are commonly altered with audio effects like pitch- and time-stretching and may only be seconds long.","Progress on this task has been minimal and is further blocked by the limited availability of training data.","Here, we show that a convolutional neural network trained on an artificial dataset can identify real-world samples in commercial hip-hop music.","We extract vocal, harmonic, and percussive elements from several databases of non-commercial music recordings using audio source separation, and train the model to fingerprint a subset of these elements in transformed versions of the original audio.","We optimize the model using a joint classification and metric learning loss and show that it achieves 13% greater precision on real-world instances of sampling than a fingerprinting system using acoustic landmarks, and that it can recognize samples that have been both pitch shifted and time stretched.","We also show that, for half of the commercial music recordings we tested, our model is capable of locating the position of a sample to within five seconds."],"url":"http://arxiv.org/abs/2502.06364v1"}
{"created":"2025-02-10 11:10:41","title":"Fine-tuning Multimodal Transformers on Edge: A Parallel Split Learning Approach","abstract":"Multimodal transformers integrate diverse data types like images, audio, and text, advancing tasks such as audio-visual understanding and image-text retrieval; yet their high parameterization limits deployment on resource-constrained edge devices. Split Learning (SL), which partitions models at a designated cut-layer to offload compute-intensive operations to the server, offers a promising approach for distributed training of multimodal transformers, though its application remains underexplored. We present MPSL, a parallel SL approach for computational efficient fine-tuning of multimodal transformers in a distributed manner, while eliminating label sharing, client synchronization, and per-client sub-model management. MPSL employs lightweight client-side tokenizers and a unified modality-agnostic encoder, allowing flexible adaptation to task-specific needs. Our evaluation across 7 multimodal datasets demonstrates that MPSL matches or outperforms Federated Learning, reduces client-side computations by 250x, and achieves superior scalability in communication cost with model growth. Through extensive analysis, we highlight task suitability, trade-offs, and scenarios where MPSL excels, inspiring further exploration.","sentences":["Multimodal transformers integrate diverse data types like images, audio, and text, advancing tasks such as audio-visual understanding and image-text retrieval; yet their high parameterization limits deployment on resource-constrained edge devices.","Split Learning (SL), which partitions models at a designated cut-layer to offload compute-intensive operations to the server, offers a promising approach for distributed training of multimodal transformers, though its application remains underexplored.","We present MPSL, a parallel SL approach for computational efficient fine-tuning of multimodal transformers in a distributed manner, while eliminating label sharing, client synchronization, and per-client sub-model management.","MPSL employs lightweight client-side tokenizers and a unified modality-agnostic encoder, allowing flexible adaptation to task-specific needs.","Our evaluation across 7 multimodal datasets demonstrates that MPSL matches or outperforms Federated Learning, reduces client-side computations by 250x, and achieves superior scalability in communication cost with model growth.","Through extensive analysis, we highlight task suitability, trade-offs, and scenarios where MPSL excels, inspiring further exploration."],"url":"http://arxiv.org/abs/2502.06355v1"}
{"created":"2025-02-10 10:58:57","title":"Provably Near-Optimal Federated Ensemble Distillation with Negligible Overhead","abstract":"Federated ensemble distillation addresses client heterogeneity by generating pseudo-labels for an unlabeled server dataset based on client predictions and training the server model using the pseudo-labeled dataset. The unlabeled server dataset can either be pre-existing or generated through a data-free approach. The effectiveness of this approach critically depends on the method of assigning weights to client predictions when creating pseudo-labels, especially in highly heterogeneous settings. Inspired by theoretical results from GANs, we propose a provably near-optimal weighting method that leverages client discriminators trained with a server-distributed generator and local datasets. Our experiments on various image classification tasks demonstrate that the proposed method significantly outperforms baselines. Furthermore, we show that the additional communication cost, client-side privacy leakage, and client-side computational overhead introduced by our method are negligible, both in scenarios with and without a pre-existing server dataset.","sentences":["Federated ensemble distillation addresses client heterogeneity by generating pseudo-labels for an unlabeled server dataset based on client predictions and training the server model using the pseudo-labeled dataset.","The unlabeled server dataset can either be pre-existing or generated through a data-free approach.","The effectiveness of this approach critically depends on the method of assigning weights to client predictions when creating pseudo-labels, especially in highly heterogeneous settings.","Inspired by theoretical results from GANs, we propose a provably near-optimal weighting method that leverages client discriminators trained with a server-distributed generator and local datasets.","Our experiments on various image classification tasks demonstrate that the proposed method significantly outperforms baselines.","Furthermore, we show that the additional communication cost, client-side privacy leakage, and client-side computational overhead introduced by our method are negligible, both in scenarios with and without a pre-existing server dataset."],"url":"http://arxiv.org/abs/2502.06349v1"}
{"created":"2025-02-10 10:52:17","title":"Causal Lifting of Neural Representations: Zero-Shot Generalization for Causal Inferences","abstract":"A plethora of real-world scientific investigations is waiting to scale with the support of trustworthy predictive models that can reduce the need for costly data annotations. We focus on causal inferences on a target experiment with unlabeled factual outcomes, retrieved by a predictive model fine-tuned on a labeled similar experiment. First, we show that factual outcome estimation via Empirical Risk Minimization (ERM) may fail to yield valid causal inferences on the target population, even in a randomized controlled experiment and infinite training samples. Then, we propose to leverage the observed experimental settings during training to empower generalization to downstream interventional investigations, ``Causal Lifting'' the predictive model. We propose Deconfounded Empirical Risk Minimization (DERM), a new simple learning procedure minimizing the risk over a fictitious target population, preventing potential confounding effects. We validate our method on both synthetic and real-world scientific data. Notably, for the first time, we zero-shot generalize causal inferences on ISTAnt dataset (without annotation) by causal lifting a predictive model on our experiment variant.","sentences":["A plethora of real-world scientific investigations is waiting to scale with the support of trustworthy predictive models that can reduce the need for costly data annotations.","We focus on causal inferences on a target experiment with unlabeled factual outcomes, retrieved by a predictive model fine-tuned on a labeled similar experiment.","First, we show that factual outcome estimation via Empirical Risk Minimization (ERM) may fail to yield valid causal inferences on the target population, even in a randomized controlled experiment and infinite training samples.","Then, we propose to leverage the observed experimental settings during training to empower generalization to downstream interventional investigations, ``Causal Lifting'' the predictive model.","We propose Deconfounded Empirical Risk Minimization (DERM), a new simple learning procedure minimizing the risk over a fictitious target population, preventing potential confounding effects.","We validate our method on both synthetic and real-world scientific data.","Notably, for the first time, we zero-shot generalize causal inferences on ISTAnt dataset (without annotation) by causal lifting a predictive model on our experiment variant."],"url":"http://arxiv.org/abs/2502.06343v1"}
{"created":"2025-02-10 10:38:33","title":"Zero-shot Depth Completion via Test-time Alignment with Affine-invariant Depth Prior","abstract":"Depth completion, predicting dense depth maps from sparse depth measurements, is an ill-posed problem requiring prior knowledge. Recent methods adopt learning-based approaches to implicitly capture priors, but the priors primarily fit in-domain data and do not generalize well to out-of-domain scenarios. To address this, we propose a zero-shot depth completion method composed of an affine-invariant depth diffusion model and test-time alignment. We use pre-trained depth diffusion models as depth prior knowledge, which implicitly understand how to fill in depth for scenes. Our approach aligns the affine-invariant depth prior with metric-scale sparse measurements, enforcing them as hard constraints via an optimization loop at test-time. Our zero-shot depth completion method demonstrates generalization across various domain datasets, achieving up to a 21\\% average performance improvement over the previous state-of-the-art methods while enhancing spatial understanding by sharpening scene details. We demonstrate that aligning a monocular affine-invariant depth prior with sparse metric measurements is a proven strategy to achieve domain-generalizable depth completion without relying on extensive training data. Project page: https://hyoseok1223.github.io/zero-shot-depth-completion/.","sentences":["Depth completion, predicting dense depth maps from sparse depth measurements, is an ill-posed problem requiring prior knowledge.","Recent methods adopt learning-based approaches to implicitly capture priors, but the priors primarily fit in-domain data and do not generalize well to out-of-domain scenarios.","To address this, we propose a zero-shot depth completion method composed of an affine-invariant depth diffusion model and test-time alignment.","We use pre-trained depth diffusion models as depth prior knowledge, which implicitly understand how to fill in depth for scenes.","Our approach aligns the affine-invariant depth prior with metric-scale sparse measurements, enforcing them as hard constraints via an optimization loop at test-time.","Our zero-shot depth completion method demonstrates generalization across various domain datasets, achieving up to a 21\\% average performance improvement over the previous state-of-the-art methods while enhancing spatial understanding by sharpening scene details.","We demonstrate that aligning a monocular affine-invariant depth prior with sparse metric measurements is a proven strategy to achieve domain-generalizable depth completion without relying on extensive training data.","Project page: https://hyoseok1223.github.io/zero-shot-depth-completion/."],"url":"http://arxiv.org/abs/2502.06338v1"}
{"created":"2025-02-10 10:37:36","title":"Accelerating Outlier-robust Rotation Estimation by Stereographic Projection","abstract":"Rotation estimation plays a fundamental role in many computer vision and robot tasks. However, efficiently estimating rotation in large inputs containing numerous outliers (i.e., mismatches) and noise is a recognized challenge. Many robust rotation estimation methods have been designed to address this challenge. Unfortunately, existing methods are often inapplicable due to their long computation time and the risk of local optima. In this paper, we propose an efficient and robust rotation estimation method. Specifically, our method first investigates geometric constraints involving only the rotation axis. Then, it uses stereographic projection and spatial voting techniques to identify the rotation axis and angle. Furthermore, our method efficiently obtains the optimal rotation estimation and can estimate multiple rotations simultaneously. To verify the feasibility of our method, we conduct comparative experiments using both synthetic and real-world data. The results show that, with GPU assistance, our method can solve large-scale ($10^6$ points) and severely corrupted (90\\% outlier rate) rotation estimation problems within 0.07 seconds, with an angular error of only 0.01 degrees, which is superior to existing methods in terms of accuracy and efficiency.","sentences":["Rotation estimation plays a fundamental role in many computer vision and robot tasks.","However, efficiently estimating rotation in large inputs containing numerous outliers (i.e., mismatches) and noise is a recognized challenge.","Many robust rotation estimation methods have been designed to address this challenge.","Unfortunately, existing methods are often inapplicable due to their long computation time and the risk of local optima.","In this paper, we propose an efficient and robust rotation estimation method.","Specifically, our method first investigates geometric constraints involving only the rotation axis.","Then, it uses stereographic projection and spatial voting techniques to identify the rotation axis and angle.","Furthermore, our method efficiently obtains the optimal rotation estimation and can estimate multiple rotations simultaneously.","To verify the feasibility of our method, we conduct comparative experiments using both synthetic and real-world data.","The results show that, with GPU assistance, our method can solve large-scale ($10^6$ points) and severely corrupted (90\\% outlier rate) rotation estimation problems within 0.07 seconds, with an angular error of only 0.01 degrees, which is superior to existing methods in terms of accuracy and efficiency."],"url":"http://arxiv.org/abs/2502.06337v1"}
{"created":"2025-02-10 10:37:21","title":"DefTransNet: A Transformer-based Method for Non-Rigid Point Cloud Registration in the Simulation of Soft Tissue Deformation","abstract":"Soft-tissue surgeries, such as tumor resections, are complicated by tissue deformations that can obscure the accurate location and shape of tissues. By representing tissue surfaces as point clouds and applying non-rigid point cloud registration (PCR) methods, surgeons can better understand tissue deformations before, during, and after surgery. Existing non-rigid PCR methods, such as feature-based approaches, struggle with robustness against challenges like noise, outliers, partial data, and large deformations, making accurate point correspondence difficult. Although learning-based PCR methods, particularly Transformer-based approaches, have recently shown promise due to their attention mechanisms for capturing interactions, their robustness remains limited in challenging scenarios. In this paper, we present DefTransNet, a novel end-to-end Transformer-based architecture for non-rigid PCR. DefTransNet is designed to address the key challenges of deformable registration, including large deformations, outliers, noise, and partial data, by inputting source and target point clouds and outputting displacement vector fields. The proposed method incorporates a learnable transformation matrix to enhance robustness to affine transformations, integrates global and local geometric information, and captures long-range dependencies among points using Transformers. We validate our approach on four datasets: ModelNet, SynBench, 4DMatch, and DeformedTissue, using both synthetic and real-world data to demonstrate the generalization of our proposed method. Experimental results demonstrate that DefTransNet outperforms current state-of-the-art registration networks across various challenging conditions. Our code and data are publicly available.","sentences":["Soft-tissue surgeries, such as tumor resections, are complicated by tissue deformations that can obscure the accurate location and shape of tissues.","By representing tissue surfaces as point clouds and applying non-rigid point cloud registration (PCR) methods, surgeons can better understand tissue deformations before, during, and after surgery.","Existing non-rigid PCR methods, such as feature-based approaches, struggle with robustness against challenges like noise, outliers, partial data, and large deformations, making accurate point correspondence difficult.","Although learning-based PCR methods, particularly Transformer-based approaches, have recently shown promise due to their attention mechanisms for capturing interactions, their robustness remains limited in challenging scenarios.","In this paper, we present DefTransNet, a novel end-to-end Transformer-based architecture for non-rigid PCR.","DefTransNet is designed to address the key challenges of deformable registration, including large deformations, outliers, noise, and partial data, by inputting source and target point clouds and outputting displacement vector fields.","The proposed method incorporates a learnable transformation matrix to enhance robustness to affine transformations, integrates global and local geometric information, and captures long-range dependencies among points using Transformers.","We validate our approach on four datasets: ModelNet, SynBench, 4DMatch, and DeformedTissue, using both synthetic and real-world data to demonstrate the generalization of our proposed method.","Experimental results demonstrate that DefTransNet outperforms current state-of-the-art registration networks across various challenging conditions.","Our code and data are publicly available."],"url":"http://arxiv.org/abs/2502.06336v1"}
{"created":"2025-02-10 10:36:42","title":"Microcanonical Langevin Ensembles: Advancing the Sampling of Bayesian Neural Networks","abstract":"Despite recent advances, sampling-based inference for Bayesian Neural Networks (BNNs) remains a significant challenge in probabilistic deep learning. While sampling-based approaches do not require a variational distribution assumption, current state-of-the-art samplers still struggle to navigate the complex and highly multimodal posteriors of BNNs. As a consequence, sampling still requires considerably longer inference times than non-Bayesian methods even for small neural networks, despite recent advances in making software implementations more efficient. Besides the difficulty of finding high-probability regions, the time until samplers provide sufficient exploration of these areas remains unpredictable. To tackle these challenges, we introduce an ensembling approach that leverages strategies from optimization and a recently proposed sampler called Microcanonical Langevin Monte Carlo (MCLMC) for efficient, robust and predictable sampling performance. Compared to approaches based on the state-of-the-art No-U-Turn Sampler, our approach delivers substantial speedups up to an order of magnitude, while maintaining or improving predictive performance and uncertainty quantification across diverse tasks and data modalities. The suggested Microcanonical Langevin Ensembles and modifications to MCLMC additionally enhance the method's predictability in resource requirements, facilitating easier parallelization. All in all, the proposed method offers a promising direction for practical, scalable inference for BNNs.","sentences":["Despite recent advances, sampling-based inference for Bayesian Neural Networks (BNNs) remains a significant challenge in probabilistic deep learning.","While sampling-based approaches do not require a variational distribution assumption, current state-of-the-art samplers still struggle to navigate the complex and highly multimodal posteriors of BNNs.","As a consequence, sampling still requires considerably longer inference times than non-Bayesian methods even for small neural networks, despite recent advances in making software implementations more efficient.","Besides the difficulty of finding high-probability regions, the time until samplers provide sufficient exploration of these areas remains unpredictable.","To tackle these challenges, we introduce an ensembling approach that leverages strategies from optimization and a recently proposed sampler called Microcanonical Langevin Monte Carlo (MCLMC) for efficient, robust and predictable sampling performance.","Compared to approaches based on the state-of-the-art No-U-Turn Sampler, our approach delivers substantial speedups up to an order of magnitude, while maintaining or improving predictive performance and uncertainty quantification across diverse tasks and data modalities.","The suggested Microcanonical Langevin Ensembles and modifications to MCLMC additionally enhance the method's predictability in resource requirements, facilitating easier parallelization.","All in all, the proposed method offers a promising direction for practical, scalable inference for BNNs."],"url":"http://arxiv.org/abs/2502.06335v1"}
{"created":"2025-02-10 10:29:46","title":"Performance Analysis of Multi-Hop Networks at Terahertz Frequencies","abstract":"The emergence of THz (Terahertz) frequency wireless networks holds great potential for advancing various high-demand services, including Industrial Internet of Things (IIoT) applications. These use cases benefit significantly from the ultra-high data rates, low latency, and high spatial resolution offered by THz frequencies. However, a primary well-known challenge of THz networks is their limited coverage range due to high path loss and vulnerability to obstructions. This paper addresses this limitation by proposing two novel multi-hop protocols, Table-Less (TL) and Table-Based (TB), respectively, both avoiding centralized control and/or control plane transmissions. Indeed, both solutions are distributed, simple, and rapidly adaptable to network changes. Simulation results demonstrate the effectiveness of our approaches, as well as revealing interesting trade-offs between TL and TB routing protocols, both in a real IIoT THz network and under static and dynamic conditions.","sentences":["The emergence of THz (Terahertz) frequency wireless networks holds great potential for advancing various high-demand services, including Industrial Internet of Things (IIoT) applications.","These use cases benefit significantly from the ultra-high data rates, low latency, and high spatial resolution offered by THz frequencies.","However, a primary well-known challenge of THz networks is their limited coverage range due to high path loss and vulnerability to obstructions.","This paper addresses this limitation by proposing two novel multi-hop protocols, Table-Less (TL) and Table-Based (TB), respectively, both avoiding centralized control and/or control plane transmissions.","Indeed, both solutions are distributed, simple, and rapidly adaptable to network changes.","Simulation results demonstrate the effectiveness of our approaches, as well as revealing interesting trade-offs between TL and TB routing protocols, both in a real IIoT THz network and under static and dynamic conditions."],"url":"http://arxiv.org/abs/2502.06330v1"}
{"created":"2025-02-10 10:28:11","title":"Prompt-Driven Continual Graph Learning","abstract":"Continual Graph Learning (CGL), which aims to accommodate new tasks over evolving graph data without forgetting prior knowledge, is garnering significant research interest. Mainstream solutions adopt the memory replay-based idea, ie, caching representative data from earlier tasks for retraining the graph model. However, this strategy struggles with scalability issues for constantly evolving graphs and raises concerns regarding data privacy. Inspired by recent advancements in the prompt-based learning paradigm, this paper introduces a novel prompt-driven continual graph learning (PROMPTCGL) framework, which learns a separate prompt for each incoming task and maintains the underlying graph neural network model fixed. In this way, PROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous tasks. More specifically, we propose hierarchical prompting to instruct the model from both feature- and topology-level to fully address the variability of task graphs in dynamic continual learning. Additionally, we develop a personalized prompt generator to generate tailored prompts for each graph node while minimizing the number of prompts needed, leading to constant memory consumption regardless of the graph scale. Extensive experiments on four benchmarks show that PROMPTCGL achieves superior performance against existing CGL approaches while significantly reducing memory consumption. Our code is available at https://github.com/QiWang98/PromptCGL.","sentences":["Continual Graph Learning (CGL), which aims to accommodate new tasks over evolving graph data without forgetting prior knowledge, is garnering significant research interest.","Mainstream solutions adopt the memory replay-based idea, ie, caching representative data from earlier tasks for retraining the graph model.","However, this strategy struggles with scalability issues for constantly evolving graphs and raises concerns regarding data privacy.","Inspired by recent advancements in the prompt-based learning paradigm, this paper introduces a novel prompt-driven continual graph learning (PROMPTCGL) framework, which learns a separate prompt for each incoming task and maintains the underlying graph neural network model fixed.","In this way, PROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous tasks.","More specifically, we propose hierarchical prompting to instruct the model from both feature- and topology-level to fully address the variability of task graphs in dynamic continual learning.","Additionally, we develop a personalized prompt generator to generate tailored prompts for each graph node while minimizing the number of prompts needed, leading to constant memory consumption regardless of the graph scale.","Extensive experiments on four benchmarks show that PROMPTCGL achieves superior performance against existing CGL approaches while significantly reducing memory consumption.","Our code is available at https://github.com/QiWang98/PromptCGL."],"url":"http://arxiv.org/abs/2502.06327v1"}
{"created":"2025-02-10 10:20:11","title":"UniDemoir\u00e9: Towards Universal Image Demoir\u00e9ing with Data Generation and Synthesis","abstract":"Image demoir\\'eing poses one of the most formidable challenges in image restoration, primarily due to the unpredictable and anisotropic nature of moir\\'e patterns. Limited by the quantity and diversity of training data, current methods tend to overfit to a single moir\\'e domain, resulting in performance degradation for new domains and restricting their robustness in real-world applications. In this paper, we propose a universal image demoir\\'eing solution, UniDemoir\\'e, which has superior generalization capability. Notably, we propose innovative and effective data generation and synthesis methods that can automatically provide vast high-quality moir\\'e images to train a universal demoir\\'eing model. Our extensive experiments demonstrate the cutting-edge performance and broad potential of our approach for generalized image demoir\\'eing.","sentences":["Image demoir\\'eing poses one of the most formidable challenges in image restoration, primarily due to the unpredictable and anisotropic nature of moir\\'e patterns.","Limited by the quantity and diversity of training data, current methods tend to overfit to a single moir\\'e domain, resulting in performance degradation for new domains and restricting their robustness in real-world applications.","In this paper, we propose a universal image demoir\\'eing solution, UniDemoir\\'e, which has superior generalization capability.","Notably, we propose innovative and effective data generation and synthesis methods that can automatically provide vast high-quality moir\\'e images to train a universal demoir\\'eing model.","Our extensive experiments demonstrate the cutting-edge performance and broad potential of our approach for generalized image demoir\\'eing."],"url":"http://arxiv.org/abs/2502.06324v1"}
{"created":"2025-02-10 10:13:57","title":"Tracezip: Efficient Distributed Tracing via Trace Compression","abstract":"Distributed tracing serves as a fundamental building block in the monitoring and testing of cloud service systems. To reduce computational and storage overheads, the de facto practice is to capture fewer traces via sampling. However, existing work faces a trade-off between the completeness of tracing and system overhead. On one hand, head-based sampling indiscriminately selects requests to trace when they enter the system, which may miss critical events. On the other hand, tail-based sampling traces all requests and selectively persist the edge-case traces, which entails the overheads related to trace collection and ingestion. Taking a different path, in this paper we propose Tracezip to enhance the efficiency of distributed tracing via trace compression. Our key insight is that there exists significant redundancy among traces, which results in repetitive transmission of identical data between the services and backend. We design a new data structure named Span Retrieval Tree (SRT) that continuously encapsulates such redundancy at the service side and transforms trace spans into a lightweight form. At the backend, the full traces can be seamlessly reconstructed by retrieving the common data already delivered by previous spans. Tracezip includes a series of strategies to optimize the structure of SRT and a differential update mechanism to efficiently synchronize SRT between services and backend. Our evaluation on microservices benchmarks, popular cloud service systems, and production trace data demonstrate that Tracezip can achieve substantial performance gains in trace collection, with negligible overhead. We have implemented Tracezip inside OpenTelemetry Collector, making it compatible with existing tracing APIs.","sentences":["Distributed tracing serves as a fundamental building block in the monitoring and testing of cloud service systems.","To reduce computational and storage overheads, the de facto practice is to capture fewer traces via sampling.","However, existing work faces a trade-off between the completeness of tracing and system overhead.","On one hand, head-based sampling indiscriminately selects requests to trace when they enter the system, which may miss critical events.","On the other hand, tail-based sampling traces all requests and selectively persist the edge-case traces, which entails the overheads related to trace collection and ingestion.","Taking a different path, in this paper we propose Tracezip to enhance the efficiency of distributed tracing via trace compression.","Our key insight is that there exists significant redundancy among traces, which results in repetitive transmission of identical data between the services and backend.","We design a new data structure named Span Retrieval Tree (SRT) that continuously encapsulates such redundancy at the service side and transforms trace spans into a lightweight form.","At the backend, the full traces can be seamlessly reconstructed by retrieving the common data already delivered by previous spans.","Tracezip includes a series of strategies to optimize the structure of SRT and a differential update mechanism to efficiently synchronize SRT between services and backend.","Our evaluation on microservices benchmarks, popular cloud service systems, and production trace data demonstrate that Tracezip can achieve substantial performance gains in trace collection, with negligible overhead.","We have implemented Tracezip inside OpenTelemetry Collector, making it compatible with existing tracing APIs."],"url":"http://arxiv.org/abs/2502.06318v1"}
{"created":"2025-02-10 10:10:30","title":"The digital labour of artificial intelligence in Latin America: a comparison of Argentina, Brazil, and Venezuela","abstract":"The current hype around artificial intelligence (AI) conceals the substantial human intervention underlying its development. This article lifts the veil on the precarious and low-paid 'data workers' who prepare data to train, test, check, and otherwise support models in the shadow of globalized AI production. We use original questionnaire and interview data collected from 220 workers in Argentina (2021-22), 477 in Brazil (2023), and 214 in Venezuela (2021-22). We compare them to detect common patterns and reveal the specificities of data work in Latin America, while disclosing its role in AI production.We show that data work is intertwined with economic hardship, inequalities, and informality. Despite workers' high educational attainment, disadvantage is widespread, though with cross-country disparities. By acknowledging the interconnections between AI development, data work, and globalized production, we provide insights for the regulation of AI and the future of work, aiming to achieve positive outcomes for all stakeholders.","sentences":["The current hype around artificial intelligence (AI) conceals the substantial human intervention underlying its development.","This article lifts the veil on the precarious and low-paid 'data workers' who prepare data to train, test, check, and otherwise support models in the shadow of globalized AI production.","We use original questionnaire and interview data collected from 220 workers in Argentina (2021-22), 477 in Brazil (2023), and 214 in Venezuela (2021-22).","We compare them to detect common patterns and reveal the specificities of data work in Latin America, while disclosing its role in AI production.","We show that data work is intertwined with economic hardship, inequalities, and informality.","Despite workers' high educational attainment, disadvantage is widespread, though with cross-country disparities.","By acknowledging the interconnections between AI development, data work, and globalized production, we provide insights for the regulation of AI and the future of work, aiming to achieve positive outcomes for all stakeholders."],"url":"http://arxiv.org/abs/2502.06317v1"}
{"created":"2025-02-10 10:06:46","title":"From Pixels to Components: Eigenvector Masking for Visual Representation Learning","abstract":"Predicting masked from visible parts of an image is a powerful self-supervised approach for visual representation learning. However, the common practice of masking random patches of pixels exhibits certain failure modes, which can prevent learning meaningful high-level features, as required for downstream tasks. We propose an alternative masking strategy that operates on a suitable transformation of the data rather than on the raw pixels. Specifically, we perform principal component analysis and then randomly mask a subset of components, which accounts for a fixed ratio of the data variance. The learning task then amounts to reconstructing the masked components from the visible ones. Compared to local patches of pixels, the principal components of images carry more global information. We thus posit that predicting masked from visible components involves more high-level features, allowing our masking strategy to extract more useful representations. This is corroborated by our empirical findings which demonstrate improved image classification performance for component over pixel masking. Our method thus constitutes a simple and robust data-driven alternative to traditional masked image modeling approaches.","sentences":["Predicting masked from visible parts of an image is a powerful self-supervised approach for visual representation learning.","However, the common practice of masking random patches of pixels exhibits certain failure modes, which can prevent learning meaningful high-level features, as required for downstream tasks.","We propose an alternative masking strategy that operates on a suitable transformation of the data rather than on the raw pixels.","Specifically, we perform principal component analysis and then randomly mask a subset of components, which accounts for a fixed ratio of the data variance.","The learning task then amounts to reconstructing the masked components from the visible ones.","Compared to local patches of pixels, the principal components of images carry more global information.","We thus posit that predicting masked from visible components involves more high-level features, allowing our masking strategy to extract more useful representations.","This is corroborated by our empirical findings which demonstrate improved image classification performance for component over pixel masking.","Our method thus constitutes a simple and robust data-driven alternative to traditional masked image modeling approaches."],"url":"http://arxiv.org/abs/2502.06314v1"}
{"created":"2025-02-10 09:49:46","title":"Data-aware Dynamic Execution of Irregular Workloads on Heterogeneous Systems","abstract":"Current approaches to scheduling workloads on heterogeneous systems with specialized accelerators often rely on manual partitioning, offloading tasks with specific compute patterns to accelerators. This method requires extensive experimentation and human effort to identify the tasks suitable for the accelerator. To solve this problem, we introduce DyPe, a scheduling framework tailored for heterogeneous systems with specialized accelerators. Our method automatically partitions, deploys, and reschedules execution when necessary by dynamically analyzing the characteristics of the input data and leveraging the interoperator parallelism among heterogeneous devices.   DyPe navigates a multi-objective, multi-constraint design space that considers both system constraints and application requirements, which allows it to discover Pareto-optimal mapping configurations, improving the system's overall performance and effectively managing energy-performance trade-offs. To demonstrate the benefits of our approach on real hardware, we build a heterogeneous system of GPUs and FPGAs with peer-to-peer data transfers. The experiments show that conventional static scheduling is optimal for 13 out of 86 cases for different workloads and system settings while DyPe is adaptable and able to find the optimal schedule in 77 out of 86 cases, with an average of only 3.95% performance or energy efficiency loss in the sub-optimal cases. Performance evaluation of DyPe shows an average of 1.53x throughput and 1.09x energy efficiency improvement over the static schedule baseline and 1.44x throughput and 1.66x energy efficiency over the GPU-only baseline.","sentences":["Current approaches to scheduling workloads on heterogeneous systems with specialized accelerators often rely on manual partitioning, offloading tasks with specific compute patterns to accelerators.","This method requires extensive experimentation and human effort to identify the tasks suitable for the accelerator.","To solve this problem, we introduce DyPe, a scheduling framework tailored for heterogeneous systems with specialized accelerators.","Our method automatically partitions, deploys, and reschedules execution when necessary by dynamically analyzing the characteristics of the input data and leveraging the interoperator parallelism among heterogeneous devices.   ","DyPe navigates a multi-objective, multi-constraint design space that considers both system constraints and application requirements, which allows it to discover Pareto-optimal mapping configurations, improving the system's overall performance and effectively managing energy-performance trade-offs.","To demonstrate the benefits of our approach on real hardware, we build a heterogeneous system of GPUs and FPGAs with peer-to-peer data transfers.","The experiments show that conventional static scheduling is optimal for 13 out of 86 cases for different workloads and system settings while DyPe is adaptable and able to find the optimal schedule in 77 out of 86 cases, with an average of only 3.95% performance or energy efficiency loss in the sub-optimal cases.","Performance evaluation of DyPe shows an average of 1.53x throughput and 1.09x energy efficiency improvement over the static schedule baseline and 1.44x throughput and 1.66x energy efficiency over the GPU-only baseline."],"url":"http://arxiv.org/abs/2502.06304v1"}
{"created":"2025-02-10 09:31:12","title":"Enhancing Ground-to-Aerial Image Matching for Visual Misinformation Detection Using Semantic Segmentation","abstract":"The recent advancements in generative AI techniques, which have significantly increased the online dissemination of altered images and videos, have raised serious concerns about the credibility of digital media available on the Internet and distributed through information channels and social networks. This issue particularly affects domains that rely heavily on trustworthy data, such as journalism, forensic analysis, and Earth observation. To address these concerns, the ability to geolocate a non-geo-tagged ground-view image without external information, such as GPS coordinates, has become increasingly critical. This study tackles the challenge of linking a ground-view image, potentially exhibiting varying fields of view (FoV), to its corresponding satellite image without the aid of GPS data. To achieve this, we propose a novel four-stream Siamese-like architecture, the Quadruple Semantic Align Net (SAN-QUAD), which extends previous state-of-the-art (SOTA) approaches by leveraging semantic segmentation applied to both ground and satellite imagery. Experimental results on a subset of the CVUSA dataset demonstrate significant improvements of up to 9.8\\% over prior methods across various FoV settings.","sentences":["The recent advancements in generative AI techniques, which have significantly increased the online dissemination of altered images and videos, have raised serious concerns about the credibility of digital media available on the Internet and distributed through information channels and social networks.","This issue particularly affects domains that rely heavily on trustworthy data, such as journalism, forensic analysis, and Earth observation.","To address these concerns, the ability to geolocate a non-geo-tagged ground-view image without external information, such as GPS coordinates, has become increasingly critical.","This study tackles the challenge of linking a ground-view image, potentially exhibiting varying fields of view (FoV), to its corresponding satellite image without the aid of GPS data.","To achieve this, we propose a novel four-stream Siamese-like architecture, the Quadruple Semantic Align Net (SAN-QUAD), which extends previous state-of-the-art (SOTA) approaches by leveraging semantic segmentation applied to both ground and satellite imagery.","Experimental results on a subset of the CVUSA dataset demonstrate significant improvements of up to 9.8\\% over prior methods across various FoV settings."],"url":"http://arxiv.org/abs/2502.06288v1"}
{"created":"2025-02-10 09:30:34","title":"CT-UIO: Continuous-Time UWB-Inertial-Odometer Localization Using Non-Uniform B-spline with Fewer Anchors","abstract":"Ultra-wideband (UWB) based positioning with fewer anchors has attracted significant research interest in recent years, especially under energy-constrained conditions. However, most existing methods rely on discrete-time representations and smoothness priors to infer a robot's motion states, which often struggle with ensuring multi-sensor data synchronization. In this paper, we present an efficient UWB-Inertial-odometer localization system, utilizing a non-uniform B-spline framework with fewer anchors. Unlike traditional uniform B-spline-based continuous-time methods, we introduce an adaptive knot-span adjustment strategy for non-uniform continuous-time trajectory representation. This is accomplished by adjusting control points dynamically based on movement speed. To enable efficient fusion of IMU and odometer data, we propose an improved Extended Kalman Filter (EKF) with innovation-based adaptive estimation to provide short-term accurate motion prior. Furthermore, to address the challenge of achieving a fully observable UWB localization system under few-anchor conditions, the Virtual Anchor (VA) generation method based on multiple hypotheses is proposed. At the backend, we propose a CT-UIO factor graph with an adaptive sliding window for global trajectory estimation. Comprehensive experiments conducted on corridor and exhibition hall datasets validate the proposed system's high precision and robust performance. The codebase and datasets of this work will be open-sourced at https://github.com/JasonSun623/CT-UIO.","sentences":["Ultra-wideband (UWB) based positioning with fewer anchors has attracted significant research interest in recent years, especially under energy-constrained conditions.","However, most existing methods rely on discrete-time representations and smoothness priors to infer a robot's motion states, which often struggle with ensuring multi-sensor data synchronization.","In this paper, we present an efficient UWB-Inertial-odometer localization system, utilizing a non-uniform B-spline framework with fewer anchors.","Unlike traditional uniform B-spline-based continuous-time methods, we introduce an adaptive knot-span adjustment strategy for non-uniform continuous-time trajectory representation.","This is accomplished by adjusting control points dynamically based on movement speed.","To enable efficient fusion of IMU and odometer data, we propose an improved Extended Kalman Filter (EKF) with innovation-based adaptive estimation to provide short-term accurate motion prior.","Furthermore, to address the challenge of achieving a fully observable UWB localization system under few-anchor conditions, the Virtual Anchor (VA) generation method based on multiple hypotheses is proposed.","At the backend, we propose a CT-UIO factor graph with an adaptive sliding window for global trajectory estimation.","Comprehensive experiments conducted on corridor and exhibition hall datasets validate the proposed system's high precision and robust performance.","The codebase and datasets of this work will be open-sourced at https://github.com/JasonSun623/CT-UIO."],"url":"http://arxiv.org/abs/2502.06287v1"}
{"created":"2025-02-10 09:23:22","title":"IceBerg: Debiased Self-Training for Class-Imbalanced Node Classification","abstract":"Graph Neural Networks (GNNs) have achieved great success in dealing with non-Euclidean graph-structured data and have been widely deployed in many real-world applications. However, their effectiveness is often jeopardized under class-imbalanced training sets. Most existing studies have analyzed class-imbalanced node classification from a supervised learning perspective, but they do not fully utilize the large number of unlabeled nodes in semi-supervised scenarios. We claim that the supervised signal is just the tip of the iceberg and a large number of unlabeled nodes have not yet been effectively utilized. In this work, we propose IceBerg, a debiased self-training framework to address the class-imbalanced and few-shot challenges for GNNs at the same time. Specifically, to figure out the Matthew effect and label distribution shift in self-training, we propose Double Balancing, which can largely improve the performance of existing baselines with just a few lines of code as a simple plug-and-play module. Secondly, to enhance the long-range propagation capability of GNNs, we disentangle the propagation and transformation operations of GNNs. Therefore, the weak supervision signals can propagate more effectively to address the few-shot issue. In summary, we find that leveraging unlabeled nodes can significantly enhance the performance of GNNs in class-imbalanced and few-shot scenarios, and even small, surgical modifications can lead to substantial performance improvements. Systematic experiments on benchmark datasets show that our method can deliver considerable performance gain over existing class-imbalanced node classification baselines. Additionally, due to IceBerg's outstanding ability to leverage unsupervised signals, it also achieves state-of-the-art results in few-shot node classification scenarios. The code of IceBerg is available at: https://github.com/ZhixunLEE/IceBerg.","sentences":["Graph Neural Networks (GNNs) have achieved great success in dealing with non-Euclidean graph-structured data and have been widely deployed in many real-world applications.","However, their effectiveness is often jeopardized under class-imbalanced training sets.","Most existing studies have analyzed class-imbalanced node classification from a supervised learning perspective, but they do not fully utilize the large number of unlabeled nodes in semi-supervised scenarios.","We claim that the supervised signal is just the tip of the iceberg and a large number of unlabeled nodes have not yet been effectively utilized.","In this work, we propose IceBerg, a debiased self-training framework to address the class-imbalanced and few-shot challenges for GNNs at the same time.","Specifically, to figure out the Matthew effect and label distribution shift in self-training, we propose Double Balancing, which can largely improve the performance of existing baselines with just a few lines of code as a simple plug-and-play module.","Secondly, to enhance the long-range propagation capability of GNNs, we disentangle the propagation and transformation operations of GNNs.","Therefore, the weak supervision signals can propagate more effectively to address the few-shot issue.","In summary, we find that leveraging unlabeled nodes can significantly enhance the performance of GNNs in class-imbalanced and few-shot scenarios, and even small, surgical modifications can lead to substantial performance improvements.","Systematic experiments on benchmark datasets show that our method can deliver considerable performance gain over existing class-imbalanced node classification baselines.","Additionally, due to IceBerg's outstanding ability to leverage unsupervised signals, it also achieves state-of-the-art results in few-shot node classification scenarios.","The code of IceBerg is available at: https://github.com/ZhixunLEE/IceBerg."],"url":"http://arxiv.org/abs/2502.06280v1"}
{"created":"2025-02-10 09:23:03","title":"DebateBench: A Challenging Long Context Reasoning Benchmark For Large Language Models","abstract":"We introduce DebateBench, a novel dataset consisting of an extensive collection of transcripts and metadata from some of the world's most prestigious competitive debates. The dataset consists of British Parliamentary debates from prestigious debating tournaments on diverse topics, annotated with detailed speech-level scores and house rankings sourced from official adjudication data. We curate 256 speeches across 32 debates with each debate being over 1 hour long with each input being an average of 32,000 tokens. Designed to capture long-context, large-scale reasoning tasks, DebateBench provides a benchmark for evaluating modern large language models (LLMs) on their ability to engage in argumentation, deliberation, and alignment with human experts. To do well on DebateBench, the LLMs must perform in-context learning to understand the rules and evaluation criteria of the debates, then analyze 8 seven minute long speeches and reason about the arguments presented by all speakers to give the final results. Our preliminary evaluation using GPT o1, GPT-4o, and Claude Haiku, shows that LLMs struggle to perform well on DebateBench, highlighting the need to develop more sophisticated techniques for improving their performance.","sentences":["We introduce DebateBench, a novel dataset consisting of an extensive collection of transcripts and metadata from some of the world's most prestigious competitive debates.","The dataset consists of British Parliamentary debates from prestigious debating tournaments on diverse topics, annotated with detailed speech-level scores and house rankings sourced from official adjudication data.","We curate 256 speeches across 32 debates with each debate being over 1 hour long with each input being an average of 32,000 tokens.","Designed to capture long-context, large-scale reasoning tasks, DebateBench provides a benchmark for evaluating modern large language models (LLMs) on their ability to engage in argumentation, deliberation, and alignment with human experts.","To do well on DebateBench, the LLMs must perform in-context learning to understand the rules and evaluation criteria of the debates, then analyze 8 seven minute long speeches and reason about the arguments presented by all speakers to give the final results.","Our preliminary evaluation using GPT o1, GPT-4o, and Claude Haiku, shows that LLMs struggle to perform well on DebateBench, highlighting the need to develop more sophisticated techniques for improving their performance."],"url":"http://arxiv.org/abs/2502.06279v1"}
{"created":"2025-02-10 09:13:11","title":"Beyond Batch Learning: Global Awareness Enhanced Domain Adaptation","abstract":"In domain adaptation (DA), the effectiveness of deep learning-based models is often constrained by batch learning strategies that fail to fully apprehend the global statistical and geometric characteristics of data distributions. Addressing this gap, we introduce 'Global Awareness Enhanced Domain Adaptation' (GAN-DA), a novel approach that transcends traditional batch-based limitations. GAN-DA integrates a unique predefined feature representation (PFR) to facilitate the alignment of cross-domain distributions, thereby achieving a comprehensive global statistical awareness. This representation is innovatively expanded to encompass orthogonal and common feature aspects, which enhances the unification of global manifold structures and refines decision boundaries for more effective DA. Our extensive experiments, encompassing 27 diverse cross-domain image classification tasks, demonstrate GAN-DA's remarkable superiority, outperforming 24 established DA methods by a significant margin. Furthermore, our in-depth analyses shed light on the decision-making processes, revealing insights into the adaptability and efficiency of GAN-DA. This approach not only addresses the limitations of existing DA methodologies but also sets a new benchmark in the realm of domain adaptation, offering broad implications for future research and applications in this field.","sentences":["In domain adaptation (DA), the effectiveness of deep learning-based models is often constrained by batch learning strategies that fail to fully apprehend the global statistical and geometric characteristics of data distributions.","Addressing this gap, we introduce 'Global Awareness Enhanced Domain Adaptation' (GAN-DA), a novel approach that transcends traditional batch-based limitations.","GAN-DA integrates a unique predefined feature representation (PFR) to facilitate the alignment of cross-domain distributions, thereby achieving a comprehensive global statistical awareness.","This representation is innovatively expanded to encompass orthogonal and common feature aspects, which enhances the unification of global manifold structures and refines decision boundaries for more effective DA.","Our extensive experiments, encompassing 27 diverse cross-domain image classification tasks, demonstrate GAN-DA's remarkable superiority, outperforming 24 established DA methods by a significant margin.","Furthermore, our in-depth analyses shed light on the decision-making processes, revealing insights into the adaptability and efficiency of GAN-DA.","This approach not only addresses the limitations of existing DA methodologies but also sets a new benchmark in the realm of domain adaptation, offering broad implications for future research and applications in this field."],"url":"http://arxiv.org/abs/2502.06272v1"}
{"created":"2025-02-10 08:28:46","title":"DGNO: A Novel Physics-aware Neural Operator for Solving Forward and Inverse PDE Problems based on Deep, Generative Probabilistic Modeling","abstract":"Solving parametric partial differential equations (PDEs) and associated PDE-based, inverse problems is a central task in engineering and physics, yet existing neural operator methods struggle with high-dimensional, discontinuous inputs and require large amounts of {\\em labeled} training data. We propose the Deep Generative Neural Operator (DGNO), a physics-aware framework that addresses these challenges by leveraging a deep, generative, probabilistic model in combination with a set of lower-dimensional, latent variables that simultaneously encode PDE-inputs and PDE-outputs. This formulation can make use of unlabeled data and significantly improves inverse problem-solving, particularly for discontinuous or discrete-valued input functions. DGNO enforces physics constraints without labeled data by incorporating as virtual observables, weak-form residuals based on compactly supported radial basis functions (CSRBFs). These relax regularity constraints and eliminate higher-order derivatives from the objective function. We also introduce MultiONet, a novel neural operator architecture, which is a more expressive generalization of the popular DeepONet that significantly enhances the approximating power of the proposed model. These innovations make DGNO particularly effective for challenging forward and inverse, PDE-based problems, such as those involving multi-phase media. Numerical experiments demonstrate that DGNO achieves higher accuracy across multiple benchmarks while exhibiting robustness to noise and strong generalization to out-of-distribution cases. Its adaptability, and the ability to handle sparse, noisy data while providing probabilistic estimates, make DGNO a powerful tool for scientific and engineering applications.","sentences":["Solving parametric partial differential equations (PDEs) and associated PDE-based, inverse problems is a central task in engineering and physics, yet existing neural operator methods struggle with high-dimensional, discontinuous inputs and require large amounts of {\\em labeled} training data.","We propose the Deep Generative Neural Operator (DGNO), a physics-aware framework that addresses these challenges by leveraging a deep, generative, probabilistic model in combination with a set of lower-dimensional, latent variables that simultaneously encode PDE-inputs and PDE-outputs.","This formulation can make use of unlabeled data and significantly improves inverse problem-solving, particularly for discontinuous or discrete-valued input functions.","DGNO enforces physics constraints without labeled data by incorporating as virtual observables, weak-form residuals based on compactly supported radial basis functions (CSRBFs).","These relax regularity constraints and eliminate higher-order derivatives from the objective function.","We also introduce MultiONet, a novel neural operator architecture, which is a more expressive generalization of the popular DeepONet that significantly enhances the approximating power of the proposed model.","These innovations make DGNO particularly effective for challenging forward and inverse, PDE-based problems, such as those involving multi-phase media.","Numerical experiments demonstrate that DGNO achieves higher accuracy across multiple benchmarks while exhibiting robustness to noise and strong generalization to out-of-distribution cases.","Its adaptability, and the ability to handle sparse, noisy data while providing probabilistic estimates, make DGNO a powerful tool for scientific and engineering applications."],"url":"http://arxiv.org/abs/2502.06250v1"}
{"created":"2025-02-10 08:23:05","title":"PiKE: Adaptive Data Mixing for Multi-Task Learning Under Low Gradient Conflicts","abstract":"Modern machine learning models are trained on diverse datasets and tasks to improve generalization. A key challenge in multitask learning is determining the optimal data mixing and sampling strategy across different data sources. Prior research in this multi-task learning setting has primarily focused on mitigating gradient conflicts between tasks. However, we observe that many real-world multitask learning scenarios-such as multilingual training and multi-domain learning in large foundation models-exhibit predominantly positive task interactions with minimal or no gradient conflict. Building on this insight, we introduce PiKE (Positive gradient interaction-based K-task weights Estimator), an adaptive data mixing algorithm that dynamically adjusts task contributions throughout training. PiKE optimizes task sampling to minimize overall loss, effectively leveraging positive gradient interactions with almost no additional computational overhead. We establish theoretical convergence guarantees for PiKE and demonstrate its superiority over static and non-adaptive mixing strategies. Additionally, we extend PiKE to promote fair learning across tasks, ensuring balanced progress and preventing task underrepresentation. Empirical evaluations on large-scale language model pretraining show that PiKE consistently outperforms existing heuristic and static mixing strategies, leading to faster convergence and improved downstream task performance.","sentences":["Modern machine learning models are trained on diverse datasets and tasks to improve generalization.","A key challenge in multitask learning is determining the optimal data mixing and sampling strategy across different data sources.","Prior research in this multi-task learning setting has primarily focused on mitigating gradient conflicts between tasks.","However, we observe that many real-world multitask learning scenarios-such as multilingual training and multi-domain learning in large foundation models-exhibit predominantly positive task interactions with minimal or no gradient conflict.","Building on this insight, we introduce PiKE (Positive gradient interaction-based K-task weights Estimator), an adaptive data mixing algorithm that dynamically adjusts task contributions throughout training.","PiKE optimizes task sampling to minimize overall loss, effectively leveraging positive gradient interactions with almost no additional computational overhead.","We establish theoretical convergence guarantees for PiKE and demonstrate its superiority over static and non-adaptive mixing strategies.","Additionally, we extend PiKE to promote fair learning across tasks, ensuring balanced progress and preventing task underrepresentation.","Empirical evaluations on large-scale language model pretraining show that PiKE consistently outperforms existing heuristic and static mixing strategies, leading to faster convergence and improved downstream task performance."],"url":"http://arxiv.org/abs/2502.06244v1"}
{"created":"2025-02-10 08:22:25","title":"Multi-Scale Transformer Architecture for Accurate Medical Image Classification","abstract":"This study introduces an AI-driven skin lesion classification algorithm built on an enhanced Transformer architecture, addressing the challenges of accuracy and robustness in medical image analysis. By integrating a multi-scale feature fusion mechanism and refining the self-attention process, the model effectively extracts both global and local features, enhancing its ability to detect lesions with ambiguous boundaries and intricate structures. Performance evaluation on the ISIC 2017 dataset demonstrates that the improved Transformer surpasses established AI models, including ResNet50, VGG19, ResNext, and Vision Transformer, across key metrics such as accuracy, AUC, F1-Score, and Precision. Grad-CAM visualizations further highlight the interpretability of the model, showcasing strong alignment between the algorithm's focus areas and actual lesion sites. This research underscores the transformative potential of advanced AI models in medical imaging, paving the way for more accurate and reliable diagnostic tools. Future work will explore the scalability of this approach to broader medical imaging tasks and investigate the integration of multimodal data to enhance AI-driven diagnostic frameworks for intelligent healthcare.","sentences":["This study introduces an AI-driven skin lesion classification algorithm built on an enhanced Transformer architecture, addressing the challenges of accuracy and robustness in medical image analysis.","By integrating a multi-scale feature fusion mechanism and refining the self-attention process, the model effectively extracts both global and local features, enhancing its ability to detect lesions with ambiguous boundaries and intricate structures.","Performance evaluation on the ISIC 2017 dataset demonstrates that the improved Transformer surpasses established AI models, including ResNet50, VGG19, ResNext, and Vision Transformer, across key metrics such as accuracy, AUC, F1-Score, and Precision.","Grad-CAM visualizations further highlight the interpretability of the model, showcasing strong alignment between the algorithm's focus areas and actual lesion sites.","This research underscores the transformative potential of advanced AI models in medical imaging, paving the way for more accurate and reliable diagnostic tools.","Future work will explore the scalability of this approach to broader medical imaging tasks and investigate the integration of multimodal data to enhance AI-driven diagnostic frameworks for intelligent healthcare."],"url":"http://arxiv.org/abs/2502.06243v1"}
{"created":"2025-02-10 07:58:49","title":"Unsupervised deep learning for semantic segmentation of multispectral LiDAR forest point clouds","abstract":"Point clouds captured with laser scanning systems from forest environments can be utilized in a wide variety of applications within forestry and plant ecology, such as the estimation of tree stem attributes, leaf angle distribution, and above-ground biomass. However, effectively utilizing the data in such tasks requires the semantic segmentation of the data into wood and foliage points, also known as leaf-wood separation. The traditional approach to leaf-wood separation has been geometry- and radiometry-based unsupervised algorithms, which tend to perform poorly on data captured with airborne laser scanning (ALS) systems, even with a high point density. While recent machine and deep learning approaches achieve great results even on sparse point clouds, they require manually labeled training data, which is often extremely laborious to produce. Multispectral (MS) information has been demonstrated to have potential for improving the accuracy of leaf-wood separation, but quantitative assessment of its effects has been lacking. This study proposes a fully unsupervised deep learning method, GrowSP-ForMS, which is specifically designed for leaf-wood separation of high-density MS ALS point clouds and based on the GrowSP architecture. GrowSP-ForMS achieved a mean accuracy of 84.3% and a mean intersection over union (mIoU) of 69.6% on our MS test set, outperforming the unsupervised reference methods by a significant margin. When compared to supervised deep learning methods, our model performed similarly to the slightly older PointNet architecture but was outclassed by more recent approaches. Finally, two ablation studies were conducted, which demonstrated that our proposed changes increased the test set mIoU of GrowSP-ForMS by 29.4 percentage points (pp) in comparison to the original GrowSP model and that utilizing MS data improved the mIoU by 5.6 pp from the monospectral case.","sentences":["Point clouds captured with laser scanning systems from forest environments can be utilized in a wide variety of applications within forestry and plant ecology, such as the estimation of tree stem attributes, leaf angle distribution, and above-ground biomass.","However, effectively utilizing the data in such tasks requires the semantic segmentation of the data into wood and foliage points, also known as leaf-wood separation.","The traditional approach to leaf-wood separation has been geometry- and radiometry-based unsupervised algorithms, which tend to perform poorly on data captured with airborne laser scanning (ALS) systems, even with a high point density.","While recent machine and deep learning approaches achieve great results even on sparse point clouds, they require manually labeled training data, which is often extremely laborious to produce.","Multispectral (MS) information has been demonstrated to have potential for improving the accuracy of leaf-wood separation, but quantitative assessment of its effects has been lacking.","This study proposes a fully unsupervised deep learning method, GrowSP-ForMS, which is specifically designed for leaf-wood separation of high-density MS ALS point clouds and based on the GrowSP architecture.","GrowSP-ForMS achieved a mean accuracy of 84.3% and a mean intersection over union (mIoU) of 69.6% on our MS test set, outperforming the unsupervised reference methods by a significant margin.","When compared to supervised deep learning methods, our model performed similarly to the slightly older PointNet architecture but was outclassed by more recent approaches.","Finally, two ablation studies were conducted, which demonstrated that our proposed changes increased the test set mIoU of GrowSP-ForMS by 29.4 percentage points (pp) in comparison to the original GrowSP model and that utilizing MS data improved the mIoU by 5.6 pp from the monospectral case."],"url":"http://arxiv.org/abs/2502.06227v1"}
{"created":"2025-02-10 07:50:22","title":"Fully Exploiting Vision Foundation Model's Profound Prior Knowledge for Generalizable RGB-Depth Driving Scene Parsing","abstract":"Recent vision foundation models (VFMs), typically based on Vision Transformer (ViT), have significantly advanced numerous computer vision tasks. Despite their success in tasks focused solely on RGB images, the potential of VFMs in RGB-depth driving scene parsing remains largely under-explored. In this article, we take one step toward this emerging research area by investigating a feasible technique to fully exploit VFMs for generalizable RGB-depth driving scene parsing. Specifically, we explore the inherent characteristics of RGB and depth data, thereby presenting a Heterogeneous Feature Integration Transformer (HFIT). This network enables the efficient extraction and integration of comprehensive heterogeneous features without re-training ViTs. Relative depth prediction results from VFMs, used as inputs to the HFIT side adapter, overcome the limitations of the dependence on depth maps. Our proposed HFIT demonstrates superior performance compared to all other traditional single-modal and data-fusion scene parsing networks, pre-trained VFMs, and ViT adapters on the Cityscapes and KITTI Semantics datasets. We believe this novel strategy paves the way for future innovations in VFM-based data-fusion techniques for driving scene parsing. Our source code is publicly available at https://mias.group/HFIT.","sentences":["Recent vision foundation models (VFMs), typically based on Vision Transformer (ViT), have significantly advanced numerous computer vision tasks.","Despite their success in tasks focused solely on RGB images, the potential of VFMs in RGB-depth driving scene parsing remains largely under-explored.","In this article, we take one step toward this emerging research area by investigating a feasible technique to fully exploit VFMs for generalizable RGB-depth driving scene parsing.","Specifically, we explore the inherent characteristics of RGB and depth data, thereby presenting a Heterogeneous Feature Integration Transformer (HFIT).","This network enables the efficient extraction and integration of comprehensive heterogeneous features without re-training ViTs.","Relative depth prediction results from VFMs, used as inputs to the HFIT side adapter, overcome the limitations of the dependence on depth maps.","Our proposed HFIT demonstrates superior performance compared to all other traditional single-modal and data-fusion scene parsing networks, pre-trained VFMs, and ViT adapters on the Cityscapes and KITTI Semantics datasets.","We believe this novel strategy paves the way for future innovations in VFM-based data-fusion techniques for driving scene parsing.","Our source code is publicly available at https://mias.group/HFIT."],"url":"http://arxiv.org/abs/2502.06219v1"}
{"created":"2025-02-10 07:49:35","title":"Examining False Positives under Inference Scaling for Mathematical Reasoning","abstract":"Recent advancements in language models have led to significant improvements in mathematical reasoning across various benchmarks. However, most of these benchmarks rely on automatic evaluation methods that only compare final answers using heuristics, without verifying the underlying reasoning steps. This limitation results in false positive solutions, where models may produce correct final answers but with flawed deduction paths. In this paper, we systematically examine the prevalence of false positive solutions in mathematical problem solving for language models. We analyze the characteristics and extent of this issue across different open-source models, datasets of varying difficulty levels, and decoding strategies. Specifically, we explore how false positives influence the inference time scaling behavior of language models. Our experimental results reveal that: (1) false positive solutions persist across different models, datasets, and decoding methods, (2) sampling-based inference time scaling methods do not alleviate the problem, and (3) the pass@N evaluation metric is more susceptible to false positives, suggesting a significantly lower scaling ceiling than what automatic evaluations indicate. Additionally, we analyze specific instances of false positives and discuss potential limitations in self-improvement techniques and synthetic data generation under such conditions.","sentences":["Recent advancements in language models have led to significant improvements in mathematical reasoning across various benchmarks.","However, most of these benchmarks rely on automatic evaluation methods that only compare final answers using heuristics, without verifying the underlying reasoning steps.","This limitation results in false positive solutions, where models may produce correct final answers but with flawed deduction paths.","In this paper, we systematically examine the prevalence of false positive solutions in mathematical problem solving for language models.","We analyze the characteristics and extent of this issue across different open-source models, datasets of varying difficulty levels, and decoding strategies.","Specifically, we explore how false positives influence the inference time scaling behavior of language models.","Our experimental results reveal that: (1) false positive solutions persist across different models, datasets, and decoding methods, (2) sampling-based inference time scaling methods do not alleviate the problem, and (3) the pass@N evaluation metric is more susceptible to false positives, suggesting a significantly lower scaling ceiling than what automatic evaluations indicate.","Additionally, we analyze specific instances of false positives and discuss potential limitations in self-improvement techniques and synthetic data generation under such conditions."],"url":"http://arxiv.org/abs/2502.06217v1"}
{"created":"2025-02-10 07:33:49","title":"LessLeak-Bench: A First Investigation of Data Leakage in LLMs Across 83 Software Engineering Benchmarks","abstract":"Large Language Models (LLMs) are widely utilized in software engineering (SE) tasks, such as code generation and automated program repair. However, their reliance on extensive and often undisclosed pre-training datasets raises significant concerns about data leakage, where the evaluation benchmark data is unintentionally ``seen'' by LLMs during the model's construction phase. The data leakage issue could largely undermine the validity of LLM-based research and evaluations. Despite the increasing use of LLMs in the SE community, there is no comprehensive study that assesses the extent of data leakage in SE benchmarks for LLMs yet. To address this gap, this paper presents the first large-scale analysis of data leakage in 83 SE benchmarks concerning LLMs. Our results show that in general, data leakage in SE benchmarks is minimal, with average leakage ratios of only 4.8\\%, 2.8\\%, and 0.7\\% for Python, Java, and C/C++ benchmarks, respectively. However, some benchmarks exhibit relatively higher leakage ratios, which raises concerns about their bias in evaluation. For instance, QuixBugs and BigCloneBench have leakage ratios of 100.0\\% and 55.7\\%, respectively. Furthermore, we observe that data leakage has a substantial impact on LLM evaluation. We also identify key causes of high data leakage, such as the direct inclusion of benchmark data in pre-training datasets and the use of coding platforms like LeetCode for benchmark construction. To address the data leakage, we introduce \\textbf{LessLeak-Bench}, a new benchmark that removes leaked samples from the 83 SE benchmarks, enabling more reliable LLM evaluations in future research. Our study enhances the understanding of data leakage in SE benchmarks and provides valuable insights for future research involving LLMs in SE.","sentences":["Large Language Models (LLMs) are widely utilized in software engineering (SE) tasks, such as code generation and automated program repair.","However, their reliance on extensive and often undisclosed pre-training datasets raises significant concerns about data leakage, where the evaluation benchmark data is unintentionally ``seen'' by LLMs during the model's construction phase.","The data leakage issue could largely undermine the validity of LLM-based research and evaluations.","Despite the increasing use of LLMs in the SE community, there is no comprehensive study that assesses the extent of data leakage in SE benchmarks for LLMs yet.","To address this gap, this paper presents the first large-scale analysis of data leakage in 83 SE benchmarks concerning LLMs.","Our results show that in general, data leakage in SE benchmarks is minimal, with average leakage ratios of only 4.8\\%, 2.8\\%, and 0.7\\% for Python, Java, and C/C++ benchmarks, respectively.","However, some benchmarks exhibit relatively higher leakage ratios, which raises concerns about their bias in evaluation.","For instance, QuixBugs and BigCloneBench have leakage ratios of 100.0\\% and 55.7\\%, respectively.","Furthermore, we observe that data leakage has a substantial impact on LLM evaluation.","We also identify key causes of high data leakage, such as the direct inclusion of benchmark data in pre-training datasets and the use of coding platforms like LeetCode for benchmark construction.","To address the data leakage, we introduce \\textbf{LessLeak-Bench}, a new benchmark that removes leaked samples from the 83 SE benchmarks, enabling more reliable LLM evaluations in future research.","Our study enhances the understanding of data leakage in SE benchmarks and provides valuable insights for future research involving LLMs in SE."],"url":"http://arxiv.org/abs/2502.06215v1"}
{"created":"2025-02-10 07:20:28","title":"Enhancing Cost Efficiency in Active Learning with Candidate Set Query","abstract":"This paper introduces a cost-efficient active learning (AL) framework for classification, featuring a novel query design called candidate set query. Unlike traditional AL queries requiring the oracle to examine all possible classes, our method narrows down the set of candidate classes likely to include the ground-truth class, significantly reducing the search space and labeling cost. Moreover, we leverage conformal prediction to dynamically generate small yet reliable candidate sets, adapting to model enhancement over successive AL rounds. To this end, we introduce an acquisition function designed to prioritize data points that offer high information gain at lower cost. Empirical evaluations on CIFAR-10, CIFAR-100, and ImageNet64x64 demonstrate the effectiveness and scalability of our framework. Notably, it reduces labeling cost by 42% on ImageNet64x64.","sentences":["This paper introduces a cost-efficient active learning (AL) framework for classification, featuring a novel query design called candidate set query.","Unlike traditional AL queries requiring the oracle to examine all possible classes, our method narrows down the set of candidate classes likely to include the ground-truth class, significantly reducing the search space and labeling cost.","Moreover, we leverage conformal prediction to dynamically generate small yet reliable candidate sets, adapting to model enhancement over successive AL rounds.","To this end, we introduce an acquisition function designed to prioritize data points that offer high information gain at lower cost.","Empirical evaluations on CIFAR-10, CIFAR-100, and ImageNet64x64 demonstrate the effectiveness and scalability of our framework.","Notably, it reduces labeling cost by 42% on ImageNet64x64."],"url":"http://arxiv.org/abs/2502.06209v1"}
{"created":"2025-02-10 07:03:00","title":"Non-literal Understanding of Number Words by Language Models","abstract":"Humans naturally interpret numbers non-literally, effortlessly combining context, world knowledge, and speaker intent. We investigate whether large language models (LLMs) interpret numbers similarly, focusing on hyperbole and pragmatic halo effects. Through systematic comparison with human data and computational models of pragmatic reasoning, we find that LLMs diverge from human interpretation in striking ways. By decomposing pragmatic reasoning into testable components, grounded in the Rational Speech Act framework, we pinpoint where LLM processing diverges from human cognition -- not in prior knowledge, but in reasoning with it. This insight leads us to develop a targeted solution -- chain-of-thought prompting inspired by an RSA model makes LLMs' interpretations more human-like. Our work demonstrates how computational cognitive models can both diagnose AI-human differences and guide development of more human-like language understanding capabilities.","sentences":["Humans naturally interpret numbers non-literally, effortlessly combining context, world knowledge, and speaker intent.","We investigate whether large language models (LLMs) interpret numbers similarly, focusing on hyperbole and pragmatic halo effects.","Through systematic comparison with human data and computational models of pragmatic reasoning, we find that LLMs diverge from human interpretation in striking ways.","By decomposing pragmatic reasoning into testable components, grounded in the Rational Speech Act framework, we pinpoint where LLM processing diverges from human cognition -- not in prior knowledge, but in reasoning with it.","This insight leads us to develop a targeted solution -- chain-of-thought prompting inspired by an RSA model makes LLMs' interpretations more human-like.","Our work demonstrates how computational cognitive models can both diagnose AI-human differences and guide development of more human-like language understanding capabilities."],"url":"http://arxiv.org/abs/2502.06204v1"}
{"created":"2025-02-10 06:54:16","title":"On the query complexity of sampling from non-log-concave distributions","abstract":"We study the problem of sampling from a $d$-dimensional distribution with density $p(x)\\propto e^{-f(x)}$, which does not necessarily satisfy good isoperimetric conditions.   Specifically, we show that for any $L,M$ satisfying $LM\\ge d\\ge 5$, $\\epsilon\\in \\left\\{0,\\frac{1}{32}\\right\\}$, and any algorithm with query accesses to the value of $f(x)$ and $\\nabla f(x)$, there exists an $L$-log-smooth distribution with second moment at most $M$ such that the algorithm requires $\\left\\{\\frac{LM}{d\\epsilon}\\right\\}^{\\Omega(d)}$ queries to compute a sample whose distribution is within $\\epsilon$ in total variation distance to the target distribution. We complement the lower bound with an algorithm requiring $\\left\\{\\frac{LM}{d\\epsilon}\\right\\}^{\\mathcal O(d)}$ queries, thereby characterizing the tight (up to the constant in the exponent) query complexity for sampling from the family of non-log-concave distributions.   Our results are in sharp contrast with the recent work of Huang et al. (COLT'24), where an algorithm with quasi-polynomial query complexity was proposed for sampling from a non-log-concave distribution when $M=\\mathtt{poly}(d)$. Their algorithm works under the stronger condition that all distributions along the trajectory of the Ornstein-Uhlenbeck process, starting from the target distribution, are $\\mathcal O(1)$-log-smooth. We investigate this condition and prove that it is strictly stronger than requiring the target distribution to be $\\mathcal O(1)$-log-smooth. Additionally, we study this condition in the context of mixtures of Gaussians.   Finally, we place our results within the broader theme of ``sampling versus optimization'', as studied in Ma et al. (PNAS'19). We show that for a wide range of parameters, sampling is strictly easier than optimization by a super-exponential factor in the dimension $d$.","sentences":["We study the problem of sampling from a $d$-dimensional distribution with density $p(x)\\propto e^{-f(x)}$, which does not necessarily satisfy good isoperimetric conditions.   ","Specifically, we show that for any $L,M$ satisfying $LM\\ge d\\ge 5$, $\\epsilon\\in \\left\\{0,\\frac{1}{32}\\right\\}$, and any algorithm with query accesses to the value of $f(x)$ and $\\nabla f(x)$, there exists an $L$-log-smooth distribution with second moment at most $M$ such that the algorithm requires $\\left\\{\\frac{LM}{d\\epsilon}\\right\\}^{\\Omega(d)}$ queries to compute a sample whose distribution is within $\\epsilon$ in total variation distance to the target distribution.","We complement the lower bound with an algorithm requiring $\\left\\{\\frac{LM}{d\\epsilon}\\right\\}^{\\mathcal O(d)}$ queries, thereby characterizing the tight (up to the constant in the exponent) query complexity for sampling from the family of non-log-concave distributions.   ","Our results are in sharp contrast with the recent work of Huang et al. (COLT'24), where an algorithm with quasi-polynomial query complexity was proposed for sampling from a non-log-concave distribution when $M=\\mathtt{poly}(d)$. Their algorithm works under the stronger condition that all distributions along the trajectory of the Ornstein-Uhlenbeck process, starting from the target distribution, are $\\mathcal O(1)$-log-smooth.","We investigate this condition and prove that it is strictly stronger than requiring the target distribution to be $\\mathcal O(1)$-log-smooth.","Additionally, we study this condition in the context of mixtures of Gaussians.   ","Finally, we place our results within the broader theme of ``sampling versus optimization'', as studied in Ma et al. (PNAS'19).","We show that for a wide range of parameters, sampling is strictly easier than optimization by a super-exponential factor in the dimension $d$."],"url":"http://arxiv.org/abs/2502.06200v1"}
{"created":"2025-02-10 06:51:23","title":"Improved Extrinsic Calibration of Acoustic Cameras via Batch Optimization","abstract":"Acoustic cameras have found many applications in practice. Accurate and reliable extrinsic calibration of the microphone array and visual sensors within acoustic cameras is crucial for fusing visual and auditory measurements. Existing calibration methods either require prior knowledge of the microphone array geometry or rely on grid search which suffers from slow iteration speed or poor convergence. To overcome these limitations, in this paper, we propose an automatic calibration technique using a calibration board with both visual and acoustic markers to identify each microphone position in the camera frame. We formulate the extrinsic calibration problem (between microphones and the visual sensor) as a nonlinear least squares problem and employ a batch optimization strategy to solve the associated problem. Extensive numerical simulations and realworld experiments show that the proposed method improves both the accuracy and robustness of extrinsic parameter calibration for acoustic cameras, in comparison to existing methods. To benefit the community, we open-source all the codes and data at https://github.com/AISLAB-sustech/AcousticCamera.","sentences":["Acoustic cameras have found many applications in practice.","Accurate and reliable extrinsic calibration of the microphone array and visual sensors within acoustic cameras is crucial for fusing visual and auditory measurements.","Existing calibration methods either require prior knowledge of the microphone array geometry or rely on grid search which suffers from slow iteration speed or poor convergence.","To overcome these limitations, in this paper, we propose an automatic calibration technique using a calibration board with both visual and acoustic markers to identify each microphone position in the camera frame.","We formulate the extrinsic calibration problem (between microphones and the visual sensor) as a nonlinear least squares problem and employ a batch optimization strategy to solve the associated problem.","Extensive numerical simulations and realworld experiments show that the proposed method improves both the accuracy and robustness of extrinsic parameter calibration for acoustic cameras, in comparison to existing methods.","To benefit the community, we open-source all the codes and data at https://github.com/AISLAB-sustech/AcousticCamera."],"url":"http://arxiv.org/abs/2502.06196v1"}
{"created":"2025-02-10 06:50:27","title":"Calibration of Multiple Asynchronous Microphone Arrays using Hybrid TDOA","abstract":"Accurate calibration of acoustic sensing systems made of multiple asynchronous microphone arrays is essential for satisfactory performance in sound source localization and tracking. State-of-the-art calibration methods for this type of system rely on the time difference of arrival and direction of arrival measurements among the microphone arrays (denoted as TDOA-M and DOA, respectively). In this paper, to enhance calibration accuracy, we propose to incorporate the time difference of arrival measurements between adjacent sound events (TDOAS) with respect to the microphone arrays. More specifically, we propose a two-stage calibration approach, including an initial value estimation (IVE) procedure and the final joint optimization step. The IVE stage first initializes all parameters except for microphone array orientations, using hybrid TDOA (i.e., TDOAM and TDOA-S), odometer data from a moving robot carrying a speaker, and DOA. Subsequently, microphone orientations are estimated through the iterative closest point method. The final joint optimization step estimates multiple microphone array locations, orientations, time offsets, clock drift rates, and sound source locations simultaneously. Both simulation and experiment results show that for scenarios with low or moderate TDOA noise levels, our approach outperforms existing methods in terms of accuracy. All code and data are available at https://github.com/AISLABsustech/Hybrid-TDOA-Multi-Calib.","sentences":["Accurate calibration of acoustic sensing systems made of multiple asynchronous microphone arrays is essential for satisfactory performance in sound source localization and tracking.","State-of-the-art calibration methods for this type of system rely on the time difference of arrival and direction of arrival measurements among the microphone arrays (denoted as TDOA-M and DOA, respectively).","In this paper, to enhance calibration accuracy, we propose to incorporate the time difference of arrival measurements between adjacent sound events (TDOAS) with respect to the microphone arrays.","More specifically, we propose a two-stage calibration approach, including an initial value estimation (IVE) procedure and the final joint optimization step.","The IVE stage first initializes all parameters except for microphone array orientations, using hybrid TDOA (i.e., TDOAM and TDOA-S), odometer data from a moving robot carrying a speaker, and DOA.","Subsequently, microphone orientations are estimated through the iterative closest point method.","The final joint optimization step estimates multiple microphone array locations, orientations, time offsets, clock drift rates, and sound source locations simultaneously.","Both simulation and experiment results show that for scenarios with low or moderate TDOA noise levels, our approach outperforms existing methods in terms of accuracy.","All code and data are available at https://github.com/AISLABsustech/Hybrid-TDOA-Multi-Calib."],"url":"http://arxiv.org/abs/2502.06195v1"}
{"created":"2025-02-10 06:44:26","title":"Is Science Inevitable?","abstract":"Using large-scale citation data and a breakthrough metric, the study systematically evaluates the inevitability of scientific breakthroughs. We find that scientific breakthroughs emerge as multiple discoveries rather than singular events. Through analysis of over 40 million journal articles, we identify multiple discoveries as papers that independently displace the same reference using the Disruption Index (D-index), suggesting functional equivalence. Our findings support Merton's core argument that scientific discoveries arise from historical context rather than individual genius. The results reveal a long-tail distribution pattern of multiple discoveries across various datasets, challenging Merton's Poisson model while reinforcing the structural inevitability of scientific progress.","sentences":["Using large-scale citation data and a breakthrough metric, the study systematically evaluates the inevitability of scientific breakthroughs.","We find that scientific breakthroughs emerge as multiple discoveries rather than singular events.","Through analysis of over 40 million journal articles, we identify multiple discoveries as papers that independently displace the same reference using the Disruption Index (D-index), suggesting functional equivalence.","Our findings support Merton's core argument that scientific discoveries arise from historical context rather than individual genius.","The results reveal a long-tail distribution pattern of multiple discoveries across various datasets, challenging Merton's Poisson model while reinforcing the structural inevitability of scientific progress."],"url":"http://arxiv.org/abs/2502.06190v1"}
{"created":"2025-02-10 06:18:07","title":"RideKE: Leveraging Low-Resource, User-Generated Twitter Content for Sentiment and Emotion Detection in Kenyan Code-Switched Dataset","abstract":"Social media has become a crucial open-access platform for individuals to express opinions and share experiences. However, leveraging low-resource language data from Twitter is challenging due to scarce, poor-quality content and the major variations in language use, such as slang and code-switching. Identifying tweets in these languages can be difficult as Twitter primarily supports high-resource languages. We analyze Kenyan code-switched data and evaluate four state-of-the-art (SOTA) transformer-based pretrained models for sentiment and emotion classification, using supervised and semi-supervised methods. We detail the methodology behind data collection and annotation, and the challenges encountered during the data curation phase. Our results show that XLM-R outperforms other models; for sentiment analysis, XLM-R supervised model achieves the highest accuracy (69.2\\%) and F1 score (66.1\\%), XLM-R semi-supervised (67.2\\% accuracy, 64.1\\% F1 score). In emotion analysis, DistilBERT supervised leads in accuracy (59.8\\%) and F1 score (31\\%), mBERT semi-supervised (accuracy (59\\% and F1 score 26.5\\%). AfriBERTa models show the lowest accuracy and F1 scores. All models tend to predict neutral sentiment, with Afri-BERT showing the highest bias and unique sensitivity to empathy emotion. https://github.com/NEtori21/Ride_hailing","sentences":["Social media has become a crucial open-access platform for individuals to express opinions and share experiences.","However, leveraging low-resource language data from Twitter is challenging due to scarce, poor-quality content and the major variations in language use, such as slang and code-switching.","Identifying tweets in these languages can be difficult as Twitter primarily supports high-resource languages.","We analyze Kenyan code-switched data and evaluate four state-of-the-art (SOTA) transformer-based pretrained models for sentiment and emotion classification, using supervised and semi-supervised methods.","We detail the methodology behind data collection and annotation, and the challenges encountered during the data curation phase.","Our results show that XLM-R outperforms other models; for sentiment analysis, XLM-R supervised model achieves the highest accuracy (69.2\\%) and F1 score (66.1\\%), XLM-R semi-supervised (67.2\\% accuracy, 64.1\\% F1 score).","In emotion analysis, DistilBERT supervised leads in accuracy (59.8\\%) and F1 score (31\\%), mBERT semi-supervised (accuracy (59\\% and F1 score 26.5\\%).","AfriBERTa models show the lowest accuracy and F1 scores.","All models tend to predict neutral sentiment, with Afri-BERT showing the highest bias and unique sensitivity to empathy emotion.","https://github.com/NEtori21/Ride_hailing"],"url":"http://arxiv.org/abs/2502.06180v1"}
{"created":"2025-02-10 05:44:54","title":"An Interpretable Implicit-Based Approach for Modeling Local Spatial Effects: A Case Study of Global Gross Primary Productivity","abstract":"In Earth sciences, unobserved factors exhibit non-stationary spatial distributions, causing the relationships between features and targets to display spatial heterogeneity. In geographic machine learning tasks, conventional statistical learning methods often struggle to capture spatial heterogeneity, leading to unsatisfactory prediction accuracy and unreliable interpretability. While approaches like Geographically Weighted Regression (GWR) capture local variations, they fall short of uncovering global patterns and tracking the continuous evolution of spatial heterogeneity. Motivated by this limitation, we propose a novel perspective - that is, simultaneously modeling common features across different locations alongside spatial differences using deep neural networks. The proposed method is a dual-branch neural network with an encoder-decoder structure. In the encoding stage, the method aggregates node information in a spatiotemporal conditional graph using GCN and LSTM, encoding location-specific spatiotemporal heterogeneity as an implicit conditional vector. Additionally, a self-attention-based encoder is used to extract location-invariant common features from the data. In the decoding stage, the approach employs a conditional generation strategy that predicts response variables and interpretative weights based on data features under spatiotemporal conditions. The approach is validated by predicting vegetation gross primary productivity (GPP) using global climate and land cover data from 2001 to 2020. Trained on 50 million samples and tested on 2.8 million, the proposed model achieves an RMSE of 0.836, outperforming LightGBM (1.063) and TabNet (0.944). Visualization analyses indicate that our method can reveal the distribution differences of the dominant factors of GPP across various times and locations.","sentences":["In Earth sciences, unobserved factors exhibit non-stationary spatial distributions, causing the relationships between features and targets to display spatial heterogeneity.","In geographic machine learning tasks, conventional statistical learning methods often struggle to capture spatial heterogeneity, leading to unsatisfactory prediction accuracy and unreliable interpretability.","While approaches like Geographically Weighted Regression (GWR) capture local variations, they fall short of uncovering global patterns and tracking the continuous evolution of spatial heterogeneity.","Motivated by this limitation, we propose a novel perspective - that is, simultaneously modeling common features across different locations alongside spatial differences using deep neural networks.","The proposed method is a dual-branch neural network with an encoder-decoder structure.","In the encoding stage, the method aggregates node information in a spatiotemporal conditional graph using GCN and LSTM, encoding location-specific spatiotemporal heterogeneity as an implicit conditional vector.","Additionally, a self-attention-based encoder is used to extract location-invariant common features from the data.","In the decoding stage, the approach employs a conditional generation strategy that predicts response variables and interpretative weights based on data features under spatiotemporal conditions.","The approach is validated by predicting vegetation gross primary productivity (GPP) using global climate and land cover data from 2001 to 2020.","Trained on 50 million samples and tested on 2.8 million, the proposed model achieves an RMSE of 0.836, outperforming LightGBM (1.063) and TabNet (0.944).","Visualization analyses indicate that our method can reveal the distribution differences of the dominant factors of GPP across various times and locations."],"url":"http://arxiv.org/abs/2502.06170v1"}
{"created":"2025-02-10 05:33:25","title":"Portable, High-Frequency, and High-Voltage Control Circuits for Untethered Miniature Robots Driven by Dielectric Elastomer Actuators","abstract":"In this work, we propose a high-voltage, high-frequency control circuit for the untethered applications of dielectric elastomer actuators (DEAs). The circuit board leverages low-voltage resistive components connected in series to control voltages of up to 1.8 kV within a compact size, suitable for frequencies ranging from 0 to 1 kHz. A single-channel control board weighs only 2.5 g. We tested the performance of the control circuit under different load conditions and power supplies. Based on this control circuit, along with a commercial miniature high-voltage power converter, we construct an untethered crawling robot driven by a cylindrical DEA. The 42-g untethered robots successfully obtained crawling locomotion on a bench and within a pipeline at a driving frequency of 15 Hz, while simultaneously transmitting real-time video data via an onboard camera and antenna. Our work provides a practical way to use low-voltage control electronics to achieve the untethered driving of DEAs, and therefore portable and wearable devices.","sentences":["In this work, we propose a high-voltage, high-frequency control circuit for the untethered applications of dielectric elastomer actuators (DEAs).","The circuit board leverages low-voltage resistive components connected in series to control voltages of up to 1.8 kV within a compact size, suitable for frequencies ranging from 0 to 1 kHz.","A single-channel control board weighs only 2.5 g.","We tested the performance of the control circuit under different load conditions and power supplies.","Based on this control circuit, along with a commercial miniature high-voltage power converter, we construct an untethered crawling robot driven by a cylindrical DEA.","The 42-g untethered robots successfully obtained crawling locomotion on a bench and within a pipeline at a driving frequency of 15 Hz, while simultaneously transmitting real-time video data via an onboard camera and antenna.","Our work provides a practical way to use low-voltage control electronics to achieve the untethered driving of DEAs, and therefore portable and wearable devices."],"url":"http://arxiv.org/abs/2502.06166v1"}
{"created":"2025-02-10 05:27:11","title":"Generalized Temporal Tensor Decomposition with Rank-revealing Latent-ODE","abstract":"Tensor decomposition is a fundamental tool for analyzing multi-dimensional data by learning low-rank factors to represent high-order interactions. While recent works on temporal tensor decomposition have made significant progress by incorporating continuous timestamps in latent factors, they still struggle with general tensor data with continuous indexes not only in the temporal mode but also in other modes, such as spatial coordinates in climate data. Additionally, the problem of determining the tensor rank remains largely unexplored in temporal tensor models. To address these limitations, we propose \\underline{G}eneralized temporal tensor decomposition with \\underline{R}ank-r\\underline{E}vealing laten\\underline{T}-ODE (GRET).   Our approach encodes continuous spatial indexes as learnable Fourier features and employs neural ODEs in latent space to learn the temporal trajectories of factors. To automatically reveal the rank of temporal tensors, we introduce a rank-revealing Gaussian-Gamma prior over the factor trajectories. We develop an efficient variational inference scheme with an analytical evidence lower bound, enabling sampling-free optimization. Through extensive experiments on both synthetic and real-world datasets, we demonstrate that GRET not only reveals the underlying ranks of temporal tensors but also significantly outperforms existing methods in prediction performance and robustness against noise.","sentences":["Tensor decomposition is a fundamental tool for analyzing multi-dimensional data by learning low-rank factors to represent high-order interactions.","While recent works on temporal tensor decomposition have made significant progress by incorporating continuous timestamps in latent factors, they still struggle with general tensor data with continuous indexes not only in the temporal mode but also in other modes, such as spatial coordinates in climate data.","Additionally, the problem of determining the tensor rank remains largely unexplored in temporal tensor models.","To address these limitations, we propose \\underline{G}eneralized temporal tensor decomposition with \\underline{R}ank-r\\underline{E}vealing laten\\underline{T}-ODE (GRET).   ","Our approach encodes continuous spatial indexes as learnable Fourier features and employs neural ODEs in latent space to learn the temporal trajectories of factors.","To automatically reveal the rank of temporal tensors, we introduce a rank-revealing Gaussian-Gamma prior over the factor trajectories.","We develop an efficient variational inference scheme with an analytical evidence lower bound, enabling sampling-free optimization.","Through extensive experiments on both synthetic and real-world datasets, we demonstrate that GRET not only reveals the underlying ranks of temporal tensors but also significantly outperforms existing methods in prediction performance and robustness against noise."],"url":"http://arxiv.org/abs/2502.06164v1"}
{"created":"2025-02-10 05:00:56","title":"Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile","abstract":"Despite the promise of synthesizing high-fidelity videos, Diffusion Transformers (DiTs) with 3D full attention suffer from expensive inference due to the complexity of attention computation and numerous sampling steps. For example, the popular Open-Sora-Plan model consumes more than 9 minutes for generating a single video of 29 frames. This paper addresses the inefficiency issue from two aspects: 1) Prune the 3D full attention based on the redundancy within video data; We identify a prevalent tile-style repetitive pattern in the 3D attention maps for video data, and advocate a new family of sparse 3D attention that holds a linear complexity w.r.t. the number of video frames. 2) Shorten the sampling process by adopting existing multi-step consistency distillation; We split the entire sampling trajectory into several segments and perform consistency distillation within each one to activate few-step generation capacities. We further devise a three-stage training pipeline to conjoin the low-complexity attention and few-step generation capacities. Notably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into an efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video generation with a marginal performance trade-off in VBench. In addition, we demonstrate that our approach is amenable to distributed inference, achieving an additional 3.91x speedup when running on 4 GPUs with sequence parallelism.","sentences":["Despite the promise of synthesizing high-fidelity videos, Diffusion Transformers (DiTs) with 3D full attention suffer from expensive inference due to the complexity of attention computation and numerous sampling steps.","For example, the popular Open-Sora-Plan model consumes more than 9 minutes for generating a single video of 29 frames.","This paper addresses the inefficiency issue from two aspects: 1) Prune the 3D full attention based on the redundancy within video data; We identify a prevalent tile-style repetitive pattern in the 3D attention maps for video data, and advocate a new family of sparse 3D attention that holds a linear complexity w.r.t.","the number of video frames.","2) Shorten the sampling process by adopting existing multi-step consistency distillation; We split the entire sampling trajectory into several segments and perform consistency distillation within each one to activate few-step generation capacities.","We further devise a three-stage training pipeline to conjoin the low-complexity attention and few-step generation capacities.","Notably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into an efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video generation with a marginal performance trade-off in VBench.","In addition, we demonstrate that our approach is amenable to distributed inference, achieving an additional 3.91x speedup when running on 4 GPUs with sequence parallelism."],"url":"http://arxiv.org/abs/2502.06155v1"}
{"created":"2025-02-10 04:42:11","title":"Powerformer: A Transformer with Weighted Causal Attention for Time-series Forecasting","abstract":"Transformers have recently shown strong performance in time-series forecasting, but their all-to-all attention mechanism overlooks the (temporal) causal and often (temporally) local nature of data. We introduce Powerformer, a novel Transformer variant that replaces noncausal attention weights with causal weights that are reweighted according to a smooth heavy-tailed decay. This simple yet effective modification endows the model with an inductive bias favoring temporally local dependencies, while still allowing sufficient flexibility to learn the unique correlation structure of each dataset. Our empirical results demonstrate that Powerformer not only achieves state-of-the-art accuracy on public time-series benchmarks, but also that it offers improved interpretability of attention patterns. Our analyses show that the model's locality bias is amplified during training, demonstrating an interplay between time-series data and power-law-based attention. These findings highlight the importance of domain-specific modifications to the Transformer architecture for time-series forecasting, and they establish Powerformer as a strong, efficient, and principled baseline for future research and real-world applications.","sentences":["Transformers have recently shown strong performance in time-series forecasting, but their all-to-all attention mechanism overlooks the (temporal) causal and often (temporally) local nature of data.","We introduce Powerformer, a novel Transformer variant that replaces noncausal attention weights with causal weights that are reweighted according to a smooth heavy-tailed decay.","This simple yet effective modification endows the model with an inductive bias favoring temporally local dependencies, while still allowing sufficient flexibility to learn the unique correlation structure of each dataset.","Our empirical results demonstrate that Powerformer not only achieves state-of-the-art accuracy on public time-series benchmarks, but also that it offers improved interpretability of attention patterns.","Our analyses show that the model's locality bias is amplified during training, demonstrating an interplay between time-series data and power-law-based attention.","These findings highlight the importance of domain-specific modifications to the Transformer architecture for time-series forecasting, and they establish Powerformer as a strong, efficient, and principled baseline for future research and real-world applications."],"url":"http://arxiv.org/abs/2502.06151v1"}
{"created":"2025-02-10 04:33:27","title":"Scaling Public Health Text Annotation: Zero-Shot Learning vs. Crowdsourcing for Improved Efficiency and Labeling Accuracy","abstract":"Public health researchers are increasingly interested in using social media data to study health-related behaviors, but manually labeling this data can be labor-intensive and costly. This study explores whether zero-shot labeling using large language models (LLMs) can match or surpass conventional crowd-sourced annotation for Twitter posts related to sleep disorders, physical activity, and sedentary behavior. Multiple annotation pipelines were designed to compare labels produced by domain experts, crowd workers, and LLM-driven approaches under varied prompt-engineering strategies. Our findings indicate that LLMs can rival human performance in straightforward classification tasks and significantly reduce labeling time, yet their accuracy diminishes for tasks requiring more nuanced domain knowledge. These results clarify the trade-offs between automated scalability and human expertise, demonstrating conditions under which LLM-based labeling can be efficiently integrated into public health research without undermining label quality.","sentences":["Public health researchers are increasingly interested in using social media data to study health-related behaviors, but manually labeling this data can be labor-intensive and costly.","This study explores whether zero-shot labeling using large language models (LLMs) can match or surpass conventional crowd-sourced annotation for Twitter posts related to sleep disorders, physical activity, and sedentary behavior.","Multiple annotation pipelines were designed to compare labels produced by domain experts, crowd workers, and LLM-driven approaches under varied prompt-engineering strategies.","Our findings indicate that LLMs can rival human performance in straightforward classification tasks and significantly reduce labeling time, yet their accuracy diminishes for tasks requiring more nuanced domain knowledge.","These results clarify the trade-offs between automated scalability and human expertise, demonstrating conditions under which LLM-based labeling can be efficiently integrated into public health research without undermining label quality."],"url":"http://arxiv.org/abs/2502.06150v1"}
{"created":"2025-02-10 04:23:01","title":"Guided Exploration for Efficient Relational Model Learning","abstract":"Efficient exploration is critical for learning relational models in large-scale environments with complex, long-horizon tasks. Random exploration methods often collect redundant or irrelevant data, limiting their ability to learn accurate relational models of the environment. Goal-literal babbling (GLIB) improves upon random exploration by setting and planning to novel goals, but its reliance on random actions and random novel goal selection limits its scalability to larger domains. In this work, we identify the principles underlying efficient exploration in relational domains: (1) operator initialization with demonstrations that cover the distinct lifted effects necessary for planning and (2) refining preconditions to collect maximally informative transitions by selecting informative goal-action pairs and executing plans to them. To demonstrate these principles, we introduce Baking-Large, a challenging domain with extensive state-action spaces and long-horizon tasks. We evaluate methods using oracle-driven demonstrations for operator initialization and precondition-targeting guidance to efficiently gather critical transitions. Experiments show that both the oracle demonstrations and precondition-targeting oracle guidance significantly improve sample efficiency and generalization, paving the way for future methods to use these principles to efficiently learn accurate relational models in complex domains.","sentences":["Efficient exploration is critical for learning relational models in large-scale environments with complex, long-horizon tasks.","Random exploration methods often collect redundant or irrelevant data, limiting their ability to learn accurate relational models of the environment.","Goal-literal babbling (GLIB) improves upon random exploration by setting and planning to novel goals, but its reliance on random actions and random novel goal selection limits its scalability to larger domains.","In this work, we identify the principles underlying efficient exploration in relational domains: (1) operator initialization with demonstrations that cover the distinct lifted effects necessary for planning and (2) refining preconditions to collect maximally informative transitions by selecting informative goal-action pairs and executing plans to them.","To demonstrate these principles, we introduce Baking-Large, a challenging domain with extensive state-action spaces and long-horizon tasks.","We evaluate methods using oracle-driven demonstrations for operator initialization and precondition-targeting guidance to efficiently gather critical transitions.","Experiments show that both the oracle demonstrations and precondition-targeting oracle guidance significantly improve sample efficiency and generalization, paving the way for future methods to use these principles to efficiently learn accurate relational models in complex domains."],"url":"http://arxiv.org/abs/2502.06146v1"}
{"created":"2025-02-10 03:59:27","title":"Enhanced Hybrid Deep Learning Approach for Botnet Attacks Detection in IoT Environment","abstract":"Cyberattacks in an Internet of Things (IoT) environment can have significant impacts because of the interconnected nature of devices and systems. An attacker uses a network of compromised IoT devices in a botnet attack to carry out various harmful activities. Detecting botnet attacks poses several challenges because of the intricate and evolving nature of these threats. Botnet attacks erode trust in IoT devices and systems, undermining confidence in their security, reliability, and integrity. Deep learning techniques have significantly enhanced the detection of botnet attacks due to their ability to analyze and learn from complex patterns in data. This research proposed the stacking of Deep convolutional neural networks, Bi-Directional Long Short-Term Memory (Bi-LSTM), Bi-Directional Gated Recurrent Unit (Bi-GRU), and Recurrent Neural Networks (RNN) for botnet attacks detection. The UNSW-NB15 dataset is utilized for botnet attacks detection. According to experimental results, the proposed model accurately provides for the intricate patterns and features of botnet attacks, with a testing accuracy of 99.76%. The proposed model also identifies botnets with a high ROC-AUC curve value of 99.18%. A performance comparison of the proposed method with existing state-of-the-art models confirms its higher performance. The outcomes of this research could strengthen cyber security procedures and safeguard against new attacks.","sentences":["Cyberattacks in an Internet of Things (IoT) environment can have significant impacts because of the interconnected nature of devices and systems.","An attacker uses a network of compromised IoT devices in a botnet attack to carry out various harmful activities.","Detecting botnet attacks poses several challenges because of the intricate and evolving nature of these threats.","Botnet attacks erode trust in IoT devices and systems, undermining confidence in their security, reliability, and integrity.","Deep learning techniques have significantly enhanced the detection of botnet attacks due to their ability to analyze and learn from complex patterns in data.","This research proposed the stacking of Deep convolutional neural networks, Bi-Directional Long Short-Term Memory (Bi-LSTM), Bi-Directional Gated Recurrent Unit (Bi-GRU), and Recurrent Neural Networks (RNN) for botnet attacks detection.","The UNSW-NB15 dataset is utilized for botnet attacks detection.","According to experimental results, the proposed model accurately provides for the intricate patterns and features of botnet attacks, with a testing accuracy of 99.76%.","The proposed model also identifies botnets with a high ROC-AUC curve value of 99.18%.","A performance comparison of the proposed method with existing state-of-the-art models confirms its higher performance.","The outcomes of this research could strengthen cyber security procedures and safeguard against new attacks."],"url":"http://arxiv.org/abs/2502.06138v1"}
