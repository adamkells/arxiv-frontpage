{"created":"2025-01-09 18:59:58","title":"ReFocus: Visual Editing as a Chain of Thought for Structured Image Understanding","abstract":"Structured image understanding, such as interpreting tables and charts, requires strategically refocusing across various structures and texts within an image, forming a reasoning sequence to arrive at the final answer. However, current multimodal large language models (LLMs) lack this multihop selective attention capability. In this work, we introduce ReFocus, a simple yet effective framework that equips multimodal LLMs with the ability to generate \"visual thoughts\" by performing visual editing on the input image through code, shifting and refining their visual focuses. Specifically, ReFocus enables multimodal LLMs to generate Python codes to call tools and modify the input image, sequentially drawing boxes, highlighting sections, and masking out areas, thereby enhancing the visual reasoning process. We experiment upon a wide range of structured image understanding tasks involving tables and charts. ReFocus largely improves performance on all tasks over GPT-4o without visual editing, yielding an average gain of 11.0% on table tasks and 6.8% on chart tasks. We present an in-depth analysis of the effects of different visual edits, and reasons why ReFocus can improve the performance without introducing additional information. Further, we collect a 14k training set using ReFocus, and prove that such visual chain-of-thought with intermediate information offers a better supervision than standard VQA data, reaching a 8.0% average gain over the same model trained with QA pairs and 2.6% over CoT.","sentences":["Structured image understanding, such as interpreting tables and charts, requires strategically refocusing across various structures and texts within an image, forming a reasoning sequence to arrive at the final answer.","However, current multimodal large language models (LLMs) lack this multihop selective attention capability.","In this work, we introduce ReFocus, a simple yet effective framework that equips multimodal LLMs with the ability to generate \"visual thoughts\" by performing visual editing on the input image through code, shifting and refining their visual focuses.","Specifically, ReFocus enables multimodal LLMs to generate Python codes to call tools and modify the input image, sequentially drawing boxes, highlighting sections, and masking out areas, thereby enhancing the visual reasoning process.","We experiment upon a wide range of structured image understanding tasks involving tables and charts.","ReFocus largely improves performance on all tasks over GPT-4o without visual editing, yielding an average gain of 11.0% on table tasks and 6.8% on chart tasks.","We present an in-depth analysis of the effects of different visual edits, and reasons why ReFocus can improve the performance without introducing additional information.","Further, we collect a 14k training set using ReFocus, and prove that such visual chain-of-thought with intermediate information offers a better supervision than standard VQA data, reaching a 8.0% average gain over the same model trained with QA pairs and 2.6% over CoT."],"url":"http://arxiv.org/abs/2501.05452v1"}
{"created":"2025-01-09 18:55:50","title":"A survey of textual cyber abuse detection using cutting-edge language models and large language models","abstract":"The success of social media platforms has facilitated the emergence of various forms of online abuse within digital communities. This abuse manifests in multiple ways, including hate speech, cyberbullying, emotional abuse, grooming, and sexting. In this paper, we present a comprehensive analysis of the different forms of abuse prevalent in social media, with a particular focus on how emerging technologies, such as Language Models (LMs) and Large Language Models (LLMs), are reshaping both the detection and generation of abusive content within these networks. We delve into the mechanisms through which social media abuse is perpetuated, exploring the psychological and social impact. Additionally, we examine the dual role of advanced language models-highlighting their potential to enhance automated detection systems for abusive behavior while also acknowledging their capacity to generate harmful content. This paper aims to contribute to the ongoing discourse on online safety and ethics, offering insights into the evolving landscape of cyberabuse and the technological innovations that both mitigate and exacerbate it.","sentences":["The success of social media platforms has facilitated the emergence of various forms of online abuse within digital communities.","This abuse manifests in multiple ways, including hate speech, cyberbullying, emotional abuse, grooming, and sexting.","In this paper, we present a comprehensive analysis of the different forms of abuse prevalent in social media, with a particular focus on how emerging technologies, such as Language Models (LMs) and Large Language Models (LLMs), are reshaping both the detection and generation of abusive content within these networks.","We delve into the mechanisms through which social media abuse is perpetuated, exploring the psychological and social impact.","Additionally, we examine the dual role of advanced language models-highlighting their potential to enhance automated detection systems for abusive behavior while also acknowledging their capacity to generate harmful content.","This paper aims to contribute to the ongoing discourse on online safety and ethics, offering insights into the evolving landscape of cyberabuse and the technological innovations that both mitigate and exacerbate it."],"url":"http://arxiv.org/abs/2501.05443v1"}
{"created":"2025-01-09 18:55:15","title":"Progressive Growing of Video Tokenizers for Highly Compressed Latent Spaces","abstract":"Video tokenizers are essential for latent video diffusion models, converting raw video data into spatiotemporally compressed latent spaces for efficient training. However, extending state-of-the-art video tokenizers to achieve a temporal compression ratio beyond 4x without increasing channel capacity poses significant challenges. In this work, we propose an alternative approach to enhance temporal compression. We find that the reconstruction quality of temporally subsampled videos from a low-compression encoder surpasses that of high-compression encoders applied to original videos. This indicates that high-compression models can leverage representations from lower-compression models. Building on this insight, we develop a bootstrapped high-temporal-compression model that progressively trains high-compression blocks atop well-trained lower-compression models. Our method includes a cross-level feature-mixing module to retain information from the pretrained low-compression model and guide higher-compression blocks to capture the remaining details from the full video sequence. Evaluation of video benchmarks shows that our method significantly improves reconstruction quality while increasing temporal compression compared to direct extensions of existing video tokenizers. Furthermore, the resulting compact latent space effectively trains a video diffusion model for high-quality video generation with a reduced token budget.","sentences":["Video tokenizers are essential for latent video diffusion models, converting raw video data into spatiotemporally compressed latent spaces for efficient training.","However, extending state-of-the-art video tokenizers to achieve a temporal compression ratio beyond 4x without increasing channel capacity poses significant challenges.","In this work, we propose an alternative approach to enhance temporal compression.","We find that the reconstruction quality of temporally subsampled videos from a low-compression encoder surpasses that of high-compression encoders applied to original videos.","This indicates that high-compression models can leverage representations from lower-compression models.","Building on this insight, we develop a bootstrapped high-temporal-compression model that progressively trains high-compression blocks atop well-trained lower-compression models.","Our method includes a cross-level feature-mixing module to retain information from the pretrained low-compression model and guide higher-compression blocks to capture the remaining details from the full video sequence.","Evaluation of video benchmarks shows that our method significantly improves reconstruction quality while increasing temporal compression compared to direct extensions of existing video tokenizers.","Furthermore, the resulting compact latent space effectively trains a video diffusion model for high-quality video generation with a reduced token budget."],"url":"http://arxiv.org/abs/2501.05442v1"}
{"created":"2025-01-09 18:48:55","title":"$DPF^*$: improved Depth Potential Function for scale-invariant sulcal depth estimation","abstract":"The shape of human brain is complex and highly variable, with interactions between brain size, cortical folding, and age well-documented in the literature. However, few studies have explored how global brain size influences geometric features of the cortical surface derived from anatomical MRI. In this work, we focus on sulcal depth, an imaging phenotype that has gained significant attention in both basic research and clinical applications. We make key contributions to the field by: 1) providing the first quantitative analysis of how brain size affects sulcal depth measurements; 2) introducing a novel, scale-invariant method for sulcal depth estimation based on an original formalization of the problem; 3) presenting a validation framework and sharing our code and benchmark data with the community; and 4) demonstrating the biological relevance of our new sulcal depth measure using a large sample of 1,987 subjects spanning the developmental period from 26 weeks post-conception to adulthood.","sentences":["The shape of human brain is complex and highly variable, with interactions between brain size, cortical folding, and age well-documented in the literature.","However, few studies have explored how global brain size influences geometric features of the cortical surface derived from anatomical MRI.","In this work, we focus on sulcal depth, an imaging phenotype that has gained significant attention in both basic research and clinical applications.","We make key contributions to the field by: 1) providing the first quantitative analysis of how brain size affects sulcal depth measurements; 2) introducing a novel, scale-invariant method for sulcal depth estimation based on an original formalization of the problem; 3) presenting a validation framework and sharing our code and benchmark data with the community; and 4) demonstrating the biological relevance of our new sulcal depth measure using a large sample of 1,987 subjects spanning the developmental period from 26 weeks post-conception to adulthood."],"url":"http://arxiv.org/abs/2501.05436v1"}
{"created":"2025-01-09 18:31:35","title":"Entangled Mean Estimation in High-Dimensions","abstract":"We study the task of high-dimensional entangled mean estimation in the subset-of-signals model. Specifically, given $N$ independent random points $x_1,\\ldots,x_N$ in $\\mathbb{R}^D$ and a parameter $\\alpha \\in (0, 1)$ such that each $x_i$ is drawn from a Gaussian with mean $\\mu$ and unknown covariance, and an unknown $\\alpha$-fraction of the points have identity-bounded covariances, the goal is to estimate the common mean $\\mu$. The one-dimensional version of this task has received significant attention in theoretical computer science and statistics over the past decades. Recent work [LY20; CV24] has given near-optimal upper and lower bounds for the one-dimensional setting. On the other hand, our understanding of even the information-theoretic aspects of the multivariate setting has remained limited.   In this work, we design a computationally efficient algorithm achieving an information-theoretically near-optimal error. Specifically, we show that the optimal error (up to polylogarithmic factors) is $f(\\alpha,N) + \\sqrt{D/(\\alpha N)}$, where the term $f(\\alpha,N)$ is the error of the one-dimensional problem and the second term is the sub-Gaussian error rate. Our algorithmic approach employs an iterative refinement strategy, whereby we progressively learn more accurate approximations $\\hat \\mu$ to $\\mu$. This is achieved via a novel rejection sampling procedure that removes points significantly deviating from $\\hat \\mu$, as an attempt to filter out unusually noisy samples. A complication that arises is that rejection sampling introduces bias in the distribution of the remaining points. To address this issue, we perform a careful analysis of the bias, develop an iterative dimension-reduction strategy, and employ a novel subroutine inspired by list-decodable learning that leverages the one-dimensional result.","sentences":["We study the task of high-dimensional entangled mean estimation in the subset-of-signals model.","Specifically, given $N$ independent random points $x_1,\\ldots,x_N$ in $\\mathbb{R}^D$ and a parameter $\\alpha \\in (0, 1)$ such that each $x_i$ is drawn from a Gaussian with mean $\\mu$ and unknown covariance, and an unknown $\\alpha$-fraction of the points have identity-bounded covariances, the goal is to estimate the common mean $\\mu$. The one-dimensional version of this task has received significant attention in theoretical computer science and statistics over the past decades.","Recent work [LY20; CV24] has given near-optimal upper and lower bounds for the one-dimensional setting.","On the other hand, our understanding of even the information-theoretic aspects of the multivariate setting has remained limited.   ","In this work, we design a computationally efficient algorithm achieving an information-theoretically near-optimal error.","Specifically, we show that the optimal error (up to polylogarithmic factors) is $f(\\alpha,N) +","\\sqrt{D/(\\alpha N)}$, where the term $f(\\alpha,N)$ is the error of the one-dimensional problem and the second term is the sub-Gaussian error rate.","Our algorithmic approach employs an iterative refinement strategy, whereby we progressively learn more accurate approximations $\\hat \\mu$ to $\\mu$. This is achieved via a novel rejection sampling procedure that removes points significantly deviating from $\\hat \\mu$, as an attempt to filter out unusually noisy samples.","A complication that arises is that rejection sampling introduces bias in the distribution of the remaining points.","To address this issue, we perform a careful analysis of the bias, develop an iterative dimension-reduction strategy, and employ a novel subroutine inspired by list-decodable learning that leverages the one-dimensional result."],"url":"http://arxiv.org/abs/2501.05425v1"}
{"created":"2025-01-09 18:17:27","title":"Uncertainty-aware Knowledge Tracing","abstract":"Knowledge Tracing (KT) is crucial in education assessment, which focuses on depicting students' learning states and assessing students' mastery of subjects. With the rise of modern online learning platforms, particularly massive open online courses (MOOCs), an abundance of interaction data has greatly advanced the development of the KT technology. Previous research commonly adopts deterministic representation to capture students' knowledge states, which neglects the uncertainty during student interactions and thus fails to model the true knowledge state in learning process. In light of this, we propose an Uncertainty-Aware Knowledge Tracing model (UKT) which employs stochastic distribution embeddings to represent the uncertainty in student interactions, with a Wasserstein self-attention mechanism designed to capture the transition of state distribution in student learning behaviors. Additionally, we introduce the aleatory uncertainty-aware contrastive learning loss, which strengthens the model's robustness towards different types of uncertainties. Extensive experiments on six real-world datasets demonstrate that UKT not only significantly surpasses existing deep learning-based models in KT prediction, but also shows unique advantages in handling the uncertainty of student interactions.","sentences":["Knowledge Tracing (KT) is crucial in education assessment, which focuses on depicting students' learning states and assessing students' mastery of subjects.","With the rise of modern online learning platforms, particularly massive open online courses (MOOCs), an abundance of interaction data has greatly advanced the development of the KT technology.","Previous research commonly adopts deterministic representation to capture students' knowledge states, which neglects the uncertainty during student interactions and thus fails to model the true knowledge state in learning process.","In light of this, we propose an Uncertainty-Aware Knowledge Tracing model (UKT) which employs stochastic distribution embeddings to represent the uncertainty in student interactions, with a Wasserstein self-attention mechanism designed to capture the transition of state distribution in student learning behaviors.","Additionally, we introduce the aleatory uncertainty-aware contrastive learning loss, which strengthens the model's robustness towards different types of uncertainties.","Extensive experiments on six real-world datasets demonstrate that UKT not only significantly surpasses existing deep learning-based models in KT prediction, but also shows unique advantages in handling the uncertainty of student interactions."],"url":"http://arxiv.org/abs/2501.05415v1"}
{"created":"2025-01-09 18:16:55","title":"LongProc: Benchmarking Long-Context Language Models on Long Procedural Generation","abstract":"Existing benchmarks for evaluating long-context language models (LCLMs) primarily focus on long-context recall, requiring models to produce short responses based on a few critical snippets while processing thousands of irrelevant tokens. We introduce LongProc (Long Procedural Generation), a new benchmark that requires both the integration of highly dispersed information and long-form generation. LongProc consists of six diverse procedural generation tasks, such as extracting structured information from HTML pages into a TSV format and executing complex search procedures to create travel plans. These tasks challenge LCLMs by testing their ability to follow detailed procedural instructions, synthesize and reason over dispersed information, and generate structured, long-form outputs (up to 8K tokens). Furthermore, as these tasks adhere to deterministic procedures and yield structured outputs, they enable reliable rule-based evaluation. We evaluate 17 LCLMs on LongProc across three difficulty levels, with maximum numbers of output tokens set at 500, 2K, and 8K. Notably, while all tested models claim a context window size above 32K tokens, open-weight models typically falter on 2K-token tasks, and closed-source models like GPT-4o show significant degradation on 8K-token tasks. Further analysis reveals that LCLMs struggle to maintain long-range coherence in long-form generations. These findings highlight critical limitations in current LCLMs and suggest substantial room for improvement. Data and code available at: https://princeton-pli.github.io/LongProc","sentences":["Existing benchmarks for evaluating long-context language models (LCLMs) primarily focus on long-context recall, requiring models to produce short responses based on a few critical snippets while processing thousands of irrelevant tokens.","We introduce LongProc (Long Procedural Generation), a new benchmark that requires both the integration of highly dispersed information and long-form generation.","LongProc consists of six diverse procedural generation tasks, such as extracting structured information from HTML pages into a TSV format and executing complex search procedures to create travel plans.","These tasks challenge LCLMs by testing their ability to follow detailed procedural instructions, synthesize and reason over dispersed information, and generate structured, long-form outputs (up to 8K tokens).","Furthermore, as these tasks adhere to deterministic procedures and yield structured outputs, they enable reliable rule-based evaluation.","We evaluate 17 LCLMs on LongProc across three difficulty levels, with maximum numbers of output tokens set at 500, 2K, and 8K. Notably, while all tested models claim a context window size above 32K tokens, open-weight models typically falter on 2K-token tasks, and closed-source models like GPT-4o show significant degradation on 8K-token tasks.","Further analysis reveals that LCLMs struggle to maintain long-range coherence in long-form generations.","These findings highlight critical limitations in current LCLMs and suggest substantial room for improvement.","Data and code available at: https://princeton-pli.github.io/LongProc"],"url":"http://arxiv.org/abs/2501.05414v1"}
{"created":"2025-01-09 18:13:57","title":"Seeing Sound: Assembling Sounds from Visuals for Audio-to-Image Generation","abstract":"Training audio-to-image generative models requires an abundance of diverse audio-visual pairs that are semantically aligned. Such data is almost always curated from in-the-wild videos, given the cross-modal semantic correspondence that is inherent to them. In this work, we hypothesize that insisting on the absolute need for ground truth audio-visual correspondence, is not only unnecessary, but also leads to severe restrictions in scale, quality, and diversity of the data, ultimately impairing its use in the modern generative models. That is, we propose a scalable image sonification framework where instances from a variety of high-quality yet disjoint uni-modal origins can be artificially paired through a retrieval process that is empowered by reasoning capabilities of modern vision-language models. To demonstrate the efficacy of this approach, we use our sonified images to train an audio-to-image generative model that performs competitively against state-of-the-art. Finally, through a series of ablation studies, we exhibit several intriguing auditory capabilities like semantic mixing and interpolation, loudness calibration and acoustic space modeling through reverberation that our model has implicitly developed to guide the image generation process.","sentences":["Training audio-to-image generative models requires an abundance of diverse audio-visual pairs that are semantically aligned.","Such data is almost always curated from in-the-wild videos, given the cross-modal semantic correspondence that is inherent to them.","In this work, we hypothesize that insisting on the absolute need for ground truth audio-visual correspondence, is not only unnecessary, but also leads to severe restrictions in scale, quality, and diversity of the data, ultimately impairing its use in the modern generative models.","That is, we propose a scalable image sonification framework","where instances from a variety of high-quality yet disjoint uni-modal origins can be artificially paired through a retrieval process that is empowered by reasoning capabilities of modern vision-language models.","To demonstrate the efficacy of this approach, we use our sonified images to train an audio-to-image generative model that performs competitively against state-of-the-art.","Finally, through a series of ablation studies, we exhibit several intriguing auditory capabilities like semantic mixing and interpolation, loudness calibration and acoustic space modeling through reverberation that our model has implicitly developed to guide the image generation process."],"url":"http://arxiv.org/abs/2501.05413v1"}
{"created":"2025-01-09 18:05:33","title":"TimeRL: Efficient Deep Reinforcement Learning with Polyhedral Dependence Graphs","abstract":"Modern deep learning (DL) workloads increasingly use complex deep reinforcement learning (DRL) algorithms that generate training data within the learning loop. This results in programs with several nested loops and dynamic data dependencies between tensors. While DL systems with eager execution support such dynamism, they lack the optimizations and smart scheduling of graph-based execution. Graph-based execution, however, cannot express dynamic tensor shapes, instead requiring the use of multiple static subgraphs. Either execution model for DRL thus leads to redundant computation, reduced parallelism, and less efficient memory management.   We describe TimeRL, a system for executing dynamic DRL programs that combines the dynamism of eager execution with the whole-program optimizations and scheduling of graph-based execution. TimeRL achieves this by introducing the declarative programming model of recurrent tensors, which allows users to define dynamic dependencies as intuitive recurrence equations. TimeRL translates recurrent tensors into a polyhedral dependence graph (PDG) with dynamic dependencies as symbolic expressions. Through simple PDG transformations, TimeRL applies whole-program optimizations, such as automatic vectorization, incrementalization, and operator fusion. The PDG also allows for the computation of an efficient program-wide execution schedule, which decides on buffer deallocations, buffer donations, and GPU/CPU memory swapping. We show that TimeRL executes current DRL algorithms up to 47$\\times$ faster than existing DRL systems, while using 16$\\times$ less GPU peak memory.","sentences":["Modern deep learning (DL) workloads increasingly use complex deep reinforcement learning (DRL) algorithms that generate training data within the learning loop.","This results in programs with several nested loops and dynamic data dependencies between tensors.","While DL systems with eager execution support such dynamism, they lack the optimizations and smart scheduling of graph-based execution.","Graph-based execution, however, cannot express dynamic tensor shapes, instead requiring the use of multiple static subgraphs.","Either execution model for DRL thus leads to redundant computation, reduced parallelism, and less efficient memory management.   ","We describe TimeRL, a system for executing dynamic DRL programs that combines the dynamism of eager execution with the whole-program optimizations and scheduling of graph-based execution.","TimeRL achieves this by introducing the declarative programming model of recurrent tensors, which allows users to define dynamic dependencies as intuitive recurrence equations.","TimeRL translates recurrent tensors into a polyhedral dependence graph (PDG) with dynamic dependencies as symbolic expressions.","Through simple PDG transformations, TimeRL applies whole-program optimizations, such as automatic vectorization, incrementalization, and operator fusion.","The PDG also allows for the computation of an efficient program-wide execution schedule, which decides on buffer deallocations, buffer donations, and GPU/CPU memory swapping.","We show that TimeRL executes current DRL algorithms up to 47$\\times$ faster than existing DRL systems, while using 16$\\times$ less GPU peak memory."],"url":"http://arxiv.org/abs/2501.05408v1"}
{"created":"2025-01-09 17:57:56","title":"TimeDP: Learning to Generate Multi-Domain Time Series with Domain Prompts","abstract":"Time series generation models are crucial for applications like data augmentation and privacy preservation. Most existing time series generation models are typically designed to generate data from one specified domain. While leveraging data from other domain for better generalization is proved to work in other application areas, this approach remains challenging for time series modeling due to the large divergence in patterns among different real world time series categories. In this paper, we propose a multi-domain time series diffusion model with domain prompts, named TimeDP. In TimeDP, we utilize a time series semantic prototype module which defines time series prototypes to represent time series basis, each prototype vector serving as \"word\" representing some elementary time series feature. A prototype assignment module is applied to extract the extract domain specific prototype weights, for learning domain prompts as generation condition. During sampling, we extract \"domain prompt\" with few-shot samples from the target domain and use the domain prompts as condition to generate time series samples. Experiments demonstrate that our method outperforms baselines to provide the state-of-the-art in-domain generation quality and strong unseen domain generation capability.","sentences":["Time series generation models are crucial for applications like data augmentation and privacy preservation.","Most existing time series generation models are typically designed to generate data from one specified domain.","While leveraging data from other domain for better generalization is proved to work in other application areas, this approach remains challenging for time series modeling due to the large divergence in patterns among different real world time series categories.","In this paper, we propose a multi-domain time series diffusion model with domain prompts, named TimeDP.","In TimeDP, we utilize a time series semantic prototype module which defines time series prototypes to represent time series basis, each prototype vector serving as \"word\" representing some elementary time series feature.","A prototype assignment module is applied to extract the extract domain specific prototype weights, for learning domain prompts as generation condition.","During sampling, we extract \"domain prompt\" with few-shot samples from the target domain and use the domain prompts as condition to generate time series samples.","Experiments demonstrate that our method outperforms baselines to provide the state-of-the-art in-domain generation quality and strong unseen domain generation capability."],"url":"http://arxiv.org/abs/2501.05403v1"}
{"created":"2025-01-09 17:50:56","title":"BRATI: Bidirectional Recurrent Attention for Time-Series Imputation","abstract":"Missing data in time-series analysis poses significant challenges, affecting the reliability of downstream applications. Imputation, the process of estimating missing values, has emerged as a key solution. This paper introduces BRATI, a novel deep-learning model designed to address multivariate time-series imputation by combining Bidirectional Recurrent Networks and Attention mechanisms. BRATI processes temporal dependencies and feature correlations across long and short time horizons, utilizing two imputation blocks that operate in opposite temporal directions. Each block integrates recurrent layers and attention mechanisms to effectively resolve long-term dependencies.   We evaluate BRATI on three real-world datasets under diverse missing-data scenarios: randomly missing values, fixed-length missing sequences, and variable-length missing sequences. Our findings demonstrate that BRATI consistently outperforms state-of-the-art models, delivering superior accuracy and robustness in imputing multivariate time-series data.","sentences":["Missing data in time-series analysis poses significant challenges, affecting the reliability of downstream applications.","Imputation, the process of estimating missing values, has emerged as a key solution.","This paper introduces BRATI, a novel deep-learning model designed to address multivariate time-series imputation by combining Bidirectional Recurrent Networks and Attention mechanisms.","BRATI processes temporal dependencies and feature correlations across long and short time horizons, utilizing two imputation blocks that operate in opposite temporal directions.","Each block integrates recurrent layers and attention mechanisms to effectively resolve long-term dependencies.   ","We evaluate BRATI on three real-world datasets under diverse missing-data scenarios: randomly missing values, fixed-length missing sequences, and variable-length missing sequences.","Our findings demonstrate that BRATI consistently outperforms state-of-the-art models, delivering superior accuracy and robustness in imputing multivariate time-series data."],"url":"http://arxiv.org/abs/2501.05401v1"}
{"created":"2025-01-09 17:47:34","title":"Mechanistic understanding and validation of large AI models with SemanticLens","abstract":"Unlike human-engineered systems such as aeroplanes, where each component's role and dependencies are well understood, the inner workings of AI models remain largely opaque, hindering verifiability and undermining trust. This paper introduces SemanticLens, a universal explanation method for neural networks that maps hidden knowledge encoded by components (e.g., individual neurons) into the semantically structured, multimodal space of a foundation model such as CLIP. In this space, unique operations become possible, including (i) textual search to identify neurons encoding specific concepts, (ii) systematic analysis and comparison of model representations, (iii) automated labelling of neurons and explanation of their functional roles, and (iv) audits to validate decision-making against requirements. Fully scalable and operating without human input, SemanticLens is shown to be effective for debugging and validation, summarizing model knowledge, aligning reasoning with expectations (e.g., adherence to the ABCDE-rule in melanoma classification), and detecting components tied to spurious correlations and their associated training data. By enabling component-level understanding and validation, the proposed approach helps bridge the \"trust gap\" between AI models and traditional engineered systems. We provide code for SemanticLens on https://github.com/jim-berend/semanticlens and a demo on https://semanticlens.hhi-research-insights.eu.","sentences":["Unlike human-engineered systems such as aeroplanes, where each component's role and dependencies are well understood, the inner workings of AI models remain largely opaque, hindering verifiability and undermining trust.","This paper introduces SemanticLens, a universal explanation method for neural networks that maps hidden knowledge encoded by components (e.g., individual neurons) into the semantically structured, multimodal space of a foundation model such as CLIP.","In this space, unique operations become possible, including (i) textual search to identify neurons encoding specific concepts, (ii) systematic analysis and comparison of model representations, (iii) automated labelling of neurons and explanation of their functional roles, and (iv) audits to validate decision-making against requirements.","Fully scalable and operating without human input, SemanticLens is shown to be effective for debugging and validation, summarizing model knowledge, aligning reasoning with expectations (e.g., adherence to the ABCDE-rule in melanoma classification), and detecting components tied to spurious correlations and their associated training data.","By enabling component-level understanding and validation, the proposed approach helps bridge the \"trust gap\" between AI models and traditional engineered systems.","We provide code for SemanticLens on https://github.com/jim-berend/semanticlens and a demo on https://semanticlens.hhi-research-insights.eu."],"url":"http://arxiv.org/abs/2501.05398v1"}
{"created":"2025-01-09 17:04:33","title":"Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID Guidance","abstract":"Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in reconstructing detailed 3D scenes within multi-view setups and the emergence of large 2D human foundation models, we introduce Arc2Avatar, the first SDS-based method utilizing a human face foundation model as guidance with just a single image as input. To achieve that, we extend such a model for diverse-view human head generation by fine-tuning on synthetic data and modifying its conditioning. Our avatars maintain a dense correspondence with a human face mesh template, allowing blendshape-based expression generation. This is achieved through a modified 3DGS approach, connectivity regularizers, and a strategic initialization tailored for our task. Additionally, we propose an optional efficient SDS-based correction step to refine the blendshape expressions, enhancing realism and diversity. Experiments demonstrate that Arc2Avatar achieves state-of-the-art realism and identity preservation, effectively addressing color issues by allowing the use of very low guidance, enabled by our strong identity prior and initialization strategy, without compromising detail.","sentences":["Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in reconstructing detailed 3D scenes within multi-view setups and the emergence of large 2D human foundation models, we introduce Arc2Avatar, the first SDS-based method utilizing a human face foundation model as guidance with just a single image as input.","To achieve that, we extend such a model for diverse-view human head generation by fine-tuning on synthetic data and modifying its conditioning.","Our avatars maintain a dense correspondence with a human face mesh template, allowing blendshape-based expression generation.","This is achieved through a modified 3DGS approach, connectivity regularizers, and a strategic initialization tailored for our task.","Additionally, we propose an optional efficient SDS-based correction step to refine the blendshape expressions, enhancing realism and diversity.","Experiments demonstrate that Arc2Avatar achieves state-of-the-art realism and identity preservation, effectively addressing color issues by allowing the use of very low guidance, enabled by our strong identity prior and initialization strategy, without compromising detail."],"url":"http://arxiv.org/abs/2501.05379v1"}
{"created":"2025-01-09 16:59:56","title":"Byzantine Fault Tolerant Protocols with Near-Constant Work per Node without Signatures","abstract":"Numerous distributed tasks have to be handled in a setting where a fraction of nodes behaves Byzantine, that is, deviates arbitrarily from the intended protocol. Resilient, deterministic protocols rely on the detection of majorities to avoid inconsistencies if there is a Byzantine minority, which requires individual nodes to handle a communication load that is proportional to the size of the network -- an intolerable disadvantage in large networks.   Randomized protocols circumvent this by probing only small parts of the network, thus allowing for consistent decisions quickly and with a high level of confidence with communication that is near-constant in the network size. However, such protocols usually come with the drawback of limiting the fault tolerance of the protocol. For instance, by severely restricting the number or type of failures that the protocol can tolerate.   We present randomized protocols to reliably aggregate and broadcast information, form consensus and compute common coins that tolerate a constant fraction of Byzantine failures, do not require cryptographic methods and have a near-constant time and message complexity per node. Our main technique is to compute a system of witness committees as a pre-computation step almost optimally. This pre-computation step allows to solve the aforementioned distributed tasks repeatedly and efficiently, but may have far reaching further applications, e.g., for sharding of distributed data structures.","sentences":["Numerous distributed tasks have to be handled in a setting where a fraction of nodes behaves Byzantine, that is, deviates arbitrarily from the intended protocol.","Resilient, deterministic protocols rely on the detection of majorities to avoid inconsistencies if there is a Byzantine minority, which requires individual nodes to handle a communication load that is proportional to the size of the network -- an intolerable disadvantage in large networks.   ","Randomized protocols circumvent this by probing only small parts of the network, thus allowing for consistent decisions quickly and with a high level of confidence with communication that is near-constant in the network size.","However, such protocols usually come with the drawback of limiting the fault tolerance of the protocol.","For instance, by severely restricting the number or type of failures that the protocol can tolerate.   ","We present randomized protocols to reliably aggregate and broadcast information, form consensus and compute common coins that tolerate a constant fraction of Byzantine failures, do not require cryptographic methods and have a near-constant time and message complexity per node.","Our main technique is to compute a system of witness committees as a pre-computation step almost optimally.","This pre-computation step allows to solve the aforementioned distributed tasks repeatedly and efficiently, but may have far reaching further applications, e.g., for sharding of distributed data structures."],"url":"http://arxiv.org/abs/2501.05377v1"}
{"created":"2025-01-09 16:49:04","title":"Developing a Foundation of Vector Symbolic Architectures Using Category Theory","abstract":"At the risk of overstating the case, connectionist approaches to machine learning, i.e. neural networks, are enjoying a small vogue right now. However, these methods require large volumes of data and produce models that are uninterpretable to humans. An alternative framework that is compatible with neural networks and gradient-based learning, but explicitly models compositionality, is Vector Symbolic Architectures (VSAs). VSAs are a family of algebras on high-dimensional vector representations. They arose in cognitive science from the need to unify neural processing and the kind of symbolic reasoning that humans perform. While machine learning methods have benefited from category theoretical analyses, VSAs have not yet received similar treatment. In this paper, we present a first attempt at applying category theory to VSAs. Specifically, we conduct a brief literature survey demonstrating the lacking intersection of these two topics, provide a list of desiderata for VSAs, and propose that VSAs may be understood as a (division) rig in a category enriched over a monoid in Met (the category of Lawvere metric spaces). This final contribution suggests that VSAs may be generalised beyond current implementations. It is our hope that grounding VSAs in category theory will lead to more rigorous connections with other research, both within and beyond, learning and cognition.","sentences":["At the risk of overstating the case, connectionist approaches to machine learning, i.e. neural networks, are enjoying a small vogue right now.","However, these methods require large volumes of data and produce models that are uninterpretable to humans.","An alternative framework that is compatible with neural networks and gradient-based learning, but explicitly models compositionality, is Vector Symbolic Architectures (VSAs).","VSAs are a family of algebras on high-dimensional vector representations.","They arose in cognitive science from the need to unify neural processing and the kind of symbolic reasoning that humans perform.","While machine learning methods have benefited from category theoretical analyses, VSAs have not yet received similar treatment.","In this paper, we present a first attempt at applying category theory to VSAs.","Specifically, we conduct a brief literature survey demonstrating the lacking intersection of these two topics, provide a list of desiderata for VSAs, and propose that VSAs may be understood as a (division) rig in a category enriched over a monoid in Met (the category of Lawvere metric spaces).","This final contribution suggests that VSAs may be generalised beyond current implementations.","It is our hope that grounding VSAs in category theory will lead to more rigorous connections with other research, both within and beyond, learning and cognition."],"url":"http://arxiv.org/abs/2501.05368v1"}
{"created":"2025-01-09 16:21:12","title":"Video-Conferencing Beyond Screen-Sharing and Thumbnail Webcam Videos: Gesture-Aware Augmented Reality Video for Data-Rich Remote Presentations","abstract":"Synchronous data-rich conversations are commonplace within enterprise organizations, taking place at varying degrees of formality between stakeholders at different levels of data literacy. In these conversations, representations of data are used to analyze past decisions, inform future course of action, as well as persuade customers, investors, and executives. However, it is difficult to conduct these conversations between remote stakeholders due to poor support for presenting data when video-conferencing, resulting in disappointing audience experiences. In this position statement, I reflect on our recent work incorporating multimodal interaction and augmented reality video, suggesting that video-conferencing does not need to be limited to screen-sharing and relegating a speaker's video to a separate thumbnail view. I also comment on future research directions and collaboration opportunities.","sentences":["Synchronous data-rich conversations are commonplace within enterprise organizations, taking place at varying degrees of formality between stakeholders at different levels of data literacy.","In these conversations, representations of data are used to analyze past decisions, inform future course of action, as well as persuade customers, investors, and executives.","However, it is difficult to conduct these conversations between remote stakeholders due to poor support for presenting data when video-conferencing, resulting in disappointing audience experiences.","In this position statement, I reflect on our recent work incorporating multimodal interaction and augmented reality video, suggesting that video-conferencing does not need to be limited to screen-sharing and relegating a speaker's video to a separate thumbnail view.","I also comment on future research directions and collaboration opportunities."],"url":"http://arxiv.org/abs/2501.05345v1"}
{"created":"2025-01-09 15:50:02","title":"The explanation dialogues: an expert focus study to understand requirements towards explanations within the GDPR","abstract":"Explainable AI (XAI) provides methods to understand non-interpretable machine learning models. However, we have little knowledge about what legal experts expect from these explanations, including their legal compliance with, and value against European Union legislation. To close this gap, we present the Explanation Dialogues, an expert focus study to uncover the expectations, reasoning, and understanding of legal experts and practitioners towards XAI, with a specific focus on the European General Data Protection Regulation. The study consists of an online questionnaire and follow-up interviews, and is centered around a use-case in the credit domain. We extract both a set of hierarchical and interconnected codes using grounded theory, and present the standpoints of the participating experts towards XAI. We find that the presented explanations are hard to understand and lack information, and discuss issues that can arise from the different interests of the data controller and subject. Finally, we present a set of recommendations for developers of XAI methods, and indications of legal areas of discussion. Among others, recommendations address the presentation, choice, and content of an explanation, technical risks as well as the end-user, while we provide legal pointers to the contestability of explanations, transparency thresholds, intellectual property rights as well as the relationship between involved parties.","sentences":["Explainable AI (XAI) provides methods to understand non-interpretable machine learning models.","However, we have little knowledge about what legal experts expect from these explanations, including their legal compliance with, and value against European Union legislation.","To close this gap, we present the Explanation Dialogues, an expert focus study to uncover the expectations, reasoning, and understanding of legal experts and practitioners towards XAI, with a specific focus on the European General Data Protection Regulation.","The study consists of an online questionnaire and follow-up interviews, and is centered around a use-case in the credit domain.","We extract both a set of hierarchical and interconnected codes using grounded theory, and present the standpoints of the participating experts towards XAI.","We find that the presented explanations are hard to understand and lack information, and discuss issues that can arise from the different interests of the data controller and subject.","Finally, we present a set of recommendations for developers of XAI methods, and indications of legal areas of discussion.","Among others, recommendations address the presentation, choice, and content of an explanation, technical risks as well as the end-user, while we provide legal pointers to the contestability of explanations, transparency thresholds, intellectual property rights as well as the relationship between involved parties."],"url":"http://arxiv.org/abs/2501.05325v1"}
{"created":"2025-01-09 15:48:29","title":"Distributed Learning and Inference Systems: A Networking Perspective","abstract":"Machine learning models have achieved, and in some cases surpassed, human-level performance in various tasks, mainly through centralized training of static models and the use of large models stored in centralized clouds for inference. However, this centralized approach has several drawbacks, including privacy concerns, high storage demands, a single point of failure, and significant computing requirements. These challenges have driven interest in developing alternative decentralized and distributed methods for AI training and inference. Distribution introduces additional complexity, as it requires managing multiple moving parts. To address these complexities and fill a gap in the development of distributed AI systems, this work proposes a novel framework, Data and Dynamics-Aware Inference and Training Networks (DA-ITN). The different components of DA-ITN and their functions are explored, and the associated challenges and research areas are highlighted.","sentences":["Machine learning models have achieved, and in some cases surpassed, human-level performance in various tasks, mainly through centralized training of static models and the use of large models stored in centralized clouds for inference.","However, this centralized approach has several drawbacks, including privacy concerns, high storage demands, a single point of failure, and significant computing requirements.","These challenges have driven interest in developing alternative decentralized and distributed methods for AI training and inference.","Distribution introduces additional complexity, as it requires managing multiple moving parts.","To address these complexities and fill a gap in the development of distributed AI systems, this work proposes a novel framework, Data and Dynamics-Aware Inference and Training Networks (DA-ITN).","The different components of DA-ITN and their functions are explored, and the associated challenges and research areas are highlighted."],"url":"http://arxiv.org/abs/2501.05323v1"}
{"created":"2025-01-09 15:25:07","title":"Private Selection with Heterogeneous Sensitivities","abstract":"Differentially private (DP) selection involves choosing a high-scoring candidate from a finite candidate pool, where each score depends on a sensitive dataset. This problem arises naturally in a variety of contexts including model selection, hypothesis testing, and within many DP algorithms. Classical methods, such as Report Noisy Max (RNM), assume all candidates' scores are equally sensitive to changes in a single individual's data, but this often isn't the case. To address this, algorithms like the Generalised Exponential Mechanism (GEM) leverage variability in candidate sensitivities. However, we observe that while these algorithms can outperform RNM in some situations, they may underperform in others - they can even perform worse than random selection. In this work, we explore how the distribution of scores and sensitivities impacts DP selection mechanisms. In all settings we study, we find that there exists a mechanism that utilises heterogeneity in the candidate sensitivities that outperforms standard mechanisms like RNM. However, no single mechanism uniformly outperforms RNM. We propose using the correlation between the scores and sensitivities as the basis for deciding which DP selection mechanism to use. Further, we design a slight variant of GEM, modified GEM that generally performs well whenever GEM performs poorly. Relying on the correlation heuristic we propose combined GEM, which adaptively chooses between GEM and modified GEM and outperforms both in polarised settings.","sentences":["Differentially private (DP) selection involves choosing a high-scoring candidate from a finite candidate pool, where each score depends on a sensitive dataset.","This problem arises naturally in a variety of contexts including model selection, hypothesis testing, and within many DP algorithms.","Classical methods, such as Report Noisy Max (RNM), assume all candidates' scores are equally sensitive to changes in a single individual's data, but this often isn't the case.","To address this, algorithms like the Generalised Exponential Mechanism (GEM) leverage variability in candidate sensitivities.","However, we observe that while these algorithms can outperform RNM in some situations, they may underperform in others - they can even perform worse than random selection.","In this work, we explore how the distribution of scores and sensitivities impacts DP selection mechanisms.","In all settings we study, we find that there exists a mechanism that utilises heterogeneity in the candidate sensitivities that outperforms standard mechanisms like RNM.","However, no single mechanism uniformly outperforms RNM.","We propose using the correlation between the scores and sensitivities as the basis for deciding which DP selection mechanism to use.","Further, we design a slight variant of GEM, modified GEM that generally performs well whenever GEM performs poorly.","Relying on the correlation heuristic we propose combined GEM, which adaptively chooses between GEM and modified GEM and outperforms both in polarised settings."],"url":"http://arxiv.org/abs/2501.05309v1"}
{"created":"2025-01-09 14:57:19","title":"GaussDB-Global: A Geographically Distributed Database System","abstract":"Geographically distributed database systems use remote replication to protect against regional failures. These systems are sensitive to severe latency penalties caused by centralized transaction management, remote access to sharded data, and log shipping over long distances. To tackle these issues, we present GaussDB-Global, a sharded geographically distributed database system with asynchronous replication, for OLTP applications. To tackle the transaction management bottleneck, we take a decentralized approach using synchronized clocks. Our system can seamlessly transition between centralized and decentralized transaction management, providing efficient fault tolerance and streamlining deployment. To alleviate the remote read and log shipping issues, we support reads on asynchronous replicas with strong consistency, tunable freshness guarantees, and dynamic load balancing. Our experimental results on a geographically distributed cluster show that our approach provides up to 14x higher read throughput, and 50% more TPC-C throughput compared to our baseline.","sentences":["Geographically distributed database systems use remote replication to protect against regional failures.","These systems are sensitive to severe latency penalties caused by centralized transaction management, remote access to sharded data, and log shipping over long distances.","To tackle these issues, we present GaussDB-Global, a sharded geographically distributed database system with asynchronous replication, for OLTP applications.","To tackle the transaction management bottleneck, we take a decentralized approach using synchronized clocks.","Our system can seamlessly transition between centralized and decentralized transaction management, providing efficient fault tolerance and streamlining deployment.","To alleviate the remote read and log shipping issues, we support reads on asynchronous replicas with strong consistency, tunable freshness guarantees, and dynamic load balancing.","Our experimental results on a geographically distributed cluster show that our approach provides up to 14x higher read throughput, and 50% more TPC-C throughput compared to our baseline."],"url":"http://arxiv.org/abs/2501.05295v1"}
{"created":"2025-01-09 14:39:40","title":"Off-Policy Evaluation and Counterfactual Methods in Dynamic Auction Environments","abstract":"Counterfactual estimators are critical for learning and refining policies using logged data, a process known as Off-Policy Evaluation (OPE). OPE allows researchers to assess new policies without costly experiments, speeding up the evaluation process. Online experimental methods, such as A/B tests, are effective but often slow, thus delaying the policy selection and optimization process.   In this work, we explore the application of OPE methods in the context of resource allocation in dynamic auction environments. Given the competitive nature of environments where rapid decision-making is crucial for gaining a competitive edge, the ability to quickly and accurately assess algorithmic performance is essential. By utilizing counterfactual estimators as a preliminary step before conducting A/B tests, we aim to streamline the evaluation process, reduce the time and resources required for experimentation, and enhance confidence in the chosen policies. Our investigation focuses on the feasibility and effectiveness of using these estimators to predict the outcomes of potential resource allocation strategies, evaluate their performance, and facilitate more informed decision-making in policy selection. Motivated by the outcomes of our initial study, we envision an advanced analytics system designed to seamlessly and dynamically assess new resource allocation strategies and policies.","sentences":["Counterfactual estimators are critical for learning and refining policies using logged data, a process known as Off-Policy Evaluation (OPE).","OPE allows researchers to assess new policies without costly experiments, speeding up the evaluation process.","Online experimental methods, such as A/B tests, are effective but often slow, thus delaying the policy selection and optimization process.   ","In this work, we explore the application of OPE methods in the context of resource allocation in dynamic auction environments.","Given the competitive nature of environments where rapid decision-making is crucial for gaining a competitive edge, the ability to quickly and accurately assess algorithmic performance is essential.","By utilizing counterfactual estimators as a preliminary step before conducting A/B tests, we aim to streamline the evaluation process, reduce the time and resources required for experimentation, and enhance confidence in the chosen policies.","Our investigation focuses on the feasibility and effectiveness of using these estimators to predict the outcomes of potential resource allocation strategies, evaluate their performance, and facilitate more informed decision-making in policy selection.","Motivated by the outcomes of our initial study, we envision an advanced analytics system designed to seamlessly and dynamically assess new resource allocation strategies and policies."],"url":"http://arxiv.org/abs/2501.05278v1"}
{"created":"2025-01-09 14:31:54","title":"Solving the Catastrophic Forgetting Problem in Generalized Category Discovery","abstract":"Generalized Category Discovery (GCD) aims to identify a mix of known and novel categories within unlabeled data sets, providing a more realistic setting for image recognition. Essentially, GCD needs to remember existing patterns thoroughly to recognize novel categories. Recent state-of-the-art method SimGCD transfers the knowledge from known-class data to the learning of novel classes through debiased learning. However, some patterns are catastrophically forgot during adaptation and thus lead to poor performance in novel categories classification. To address this issue, we propose a novel learning approach, LegoGCD, which is seamlessly integrated into previous methods to enhance the discrimination of novel classes while maintaining performance on previously encountered known classes. Specifically, we design two types of techniques termed as Local Entropy Regularization (LER) and Dual-views Kullback Leibler divergence constraint (DKL). The LER optimizes the distribution of potential known class samples in unlabeled data, thus ensuring the preservation of knowledge related to known categories while learning novel classes. Meanwhile, DKL introduces Kullback Leibler divergence to encourage the model to produce a similar prediction distribution of two view samples from the same image. In this way, it successfully avoids mismatched prediction and generates more reliable potential known class samples simultaneously. Extensive experiments validate that the proposed LegoGCD effectively addresses the known category forgetting issue across all datasets, eg, delivering a 7.74% and 2.51% accuracy boost on known and novel classes in CUB, respectively. Our code is available at: https://github.com/Cliffia123/LegoGCD.","sentences":["Generalized Category Discovery (GCD) aims to identify a mix of known and novel categories within unlabeled data sets, providing a more realistic setting for image recognition.","Essentially, GCD needs to remember existing patterns thoroughly to recognize novel categories.","Recent state-of-the-art method SimGCD transfers the knowledge from known-class data to the learning of novel classes through debiased learning.","However, some patterns are catastrophically forgot during adaptation and thus lead to poor performance in novel categories classification.","To address this issue, we propose a novel learning approach, LegoGCD, which is seamlessly integrated into previous methods to enhance the discrimination of novel classes while maintaining performance on previously encountered known classes.","Specifically, we design two types of techniques termed as Local Entropy Regularization (LER) and Dual-views Kullback Leibler divergence constraint (DKL).","The LER optimizes the distribution of potential known class samples in unlabeled data, thus ensuring the preservation of knowledge related to known categories while learning novel classes.","Meanwhile, DKL introduces Kullback Leibler divergence to encourage the model to produce a similar prediction distribution of two view samples from the same image.","In this way, it successfully avoids mismatched prediction and generates more reliable potential known class samples simultaneously.","Extensive experiments validate that the proposed LegoGCD effectively addresses the known category forgetting issue across all datasets, eg, delivering a 7.74% and 2.51% accuracy boost on known and novel classes in CUB, respectively.","Our code is available at: https://github.com/Cliffia123/LegoGCD."],"url":"http://arxiv.org/abs/2501.05272v1"}
{"created":"2025-01-09 14:26:50","title":"CellViT++: Energy-Efficient and Adaptive Cell Segmentation and Classification Using Foundation Models","abstract":"Digital Pathology is a cornerstone in the diagnosis and treatment of diseases. A key task in this field is the identification and segmentation of cells in hematoxylin and eosin-stained images. Existing methods for cell segmentation often require extensive annotated datasets for training and are limited to a predefined cell classification scheme. To overcome these limitations, we propose $\\text{CellViT}^{{\\scriptscriptstyle ++}}$, a framework for generalized cell segmentation in digital pathology. $\\text{CellViT}^{{\\scriptscriptstyle ++}}$ utilizes Vision Transformers with foundation models as encoders to compute deep cell features and segmentation masks simultaneously. To adapt to unseen cell types, we rely on a computationally efficient approach. It requires minimal data for training and leads to a drastically reduced carbon footprint. We demonstrate excellent performance on seven different datasets, covering a broad spectrum of cell types, organs, and clinical settings. The framework achieves remarkable zero-shot segmentation and data-efficient cell-type classification. Furthermore, we show that $\\text{CellViT}^{{\\scriptscriptstyle ++}}$ can leverage immunofluorescence stainings to generate training datasets without the need for pathologist annotations. The automated dataset generation approach surpasses the performance of networks trained on manually labeled data, demonstrating its effectiveness in creating high-quality training datasets without expert annotations. To advance digital pathology, $\\text{CellViT}^{{\\scriptscriptstyle ++}}$ is available as an open-source framework featuring a user-friendly, web-based interface for visualization and annotation. The code is available under https://github.com/TIO-IKIM/CellViT-plus-plus.","sentences":["Digital Pathology is a cornerstone in the diagnosis and treatment of diseases.","A key task in this field is the identification and segmentation of cells in hematoxylin and eosin-stained images.","Existing methods for cell segmentation often require extensive annotated datasets for training and are limited to a predefined cell classification scheme.","To overcome these limitations, we propose $\\text{CellViT}^{{\\scriptscriptstyle ++}}$, a framework for generalized cell segmentation in digital pathology.","$\\text{CellViT}^{{\\scriptscriptstyle ++}}$ utilizes Vision Transformers with foundation models as encoders to compute deep cell features and segmentation masks simultaneously.","To adapt to unseen cell types, we rely on a computationally efficient approach.","It requires minimal data for training and leads to a drastically reduced carbon footprint.","We demonstrate excellent performance on seven different datasets, covering a broad spectrum of cell types, organs, and clinical settings.","The framework achieves remarkable zero-shot segmentation and data-efficient cell-type classification.","Furthermore, we show that $\\text{CellViT}^{{\\scriptscriptstyle ++}}$ can leverage immunofluorescence stainings to generate training datasets without the need for pathologist annotations.","The automated dataset generation approach surpasses the performance of networks trained on manually labeled data, demonstrating its effectiveness in creating high-quality training datasets without expert annotations.","To advance digital pathology, $\\text{CellViT}^{{\\scriptscriptstyle ++}}$ is available as an open-source framework featuring a user-friendly, web-based interface for visualization and annotation.","The code is available under https://github.com/TIO-IKIM/CellViT-plus-plus."],"url":"http://arxiv.org/abs/2501.05269v1"}
{"created":"2025-01-09 14:23:49","title":"Distributed Graph Algorithms with Predictions","abstract":"We initiate the study of deterministic distributed graph algorithms with predictions in synchronous message passing systems. The process at each node in the graph is given a prediction, which is some extra information about the problem instance that may be incorrect. The processes may use the predictions to help them solve the problem. The overall goal is to develop algorithms that both work faster when predictions are good and do not work much worse than algorithms without predictions when predictions are bad. Concepts from the more general area of algorithms with predictions, such as error measures, consistency, robustness, and smoothness, are adapted to distributed graph algorithms with predictions.   We consider algorithms with predictions for four distributed graph problems, Maximal Independent Set, Maximal Matching, $(\\Delta+1)$-Vertex Coloring, and $(2\\Delta-1)$-Edge Coloring, where $\\Delta$ denotes the degree of the graph. For each, we define an appropriate error measure. We present generic templates that can be used to design deterministic distributed graph algorithms with predictions from existing algorithms without predictions. Using these templates, we develop algorithms with predictions for Maximal Independent Set. Alternative error measures for the Maximal Independent Set problem are also considered. We obtain algorithms with predictions for general graphs and for rooted trees and analyze them using two of these error measures.","sentences":["We initiate the study of deterministic distributed graph algorithms with predictions in synchronous message passing systems.","The process at each node in the graph is given a prediction, which is some extra information about the problem instance that may be incorrect.","The processes may use the predictions to help them solve the problem.","The overall goal is to develop algorithms that both work faster when predictions are good and do not work much worse than algorithms without predictions when predictions are bad.","Concepts from the more general area of algorithms with predictions, such as error measures, consistency, robustness, and smoothness, are adapted to distributed graph algorithms with predictions.   ","We consider algorithms with predictions for four distributed graph problems, Maximal Independent Set, Maximal Matching, $(\\Delta+1)$-Vertex Coloring, and $(2\\Delta-1)$-Edge Coloring, where $\\Delta$ denotes the degree of the graph.","For each, we define an appropriate error measure.","We present generic templates that can be used to design deterministic distributed graph algorithms with predictions from existing algorithms without predictions.","Using these templates, we develop algorithms with predictions for Maximal Independent Set.","Alternative error measures for the Maximal Independent Set problem are also considered.","We obtain algorithms with predictions for general graphs and for rooted trees and analyze them using two of these error measures."],"url":"http://arxiv.org/abs/2501.05267v1"}
{"created":"2025-01-09 14:19:46","title":"Patch-GAN Transfer Learning with Reconstructive Models for Cloud Removal","abstract":"Cloud removal plays a crucial role in enhancing remote sensing image analysis, yet accurately reconstructing cloud-obscured regions remains a significant challenge. Recent advancements in generative models have made the generation of realistic images increasingly accessible, offering new opportunities for this task. Given the conceptual alignment between image generation and cloud removal tasks, generative models present a promising approach for addressing cloud removal in remote sensing. In this work, we propose a deep transfer learning approach built on a generative adversarial network (GAN) framework to explore the potential of the novel masked autoencoder (MAE) image reconstruction model in cloud removal. Due to the complexity of remote sensing imagery, we further propose using a patch-wise discriminator to determine whether each patch of the image is real or not. The proposed reconstructive transfer learning approach demonstrates significant improvements in cloud removal performance compared to other GAN-based methods. Additionally, whilst direct comparisons with some of the state-of-the-art cloud removal techniques are limited due to unclear details regarding their train/test data splits, the proposed model achieves competitive results based on available benchmarks.","sentences":["Cloud removal plays a crucial role in enhancing remote sensing image analysis, yet accurately reconstructing cloud-obscured regions remains a significant challenge.","Recent advancements in generative models have made the generation of realistic images increasingly accessible, offering new opportunities for this task.","Given the conceptual alignment between image generation and cloud removal tasks, generative models present a promising approach for addressing cloud removal in remote sensing.","In this work, we propose a deep transfer learning approach built on a generative adversarial network (GAN) framework to explore the potential of the novel masked autoencoder (MAE) image reconstruction model in cloud removal.","Due to the complexity of remote sensing imagery, we further propose using a patch-wise discriminator to determine whether each patch of the image is real or not.","The proposed reconstructive transfer learning approach demonstrates significant improvements in cloud removal performance compared to other GAN-based methods.","Additionally, whilst direct comparisons with some of the state-of-the-art cloud removal techniques are limited due to unclear details regarding their train/test data splits, the proposed model achieves competitive results based on available benchmarks."],"url":"http://arxiv.org/abs/2501.05265v1"}
{"created":"2025-01-09 14:19:33","title":"Towards Balanced Continual Multi-Modal Learning in Human Pose Estimation","abstract":"3D human pose estimation (3D HPE) has emerged as a prominent research topic, particularly in the realm of RGB-based methods. However, RGB images are susceptible to limitations such as sensitivity to lighting conditions and potential user discomfort. Consequently, multi-modal sensing, which leverages non-intrusive sensors, is gaining increasing attention. Nevertheless, multi-modal 3D HPE still faces challenges, including modality imbalance and the imperative for continual learning. In this work, we introduce a novel balanced continual multi-modal learning method for 3D HPE, which harnesses the power of RGB, LiDAR, mmWave, and WiFi. Specifically, we propose a Shapley value-based contribution algorithm to quantify the contribution of each modality and identify modality imbalance. To address this imbalance, we employ a re-learning strategy. Furthermore, recognizing that raw data is prone to noise contamination, we develop a novel denoising continual learning approach. This approach incorporates a noise identification and separation module to mitigate the adverse effects of noise and collaborates with the balanced learning strategy to enhance optimization. Additionally, an adaptive EWC mechanism is employed to alleviate catastrophic forgetting. We conduct extensive experiments on the widely-adopted multi-modal dataset, MM-Fi, which demonstrate the superiority of our approach in boosting 3D pose estimation and mitigating catastrophic forgetting in complex scenarios. We will release our codes.","sentences":["3D human pose estimation (3D HPE) has emerged as a prominent research topic, particularly in the realm of RGB-based methods.","However, RGB images are susceptible to limitations such as sensitivity to lighting conditions and potential user discomfort.","Consequently, multi-modal sensing, which leverages non-intrusive sensors, is gaining increasing attention.","Nevertheless, multi-modal 3D HPE still faces challenges, including modality imbalance and the imperative for continual learning.","In this work, we introduce a novel balanced continual multi-modal learning method for 3D HPE, which harnesses the power of RGB, LiDAR, mmWave, and WiFi.","Specifically, we propose a Shapley value-based contribution algorithm to quantify the contribution of each modality and identify modality imbalance.","To address this imbalance, we employ a re-learning strategy.","Furthermore, recognizing that raw data is prone to noise contamination, we develop a novel denoising continual learning approach.","This approach incorporates a noise identification and separation module to mitigate the adverse effects of noise and collaborates with the balanced learning strategy to enhance optimization.","Additionally, an adaptive EWC mechanism is employed to alleviate catastrophic forgetting.","We conduct extensive experiments on the widely-adopted multi-modal dataset, MM-Fi, which demonstrate the superiority of our approach in boosting 3D pose estimation and mitigating catastrophic forgetting in complex scenarios.","We will release our codes."],"url":"http://arxiv.org/abs/2501.05264v1"}
{"created":"2025-01-09 14:16:43","title":"QMDB: Quick Merkle Database","abstract":"Updating, managing, and proving world state are key bottlenecks facing the execution layer of blockchains today. Existing storage solutions are not flash-optimized and suffer from high flash write amplification and excessive DRAM requirements, forcing a trade-off between throughput and decentralization. We present the Quick Merkle Database (QMDB), an SSD-optimized authenticated data structure that delivers a superset of the features of existing databases. QMDB's append-only design enables 1 SSD read per state access, $O(1)$ I/Os for updates, and in-memory Merkleization on a DRAM footprint small enough to fit on consumer-grade PCs. We demonstrate that QMDB offers a significant leap in throughput ($6 \\times$ over RocksDB and $8 \\times$ over a state-of-the-art verifiable database) and validate its scalability on datasets up to 15 billion entries ($10 \\times$ Ethereum's state size in 2024). Our projections indicate QMDB could store a theoretical maximum of 280 billion entries on a single machine, far exceeding current blockchain requirements. QMDB scales across both commodity and enterprise hardware, achieving up to 2 million state updates per second. QMDB sets a new benchmark for verifiable databases, alleviating today's storage bottlenecks, lowering barriers to blockchain participation, and unlocking new blockchain applications.","sentences":["Updating, managing, and proving world state are key bottlenecks facing the execution layer of blockchains today.","Existing storage solutions are not flash-optimized and suffer from high flash write amplification and excessive DRAM requirements, forcing a trade-off between throughput and decentralization.","We present the Quick Merkle Database (QMDB), an SSD-optimized authenticated data structure that delivers a superset of the features of existing databases.","QMDB's append-only design enables 1 SSD read per state access, $O(1)$ I/Os for updates, and in-memory Merkleization on a DRAM footprint small enough to fit on consumer-grade PCs.","We demonstrate that QMDB offers a significant leap in throughput ($6 \\times$ over RocksDB","and $8 \\times$ over a state-of-the-art verifiable database) and validate its scalability on datasets up to 15 billion entries ($10 \\times$ Ethereum's state size in 2024).","Our projections indicate QMDB could store a theoretical maximum of 280 billion entries on a single machine, far exceeding current blockchain requirements.","QMDB scales across both commodity and enterprise hardware, achieving up to 2 million state updates per second.","QMDB sets a new benchmark for verifiable databases, alleviating today's storage bottlenecks, lowering barriers to blockchain participation, and unlocking new blockchain applications."],"url":"http://arxiv.org/abs/2501.05262v1"}
{"created":"2025-01-09 14:14:18","title":"Enhancing Plagiarism Detection in Marathi with a Weighted Ensemble of TF-IDF and BERT Embeddings for Low-Resource Language Processing","abstract":"Plagiarism involves using another person's work or concepts without proper attribution, presenting them as original creations. With the growing amount of data communicated in regional languages such as Marathi -- one of India's regional languages -- it is crucial to design robust plagiarism detection systems tailored for low-resource languages. Language models like Bidirectional Encoder Representations from Transformers (BERT) have demonstrated exceptional capability in text representation and feature extraction, making them essential tools for semantic analysis and plagiarism detection. However, the application of BERT for low-resource languages remains under-explored, particularly in the context of plagiarism detection. This paper presents a method to enhance the accuracy of plagiarism detection for Marathi texts using BERT sentence embeddings in conjunction with Term Frequency-Inverse Document Frequency (TF-IDF) feature representation. This approach effectively captures statistical, semantic, and syntactic aspects of text features through a weighted voting ensemble of machine learning models.","sentences":["Plagiarism involves using another person's work or concepts without proper attribution, presenting them as original creations.","With the growing amount of data communicated in regional languages such as Marathi -- one of India's regional languages -- it is crucial to design robust plagiarism detection systems tailored for low-resource languages.","Language models like Bidirectional Encoder Representations from Transformers (BERT) have demonstrated exceptional capability in text representation and feature extraction, making them essential tools for semantic analysis and plagiarism detection.","However, the application of BERT for low-resource languages remains under-explored, particularly in the context of plagiarism detection.","This paper presents a method to enhance the accuracy of plagiarism detection for Marathi texts using BERT sentence embeddings in conjunction with Term Frequency-Inverse Document Frequency (TF-IDF) feature representation.","This approach effectively captures statistical, semantic, and syntactic aspects of text features through a weighted voting ensemble of machine learning models."],"url":"http://arxiv.org/abs/2501.05260v1"}
{"created":"2025-01-09 13:54:59","title":"Domain-Incremental Semantic Segmentation for Autonomous Driving under Adverse Driving Conditions","abstract":"Semantic segmentation for autonomous driving is an even more challenging task when faced with adverse driving conditions. Standard models trained on data recorded under ideal conditions show a deteriorated performance in unfavorable weather or illumination conditions. Fine-tuning on the new task or condition would lead to overwriting the previously learned information resulting in catastrophic forgetting. Adapting to the new conditions through traditional domain adaption methods improves the performance on the target domain at the expense of the source domain. Addressing these issues, we propose an architecture-based domain-incremental learning approach called Progressive Semantic Segmentation (PSS). PSS is a task-agnostic, dynamically growing collection of domain-specific segmentation models. The task of inferring the domain and subsequently selecting the appropriate module for segmentation is carried out using a collection of convolutional autoencoders. We extensively evaluate our proposed approach using several datasets at varying levels of granularity in the categorization of adverse driving conditions. Furthermore, we demonstrate the generalization of the proposed approach to similar and unseen domains.","sentences":["Semantic segmentation for autonomous driving is an even more challenging task when faced with adverse driving conditions.","Standard models trained on data recorded under ideal conditions show a deteriorated performance in unfavorable weather or illumination conditions.","Fine-tuning on the new task or condition would lead to overwriting the previously learned information resulting in catastrophic forgetting.","Adapting to the new conditions through traditional domain adaption methods improves the performance on the target domain at the expense of the source domain.","Addressing these issues, we propose an architecture-based domain-incremental learning approach called Progressive Semantic Segmentation (PSS).","PSS is a task-agnostic, dynamically growing collection of domain-specific segmentation models.","The task of inferring the domain and subsequently selecting the appropriate module for segmentation is carried out using a collection of convolutional autoencoders.","We extensively evaluate our proposed approach using several datasets at varying levels of granularity in the categorization of adverse driving conditions.","Furthermore, we demonstrate the generalization of the proposed approach to similar and unseen domains."],"url":"http://arxiv.org/abs/2501.05246v1"}
{"created":"2025-01-09 13:44:42","title":"Is Your Autonomous Vehicle Safe? Understanding the Threat of Electromagnetic Signal Injection Attacks on Traffic Scene Perception","abstract":"Autonomous vehicles rely on camera-based perception systems to comprehend their driving environment and make crucial decisions, thereby ensuring vehicles to steer safely. However, a significant threat known as Electromagnetic Signal Injection Attacks (ESIA) can distort the images captured by these cameras, leading to incorrect AI decisions and potentially compromising the safety of autonomous vehicles. Despite the serious implications of ESIA, there is limited understanding of its impacts on the robustness of AI models across various and complex driving scenarios. To address this gap, our research analyzes the performance of different models under ESIA, revealing their vulnerabilities to the attacks. Moreover, due to the challenges in obtaining real-world attack data, we develop a novel ESIA simulation method and generate a simulated attack dataset for different driving scenarios. Our research provides a comprehensive simulation and evaluation framework, aiming to enhance the development of more robust AI models and secure intelligent systems, ultimately contributing to the advancement of safer and more reliable technology across various fields.","sentences":["Autonomous vehicles rely on camera-based perception systems to comprehend their driving environment and make crucial decisions, thereby ensuring vehicles to steer safely.","However, a significant threat known as Electromagnetic Signal Injection Attacks (ESIA) can distort the images captured by these cameras, leading to incorrect AI decisions and potentially compromising the safety of autonomous vehicles.","Despite the serious implications of ESIA, there is limited understanding of its impacts on the robustness of AI models across various and complex driving scenarios.","To address this gap, our research analyzes the performance of different models under ESIA, revealing their vulnerabilities to the attacks.","Moreover, due to the challenges in obtaining real-world attack data, we develop a novel ESIA simulation method and generate a simulated attack dataset for different driving scenarios.","Our research provides a comprehensive simulation and evaluation framework, aiming to enhance the development of more robust AI models and secure intelligent systems, ultimately contributing to the advancement of safer and more reliable technology across various fields."],"url":"http://arxiv.org/abs/2501.05239v1"}
{"created":"2025-01-09 13:36:37","title":"Harnessing Large Language and Vision-Language Models for Robust Out-of-Distribution Detection","abstract":"Out-of-distribution (OOD) detection has seen significant advancements with zero-shot approaches by leveraging the powerful Vision-Language Models (VLMs) such as CLIP. However, prior research works have predominantly focused on enhancing Far-OOD performance, while potentially compromising Near-OOD efficacy, as observed from our pilot study. To address this issue, we propose a novel strategy to enhance zero-shot OOD detection performances for both Far-OOD and Near-OOD scenarios by innovatively harnessing Large Language Models (LLMs) and VLMs. Our approach first exploit an LLM to generate superclasses of the ID labels and their corresponding background descriptions followed by feature extraction using CLIP. We then isolate the core semantic features for ID data by subtracting background features from the superclass features. The refined representation facilitates the selection of more appropriate negative labels for OOD data from a comprehensive candidate label set of WordNet, thereby enhancing the performance of zero-shot OOD detection in both scenarios. Furthermore, we introduce novel few-shot prompt tuning and visual prompt tuning to adapt the proposed framework to better align with the target distribution. Experimental results demonstrate that the proposed approach consistently outperforms current state-of-the-art methods across multiple benchmarks, with an improvement of up to 2.9% in AUROC and a reduction of up to 12.6% in FPR95. Additionally, our method exhibits superior robustness against covariate shift across different domains, further highlighting its effectiveness in real-world scenarios.","sentences":["Out-of-distribution (OOD) detection has seen significant advancements with zero-shot approaches by leveraging the powerful Vision-Language Models (VLMs) such as CLIP.","However, prior research works have predominantly focused on enhancing Far-OOD performance, while potentially compromising Near-OOD efficacy, as observed from our pilot study.","To address this issue, we propose a novel strategy to enhance zero-shot OOD detection performances for both Far-OOD and Near-OOD scenarios by innovatively harnessing Large Language Models (LLMs) and VLMs.","Our approach first exploit an LLM to generate superclasses of the ID labels and their corresponding background descriptions followed by feature extraction using CLIP.","We then isolate the core semantic features for ID data by subtracting background features from the superclass features.","The refined representation facilitates the selection of more appropriate negative labels for OOD data from a comprehensive candidate label set of WordNet, thereby enhancing the performance of zero-shot OOD detection in both scenarios.","Furthermore, we introduce novel few-shot prompt tuning and visual prompt tuning to adapt the proposed framework to better align with the target distribution.","Experimental results demonstrate that the proposed approach consistently outperforms current state-of-the-art methods across multiple benchmarks, with an improvement of up to 2.9% in AUROC and a reduction of up to 12.6% in FPR95.","Additionally, our method exhibits superior robustness against covariate shift across different domains, further highlighting its effectiveness in real-world scenarios."],"url":"http://arxiv.org/abs/2501.05228v1"}
{"created":"2025-01-09 13:29:54","title":"Light Transport-aware Diffusion Posterior Sampling for Single-View Reconstruction of 3D Volumes","abstract":"We introduce a single-view reconstruction technique of volumetric fields in which multiple light scattering effects are omnipresent, such as in clouds. We model the unknown distribution of volumetric fields using an unconditional diffusion model trained on a novel benchmark dataset comprising 1,000 synthetically simulated volumetric density fields. The neural diffusion model is trained on the latent codes of a novel, diffusion-friendly, monoplanar representation. The generative model is used to incorporate a tailored parametric diffusion posterior sampling technique into different reconstruction tasks. A physically-based differentiable volume renderer is employed to provide gradients with respect to light transport in the latent space. This stands in contrast to classic NeRF approaches and makes the reconstructions better aligned with observed data. Through various experiments, we demonstrate single-view reconstruction of volumetric clouds at a previously unattainable quality.","sentences":["We introduce a single-view reconstruction technique of volumetric fields in which multiple light scattering effects are omnipresent, such as in clouds.","We model the unknown distribution of volumetric fields using an unconditional diffusion model trained on a novel benchmark dataset comprising 1,000 synthetically simulated volumetric density fields.","The neural diffusion model is trained on the latent codes of a novel, diffusion-friendly, monoplanar representation.","The generative model is used to incorporate a tailored parametric diffusion posterior sampling technique into different reconstruction tasks.","A physically-based differentiable volume renderer is employed to provide gradients with respect to light transport in the latent space.","This stands in contrast to classic NeRF approaches and makes the reconstructions better aligned with observed data.","Through various experiments, we demonstrate single-view reconstruction of volumetric clouds at a previously unattainable quality."],"url":"http://arxiv.org/abs/2501.05226v1"}
{"created":"2025-01-09 13:28:07","title":"Implementation Pitfalls for Carbonate Mineral Dissolution -- a Technical Note","abstract":"In systems with slow reaction kinetics, such as mineral dissolution processes, chemical equilibrium cannot be assumed and an accurate understanding of reaction rates is essential; discrepancies in parameter reporting can greatly affect simulation results. This technical note identifies an issue with the reporting of rate parameters for carbonate mineral dissolution in a widely used database for reactive transport modeling based on Palandri and Kharaka 2004. This misrepresentation leads to a considerable overestimation of reaction timescales. Using the simulators Reaktoro and DuMuX, we simulated a simple calcite dissolution batch test and compared the results to experimental data. By adjusting the parameter to align with established literature, we demonstrate an improved fit between simulated and experimental data. Discrepancies in reaction timescales were reduced by an order of magnitude, emphasizing the importance of regular validation of simulations with experimental data.","sentences":["In systems with slow reaction kinetics, such as mineral dissolution processes, chemical equilibrium cannot be assumed and an accurate understanding of reaction rates is essential; discrepancies in parameter reporting can greatly affect simulation results.","This technical note identifies an issue with the reporting of rate parameters for carbonate mineral dissolution in a widely used database for reactive transport modeling based on Palandri and Kharaka 2004.","This misrepresentation leads to a considerable overestimation of reaction timescales.","Using the simulators Reaktoro and DuMuX, we simulated a simple calcite dissolution batch test and compared the results to experimental data.","By adjusting the parameter to align with established literature, we demonstrate an improved fit between simulated and experimental data.","Discrepancies in reaction timescales were reduced by an order of magnitude, emphasizing the importance of regular validation of simulations with experimental data."],"url":"http://arxiv.org/abs/2501.05225v1"}
{"created":"2025-01-09 13:19:59","title":"EVA-S2PLoR: A Secure Element-wise Multiplication Meets Logistic Regression on Heterogeneous Database","abstract":"Accurate nonlinear computation is a key challenge in privacy-preserving machine learning (PPML). Most existing frameworks approximate it through linear operations, resulting in significant precision loss. This paper proposes an efficient, verifiable and accurate security 2-party logistic regression framework (EVA-S2PLoR), which achieves accurate nonlinear function computation through a novel secure element-wise multiplication protocol and its derived protocols. Our framework primarily includes secure 2-party vector element-wise multiplication, addition to multiplication, reciprocal, and sigmoid function based on data disguising technology, where high efficiency and accuracy are guaranteed by the simple computation flow based on the real number domain and the few number of fixed communication rounds. We provide secure and robust anomaly detection through dimension transformation and Monte Carlo methods. EVA-S2PLoR outperforms many advanced frameworks in terms of precision (improving the performance of the sigmoid function by about 10 orders of magnitude compared to most frameworks) and delivers the best overall performance in secure logistic regression experiments.","sentences":["Accurate nonlinear computation is a key challenge in privacy-preserving machine learning (PPML).","Most existing frameworks approximate it through linear operations, resulting in significant precision loss.","This paper proposes an efficient, verifiable and accurate security 2-party logistic regression framework (EVA-S2PLoR), which achieves accurate nonlinear function computation through a novel secure element-wise multiplication protocol and its derived protocols.","Our framework primarily includes secure 2-party vector element-wise multiplication, addition to multiplication, reciprocal, and sigmoid function based on data disguising technology, where high efficiency and accuracy are guaranteed by the simple computation flow based on the real number domain and the few number of fixed communication rounds.","We provide secure and robust anomaly detection through dimension transformation and Monte Carlo methods.","EVA-S2PLoR outperforms many advanced frameworks in terms of precision (improving the performance of the sigmoid function by about 10 orders of magnitude compared to most frameworks) and delivers the best overall performance in secure logistic regression experiments."],"url":"http://arxiv.org/abs/2501.05223v1"}
{"created":"2025-01-09 13:13:24","title":"A Novel Approach to Scalable and Automatic Topic-Controlled Question Generation in Education","abstract":"The development of Automatic Question Generation (QG) models has the potential to significantly improve educational practices by reducing the teacher workload associated with creating educational content. This paper introduces a novel approach to educational question generation that controls the topical focus of questions. The proposed Topic-Controlled Question Generation (T-CQG) method enhances the relevance and effectiveness of the generated content for educational purposes. Our approach uses fine-tuning on a pre-trained T5-small model, employing specially created datasets tailored to educational needs. The research further explores the impacts of pre-training strategies, quantisation, and data augmentation on the model's performance. We specifically address the challenge of generating semantically aligned questions with paragraph-level contexts, thereby improving the topic specificity of the generated questions. In addition, we introduce and explore novel evaluation methods to assess the topical relatedness of the generated questions. Our results, validated through rigorous offline and human-backed evaluations, demonstrate that the proposed models effectively generate high-quality, topic-focused questions. These models have the potential to reduce teacher workload and support personalised tutoring systems by serving as bespoke question generators. With its relatively small number of parameters, the proposals not only advance the capabilities of question generation models for handling specific educational topics but also offer a scalable solution that reduces infrastructure costs. This scalability makes them feasible for widespread use in education without reliance on proprietary large language models like ChatGPT.","sentences":["The development of Automatic Question Generation (QG) models has the potential to significantly improve educational practices by reducing the teacher workload associated with creating educational content.","This paper introduces a novel approach to educational question generation that controls the topical focus of questions.","The proposed Topic-Controlled Question Generation (T-CQG) method enhances the relevance and effectiveness of the generated content for educational purposes.","Our approach uses fine-tuning on a pre-trained T5-small model, employing specially created datasets tailored to educational needs.","The research further explores the impacts of pre-training strategies, quantisation, and data augmentation on the model's performance.","We specifically address the challenge of generating semantically aligned questions with paragraph-level contexts, thereby improving the topic specificity of the generated questions.","In addition, we introduce and explore novel evaluation methods to assess the topical relatedness of the generated questions.","Our results, validated through rigorous offline and human-backed evaluations, demonstrate that the proposed models effectively generate high-quality, topic-focused questions.","These models have the potential to reduce teacher workload and support personalised tutoring systems by serving as bespoke question generators.","With its relatively small number of parameters, the proposals not only advance the capabilities of question generation models for handling specific educational topics but also offer a scalable solution that reduces infrastructure costs.","This scalability makes them feasible for widespread use in education without reliance on proprietary large language models like ChatGPT."],"url":"http://arxiv.org/abs/2501.05220v1"}
{"created":"2025-01-09 12:48:15","title":"An Algorithmic Approach for Causal Health Equity: A Look at Race Differentials in Intensive Care Unit (ICU) Outcomes","abstract":"The new era of large-scale data collection and analysis presents an opportunity for diagnosing and understanding the causes of health inequities. In this study, we describe a framework for systematically analyzing health disparities using causal inference. The framework is illustrated by investigating racial and ethnic disparities in intensive care unit (ICU) outcome between majority and minority groups in Australia (Indigenous vs. Non-Indigenous) and the United States (African-American vs. White). We demonstrate that commonly used statistical measures for quantifying inequity are insufficient, and focus on attributing the observed disparity to the causal mechanisms that generate it. We find that minority patients are younger at admission, have worse chronic health, are more likely to be admitted for urgent and non-elective reasons, and have higher illness severity. At the same time, however, we find a protective direct effect of belonging to a minority group, with minority patients showing improved survival compared to their majority counterparts, with all other variables kept equal. We demonstrate that this protective effect is related to the increased probability of being admitted to ICU, with minority patients having an increased risk of ICU admission. We also find that minority patients, while showing improved survival, are more likely to be readmitted to ICU. Thus, due to worse access to primary health care, minority patients are more likely to end up in ICU for preventable conditions, causing a reduction in the mortality rates and creating an effect that appears to be protective. Since the baseline risk of ICU admission may serve as proxy for lack of access to primary care, we developed the Indigenous Intensive Care Equity (IICE) Radar, a monitoring system for tracking the over-utilization of ICU resources by the Indigenous population of Australia across geographical areas.","sentences":["The new era of large-scale data collection and analysis presents an opportunity for diagnosing and understanding the causes of health inequities.","In this study, we describe a framework for systematically analyzing health disparities using causal inference.","The framework is illustrated by investigating racial and ethnic disparities in intensive care unit (ICU) outcome between majority and minority groups in Australia (Indigenous vs. Non-Indigenous) and the United States (African-American vs. White).","We demonstrate that commonly used statistical measures for quantifying inequity are insufficient, and focus on attributing the observed disparity to the causal mechanisms that generate it.","We find that minority patients are younger at admission, have worse chronic health, are more likely to be admitted for urgent and non-elective reasons, and have higher illness severity.","At the same time, however, we find a protective direct effect of belonging to a minority group, with minority patients showing improved survival compared to their majority counterparts, with all other variables kept equal.","We demonstrate that this protective effect is related to the increased probability of being admitted to ICU, with minority patients having an increased risk of ICU admission.","We also find that minority patients, while showing improved survival, are more likely to be readmitted to ICU.","Thus, due to worse access to primary health care, minority patients are more likely to end up in ICU for preventable conditions, causing a reduction in the mortality rates and creating an effect that appears to be protective.","Since the baseline risk of ICU admission may serve as proxy for lack of access to primary care, we developed the Indigenous Intensive Care Equity (IICE) Radar, a monitoring system for tracking the over-utilization of ICU resources by the Indigenous population of Australia across geographical areas."],"url":"http://arxiv.org/abs/2501.05197v1"}
{"created":"2025-01-09 11:41:20","title":"KabaddiPy: A package to enable access to Professional Kabaddi Data","abstract":"Kabaddi, a contact team sport of Indian origin, has seen a dramatic rise in global popularity, highlighted by the upcoming Kabaddi World Cup in 2025 with over sixteen international teams participating, alongside flourishing national leagues such as the Indian Pro Kabaddi League (230 million viewers) and the British Kabaddi League. We present the first open-source Python module to make Kabaddi statistical data easily accessible from multiple scattered sources across the internet. The module was developed by systematically web-scraping and collecting team-wise, player-wise and match-by-match data. The data has been cleaned, organized, and categorized into team overviews and player metrics, each filterable by season. The players are classified as raiders and defenders, with their best strategies for attacking, counter-attacking, and defending against different teams highlighted. Our module enables continuous monitoring of exponentially growing data streams, aiding researchers to quickly start building upon the data to answer critical questions, such as the impact of player inclusion/exclusion on team performance, scoring patterns against specific teams, and break down opponent gameplay. The data generated from Kabaddi tournaments has been sparsely used, and coaches and players rely heavily on intuition to make decisions and craft strategies. Our module can be utilized to build predictive models, craft uniquely strategic gameplays to target opponents and identify hidden correlations in the data. This open source module has the potential to increase time-efficiency, encourage analytical studies of Kabaddi gameplay and player dynamics and foster reproducible research. The data and code are publicly available: https://github.com/kabaddiPy/kabaddiPy","sentences":["Kabaddi, a contact team sport of Indian origin, has seen a dramatic rise in global popularity, highlighted by the upcoming Kabaddi World Cup in 2025 with over sixteen international teams participating, alongside flourishing national leagues such as the Indian Pro Kabaddi League (230 million viewers) and the British Kabaddi League.","We present the first open-source Python module to make Kabaddi statistical data easily accessible from multiple scattered sources across the internet.","The module was developed by systematically web-scraping and collecting team-wise, player-wise and match-by-match data.","The data has been cleaned, organized, and categorized into team overviews and player metrics, each filterable by season.","The players are classified as raiders and defenders, with their best strategies for attacking, counter-attacking, and defending against different teams highlighted.","Our module enables continuous monitoring of exponentially growing data streams, aiding researchers to quickly start building upon the data to answer critical questions, such as the impact of player inclusion/exclusion on team performance, scoring patterns against specific teams, and break down opponent gameplay.","The data generated from Kabaddi tournaments has been sparsely used, and coaches and players rely heavily on intuition to make decisions and craft strategies.","Our module can be utilized to build predictive models, craft uniquely strategic gameplays to target opponents and identify hidden correlations in the data.","This open source module has the potential to increase time-efficiency, encourage analytical studies of Kabaddi gameplay and player dynamics and foster reproducible research.","The data and code are publicly available: https://github.com/kabaddiPy/kabaddiPy"],"url":"http://arxiv.org/abs/2501.05168v1"}
{"created":"2025-01-09 11:19:40","title":"Biomedical Relation Extraction via Adaptive Document-Relation Cross-Mapping and Concept Unique Identifier","abstract":"Document-Level Biomedical Relation Extraction (Bio-RE) aims to identify relations between biomedical entities within extensive texts, serving as a crucial subfield of biomedical text mining. Existing Bio-RE methods struggle with cross-sentence inference, which is essential for capturing relations spanning multiple sentences. Moreover, previous methods often overlook the incompleteness of documents and lack the integration of external knowledge, limiting contextual richness. Besides, the scarcity of annotated data further hampers model training. Recent advancements in large language models (LLMs) have inspired us to explore all the above issues for document-level Bio-RE. Specifically, we propose a document-level Bio-RE framework via LLM Adaptive Document-Relation Cross-Mapping (ADRCM) Fine-Tuning and Concept Unique Identifier (CUI) Retrieval-Augmented Generation (RAG). First, we introduce the Iteration-of-REsummary (IoRs) prompt for solving the data scarcity issue. In this way, Bio-RE task-specific synthetic data can be generated by guiding ChatGPT to focus on entity relations and iteratively refining synthetic data. Next, we propose ADRCM fine-tuning, a novel fine-tuning recipe that establishes mappings across different documents and relations, enhancing the model's contextual understanding and cross-sentence inference capabilities. Finally, during the inference, a biomedical-specific RAG approach, named CUI RAG, is designed to leverage CUIs as indexes for entities, narrowing the retrieval scope and enriching the relevant document contexts. Experiments conducted on three Bio-RE datasets (GDA, CDR, and BioRED) demonstrate the state-of-the-art performance of our proposed method by comparing it with other related works.","sentences":["Document-Level Biomedical Relation Extraction (Bio-RE) aims to identify relations between biomedical entities within extensive texts, serving as a crucial subfield of biomedical text mining.","Existing Bio-RE methods struggle with cross-sentence inference, which is essential for capturing relations spanning multiple sentences.","Moreover, previous methods often overlook the incompleteness of documents and lack the integration of external knowledge, limiting contextual richness.","Besides, the scarcity of annotated data further hampers model training.","Recent advancements in large language models (LLMs) have inspired us to explore all the above issues for document-level Bio-RE.","Specifically, we propose a document-level Bio-RE framework via LLM Adaptive Document-Relation Cross-Mapping (ADRCM)","Fine-Tuning and Concept Unique Identifier (CUI) Retrieval-Augmented Generation (RAG).","First, we introduce the Iteration-of-REsummary (IoRs) prompt for solving the data scarcity issue.","In this way, Bio-RE task-specific synthetic data can be generated by guiding ChatGPT to focus on entity relations and iteratively refining synthetic data.","Next, we propose ADRCM fine-tuning, a novel fine-tuning recipe that establishes mappings across different documents and relations, enhancing the model's contextual understanding and cross-sentence inference capabilities.","Finally, during the inference, a biomedical-specific RAG approach, named CUI RAG, is designed to leverage CUIs as indexes for entities, narrowing the retrieval scope and enriching the relevant document contexts.","Experiments conducted on three Bio-RE datasets (GDA, CDR, and BioRED) demonstrate the state-of-the-art performance of our proposed method by comparing it with other related works."],"url":"http://arxiv.org/abs/2501.05155v1"}
{"created":"2025-01-09 10:56:50","title":"A Systematic Literature Review on Deep Learning-based Depth Estimation in Computer Vision","abstract":"Depth estimation (DE) provides spatial information about a scene and enables tasks such as 3D reconstruction, object detection, and scene understanding. Recently, there has been an increasing interest in using deep learning (DL)-based methods for DE. Traditional techniques rely on handcrafted features that often struggle to generalise to diverse scenes and require extensive manual tuning. However, DL models for DE can automatically extract relevant features from input data, adapt to various scene conditions, and generalise well to unseen environments. Numerous DL-based methods have been developed, making it necessary to survey and synthesize the state-of-the-art (SOTA). Previous reviews on DE have mainly focused on either monocular or stereo-based techniques, rather than comprehensively reviewing DE. Furthermore, to the best of our knowledge, there is no systematic literature review (SLR) that comprehensively focuses on DE. Therefore, this SLR study is being conducted. Initially, electronic databases were searched for relevant publications, resulting in 1284 publications. Using defined exclusion and quality criteria, 128 publications were shortlisted and further filtered to select 59 high-quality primary studies. These studies were analysed to extract data and answer defined research questions. Based on the results, DL methods were developed for mainly three different types of DE: monocular, stereo, and multi-view. 20 publicly available datasets were used to train, test, and evaluate DL models for DE, with KITTI, NYU Depth V2, and Make 3D being the most used datasets. 29 evaluation metrics were used to assess the performance of DE. 35 base models were reported in the primary studies, and the top five most-used base models were ResNet-50, ResNet-18, ResNet-101, U-Net, and VGG-16. Finally, the lack of ground truth data was among the most significant challenges reported by primary studies.","sentences":["Depth estimation (DE) provides spatial information about a scene and enables tasks such as 3D reconstruction, object detection, and scene understanding.","Recently, there has been an increasing interest in using deep learning (DL)-based methods for DE.","Traditional techniques rely on handcrafted features that often struggle to generalise to diverse scenes and require extensive manual tuning.","However, DL models for DE can automatically extract relevant features from input data, adapt to various scene conditions, and generalise well to unseen environments.","Numerous DL-based methods have been developed, making it necessary to survey and synthesize the state-of-the-art (SOTA).","Previous reviews on DE have mainly focused on either monocular or stereo-based techniques, rather than comprehensively reviewing DE.","Furthermore, to the best of our knowledge, there is no systematic literature review (SLR) that comprehensively focuses on DE.","Therefore, this SLR study is being conducted.","Initially, electronic databases were searched for relevant publications, resulting in 1284 publications.","Using defined exclusion and quality criteria, 128 publications were shortlisted and further filtered to select 59 high-quality primary studies.","These studies were analysed to extract data and answer defined research questions.","Based on the results, DL methods were developed for mainly three different types of DE: monocular, stereo, and multi-view.","20 publicly available datasets were used to train, test, and evaluate DL models for DE, with KITTI, NYU Depth V2, and Make 3D being the most used datasets.","29 evaluation metrics were used to assess the performance of DE. 35 base models were reported in the primary studies, and the top five most-used base models were ResNet-50, ResNet-18, ResNet-101, U-Net, and VGG-16.","Finally, the lack of ground truth data was among the most significant challenges reported by primary studies."],"url":"http://arxiv.org/abs/2501.05147v1"}
{"created":"2025-01-09 10:44:10","title":"Preference Queries over Taxonomic Domains","abstract":"When composing multiple preferences characterizing the most suitable results for a user, several issues may arise. Indeed, preferences can be partially contradictory, suffer from a mismatch with the level of detail of the actual data, and even lack natural properties such as transitivity. In this paper we formally investigate the problem of retrieving the best results complying with multiple preferences expressed in a logic-based language. Data are stored in relational tables with taxonomic domains, which allow the specification of preferences also over values that are more generic than those in the database. In this framework, we introduce two operators that rewrite preferences for enforcing the important properties of transitivity, which guarantees soundness of the result, and specificity, which solves all conflicts among preferences. Although, as we show, these two properties cannot be fully achieved together, we use our operators to identify the only two alternatives that ensure transitivity and minimize the residual conflicts. Building on this finding, we devise a technique, based on an original heuristics, for selecting the best results according to the two possible alternatives. We finally show, with a number of experiments over both synthetic and real-world datasets, the effectiveness and practical feasibility of the overall approach.","sentences":["When composing multiple preferences characterizing the most suitable results for a user, several issues may arise.","Indeed, preferences can be partially contradictory, suffer from a mismatch with the level of detail of the actual data, and even lack natural properties such as transitivity.","In this paper we formally investigate the problem of retrieving the best results complying with multiple preferences expressed in a logic-based language.","Data are stored in relational tables with taxonomic domains, which allow the specification of preferences also over values that are more generic than those in the database.","In this framework, we introduce two operators that rewrite preferences for enforcing the important properties of transitivity, which guarantees soundness of the result, and specificity, which solves all conflicts among preferences.","Although, as we show, these two properties cannot be fully achieved together, we use our operators to identify the only two alternatives that ensure transitivity and minimize the residual conflicts.","Building on this finding, we devise a technique, based on an original heuristics, for selecting the best results according to the two possible alternatives.","We finally show, with a number of experiments over both synthetic and real-world datasets, the effectiveness and practical feasibility of the overall approach."],"url":"http://arxiv.org/abs/2501.05138v1"}
{"created":"2025-01-09 10:33:16","title":"Learning In-Distribution Representations for Anomaly Detection","abstract":"Anomaly detection involves identifying data patterns that deviate from the anticipated norm. Traditional methods struggle in high-dimensional spaces due to the curse of dimensionality. In recent years, self-supervised learning, particularly through contrastive objectives, has driven advances in anomaly detection. However, vanilla contrastive learning struggles to align with the unique demands of anomaly detection, as it lacks a pretext task tailored to the homogeneous nature of In-Distribution (ID) data and the diversity of Out-of-Distribution (OOD) anomalies. Methods that attempt to address these challenges, such as introducing hard negatives through synthetic outliers, Outlier Exposure (OE), and supervised objectives, often rely on pretext tasks that fail to balance compact clustering of ID samples with sufficient separation from OOD data. In this work, we propose Focused In-distribution Representation Modeling (FIRM), a contrastive learning objective specifically designed for anomaly detection. Unlike existing approaches, FIRM incorporates synthetic outliers into its pretext task in a way that actively shapes the representation space, promoting compact clustering of ID samples while enforcing strong separation from outliers. This formulation addresses the challenges of class collision, enhancing both the compactness of ID representations and the discriminative power of the learned feature space. We show that FIRM surpasses other contrastive methods in standard benchmarks, significantly enhancing anomaly detection compared to both traditional and supervised contrastive learning objectives. Our ablation studies confirm that FIRM consistently improves the quality of representations and shows robustness across a range of scoring methods. The code is available at: https://github.com/willtl/firm.","sentences":["Anomaly detection involves identifying data patterns that deviate from the anticipated norm.","Traditional methods struggle in high-dimensional spaces due to the curse of dimensionality.","In recent years, self-supervised learning, particularly through contrastive objectives, has driven advances in anomaly detection.","However, vanilla contrastive learning struggles to align with the unique demands of anomaly detection, as it lacks a pretext task tailored to the homogeneous nature of In-Distribution (ID) data and the diversity of Out-of-Distribution (OOD) anomalies.","Methods that attempt to address these challenges, such as introducing hard negatives through synthetic outliers, Outlier Exposure (OE), and supervised objectives, often rely on pretext tasks that fail to balance compact clustering of ID samples with sufficient separation from OOD data.","In this work, we propose Focused In-distribution Representation Modeling (FIRM), a contrastive learning objective specifically designed for anomaly detection.","Unlike existing approaches, FIRM incorporates synthetic outliers into its pretext task in a way that actively shapes the representation space, promoting compact clustering of ID samples while enforcing strong separation from outliers.","This formulation addresses the challenges of class collision, enhancing both the compactness of ID representations and the discriminative power of the learned feature space.","We show that FIRM surpasses other contrastive methods in standard benchmarks, significantly enhancing anomaly detection compared to both traditional and supervised contrastive learning objectives.","Our ablation studies confirm that FIRM consistently improves the quality of representations and shows robustness across a range of scoring methods.","The code is available at: https://github.com/willtl/firm."],"url":"http://arxiv.org/abs/2501.05130v1"}
{"created":"2025-01-09 10:26:14","title":"Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model","abstract":"Most Large Vision-Language Models (LVLMs) to date are trained predominantly on English data, which makes them struggle to understand non-English input and fail to generate output in the desired target language. Existing efforts mitigate these issues by adding multilingual training data, but do so in a largely ad-hoc manner, lacking insight into how different training mixes tip the scale for different groups of languages. In this work, we present a comprehensive investigation into the training strategies for massively multilingual LVLMs. First, we conduct a series of multi-stage experiments spanning 13 downstream vision-language tasks and 43 languages, systematically examining: (1) the number of training languages that can be included without degrading English performance and (2) optimal language distributions of pre-training as well as (3) instruction-tuning data. Further, we (4) investigate how to improve multilingual text-in-image understanding, and introduce a new benchmark for the task. Surprisingly, our analysis reveals that one can (i) include as many as 100 training languages simultaneously (ii) with as little as 25-50\\% of non-English data, to greatly improve multilingual performance while retaining strong English performance. We further find that (iii) including non-English OCR data in pre-training and instruction-tuning is paramount for improving multilingual text-in-image understanding. Finally, we put all our findings together and train Centurio, a 100-language LVLM, offering state-of-the-art performance in an evaluation covering 14 tasks and 56 languages.","sentences":["Most Large Vision-Language Models (LVLMs) to date are trained predominantly on English data, which makes them struggle to understand non-English input and fail to generate output in the desired target language.","Existing efforts mitigate these issues by adding multilingual training data, but do so in a largely ad-hoc manner, lacking insight into how different training mixes tip the scale for different groups of languages.","In this work, we present a comprehensive investigation into the training strategies for massively multilingual LVLMs.","First, we conduct a series of multi-stage experiments spanning 13 downstream vision-language tasks and 43 languages, systematically examining: (1) the number of training languages that can be included without degrading English performance and (2) optimal language distributions of pre-training as well as (3) instruction-tuning data.","Further, we (4) investigate how to improve multilingual text-in-image understanding, and introduce a new benchmark for the task.","Surprisingly, our analysis reveals that one can (i) include as many as 100 training languages simultaneously (ii) with as little as 25-50\\% of non-English data, to greatly improve multilingual performance while retaining strong English performance.","We further find that (iii) including non-English OCR data in pre-training and instruction-tuning is paramount for improving multilingual text-in-image understanding.","Finally, we put all our findings together and train Centurio, a 100-language LVLM, offering state-of-the-art performance in an evaluation covering 14 tasks and 56 languages."],"url":"http://arxiv.org/abs/2501.05122v1"}
{"created":"2025-01-09 09:37:27","title":"Motion-X++: A Large-Scale Multimodal 3D Whole-body Human Motion Dataset","abstract":"In this paper, we introduce Motion-X++, a large-scale multimodal 3D expressive whole-body human motion dataset. Existing motion datasets predominantly capture body-only poses, lacking facial expressions, hand gestures, and fine-grained pose descriptions, and are typically limited to lab settings with manually labeled text descriptions, thereby restricting their scalability. To address this issue, we develop a scalable annotation pipeline that can automatically capture 3D whole-body human motion and comprehensive textural labels from RGB videos and build the Motion-X dataset comprising 81.1K text-motion pairs. Furthermore, we extend Motion-X into Motion-X++ by improving the annotation pipeline, introducing more data modalities, and scaling up the data quantities. Motion-X++ provides 19.5M 3D whole-body pose annotations covering 120.5K motion sequences from massive scenes, 80.8K RGB videos, 45.3K audios, 19.5M frame-level whole-body pose descriptions, and 120.5K sequence-level semantic labels. Comprehensive experiments validate the accuracy of our annotation pipeline and highlight Motion-X++'s significant benefits for generating expressive, precise, and natural motion with paired multimodal labels supporting several downstream tasks, including text-driven whole-body motion generation,audio-driven motion generation, 3D whole-body human mesh recovery, and 2D whole-body keypoints estimation, etc.","sentences":["In this paper, we introduce Motion-X++, a large-scale multimodal 3D expressive whole-body human motion dataset.","Existing motion datasets predominantly capture body-only poses, lacking facial expressions, hand gestures, and fine-grained pose descriptions, and are typically limited to lab settings with manually labeled text descriptions, thereby restricting their scalability.","To address this issue, we develop a scalable annotation pipeline that can automatically capture 3D whole-body human motion and comprehensive textural labels from RGB videos and build the Motion-X dataset comprising 81.1K text-motion pairs.","Furthermore, we extend Motion-X into Motion-X++ by improving the annotation pipeline, introducing more data modalities, and scaling up the data quantities.","Motion-X++ provides 19.5M 3D whole-body pose annotations covering 120.5K motion sequences from massive scenes, 80.8K RGB videos, 45.3K audios, 19.5M frame-level whole-body pose descriptions, and 120.5K sequence-level semantic labels.","Comprehensive experiments validate the accuracy of our annotation pipeline and highlight Motion-X++'s significant benefits for generating expressive, precise, and natural motion with paired multimodal labels supporting several downstream tasks, including text-driven whole-body motion generation,audio-driven motion generation, 3D whole-body human mesh recovery, and 2D whole-body keypoints estimation, etc."],"url":"http://arxiv.org/abs/2501.05098v1"}
{"created":"2025-01-09 09:21:09","title":"Advancing ALS Applications with Large-Scale Pre-training: Dataset Development and Downstream Assessment","abstract":"The pre-training and fine-tuning paradigm has revolutionized satellite remote sensing applications. However, this approach remains largely underexplored for airborne laser scanning (ALS), an important technology for applications such as forest management and urban planning. In this study, we address this gap by constructing a large-scale ALS point cloud dataset and evaluating its impact on downstream applications. Our dataset comprises ALS point clouds collected across the contiguous United States, provided by the United States Geological Survey's 3D Elevation Program. To ensure efficient data collection while capturing diverse land cover and terrain types, we introduce a geospatial sampling method that selects point cloud tiles based on land cover maps and digital elevation models. As a baseline self-supervised learning model, we adopt BEV-MAE, a state-of-the-art masked autoencoder for 3D outdoor point clouds, and pre-train it on the constructed dataset. The pre-trained models are subsequently fine-tuned for downstream tasks, including tree species classification, terrain scene recognition, and point cloud semantic segmentation. Our results show that the pre-trained models significantly outperform their scratch counterparts across all downstream tasks, demonstrating the transferability of the representations learned from the proposed dataset. Furthermore, we observe that scaling the dataset using our geospatial sampling method consistently enhances performance, whereas pre-training on datasets constructed with random sampling fails to achieve similar improvements. These findings highlight the utility of the constructed dataset and the effectiveness of our sampling strategy in the pre-training and fine-tuning paradigm. The source code and pre-trained models will be made publicly available at \\url{https://github.com/martianxiu/ALS_pretraining}.","sentences":["The pre-training and fine-tuning paradigm has revolutionized satellite remote sensing applications.","However, this approach remains largely underexplored for airborne laser scanning (ALS), an important technology for applications such as forest management and urban planning.","In this study, we address this gap by constructing a large-scale ALS point cloud dataset and evaluating its impact on downstream applications.","Our dataset comprises ALS point clouds collected across the contiguous United States, provided by the United States Geological Survey's 3D Elevation Program.","To ensure efficient data collection while capturing diverse land cover and terrain types, we introduce a geospatial sampling method that selects point cloud tiles based on land cover maps and digital elevation models.","As a baseline self-supervised learning model, we adopt BEV-MAE, a state-of-the-art masked autoencoder for 3D outdoor point clouds, and pre-train it on the constructed dataset.","The pre-trained models are subsequently fine-tuned for downstream tasks, including tree species classification, terrain scene recognition, and point cloud semantic segmentation.","Our results show that the pre-trained models significantly outperform their scratch counterparts across all downstream tasks, demonstrating the transferability of the representations learned from the proposed dataset.","Furthermore, we observe that scaling the dataset using our geospatial sampling method consistently enhances performance, whereas pre-training on datasets constructed with random sampling fails to achieve similar improvements.","These findings highlight the utility of the constructed dataset and the effectiveness of our sampling strategy in the pre-training and fine-tuning paradigm.","The source code and pre-trained models will be made publicly available at \\url{https://github.com/martianxiu/ALS_pretraining}."],"url":"http://arxiv.org/abs/2501.05095v1"}
{"created":"2025-01-09 09:11:40","title":"Enhanced Quantile Regression with Spiking Neural Networks for Long-Term System Health Prognostics","abstract":"This paper presents a novel predictive maintenance framework centered on Enhanced Quantile Regression Neural Networks EQRNNs, for anticipating system failures in industrial robotics. We address the challenge of early failure detection through a hybrid approach that combines advanced neural architectures. The system leverages dual computational stages: first implementing an EQRNN optimized for processing multi-sensor data streams including vibration, thermal, and power signatures, followed by an integrated Spiking Neural Network SNN, layer that enables microsecond-level response times. This architecture achieves notable accuracy rates of 92.3\\% in component failure prediction with a 90-hour advance warning window. Field testing conducted on an industrial scale with 50 robotic systems demonstrates significant operational improvements, yielding a 94\\% decrease in unexpected system failures and 76\\% reduction in maintenance-related downtimes. The framework's effectiveness in processing complex, multi-modal sensor data while maintaining computational efficiency validates its applicability for Industry 4.0 manufacturing environments.","sentences":["This paper presents a novel predictive maintenance framework centered on Enhanced Quantile Regression Neural Networks EQRNNs, for anticipating system failures in industrial robotics.","We address the challenge of early failure detection through a hybrid approach that combines advanced neural architectures.","The system leverages dual computational stages: first implementing an EQRNN optimized for processing multi-sensor data streams including vibration, thermal, and power signatures, followed by an integrated Spiking Neural Network SNN, layer that enables microsecond-level response times.","This architecture achieves notable accuracy rates of 92.3\\% in component failure prediction with a 90-hour advance warning window.","Field testing conducted on an industrial scale with 50 robotic systems demonstrates significant operational improvements, yielding a 94\\% decrease in unexpected system failures and 76\\% reduction in maintenance-related downtimes.","The framework's effectiveness in processing complex, multi-modal sensor data while maintaining computational efficiency validates its applicability for Industry 4.0 manufacturing environments."],"url":"http://arxiv.org/abs/2501.05087v1"}
{"created":"2025-01-09 09:00:32","title":"Analyzing Memorization in Large Language Models through the Lens of Model Attribution","abstract":"Large Language Models (LLMs) are prevalent in modern applications but often memorize training data, leading to privacy breaches and copyright issues. Existing research has mainly focused on posthoc analyses, such as extracting memorized content or developing memorization metrics, without exploring the underlying architectural factors that contribute to memorization. In this work, we investigate memorization from an architectural lens by analyzing how attention modules at different layers impact its memorization and generalization performance. Using attribution techniques, we systematically intervene in the LLM architecture by bypassing attention modules at specific blocks while keeping other components like layer normalization and MLP transformations intact. We provide theorems analyzing our intervention mechanism from a mathematical view, bounding the difference in layer outputs with and without our attributions. Our theoretical and empirical analyses reveal that attention modules in deeper transformer blocks are primarily responsible for memorization, whereas earlier blocks are crucial for the models generalization and reasoning capabilities. We validate our findings through comprehensive experiments on different LLM families (Pythia and GPTNeo) and five benchmark datasets. Our insights offer a practical approach to mitigate memorization in LLMs while preserving their performance, contributing to safer and more ethical deployment in real world applications.","sentences":["Large Language Models (LLMs) are prevalent in modern applications but often memorize training data, leading to privacy breaches and copyright issues.","Existing research has mainly focused on posthoc analyses, such as extracting memorized content or developing memorization metrics, without exploring the underlying architectural factors that contribute to memorization.","In this work, we investigate memorization from an architectural lens by analyzing how attention modules at different layers impact its memorization and generalization performance.","Using attribution techniques, we systematically intervene in the LLM architecture by bypassing attention modules at specific blocks while keeping other components like layer normalization and MLP transformations intact.","We provide theorems analyzing our intervention mechanism from a mathematical view, bounding the difference in layer outputs with and without our attributions.","Our theoretical and empirical analyses reveal that attention modules in deeper transformer blocks are primarily responsible for memorization, whereas earlier blocks are crucial for the models generalization and reasoning capabilities.","We validate our findings through comprehensive experiments on different LLM families (Pythia and GPTNeo) and five benchmark datasets.","Our insights offer a practical approach to mitigate memorization in LLMs while preserving their performance, contributing to safer and more ethical deployment in real world applications."],"url":"http://arxiv.org/abs/2501.05078v1"}
{"created":"2025-01-09 08:59:23","title":"TipSegNet: Fingertip Segmentation in Contactless Fingerprint Imaging","abstract":"Contactless fingerprint recognition systems offer a hygienic, user-friendly, and efficient alternative to traditional contact-based methods. However, their accuracy heavily relies on precise fingertip detection and segmentation, particularly under challenging background conditions. This paper introduces TipSegNet, a novel deep learning model that achieves state-of-the-art performance in segmenting fingertips directly from grayscale hand images. TipSegNet leverages a ResNeXt-101 backbone for robust feature extraction, combined with a Feature Pyramid Network (FPN) for multi-scale representation, enabling accurate segmentation across varying finger poses and image qualities. Furthermore, we employ an extensive data augmentation strategy to enhance the model's generalizability and robustness. TipSegNet outperforms existing methods, achieving a mean Intersection over Union (mIoU) of 0.987 and an accuracy of 0.999, representing a significant advancement in contactless fingerprint segmentation. This enhanced accuracy has the potential to substantially improve the reliability and effectiveness of contactless biometric systems in real-world applications.","sentences":["Contactless fingerprint recognition systems offer a hygienic, user-friendly, and efficient alternative to traditional contact-based methods.","However, their accuracy heavily relies on precise fingertip detection and segmentation, particularly under challenging background conditions.","This paper introduces TipSegNet, a novel deep learning model that achieves state-of-the-art performance in segmenting fingertips directly from grayscale hand images.","TipSegNet leverages a ResNeXt-101 backbone for robust feature extraction, combined with a Feature Pyramid Network (FPN) for multi-scale representation, enabling accurate segmentation across varying finger poses and image qualities.","Furthermore, we employ an extensive data augmentation strategy to enhance the model's generalizability and robustness.","TipSegNet outperforms existing methods, achieving a mean Intersection over Union (mIoU) of 0.987 and an accuracy of 0.999, representing a significant advancement in contactless fingerprint segmentation.","This enhanced accuracy has the potential to substantially improve the reliability and effectiveness of contactless biometric systems in real-world applications."],"url":"http://arxiv.org/abs/2501.05076v1"}
{"created":"2025-01-09 08:59:14","title":"A Text-Based Knowledge-Embedded Soft Sensing Modeling Approach for General Industrial Process Tasks Based on Large Language Model","abstract":"Data-driven soft sensors (DDSS) have become mainstream methods for predicting key performance indicators in process industries. However, DDSS development requires complex and costly customized designs tailored to various tasks during the modeling process. Moreover, DDSS are constrained to a single structured data modality, limiting their ability to incorporate additional contextual knowledge. Furthermore, DDSSs' limited representation learning leads to weak predictive performance with scarce data. To address these challenges, we propose a general framework named LLM-TKESS (large language model for text-based knowledge-embedded soft sensing), harnessing the powerful general problem-solving capabilities, cross-modal knowledge transfer abilities, and few-shot capabilities of LLM for enhanced soft sensing modeling. Specifically, an auxiliary variable series encoder (AVS Encoder) is proposed to unleash LLM's potential for capturing temporal relationships within series and spatial semantic relationships among auxiliary variables. Then, we propose a two-stage fine-tuning alignment strategy: in the first stage, employing parameter-efficient fine-tuning through autoregressive training adjusts LLM to rapidly accommodate process variable data, resulting in a soft sensing foundation model (SSFM). Subsequently, by training adapters, we adapt the SSFM to various downstream tasks without modifying its architecture. Then, we propose two text-based knowledge-embedded soft sensors, integrating new natural language modalities to overcome the limitations of pure structured data models. Furthermore, benefiting from LLM's pre-existing world knowledge, our model demonstrates outstanding predictive capabilities in small sample conditions. Using the thermal deformation of air preheater rotor as a case study, we validate through extensive experiments that LLM-TKESS exhibits outstanding performance.","sentences":["Data-driven soft sensors (DDSS) have become mainstream methods for predicting key performance indicators in process industries.","However, DDSS development requires complex and costly customized designs tailored to various tasks during the modeling process.","Moreover, DDSS are constrained to a single structured data modality, limiting their ability to incorporate additional contextual knowledge.","Furthermore, DDSSs' limited representation learning leads to weak predictive performance with scarce data.","To address these challenges, we propose a general framework named LLM-TKESS (large language model for text-based knowledge-embedded soft sensing), harnessing the powerful general problem-solving capabilities, cross-modal knowledge transfer abilities, and few-shot capabilities of LLM for enhanced soft sensing modeling.","Specifically, an auxiliary variable series encoder (AVS Encoder) is proposed to unleash LLM's potential for capturing temporal relationships within series and spatial semantic relationships among auxiliary variables.","Then, we propose a two-stage fine-tuning alignment strategy: in the first stage, employing parameter-efficient fine-tuning through autoregressive training adjusts LLM to rapidly accommodate process variable data, resulting in a soft sensing foundation model (SSFM).","Subsequently, by training adapters, we adapt the SSFM to various downstream tasks without modifying its architecture.","Then, we propose two text-based knowledge-embedded soft sensors, integrating new natural language modalities to overcome the limitations of pure structured data models.","Furthermore, benefiting from LLM's pre-existing world knowledge, our model demonstrates outstanding predictive capabilities in small sample conditions.","Using the thermal deformation of air preheater rotor as a case study, we validate through extensive experiments that LLM-TKESS exhibits outstanding performance."],"url":"http://arxiv.org/abs/2501.05075v1"}
{"created":"2025-01-09 08:44:06","title":"D3RM: A Discrete Denoising Diffusion Refinement Model for Piano Transcription","abstract":"Diffusion models have been widely used in the generative domain due to their convincing performance in modeling complex data distributions. Moreover, they have shown competitive results on discriminative tasks, such as image segmentation. While diffusion models have also been explored for automatic music transcription, their performance has yet to reach a competitive level. In this paper, we focus on discrete diffusion model's refinement capabilities and present a novel architecture for piano transcription. Our model utilizes Neighborhood Attention layers as the denoising module, gradually predicting the target high-resolution piano roll, conditioned on the finetuned features of a pretrained acoustic model. To further enhance refinement, we devise a novel strategy which applies distinct transition states during training and inference stage of discrete diffusion models. Experiments on the MAESTRO dataset show that our approach outperforms previous diffusion-based piano transcription models and the baseline model in terms of F1 score. Our code is available in https://github.com/hanshounsu/d3rm.","sentences":["Diffusion models have been widely used in the generative domain due to their convincing performance in modeling complex data distributions.","Moreover, they have shown competitive results on discriminative tasks, such as image segmentation.","While diffusion models have also been explored for automatic music transcription, their performance has yet to reach a competitive level.","In this paper, we focus on discrete diffusion model's refinement capabilities and present a novel architecture for piano transcription.","Our model utilizes Neighborhood Attention layers as the denoising module, gradually predicting the target high-resolution piano roll, conditioned on the finetuned features of a pretrained acoustic model.","To further enhance refinement, we devise a novel strategy which applies distinct transition states during training and inference stage of discrete diffusion models.","Experiments on the MAESTRO dataset show that our approach outperforms previous diffusion-based piano transcription models and the baseline model in terms of F1 score.","Our code is available in https://github.com/hanshounsu/d3rm."],"url":"http://arxiv.org/abs/2501.05068v1"}
{"created":"2025-01-09 08:43:09","title":"Improving Skeleton-based Action Recognition with Interactive Object Information","abstract":"Human skeleton information is important in skeleton-based action recognition, which provides a simple and efficient way to describe human pose. However, existing skeleton-based methods focus more on the skeleton, ignoring the objects interacting with humans, resulting in poor performance in recognizing actions that involve object interactions. We propose a new action recognition framework introducing object nodes to supplement absent interactive object information. We also propose Spatial Temporal Variable Graph Convolutional Networks (ST-VGCN) to effectively model the Variable Graph (VG) containing object nodes. Specifically, in order to validate the role of interactive object information, by leveraging a simple self-training approach, we establish a new dataset, JXGC 24, and an extended dataset, NTU RGB+D+Object 60, including more than 2 million additional object nodes. At the same time, we designe the Variable Graph construction method to accommodate a variable number of nodes for graph structure. Additionally, we are the first to explore the overfitting issue introduced by incorporating additional object information, and we propose a VG-based data augmentation method to address this issue, called Random Node Attack. Finally, regarding the network structure, we introduce two fusion modules, CAF and WNPool, along with a novel Node Balance Loss, to enhance the comprehensive performance by effectively fusing and balancing skeleton and object node information. Our method surpasses the previous state-of-the-art on multiple skeleton-based action recognition benchmarks. The accuracy of our method on NTU RGB+D 60 cross-subject split is 96.7\\%, and on cross-view split, it is 99.2\\%.","sentences":["Human skeleton information is important in skeleton-based action recognition, which provides a simple and efficient way to describe human pose.","However, existing skeleton-based methods focus more on the skeleton, ignoring the objects interacting with humans, resulting in poor performance in recognizing actions that involve object interactions.","We propose a new action recognition framework introducing object nodes to supplement absent interactive object information.","We also propose Spatial Temporal Variable Graph Convolutional Networks (ST-VGCN) to effectively model the Variable Graph (VG) containing object nodes.","Specifically, in order to validate the role of interactive object information, by leveraging a simple self-training approach, we establish a new dataset, JXGC 24, and an extended dataset, NTU RGB+D+Object 60, including more than 2 million additional object nodes.","At the same time, we designe the Variable Graph construction method to accommodate a variable number of nodes for graph structure.","Additionally, we are the first to explore the overfitting issue introduced by incorporating additional object information, and we propose a VG-based data augmentation method to address this issue, called Random Node Attack.","Finally, regarding the network structure, we introduce two fusion modules, CAF and WNPool, along with a novel Node Balance Loss, to enhance the comprehensive performance by effectively fusing and balancing skeleton and object node information.","Our method surpasses the previous state-of-the-art on multiple skeleton-based action recognition benchmarks.","The accuracy of our method on NTU RGB+D 60 cross-subject split is 96.7\\%, and on cross-view split, it is 99.2\\%."],"url":"http://arxiv.org/abs/2501.05066v1"}
{"created":"2025-01-09 08:24:10","title":"TAPFed: Threshold Secure Aggregation for Privacy-Preserving Federated Learning","abstract":"Federated learning is a computing paradigm that enhances privacy by enabling multiple parties to collaboratively train a machine learning model without revealing personal data. However, current research indicates that traditional federated learning platforms are unable to ensure privacy due to privacy leaks caused by the interchange of gradients. To achieve privacy-preserving federated learning, integrating secure aggregation mechanisms is essential. Unfortunately, existing solutions are vulnerable to recently demonstrated inference attacks such as the disaggregation attack. This paper proposes TAPFed, an approach for achieving privacy-preserving federated learning in the context of multiple decentralized aggregators with malicious actors. TAPFed uses a proposed threshold functional encryption scheme and allows for a certain number of malicious aggregators while maintaining security and privacy. We provide formal security and privacy analyses of TAPFed and compare it to various baselines through experimental evaluation. Our results show that TAPFed offers equivalent performance in terms of model quality compared to state-of-the-art approaches while reducing transmission overhead by 29%-45% across different model training scenarios. Most importantly, TAPFed can defend against recently demonstrated inference attacks caused by curious aggregators, which the majority of existing approaches are susceptible to.","sentences":["Federated learning is a computing paradigm that enhances privacy by enabling multiple parties to collaboratively train a machine learning model without revealing personal data.","However, current research indicates that traditional federated learning platforms are unable to ensure privacy due to privacy leaks caused by the interchange of gradients.","To achieve privacy-preserving federated learning, integrating secure aggregation mechanisms is essential.","Unfortunately, existing solutions are vulnerable to recently demonstrated inference attacks such as the disaggregation attack.","This paper proposes TAPFed, an approach for achieving privacy-preserving federated learning in the context of multiple decentralized aggregators with malicious actors.","TAPFed uses a proposed threshold functional encryption scheme and allows for a certain number of malicious aggregators while maintaining security and privacy.","We provide formal security and privacy analyses of TAPFed and compare it to various baselines through experimental evaluation.","Our results show that TAPFed offers equivalent performance in terms of model quality compared to state-of-the-art approaches while reducing transmission overhead by 29%-45% across different model training scenarios.","Most importantly, TAPFed can defend against recently demonstrated inference attacks caused by curious aggregators, which the majority of existing approaches are susceptible to."],"url":"http://arxiv.org/abs/2501.05053v1"}
{"created":"2025-01-09 08:15:12","title":"Approximate Minimum Tree Cover in All Symmetric Monotone Norms Simultaneously","abstract":"We study the problem of partitioning a set of $n$ objects in a metric space into $k$ clusters $V_1,\\dots,V_k$. The quality of the clustering is measured by considering the vector of cluster costs and then minimizing some monotone symmetric norm of that vector (in particular, this includes the $\\ell_p$-norms). For the costs of the clusters we take the weight of a minimum-weight spanning tree on the objects in~$V_i$, which may serve as a proxy for the cost of traversing all objects in the cluster, but also as a shape-invariant measure of cluster density similar to Single-Linkage Clustering.   This setting has been studied by Even, Garg, K\\\"onemann, Ravi, Sinha (Oper. Res. Lett.}, 2004) for the setting of minimizing the weight of the largest cluster (i.e., using $\\ell_\\infty$) as Min-Max Tree Cover, for which they gave a constant-factor approximation. We provide a careful adaptation of their algorithm to compute solutions which are approximately optimal with respect to all monotone symmetric norms simultaneously, and show how to find them in polynomial time. In fact, our algorithm is purely combinatorial and can process metric spaces with 10,000 points in less than a second.   As an extension, we also consider the case where instead of a target number of clusters we are provided with a set of depots in the space such that every cluster should contain at least one such depot. For this setting also we are able to give a polynomial time algorithm computing a constant factor approximation with respect to all monotone symmetric norms simultaneously.   To show that the algorithmic results are tight up to the precise constant of approximation attainable, we also prove that such clustering problems are already APX-hard when considering only one single $\\ell_p$ norm for the objective.","sentences":["We study the problem of partitioning a set of $n$ objects in a metric space into $k$ clusters $V_1,\\dots,V_k$.","The quality of the clustering is measured by considering the vector of cluster costs and then minimizing some monotone symmetric norm of that vector (in particular, this includes the $\\ell_p$-norms).","For the costs of the clusters we take the weight of a minimum-weight spanning tree on the objects in~$V_i$, which may serve as a proxy for the cost of traversing all objects in the cluster, but also as a shape-invariant measure of cluster density similar to Single-Linkage Clustering.   ","This setting has been studied by Even, Garg, K\\\"onemann, Ravi, Sinha (Oper.","Res.","Lett.}, 2004) for the setting of minimizing the weight of the largest cluster (i.e., using $\\ell_\\infty$) as Min-Max Tree Cover, for which they gave a constant-factor approximation.","We provide a careful adaptation of their algorithm to compute solutions which are approximately optimal with respect to all monotone symmetric norms simultaneously, and show how to find them in polynomial time.","In fact, our algorithm is purely combinatorial and can process metric spaces with 10,000 points in less than a second.   ","As an extension, we also consider the case where instead of a target number of clusters we are provided with a set of depots in the space such that every cluster should contain at least one such depot.","For this setting also we are able to give a polynomial time algorithm computing a constant factor approximation with respect to all monotone symmetric norms simultaneously.   ","To show that the algorithmic results are tight up to the precise constant of approximation attainable, we also prove that such clustering problems are already APX-hard when considering only one single $\\ell_p$ norm for the objective."],"url":"http://arxiv.org/abs/2501.05048v1"}
{"created":"2025-01-09 07:54:24","title":"SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution","abstract":"Large Language Models (LLMs) have demonstrated remarkable proficiency across a variety of complex tasks. One significant application of LLMs is in tackling software engineering challenges, particularly in resolving real-world tasks on GitHub by fixing code based on the issues reported by the users. However, many current approaches rely on proprietary LLMs, which limits reproducibility, accessibility, and transparency. The critical components of LLMs for addressing software engineering issues and how their capabilities can be effectively enhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a novel open-source LLM designed to effectively and efficiently resolve GitHub issues. SWE-Fixer comprises two essential modules: a code file retrieval module and a code editing module. The retrieval module employs BM25 along with a lightweight LLM model to achieve coarse-to-fine file retrieval. Subsequently, the code editing module utilizes the other LLM model to generate patches for the identified files. Then, to mitigate the lack of publicly available datasets, we compile an extensive dataset that includes 110K GitHub issues along with their corresponding patches, and train the two modules of SWE-Fixer separately. We assess our approach on the SWE-Bench Lite and Verified benchmarks, achieving state-of-the-art performance among open-source models with scores of 23.3% and 30.2%, respectively. These outcomes highlight the efficacy of our approach. We will make our model, dataset, and code publicly available at https://github.com/InternLM/SWE-Fixer.","sentences":["Large Language Models (LLMs) have demonstrated remarkable proficiency across a variety of complex tasks.","One significant application of LLMs is in tackling software engineering challenges, particularly in resolving real-world tasks on GitHub by fixing code based on the issues reported by the users.","However, many current approaches rely on proprietary LLMs, which limits reproducibility, accessibility, and transparency.","The critical components of LLMs for addressing software engineering issues and how their capabilities can be effectively enhanced remain unclear.","To address these challenges, we introduce SWE-Fixer, a novel open-source LLM designed to effectively and efficiently resolve GitHub issues.","SWE-Fixer comprises two essential modules: a code file retrieval module and a code editing module.","The retrieval module employs BM25 along with a lightweight LLM model to achieve coarse-to-fine file retrieval.","Subsequently, the code editing module utilizes the other LLM model to generate patches for the identified files.","Then, to mitigate the lack of publicly available datasets, we compile an extensive dataset that includes 110K GitHub issues along with their corresponding patches, and train the two modules of SWE-Fixer separately.","We assess our approach on the SWE-Bench Lite and Verified benchmarks, achieving state-of-the-art performance among open-source models with scores of 23.3% and 30.2%, respectively.","These outcomes highlight the efficacy of our approach.","We will make our model, dataset, and code publicly available at https://github.com/InternLM/SWE-Fixer."],"url":"http://arxiv.org/abs/2501.05040v1"}
{"created":"2025-01-09 07:51:14","title":"LongViTU: Instruction Tuning for Long-Form Video Understanding","abstract":"This paper introduce LongViTU, a large-scale (~121k QA pairs, ~900h videos), automatically generated dataset for long-form video understanding. We developed a systematic approach that organizes videos into a hierarchical tree structure and incorporates self-revision mechanisms to ensure high-quality QA pairs. Each QA pair in LongViTU features: 1) long-term context (average certificate length of 4.6 minutes); 2) rich knowledge and condensed reasoning (commonsense, causality, planning, etc.); and 3) explicit timestamp labels for relevant events. LongViTU also serves as a benchmark for instruction following in long-form and streaming video understanding. We evaluate the open-source state-of-the-art long video understanding model, LongVU, and the commercial model, Gemini-1.5-Pro, on our benchmark. They achieve GPT-4 scores of 49.9 and 52.3, respectively, underscoring the substantial challenge posed by our benchmark. Further supervised fine-tuning (SFT) on LongVU led to performance improvements of 12.0% on our benchmark, 2.2% on the in-distribution (ID) benchmark EgoSchema, 1.0%, 2.2% and 1.2% on the out-of-distribution (OOD) benchmarks VideoMME (Long), WorldQA and OpenEQA, respectively. These outcomes demonstrate LongViTU's high data quality and robust OOD generalizability.","sentences":["This paper introduce LongViTU, a large-scale (~121k QA pairs, ~900h videos), automatically generated dataset for long-form video understanding.","We developed a systematic approach that organizes videos into a hierarchical tree structure and incorporates self-revision mechanisms to ensure high-quality QA pairs.","Each QA pair in LongViTU features: 1) long-term context (average certificate length of 4.6 minutes); 2) rich knowledge and condensed reasoning (commonsense, causality, planning, etc.); and 3) explicit timestamp labels for relevant events.","LongViTU also serves as a benchmark for instruction following in long-form and streaming video understanding.","We evaluate the open-source state-of-the-art long video understanding model, LongVU, and the commercial model, Gemini-1.5-Pro, on our benchmark.","They achieve GPT-4 scores of 49.9 and 52.3, respectively, underscoring the substantial challenge posed by our benchmark.","Further supervised fine-tuning (SFT) on LongVU led to performance improvements of 12.0% on our benchmark, 2.2% on the in-distribution (ID) benchmark EgoSchema, 1.0%, 2.2% and 1.2% on the out-of-distribution (OOD) benchmarks VideoMME (Long), WorldQA and OpenEQA, respectively.","These outcomes demonstrate LongViTU's high data quality and robust OOD generalizability."],"url":"http://arxiv.org/abs/2501.05037v1"}
{"created":"2025-01-09 07:49:37","title":"Towards Fingerprint Mosaicking Artifact Detection: A Self-Supervised Deep Learning Approach","abstract":"Fingerprint mosaicking, which is the process of combining multiple fingerprint images into a single master fingerprint, is an essential process in modern biometric systems. However, it is prone to errors that can significantly degrade fingerprint image quality. This paper proposes a novel deep learning-based approach to detect and score mosaicking artifacts in fingerprint images. Our method leverages a self-supervised learning framework to train a model on large-scale unlabeled fingerprint data, eliminating the need for manual artifact annotation. The proposed model effectively identifies mosaicking errors, achieving high accuracy on various fingerprint modalities, including contactless, rolled, and pressed fingerprints and furthermore proves to be robust to different data sources. Additionally, we introduce a novel mosaicking artifact score to quantify the severity of errors, enabling automated evaluation of fingerprint images. By addressing the challenges of mosaicking artifact detection, our work contributes to improving the accuracy and reliability of fingerprint-based biometric systems.","sentences":["Fingerprint mosaicking, which is the process of combining multiple fingerprint images into a single master fingerprint, is an essential process in modern biometric systems.","However, it is prone to errors that can significantly degrade fingerprint image quality.","This paper proposes a novel deep learning-based approach to detect and score mosaicking artifacts in fingerprint images.","Our method leverages a self-supervised learning framework to train a model on large-scale unlabeled fingerprint data, eliminating the need for manual artifact annotation.","The proposed model effectively identifies mosaicking errors, achieving high accuracy on various fingerprint modalities, including contactless, rolled, and pressed fingerprints and furthermore proves to be robust to different data sources.","Additionally, we introduce a novel mosaicking artifact score to quantify the severity of errors, enabling automated evaluation of fingerprint images.","By addressing the challenges of mosaicking artifact detection, our work contributes to improving the accuracy and reliability of fingerprint-based biometric systems."],"url":"http://arxiv.org/abs/2501.05034v1"}
{"created":"2025-01-09 07:43:49","title":"ECBench: Can Multi-modal Foundation Models Understand the Egocentric World? A Holistic Embodied Cognition Benchmark","abstract":"The enhancement of generalization in robots by large vision-language models (LVLMs) is increasingly evident. Therefore, the embodied cognitive abilities of LVLMs based on egocentric videos are of great interest. However, current datasets for embodied video question answering lack comprehensive and systematic evaluation frameworks. Critical embodied cognitive issues, such as robotic self-cognition, dynamic scene perception, and hallucination, are rarely addressed. To tackle these challenges, we propose ECBench, a high-quality benchmark designed to systematically evaluate the embodied cognitive abilities of LVLMs. ECBench features a diverse range of scene video sources, open and varied question formats, and 30 dimensions of embodied cognition. To ensure quality, balance, and high visual dependence, ECBench uses class-independent meticulous human annotation and multi-round question screening strategies. Additionally, we introduce ECEval, a comprehensive evaluation system that ensures the fairness and rationality of the indicators. Utilizing ECBench, we conduct extensive evaluations of proprietary, open-source, and task-specific LVLMs. ECBench is pivotal in advancing the embodied cognitive capabilities of LVLMs, laying a solid foundation for developing reliable core models for embodied agents. All data and code are available at https://github.com/Rh-Dang/ECBench.","sentences":["The enhancement of generalization in robots by large vision-language models (LVLMs) is increasingly evident.","Therefore, the embodied cognitive abilities of LVLMs based on egocentric videos are of great interest.","However, current datasets for embodied video question answering lack comprehensive and systematic evaluation frameworks.","Critical embodied cognitive issues, such as robotic self-cognition, dynamic scene perception, and hallucination, are rarely addressed.","To tackle these challenges, we propose ECBench, a high-quality benchmark designed to systematically evaluate the embodied cognitive abilities of LVLMs.","ECBench features a diverse range of scene video sources, open and varied question formats, and 30 dimensions of embodied cognition.","To ensure quality, balance, and high visual dependence, ECBench uses class-independent meticulous human annotation and multi-round question screening strategies.","Additionally, we introduce ECEval, a comprehensive evaluation system that ensures the fairness and rationality of the indicators.","Utilizing ECBench, we conduct extensive evaluations of proprietary, open-source, and task-specific LVLMs.","ECBench is pivotal in advancing the embodied cognitive capabilities of LVLMs, laying a solid foundation for developing reliable core models for embodied agents.","All data and code are available at https://github.com/Rh-Dang/ECBench."],"url":"http://arxiv.org/abs/2501.05031v1"}
{"created":"2025-01-09 07:33:11","title":"Sampling Unlabeled Chordal Graphs in Expected Polynomial Time","abstract":"We design an algorithm that generates an $n$-vertex unlabeled chordal graph uniformly at random in expected polynomial time. Along the way, we develop the following two results: (1) an $\\mathsf{FPT}$ algorithm for counting and sampling labeled chordal graphs with a given automorphism $\\pi$, parameterized by the number of moved points of $\\pi$, and (2) a proof that the probability that a random $n$-vertex labeled chordal graph has a given automorphism $\\pi\\in S_n$ is at most $1/2^{c\\max\\{\\mu^2,n\\}}$, where $\\mu$ is the number of moved points of $\\pi$ and $c$ is a constant. Our algorithm for sampling unlabeled chordal graphs calls the aforementioned $\\mathsf{FPT}$ algorithm as a black box with potentially large values of the parameter $\\mu$, but the probability of calling this algorithm with a large value of $\\mu$ is exponentially small.","sentences":["We design an algorithm that generates an $n$-vertex unlabeled chordal graph uniformly at random in expected polynomial time.","Along the way, we develop the following two results: (1) an $\\mathsf{FPT}$ algorithm for counting and sampling labeled chordal graphs with a given automorphism $\\pi$, parameterized by the number of moved points of $\\pi$, and (2) a proof that the probability that a random $n$-vertex labeled chordal graph has a given automorphism $\\pi\\in S_n$ is at most $1/2^{c\\max\\{\\mu^2,n\\}}$, where $\\mu$ is the number of moved points of $\\pi$ and $c$ is a constant.","Our algorithm for sampling unlabeled chordal graphs calls the aforementioned $\\mathsf{FPT}$ algorithm as a black box with potentially large values of the parameter $\\mu$, but the probability of calling this algorithm with a large value of $\\mu$ is exponentially small."],"url":"http://arxiv.org/abs/2501.05024v1"}
{"created":"2025-01-09 07:18:48","title":"Continuous Knowledge-Preserving Decomposition for Few-Shot Continual Learning","abstract":"Few-shot class-incremental learning (FSCIL) involves learning new classes from limited data while retaining prior knowledge, and often results in catastrophic forgetting. Existing methods either freeze backbone networks to preserve knowledge, which limits adaptability, or rely on additional modules or prompts, introducing inference overhead. To this end, we propose Continuous Knowledge-Preserving Decomposition for FSCIL (CKPD-FSCIL), a framework that decomposes a model's weights into two parts: one that compacts existing knowledge (knowledge-sensitive components) and another that carries redundant capacity to accommodate new abilities (redundant-capacity components). The decomposition is guided by a covariance matrix from replay samples, ensuring principal components align with classification abilities. During adaptation, we freeze the knowledge-sensitive components and only adapt the redundant-capacity components, fostering plasticity while minimizing interference without changing the architecture or increasing overhead. Additionally, CKPD introduces an adaptive layer selection strategy to identify layers with redundant capacity, dynamically allocating adapters. Experiments on multiple benchmarks show that CKPD-FSCIL outperforms state-of-the-art methods.","sentences":["Few-shot class-incremental learning (FSCIL) involves learning new classes from limited data while retaining prior knowledge, and often results in catastrophic forgetting.","Existing methods either freeze backbone networks to preserve knowledge, which limits adaptability, or rely on additional modules or prompts, introducing inference overhead.","To this end, we propose Continuous Knowledge-Preserving Decomposition for FSCIL (CKPD-FSCIL), a framework that decomposes a model's weights into two parts: one that compacts existing knowledge (knowledge-sensitive components) and another that carries redundant capacity to accommodate new abilities (redundant-capacity components).","The decomposition is guided by a covariance matrix from replay samples, ensuring principal components align with classification abilities.","During adaptation, we freeze the knowledge-sensitive components and only adapt the redundant-capacity components, fostering plasticity while minimizing interference without changing the architecture or increasing overhead.","Additionally, CKPD introduces an adaptive layer selection strategy to identify layers with redundant capacity, dynamically allocating adapters.","Experiments on multiple benchmarks show that CKPD-FSCIL outperforms state-of-the-art methods."],"url":"http://arxiv.org/abs/2501.05017v1"}
{"created":"2025-01-09 07:11:51","title":"A Scalable System for Visual Analysis of Ocean Data","abstract":"Oceanographers rely on visual analysis to interpret model simulations, identify events and phenomena, and track dynamic ocean processes. The ever increasing resolution and complexity of ocean data due to its dynamic nature and multivariate relationships demands a scalable and adaptable visualization tool for interactive exploration. We introduce pyParaOcean, a scalable and interactive visualization system designed specifically for ocean data analysis. pyParaOcean offers specialized modules for common oceanographic analysis tasks, including eddy identification and salinity movement tracking. These modules seamlessly integrate with ParaView as filters, ensuring a user-friendly and easy-to-use system while leveraging the parallelization capabilities of ParaView and a plethora of inbuilt general-purpose visualization functionalities. The creation of an auxiliary dataset stored as a Cinema database helps address I/O and network bandwidth bottlenecks while supporting the generation of quick overview visualizations. We present a case study on the Bay of Bengal (BoB) to demonstrate the utility of the system and scaling studies to evaluate the efficiency of the system.","sentences":["Oceanographers rely on visual analysis to interpret model simulations, identify events and phenomena, and track dynamic ocean processes.","The ever increasing resolution and complexity of ocean data due to its dynamic nature and multivariate relationships demands a scalable and adaptable visualization tool for interactive exploration.","We introduce pyParaOcean, a scalable and interactive visualization system designed specifically for ocean data analysis.","pyParaOcean offers specialized modules for common oceanographic analysis tasks, including eddy identification and salinity movement tracking.","These modules seamlessly integrate with ParaView as filters, ensuring a user-friendly and easy-to-use system while leveraging the parallelization capabilities of ParaView and a plethora of inbuilt general-purpose visualization functionalities.","The creation of an auxiliary dataset stored as a Cinema database helps address I/O and network bandwidth bottlenecks while supporting the generation of quick overview visualizations.","We present a case study on the Bay of Bengal (BoB) to demonstrate the utility of the system and scaling studies to evaluate the efficiency of the system."],"url":"http://arxiv.org/abs/2501.05009v1"}
{"created":"2025-01-09 06:59:08","title":"CHASE: A Native Relational Database for Hybrid Queries on Structured and Unstructured Data","abstract":"Querying both structured and unstructured data has become a new paradigm in data analytics and recommendation. With unstructured data, such as text and videos, are converted to high-dimensional vectors and queried with approximate nearest neighbor search (ANNS). State-of-the-art database systems implement vector search as a plugin in the relational query engine, which tries to utilize the ANN index to enhance performance. After investigating a broad range of hybrid queries, we find that such designs may miss potential optimization opportunities and achieve suboptimal performance for certain queries. In this paper, we propose CHASE, a query engine that is natively designed to support efficient hybrid queries on structured and unstructured data. CHASE performs specific designs and optimizations on multiple stages in query processing. First, semantic analysis is performed to categorize queries and optimize query plans dynamically. Second, new physical operators are implemented to avoid redundant computations, which is the case with existing operators. Third, compilation-based techniques are adopted for efficient machine code generation. Extensive evaluations using real-world datasets demonstrate that CHASE achieves substantial performance improvements, with speedups ranging from 13% to an extraordinary 7500 times compared to existing systems. These results highlight CHASE's potential as a robust solution for executing hybrid queries.","sentences":["Querying both structured and unstructured data has become a new paradigm in data analytics and recommendation.","With unstructured data, such as text and videos, are converted to high-dimensional vectors and queried with approximate nearest neighbor search (ANNS).","State-of-the-art database systems implement vector search as a plugin in the relational query engine, which tries to utilize the ANN index to enhance performance.","After investigating a broad range of hybrid queries, we find that such designs may miss potential optimization opportunities and achieve suboptimal performance for certain queries.","In this paper, we propose CHASE, a query engine that is natively designed to support efficient hybrid queries on structured and unstructured data.","CHASE performs specific designs and optimizations on multiple stages in query processing.","First, semantic analysis is performed to categorize queries and optimize query plans dynamically.","Second, new physical operators are implemented to avoid redundant computations, which is the case with existing operators.","Third, compilation-based techniques are adopted for efficient machine code generation.","Extensive evaluations using real-world datasets demonstrate that CHASE achieves substantial performance improvements, with speedups ranging from 13% to an extraordinary 7500 times compared to existing systems.","These results highlight CHASE's potential as a robust solution for executing hybrid queries."],"url":"http://arxiv.org/abs/2501.05006v1"}
{"created":"2025-01-09 06:29:50","title":"Load Forecasting for Households and Energy Communities: Are Deep Learning Models Worth the Effort?","abstract":"Accurate load forecasting is crucial for predictive control in many energy domain applications, with significant economic and ecological implications. To address these implications, this study provides an extensive benchmark of state-of-the-art deep learning models for short-term load forecasting in energy communities. Namely, LSTM, xLSTM, and Transformers are compared with benchmarks such as KNNs, synthetic load models, and persistence forecasting models. This comparison considers different scales of aggregation (e.g., number of household loads) and varying training data availability (e.g., training data time spans). Further, the impact of transfer learning from synthetic (standard) load profiles and the deep learning model size (i.e., parameter count) is investigated in terms of forecasting error. Implementations are publicly available and other researchers are encouraged to benchmark models using this framework. Additionally, a comprehensive case study, comprising an energy community of 50 households and a battery storage demonstrates the beneficial financial implications of accurate predictions. Key findings of this research include: (1) Simple persistence benchmarks outperform deep learning models for short-term load forecasting when the available training data is limited to six months or less; (2) Pretraining with publicly available synthetic load profiles improves the normalized Mean Absolute Error (nMAE) by an average of 1.28%pt during the first nine months of training data; (3) Increased aggregation significantly enhances the performance of deep learning models relative to persistence benchmarks; (4) Improved load forecasting, with an nMAE reduction of 1.1%pt, translates to an economic benefit of approximately 600EUR per year in an energy community comprising 50 households.","sentences":["Accurate load forecasting is crucial for predictive control in many energy domain applications, with significant economic and ecological implications.","To address these implications, this study provides an extensive benchmark of state-of-the-art deep learning models for short-term load forecasting in energy communities.","Namely, LSTM, xLSTM, and Transformers are compared with benchmarks such as KNNs, synthetic load models, and persistence forecasting models.","This comparison considers different scales of aggregation (e.g., number of household loads) and varying training data availability (e.g., training data time spans).","Further, the impact of transfer learning from synthetic (standard) load profiles and the deep learning model size (i.e., parameter count) is investigated in terms of forecasting error.","Implementations are publicly available and other researchers are encouraged to benchmark models using this framework.","Additionally, a comprehensive case study, comprising an energy community of 50 households and a battery storage demonstrates the beneficial financial implications of accurate predictions.","Key findings of this research include: (1) Simple persistence benchmarks outperform deep learning models for short-term load forecasting when the available training data is limited to six months or less; (2) Pretraining with publicly available synthetic load profiles improves the normalized Mean Absolute Error (nMAE) by an average of 1.28%pt during the first nine months of training data; (3) Increased aggregation significantly enhances the performance of deep learning models relative to persistence benchmarks; (4) Improved load forecasting, with an nMAE reduction of 1.1%pt, translates to an economic benefit of approximately 600EUR per year in an energy community comprising 50 households."],"url":"http://arxiv.org/abs/2501.05000v1"}
{"created":"2025-01-09 06:26:28","title":"GiNet: Integrating Sequential and Context-Aware Learning for Battery Capacity Prediction","abstract":"The surging demand for batteries requires advanced battery management systems, where battery capacity modelling is a key functionality. In this paper, we aim to achieve accurate battery capacity prediction by learning from historical measurements of battery dynamics. We propose GiNet, a gated recurrent units enhanced Informer network, for predicting battery's capacity. The novelty and competitiveness of GiNet lies in its capability of capturing sequential and contextual information from raw battery data and reflecting the battery's complex behaviors with both temporal dynamics and long-term dependencies. We conducted an experimental study based on a publicly available dataset to showcase GiNet's strength of gaining a holistic understanding of battery behavior and predicting battery capacity accurately. GiNet achieves 0.11 mean absolute error for predicting the battery capacity in a sequence of future time slots without knowing the historical battery capacity. It also outperforms the latest algorithms significantly with 27% error reduction on average compared to Informer. The promising results highlight the importance of customized and optimized integration of algorithm and battery knowledge and shed light on other industry applications as well.","sentences":["The surging demand for batteries requires advanced battery management systems, where battery capacity modelling is a key functionality.","In this paper, we aim to achieve accurate battery capacity prediction by learning from historical measurements of battery dynamics.","We propose GiNet, a gated recurrent units enhanced Informer network, for predicting battery's capacity.","The novelty and competitiveness of GiNet lies in its capability of capturing sequential and contextual information from raw battery data and reflecting the battery's complex behaviors with both temporal dynamics and long-term dependencies.","We conducted an experimental study based on a publicly available dataset to showcase GiNet's strength of gaining a holistic understanding of battery behavior and predicting battery capacity accurately.","GiNet achieves 0.11 mean absolute error for predicting the battery capacity in a sequence of future time slots without knowing the historical battery capacity.","It also outperforms the latest algorithms significantly with 27% error reduction on average compared to Informer.","The promising results highlight the importance of customized and optimized integration of algorithm and battery knowledge and shed light on other industry applications as well."],"url":"http://arxiv.org/abs/2501.04997v1"}
{"created":"2025-01-09 06:18:39","title":"ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State Drives","abstract":"Unlike non-volatile memory that resides on the processor memory bus, memory-semantic solid-state drives (SSDs) support both byte and block access granularity via PCIe or CXL interconnects. They provide scalable memory capacity using NAND flash at a much lower cost. In addition, they have different performance characteristics for their dual byte/block interface respectively, while offering essential memory semantics for upper-level software. Such a byte-accessible storage device provides new implications on the software system design.   In this paper, we develop a new file system, named ByteFS, by rethinking the design primitives of file systems and SSD firmware to exploit the advantages of both byte and block-granular data accesses. ByteFS supports byte-granular data persistence to retain the persistence nature of SSDs. It extends the core data structure of file systems by enabling dual byte/block-granular data accesses. To facilitate the support for byte-granular writes, \\pname{} manages the internal DRAM of SSD firmware in a log-structured manner and enables data coalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also enables coordinated data caching between the host page cache and SSD cache for best utilizing the precious memory resource. We implement ByteFS on both a real programmable SSD and an emulated memory-semantic SSD for sensitivity study. Compared to state-of-the-art file systems for non-volatile memory and conventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while preserving the essential properties of a file system. ByteFS also reduces the write traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes caused by both metadata and data updates in file systems.","sentences":["Unlike non-volatile memory that resides on the processor memory bus, memory-semantic solid-state drives (SSDs) support both byte and block access granularity via PCIe or CXL interconnects.","They provide scalable memory capacity using NAND flash at a much lower cost.","In addition, they have different performance characteristics for their dual byte/block interface respectively, while offering essential memory semantics for upper-level software.","Such a byte-accessible storage device provides new implications on the software system design.   ","In this paper, we develop a new file system, named ByteFS, by rethinking the design primitives of file systems and SSD firmware to exploit the advantages of both byte and block-granular data accesses.","ByteFS supports byte-granular data persistence to retain the persistence nature of SSDs.","It extends the core data structure of file systems by enabling dual byte/block-granular data accesses.","To facilitate the support for byte-granular writes, \\pname{} manages the internal DRAM of SSD firmware in a log-structured manner and enables data coalescing to reduce the unnecessary I/O traffic to flash chips.","ByteFS also enables coordinated data caching between the host page cache and SSD cache for best utilizing the precious memory resource.","We implement ByteFS on both a real programmable SSD and an emulated memory-semantic SSD for sensitivity study.","Compared to state-of-the-art file systems for non-volatile memory and conventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while preserving the essential properties of a file system.","ByteFS also reduces the write traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes caused by both metadata and data updates in file systems."],"url":"http://arxiv.org/abs/2501.04993v1"}
{"created":"2025-01-09 06:01:34","title":"Intelligent Sailing Model for Open Sea Navigation","abstract":"Autonomous vessels potentially enhance safety and reliability of seaborne trade. To facilitate the development of autonomous vessels, high-fidelity simulations are required to model realistic interactions with other vessels. However, modeling realistic interactive maritime traffic is challenging due to the unstructured environment, coarsely specified traffic rules, and largely varying vessel types. Currently, there is no standard for simulating interactive maritime environments in order to rigorously benchmark autonomous vessel algorithms. In this paper, we introduce the first intelligent sailing model (ISM), which simulates rule-compliant vessels for navigation on the open sea. An ISM vessel reacts to other traffic participants according to maritime traffic rules while at the same time solving a motion planning task characterized by waypoints. In particular, the ISM monitors the applicable rules, generates rule-compliant waypoints accordingly, and utilizes a model predictive control for tracking the waypoints. We evaluate the ISM in two environments: interactive traffic with only ISM vessels and mixed traffic where some vessel trajectories are from recorded real-world maritime traffic data or handcrafted for criticality. Our results show that simulations with many ISM vessels of different vessel types are rule-compliant and scalable. We tested 4,049 critical traffic scenarios. For interactive traffic with ISM vessels, no collisions occurred while goal-reaching rates of about 97 percent were achieved. We believe that our ISM can serve as a standard for challenging and realistic maritime traffic simulation to accelerate autonomous vessel development.","sentences":["Autonomous vessels potentially enhance safety and reliability of seaborne trade.","To facilitate the development of autonomous vessels, high-fidelity simulations are required to model realistic interactions with other vessels.","However, modeling realistic interactive maritime traffic is challenging due to the unstructured environment, coarsely specified traffic rules, and largely varying vessel types.","Currently, there is no standard for simulating interactive maritime environments in order to rigorously benchmark autonomous vessel algorithms.","In this paper, we introduce the first intelligent sailing model (ISM), which simulates rule-compliant vessels for navigation on the open sea.","An ISM vessel reacts to other traffic participants according to maritime traffic rules while at the same time solving a motion planning task characterized by waypoints.","In particular, the ISM monitors the applicable rules, generates rule-compliant waypoints accordingly, and utilizes a model predictive control for tracking the waypoints.","We evaluate the ISM in two environments: interactive traffic with only ISM vessels and mixed traffic where some vessel trajectories are from recorded real-world maritime traffic data or handcrafted for criticality.","Our results show that simulations with many ISM vessels of different vessel types are rule-compliant and scalable.","We tested 4,049 critical traffic scenarios.","For interactive traffic with ISM vessels, no collisions occurred while goal-reaching rates of about 97 percent were achieved.","We believe that our ISM can serve as a standard for challenging and realistic maritime traffic simulation to accelerate autonomous vessel development."],"url":"http://arxiv.org/abs/2501.04988v1"}
{"created":"2025-01-09 05:45:03","title":"CuRLA: Curriculum Learning Based Deep Reinforcement Learning for Autonomous Driving","abstract":"In autonomous driving, traditional Computer Vision (CV) agents often struggle in unfamiliar situations due to biases in the training data. Deep Reinforcement Learning (DRL) agents address this by learning from experience and maximizing rewards, which helps them adapt to dynamic environments. However, ensuring their generalization remains challenging, especially with static training environments. Additionally, DRL models lack transparency, making it difficult to guarantee safety in all scenarios, particularly those not seen during training. To tackle these issues, we propose a method that combines DRL with Curriculum Learning for autonomous driving. Our approach uses a Proximal Policy Optimization (PPO) agent and a Variational Autoencoder (VAE) to learn safe driving in the CARLA simulator. The agent is trained using two-fold curriculum learning, progressively increasing environment difficulty and incorporating a collision penalty in the reward function to promote safety. This method improves the agent's adaptability and reliability in complex environments, and understand the nuances of balancing multiple reward components from different feedback signals in a single scalar reward function. Keywords: Computer Vision, Deep Reinforcement Learning, Variational Autoencoder, Proximal Policy Optimization, Curriculum Learning, Autonomous Driving.","sentences":["In autonomous driving, traditional Computer Vision (CV) agents often struggle in unfamiliar situations due to biases in the training data.","Deep Reinforcement Learning (DRL) agents address this by learning from experience and maximizing rewards, which helps them adapt to dynamic environments.","However, ensuring their generalization remains challenging, especially with static training environments.","Additionally, DRL models lack transparency, making it difficult to guarantee safety in all scenarios, particularly those not seen during training.","To tackle these issues, we propose a method that combines DRL with Curriculum Learning for autonomous driving.","Our approach uses a Proximal Policy Optimization (PPO) agent and a Variational Autoencoder (VAE) to learn safe driving in the CARLA simulator.","The agent is trained using two-fold curriculum learning, progressively increasing environment difficulty and incorporating a collision penalty in the reward function to promote safety.","This method improves the agent's adaptability and reliability in complex environments, and understand the nuances of balancing multiple reward components from different feedback signals in a single scalar reward function.","Keywords: Computer Vision, Deep Reinforcement Learning, Variational Autoencoder, Proximal Policy Optimization, Curriculum Learning, Autonomous Driving."],"url":"http://arxiv.org/abs/2501.04982v1"}
{"created":"2025-01-09 05:06:44","title":"SensorQA: A Question Answering Benchmark for Daily-Life Monitoring","abstract":"With the rapid growth in sensor data, effectively interpreting and interfacing with these data in a human-understandable way has become crucial. While existing research primarily focuses on learning classification models, fewer studies have explored how end users can actively extract useful insights from sensor data, often hindered by the lack of a proper dataset. To address this gap, we introduce \\Dataset, the first human-created question-answering (QA) dataset for long-term time-series sensor data for daily life monitoring. \\Dataset is created by human workers and includes 5.6K diverse and practical queries that reflect genuine human interests, paired with accurate answers derived from sensor data. We further establish benchmarks for state-of-the-art AI models on this dataset and evaluate their performance on typical edge devices. Our results reveal a gap between current models and optimal QA performance and efficiency, highlighting the need for new contributions. The dataset and code are available at: \\url{https://github.com/benjamin-reichman/SensorQA}.","sentences":["With the rapid growth in sensor data, effectively interpreting and interfacing with these data in a human-understandable way has become crucial.","While existing research primarily focuses on learning classification models, fewer studies have explored how end users can actively extract useful insights from sensor data, often hindered by the lack of a proper dataset.","To address this gap, we introduce \\Dataset, the first human-created question-answering (QA) dataset for long-term time-series sensor data for daily life monitoring.","\\Dataset is created by human workers and includes 5.6K diverse and practical queries that reflect genuine human interests, paired with accurate answers derived from sensor data.","We further establish benchmarks for state-of-the-art AI models on this dataset and evaluate their performance on typical edge devices.","Our results reveal a gap between current models and optimal QA performance and efficiency, highlighting the need for new contributions.","The dataset and code are available at: \\url{https://github.com/benjamin-reichman/SensorQA}."],"url":"http://arxiv.org/abs/2501.04974v1"}
{"created":"2025-01-09 04:47:51","title":"AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture for Autonomous Driving with LiDAR Data","abstract":"As opposed to human drivers, current autonomous driving systems still require vast amounts of labeled data to train. Recently, world models have been proposed to simultaneously enhance autonomous driving capabilities by improving the way these systems understand complex real-world environments and reduce their data demands via self-supervised pre-training. In this paper, we present AD-L-JEPA (aka Autonomous Driving with LiDAR data via a Joint Embedding Predictive Architecture), a novel self-supervised pre-training framework for autonomous driving with LiDAR data that, as opposed to existing methods, is neither generative nor contrastive. Our method learns spatial world models with a joint embedding predictive architecture. Instead of explicitly generating masked unknown regions, our self-supervised world models predict Bird's Eye View (BEV) embeddings to represent the diverse nature of autonomous driving scenes. Our approach furthermore eliminates the need to manually create positive and negative pairs, as is the case in contrastive learning. AD-L-JEPA leads to simpler implementation and enhanced learned representations. We qualitatively and quantitatively demonstrate high-quality of embeddings learned with AD-L-JEPA. We furthermore evaluate the accuracy and label efficiency of AD-L-JEPA on popular downstream tasks such as LiDAR 3D object detection and associated transfer learning. Our experimental evaluation demonstrates that AD-L-JEPA is a plausible approach for self-supervised pre-training in autonomous driving applications and is the best available approach outperforming SOTA, including most recently proposed Occupancy-MAE [1] and ALSO [2]. The source code of AD-L-JEPA is available at https://github.com/HaoranZhuExplorer/AD-L-JEPA-Release.","sentences":["As opposed to human drivers, current autonomous driving systems still require vast amounts of labeled data to train.","Recently, world models have been proposed to simultaneously enhance autonomous driving capabilities by improving the way these systems understand complex real-world environments and reduce their data demands via self-supervised pre-training.","In this paper, we present AD-L-JEPA (aka Autonomous Driving with LiDAR data via a Joint Embedding Predictive Architecture), a novel self-supervised pre-training framework for autonomous driving with LiDAR data that, as opposed to existing methods, is neither generative nor contrastive.","Our method learns spatial world models with a joint embedding predictive architecture.","Instead of explicitly generating masked unknown regions, our self-supervised world models predict Bird's Eye View (BEV) embeddings to represent the diverse nature of autonomous driving scenes.","Our approach furthermore eliminates the need to manually create positive and negative pairs, as is the case in contrastive learning.","AD-L-JEPA leads to simpler implementation and enhanced learned representations.","We qualitatively and quantitatively demonstrate high-quality of embeddings learned with AD-L-JEPA.","We furthermore evaluate the accuracy and label efficiency of AD-L-JEPA on popular downstream tasks such as LiDAR 3D object detection and associated transfer learning.","Our experimental evaluation demonstrates that AD-L-JEPA is a plausible approach for self-supervised pre-training in autonomous driving applications and is the best available approach outperforming SOTA, including most recently proposed Occupancy-MAE","[1] and ALSO","[2].","The source code of AD-L-JEPA is available at https://github.com/HaoranZhuExplorer/AD-L-JEPA-Release."],"url":"http://arxiv.org/abs/2501.04969v1"}
{"created":"2025-01-09 04:41:50","title":"Targeted Adversarial Denoising Autoencoders (TADA) for Neural Time Series Filtration","abstract":"Current machine learning (ML)-based algorithms for filtering electroencephalography (EEG) time series data face challenges related to cumbersome training times, regularization, and accurate reconstruction. To address these shortcomings, we present an ML filtration algorithm driven by a logistic covariance-targeted adversarial denoising autoencoder (TADA). We hypothesize that the expressivity of a targeted, correlation-driven convolutional autoencoder will enable effective time series filtration while minimizing compute requirements (e.g., runtime, model size). Furthermore, we expect that adversarial training with covariance rescaling will minimize signal degradation. To test this hypothesis, a TADA system prototype was trained and evaluated on the task of removing electromyographic (EMG) noise from EEG data in the EEGdenoiseNet dataset, which includes EMG and EEG data from 67 subjects. The TADA filter surpasses conventional signal filtration algorithms across quantitative metrics (Correlation Coefficient, Temporal RRMSE, Spectral RRMSE), and performs competitively against other deep learning architectures at a reduced model size of less than 400,000 trainable parameters. Further experimentation will be necessary to assess the viability of TADA on a wider range of deployment cases.","sentences":["Current machine learning (ML)-based algorithms for filtering electroencephalography (EEG) time series data face challenges related to cumbersome training times, regularization, and accurate reconstruction.","To address these shortcomings, we present an ML filtration algorithm driven by a logistic covariance-targeted adversarial denoising autoencoder (TADA).","We hypothesize that the expressivity of a targeted, correlation-driven convolutional autoencoder will enable effective time series filtration while minimizing compute requirements (e.g., runtime, model size).","Furthermore, we expect that adversarial training with covariance rescaling will minimize signal degradation.","To test this hypothesis, a TADA system prototype was trained and evaluated on the task of removing electromyographic (EMG) noise from EEG data in the EEGdenoiseNet dataset, which includes EMG and EEG data from 67 subjects.","The TADA filter surpasses conventional signal filtration algorithms across quantitative metrics (Correlation Coefficient, Temporal RRMSE, Spectral RRMSE), and performs competitively against other deep learning architectures at a reduced model size of less than 400,000 trainable parameters.","Further experimentation will be necessary to assess the viability of TADA on a wider range of deployment cases."],"url":"http://arxiv.org/abs/2501.04967v1"}
{"created":"2025-01-09 04:26:15","title":"Demystifying Domain-adaptive Post-training for Financial LLMs","abstract":"Domain-adaptive post-training of large language models (LLMs) has emerged as a promising approach for specialized domains such as medicine and finance. However, significant challenges remain in identifying optimal adaptation criteria and training strategies across varying data and model configurations. To address these challenges, we introduce FINDAP, a systematic and fine-grained investigation into domain-adaptive post-training of LLMs for the finance domain. Our approach begins by identifying the core capabilities required for the target domain and designing a comprehensive evaluation suite aligned with these needs. We then analyze the effectiveness of key post-training stages, including continual pretraining, instruction tuning, and preference alignment. Building on these insights, we propose an effective training recipe centered on a novel preference data distillation method, which leverages process signals from a generative reward model. The resulting model, Llama-Fin, achieves state-of-the-art performance across a wide range of financial tasks. Our analysis also highlights how each post-training stage contributes to distinct capabilities, uncovering specific challenges and effective solutions, providing valuable insights for domain adaptation of LLMs. Project page: https://github.com/SalesforceAIResearch/FinDap","sentences":["Domain-adaptive post-training of large language models (LLMs) has emerged as a promising approach for specialized domains such as medicine and finance.","However, significant challenges remain in identifying optimal adaptation criteria and training strategies across varying data and model configurations.","To address these challenges, we introduce FINDAP, a systematic and fine-grained investigation into domain-adaptive post-training of LLMs for the finance domain.","Our approach begins by identifying the core capabilities required for the target domain and designing a comprehensive evaluation suite aligned with these needs.","We then analyze the effectiveness of key post-training stages, including continual pretraining, instruction tuning, and preference alignment.","Building on these insights, we propose an effective training recipe centered on a novel preference data distillation method, which leverages process signals from a generative reward model.","The resulting model, Llama-Fin, achieves state-of-the-art performance across a wide range of financial tasks.","Our analysis also highlights how each post-training stage contributes to distinct capabilities, uncovering specific challenges and effective solutions, providing valuable insights for domain adaptation of LLMs.","Project page: https://github.com/SalesforceAIResearch/FinDap"],"url":"http://arxiv.org/abs/2501.04961v1"}
{"created":"2025-01-09 03:59:10","title":"Open Problems in Machine Unlearning for AI Safety","abstract":"As AI systems become more capable, widely deployed, and increasingly autonomous in critical areas such as cybersecurity, biological research, and healthcare, ensuring their safety and alignment with human values is paramount. Machine unlearning -- the ability to selectively forget or suppress specific types of knowledge -- has shown promise for privacy and data removal tasks, which has been the primary focus of existing research. More recently, its potential application to AI safety has gained attention. In this paper, we identify key limitations that prevent unlearning from serving as a comprehensive solution for AI safety, particularly in managing dual-use knowledge in sensitive domains like cybersecurity and chemical, biological, radiological, and nuclear (CBRN) safety. In these contexts, information can be both beneficial and harmful, and models may combine seemingly harmless information for harmful purposes -- unlearning this information could strongly affect beneficial uses. We provide an overview of inherent constraints and open problems, including the broader side effects of unlearning dangerous knowledge, as well as previously unexplored tensions between unlearning and existing safety mechanisms. Finally, we investigate challenges related to evaluation, robustness, and the preservation of safety features during unlearning. By mapping these limitations and open challenges, we aim to guide future research toward realistic applications of unlearning within a broader AI safety framework, acknowledging its limitations and highlighting areas where alternative approaches may be required.","sentences":["As AI systems become more capable, widely deployed, and increasingly autonomous in critical areas such as cybersecurity, biological research, and healthcare, ensuring their safety and alignment with human values is paramount.","Machine unlearning -- the ability to selectively forget or suppress specific types of knowledge -- has shown promise for privacy and data removal tasks, which has been the primary focus of existing research.","More recently, its potential application to AI safety has gained attention.","In this paper, we identify key limitations that prevent unlearning from serving as a comprehensive solution for AI safety, particularly in managing dual-use knowledge in sensitive domains like cybersecurity and chemical, biological, radiological, and nuclear (CBRN) safety.","In these contexts, information can be both beneficial and harmful, and models may combine seemingly harmless information for harmful purposes -- unlearning this information could strongly affect beneficial uses.","We provide an overview of inherent constraints and open problems, including the broader side effects of unlearning dangerous knowledge, as well as previously unexplored tensions between unlearning and existing safety mechanisms.","Finally, we investigate challenges related to evaluation, robustness, and the preservation of safety features during unlearning.","By mapping these limitations and open challenges, we aim to guide future research toward realistic applications of unlearning within a broader AI safety framework, acknowledging its limitations and highlighting areas where alternative approaches may be required."],"url":"http://arxiv.org/abs/2501.04952v1"}
{"created":"2025-01-09 03:58:02","title":"MORDA: A Synthetic Dataset to Facilitate Adaptation of Object Detectors to Unseen Real-target Domain While Preserving Performance on Real-source Domain","abstract":"Deep neural network (DNN) based perception models are indispensable in the development of autonomous vehicles (AVs). However, their reliance on large-scale, high-quality data is broadly recognized as a burdensome necessity due to the substantial cost of data acquisition and labeling. Further, the issue is not a one-time concern, as AVs might need a new dataset if they are to be deployed to another region (real-target domain) that the in-hand dataset within the real-source domain cannot incorporate. To mitigate this burden, we propose leveraging synthetic environments as an auxiliary domain where the characteristics of real domains are reproduced. This approach could enable indirect experience about the real-target domain in a time- and cost-effective manner. As a practical demonstration of our methodology, nuScenes and South Korea are employed to represent real-source and real-target domains, respectively. That means we construct digital twins for several regions of South Korea, and the data-acquisition framework of nuScenes is reproduced. Blending the aforementioned components within a simulator allows us to obtain a synthetic-fusion domain in which we forge our novel driving dataset, MORDA: Mixture Of Real-domain characteristics for synthetic-data-assisted Domain Adaptation. To verify the value of synthetic features that MORDA provides in learning about driving environments of South Korea, 2D/3D detectors are trained solely on a combination of nuScenes and MORDA. Afterward, their performance is evaluated on the unforeseen real-world dataset (AI-Hub) collected in South Korea. Our experiments present that MORDA can significantly improve mean Average Precision (mAP) on AI-Hub dataset while that on nuScenes is retained or slightly enhanced.","sentences":["Deep neural network (DNN) based perception models are indispensable in the development of autonomous vehicles (AVs).","However, their reliance on large-scale, high-quality data is broadly recognized as a burdensome necessity due to the substantial cost of data acquisition and labeling.","Further, the issue is not a one-time concern, as AVs might need a new dataset if they are to be deployed to another region (real-target domain) that the in-hand dataset within the real-source domain cannot incorporate.","To mitigate this burden, we propose leveraging synthetic environments as an auxiliary domain where the characteristics of real domains are reproduced.","This approach could enable indirect experience about the real-target domain in a time- and cost-effective manner.","As a practical demonstration of our methodology, nuScenes and South Korea are employed to represent real-source and real-target domains, respectively.","That means we construct digital twins for several regions of South Korea, and the data-acquisition framework of nuScenes is reproduced.","Blending the aforementioned components within a simulator allows us to obtain a synthetic-fusion domain in which we forge our novel driving dataset, MORDA: Mixture Of Real-domain characteristics for synthetic-data-assisted Domain Adaptation.","To verify the value of synthetic features that MORDA provides in learning about driving environments of South Korea, 2D/3D detectors are trained solely on a combination of nuScenes and MORDA.","Afterward, their performance is evaluated on the unforeseen real-world dataset (AI-Hub) collected in South Korea.","Our experiments present that MORDA can significantly improve mean Average Precision (mAP) on AI-Hub dataset while that on nuScenes is retained or slightly enhanced."],"url":"http://arxiv.org/abs/2501.04950v1"}
{"created":"2025-01-09 03:34:07","title":"Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of Large Language Models","abstract":"It is crucial for large language models (LLMs) to follow instructions that involve multiple constraints. However, soft constraints are semantically related and difficult to verify through automated methods. These constraints remain a significant challenge for LLMs. To enhance the ability of LLMs to follow soft constraints, we initially design a pipeline to obtain high-quality outputs automatically. Additionally, to fully utilize the acquired data, we introduce a training paradigm based on curriculum learning. We experimentally evaluate the effectiveness of our methods in improving LLMs' soft constraint following ability and analyze the factors driving the improvements. The datasets and code are publicly available at https://github.com/Rainier-rq/FollowSoftConstraints.","sentences":["It is crucial for large language models (LLMs) to follow instructions that involve multiple constraints.","However, soft constraints are semantically related and difficult to verify through automated methods.","These constraints remain a significant challenge for LLMs.","To enhance the ability of LLMs to follow soft constraints, we initially design a pipeline to obtain high-quality outputs automatically.","Additionally, to fully utilize the acquired data, we introduce a training paradigm based on curriculum learning.","We experimentally evaluate the effectiveness of our methods in improving LLMs' soft constraint following ability and analyze the factors driving the improvements.","The datasets and code are publicly available at https://github.com/Rainier-rq/FollowSoftConstraints."],"url":"http://arxiv.org/abs/2501.04945v1"}
{"created":"2025-01-09 03:18:27","title":"Vision Graph Non-Contrastive Learning for Audio Deepfake Detection with Limited Labels","abstract":"Recent advancements in audio deepfake detection have leveraged graph neural networks (GNNs) to model frequency and temporal interdependencies in audio data, effectively identifying deepfake artifacts. However, the reliance of GNN-based methods on substantial labeled data for graph construction and robust performance limits their applicability in scenarios with limited labeled data. Although vast amounts of audio data exist, the process of labeling samples as genuine or fake remains labor-intensive and costly. To address this challenge, we propose SIGNL (Spatio-temporal vIsion Graph Non-contrastive Learning), a novel framework that maintains high GNN performance in low-label settings. SIGNL constructs spatio-temporal graphs by representing patches from the audio's visual spectrogram as nodes. These graph structures are modeled using vision graph convolutional (GC) encoders pre-trained through graph non-contrastive learning, a label-free that maximizes the similarity between positive pairs. The pre-trained encoders are then fine-tuned for audio deepfake detection, reducing reliance on labeled data. Experiments demonstrate that SIGNL outperforms state-of-the-art baselines across multiple audio deepfake detection datasets, achieving the lowest Equal Error Rate (EER) with as little as 5% labeled data. Additionally, SIGNL exhibits strong cross-domain generalization, achieving the lowest EER in evaluations involving diverse attack types and languages in the In-The-Wild dataset.","sentences":["Recent advancements in audio deepfake detection have leveraged graph neural networks (GNNs) to model frequency and temporal interdependencies in audio data, effectively identifying deepfake artifacts.","However, the reliance of GNN-based methods on substantial labeled data for graph construction and robust performance limits their applicability in scenarios with limited labeled data.","Although vast amounts of audio data exist, the process of labeling samples as genuine or fake remains labor-intensive and costly.","To address this challenge, we propose SIGNL (Spatio-temporal vIsion Graph Non-contrastive Learning), a novel framework that maintains high GNN performance in low-label settings.","SIGNL constructs spatio-temporal graphs by representing patches from the audio's visual spectrogram as nodes.","These graph structures are modeled using vision graph convolutional (GC) encoders pre-trained through graph non-contrastive learning, a label-free that maximizes the similarity between positive pairs.","The pre-trained encoders are then fine-tuned for audio deepfake detection, reducing reliance on labeled data.","Experiments demonstrate that SIGNL outperforms state-of-the-art baselines across multiple audio deepfake detection datasets, achieving the lowest Equal Error Rate (EER) with as little as 5% labeled data.","Additionally, SIGNL exhibits strong cross-domain generalization, achieving the lowest EER in evaluations involving diverse attack types and languages in the In-The-Wild dataset."],"url":"http://arxiv.org/abs/2501.04942v1"}
{"created":"2025-01-09 03:14:03","title":"A New Perspective on Privacy Protection in Federated Learning with Granular-Ball Computing","abstract":"Federated Learning (FL) facilitates collaborative model training while prioritizing privacy by avoiding direct data sharing. However, most existing articles attempt to address challenges within the model's internal parameters and corresponding outputs, while neglecting to solve them at the input level. To address this gap, we propose a novel framework called Granular-Ball Federated Learning (GrBFL) for image classification. GrBFL diverges from traditional methods that rely on the finest-grained input data. Instead, it segments images into multiple regions with optimal coarse granularity, which are then reconstructed into a graph structure. We designed a two-dimensional binary search segmentation algorithm based on variance constraints for GrBFL, which effectively removes redundant information while preserving key representative features. Extensive theoretical analysis and experiments demonstrate that GrBFL not only safeguards privacy and enhances efficiency but also maintains robust utility, consistently outperforming other state-of-the-art FL methods. The code is available at https://github.com/AIGNLAI/GrBFL.","sentences":["Federated Learning (FL) facilitates collaborative model training while prioritizing privacy by avoiding direct data sharing.","However, most existing articles attempt to address challenges within the model's internal parameters and corresponding outputs, while neglecting to solve them at the input level.","To address this gap, we propose a novel framework called Granular-Ball Federated Learning (GrBFL) for image classification.","GrBFL diverges from traditional methods that rely on the finest-grained input data.","Instead, it segments images into multiple regions with optimal coarse granularity, which are then reconstructed into a graph structure.","We designed a two-dimensional binary search segmentation algorithm based on variance constraints for GrBFL, which effectively removes redundant information while preserving key representative features.","Extensive theoretical analysis and experiments demonstrate that GrBFL not only safeguards privacy and enhances efficiency but also maintains robust utility, consistently outperforming other state-of-the-art FL methods.","The code is available at https://github.com/AIGNLAI/GrBFL."],"url":"http://arxiv.org/abs/2501.04940v1"}
{"created":"2025-01-09 02:36:21","title":"Image2CADSeq: Computer-Aided Design Sequence and Knowledge Inference from Product Images","abstract":"Computer-aided design (CAD) tools empower designers to design and modify 3D models through a series of CAD operations, commonly referred to as a CAD sequence. In scenarios where digital CAD files are not accessible, reverse engineering (RE) has been used to reconstruct 3D CAD models. Recent advances have seen the rise of data-driven approaches for RE, with a primary focus on converting 3D data, such as point clouds, into 3D models in boundary representation (B-rep) format. However, obtaining 3D data poses significant challenges, and B-rep models do not reveal knowledge about the 3D modeling process of designs. To this end, our research introduces a novel data-driven approach with an Image2CADSeq neural network model. This model aims to reverse engineer CAD models by processing images as input and generating CAD sequences. These sequences can then be translated into B-rep models using a solid modeling kernel. Unlike B-rep models, CAD sequences offer enhanced flexibility to modify individual steps of model creation, providing a deeper understanding of the construction process of CAD models. To quantitatively and rigorously evaluate the predictive performance of the Image2CADSeq model, we have developed a multi-level evaluation framework for model assessment. The model was trained on a specially synthesized dataset, and various network architectures were explored to optimize the performance. The experimental and validation results show great potential for the model in generating CAD sequences from 2D image data.","sentences":["Computer-aided design (CAD) tools empower designers to design and modify 3D models through a series of CAD operations, commonly referred to as a CAD sequence.","In scenarios where digital CAD files are not accessible, reverse engineering (RE) has been used to reconstruct 3D CAD models.","Recent advances have seen the rise of data-driven approaches for RE, with a primary focus on converting 3D data, such as point clouds, into 3D models in boundary representation (B-rep) format.","However, obtaining 3D data poses significant challenges, and B-rep models do not reveal knowledge about the 3D modeling process of designs.","To this end, our research introduces a novel data-driven approach with an Image2CADSeq neural network model.","This model aims to reverse engineer CAD models by processing images as input and generating CAD sequences.","These sequences can then be translated into B-rep models using a solid modeling kernel.","Unlike B-rep models, CAD sequences offer enhanced flexibility to modify individual steps of model creation, providing a deeper understanding of the construction process of CAD models.","To quantitatively and rigorously evaluate the predictive performance of the Image2CADSeq model, we have developed a multi-level evaluation framework for model assessment.","The model was trained on a specially synthesized dataset, and various network architectures were explored to optimize the performance.","The experimental and validation results show great potential for the model in generating CAD sequences from 2D image data."],"url":"http://arxiv.org/abs/2501.04928v1"}
{"created":"2025-01-09 02:32:40","title":"Investigating Numerical Translation with Large Language Models","abstract":"The inaccurate translation of numbers can lead to significant security issues, ranging from financial setbacks to medical inaccuracies. While large language models (LLMs) have made significant advancements in machine translation, their capacity for translating numbers has not been thoroughly explored. This study focuses on evaluating the reliability of LLM-based machine translation systems when handling numerical data. In order to systematically test the numerical translation capabilities of currently open source LLMs, we have constructed a numerical translation dataset between Chinese and English based on real business data, encompassing ten types of numerical translation. Experiments on the dataset indicate that errors in numerical translation are a common issue, with most open-source LLMs faltering when faced with our test scenarios. Especially when it comes to numerical types involving large units like ``million\", ``billion\", and \"yi\", even the latest llama3.1 8b model can have error rates as high as 20%. Finally, we introduce three potential strategies to mitigate the numerical mistranslations for large units.","sentences":["The inaccurate translation of numbers can lead to significant security issues, ranging from financial setbacks to medical inaccuracies.","While large language models (LLMs) have made significant advancements in machine translation, their capacity for translating numbers has not been thoroughly explored.","This study focuses on evaluating the reliability of LLM-based machine translation systems when handling numerical data.","In order to systematically test the numerical translation capabilities of currently open source LLMs, we have constructed a numerical translation dataset between Chinese and English based on real business data, encompassing ten types of numerical translation.","Experiments on the dataset indicate that errors in numerical translation are a common issue, with most open-source LLMs faltering when faced with our test scenarios.","Especially when it comes to numerical types involving large units like ``million\", ``billion\", and \"yi\", even the latest llama3.1 8b model can have error rates as high as 20%.","Finally, we introduce three potential strategies to mitigate the numerical mistranslations for large units."],"url":"http://arxiv.org/abs/2501.04927v1"}
{"created":"2025-01-09 02:14:12","title":"SpecTf: Transformers Enable Data-Driven Imaging Spectroscopy Cloud Detection","abstract":"Current and upcoming generations of visible-shortwave infrared (VSWIR) imaging spectrometers promise unprecedented capacity to quantify Earth System processes across the globe. However, reliable cloud screening remains a fundamental challenge for these instruments, where traditional spatial and temporal approaches are limited by cloud variability and limited temporal coverage. The Spectroscopic Transformer (SpecTf) addresses these challenges with a spectroscopy-specific deep learning architecture that performs cloud detection using only spectral information (no spatial or temporal data are required). By treating spectral measurements as sequences rather than image channels, SpecTf learns fundamental physical relationships without relying on spatial context. Our experiments demonstrate that SpecTf significantly outperforms the current baseline approach implemented for the EMIT instrument, and performs comparably with other machine learning methods with orders of magnitude fewer learned parameters. Critically, we demonstrate SpecTf's inherent interpretability through its attention mechanism, revealing physically meaningful spectral features the model has learned. Finally, we present SpecTf's potential for cross-instrument generalization by applying it to a different instrument on a different platform without modifications, opening the door to instrument agnostic data driven algorithms for future imaging spectroscopy tasks.","sentences":["Current and upcoming generations of visible-shortwave infrared (VSWIR) imaging spectrometers promise unprecedented capacity to quantify Earth System processes across the globe.","However, reliable cloud screening remains a fundamental challenge for these instruments, where traditional spatial and temporal approaches are limited by cloud variability and limited temporal coverage.","The Spectroscopic Transformer (SpecTf) addresses these challenges with a spectroscopy-specific deep learning architecture that performs cloud detection using only spectral information (no spatial or temporal data are required).","By treating spectral measurements as sequences rather than image channels, SpecTf learns fundamental physical relationships without relying on spatial context.","Our experiments demonstrate that SpecTf significantly outperforms the current baseline approach implemented for the EMIT instrument, and performs comparably with other machine learning methods with orders of magnitude fewer learned parameters.","Critically, we demonstrate SpecTf's inherent interpretability through its attention mechanism, revealing physically meaningful spectral features the model has learned.","Finally, we present SpecTf's potential for cross-instrument generalization by applying it to a different instrument on a different platform without modifications, opening the door to instrument agnostic data driven algorithms for future imaging spectroscopy tasks."],"url":"http://arxiv.org/abs/2501.04916v1"}
{"created":"2025-01-09 01:47:41","title":"HaVen: Hallucination-Mitigated LLM for Verilog Code Generation Aligned with HDL Engineers","abstract":"Recently, the use of large language models (LLMs) for Verilog code generation has attracted great research interest to enable hardware design automation. However, previous works have shown a gap between the ability of LLMs and the practical demands of hardware description language (HDL) engineering. This gap includes differences in how engineers phrase questions and hallucinations in the code generated. To address these challenges, we introduce HaVen, a novel LLM framework designed to mitigate hallucinations and align Verilog code generation with the practices of HDL engineers. HaVen tackles hallucination issues by proposing a comprehensive taxonomy and employing a chain-of-thought (CoT) mechanism to translate symbolic modalities (e.g. truth tables, state diagrams, etc.) into accurate natural language descriptions. Furthermore, HaVen bridges this gap by using a data augmentation strategy. It synthesizes high-quality instruction-code pairs that match real HDL engineering practices. Our experiments demonstrate that HaVen significantly improves the correctness of Verilog code generation, outperforming state-of-the-art LLM-based Verilog generation methods on VerilogEval and RTLLM benchmark. HaVen is publicly available at https://github.com/Intelligent-Computing-Research-Group/HaVen.","sentences":["Recently, the use of large language models (LLMs) for Verilog code generation has attracted great research interest to enable hardware design automation.","However, previous works have shown a gap between the ability of LLMs and the practical demands of hardware description language (HDL) engineering.","This gap includes differences in how engineers phrase questions and hallucinations in the code generated.","To address these challenges, we introduce HaVen, a novel LLM framework designed to mitigate hallucinations and align Verilog code generation with the practices of HDL engineers.","HaVen tackles hallucination issues by proposing a comprehensive taxonomy and employing a chain-of-thought (CoT) mechanism to translate symbolic modalities (e.g. truth tables, state diagrams, etc.) into accurate natural language descriptions.","Furthermore, HaVen bridges this gap by using a data augmentation strategy.","It synthesizes high-quality instruction-code pairs that match real HDL engineering practices.","Our experiments demonstrate that HaVen significantly improves the correctness of Verilog code generation, outperforming state-of-the-art LLM-based Verilog generation methods on VerilogEval and RTLLM benchmark.","HaVen is publicly available at https://github.com/Intelligent-Computing-Research-Group/HaVen."],"url":"http://arxiv.org/abs/2501.04908v1"}
{"created":"2025-01-09 01:32:44","title":"JELLY: Joint Emotion Recognition and Context Reasoning with LLMs for Conversational Speech Synthesis","abstract":"Recently, there has been a growing demand for conversational speech synthesis (CSS) that generates more natural speech by considering the conversational context. To address this, we introduce JELLY, a novel CSS framework that integrates emotion recognition and context reasoning for generating appropriate speech in conversation by fine-tuning a large language model (LLM) with multiple partial LoRA modules. We propose an Emotion-aware Q-former encoder, which enables the LLM to perceive emotions in speech. The encoder is trained to align speech emotions with text, utilizing datasets of emotional speech. The entire model is then fine-tuned with conversational speech data to infer emotional context for generating emotionally appropriate speech in conversation. Our experimental results demonstrate that JELLY excels in emotional context modeling, synthesizing speech that naturally aligns with conversation, while mitigating the scarcity of emotional conversational speech datasets.","sentences":["Recently, there has been a growing demand for conversational speech synthesis (CSS) that generates more natural speech by considering the conversational context.","To address this, we introduce JELLY, a novel CSS framework that integrates emotion recognition and context reasoning for generating appropriate speech in conversation by fine-tuning a large language model (LLM) with multiple partial LoRA modules.","We propose an Emotion-aware Q-former encoder, which enables the LLM to perceive emotions in speech.","The encoder is trained to align speech emotions with text, utilizing datasets of emotional speech.","The entire model is then fine-tuned with conversational speech data to infer emotional context for generating emotionally appropriate speech in conversation.","Our experimental results demonstrate that JELLY excels in emotional context modeling, synthesizing speech that naturally aligns with conversation, while mitigating the scarcity of emotional conversational speech datasets."],"url":"http://arxiv.org/abs/2501.04904v1"}
{"created":"2025-01-09 01:25:13","title":"Beyond Life: A Digital Will Solution for Posthumous Data Management","abstract":"In the digital era, managing posthumous data presents a growing challenge, with current technical solutions often falling short in practicality. Existing tools are typically closed-source, lack transparency, fail to offer cross-platform support, and provide limited access control. This paper introduces `Beyond Life', a cross-platform digital will management solution designed to securely handle and distribute digital assets after death. At the core of this solution is a customized Ciphertext-Policy Attribute-Based Encryption (CP-ABE) scheme, referred to as PD-CP-ABE, which enables efficient, fine-grained control over access to will content at scale. Unlike existing systems, Beyond Life operates independently of service providers, offering users greater transparency and control over how their will is generated, stored, and executed. The system is also designed to be portable, allowing users to change their will service provider. The proposed system has been fully developed and rigorously evaluated to ensure performance and real-world feasibility. The system implementation is made publicly available.","sentences":["In the digital era, managing posthumous data presents a growing challenge, with current technical solutions often falling short in practicality.","Existing tools are typically closed-source, lack transparency, fail to offer cross-platform support, and provide limited access control.","This paper introduces `Beyond Life', a cross-platform digital will management solution designed to securely handle and distribute digital assets after death.","At the core of this solution is a customized Ciphertext-Policy Attribute-Based Encryption (CP-ABE) scheme, referred to as PD-CP-ABE, which enables efficient, fine-grained control over access to will content at scale.","Unlike existing systems, Beyond Life operates independently of service providers, offering users greater transparency and control over how their will is generated, stored, and executed.","The system is also designed to be portable, allowing users to change their will service provider.","The proposed system has been fully developed and rigorously evaluated to ensure performance and real-world feasibility.","The system implementation is made publicly available."],"url":"http://arxiv.org/abs/2501.04900v1"}
{"created":"2025-01-09 01:03:14","title":"Online Continual Learning: A Systematic Literature Review of Approaches, Challenges, and Benchmarks","abstract":"Online Continual Learning (OCL) is a critical area in machine learning, focusing on enabling models to adapt to evolving data streams in real-time while addressing challenges such as catastrophic forgetting and the stability-plasticity trade-off. This study conducts the first comprehensive Systematic Literature Review (SLR) on OCL, analyzing 81 approaches, extracting over 1,000 features (specific tasks addressed by these approaches), and identifying more than 500 components (sub-models within approaches, including algorithms and tools). We also review 83 datasets spanning applications like image classification, object detection, and multimodal vision-language tasks. Our findings highlight key challenges, including reducing computational overhead, developing domain-agnostic solutions, and improving scalability in resource-constrained environments. Furthermore, we identify promising directions for future research, such as leveraging self-supervised learning for multimodal and sequential data, designing adaptive memory mechanisms that integrate sparse retrieval and generative replay, and creating efficient frameworks for real-world applications with noisy or evolving task boundaries. By providing a rigorous and structured synthesis of the current state of OCL, this review offers a valuable resource for advancing this field and addressing its critical challenges and opportunities. The complete SLR methodology steps and extracted data are publicly available through the provided link: https://github.com/kiyan-rezaee/ Systematic-Literature-Review-on-Online-Continual-Learning","sentences":["Online Continual Learning (OCL) is a critical area in machine learning, focusing on enabling models to adapt to evolving data streams in real-time while addressing challenges such as catastrophic forgetting and the stability-plasticity trade-off.","This study conducts the first comprehensive Systematic Literature Review (SLR) on OCL, analyzing 81 approaches, extracting over 1,000 features (specific tasks addressed by these approaches), and identifying more than 500 components (sub-models within approaches, including algorithms and tools).","We also review 83 datasets spanning applications like image classification, object detection, and multimodal vision-language tasks.","Our findings highlight key challenges, including reducing computational overhead, developing domain-agnostic solutions, and improving scalability in resource-constrained environments.","Furthermore, we identify promising directions for future research, such as leveraging self-supervised learning for multimodal and sequential data, designing adaptive memory mechanisms that integrate sparse retrieval and generative replay, and creating efficient frameworks for real-world applications with noisy or evolving task boundaries.","By providing a rigorous and structured synthesis of the current state of OCL, this review offers a valuable resource for advancing this field and addressing its critical challenges and opportunities.","The complete SLR methodology steps and extracted data are publicly available through the provided link: https://github.com/kiyan-rezaee/ Systematic-Literature-Review-on-Online-Continual-Learning"],"url":"http://arxiv.org/abs/2501.04897v1"}
{"created":"2025-01-09 00:50:44","title":"Quantifying Itch and its Impact on Sleep Using Machine Learning and Radio Signals","abstract":"Chronic itch affects 13% of the US population, is highly debilitating, and underlies many medical conditions. A major challenge in clinical care and new therapeutics development is the lack of an objective measure for quantifying itch, leading to reliance on subjective measures like patients' self-assessment of itch severity. In this paper, we show that a home radio device paired with artificial intelligence (AI) can concurrently capture scratching and evaluate its impact on sleep quality by analyzing radio signals bouncing in the environment. The device eliminates the need for wearable sensors or skin contact, enabling monitoring of chronic itch over extended periods at home without burdening patients or interfering with their skin condition. To validate the technology, we conducted an observational clinical study of chronic pruritus patients, monitored at home for one month using both the radio device and an infrared camera. Comparing the output of the device to ground truth data from the camera demonstrates its feasibility and accuracy (ROC AUC = 0.997, sensitivity = 0.825, specificity = 0.997). The results reveal a significant correlation between scratching and low sleep quality, manifested as a reduction in sleep efficiency (R = 0.6, p < 0.001) and an increase in sleep latency (R = 0.68, p < 0.001). Our study underscores the potential of passive, long-term, at-home monitoring of chronic scratching and its sleep implications, offering a valuable tool for both clinical care of chronic itch patients and pharmaceutical clinical trials.","sentences":["Chronic itch affects 13% of the US population, is highly debilitating, and underlies many medical conditions.","A major challenge in clinical care and new therapeutics development is the lack of an objective measure for quantifying itch, leading to reliance on subjective measures like patients' self-assessment of itch severity.","In this paper, we show that a home radio device paired with artificial intelligence (AI) can concurrently capture scratching and evaluate its impact on sleep quality by analyzing radio signals bouncing in the environment.","The device eliminates the need for wearable sensors or skin contact, enabling monitoring of chronic itch over extended periods at home without burdening patients or interfering with their skin condition.","To validate the technology, we conducted an observational clinical study of chronic pruritus patients, monitored at home for one month using both the radio device and an infrared camera.","Comparing the output of the device to ground truth data from the camera demonstrates its feasibility and accuracy (ROC AUC = 0.997, sensitivity = 0.825, specificity = 0.997).","The results reveal a significant correlation between scratching and low sleep quality, manifested as a reduction in sleep efficiency (R = 0.6, p < 0.001) and an increase in sleep latency (R = 0.68, p < 0.001).","Our study underscores the potential of passive, long-term, at-home monitoring of chronic scratching and its sleep implications, offering a valuable tool for both clinical care of chronic itch patients and pharmaceutical clinical trials."],"url":"http://arxiv.org/abs/2501.04896v1"}
{"created":"2025-01-08 23:28:28","title":"Leveraging Log Probabilities in Language Models to Forecast Future Events","abstract":"In the constantly changing field of data-driven decision making, accurately predicting future events is crucial for strategic planning in various sectors. The emergence of Large Language Models (LLMs) marks a significant advancement in this area, offering advanced tools that utilise extensive text data for prediction. In this industry paper, we introduce a novel method for AI-driven foresight using LLMs. Building on top of previous research, we employ data on current trends and their trajectories for generating forecasts on 15 different topics. Subsequently, we estimate their probabilities via a multi-step approach based on log probabilities. We show we achieve a Brier score of 0.186, meaning a +26% improvement over random chance and a +19% improvement over widely-available AI systems.","sentences":["In the constantly changing field of data-driven decision making, accurately predicting future events is crucial for strategic planning in various sectors.","The emergence of Large Language Models (LLMs) marks a significant advancement in this area, offering advanced tools that utilise extensive text data for prediction.","In this industry paper, we introduce a novel method for AI-driven foresight using LLMs.","Building on top of previous research, we employ data on current trends and their trajectories for generating forecasts on 15 different topics.","Subsequently, we estimate their probabilities via a multi-step approach based on log probabilities.","We show we achieve a Brier score of 0.186, meaning a +26% improvement over random chance and a +19% improvement over widely-available AI systems."],"url":"http://arxiv.org/abs/2501.04880v1"}
{"created":"2025-01-08 22:22:44","title":"LayerMix: Enhanced Data Augmentation through Fractal Integration for Robust Deep Learning","abstract":"Deep learning models have demonstrated remarkable performance across various computer vision tasks, yet their vulnerability to distribution shifts remains a critical challenge. Despite sophisticated neural network architectures, existing models often struggle to maintain consistent performance when confronted with Out-of-Distribution (OOD) samples, including natural corruptions, adversarial perturbations, and anomalous patterns. We introduce LayerMix, an innovative data augmentation approach that systematically enhances model robustness through structured fractal-based image synthesis. By meticulously integrating structural complexity into training datasets, our method generates semantically consistent synthetic samples that significantly improve neural network generalization capabilities. Unlike traditional augmentation techniques that rely on random transformations, LayerMix employs a structured mixing pipeline that preserves original image semantics while introducing controlled variability. Extensive experiments across multiple benchmark datasets, including CIFAR-10, CIFAR-100, ImageNet-200, and ImageNet-1K demonstrate LayerMixs superior performance in classification accuracy and substantially enhances critical Machine Learning (ML) safety metrics, including resilience to natural image corruptions, robustness against adversarial attacks, improved model calibration and enhanced prediction consistency. LayerMix represents a significant advancement toward developing more reliable and adaptable artificial intelligence systems by addressing the fundamental challenges of deep learning generalization. The code is available at https://github.com/ahmadmughees/layermix.","sentences":["Deep learning models have demonstrated remarkable performance across various computer vision tasks, yet their vulnerability to distribution shifts remains a critical challenge.","Despite sophisticated neural network architectures, existing models often struggle to maintain consistent performance when confronted with Out-of-Distribution (OOD) samples, including natural corruptions, adversarial perturbations, and anomalous patterns.","We introduce LayerMix, an innovative data augmentation approach that systematically enhances model robustness through structured fractal-based image synthesis.","By meticulously integrating structural complexity into training datasets, our method generates semantically consistent synthetic samples that significantly improve neural network generalization capabilities.","Unlike traditional augmentation techniques that rely on random transformations, LayerMix employs a structured mixing pipeline that preserves original image semantics while introducing controlled variability.","Extensive experiments across multiple benchmark datasets, including CIFAR-10, CIFAR-100, ImageNet-200, and ImageNet-1K demonstrate LayerMixs superior performance in classification accuracy and substantially enhances critical Machine Learning (ML) safety metrics, including resilience to natural image corruptions, robustness against adversarial attacks, improved model calibration and enhanced prediction consistency.","LayerMix represents a significant advancement toward developing more reliable and adaptable artificial intelligence systems by addressing the fundamental challenges of deep learning generalization.","The code is available at https://github.com/ahmadmughees/layermix."],"url":"http://arxiv.org/abs/2501.04861v1"}
{"created":"2025-01-08 22:22:15","title":"Exploring the Use of Robots for Diary Studies","abstract":"As interest in studying in-the-wild human-robot interaction grows, there is a need for methods to collect data over time and in naturalistic or potentially private environments. HRI researchers have increasingly used the diary method for these studies, asking study participants to self-administer a structured data collection instrument, i.e., a diary, over a period of time. Although the diary method offers a unique window into settings that researchers may not have access to, they also lack the interactivity and probing that interview-based methods offer. In this paper, we explore a novel data collection method in which a robot plays the role of an interactive diary. We developed the Diary Robot system and performed in-home deployments for a week to evaluate the feasibility and effectiveness of this approach. Using traditional text-based and audio-based diaries as benchmarks, we found that robots are able to effectively elicit the intended information. We reflect on our findings, and describe scenarios where the utilization of robots in diary studies as a data collection instrument may be especially applicable.","sentences":["As interest in studying in-the-wild human-robot interaction grows, there is a need for methods to collect data over time and in naturalistic or potentially private environments.","HRI researchers have increasingly used the diary method for these studies, asking study participants to self-administer a structured data collection instrument, i.e., a diary, over a period of time.","Although the diary method offers a unique window into settings that researchers may not have access to, they also lack the interactivity and probing that interview-based methods offer.","In this paper, we explore a novel data collection method in which a robot plays the role of an interactive diary.","We developed the Diary Robot system and performed in-home deployments for a week to evaluate the feasibility and effectiveness of this approach.","Using traditional text-based and audio-based diaries as benchmarks, we found that robots are able to effectively elicit the intended information.","We reflect on our findings, and describe scenarios where the utilization of robots in diary studies as a data collection instrument may be especially applicable."],"url":"http://arxiv.org/abs/2501.04860v1"}
{"created":"2025-01-08 22:17:28","title":"ETH-Tight FPT Algorithm for Makespan Minimization on Uniform Machines","abstract":"Given $n$ jobs with processing times $p_1,\\dotsc,p_n\\in\\mathbb N$ and $m\\le n$ machines with speeds $s_1,\\dotsc,s_m\\in\\mathbb N$ our goal is to allocate the jobs to machines minimizing the makespan. We present an algorithm that solves the problem in time $p_{\\max}^{O(d)} n^{O(1)}$, where $p_{\\max}$ is the maximum processing time and $d\\le p_{\\max}$ is the number of distinct processing times. This is essentially the best possible due to a lower bound based on the exponential time hypothesis (ETH).   Our result improves over prior works that had a quadratic term in $d$ in the exponent and answers an open question by Kouteck\\'y and Zink. The algorithm is based on integer programming techniques combined with novel ideas based on modular arithmetic. They can also be implemented efficiently for the more compact high-multiplicity instance encoding.","sentences":["Given $n$ jobs with processing times $p_1,\\dotsc,p_n\\in\\mathbb N$ and $m\\le n$ machines with speeds $s_1,\\dotsc,s_m\\in\\mathbb N$ our goal is to allocate the jobs to machines minimizing the makespan.","We present an algorithm that solves the problem in time $p_{\\max}^{O(d)} n^{O(1)}$, where $p_{\\max}$ is the maximum processing time and $d\\le p_{\\max}$ is the number of distinct processing times.","This is essentially the best possible due to a lower bound based on the exponential time hypothesis (ETH).   ","Our result improves over prior works that had a quadratic term in $d$ in the exponent and answers an open question by Kouteck\\'y and Zink.","The algorithm is based on integer programming techniques combined with novel ideas based on modular arithmetic.","They can also be implemented efficiently for the more compact high-multiplicity instance encoding."],"url":"http://arxiv.org/abs/2501.04859v1"}
{"created":"2025-01-08 21:15:14","title":"EDMB: Edge Detector with Mamba","abstract":"Transformer-based models have made significant progress in edge detection, but their high computational cost is prohibitive. Recently, vision Mamba have shown excellent ability in efficiently capturing long-range dependencies. Drawing inspiration from this, we propose a novel edge detector with Mamba, termed EDMB, to efficiently generate high-quality multi-granularity edges. In EDMB, Mamba is combined with a global-local architecture, therefore it can focus on both global information and fine-grained cues. The fine-grained cues play a crucial role in edge detection, but are usually ignored by ordinary Mamba. We design a novel decoder to construct learnable Gaussian distributions by fusing global features and fine-grained features. And the multi-grained edges are generated by sampling from the distributions. In order to make multi-granularity edges applicable to single-label data, we introduce Evidence Lower Bound loss to supervise the learning of the distributions. On the multi-label dataset BSDS500, our proposed EDMB achieves competitive single-granularity ODS 0.837 and multi-granularity ODS 0.851 without multi-scale test or extra PASCAL-VOC data. Remarkably, EDMB can be extended to single-label datasets such as NYUDv2 and BIPED. The source code is available at https://github.com/Li-yachuan/EDMB.","sentences":["Transformer-based models have made significant progress in edge detection, but their high computational cost is prohibitive.","Recently, vision Mamba have shown excellent ability in efficiently capturing long-range dependencies.","Drawing inspiration from this, we propose a novel edge detector with Mamba, termed EDMB, to efficiently generate high-quality multi-granularity edges.","In EDMB, Mamba is combined with a global-local architecture, therefore it can focus on both global information and fine-grained cues.","The fine-grained cues play a crucial role in edge detection, but are usually ignored by ordinary Mamba.","We design a novel decoder to construct learnable Gaussian distributions by fusing global features and fine-grained features.","And the multi-grained edges are generated by sampling from the distributions.","In order to make multi-granularity edges applicable to single-label data, we introduce Evidence Lower Bound loss to supervise the learning of the distributions.","On the multi-label dataset BSDS500, our proposed EDMB achieves competitive single-granularity ODS 0.837 and multi-granularity ODS 0.851 without multi-scale test or extra PASCAL-VOC data.","Remarkably, EDMB can be extended to single-label datasets such as NYUDv2 and BIPED.","The source code is available at https://github.com/Li-yachuan/EDMB."],"url":"http://arxiv.org/abs/2501.04846v1"}
{"created":"2025-01-08 21:07:25","title":"Blockchain-Based Secure Vehicle Auction System with Smart Contracts","abstract":"The problem of a single point of failure in centralized systems poses a great challenge to the stability of such systems. Meanwhile, the tamperability of data within centralized systems makes users reluctant to trust and use centralized applications in many scenarios, including the financial and business sectors.   Blockchain, as a new decentralized technology, addresses these issues effectively. As a typical decentralized system, blockchain can be utilized to build a data-sharing model. Users in a blockchain do not need to trust other users; instead, they trust that the majority of miner nodes are honest. Smart contracts enable developers to write distributed programs based on blockchain systems, ensuring that all code is immutable and secure.   In this paper, we analyze the security of blockchain technology to illustrate its advantages and justify its use. Furthermore, we design a new system for storing and trading vehicle information based on the Ethereum blockchain and smart contract technology. Specifically, our system allows users to upload vehicle information and auction vehicles to transfer ownership. Our application provides great convenience to buyers and owners, while the use of smart contracts enhances the security and privacy of the system.","sentences":["The problem of a single point of failure in centralized systems poses a great challenge to the stability of such systems.","Meanwhile, the tamperability of data within centralized systems makes users reluctant to trust and use centralized applications in many scenarios, including the financial and business sectors.   ","Blockchain, as a new decentralized technology, addresses these issues effectively.","As a typical decentralized system, blockchain can be utilized to build a data-sharing model.","Users in a blockchain do not need to trust other users; instead, they trust that the majority of miner nodes are honest.","Smart contracts enable developers to write distributed programs based on blockchain systems, ensuring that all code is immutable and secure.   ","In this paper, we analyze the security of blockchain technology to illustrate its advantages and justify its use.","Furthermore, we design a new system for storing and trading vehicle information based on the Ethereum blockchain and smart contract technology.","Specifically, our system allows users to upload vehicle information and auction vehicles to transfer ownership.","Our application provides great convenience to buyers and owners, while the use of smart contracts enhances the security and privacy of the system."],"url":"http://arxiv.org/abs/2501.04841v1"}
{"created":"2025-01-08 20:26:13","title":"Intelligent Gradient Boosting Algorithms for Estimating Strength of Modified Subgrade Soil","abstract":"The performance of pavement under loading depends on the strength of the subgrade. However, experimental estimation of properties of pavement strengths such as California bearing ratio (CBR), unconfined compressive strength (UCS) and resistance value (R) are often tedious, time-consuming and costly, thereby inspiring a growing interest in machine learning based tools which are simple, cheap and fast alternatives. Thus, the potential application of two boosting techniques; categorical boosting (CatBoost) and extreme gradient boosting (XGBoost) and support vector regression (SVR), is similarly explored in this study for estimation of properties of subgrade soil modified with hydrated lime activated rice husk ash (HARSH). Using 121 experimental data samples of varying proportions of HARSH, plastic limit, liquid limit, plasticity index, clay activity, optimum moisture content, and maximum dry density as input for CBR, UCS and R estimation, four evaluation metrics namely coefficient of determination (R2), root mean squared error (RMSE), mean absolute error (MAE) and mean absolute percentage error (MAPE) are used to evaluate the models' performance. The results indicate that XGBoost outperformed CatBoost and SVR in estimating these properties, yielding R2 of 0.9994, 0.9995 and 0.9999 in estimating the CBR, UCS and R respectively. Also, SVR outperformed CatBoost in estimating the CBR and R with R2 of 0.9997 respectively. On the other hand, CatBoost outperformed SVR in estimating the UCS with R2 of 0.9994. Feature sensitivity analysis shows that the three machine learning techniques are unanimous that increasing HARSH proportion lead to values of the estimated properties respectively. A comparison with previous results also shows superiority of XGBoost in estimating subgrade properties.","sentences":["The performance of pavement under loading depends on the strength of the subgrade.","However, experimental estimation of properties of pavement strengths such as California bearing ratio (CBR), unconfined compressive strength (UCS) and resistance value (R) are often tedious, time-consuming and costly, thereby inspiring a growing interest in machine learning based tools which are simple, cheap and fast alternatives.","Thus, the potential application of two boosting techniques; categorical boosting (CatBoost) and extreme gradient boosting (XGBoost) and support vector regression (SVR), is similarly explored in this study for estimation of properties of subgrade soil modified with hydrated lime activated rice husk ash (HARSH).","Using 121 experimental data samples of varying proportions of HARSH, plastic limit, liquid limit, plasticity index, clay activity, optimum moisture content, and maximum dry density as input for CBR, UCS and R estimation, four evaluation metrics namely coefficient of determination (R2), root mean squared error (RMSE), mean absolute error (MAE) and mean absolute percentage error (MAPE) are used to evaluate the models' performance.","The results indicate that XGBoost outperformed CatBoost and SVR in estimating these properties, yielding R2 of 0.9994, 0.9995 and 0.9999 in estimating the CBR, UCS and R respectively.","Also, SVR outperformed CatBoost in estimating the CBR and R with R2 of 0.9997 respectively.","On the other hand, CatBoost outperformed SVR in estimating the UCS with R2 of 0.9994.","Feature sensitivity analysis shows that the three machine learning techniques are unanimous that increasing HARSH proportion lead to values of the estimated properties respectively.","A comparison with previous results also shows superiority of XGBoost in estimating subgrade properties."],"url":"http://arxiv.org/abs/2501.04826v1"}
{"created":"2025-01-08 20:22:16","title":"Learning Robot Safety from Sparse Human Feedback using Conformal Prediction","abstract":"Ensuring robot safety can be challenging; user-defined constraints can miss edge cases, policies can become unsafe even when trained from safe data, and safety can be subjective. Thus, we learn about robot safety by showing policy trajectories to a human who flags unsafe behavior. From this binary feedback, we use the statistical method of conformal prediction to identify a region of states, potentially in learned latent space, guaranteed to contain a user-specified fraction of future policy errors. Our method is sample-efficient, as it builds on nearest neighbor classification and avoids withholding data as is common with conformal prediction. By alerting if the robot reaches the suspected unsafe region, we obtain a warning system that mimics the human's safety preferences with guaranteed miss rate. From video labeling, our system can detect when a quadcopter visuomotor policy will fail to steer through a designated gate. We present an approach for policy improvement by avoiding the suspected unsafe region. With it we improve a model predictive controller's safety, as shown in experimental testing with 30 quadcopter flights across 6 navigation tasks. Code and videos are provided.","sentences":["Ensuring robot safety can be challenging; user-defined constraints can miss edge cases, policies can become unsafe even when trained from safe data, and safety can be subjective.","Thus, we learn about robot safety by showing policy trajectories to a human who flags unsafe behavior.","From this binary feedback, we use the statistical method of conformal prediction to identify a region of states, potentially in learned latent space, guaranteed to contain a user-specified fraction of future policy errors.","Our method is sample-efficient, as it builds on nearest neighbor classification and avoids withholding data as is common with conformal prediction.","By alerting if the robot reaches the suspected unsafe region, we obtain a warning system that mimics the human's safety preferences with guaranteed miss rate.","From video labeling, our system can detect when a quadcopter visuomotor policy will fail to steer through a designated gate.","We present an approach for policy improvement by avoiding the suspected unsafe region.","With it we improve a model predictive controller's safety, as shown in experimental testing with 30 quadcopter flights across 6 navigation tasks.","Code and videos are provided."],"url":"http://arxiv.org/abs/2501.04823v1"}
{"created":"2025-01-08 20:14:07","title":"Decentralised Resource Sharing in TinyML: Wireless Bilayer Gossip Parallel SGD for Collaborative Learning","abstract":"With the growing computational capabilities of microcontroller units (MCUs), edge devices can now support machine learning models. However, deploying decentralised federated learning (DFL) on such devices presents key challenges, including intermittent connectivity, limited communication range, and dynamic network topologies. This paper proposes a novel framework, bilayer Gossip Decentralised Parallel Stochastic Gradient Descent (GD PSGD), designed to address these issues in resource-constrained environments. The framework incorporates a hierarchical communication structure using Distributed Kmeans (DKmeans) clustering for geographic grouping and a gossip protocol for efficient model aggregation across two layers: intra-cluster and inter-cluster. We evaluate the framework's performance against the Centralised Federated Learning (CFL) baseline using the MCUNet model on the CIFAR-10 dataset under IID and Non-IID conditions. Results demonstrate that the proposed method achieves comparable accuracy to CFL on IID datasets, requiring only 1.8 additional rounds for convergence. On Non-IID datasets, the accuracy loss remains under 8\\% for moderate data imbalance. These findings highlight the framework's potential to support scalable and privacy-preserving learning on edge devices with minimal performance trade-offs.","sentences":["With the growing computational capabilities of microcontroller units (MCUs), edge devices can now support machine learning models.","However, deploying decentralised federated learning (DFL) on such devices presents key challenges, including intermittent connectivity, limited communication range, and dynamic network topologies.","This paper proposes a novel framework, bilayer Gossip Decentralised Parallel Stochastic Gradient Descent (GD PSGD), designed to address these issues in resource-constrained environments.","The framework incorporates a hierarchical communication structure using Distributed Kmeans (DKmeans) clustering for geographic grouping and a gossip protocol for efficient model aggregation across two layers: intra-cluster and inter-cluster.","We evaluate the framework's performance against the Centralised Federated Learning (CFL) baseline using the MCUNet model on the CIFAR-10 dataset under IID and Non-IID conditions.","Results demonstrate that the proposed method achieves comparable accuracy to CFL on IID datasets, requiring only 1.8 additional rounds for convergence.","On Non-IID datasets, the accuracy loss remains under 8\\% for moderate data imbalance.","These findings highlight the framework's potential to support scalable and privacy-preserving learning on edge devices with minimal performance trade-offs."],"url":"http://arxiv.org/abs/2501.04817v1"}
{"created":"2025-01-08 20:03:48","title":"Improved Approximation Algorithms for (1,2)-TSP and Max-TSP Using Path Covers in the Semi-Streaming Model","abstract":"We investigate semi-streaming algorithms for the Traveling Salesman Problem (TSP). Specifically, we focus on a variant known as the $(1,2)$-TSP, where the distances between any two vertices are either one or two. Our primary emphasis is on the closely related Maximum Path Cover Problem, which aims to find a collection of vertex-disjoint paths that covers the maximum number of edges in a graph. We propose an algorithm that, for any $\\epsilon > 0$, achieves a $(\\frac{2}{3}-\\epsilon)$-approximation of the maximum path cover size for an $n$-vertex graph, using $\\text{poly}(\\frac{1}{\\epsilon})$ passes. This result improves upon the previous $\\frac{1}{2}$-approximation by Behnezhad and et al. [ICALP 2024] in the semi-streaming model. Building on this result, we design a semi-streaming algorithm that constructs a tour for an instance of \\tsp with an approximation factor of $(\\frac{4}{3} + \\epsilon)$, improving upon the previous $\\frac{3}{2}$-approximation actor algorithm by Behnezhad and et al. [ICALP 2024] (Although it is not explicitly stated in this paper that their algorithm works in the semi-streaming model, it is easy to verify). Furthermore, we extend our approach to develop an approximation algorithm for the Maximum TSP (Max-TSP), where the goal is to find a Hamiltonian cycle with the maximum possible weight in a given weighted graph $G$. Our algorithm provides a $(\\frac{7}{12} - \\epsilon)$-approximation for Max-TSP in $\\text{poly}(\\frac{1}{\\epsilon})$ passes, improving on the previously known $(\\frac{1}{2}-\\epsilon)$-approximation obtained via maximum weight matching in the semi-streaming model.","sentences":["We investigate semi-streaming algorithms for the Traveling Salesman Problem (TSP).","Specifically, we focus on a variant known as the $(1,2)$-TSP, where the distances between any two vertices are either one or two.","Our primary emphasis is on the closely related Maximum Path Cover Problem, which aims to find a collection of vertex-disjoint paths that covers the maximum number of edges in a graph.","We propose an algorithm that, for any $\\epsilon > 0$, achieves a $(\\frac{2}{3}-\\epsilon)$-approximation of the maximum path cover size for an $n$-vertex graph, using $\\text{poly}(\\frac{1}{\\epsilon})$ passes.","This result improves upon the previous $\\frac{1}{2}$-approximation by Behnezhad and et al.","[ICALP 2024] in the semi-streaming model.","Building on this result, we design a semi-streaming algorithm that constructs a tour for an instance of \\tsp with an approximation factor of $(\\frac{4}{3} + \\epsilon)$, improving upon the previous $\\frac{3}{2}$-approximation actor algorithm by Behnezhad and et al.","[ICALP 2024]","(Although it is not explicitly stated in this paper that their algorithm works in the semi-streaming model, it is easy to verify).","Furthermore, we extend our approach to develop an approximation algorithm for the Maximum TSP (Max-TSP), where the goal is to find a Hamiltonian cycle with the maximum possible weight in a given weighted graph $G$.","Our algorithm provides a $(\\frac{7}{12} - \\epsilon)$-approximation for Max-TSP in $\\text{poly}(\\frac{1}{\\epsilon})$ passes, improving on the previously known $(\\frac{1}{2}-\\epsilon)$-approximation obtained via maximum weight matching in the semi-streaming model."],"url":"http://arxiv.org/abs/2501.04813v1"}
{"created":"2025-01-08 19:18:18","title":"An Analysis on Stabilizability and Reliability Relationship in Wireless Networked Control Systems","abstract":"The stabilizability of wireless networked control systems (WNCSs) is a deterministic binary valued parameter proven to hold if the communication data rate is higher than the sum of the logarithm of unstable eigenvalues of the open-loop control system. In this analysis, it is assumed that the communication system provides a fixed deterministic transmission rate between the sensors and controllers. Due to the stochastic parameters of communication channels, such as small-scale fading, the instantaneous rate is an intrinsically stochastic parameter. In this sense, it is a common practice in the literature to use the deterministic ergodic rate in analyzing the asymptotic stabilizability. Theoretically, there exists no work in the literature investigating how the ergodic rate can be incorporated into the analysis of asymptotic stabilizability. Considering the stochastic nature of channel parameters, we introduce the concept of probability of stabilizability by interconnecting communication link reliability with the system's unstable eigenvalues and derive a closed-form expression that quantifies this metric. Numerical results are provided to visualize how communication and control systems' parameters affect the probability of stabilizability of the overall system.","sentences":["The stabilizability of wireless networked control systems (WNCSs) is a deterministic binary valued parameter proven to hold if the communication data rate is higher than the sum of the logarithm of unstable eigenvalues of the open-loop control system.","In this analysis, it is assumed that the communication system provides a fixed deterministic transmission rate between the sensors and controllers.","Due to the stochastic parameters of communication channels, such as small-scale fading, the instantaneous rate is an intrinsically stochastic parameter.","In this sense, it is a common practice in the literature to use the deterministic ergodic rate in analyzing the asymptotic stabilizability.","Theoretically, there exists no work in the literature investigating how the ergodic rate can be incorporated into the analysis of asymptotic stabilizability.","Considering the stochastic nature of channel parameters, we introduce the concept of probability of stabilizability by interconnecting communication link reliability with the system's unstable eigenvalues and derive a closed-form expression that quantifies this metric.","Numerical results are provided to visualize how communication and control systems' parameters affect the probability of stabilizability of the overall system."],"url":"http://arxiv.org/abs/2501.04792v1"}
{"created":"2025-01-08 19:01:32","title":"Traffic Simulations: Multi-City Calibration of Metropolitan Highway Networks","abstract":"This paper proposes an approach to perform travel demand calibration for high-resolution stochastic traffic simulators. It employs abundant travel times at the path-level, departing from the standard practice of resorting to scarce segment-level sensor counts. The proposed approach is shown to tackle high-dimensional instances in a sample-efficient way. For the first time, case studies on 6 metropolitan highway networks are carried out, considering a total of 54 calibration scenarios. This is the first work to show the ability of a calibration algorithm to systematically scale across networks. Compared to the state-of-the-art simultaneous perturbation stochastic approximation (SPSA) algorithm, the proposed approach enhances fit to field data by an average 43.5% with a maximum improvement of 80.0%, and does so within fewer simulation calls.","sentences":["This paper proposes an approach to perform travel demand calibration for high-resolution stochastic traffic simulators.","It employs abundant travel times at the path-level, departing from the standard practice of resorting to scarce segment-level sensor counts.","The proposed approach is shown to tackle high-dimensional instances in a sample-efficient way.","For the first time, case studies on 6 metropolitan highway networks are carried out, considering a total of 54 calibration scenarios.","This is the first work to show the ability of a calibration algorithm to systematically scale across networks.","Compared to the state-of-the-art simultaneous perturbation stochastic approximation (SPSA) algorithm, the proposed approach enhances fit to field data by an average 43.5% with a maximum improvement of 80.0%, and does so within fewer simulation calls."],"url":"http://arxiv.org/abs/2501.04783v1"}
{"created":"2025-01-08 18:35:48","title":"Video Summarisation with Incident and Context Information using Generative AI","abstract":"The proliferation of video content production has led to vast amounts of data, posing substantial challenges in terms of analysis efficiency and resource utilization. Addressing this issue calls for the development of robust video analysis tools. This paper proposes a novel approach leveraging Generative Artificial Intelligence (GenAI) to facilitate streamlined video analysis. Our tool aims to deliver tailored textual summaries of user-defined queries, offering a focused insight amidst extensive video datasets. Unlike conventional frameworks that offer generic summaries or limited action recognition, our method harnesses the power of GenAI to distil relevant information, enhancing analysis precision and efficiency. Employing YOLO-V8 for object detection and Gemini for comprehensive video and text analysis, our solution achieves heightened contextual accuracy. By combining YOLO with Gemini, our approach furnishes textual summaries extracted from extensive CCTV footage, enabling users to swiftly navigate and verify pertinent events without the need for exhaustive manual review. The quantitative evaluation revealed a similarity of 72.8%, while the qualitative assessment rated an accuracy of 85%, demonstrating the capability of the proposed method.","sentences":["The proliferation of video content production has led to vast amounts of data, posing substantial challenges in terms of analysis efficiency and resource utilization.","Addressing this issue calls for the development of robust video analysis tools.","This paper proposes a novel approach leveraging Generative Artificial Intelligence (GenAI) to facilitate streamlined video analysis.","Our tool aims to deliver tailored textual summaries of user-defined queries, offering a focused insight amidst extensive video datasets.","Unlike conventional frameworks that offer generic summaries or limited action recognition, our method harnesses the power of GenAI to distil relevant information, enhancing analysis precision and efficiency.","Employing YOLO-V8 for object detection and Gemini for comprehensive video and text analysis, our solution achieves heightened contextual accuracy.","By combining YOLO with Gemini, our approach furnishes textual summaries extracted from extensive CCTV footage, enabling users to swiftly navigate and verify pertinent events without the need for exhaustive manual review.","The quantitative evaluation revealed a similarity of 72.8%, while the qualitative assessment rated an accuracy of 85%, demonstrating the capability of the proposed method."],"url":"http://arxiv.org/abs/2501.04764v1"}
