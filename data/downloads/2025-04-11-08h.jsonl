{"created":"2025-04-10 17:59:55","title":"Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction","abstract":"We introduce Geo4D, a method to repurpose video diffusion models for monocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic prior captured by such video models, Geo4D can be trained using only synthetic data while generalizing well to real data in a zero-shot manner. Geo4D predicts several complementary geometric modalities, namely point, depth, and ray maps. It uses a new multi-modal alignment algorithm to align and fuse these modalities, as well as multiple sliding windows, at inference time, thus obtaining robust and accurate 4D reconstruction of long videos. Extensive experiments across multiple benchmarks show that Geo4D significantly surpasses state-of-the-art video depth estimation methods, including recent methods such as MonST3R, which are also designed to handle dynamic scenes.","sentences":["We introduce Geo4D, a method to repurpose video diffusion models for monocular 3D reconstruction of dynamic scenes.","By leveraging the strong dynamic prior captured by such video models, Geo4D can be trained using only synthetic data while generalizing well to real data in a zero-shot manner.","Geo4D predicts several complementary geometric modalities, namely point, depth, and ray maps.","It uses a new multi-modal alignment algorithm to align and fuse these modalities, as well as multiple sliding windows, at inference time, thus obtaining robust and accurate 4D reconstruction of long videos.","Extensive experiments across multiple benchmarks show that Geo4D significantly surpasses state-of-the-art video depth estimation methods, including recent methods such as MonST3R, which are also designed to handle dynamic scenes."],"url":"http://arxiv.org/abs/2504.07961v1"}
{"created":"2025-04-10 17:59:31","title":"CCMNet: Leveraging Calibrated Color Correction Matrices for Cross-Camera Color Constancy","abstract":"Computational color constancy, or white balancing, is a key module in a camera's image signal processor (ISP) that corrects color casts from scene lighting. Because this operation occurs in the camera-specific raw color space, white balance algorithms must adapt to different cameras. This paper introduces a learning-based method for cross-camera color constancy that generalizes to new cameras without retraining. Our method leverages pre-calibrated color correction matrices (CCMs) available on ISPs that map the camera's raw color space to a standard space (e.g., CIE XYZ). Our method uses these CCMs to transform predefined illumination colors (i.e., along the Planckian locus) into the test camera's raw space. The mapped illuminants are encoded into a compact camera fingerprint embedding (CFE) that enables the network to adapt to unseen cameras. To prevent overfitting due to limited cameras and CCMs during training, we introduce a data augmentation technique that interpolates between cameras and their CCMs. Experimental results across multiple datasets and backbones show that our method achieves state-of-the-art cross-camera color constancy while remaining lightweight and relying only on data readily available in camera ISPs.","sentences":["Computational color constancy, or white balancing, is a key module in a camera's image signal processor (ISP) that corrects color casts from scene lighting.","Because this operation occurs in the camera-specific raw color space, white balance algorithms must adapt to different cameras.","This paper introduces a learning-based method for cross-camera color constancy that generalizes to new cameras without retraining.","Our method leverages pre-calibrated color correction matrices (CCMs) available on ISPs that map the camera's raw color space to a standard space (e.g., CIE XYZ).","Our method uses these CCMs to transform predefined illumination colors (i.e., along the Planckian locus) into the test camera's raw space.","The mapped illuminants are encoded into a compact camera fingerprint embedding (CFE) that enables the network to adapt to unseen cameras.","To prevent overfitting due to limited cameras and CCMs during training, we introduce a data augmentation technique that interpolates between cameras and their CCMs.","Experimental results across multiple datasets and backbones show that our method achieves state-of-the-art cross-camera color constancy while remaining lightweight and relying only on data readily available in camera ISPs."],"url":"http://arxiv.org/abs/2504.07959v1"}
{"created":"2025-04-10 17:59:22","title":"Detect Anything 3D in the Wild","abstract":"Despite the success of deep learning in close-set 3D object detection, existing approaches struggle with zero-shot generalization to novel objects and camera configurations. We introduce DetAny3D, a promptable 3D detection foundation model capable of detecting any novel object under arbitrary camera configurations using only monocular inputs. Training a foundation model for 3D detection is fundamentally constrained by the limited availability of annotated 3D data, which motivates DetAny3D to leverage the rich prior knowledge embedded in extensively pre-trained 2D foundation models to compensate for this scarcity. To effectively transfer 2D knowledge to 3D, DetAny3D incorporates two core modules: the 2D Aggregator, which aligns features from different 2D foundation models, and the 3D Interpreter with Zero-Embedding Mapping, which mitigates catastrophic forgetting in 2D-to-3D knowledge transfer. Experimental results validate the strong generalization of our DetAny3D, which not only achieves state-of-the-art performance on unseen categories and novel camera configurations, but also surpasses most competitors on in-domain data.DetAny3D sheds light on the potential of the 3D foundation model for diverse applications in real-world scenarios, e.g., rare object detection in autonomous driving, and demonstrates promise for further exploration of 3D-centric tasks in open-world settings. More visualization results can be found at DetAny3D project page.","sentences":["Despite the success of deep learning in close-set 3D object detection, existing approaches struggle with zero-shot generalization to novel objects and camera configurations.","We introduce DetAny3D, a promptable 3D detection foundation model capable of detecting any novel object under arbitrary camera configurations using only monocular inputs.","Training a foundation model for 3D detection is fundamentally constrained by the limited availability of annotated 3D data, which motivates DetAny3D to leverage the rich prior knowledge embedded in extensively pre-trained 2D foundation models to compensate for this scarcity.","To effectively transfer 2D knowledge to 3D, DetAny3D incorporates two core modules: the 2D Aggregator, which aligns features from different 2D foundation models, and the 3D Interpreter with Zero-Embedding Mapping, which mitigates catastrophic forgetting in 2D-to-3D knowledge transfer.","Experimental results validate the strong generalization of our DetAny3D, which not only achieves state-of-the-art performance on unseen categories and novel camera configurations, but also surpasses most competitors on in-domain data.","DetAny3D sheds light on the potential of the 3D foundation model for diverse applications in real-world scenarios, e.g., rare object detection in autonomous driving, and demonstrates promise for further exploration of 3D-centric tasks in open-world settings.","More visualization results can be found at DetAny3D project page."],"url":"http://arxiv.org/abs/2504.07958v1"}
{"created":"2025-04-10 17:59:12","title":"MM-IFEngine: Towards Multimodal Instruction Following","abstract":"The Instruction Following (IF) ability measures how well Multi-modal Large Language Models (MLLMs) understand exactly what users are telling them and whether they are doing it right. Existing multimodal instruction following training data is scarce, the benchmarks are simple with atomic instructions, and the evaluation strategies are imprecise for tasks demanding exact output constraints. To address this, we present MM-IFEngine, an effective pipeline to generate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields large-scale, diverse, and high-quality training data MM-IFInstruct-23k, which is suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for Direct Preference Optimization (DPO). We further introduce MM-IFEval, a challenging and diverse multi-modal instruction-following benchmark that includes (1) both compose-level constraints for output responses and perception-level constraints tied to the input images, and (2) a comprehensive evaluation pipeline incorporating both rule-based assessment and judge model. We conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on MM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF benchmarks, such as MM-IFEval (+10.2$\\%$), MIA (+7.6$\\%$), and IFEval (+12.3$\\%$). The full data and evaluation code will be released on https://github.com/SYuan03/MM-IFEngine.","sentences":["The Instruction Following (IF) ability measures how well Multi-modal Large Language Models (MLLMs) understand exactly what users are telling them and whether they are doing it right.","Existing multimodal instruction following training data is scarce, the benchmarks are simple with atomic instructions, and the evaluation strategies are imprecise for tasks demanding exact output constraints.","To address this, we present MM-IFEngine, an effective pipeline to generate high-quality image-instruction pairs.","Our MM-IFEngine pipeline yields large-scale, diverse, and high-quality training data MM-IFInstruct-23k, which is suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for Direct Preference Optimization (DPO).","We further introduce MM-IFEval, a challenging and diverse multi-modal instruction-following benchmark that includes (1) both compose-level constraints for output responses and perception-level constraints tied to the input images, and (2) a comprehensive evaluation pipeline incorporating both rule-based assessment and judge model.","We conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on MM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF benchmarks, such as MM-IFEval (+10.2$\\%$), MIA (+7.6$\\%$), and IFEval (+12.3$\\%$).","The full data and evaluation code will be released on https://github.com/SYuan03/MM-IFEngine."],"url":"http://arxiv.org/abs/2504.07957v1"}
{"created":"2025-04-10 17:54:02","title":"GenEAva: Generating Cartoon Avatars with Fine-Grained Facial Expressions from Realistic Diffusion-based Faces","abstract":"Cartoon avatars have been widely used in various applications, including social media, online tutoring, and gaming. However, existing cartoon avatar datasets and generation methods struggle to present highly expressive avatars with fine-grained facial expressions and are often inspired from real-world identities, raising privacy concerns. To address these challenges, we propose a novel framework, GenEAva, for generating high-quality cartoon avatars with fine-grained facial expressions. Our approach fine-tunes a state-of-the-art text-to-image diffusion model to synthesize highly detailed and expressive facial expressions. We then incorporate a stylization model that transforms these realistic faces into cartoon avatars while preserving both identity and expression. Leveraging this framework, we introduce the first expressive cartoon avatar dataset, GenEAva 1.0, specifically designed to capture 135 fine-grained facial expressions, featuring 13,230 expressive cartoon avatars with a balanced distribution across genders, racial groups, and age ranges. We demonstrate that our fine-tuned model generates more expressive faces than the state-of-the-art text-to-image diffusion model SDXL. We also verify that the cartoon avatars generated by our framework do not include memorized identities from fine-tuning data. The proposed framework and dataset provide a diverse and expressive benchmark for future research in cartoon avatar generation.","sentences":["Cartoon avatars have been widely used in various applications, including social media, online tutoring, and gaming.","However, existing cartoon avatar datasets and generation methods struggle to present highly expressive avatars with fine-grained facial expressions and are often inspired from real-world identities, raising privacy concerns.","To address these challenges, we propose a novel framework, GenEAva, for generating high-quality cartoon avatars with fine-grained facial expressions.","Our approach fine-tunes a state-of-the-art text-to-image diffusion model to synthesize highly detailed and expressive facial expressions.","We then incorporate a stylization model that transforms these realistic faces into cartoon avatars while preserving both identity and expression.","Leveraging this framework, we introduce the first expressive cartoon avatar dataset, GenEAva 1.0, specifically designed to capture 135 fine-grained facial expressions, featuring 13,230 expressive cartoon avatars with a balanced distribution across genders, racial groups, and age ranges.","We demonstrate that our fine-tuned model generates more expressive faces than the state-of-the-art text-to-image diffusion model SDXL.","We also verify that the cartoon avatars generated by our framework do not include memorized identities from fine-tuning data.","The proposed framework and dataset provide a diverse and expressive benchmark for future research in cartoon avatar generation."],"url":"http://arxiv.org/abs/2504.07945v1"}
{"created":"2025-04-10 17:53:31","title":"HoloPart: Generative 3D Part Amodal Segmentation","abstract":"3D part amodal segmentation--decomposing a 3D shape into complete, semantically meaningful parts, even when occluded--is a challenging but crucial task for 3D content creation and understanding. Existing 3D part segmentation methods only identify visible surface patches, limiting their utility. Inspired by 2D amodal segmentation, we introduce this novel task to the 3D domain and propose a practical, two-stage approach, addressing the key challenges of inferring occluded 3D geometry, maintaining global shape consistency, and handling diverse shapes with limited training data. First, we leverage existing 3D part segmentation to obtain initial, incomplete part segments. Second, we introduce HoloPart, a novel diffusion-based model, to complete these segments into full 3D parts. HoloPart utilizes a specialized architecture with local attention to capture fine-grained part geometry and global shape context attention to ensure overall shape consistency. We introduce new benchmarks based on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart significantly outperforms state-of-the-art shape completion methods. By incorporating HoloPart with existing segmentation techniques, we achieve promising results on 3D part amodal segmentation, opening new avenues for applications in geometry editing, animation, and material assignment.","sentences":["3D part amodal segmentation--decomposing a 3D shape into complete, semantically meaningful parts, even when occluded--is a challenging but crucial task for 3D content creation and understanding.","Existing 3D part segmentation methods only identify visible surface patches, limiting their utility.","Inspired by 2D amodal segmentation, we introduce this novel task to the 3D domain and propose a practical, two-stage approach, addressing the key challenges of inferring occluded 3D geometry, maintaining global shape consistency, and handling diverse shapes with limited training data.","First, we leverage existing 3D part segmentation to obtain initial, incomplete part segments.","Second, we introduce HoloPart, a novel diffusion-based model, to complete these segments into full 3D parts.","HoloPart utilizes a specialized architecture with local attention to capture fine-grained part geometry and global shape context attention to ensure overall shape consistency.","We introduce new benchmarks based on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart significantly outperforms state-of-the-art shape completion methods.","By incorporating HoloPart with existing segmentation techniques, we achieve promising results on 3D part amodal segmentation, opening new avenues for applications in geometry editing, animation, and material assignment."],"url":"http://arxiv.org/abs/2504.07943v1"}
{"created":"2025-04-10 17:51:38","title":"Beyond the Frame: Generating 360\u00b0 Panoramic Videos from Perspective Videos","abstract":"360{\\deg} videos have emerged as a promising medium to represent our dynamic visual world. Compared to the \"tunnel vision\" of standard cameras, their borderless field of view offers a more complete perspective of our surroundings. While existing video models excel at producing standard videos, their ability to generate full panoramic videos remains elusive. In this paper, we investigate the task of video-to-360{\\deg} generation: given a perspective video as input, our goal is to generate a full panoramic video that is consistent with the original video. Unlike conventional video generation tasks, the output's field of view is significantly larger, and the model is required to have a deep understanding of both the spatial layout of the scene and the dynamics of objects to maintain spatio-temporal consistency. To address these challenges, we first leverage the abundant 360{\\deg} videos available online and develop a high-quality data filtering pipeline to curate pairwise training data. We then carefully design a series of geometry- and motion-aware operations to facilitate the learning process and improve the quality of 360{\\deg} video generation. Experimental results demonstrate that our model can generate realistic and coherent 360{\\deg} videos from in-the-wild perspective video. In addition, we showcase its potential applications, including video stabilization, camera viewpoint control, and interactive visual question answering.","sentences":["360{\\deg} videos have emerged as a promising medium to represent our dynamic visual world.","Compared to the \"tunnel vision\" of standard cameras, their borderless field of view offers a more complete perspective of our surroundings.","While existing video models excel at producing standard videos, their ability to generate full panoramic videos remains elusive.","In this paper, we investigate the task of video-to-360{\\deg} generation: given a perspective video as input, our goal is to generate a full panoramic video that is consistent with the original video.","Unlike conventional video generation tasks, the output's field of view is significantly larger, and the model is required to have a deep understanding of both the spatial layout of the scene and the dynamics of objects to maintain spatio-temporal consistency.","To address these challenges, we first leverage the abundant 360{\\deg} videos available online and develop a high-quality data filtering pipeline to curate pairwise training data.","We then carefully design a series of geometry- and motion-aware operations to facilitate the learning process and improve the quality of 360{\\deg} video generation.","Experimental results demonstrate that our model can generate realistic and coherent 360{\\deg} videos from in-the-wild perspective video.","In addition, we showcase its potential applications, including video stabilization, camera viewpoint control, and interactive visual question answering."],"url":"http://arxiv.org/abs/2504.07940v1"}
{"created":"2025-04-10 17:51:37","title":"Echo: An Open-Source, Low-Cost Teleoperation System with Force Feedback for Dataset Collection in Robot Learning","abstract":"In this article, we propose Echo, a novel joint-matching teleoperation system designed to enhance the collection of datasets for manual and bimanual tasks. Our system is specifically tailored for controlling the UR manipulator and features a custom controller with force feedback and adjustable sensitivity modes, enabling precise and intuitive operation. Additionally, Echo integrates a user-friendly dataset recording interface, simplifying the process of collecting high-quality training data for imitation learning. The system is designed to be reliable, cost-effective, and easily reproducible, making it an accessible tool for researchers, laboratories, and startups passionate about advancing robotics through imitation learning. Although the current implementation focuses on the UR manipulator, Echo architecture is reconfigurable and can be adapted to other manipulators and humanoid systems. We demonstrate the effectiveness of Echo through a series of experiments, showcasing its ability to perform complex bimanual tasks and its potential to accelerate research in the field. We provide assembly instructions, a hardware description, and code at https://eterwait.github.io/Echo/.","sentences":["In this article, we propose Echo, a novel joint-matching teleoperation system designed to enhance the collection of datasets for manual and bimanual tasks.","Our system is specifically tailored for controlling the UR manipulator and features a custom controller with force feedback and adjustable sensitivity modes, enabling precise and intuitive operation.","Additionally, Echo integrates a user-friendly dataset recording interface, simplifying the process of collecting high-quality training data for imitation learning.","The system is designed to be reliable, cost-effective, and easily reproducible, making it an accessible tool for researchers, laboratories, and startups passionate about advancing robotics through imitation learning.","Although the current implementation focuses on the UR manipulator, Echo architecture is reconfigurable and can be adapted to other manipulators and humanoid systems.","We demonstrate the effectiveness of Echo through a series of experiments, showcasing its ability to perform complex bimanual tasks and its potential to accelerate research in the field.","We provide assembly instructions, a hardware description, and code at https://eterwait.github.io/Echo/."],"url":"http://arxiv.org/abs/2504.07939v1"}
{"created":"2025-04-10 17:51:14","title":"Development of a Quantum-Resistant File Transfer System with Blockchain Audit Trail","abstract":"This paper presents a condensed system architecture for a file transfer solution that leverages post quantum cryptography and blockchain to secure data against quantum threats. The architecture integrates NIST standardized algorithms CRYSTALS Kyber for encryption and CRYSTALS Dilithium for digital signatures with an immutable blockchain ledger to provide an auditable, decentralized storage mechanism. The system is modular, comprising a Sender module for secure encryption and signing, a central User Storage module for decryption, reencryption, and blockchain logging, and a Requestor module for authenticated data access. We include detailed pseudocode, analyze security risks, and offer performance insights to demonstrate the system's robustness, scalability, and transparency.","sentences":["This paper presents a condensed system architecture for a file transfer solution that leverages post quantum cryptography and blockchain to secure data against quantum threats.","The architecture integrates NIST standardized algorithms CRYSTALS Kyber for encryption and CRYSTALS Dilithium for digital signatures with an immutable blockchain ledger to provide an auditable, decentralized storage mechanism.","The system is modular, comprising a Sender module for secure encryption and signing, a central User Storage module for decryption, reencryption, and blockchain logging, and a Requestor module for authenticated data access.","We include detailed pseudocode, analyze security risks, and offer performance insights to demonstrate the system's robustness, scalability, and transparency."],"url":"http://arxiv.org/abs/2504.07938v1"}
{"created":"2025-04-10 17:49:05","title":"SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement","abstract":"In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropriately challenging samples can substantially boost reasoning capabilities even when the dataset is small. Despite being intuitive, the main challenge remains in accurately quantifying sample difficulty to enable effective data filtering. To this end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS) to achieve that. Starting from our curated 70k open-source training samples, we introduce an MCTS-based selection method that quantifies sample difficulty based on the number of iterations required by the VLMs to solve each problem. This explicit step-by-step reasoning in MCTS enforces the model to think longer and better identifies samples that are genuinely challenging. We filter and retain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our final model, ThinkLite-VL. Evaluation results on eight benchmarks show that ThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%, using only 11k training samples with no knowledge distillation. This significantly outperforms all existing 7B-level reasoning VLMs, and our fairly comparable baselines that use classic selection methods such as accuracy-based filtering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of 75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are available at https://github.com/si0wang/ThinkLite-VL.","sentences":["In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation.","Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical.","Appropriately challenging samples can substantially boost reasoning capabilities even when the dataset is small.","Despite being intuitive, the main challenge remains in accurately quantifying sample difficulty to enable effective data filtering.","To this end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS) to achieve that.","Starting from our curated 70k open-source training samples, we introduce an MCTS-based selection method that quantifies sample difficulty based on the number of iterations required by the VLMs to solve each problem.","This explicit step-by-step reasoning in MCTS enforces the model to think longer and better identifies samples that are genuinely challenging.","We filter and retain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our final model, ThinkLite-VL.","Evaluation results on eight benchmarks show that ThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%, using only 11k training samples with no knowledge distillation.","This significantly outperforms all existing 7B-level reasoning VLMs, and our fairly comparable baselines that use classic selection methods such as accuracy-based filtering.","Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of 75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1.","Our code, data, and model are available at https://github.com/si0wang/ThinkLite-VL."],"url":"http://arxiv.org/abs/2504.07934v1"}
{"created":"2025-04-10 17:36:23","title":"Directed Temporal Tree Realization for Periodic Public Transport: Easy and Hard Cases","abstract":"We study the complexity of the directed periodic temporal graph realization problem. This work is motivated by the design of periodic schedules in public transport with constraints on the quality of service. Namely, we require that the fastest path between (important) pairs of vertices is upper bounded by a specified maximum duration, encoded in an upper distance matrix $D$. While previous work has considered the undirected version of the problem, the application in public transport schedule design requires the flexibility to assign different departure times to the two directions of an edge. A problem instance can only be feasible if all values of the distance matrix are at least shortest path distances. However, the task of realizing exact fastest path distances in a periodic temporal graph is often too restrictive. Therefore, we introduce a minimum slack parameter $k$ that describes a lower bound on the maximum allowed waiting time on each path. We concentrate on tree topologies and provide a full characterization of the complexity landscape with respect to the period $\\Delta$ and the minimum slack parameter~$k$, showing a sharp threshold between NP-complete cases and cases which are always realizable. We also provide hardness results for the special case of period $\\Delta = 2$ for general directed and undirected graphs.","sentences":["We study the complexity of the directed periodic temporal graph realization problem.","This work is motivated by the design of periodic schedules in public transport with constraints on the quality of service.","Namely, we require that the fastest path between (important) pairs of vertices is upper bounded by a specified maximum duration, encoded in an upper distance matrix $D$. While previous work has considered the undirected version of the problem, the application in public transport schedule design requires the flexibility to assign different departure times to the two directions of an edge.","A problem instance can only be feasible if all values of the distance matrix are at least shortest path distances.","However, the task of realizing exact fastest path distances in a periodic temporal graph is often too restrictive.","Therefore, we introduce a minimum slack parameter $k$ that describes a lower bound on the maximum allowed waiting time on each path.","We concentrate on tree topologies and provide a full characterization of the complexity landscape with respect to the period $\\Delta$ and the minimum slack parameter~$k$, showing a sharp threshold between NP-complete cases and cases which are always realizable.","We also provide hardness results for the special case of period $\\Delta = 2$ for general directed and undirected graphs."],"url":"http://arxiv.org/abs/2504.07920v1"}
{"created":"2025-04-10 17:30:07","title":"Semantically Encoding Activity Labels for Context-Aware Human Activity Recognition","abstract":"Prior work has primarily formulated CA-HAR as a multi-label classification problem, where model inputs are time-series sensor data and target labels are binary encodings representing whether a given activity or context occurs. These CA-HAR methods either predicted each label independently or manually imposed relationships using graphs. However, both strategies often neglect an essential aspect: activity labels have rich semantic relationships. For instance, walking, jogging, and running activities share similar movement patterns but differ in pace and intensity, indicating that they are semantically related. Consequently, prior CA-HAR methods often struggled to accurately capture these inherent and nuanced relationships, particularly on datasets with noisy labels typically used for CA-HAR or situations where the ideal sensor type is unavailable (e.g., recognizing speech without audio sensors). To address this limitation, we propose SEAL, which leverage LMs to encode CA-HAR activity labels to capture semantic relationships. LMs generate vector embeddings that preserve rich semantic information from natural language. Our SEAL approach encodes input-time series sensor data from smart devices and their associated activity and context labels (text) as vector embeddings. During training, SEAL aligns the sensor data representations with their corresponding activity/context label embeddings in a shared embedding space. At inference time, SEAL performs a similarity search, returning the CA-HAR label with the embedding representation closest to the input data. Although LMs have been widely explored in other domains, surprisingly, their potential in CA-HAR has been underexplored, making our approach a novel contribution to the field. Our research opens up new possibilities for integrating more advanced LMs into CA-HAR tasks.","sentences":["Prior work has primarily formulated CA-HAR as a multi-label classification problem, where model inputs are time-series sensor data and target labels are binary encodings representing whether a given activity or context occurs.","These CA-HAR methods either predicted each label independently or manually imposed relationships using graphs.","However, both strategies often neglect an essential aspect: activity labels have rich semantic relationships.","For instance, walking, jogging, and running activities share similar movement patterns but differ in pace and intensity, indicating that they are semantically related.","Consequently, prior CA-HAR methods often struggled to accurately capture these inherent and nuanced relationships, particularly on datasets with noisy labels typically used for CA-HAR or situations where the ideal sensor type is unavailable (e.g., recognizing speech without audio sensors).","To address this limitation, we propose SEAL, which leverage LMs to encode CA-HAR activity labels to capture semantic relationships.","LMs generate vector embeddings that preserve rich semantic information from natural language.","Our SEAL approach encodes input-time series sensor data from smart devices and their associated activity and context labels (text) as vector embeddings.","During training, SEAL aligns the sensor data representations with their corresponding activity/context label embeddings in a shared embedding space.","At inference time, SEAL performs a similarity search, returning the CA-HAR label with the embedding representation closest to the input data.","Although LMs have been widely explored in other domains, surprisingly, their potential in CA-HAR has been underexplored, making our approach a novel contribution to the field.","Our research opens up new possibilities for integrating more advanced LMs into CA-HAR tasks."],"url":"http://arxiv.org/abs/2504.07916v1"}
{"created":"2025-04-10 17:15:53","title":"Echo Chamber: RL Post-training Amplifies Behaviors Learned in Pretraining","abstract":"Reinforcement learning (RL)-based fine-tuning has become a crucial step in post-training language models for advanced mathematical reasoning and coding. Following the success of frontier reasoning models, recent work has demonstrated that RL fine-tuning consistently improves performance, even in smaller-scale models; however, the underlying mechanisms driving these improvements are not well-understood. Understanding the effects of RL fine-tuning requires disentangling its interaction with pretraining data composition, hyperparameters, and model scale, but such problems are exacerbated by the lack of transparency regarding the training data used in many existing models. In this work, we present a systematic end-to-end study of RL fine-tuning for mathematical reasoning by training models entirely from scratch on different mixtures of fully open datasets. We investigate the effects of various RL fine-tuning algorithms (PPO, GRPO, and Expert Iteration) across models of different scales. Our study reveals that RL algorithms consistently converge towards a dominant output distribution, amplifying patterns in the pretraining data. We also find that models of different scales trained on the same data mixture will converge to distinct output distributions, suggesting that there are scale-dependent biases in model generalization. Moreover, we find that RL post-training on simpler questions can lead to performance gains on harder ones, indicating that certain reasoning capabilities generalize across tasks. Our findings show that small-scale proxies in controlled settings can elicit interesting insights regarding the role of RL in shaping language model behavior.","sentences":["Reinforcement learning (RL)-based fine-tuning has become a crucial step in post-training language models for advanced mathematical reasoning and coding.","Following the success of frontier reasoning models, recent work has demonstrated that RL fine-tuning consistently improves performance, even in smaller-scale models; however, the underlying mechanisms driving these improvements are not well-understood.","Understanding the effects of RL fine-tuning requires disentangling its interaction with pretraining data composition, hyperparameters, and model scale, but such problems are exacerbated by the lack of transparency regarding the training data used in many existing models.","In this work, we present a systematic end-to-end study of RL fine-tuning for mathematical reasoning by training models entirely from scratch on different mixtures of fully open datasets.","We investigate the effects of various RL fine-tuning algorithms (PPO, GRPO, and Expert Iteration) across models of different scales.","Our study reveals that RL algorithms consistently converge towards a dominant output distribution, amplifying patterns in the pretraining data.","We also find that models of different scales trained on the same data mixture will converge to distinct output distributions, suggesting that there are scale-dependent biases in model generalization.","Moreover, we find that RL post-training on simpler questions can lead to performance gains on harder ones, indicating that certain reasoning capabilities generalize across tasks.","Our findings show that small-scale proxies in controlled settings can elicit interesting insights regarding the role of RL in shaping language model behavior."],"url":"http://arxiv.org/abs/2504.07912v1"}
{"created":"2025-04-10 17:15:50","title":"The Urban Impact of AI: Modeling Feedback Loops in Next-Venue Recommendation","abstract":"Next-venue recommender systems are increasingly embedded in location-based services, shaping individual mobility decisions in urban environments. While their predictive accuracy has been extensively studied, less attention has been paid to their systemic impact on urban dynamics. In this work, we introduce a simulation framework to model the human-AI feedback loop underpinning next-venue recommendation, capturing how algorithmic suggestions influence individual behavior, which in turn reshapes the data used to retrain the models. Our simulations, grounded in real-world mobility data, systematically explore the effects of algorithmic adoption across a range of recommendation strategies. We find that while recommender systems consistently increase individual-level diversity in visited venues, they may simultaneously amplify collective inequality by concentrating visits on a limited subset of popular places. This divergence extends to the structure of social co-location networks, revealing broader implications for urban accessibility and spatial segregation. Our framework operationalizes the feedback loop in next-venue recommendation and offers a novel lens through which to assess the societal impact of AI-assisted mobility-providing a computational tool to anticipate future risks, evaluate regulatory interventions, and inform the design of ethic algorithmic systems.","sentences":["Next-venue recommender systems are increasingly embedded in location-based services, shaping individual mobility decisions in urban environments.","While their predictive accuracy has been extensively studied, less attention has been paid to their systemic impact on urban dynamics.","In this work, we introduce a simulation framework to model the human-AI feedback loop underpinning next-venue recommendation, capturing how algorithmic suggestions influence individual behavior, which in turn reshapes the data used to retrain the models.","Our simulations, grounded in real-world mobility data, systematically explore the effects of algorithmic adoption across a range of recommendation strategies.","We find that while recommender systems consistently increase individual-level diversity in visited venues, they may simultaneously amplify collective inequality by concentrating visits on a limited subset of popular places.","This divergence extends to the structure of social co-location networks, revealing broader implications for urban accessibility and spatial segregation.","Our framework operationalizes the feedback loop in next-venue recommendation and offers a novel lens through which to assess the societal impact of AI-assisted mobility-providing a computational tool to anticipate future risks, evaluate regulatory interventions, and inform the design of ethic algorithmic systems."],"url":"http://arxiv.org/abs/2504.07911v1"}
{"created":"2025-04-10 16:30:13","title":"Hodge Laplacians and Hodge Diffusion Maps","abstract":"We introduce Hodge Diffusion Maps, a novel manifold learning algorithm designed to analyze and extract topological information from high-dimensional data-sets. This method approximates the exterior derivative acting on differential forms, thereby providing an approximation of the Hodge Laplacian operator. Hodge Diffusion Maps extend existing non-linear dimensionality reduction techniques, including vector diffusion maps, as well as the theories behind diffusion maps and Laplacian Eigenmaps. Our approach captures higher-order topological features of the data-set by projecting it into lower-dimensional Euclidean spaces using the Hodge Laplacian. We develop a theoretical framework to estimate the approximation error of the exterior derivative, based on sample points distributed over a real manifold. Numerical experiments support and validate the proposed methodology.","sentences":["We introduce Hodge Diffusion Maps, a novel manifold learning algorithm designed to analyze and extract topological information from high-dimensional data-sets.","This method approximates the exterior derivative acting on differential forms, thereby providing an approximation of the Hodge Laplacian operator.","Hodge Diffusion Maps extend existing non-linear dimensionality reduction techniques, including vector diffusion maps, as well as the theories behind diffusion maps and Laplacian Eigenmaps.","Our approach captures higher-order topological features of the data-set by projecting it into lower-dimensional Euclidean spaces using the Hodge Laplacian.","We develop a theoretical framework to estimate the approximation error of the exterior derivative, based on sample points distributed over a real manifold.","Numerical experiments support and validate the proposed methodology."],"url":"http://arxiv.org/abs/2504.07910v1"}
{"created":"2025-04-10 16:29:26","title":"Porting an LLM based Application from ChatGPT to an On-Premise Environment","abstract":"Given the data-intensive nature of Machine Learning (ML) systems in general, and Large Language Models (LLM) in particular, using them in cloud based environments can become a challenge due to legislation related to privacy and security of data. Taking such aspects into consideration implies porting the LLMs to an on-premise environment, where privacy and security can be controlled. In this paper, we study this porting process of a real-life application using ChatGPT, which runs in a public cloud, to an on-premise environment. The application being ported is AIPA, a system that leverages Large Language Models (LLMs) and sophisticated data analytics to enhance the assessment of procurement call bids. The main considerations in the porting process include transparency of open source models and cost of hardware, which are central design choices of the on-premise environment. In addition to presenting the porting process, we evaluate downsides and benefits associated with porting.","sentences":["Given the data-intensive nature of Machine Learning (ML) systems in general, and Large Language Models (LLM) in particular, using them in cloud based environments can become a challenge due to legislation related to privacy and security of data.","Taking such aspects into consideration implies porting the LLMs to an on-premise environment, where privacy and security can be controlled.","In this paper, we study this porting process of a real-life application using ChatGPT, which runs in a public cloud, to an on-premise environment.","The application being ported is AIPA, a system that leverages Large Language Models (LLMs) and sophisticated data analytics to enhance the assessment of procurement call bids.","The main considerations in the porting process include transparency of open source models and cost of hardware, which are central design choices of the on-premise environment.","In addition to presenting the porting process, we evaluate downsides and benefits associated with porting."],"url":"http://arxiv.org/abs/2504.07907v1"}
{"created":"2025-04-10 16:24:28","title":"Redefining Machine Translation on Social Network Services with Large Language Models","abstract":"The globalization of social interactions has heightened the need for machine translation (MT) on Social Network Services (SNS), yet traditional models struggle with culturally nuanced content like memes, slang, and pop culture references. While large language models (LLMs) have advanced general-purpose translation, their performance on SNS-specific content remains limited due to insufficient specialized training data and evaluation benchmarks. This paper introduces RedTrans, a 72B LLM tailored for SNS translation, trained on a novel dataset developed through three innovations: (1) Supervised Finetuning with Dual-LLM Back-Translation Sampling, an unsupervised sampling method using LLM-based back-translation to select diverse data for large-scale finetuning; (2) Rewritten Preference Optimization (RePO), an algorithm that identifies and corrects erroneous preference pairs through expert annotation, building reliable preference corpora; and (3) RedTrans-Bench, the first benchmark for SNS translation, evaluating phenomena like humor localization, emoji semantics, and meme adaptation. Experiments show RedTrans outperforms state-of-the-art LLMs. Besides, RedTrans has already been deployed in a real-world production environment, demonstrating that domain-specific adaptation, effectively bridges the gap between generic and culturally grounded translation systems.","sentences":["The globalization of social interactions has heightened the need for machine translation (MT) on Social Network Services (SNS), yet traditional models struggle with culturally nuanced content like memes, slang, and pop culture references.","While large language models (LLMs) have advanced general-purpose translation, their performance on SNS-specific content remains limited due to insufficient specialized training data and evaluation benchmarks.","This paper introduces RedTrans, a 72B LLM tailored for SNS translation, trained on a novel dataset developed through three innovations: (1) Supervised Finetuning with Dual-LLM Back-Translation Sampling, an unsupervised sampling method using LLM-based back-translation to select diverse data for large-scale finetuning; (2) Rewritten Preference Optimization (RePO), an algorithm that identifies and corrects erroneous preference pairs through expert annotation, building reliable preference corpora; and (3) RedTrans-Bench, the first benchmark for SNS translation, evaluating phenomena like humor localization, emoji semantics, and meme adaptation.","Experiments show RedTrans outperforms state-of-the-art LLMs.","Besides, RedTrans has already been deployed in a real-world production environment, demonstrating that domain-specific adaptation, effectively bridges the gap between generic and culturally grounded translation systems."],"url":"http://arxiv.org/abs/2504.07901v1"}
{"created":"2025-04-10 16:00:59","title":"Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge","abstract":"Large Language Models (LLMs) have revolutionized artificial intelligence, driving advancements in machine translation, summarization, and conversational agents. However, their increasing integration into critical societal domains has raised concerns about embedded biases, which can perpetuate stereotypes and compromise fairness. These biases stem from various sources, including historical inequalities in training data, linguistic imbalances, and adversarial manipulation. Despite mitigation efforts, recent studies indicate that LLMs remain vulnerable to adversarial attacks designed to elicit biased responses. This work proposes a scalable benchmarking framework to evaluate LLM robustness against adversarial bias elicitation. Our methodology involves (i) systematically probing models with a multi-task approach targeting biases across various sociocultural dimensions, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach for automated assessment of model responses, and (iii) employing jailbreak techniques to investigate vulnerabilities in safety mechanisms. Our analysis examines prevalent biases in both small and large state-of-the-art models and their impact on model safety. Additionally, we assess the safety of domain-specific models fine-tuned for critical fields, such as medicine. Finally, we release a curated dataset of bias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability benchmarking. Our findings reveal critical trade-offs between model size and safety, aiding the development of fairer and more robust future language models.","sentences":["Large Language Models (LLMs) have revolutionized artificial intelligence, driving advancements in machine translation, summarization, and conversational agents.","However, their increasing integration into critical societal domains has raised concerns about embedded biases, which can perpetuate stereotypes and compromise fairness.","These biases stem from various sources, including historical inequalities in training data, linguistic imbalances, and adversarial manipulation.","Despite mitigation efforts, recent studies indicate that LLMs remain vulnerable to adversarial attacks designed to elicit biased responses.","This work proposes a scalable benchmarking framework to evaluate LLM robustness against adversarial bias elicitation.","Our methodology involves (i) systematically probing models with a multi-task approach targeting biases across various sociocultural dimensions, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach for automated assessment of model responses, and (iii) employing jailbreak techniques to investigate vulnerabilities in safety mechanisms.","Our analysis examines prevalent biases in both small and large state-of-the-art models and their impact on model safety.","Additionally, we assess the safety of domain-specific models fine-tuned for critical fields, such as medicine.","Finally, we release a curated dataset of bias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability benchmarking.","Our findings reveal critical trade-offs between model size and safety, aiding the development of fairer and more robust future language models."],"url":"http://arxiv.org/abs/2504.07887v1"}
{"created":"2025-04-10 15:45:07","title":"Open Datasets for Grid Modeling and Visualization: An Alberta Power Network Case","abstract":"In the power and energy industry, multiple entities in grid operational logs are frequently recorded and updated. Thanks to recent advances in IT facilities and smart metering services, a variety of datasets such as system load, generation mix, and grid connection are often publicly available. While these resources are valuable in evaluating power grid's operational conditions and system resilience, the lack of fine-grained, accurate locational information constrain the usage of current data, which further hinders the development of smart grid and renewables integration. For instance, electricity end users are not aware of nodal generation mix or carbon emissions, while the general public have limited understanding about the effect of demand response or renewables integration if only the whole system's demands and generations are available. In this work, we focus on recovering power grid topology and line flow directions from open public dataset. Taking the Alberta grid as a working example, we start from mapping multi-modal power system datasets to the grid topology integrated with geographical information. By designing a novel optimization-based scheme to recover line flow directions, we are able to analyze and visualize the interactions between generations and demand vectors in an efficient manner. Proposed research is fully open-sourced and highly generalizable, which can help model and visualize grid information, create synthetic dataset, and facilitate analytics and decision-making framework for clean energy transition.","sentences":["In the power and energy industry, multiple entities in grid operational logs are frequently recorded and updated.","Thanks to recent advances in IT facilities and smart metering services, a variety of datasets such as system load, generation mix, and grid connection are often publicly available.","While these resources are valuable in evaluating power grid's operational conditions and system resilience, the lack of fine-grained, accurate locational information constrain the usage of current data, which further hinders the development of smart grid and renewables integration.","For instance, electricity end users are not aware of nodal generation mix or carbon emissions, while the general public have limited understanding about the effect of demand response or renewables integration if only the whole system's demands and generations are available.","In this work, we focus on recovering power grid topology and line flow directions from open public dataset.","Taking the Alberta grid as a working example, we start from mapping multi-modal power system datasets to the grid topology integrated with geographical information.","By designing a novel optimization-based scheme to recover line flow directions, we are able to analyze and visualize the interactions between generations and demand vectors in an efficient manner.","Proposed research is fully open-sourced and highly generalizable, which can help model and visualize grid information, create synthetic dataset, and facilitate analytics and decision-making framework for clean energy transition."],"url":"http://arxiv.org/abs/2504.07870v1"}
{"created":"2025-04-10 15:32:57","title":"Empowering Global Voices: A Data-Efficient, Phoneme-Tone Adaptive Approach to High-Fidelity Speech Synthesis","abstract":"Text-to-speech (TTS) technology has achieved impressive results for widely spoken languages, yet many under-resourced languages remain challenged by limited data and linguistic complexities. In this paper, we present a novel methodology that integrates a data-optimized framework with an advanced acoustic model to build high-quality TTS systems for low-resource scenarios. We demonstrate the effectiveness of our approach using Thai as an illustrative case, where intricate phonetic rules and sparse resources are effectively addressed. Our method enables zero-shot voice cloning and improved performance across diverse client applications, ranging from finance to healthcare, education, and law. Extensive evaluations - both subjective and objective - confirm that our model meets state-of-the-art standards, offering a scalable solution for TTS production in data-limited settings, with significant implications for broader industry adoption and multilingual accessibility.","sentences":["Text-to-speech (TTS) technology has achieved impressive results for widely spoken languages, yet many under-resourced languages remain challenged by limited data and linguistic complexities.","In this paper, we present a novel methodology that integrates a data-optimized framework with an advanced acoustic model to build high-quality TTS systems for low-resource scenarios.","We demonstrate the effectiveness of our approach using Thai as an illustrative case, where intricate phonetic rules and sparse resources are effectively addressed.","Our method enables zero-shot voice cloning and improved performance across diverse client applications, ranging from finance to healthcare, education, and law.","Extensive evaluations - both subjective and objective - confirm that our model meets state-of-the-art standards, offering a scalable solution for TTS production in data-limited settings, with significant implications for broader industry adoption and multilingual accessibility."],"url":"http://arxiv.org/abs/2504.07858v1"}
{"created":"2025-04-10 15:31:17","title":"The KL3M Data Project: Copyright-Clean Training Resources for Large Language Models","abstract":"Practically all large language models have been pre-trained on data that is subject to global uncertainty related to copyright infringement and breach of contract. This creates potential risk for users and developers due to this uncertain legal status. The KL3M Data Project directly confronts this critical issue by introducing the largest comprehensive training data pipeline that minimizes risks related to copyright or breach of contract. The foundation of this project is a corpus of over 132 million documents and trillions of tokens spanning 16 different sources that have been verified to meet the strict copyright and licensing protocol detailed herein. We are releasing the entire pipeline, including 1) the source code to acquire and process these documents, 2) the original document formats with associated provenance and metadata, 3) extracted content in a standardized format, 4) pre-tokenized representations of the documents, and 5) various mid- and post-train resources such as question-answer, summarization, conversion, drafting, classification, prediction, and conversational data. All of these resources are freely available to the public on S3, Hugging Face, and GitHub under CC-BY terms. We are committed to continuing this project in furtherance of a more ethical, legal, and sustainable approach to the development and use of AI models.","sentences":["Practically all large language models have been pre-trained on data that is subject to global uncertainty related to copyright infringement and breach of contract.","This creates potential risk for users and developers due to this uncertain legal status.","The KL3M Data Project directly confronts this critical issue by introducing the largest comprehensive training data pipeline that minimizes risks related to copyright or breach of contract.","The foundation of this project is a corpus of over 132 million documents and trillions of tokens spanning 16 different sources that have been verified to meet the strict copyright and licensing protocol detailed herein.","We are releasing the entire pipeline, including 1) the source code to acquire and process these documents, 2) the original document formats with associated provenance and metadata, 3) extracted content in a standardized format, 4) pre-tokenized representations of the documents, and 5) various mid- and post-train resources such as question-answer, summarization, conversion, drafting, classification, prediction, and conversational data.","All of these resources are freely available to the public on S3, Hugging Face, and GitHub under CC-BY terms.","We are committed to continuing this project in furtherance of a more ethical, legal, and sustainable approach to the development and use of AI models."],"url":"http://arxiv.org/abs/2504.07854v1"}
{"created":"2025-04-10 15:29:26","title":"V2V3D: View-to-View Denoised 3D Reconstruction for Light-Field Microscopy","abstract":"Light field microscopy (LFM) has gained significant attention due to its ability to capture snapshot-based, large-scale 3D fluorescence images. However, existing LFM reconstruction algorithms are highly sensitive to sensor noise or require hard-to-get ground-truth annotated data for training. To address these challenges, this paper introduces V2V3D, an unsupervised view2view-based framework that establishes a new paradigm for joint optimization of image denoising and 3D reconstruction in a unified architecture. We assume that the LF images are derived from a consistent 3D signal, with the noise in each view being independent. This enables V2V3D to incorporate the principle of noise2noise for effective denoising. To enhance the recovery of high-frequency details, we propose a novel wave-optics-based feature alignment technique, which transforms the point spread function, used for forward propagation in wave optics, into convolution kernels specifically designed for feature alignment. Moreover, we introduce an LFM dataset containing LF images and their corresponding 3D intensity volumes. Extensive experiments demonstrate that our approach achieves high computational efficiency and outperforms the other state-of-the-art methods. These advancements position V2V3D as a promising solution for 3D imaging under challenging conditions.","sentences":["Light field microscopy (LFM) has gained significant attention due to its ability to capture snapshot-based, large-scale 3D fluorescence images.","However, existing LFM reconstruction algorithms are highly sensitive to sensor noise or require hard-to-get ground-truth annotated data for training.","To address these challenges, this paper introduces V2V3D, an unsupervised view2view-based framework that establishes a new paradigm for joint optimization of image denoising and 3D reconstruction in a unified architecture.","We assume that the LF images are derived from a consistent 3D signal, with the noise in each view being independent.","This enables V2V3D to incorporate the principle of noise2noise for effective denoising.","To enhance the recovery of high-frequency details, we propose a novel wave-optics-based feature alignment technique, which transforms the point spread function, used for forward propagation in wave optics, into convolution kernels specifically designed for feature alignment.","Moreover, we introduce an LFM dataset containing LF images and their corresponding 3D intensity volumes.","Extensive experiments demonstrate that our approach achieves high computational efficiency and outperforms the other state-of-the-art methods.","These advancements position V2V3D as a promising solution for 3D imaging under challenging conditions."],"url":"http://arxiv.org/abs/2504.07853v1"}
{"created":"2025-04-10 15:23:30","title":"Experimental Analysis of Quadcopter Drone Hover Constraints for Localization Improvements","abstract":"In this work, we evaluate the use of aerial drone hover constraints in a multisensor fusion of ground robot and drone data to improve the localization performance of a drone. In particular, we build upon our prior work on cooperative localization between an aerial drone and ground robot that fuses data from LiDAR, inertial navigation, peer-to-peer ranging, altimeter, and stereo-vision and evaluate the incorporation knowledge from the autopilot regarding when the drone is hovering. This control command data is leveraged to add constraints on the velocity state. Hover constraints can be considered important dynamic model information, such as the exploitation of zero-velocity updates in pedestrian navigation. We analyze the benefits of these constraints using an incremental factor graph optimization. Experimental data collected in a motion capture faculty is used to provide performance insights and assess the benefits of hover constraints.","sentences":["In this work, we evaluate the use of aerial drone hover constraints in a multisensor fusion of ground robot and drone data to improve the localization performance of a drone.","In particular, we build upon our prior work on cooperative localization between an aerial drone and ground robot that fuses data from LiDAR, inertial navigation, peer-to-peer ranging, altimeter, and stereo-vision and evaluate the incorporation knowledge from the autopilot regarding when the drone is hovering.","This control command data is leveraged to add constraints on the velocity state.","Hover constraints can be considered important dynamic model information, such as the exploitation of zero-velocity updates in pedestrian navigation.","We analyze the benefits of these constraints using an incremental factor graph optimization.","Experimental data collected in a motion capture faculty is used to provide performance insights and assess the benefits of hover constraints."],"url":"http://arxiv.org/abs/2504.07843v1"}
{"created":"2025-04-10 15:18:56","title":"Deep Learning-based Intrusion Detection Systems: A Survey","abstract":"Intrusion Detection Systems (IDS) have long been a hot topic in the cybersecurity community. In recent years, with the introduction of deep learning (DL) techniques, IDS have made great progress due to their increasing generalizability. The rationale behind this is that by learning the underlying patterns of known system behaviors, IDS detection can be generalized to intrusions that exploit zero-day vulnerabilities. In this survey, we refer to this type of IDS as DL-based IDS (DL-IDS). From the perspective of DL, this survey systematically reviews all the stages of DL-IDS, including data collection, log storage, log parsing, graph summarization, attack detection, and attack investigation. To accommodate current researchers, a section describing the publicly available benchmark datasets is included. This survey further discusses current challenges and potential future research directions, aiming to help researchers understand the basic ideas and visions of DL-IDS research, as well as to motivate their research interests.","sentences":["Intrusion Detection Systems (IDS) have long been a hot topic in the cybersecurity community.","In recent years, with the introduction of deep learning (DL) techniques, IDS have made great progress due to their increasing generalizability.","The rationale behind this is that by learning the underlying patterns of known system behaviors, IDS detection can be generalized to intrusions that exploit zero-day vulnerabilities.","In this survey, we refer to this type of IDS as DL-based IDS (DL-IDS).","From the perspective of DL, this survey systematically reviews all the stages of DL-IDS, including data collection, log storage, log parsing, graph summarization, attack detection, and attack investigation.","To accommodate current researchers, a section describing the publicly available benchmark datasets is included.","This survey further discusses current challenges and potential future research directions, aiming to help researchers understand the basic ideas and visions of DL-IDS research, as well as to motivate their research interests."],"url":"http://arxiv.org/abs/2504.07839v1"}
{"created":"2025-04-10 15:06:11","title":"A Hybrid Semantic RAN Protocol Stack Design for 6G System and Its Implementation","abstract":"Recently, Semantic Communication (SC) has been recognized as a crucial new paradigm in 6G, significantly improving information transmission efficiency. However, the diverse range of service types in 6G networks, such as high-data-volume services like AR/VR/MR and low-data-volume applications requiring high accuracy, such as industrial control and data collection, presents significant challenges to fully replacing the fundamental technologies with SC. Therefore, we design a Hybrid Semantic Communication Ratio Access Network (HSC-RAN) protocol stack demo for 6G systems to achieve compatibility and smooth transition between SC and non-SC. Specifically, we take the Physical Downlink Shared Channel (PDSCH) as an example, to efficiently integrate SC with Orthogonal Frequency Division Multiplexing (OFDM). Furthermore, we introduce a novel Downlink Control Information (DCI) format that jointly supports SC and non-SC, enabling real-time video transmission via SC and text transmission through non-SC. Experimental results demonstrate that our approach allows simultaneous transmission of semantic and non-semantic information while maintaining high-quality reconstruction at the receiver.","sentences":["Recently, Semantic Communication (SC) has been recognized as a crucial new paradigm in 6G, significantly improving information transmission efficiency.","However, the diverse range of service types in 6G networks, such as high-data-volume services like AR/VR/MR and low-data-volume applications requiring high accuracy, such as industrial control and data collection, presents significant challenges to fully replacing the fundamental technologies with SC.","Therefore, we design a Hybrid Semantic Communication Ratio Access Network (HSC-RAN) protocol stack demo for 6G systems to achieve compatibility and smooth transition between SC and non-SC.","Specifically, we take the Physical Downlink Shared Channel (PDSCH) as an example, to efficiently integrate SC with Orthogonal Frequency Division Multiplexing (OFDM).","Furthermore, we introduce a novel Downlink Control Information (DCI) format that jointly supports SC and non-SC, enabling real-time video transmission via SC and text transmission through non-SC.","Experimental results demonstrate that our approach allows simultaneous transmission of semantic and non-semantic information while maintaining high-quality reconstruction at the receiver."],"url":"http://arxiv.org/abs/2504.07829v1"}
{"created":"2025-04-10 15:00:20","title":"DG-STMTL: A Novel Graph Convolutional Network for Multi-Task Spatio-Temporal Traffic Forecasting","abstract":"Spatio-temporal traffic prediction is crucial in intelligent transportation systems. The key challenge of accurate prediction is how to model the complex spatio-temporal dependencies and adapt to the inherent dynamics in data. Traditional Graph Convolutional Networks (GCNs) often struggle with static adjacency matrices that introduce domain bias or learnable matrices that may be overfitting to specific patterns. This challenge becomes more complex when considering Multi-Task Learning (MTL). While MTL has the potential to enhance prediction accuracy through task synergies, it can also face significant hurdles due to task interference. To overcome these challenges, this study introduces a novel MTL framework, Dynamic Group-wise Spatio-Temporal Multi-Task Learning (DG-STMTL). DG-STMTL proposes a hybrid adjacency matrix generation module that combines static matrices with dynamic ones through a task-specific gating mechanism. We also introduce a group-wise GCN module to enhance the modelling capability of spatio-temporal dependencies. We conduct extensive experiments on two real-world datasets to evaluate our method. Results show that our method outperforms other state-of-the-arts, indicating its effectiveness and robustness.","sentences":["Spatio-temporal traffic prediction is crucial in intelligent transportation systems.","The key challenge of accurate prediction is how to model the complex spatio-temporal dependencies and adapt to the inherent dynamics in data.","Traditional Graph Convolutional Networks (GCNs) often struggle with static adjacency matrices that introduce domain bias or learnable matrices that may be overfitting to specific patterns.","This challenge becomes more complex when considering Multi-Task Learning (MTL).","While MTL has the potential to enhance prediction accuracy through task synergies, it can also face significant hurdles due to task interference.","To overcome these challenges, this study introduces a novel MTL framework, Dynamic Group-wise Spatio-Temporal Multi-Task Learning (DG-STMTL).","DG-STMTL proposes a hybrid adjacency matrix generation module that combines static matrices with dynamic ones through a task-specific gating mechanism.","We also introduce a group-wise GCN module to enhance the modelling capability of spatio-temporal dependencies.","We conduct extensive experiments on two real-world datasets to evaluate our method.","Results show that our method outperforms other state-of-the-arts, indicating its effectiveness and robustness."],"url":"http://arxiv.org/abs/2504.07822v1"}
{"created":"2025-04-10 14:52:03","title":"Siren Federate: Bridging document, relational, and graph models for exploratory graph analysis","abstract":"Investigative workflows require interactive exploratory analysis on large heterogeneous knowledge graphs. Current databases show limitations in enabling such task. This paper discusses the architecture of Siren Federate, a system that efficiently supports exploratory graph analysis by bridging document-oriented, relational and graph models. Technical contributions include distributed join algorithms, adaptive query planning, query plan folding, semantic caching, and semi-join decomposition for path query. Semi-join decomposition addresses the exponential growth of intermediate results in path-based queries. Experiments show that Siren Federate exhibits low latency and scales well with the amount of data, the number of users, and the number of computing nodes.","sentences":["Investigative workflows require interactive exploratory analysis on large heterogeneous knowledge graphs.","Current databases show limitations in enabling such task.","This paper discusses the architecture of Siren Federate, a system that efficiently supports exploratory graph analysis by bridging document-oriented, relational and graph models.","Technical contributions include distributed join algorithms, adaptive query planning, query plan folding, semantic caching, and semi-join decomposition for path query.","Semi-join decomposition addresses the exponential growth of intermediate results in path-based queries.","Experiments show that Siren Federate exhibits low latency and scales well with the amount of data, the number of users, and the number of computing nodes."],"url":"http://arxiv.org/abs/2504.07815v1"}
{"created":"2025-04-10 14:30:41","title":"Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling Representations","abstract":"Out-of-distribution (OOD) detection is critical for ensuring the reliability of deep learning systems, particularly in safety-critical applications. Likelihood-based deep generative models have historically faced criticism for their unsatisfactory performance in OOD detection, often assigning higher likelihood to OOD data than in-distribution samples when applied to image data. In this work, we demonstrate that likelihood is not inherently flawed. Rather, several properties in the images space prohibit likelihood as a valid detection score. Given a sufficiently good likelihood estimator, specifically using the probability flow formulation of a diffusion model, we show that likelihood-based methods can still perform on par with state-of-the-art methods when applied in the representation space of pre-trained encoders. The code of our work can be found at $\\href{https://github.com/limchaos/Likelihood-OOD.git}{\\texttt{https://github.com/limchaos/Likelihood-OOD.git}}$.","sentences":["Out-of-distribution (OOD) detection is critical for ensuring the reliability of deep learning systems, particularly in safety-critical applications.","Likelihood-based deep generative models have historically faced criticism for their unsatisfactory performance in OOD detection, often assigning higher likelihood to OOD data than in-distribution samples when applied to image data.","In this work, we demonstrate that likelihood is not inherently flawed.","Rather, several properties in the images space prohibit likelihood as a valid detection score.","Given a sufficiently good likelihood estimator, specifically using the probability flow formulation of a diffusion model, we show that likelihood-based methods can still perform on par with state-of-the-art methods when applied in the representation space of pre-trained encoders.","The code of our work can be found at $\\href{https://github.com/limchaos/Likelihood-OOD.git}{\\texttt{https://github.com/limchaos/Likelihood-OOD.git}}$."],"url":"http://arxiv.org/abs/2504.07793v1"}
{"created":"2025-04-10 14:23:06","title":"Fairness Mediator: Neutralize Stereotype Associations to Mitigate Bias in Large Language Models","abstract":"LLMs have demonstrated remarkable performance across diverse applications, yet they inadvertently absorb spurious correlations from training data, leading to stereotype associations between biased concepts and specific social groups. These associations perpetuate and even amplify harmful social biases, raising significant fairness concerns. To mitigate such biases, prior studies have attempted to project model embeddings into unbiased spaces during inference. However, these approaches have shown limited effectiveness due to their weak alignment with downstream social biases. Inspired by the observation that concept cognition in LLMs is primarily represented through a linear associative memory mechanism, where key-value mapping occurs in the MLP layers, we posited that biased concepts and social groups are similarly encoded as entity (key) and information (value) pairs, which can be manipulated to promote fairer associations. To this end, we propose Fairness Mediator (FairMed), a bias mitigation framework that neutralizes stereotype associations. Our framework comprises two main components: a stereotype association prober and an adversarial debiasing neutralizer. The prober captures stereotype associations encoded within MLP layer activations by employing prompts centered around biased concepts to detect the emission probabilities for social groups. Subsequently, the adversarial debiasing neutralizer intervenes in MLP activations during inference to equalize the association probabilities among different social groups. Extensive experiments across nine protected attributes show that FairMed significantly outperforms SOTA methods in effectiveness. Compared to the most effective baseline, FairMed presents competitive efficiency by cutting mitigation overhead by hundreds of minutes. FairMed also maintains the LLM's language understanding capabilities without compromising overall performance.","sentences":["LLMs have demonstrated remarkable performance across diverse applications, yet they inadvertently absorb spurious correlations from training data, leading to stereotype associations between biased concepts and specific social groups.","These associations perpetuate and even amplify harmful social biases, raising significant fairness concerns.","To mitigate such biases, prior studies have attempted to project model embeddings into unbiased spaces during inference.","However, these approaches have shown limited effectiveness due to their weak alignment with downstream social biases.","Inspired by the observation that concept cognition in LLMs is primarily represented through a linear associative memory mechanism, where key-value mapping occurs in the MLP layers, we posited that biased concepts and social groups are similarly encoded as entity (key) and information (value) pairs, which can be manipulated to promote fairer associations.","To this end, we propose Fairness Mediator (FairMed), a bias mitigation framework that neutralizes stereotype associations.","Our framework comprises two main components: a stereotype association prober and an adversarial debiasing neutralizer.","The prober captures stereotype associations encoded within MLP layer activations by employing prompts centered around biased concepts to detect the emission probabilities for social groups.","Subsequently, the adversarial debiasing neutralizer intervenes in MLP activations during inference to equalize the association probabilities among different social groups.","Extensive experiments across nine protected attributes show that FairMed significantly outperforms SOTA methods in effectiveness.","Compared to the most effective baseline, FairMed presents competitive efficiency by cutting mitigation overhead by hundreds of minutes.","FairMed also maintains the LLM's language understanding capabilities without compromising overall performance."],"url":"http://arxiv.org/abs/2504.07787v1"}
{"created":"2025-04-10 14:22:15","title":"Towards Micro-Action Recognition with Limited Annotations: An Asynchronous Pseudo Labeling and Training Approach","abstract":"Micro-Action Recognition (MAR) aims to classify subtle human actions in video. However, annotating MAR datasets is particularly challenging due to the subtlety of actions. To this end, we introduce the setting of Semi-Supervised MAR (SSMAR), where only a part of samples are labeled. We first evaluate traditional Semi-Supervised Learning (SSL) methods to SSMAR and find that these methods tend to overfit on inaccurate pseudo-labels, leading to error accumulation and degraded performance. This issue primarily arises from the common practice of directly using the predictions of classifier as pseudo-labels to train the model. To solve this issue, we propose a novel framework, called Asynchronous Pseudo Labeling and Training (APLT), which explicitly separates the pseudo-labeling process from model training. Specifically, we introduce a semi-supervised clustering method during the offline pseudo-labeling phase to generate more accurate pseudo-labels. Moreover, a self-adaptive thresholding strategy is proposed to dynamically filter noisy labels of different classes. We then build a memory-based prototype classifier based on the filtered pseudo-labels, which is fixed and used to guide the subsequent model training phase. By alternating the two pseudo-labeling and model training phases in an asynchronous manner, the model can not only be learned with more accurate pseudo-labels but also avoid the overfitting issue. Experiments on three MAR datasets show that our APLT largely outperforms state-of-the-art SSL methods. For instance, APLT improves accuracy by 14.5\\% over FixMatch on the MA-12 dataset when using only 50\\% labeled data. Code will be publicly available.","sentences":["Micro-Action Recognition (MAR) aims to classify subtle human actions in video.","However, annotating MAR datasets is particularly challenging due to the subtlety of actions.","To this end, we introduce the setting of Semi-Supervised MAR (SSMAR), where only a part of samples are labeled.","We first evaluate traditional Semi-Supervised Learning (SSL) methods to SSMAR and find that these methods tend to overfit on inaccurate pseudo-labels, leading to error accumulation and degraded performance.","This issue primarily arises from the common practice of directly using the predictions of classifier as pseudo-labels to train the model.","To solve this issue, we propose a novel framework, called Asynchronous Pseudo Labeling and Training (APLT), which explicitly separates the pseudo-labeling process from model training.","Specifically, we introduce a semi-supervised clustering method during the offline pseudo-labeling phase to generate more accurate pseudo-labels.","Moreover, a self-adaptive thresholding strategy is proposed to dynamically filter noisy labels of different classes.","We then build a memory-based prototype classifier based on the filtered pseudo-labels, which is fixed and used to guide the subsequent model training phase.","By alternating the two pseudo-labeling and model training phases in an asynchronous manner, the model can not only be learned with more accurate pseudo-labels but also avoid the overfitting issue.","Experiments on three MAR datasets show that our APLT largely outperforms state-of-the-art SSL methods.","For instance, APLT improves accuracy by 14.5\\% over FixMatch on the MA-12 dataset when using only 50\\% labeled data.","Code will be publicly available."],"url":"http://arxiv.org/abs/2504.07785v1"}
{"created":"2025-04-10 14:03:40","title":"Data over dialogue: Why artificial intelligence is unlikely to humanise medicine","abstract":"Recently, a growing number of experts in artificial intelligence (AI) and medicine have be-gun to suggest that the use of AI systems, particularly machine learning (ML) systems, is likely to humanise the practice of medicine by substantially improving the quality of clinician-patient relationships. In this thesis, however, I argue that medical ML systems are more likely to negatively impact these relationships than to improve them. In particular, I argue that the use of medical ML systems is likely to comprise the quality of trust, care, empathy, understanding, and communication between clinicians and patients.","sentences":["Recently, a growing number of experts in artificial intelligence (AI) and medicine have be-gun to suggest that the use of AI systems, particularly machine learning (ML) systems, is likely to humanise the practice of medicine by substantially improving the quality of clinician-patient relationships.","In this thesis, however, I argue that medical ML systems are more likely to negatively impact these relationships than to improve them.","In particular, I argue that the use of medical ML systems is likely to comprise the quality of trust, care, empathy, understanding, and communication between clinicians and patients."],"url":"http://arxiv.org/abs/2504.07763v1"}
{"created":"2025-04-10 14:01:22","title":"Exploring a Patch-Wise Approach for Privacy-Preserving Fake ID Detection","abstract":"In an increasingly digitalized world, verifying the authenticity of ID documents has become a critical challenge for real-life applications such as digital banking, crypto-exchanges, renting, etc. This study focuses on the topic of fake ID detection, covering several limitations in the field. In particular, no publicly available data from real ID documents exists, and most studies rely on proprietary in-house databases that are not available due to privacy reasons. In order to shed some light on this critical challenge that makes difficult to advance in the field, we explore a trade-off between privacy (i.e., amount of sensitive data available) and performance, proposing a novel patch-wise approach for privacy-preserving fake ID detection. Our proposed approach explores how privacy can be enhanced through: i) two levels of anonymization for an ID document (i.e., fully- and pseudo-anonymized), and ii) different patch size configurations, varying the amount of sensitive data visible in the patch image. Also, state-of-the-art methods such as Vision Transformers and Foundation Models are considered in the analysis. The experimental framework shows that, on an unseen database (DLC-2021), our proposal achieves 13.91% and 0% EERs at patch and ID document level, showing a good generalization to other databases. In addition to this exploration, another key contribution of our study is the release of the first publicly available database that contains 48,400 patches from both real and fake ID documents, along with the experimental framework and models, which will be available in our GitHub.","sentences":["In an increasingly digitalized world, verifying the authenticity of ID documents has become a critical challenge for real-life applications such as digital banking, crypto-exchanges, renting, etc.","This study focuses on the topic of fake ID detection, covering several limitations in the field.","In particular, no publicly available data from real ID documents exists, and most studies rely on proprietary in-house databases that are not available due to privacy reasons.","In order to shed some light on this critical challenge that makes difficult to advance in the field, we explore a trade-off between privacy (i.e., amount of sensitive data available) and performance, proposing a novel patch-wise approach for privacy-preserving fake ID detection.","Our proposed approach explores how privacy can be enhanced through: i) two levels of anonymization for an ID document (i.e., fully- and pseudo-anonymized), and ii) different patch size configurations, varying the amount of sensitive data visible in the patch image.","Also, state-of-the-art methods such as Vision Transformers and Foundation Models are considered in the analysis.","The experimental framework shows that, on an unseen database (DLC-2021), our proposal achieves 13.91% and 0% EERs at patch and ID document level, showing a good generalization to other databases.","In addition to this exploration, another key contribution of our study is the release of the first publicly available database that contains 48,400 patches from both real and fake ID documents, along with the experimental framework and models, which will be available in our GitHub."],"url":"http://arxiv.org/abs/2504.07761v1"}
{"created":"2025-04-10 13:56:33","title":"PIDSR:ComplementaryPolarizedImageDemosaicingandSuper-Resolution","abstract":"Polarization cameras can capture multiple polarized images with different polarizer angles in a single shot, bringing convenience to polarization-based downstream tasks. However, their direct outputs are color-polarization filter array (CPFA) raw images, requiring demosaicing to reconstruct full-resolution, full-color polarized images; unfortunately, this necessary step introduces artifacts that make polarization-related parameters such as the degree of polarization (DoP) and angle of polarization (AoP) prone to error. Besides, limited by the hardware design, the resolution of a polarization camera is often much lower than that of a conventional RGB camera. Existing polarized image demosaicing (PID) methods are limited in that they cannot enhance resolution, while polarized image super-resolution (PISR) methods, though designed to obtain high-resolution (HR) polarized images from the demosaicing results, tend to retain or even amplify errors in the DoP and AoP introduced by demosaicing artifacts. In this paper, we propose PIDSR, a joint framework that performs complementary Polarized Image Demosaicing and Super-Resolution, showing the ability to robustly obtain high-quality HR polarized images with more accurate DoP and AoP from a CPFA raw image in a direct manner. Experiments show our PIDSR not only achieves state-of-the-art performance on both synthetic and real data, but also facilitates downstream tasks.","sentences":["Polarization cameras can capture multiple polarized images with different polarizer angles in a single shot, bringing convenience to polarization-based downstream tasks.","However, their direct outputs are color-polarization filter array (CPFA) raw images, requiring demosaicing to reconstruct full-resolution, full-color polarized images; unfortunately, this necessary step introduces artifacts that make polarization-related parameters such as the degree of polarization (DoP) and angle of polarization (AoP) prone to error.","Besides, limited by the hardware design, the resolution of a polarization camera is often much lower than that of a conventional RGB camera.","Existing polarized image demosaicing (PID) methods are limited in that they cannot enhance resolution, while polarized image super-resolution (PISR) methods, though designed to obtain high-resolution (HR) polarized images from the demosaicing results, tend to retain or even amplify errors in the DoP and AoP introduced by demosaicing artifacts.","In this paper, we propose PIDSR, a joint framework that performs complementary Polarized Image Demosaicing and Super-Resolution, showing the ability to robustly obtain high-quality HR polarized images with more accurate DoP and AoP from a CPFA raw image in a direct manner.","Experiments show our PIDSR not only achieves state-of-the-art performance on both synthetic and real data, but also facilitates downstream tasks."],"url":"http://arxiv.org/abs/2504.07758v1"}
{"created":"2025-04-10 13:54:36","title":"Efficient Tuning of Large Language Models for Knowledge-Grounded Dialogue Generation","abstract":"Large language models (LLMs) demonstrate remarkable text comprehension and generation capabilities but often lack the ability to utilize up-to-date or domain-specific knowledge not included in their training data. To address this gap, we introduce KEDiT, an efficient method for fine-tuning LLMs for knowledge-grounded dialogue generation. KEDiT operates in two main phases: first, it employs an information bottleneck to compress retrieved knowledge into learnable parameters, retaining essential information while minimizing computational overhead. Second, a lightweight knowledge-aware adapter integrates these compressed knowledge vectors into the LLM during fine-tuning, updating less than 2\\% of the model parameters. The experimental results on the Wizard of Wikipedia and a newly constructed PubMed-Dialog dataset demonstrate that KEDiT excels in generating contextually relevant and informative responses, outperforming competitive baselines in automatic, LLM-based, and human evaluations. This approach effectively combines the strengths of pretrained LLMs with the adaptability needed for incorporating dynamic knowledge, presenting a scalable solution for fields such as medicine.","sentences":["Large language models (LLMs) demonstrate remarkable text comprehension and generation capabilities but often lack the ability to utilize up-to-date or domain-specific knowledge not included in their training data.","To address this gap, we introduce KEDiT, an efficient method for fine-tuning LLMs for knowledge-grounded dialogue generation.","KEDiT operates in two main phases: first, it employs an information bottleneck to compress retrieved knowledge into learnable parameters, retaining essential information while minimizing computational overhead.","Second, a lightweight knowledge-aware adapter integrates these compressed knowledge vectors into the LLM during fine-tuning, updating less than 2\\% of the model parameters.","The experimental results on the Wizard of Wikipedia and a newly constructed PubMed-Dialog dataset demonstrate that KEDiT excels in generating contextually relevant and informative responses, outperforming competitive baselines in automatic, LLM-based, and human evaluations.","This approach effectively combines the strengths of pretrained LLMs with the adaptability needed for incorporating dynamic knowledge, presenting a scalable solution for fields such as medicine."],"url":"http://arxiv.org/abs/2504.07754v1"}
{"created":"2025-04-10 13:40:27","title":"MMLA: Multi-Environment, Multi-Species, Low-Altitude Aerial Footage Dataset","abstract":"Real-time wildlife detection in drone imagery is critical for numerous applications, including animal ecology, conservation, and biodiversity monitoring. Low-altitude drone missions are effective for collecting fine-grained animal movement and behavior data, particularly if missions are automated for increased speed and consistency. However, little work exists on evaluating computer vision models on low-altitude aerial imagery and generalizability across different species and settings. To fill this gap, we present a novel multi-environment, multi-species, low-altitude aerial footage (MMLA) dataset. MMLA consists of drone footage collected across three diverse environments: Ol Pejeta Conservancy and Mpala Research Centre in Kenya, and The Wilds Conservation Center in Ohio, which includes five species: Plains zebras, Grevy's zebras, giraffes, onagers, and African Painted Dogs. We comprehensively evaluate three YOLO models (YOLOv5m, YOLOv8m, and YOLOv11m) for detecting animals. Results demonstrate significant performance disparities across locations and species-specific detection variations. Our work highlights the importance of evaluating detection algorithms across different environments for robust wildlife monitoring applications using drones.","sentences":["Real-time wildlife detection in drone imagery is critical for numerous applications, including animal ecology, conservation, and biodiversity monitoring.","Low-altitude drone missions are effective for collecting fine-grained animal movement and behavior data, particularly if missions are automated for increased speed and consistency.","However, little work exists on evaluating computer vision models on low-altitude aerial imagery and generalizability across different species and settings.","To fill this gap, we present a novel multi-environment, multi-species, low-altitude aerial footage (MMLA) dataset.","MMLA consists of drone footage collected across three diverse environments: Ol Pejeta Conservancy and Mpala Research Centre in Kenya, and The Wilds Conservation Center in Ohio, which includes five species: Plains zebras, Grevy's zebras, giraffes, onagers, and African Painted Dogs.","We comprehensively evaluate three YOLO models (YOLOv5m, YOLOv8m, and YOLOv11m) for detecting animals.","Results demonstrate significant performance disparities across locations and species-specific detection variations.","Our work highlights the importance of evaluating detection algorithms across different environments for robust wildlife monitoring applications using drones."],"url":"http://arxiv.org/abs/2504.07744v1"}
{"created":"2025-04-10 13:36:37","title":"Zero-Shot Cross-Domain Code Search without Fine-Tuning","abstract":"Code search aims to retrieve semantically relevant code snippets for natural language queries. While pre-trained language models (PLMs) have shown remarkable performance in this task, they struggle in cross-domain scenarios, often requiring costly fine-tuning or facing performance drops in zero-shot settings. RAPID, which generates synthetic data for model fine-tuning, is currently the only effective method for zero-shot cross-domain code search. Despite its effectiveness, RAPID demands substantial computational resources for fine-tuning and needs to maintain specialized models for each domain, underscoring the need for a zero-shot, fine-tuning-free approach for cross-domain code search.   The key to tackling zero-shot cross-domain code search lies in bridging the gaps among domains. In this work, we propose to break the query-code matching process of code search into two simpler tasks: query-comment matching and code-code matching. Our empirical study reveals the strong complementarity among the three matching schemas in zero-shot cross-domain settings, i.e., query-code, query-comment, and code-code matching. Based on the findings, we propose CodeBridge, a zero-shot, fine-tuning-free approach for cross-domain code search. Specifically, CodeBridge uses Large Language Models (LLMs) to generate comments and pseudo-code, then combines query-code, query-comment, and code-code matching via PLM-based similarity scoring and sampling-based fusion. Experimental results show that our approach outperforms the state-of-the-art PLM-based code search approaches, i.e., CoCoSoDa and UniXcoder, by an average of 21.4% and 24.9% in MRR, respectively, across three datasets. Our approach also yields results that are better than or comparable to those of the zero-shot cross-domain code search approach RAPID, which requires costly fine-tuning.","sentences":["Code search aims to retrieve semantically relevant code snippets for natural language queries.","While pre-trained language models (PLMs) have shown remarkable performance in this task, they struggle in cross-domain scenarios, often requiring costly fine-tuning or facing","performance drops in zero-shot settings.","RAPID, which generates synthetic data for model fine-tuning, is currently the only effective method for zero-shot cross-domain code search.","Despite its effectiveness, RAPID demands substantial computational resources for fine-tuning and needs to maintain specialized models for each domain, underscoring the need for a zero-shot, fine-tuning-free approach for cross-domain code search.   ","The key to tackling zero-shot cross-domain code search lies in bridging the gaps among domains.","In this work, we propose to break the query-code matching process of code search into two simpler tasks: query-comment matching and code-code matching.","Our empirical study reveals the strong complementarity among the three matching schemas in zero-shot cross-domain settings, i.e., query-code, query-comment, and code-code matching.","Based on the findings, we propose CodeBridge, a zero-shot, fine-tuning-free approach for cross-domain code search.","Specifically, CodeBridge uses Large Language Models (LLMs) to generate comments and pseudo-code, then combines query-code, query-comment, and code-code matching via PLM-based similarity scoring and sampling-based fusion.","Experimental results show that our approach outperforms the state-of-the-art PLM-based code search approaches, i.e., CoCoSoDa and UniXcoder, by an average of 21.4% and 24.9% in MRR, respectively, across three datasets.","Our approach also yields results that are better than or comparable to those of the zero-shot cross-domain code search approach RAPID, which requires costly fine-tuning."],"url":"http://arxiv.org/abs/2504.07740v1"}
{"created":"2025-04-10 13:18:48","title":"Quantum Machine Learning: Unveiling Trends, Impacts through Bibliometric Analysis","abstract":"Quantum Machine Learning (QML) is the intersection of two revolutionary fields: quantum computing and machine learning. It promises to unlock unparalleled capabilities in data analysis, model building, and problem-solving by harnessing the unique properties of quantum mechanics. This research endeavors to conduct a comprehensive bibliometric analysis of scientific information pertaining to QML covering the period from 2000 to 2023. An extensive dataset comprising 9493 scholarly works is meticulously examined to unveil notable trends, impact factors, and funding patterns within the domain. Additionally, the study employs bibliometric mapping techniques to visually illustrate the network relationships among key countries, institutions, authors, patent citations and significant keywords in QML research. The analysis reveals a consistent growth in publications over the examined period. The findings highlight the United States and China as prominent contributors, exhibiting substantial publication and citation metrics. Notably, the study concludes that QML, as a research subject, is currently in a formative stage, characterized by robust scholarly activity and ongoing development.","sentences":["Quantum Machine Learning (QML) is the intersection of two revolutionary fields: quantum computing and machine learning.","It promises to unlock unparalleled capabilities in data analysis, model building, and problem-solving by harnessing the unique properties of quantum mechanics.","This research endeavors to conduct a comprehensive bibliometric analysis of scientific information pertaining to QML covering the period from 2000 to 2023.","An extensive dataset comprising 9493 scholarly works is meticulously examined to unveil notable trends, impact factors, and funding patterns within the domain.","Additionally, the study employs bibliometric mapping techniques to visually illustrate the network relationships among key countries, institutions, authors, patent citations and significant keywords in QML research.","The analysis reveals a consistent growth in publications over the examined period.","The findings highlight the United States and China as prominent contributors, exhibiting substantial publication and citation metrics.","Notably, the study concludes that QML, as a research subject, is currently in a formative stage, characterized by robust scholarly activity and ongoing development."],"url":"http://arxiv.org/abs/2504.07726v1"}
{"created":"2025-04-10 13:18:22","title":"Approximation Algorithms for Connected Maximum Coverage, Minimum Connected Set Cover, and Node-Weighted Group Steiner Tree","abstract":"In the Connected Budgeted maximum Coverage problem (CBC), we are given a collection of subsets $\\mathcal{S}$, defined over a ground set $X$, and an undirected graph $G=(V,E)$, where each node is associated with a set of $\\mathcal{S}$. Each set in $\\mathcal{S}$ has a different cost and each element of $X$ gives a different prize. The goal is to find a subcollection $\\mathcal{S}'\\subseteq \\mathcal{S}$ such that $\\mathcal{S}'$ induces a connected subgraph in $G$, the total cost of the sets in $\\mathcal{S}'$ does not exceed a budget $B$, and the total prize of the elements covered by $\\mathcal{S}'$ is maximized. The Directed rooted Connected Budgeted maximum Coverage problem (DCBC) is a generalization of CBC where the underlying graph $G$ is directed and in the subgraph induced by $\\mathcal{S}'$ in $G$ must be an out-tree rooted at a given node.   The current best algorithms achieve approximation ratios that are linear in the size of $G$ or depend on $B$. In this paper, we provide two algorithms for CBC and DCBC that guarantee approximation ratios of $O\\left(\\frac{\\log^2|X|}{\\epsilon^2}\\right)$ and $O\\left(\\frac{\\sqrt{|V|}\\log^2|X|}{\\epsilon^2}\\right)$, resp., with a budget violation of a factor $1+\\epsilon$, where $\\epsilon\\in (0,1]$.   Our algorithms imply improved approximation factors of other related problems. For the particular case of DCBC where the prize function is additive, we improve from $O\\left(\\frac{1}{\\epsilon^2}|V|^{2/3}\\log|V|\\right)$ to $O\\left(\\frac{1}{\\epsilon^2}|V|^{1/2}\\log^2|V|\\right)$. For the minimum connected set cover, a minimization version of CBC, and its directed variant, we obtain approximation factors of $O(\\log^3|X|)$ and $O(\\sqrt{|V|}\\log^3|X|)$, resp. For the Node-Weighted Group Steiner Tree and and its directed variant, we obtain approximation factors of $O(\\log^3k)$ and $O(\\sqrt{|V|}\\log^3k)$, resp., where $k$ is the number of groups.","sentences":["In the Connected Budgeted maximum Coverage problem (CBC), we are given a collection of subsets $\\mathcal{S}$, defined over a ground set $X$, and an undirected graph $G=(V,E)$, where each node is associated with a set of $\\mathcal{S}$. Each set in $\\mathcal{S}$ has a different cost and each element of $X$ gives a different prize.","The goal is to find a subcollection $\\mathcal{S}'\\subseteq \\mathcal{S}$ such that $\\mathcal{S}'$ induces a connected subgraph in $G$, the total cost of the sets in $\\mathcal{S}'$ does not exceed a budget $B$, and the total prize of the elements covered by $\\mathcal{S}'$ is maximized.","The Directed rooted Connected Budgeted maximum Coverage problem (DCBC) is a generalization of CBC where the underlying graph $G$ is directed and in the subgraph induced by $\\mathcal{S}'$ in $G$ must be an out-tree rooted at a given node.   ","The current best algorithms achieve approximation ratios that are linear in the size of $G$ or depend on $B$. In this paper, we provide two algorithms for CBC and DCBC that guarantee approximation ratios of $O\\left(\\frac{\\log^2|X|}{\\epsilon^2}\\right)$ and $O\\left(\\frac{\\sqrt{|V|}\\log^2|X|}{\\epsilon^2}\\right)$, resp., with a budget violation of a factor $1+\\epsilon$, where $\\epsilon\\in (0,1]$.   Our algorithms imply improved approximation factors of other related problems.","For the particular case of DCBC where the prize function is additive, we improve from $O\\left(\\frac{1}{\\epsilon^2}|V|^{2/3}\\log|V|\\right)$ to $O\\left(\\frac{1}{\\epsilon^2}|V|^{1/2}\\log^2|V|\\right)$. For the minimum connected set cover, a minimization version of CBC, and its directed variant, we obtain approximation factors of $O(\\log^3|X|)$ and $O(\\sqrt{|V|}\\log^3|X|)$, resp.","For the Node-Weighted Group Steiner Tree and and its directed variant, we obtain approximation factors of $O(\\log^3k)$ and $O(\\sqrt{|V|}\\log^3k)$, resp., where $k$ is the number of groups."],"url":"http://arxiv.org/abs/2504.07725v1"}
{"created":"2025-04-10 13:17:51","title":"MRD-RAG: Enhancing Medical Diagnosis with Multi-Round Retrieval-Augmented Generation","abstract":"In recent years, accurately and quickly deploying medical large language models (LLMs) has become a significant trend. Among these, retrieval-augmented generation (RAG) has garnered significant attention due to its features of rapid deployment and privacy protection. However, existing medical RAG frameworks still have shortcomings. Most existing medical RAG frameworks are designed for single-round question answering tasks and are not suitable for multi-round diagnostic dialogue. On the other hand, existing medical multi-round RAG frameworks do not consider the interconnections between potential diseases to inquire precisely like a doctor. To address these issues, we propose a Multi-Round Diagnostic RAG (MRD-RAG) framework that mimics the doctor's diagnostic process. This RAG framework can analyze diagnosis information of potential diseases and accurately conduct multi-round diagnosis like a doctor. To evaluate the effectiveness of our proposed frameworks, we conduct experiments on two modern medical datasets and two traditional Chinese medicine datasets, with evaluations by GPT and human doctors on different methods. The results indicate that our RAG framework can significantly enhance the diagnostic performance of LLMs, highlighting the potential of our approach in medical diagnosis. The code and data can be found in our project website https://github.com/YixiangCh/MRD-RAG/tree/master.","sentences":["In recent years, accurately and quickly deploying medical large language models (LLMs) has become a significant trend.","Among these, retrieval-augmented generation (RAG) has garnered significant attention due to its features of rapid deployment and privacy protection.","However, existing medical RAG frameworks still have shortcomings.","Most existing medical RAG frameworks are designed for single-round question answering tasks and are not suitable for multi-round diagnostic dialogue.","On the other hand, existing medical multi-round RAG frameworks do not consider the interconnections between potential diseases to inquire precisely like a doctor.","To address these issues, we propose a Multi-Round Diagnostic RAG (MRD-RAG) framework that mimics the doctor's diagnostic process.","This RAG framework can analyze diagnosis information of potential diseases and accurately conduct multi-round diagnosis like a doctor.","To evaluate the effectiveness of our proposed frameworks, we conduct experiments on two modern medical datasets and two traditional Chinese medicine datasets, with evaluations by GPT and human doctors on different methods.","The results indicate that our RAG framework can significantly enhance the diagnostic performance of LLMs, highlighting the potential of our approach in medical diagnosis.","The code and data can be found in our project website https://github.com/YixiangCh/MRD-RAG/tree/master."],"url":"http://arxiv.org/abs/2504.07724v1"}
{"created":"2025-04-10 13:15:52","title":"Relaxing the Markov Requirements on Reinforcement Learning Under Weak Partial Ignorability","abstract":"Incomplete data, confounding effects, and violations of the Markov property are interrelated problems which are ubiquitous in Reinforcement Learning applications. We introduce the concept of ``partial ignorabilty\" and leverage it to establish a novel convergence theorem for adaptive Reinforcement Learning. This theoretical result relaxes the Markov assumption on the stochastic process underlying conventional $Q$-learning, deploying a generalized form of the Robbins-Monro stochastic approximation theorem to establish optimality. This result has clear downstream implications for most active subfields of Reinforcement Learning, with clear paths for extension to the field of Causal Inference.","sentences":["Incomplete data, confounding effects, and violations of the Markov property are interrelated problems which are ubiquitous in Reinforcement Learning applications.","We introduce the concept of ``partial ignorabilty\" and leverage it to establish a novel convergence theorem for adaptive Reinforcement Learning.","This theoretical result relaxes the Markov assumption on the stochastic process underlying conventional $Q$-learning, deploying a generalized form of the Robbins-Monro stochastic approximation theorem to establish optimality.","This result has clear downstream implications for most active subfields of Reinforcement Learning, with clear paths for extension to the field of Causal Inference."],"url":"http://arxiv.org/abs/2504.07722v1"}
{"created":"2025-04-10 13:04:56","title":"Merging Embedded Topics with Optimal Transport for Online Topic Modeling on Data Streams","abstract":"Topic modeling is a key component in unsupervised learning, employed to identify topics within a corpus of textual data. The rapid growth of social media generates an ever-growing volume of textual data daily, making online topic modeling methods essential for managing these data streams that continuously arrive over time. This paper introduces a novel approach to online topic modeling named StreamETM. This approach builds on the Embedded Topic Model (ETM) to handle data streams by merging models learned on consecutive partial document batches using unbalanced optimal transport. Additionally, an online change point detection algorithm is employed to identify shifts in topics over time, enabling the identification of significant changes in the dynamics of text streams. Numerical experiments on simulated and real-world data show StreamETM outperforming competitors.","sentences":["Topic modeling is a key component in unsupervised learning, employed to identify topics within a corpus of textual data.","The rapid growth of social media generates an ever-growing volume of textual data daily, making online topic modeling methods essential for managing these data streams that continuously arrive over time.","This paper introduces a novel approach to online topic modeling named StreamETM.","This approach builds on the Embedded Topic Model (ETM) to handle data streams by merging models learned on consecutive partial document batches using unbalanced optimal transport.","Additionally, an online change point detection algorithm is employed to identify shifts in topics over time, enabling the identification of significant changes in the dynamics of text streams.","Numerical experiments on simulated and real-world data show StreamETM outperforming competitors."],"url":"http://arxiv.org/abs/2504.07711v1"}
{"created":"2025-04-10 12:53:01","title":"TOCALib: Optimal control library with interpolation for bimanual manipulation and obstacles avoidance","abstract":"The paper presents a new approach for constructing a library of optimal trajectories for two robotic manipulators, Two-Arm Optimal Control and Avoidance Library (TOCALib). The optimisation takes into account kinodynamic and other constraints within the FROST framework. The novelty of the method lies in the consideration of collisions using the DCOL method, which allows obtaining symbolic expressions for assessing the presence of collisions and using them in gradient-based optimization control methods. The proposed approach allowed the implementation of complex bimanual manipulations. In this paper we used Mobile Aloha as an example of TOCALib application. The approach can be extended to other bimanual robots, as well as to gait control of bipedal robots. It can also be used to construct training data for machine learning tasks for manipulation.","sentences":["The paper presents a new approach for constructing a library of optimal trajectories for two robotic manipulators, Two-Arm Optimal Control and Avoidance Library (TOCALib).","The optimisation takes into account kinodynamic and other constraints within the FROST framework.","The novelty of the method lies in the consideration of collisions using the DCOL method, which allows obtaining symbolic expressions for assessing the presence of collisions and using them in gradient-based optimization control methods.","The proposed approach allowed the implementation of complex bimanual manipulations.","In this paper we used Mobile Aloha as an example of TOCALib application.","The approach can be extended to other bimanual robots, as well as to gait control of bipedal robots.","It can also be used to construct training data for machine learning tasks for manipulation."],"url":"http://arxiv.org/abs/2504.07708v1"}
{"created":"2025-04-10 12:49:00","title":"Managing Security Issues in Software Containers: From Practitioners Perspective","abstract":"Software development industries are increasingly adopting containers to enhance the scalability and flexibility of software applications. Security in containerized projects is a critical challenge that can lead to data breaches and performance degradation, thereby directly affecting the reliability and operations of the container services. Despite the ongoing effort to manage the security issues in containerized projects in software engineering (SE) research, more focused investigations are needed to explore the human perspective of security management and the technical approaches to security management in containerized projects. This research aims to explore security management in containerized projects by exploring how SE practitioners perceive the security issues in containerized software projects and their approach to managing such issues. A clear understanding of security management in containerized projects will enable industries to develop robust security strategies that enhance software reliability and trust. To achieve this, we conducted two separate semi-structured interview studies to examine how practitioners approach security management. The first study focused on practitioners perceptions of security challenges in containerized environments, where we interviewed 15 participants between December 2022 and October 2023. The second study explored how to enhance container security, with 20 participants interviewed between October 2024 and December 2024. Analyzing the data from both studies reveals how SE practitioners address the various security challenges in containerized projects. Our analysis also identified the technical and non-technical enablers that can be utilized to enhance security.","sentences":["Software development industries are increasingly adopting containers to enhance the scalability and flexibility of software applications.","Security in containerized projects is a critical challenge that can lead to data breaches and performance degradation, thereby directly affecting the reliability and operations of the container services.","Despite the ongoing effort to manage the security issues in containerized projects in software engineering (SE) research, more focused investigations are needed to explore the human perspective of security management and the technical approaches to security management in containerized projects.","This research aims to explore security management in containerized projects by exploring how SE practitioners perceive the security issues in containerized software projects and their approach to managing such issues.","A clear understanding of security management in containerized projects will enable industries to develop robust security strategies that enhance software reliability and trust.","To achieve this, we conducted two separate semi-structured interview studies to examine how practitioners approach security management.","The first study focused on practitioners perceptions of security challenges in containerized environments, where we interviewed 15 participants between December 2022 and October 2023.","The second study explored how to enhance container security, with 20 participants interviewed between October 2024 and December 2024.","Analyzing the data from both studies reveals how SE practitioners address the various security challenges in containerized projects.","Our analysis also identified the technical and non-technical enablers that can be utilized to enhance security."],"url":"http://arxiv.org/abs/2504.07707v1"}
{"created":"2025-04-10 12:31:55","title":"Transformer-Based Robust Underwater Inertial Navigation in Prolonged Doppler Velocity Log Outages","abstract":"Autonomous underwater vehicles (AUV) have a wide variety of applications in the marine domain, including exploration, surveying, and mapping. Their navigation systems rely heavily on fusing data from inertial sensors and a Doppler velocity log (DVL), typically via nonlinear filtering. The DVL estimates the AUV's velocity vector by transmitting acoustic beams to the seabed and analyzing the Doppler shift from the reflected signals. However, due to environmental challenges, DVL beams can deflect or fail in real-world settings, causing signal outages. In such cases, the AUV relies solely on inertial data, leading to accumulated navigation errors and mission terminations. To cope with these outages, we adopted ST-BeamsNet, a deep learning approach that uses inertial readings and prior DVL data to estimate AUV velocity during isolated outages. In this work, we extend ST-BeamsNet to address prolonged DVL outages and evaluate its impact within an extended Kalman filter framework. Experiments demonstrate that the proposed framework improves velocity RMSE by up to 63% and reduces final position error by up to 95% compared to pure inertial navigation. This is in scenarios involving up to 50 seconds of complete DVL outage.","sentences":["Autonomous underwater vehicles (AUV) have a wide variety of applications in the marine domain, including exploration, surveying, and mapping.","Their navigation systems rely heavily on fusing data from inertial sensors and a Doppler velocity log (DVL), typically via nonlinear filtering.","The DVL estimates the AUV's velocity vector by transmitting acoustic beams to the seabed and analyzing the Doppler shift from the reflected signals.","However, due to environmental challenges, DVL beams can deflect or fail in real-world settings, causing signal outages.","In such cases, the AUV relies solely on inertial data, leading to accumulated navigation errors and mission terminations.","To cope with these outages, we adopted ST-BeamsNet, a deep learning approach that uses inertial readings and prior DVL data to estimate AUV velocity during isolated outages.","In this work, we extend ST-BeamsNet to address prolonged DVL outages and evaluate its impact within an extended Kalman filter framework.","Experiments demonstrate that the proposed framework improves velocity RMSE by up to 63% and reduces final position error by up to 95% compared to pure inertial navigation.","This is in scenarios involving up to 50 seconds of complete DVL outage."],"url":"http://arxiv.org/abs/2504.07697v1"}
{"created":"2025-04-10 12:24:58","title":"Distilling Knowledge from Heterogeneous Architectures for Semantic Segmentation","abstract":"Current knowledge distillation (KD) methods for semantic segmentation focus on guiding the student to imitate the teacher's knowledge within homogeneous architectures. However, these methods overlook the diverse knowledge contained in architectures with different inductive biases, which is crucial for enabling the student to acquire a more precise and comprehensive understanding of the data during distillation. To this end, we propose for the first time a generic knowledge distillation method for semantic segmentation from a heterogeneous perspective, named HeteroAKD. Due to the substantial disparities between heterogeneous architectures, such as CNN and Transformer, directly transferring cross-architecture knowledge presents significant challenges. To eliminate the influence of architecture-specific information, the intermediate features of both the teacher and student are skillfully projected into an aligned logits space. Furthermore, to utilize diverse knowledge from heterogeneous architectures and deliver customized knowledge required by the student, a teacher-student knowledge mixing mechanism (KMM) and a teacher-student knowledge evaluation mechanism (KEM) are introduced. These mechanisms are performed by assessing the reliability and its discrepancy between heterogeneous teacher-student knowledge. Extensive experiments conducted on three main-stream benchmarks using various teacher-student pairs demonstrate that our HeteroAKD outperforms state-of-the-art KD methods in facilitating distillation between heterogeneous architectures.","sentences":["Current knowledge distillation (KD) methods for semantic segmentation focus on guiding the student to imitate the teacher's knowledge within homogeneous architectures.","However, these methods overlook the diverse knowledge contained in architectures with different inductive biases, which is crucial for enabling the student to acquire a more precise and comprehensive understanding of the data during distillation.","To this end, we propose for the first time a generic knowledge distillation method for semantic segmentation from a heterogeneous perspective, named HeteroAKD.","Due to the substantial disparities between heterogeneous architectures, such as CNN and Transformer, directly transferring cross-architecture knowledge presents significant challenges.","To eliminate the influence of architecture-specific information, the intermediate features of both the teacher and student are skillfully projected into an aligned logits space.","Furthermore, to utilize diverse knowledge from heterogeneous architectures and deliver customized knowledge required by the student, a teacher-student knowledge mixing mechanism (KMM) and a teacher-student knowledge evaluation mechanism (KEM) are introduced.","These mechanisms are performed by assessing the reliability and its discrepancy between heterogeneous teacher-student knowledge.","Extensive experiments conducted on three main-stream benchmarks using various teacher-student pairs demonstrate that our HeteroAKD outperforms state-of-the-art KD methods in facilitating distillation between heterogeneous architectures."],"url":"http://arxiv.org/abs/2504.07691v1"}
{"created":"2025-04-10 12:07:24","title":"Localization Meets Uncertainty: Uncertainty-Aware Multi-Modal Localization","abstract":"Reliable localization is critical for robot navigation in complex indoor environments. In this paper, we propose an uncertainty-aware localization method that enhances the reliability of localization outputs without modifying the prediction model itself. This study introduces a percentile-based rejection strategy that filters out unreliable 3-DoF pose predictions based on aleatoric and epistemic uncertainties the network estimates. We apply this approach to a multi-modal end-to-end localization that fuses RGB images and 2D LiDAR data, and we evaluate it across three real-world datasets collected using a commercialized serving robot. Experimental results show that applying stricter uncertainty thresholds consistently improves pose accuracy. Specifically, the mean position error is reduced by 41.0%, 56.7%, and 69.4%, and the mean orientation error by 55.6%, 65.7%, and 73.3%, when applying 90%, 80%, and 70% thresholds, respectively. Furthermore, the rejection strategy effectively removes extreme outliers, resulting in better alignment with ground truth trajectories. To the best of our knowledge, this is the first study to quantitatively demonstrate the benefits of percentile-based uncertainty rejection in multi-modal end-to-end localization tasks. Our approach provides a practical means to enhance the reliability and accuracy of localization systems in real-world deployments.","sentences":["Reliable localization is critical for robot navigation in complex indoor environments.","In this paper, we propose an uncertainty-aware localization method that enhances the reliability of localization outputs without modifying the prediction model itself.","This study introduces a percentile-based rejection strategy that filters out unreliable 3-DoF pose predictions based on aleatoric and epistemic uncertainties the network estimates.","We apply this approach to a multi-modal end-to-end localization that fuses RGB images and 2D LiDAR data, and we evaluate it across three real-world datasets collected using a commercialized serving robot.","Experimental results show that applying stricter uncertainty thresholds consistently improves pose accuracy.","Specifically, the mean position error is reduced by 41.0%, 56.7%, and 69.4%, and the mean orientation error by 55.6%, 65.7%, and 73.3%, when applying 90%, 80%, and 70% thresholds, respectively.","Furthermore, the rejection strategy effectively removes extreme outliers, resulting in better alignment with ground truth trajectories.","To the best of our knowledge, this is the first study to quantitatively demonstrate the benefits of percentile-based uncertainty rejection in multi-modal end-to-end localization tasks.","Our approach provides a practical means to enhance the reliability and accuracy of localization systems in real-world deployments."],"url":"http://arxiv.org/abs/2504.07677v1"}
{"created":"2025-04-10 11:58:56","title":"Clicks, comments, consequences: Are content creators' socio-structural and platform characteristics shaping the exposure to negative sentiment, offensive language, and hate speech on YouTube?","abstract":"Receiving negative sentiment, offensive comments, or even hate speech is a constant part of the working experience of content creators (CCs) on YouTube - a growing occupational group in the platform economy. This study investigates how socio-structural characteristics such as the age, gender, and race of CCs but also platform features including the number of subscribers, community strength, and the channel topic shape differences in the occurrence of these phenomena on that platform. Drawing on a random sample of n=3,695 YouTube channels from German-speaking countries, we conduct a comprehensive analysis combining digital trace data, enhanced with hand-coded variables to include socio-structural characteristics in social media data. Publicly visible negative sentiment, offensive language, and hate speech are detected with machine- and deep-learning methods using N=40,000,000 comments. Contrary to existing studies our findings indicate that female content creators are confronted with less negative communication. Notably, our analysis reveals that while BIPoC, who work as CCs, receive significantly more negative sentiment, they aren't exposed to more offensive comments or hate speech. Additionally, platform characteristics also play a crucial role, as channels publishing content on conspiracy theories or politics are more frequently subject to negative communication.","sentences":["Receiving negative sentiment, offensive comments, or even hate speech is a constant part of the working experience of content creators (CCs) on YouTube - a growing occupational group in the platform economy.","This study investigates how socio-structural characteristics such as the age, gender, and race of CCs but also platform features including the number of subscribers, community strength, and the channel topic shape differences in the occurrence of these phenomena on that platform.","Drawing on a random sample of n=3,695 YouTube channels from German-speaking countries, we conduct a comprehensive analysis combining digital trace data, enhanced with hand-coded variables to include socio-structural characteristics in social media data.","Publicly visible negative sentiment, offensive language, and hate speech are detected with machine- and deep-learning methods using N=40,000,000 comments.","Contrary to existing studies our findings indicate that female content creators are confronted with less negative communication.","Notably, our analysis reveals that while BIPoC, who work as CCs, receive significantly more negative sentiment, they aren't exposed to more offensive comments or hate speech.","Additionally, platform characteristics also play a crucial role, as channels publishing content on conspiracy theories or politics are more frequently subject to negative communication."],"url":"http://arxiv.org/abs/2504.07676v1"}
{"created":"2025-04-10 11:39:56","title":"S2R-HDR: A Large-Scale Rendered Dataset for HDR Fusion","abstract":"The generalization of learning-based high dynamic range (HDR) fusion is often limited by the availability of training data, as collecting large-scale HDR images from dynamic scenes is both costly and technically challenging. To address these challenges, we propose S2R-HDR, the first large-scale high-quality synthetic dataset for HDR fusion, with 24,000 HDR samples. Using Unreal Engine 5, we design a diverse set of realistic HDR scenes that encompass various dynamic elements, motion types, high dynamic range scenes, and lighting. Additionally, we develop an efficient rendering pipeline to generate realistic HDR images. To further mitigate the domain gap between synthetic and real-world data, we introduce S2R-Adapter, a domain adaptation designed to bridge this gap and enhance the generalization ability of models. Experimental results on real-world datasets demonstrate that our approach achieves state-of-the-art HDR reconstruction performance. Dataset and code will be available at https://openimaginglab.github.io/S2R-HDR.","sentences":["The generalization of learning-based high dynamic range (HDR) fusion is often limited by the availability of training data, as collecting large-scale HDR images from dynamic scenes is both costly and technically challenging.","To address these challenges, we propose S2R-HDR, the first large-scale high-quality synthetic dataset for HDR fusion, with 24,000 HDR samples.","Using Unreal Engine 5, we design a diverse set of realistic HDR scenes that encompass various dynamic elements, motion types, high dynamic range scenes, and lighting.","Additionally, we develop an efficient rendering pipeline to generate realistic HDR images.","To further mitigate the domain gap between synthetic and real-world data, we introduce S2R-Adapter, a domain adaptation designed to bridge this gap and enhance the generalization ability of models.","Experimental results on real-world datasets demonstrate that our approach achieves state-of-the-art HDR reconstruction performance.","Dataset and code will be available at https://openimaginglab.github.io/S2R-HDR."],"url":"http://arxiv.org/abs/2504.07667v1"}
{"created":"2025-04-10 11:30:25","title":"Data Requirement Goal Modeling for Machine Learning Systems","abstract":"Machine Learning (ML) has been integrated into various software and systems. Two main components are essential for training an ML model: the training data and the ML algorithm. Given the critical role of data in ML system development, it has become increasingly important to assess the quality of data attributes and ensure that the data meets specific requirements before its utilization. This work proposes an approach to guide non-experts in identifying data requirements for ML systems using goal modeling. In this approach, we first develop the Data Requirement Goal Model (DRGM) by surveying the white literature to identify and categorize the issues and challenges faced by data scientists and requirement engineers working on ML-related projects. An initial DRGM was built to accommodate common tasks that would generalize across projects. Then, based on insights from both white and gray literature, a customization mechanism is built to help adjust the tasks, KPIs, and goals' importance of different elements within the DRGM. The generated model can aid its users in evaluating different datasets using GRL evaluation strategies. We then validate the approach through two illustrative examples based on real-world projects. The results from the illustrative examples demonstrate that the data requirements identified by the proposed approach align with the requirements of real-world projects, demonstrating the practicality and effectiveness of the proposed framework. The proposed dataset selection customization mechanism and the proposed DRGM are helpful in guiding non-experts in identifying the data requirements for machine learning systems tailored to a specific ML problem. This approach also aids in evaluating different dataset alternatives to choose the optimum dataset for the problem. For future work, we recommend implementing tool support to generate the DRGM based on a chatbot interface.","sentences":["Machine Learning (ML) has been integrated into various software and systems.","Two main components are essential for training an ML model: the training data and the ML algorithm.","Given the critical role of data in ML system development, it has become increasingly important to assess the quality of data attributes and ensure that the data meets specific requirements before its utilization.","This work proposes an approach to guide non-experts in identifying data requirements for ML systems using goal modeling.","In this approach, we first develop the Data Requirement Goal Model (DRGM) by surveying the white literature to identify and categorize the issues and challenges faced by data scientists and requirement engineers working on ML-related projects.","An initial DRGM was built to accommodate common tasks that would generalize across projects.","Then, based on insights from both white and gray literature, a customization mechanism is built to help adjust the tasks, KPIs, and goals' importance of different elements within the DRGM.","The generated model can aid its users in evaluating different datasets using GRL evaluation strategies.","We then validate the approach through two illustrative examples based on real-world projects.","The results from the illustrative examples demonstrate that the data requirements identified by the proposed approach align with the requirements of real-world projects, demonstrating the practicality and effectiveness of the proposed framework.","The proposed dataset selection customization mechanism and the proposed DRGM are helpful in guiding non-experts in identifying the data requirements for machine learning systems tailored to a specific ML problem.","This approach also aids in evaluating different dataset alternatives to choose the optimum dataset for the problem.","For future work, we recommend implementing tool support to generate the DRGM based on a chatbot interface."],"url":"http://arxiv.org/abs/2504.07664v1"}
{"created":"2025-04-10 11:25:29","title":"Multiplicative assignment with upgrades","abstract":"We study a problem related to submodular function optimization and the exact matching problem for which we show a rather peculiar status: its natural LP-relaxation can have fractional optimal vertices, but there is always also an optimal integral vertex, which we can also compute in polynomial time.   More specifically, we consider the multiplicative assignment problem with upgrades in which we are given a set of customers and suppliers and we seek to assign each customer to a different supplier. Each customer has a demand and each supplier has a regular and an upgraded cost for each unit demand provided to the respective assigned client. Our goal is to upgrade at most $k$ suppliers and to compute an assignment in order to minimize the total resulting cost. This can be cast as the problem to compute an optimal matching in a bipartite graph with the additional constraint that we must select $k$ edges from a certain group of edges, similar to selecting $k$ red edges in the exact matching problem. Also, selecting the suppliers to be upgraded corresponds to maximizing a submodular set function under a cardinality constraint.   Our result yields an efficient LP-based algorithm to solve our problem optimally. In addition, we provide also a purely strongly polynomial-time algorithm for it. As an application, we obtain exact algorithms for the upgrading variant of the problem to schedule jobs on identical or uniformly related machines in order to minimize their sum of completion times, i.e., where we may upgrade up to $k$ jobs to reduce their respective processing times.","sentences":["We study a problem related to submodular function optimization and the exact matching problem for which we show a rather peculiar status: its natural LP-relaxation can have fractional optimal vertices, but there is always also an optimal integral vertex, which we can also compute in polynomial time.   ","More specifically, we consider the multiplicative assignment problem with upgrades in which we are given a set of customers and suppliers and we seek to assign each customer to a different supplier.","Each customer has a demand and each supplier has a regular and an upgraded cost for each unit demand provided to the respective assigned client.","Our goal is to upgrade at most $k$ suppliers and to compute an assignment in order to minimize the total resulting cost.","This can be cast as the problem to compute an optimal matching in a bipartite graph with the additional constraint that we must select $k$ edges from a certain group of edges, similar to selecting $k$ red edges in the exact matching problem.","Also, selecting the suppliers to be upgraded corresponds to maximizing a submodular set function under a cardinality constraint.   ","Our result yields an efficient LP-based algorithm to solve our problem optimally.","In addition, we provide also a purely strongly polynomial-time algorithm for it.","As an application, we obtain exact algorithms for the upgrading variant of the problem to schedule jobs on identical or uniformly related machines in order to minimize their sum of completion times, i.e., where we may upgrade up to $k$ jobs to reduce their respective processing times."],"url":"http://arxiv.org/abs/2504.07663v1"}
{"created":"2025-04-10 10:48:42","title":"On the Temporal Question-Answering Capabilities of Large Language Models Over Anonymized Data","abstract":"The applicability of Large Language Models (LLMs) in temporal reasoning tasks over data that is not present during training is still a field that remains to be explored. In this paper we work on this topic, focusing on structured and semi-structured anonymized data. We not only develop a direct LLM pipeline, but also compare various methodologies and conduct an in-depth analysis. We identified and examined seventeen common temporal reasoning tasks in natural language, focusing on their algorithmic components. To assess LLM performance, we created the \\textit{Reasoning and Answering Temporal Ability} dataset (RATA), featuring semi-structured anonymized data to ensure reliance on reasoning rather than on prior knowledge. We compared several methodologies, involving SoTA techniques such as Tree-of-Thought, self-reflexion and code execution, tuned specifically for this scenario. Our results suggest that achieving scalable and reliable solutions requires more than just standalone LLMs, highlighting the need for integrated approaches.","sentences":["The applicability of Large Language Models (LLMs) in temporal reasoning tasks over data that is not present during training is still a field that remains to be explored.","In this paper we work on this topic, focusing on structured and semi-structured anonymized data.","We not only develop a direct LLM pipeline, but also compare various methodologies and conduct an in-depth analysis.","We identified and examined seventeen common temporal reasoning tasks in natural language, focusing on their algorithmic components.","To assess LLM performance, we created the \\textit{Reasoning and Answering Temporal Ability} dataset (RATA), featuring semi-structured anonymized data to ensure reliance on reasoning rather than on prior knowledge.","We compared several methodologies, involving SoTA techniques such as Tree-of-Thought, self-reflexion and code execution, tuned specifically for this scenario.","Our results suggest that achieving scalable and reliable solutions requires more than just standalone LLMs, highlighting the need for integrated approaches."],"url":"http://arxiv.org/abs/2504.07646v1"}
{"created":"2025-04-10 10:48:36","title":"Prediction of Usage Probabilities of Shopping-Mall Corridors Using Heterogeneous Graph Neural Networks","abstract":"We present a method based on graph neural network (GNN) for prediction of probabilities of usage of shopping-mall corridors. The heterogeneous graph network of shops and corridor paths are obtained from floorplans of the malls by creating vector layers for corridors, shops and entrances. These are subsequently assimilated into nodes and edges of graphs. The prediction of the usage probability is based on the shop features, namely, the area and usage categories they fall into, and on the graph connecting these shops, corridor junctions and entrances by corridor paths. Though the presented method is applicable for training on datasets obtained from a field survey or from pedestrian-detecting sensors, the target data of the supervised deep-learning work flow in this work are obtained from a probability method. We also include a context-specific representation learning of latent features. The usage-probability prediction is made on each edge, which is a connection by a section of corridor path between the adjacent nodes representing the shops or corridor points. To create a feature for each edge, the hidden-layer feature vectors acquired in the message-passing GNN layers at the nodes of each edge are averaged and concatenated with the vector obtained by their multiplication. These edge-features are then passed to multilayer perceptrons (MLP) to make the final prediction of usage probability on each edge. The samples of synthetic learning dataset for each shopping mall are obtained by changing the shops' usage and area categories, and by subsequently feeding the graph into the probability model.   When including different shopping malls in a single dataset, we also propose to consider graph-level features to inform the model with specific identifying features of each mall.","sentences":["We present a method based on graph neural network (GNN) for prediction of probabilities of usage of shopping-mall corridors.","The heterogeneous graph network of shops and corridor paths are obtained from floorplans of the malls by creating vector layers for corridors, shops and entrances.","These are subsequently assimilated into nodes and edges of graphs.","The prediction of the usage probability is based on the shop features, namely, the area and usage categories they fall into, and on the graph connecting these shops, corridor junctions and entrances by corridor paths.","Though the presented method is applicable for training on datasets obtained from a field survey or from pedestrian-detecting sensors, the target data of the supervised deep-learning work flow in this work are obtained from a probability method.","We also include a context-specific representation learning of latent features.","The usage-probability prediction is made on each edge, which is a connection by a section of corridor path between the adjacent nodes representing the shops or corridor points.","To create a feature for each edge, the hidden-layer feature vectors acquired in the message-passing GNN layers at the nodes of each edge are averaged and concatenated with the vector obtained by their multiplication.","These edge-features are then passed to multilayer perceptrons (MLP) to make the final prediction of usage probability on each edge.","The samples of synthetic learning dataset for each shopping mall are obtained by changing the shops' usage and area categories, and by subsequently feeding the graph into the probability model.   ","When including different shopping malls in a single dataset, we also propose to consider graph-level features to inform the model with specific identifying features of each mall."],"url":"http://arxiv.org/abs/2504.07645v1"}
{"created":"2025-04-10 10:44:19","title":"CollEX -- A Multimodal Agentic RAG System Enabling Interactive Exploration of Scientific Collections","abstract":"In this paper, we introduce CollEx, an innovative multimodal agentic Retrieval-Augmented Generation (RAG) system designed to enhance interactive exploration of extensive scientific collections. Given the overwhelming volume and inherent complexity of scientific collections, conventional search systems often lack necessary intuitiveness and interactivity, presenting substantial barriers for learners, educators, and researchers. CollEx addresses these limitations by employing state-of-the-art Large Vision-Language Models (LVLMs) as multimodal agents accessible through an intuitive chat interface. By abstracting complex interactions via specialized agents equipped with advanced tools, CollEx facilitates curiosity-driven exploration, significantly simplifying access to diverse scientific collections and records therein. Our system integrates textual and visual modalities, supporting educational scenarios that are helpful for teachers, pupils, students, and researchers by fostering independent exploration as well as scientific excitement and curiosity. Furthermore, CollEx serves the research community by discovering interdisciplinary connections and complementing visual data. We illustrate the effectiveness of our system through a proof-of-concept application containing over 64,000 unique records across 32 collections from a local scientific collection from a public university.","sentences":["In this paper, we introduce CollEx, an innovative multimodal agentic Retrieval-Augmented Generation (RAG) system designed to enhance interactive exploration of extensive scientific collections.","Given the overwhelming volume and inherent complexity of scientific collections, conventional search systems often lack necessary intuitiveness and interactivity, presenting substantial barriers for learners, educators, and researchers.","CollEx addresses these limitations by employing state-of-the-art Large Vision-Language Models (LVLMs) as multimodal agents accessible through an intuitive chat interface.","By abstracting complex interactions via specialized agents equipped with advanced tools, CollEx facilitates curiosity-driven exploration, significantly simplifying access to diverse scientific collections and records therein.","Our system integrates textual and visual modalities, supporting educational scenarios that are helpful for teachers, pupils, students, and researchers by fostering independent exploration as well as scientific excitement and curiosity.","Furthermore, CollEx serves the research community by discovering interdisciplinary connections and complementing visual data.","We illustrate the effectiveness of our system through a proof-of-concept application containing over 64,000 unique records across 32 collections from a local scientific collection from a public university."],"url":"http://arxiv.org/abs/2504.07643v1"}
{"created":"2025-04-10 10:38:13","title":"Predicting the Lifespan of Industrial Printheads with Survival Analysis","abstract":"Accurately predicting the lifespan of critical device components is essential for maintenance planning and production optimization, making it a topic of significant interest in both academia and industry. In this work, we investigate the use of survival analysis for predicting the lifespan of production printheads developed by Canon Production Printing. Specifically, we focus on the application of five techniques to estimate survival probabilities and failure rates: the Kaplan-Meier estimator, Cox proportional hazard model, Weibull accelerated failure time model, random survival forest, and gradient boosting. The resulting estimates are further refined using isotonic regression and subsequently aggregated to determine the expected number of failures. The predictions are then validated against real-world ground truth data across multiple time windows to assess model reliability. Our quantitative evaluation using three performance metrics demonstrates that survival analysis outperforms industry-standard baseline methods for printhead lifespan prediction.","sentences":["Accurately predicting the lifespan of critical device components is essential for maintenance planning and production optimization, making it a topic of significant interest in both academia and industry.","In this work, we investigate the use of survival analysis for predicting the lifespan of production printheads developed by Canon Production Printing.","Specifically, we focus on the application of five techniques to estimate survival probabilities and failure rates: the Kaplan-Meier estimator, Cox proportional hazard model, Weibull accelerated failure time model, random survival forest, and gradient boosting.","The resulting estimates are further refined using isotonic regression and subsequently aggregated to determine the expected number of failures.","The predictions are then validated against real-world ground truth data across multiple time windows to assess model reliability.","Our quantitative evaluation using three performance metrics demonstrates that survival analysis outperforms industry-standard baseline methods for printhead lifespan prediction."],"url":"http://arxiv.org/abs/2504.07638v1"}
{"created":"2025-04-10 10:06:29","title":"CTSR: Cartesian tensor-based sparse regression for data-driven discovery of high-dimensional invariant governing equations","abstract":"Accurate and concise governing equations are crucial for understanding system dynamics. Recently, data-driven methods such as sparse regression have been employed to automatically uncover governing equations from data, representing a significant shift from traditional first-principles modeling. However, most existing methods focus on scalar equations, limiting their applicability to simple, low-dimensional scenarios, and failing to ensure rotation and reflection invariance without incurring significant computational cost or requiring additional prior knowledge. This paper proposes a Cartesian tensor-based sparse regression (CTSR) technique to accurately and efficiently uncover complex, high-dimensional governing equations while ensuring invariance. Evaluations on two two-dimensional (2D) and two three-dimensional (3D) test cases demonstrate that the proposed method achieves superior accuracy and efficiency compared to the conventional technique.","sentences":["Accurate and concise governing equations are crucial for understanding system dynamics.","Recently, data-driven methods such as sparse regression have been employed to automatically uncover governing equations from data, representing a significant shift from traditional first-principles modeling.","However, most existing methods focus on scalar equations, limiting their applicability to simple, low-dimensional scenarios, and failing to ensure rotation and reflection invariance without incurring significant computational cost or requiring additional prior knowledge.","This paper proposes a Cartesian tensor-based sparse regression (CTSR) technique to accurately and efficiently uncover complex, high-dimensional governing equations while ensuring invariance.","Evaluations on two two-dimensional (2D) and two three-dimensional (3D) test cases demonstrate that the proposed method achieves superior accuracy and efficiency compared to the conventional technique."],"url":"http://arxiv.org/abs/2504.07618v1"}
{"created":"2025-04-10 10:05:15","title":"VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model","abstract":"Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMs' performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the \"OD aha moment\", the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at https://github.com/om-ai-lab/VLM-R1","sentences":["Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design.","The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation.","In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations.","This property makes them naturally compatible with rule-based reward mechanisms.","Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities.","To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMs' performance on general vision-language tasks.","Using this framework, we further explore the feasibility of applying RL to visual domain.","Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability.","Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the \"OD aha moment\", the impact of training data quality, and the scaling behavior of RL across different model sizes.","Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community.","Our code and model are available at https://github.com/om-ai-lab/VLM-R1"],"url":"http://arxiv.org/abs/2504.07615v1"}
{"created":"2025-04-10 09:54:57","title":"RASMD: RGB And SWIR Multispectral Driving Dataset for Robust Perception in Adverse Conditions","abstract":"Current autonomous driving algorithms heavily rely on the visible spectrum, which is prone to performance degradation in adverse conditions like fog, rain, snow, glare, and high contrast. Although other spectral bands like near-infrared (NIR) and long-wave infrared (LWIR) can enhance vision perception in such situations, they have limitations and lack large-scale datasets and benchmarks. Short-wave infrared (SWIR) imaging offers several advantages over NIR and LWIR. However, no publicly available large-scale datasets currently incorporate SWIR data for autonomous driving. To address this gap, we introduce the RGB and SWIR Multispectral Driving (RASMD) dataset, which comprises 100,000 synchronized and spatially aligned RGB-SWIR image pairs collected across diverse locations, lighting, and weather conditions. In addition, we provide a subset for RGB-SWIR translation and object detection annotations for a subset of challenging traffic scenarios to demonstrate the utility of SWIR imaging through experiments on both object detection and RGB-to-SWIR image translation. Our experiments show that combining RGB and SWIR data in an ensemble framework significantly improves detection accuracy compared to RGB-only approaches, particularly in conditions where visible-spectrum sensors struggle. We anticipate that the RASMD dataset will advance research in multispectral imaging for autonomous driving and robust perception systems.","sentences":["Current autonomous driving algorithms heavily rely on the visible spectrum, which is prone to performance degradation in adverse conditions like fog, rain, snow, glare, and high contrast.","Although other spectral bands like near-infrared (NIR) and long-wave infrared (LWIR) can enhance vision perception in such situations, they have limitations and lack large-scale datasets and benchmarks.","Short-wave infrared (SWIR) imaging offers several advantages over NIR and LWIR.","However, no publicly available large-scale datasets currently incorporate SWIR data for autonomous driving.","To address this gap, we introduce the RGB and SWIR Multispectral Driving (RASMD) dataset, which comprises 100,000 synchronized and spatially aligned RGB-SWIR image pairs collected across diverse locations, lighting, and weather conditions.","In addition, we provide a subset for RGB-SWIR translation and object detection annotations for a subset of challenging traffic scenarios to demonstrate the utility of SWIR imaging through experiments on both object detection and RGB-to-SWIR image translation.","Our experiments show that combining RGB and SWIR data in an ensemble framework significantly improves detection accuracy compared to RGB-only approaches, particularly in conditions where visible-spectrum sensors struggle.","We anticipate that the RASMD dataset will advance research in multispectral imaging for autonomous driving and robust perception systems."],"url":"http://arxiv.org/abs/2504.07603v1"}
{"created":"2025-04-10 09:51:22","title":"On Model and Data Scaling for Skeleton-based Self-Supervised Gait Recognition","abstract":"Gait recognition from video streams is a challenging problem in computer vision biometrics due to the subtle differences between gaits and numerous confounding factors. Recent advancements in self-supervised pretraining have led to the development of robust gait recognition models that are invariant to walking covariates. While neural scaling laws have transformed model development in other domains by linking performance to data, model size, and compute, their applicability to gait remains unexplored. In this work, we conduct the first empirical study scaling on skeleton-based self-supervised gait recognition to quantify the effect of data quantity, model size and compute on downstream gait recognition performance. We pretrain multiple variants of GaitPT - a transformer-based architecture - on a dataset of 2.7 million walking sequences collected in the wild. We evaluate zero-shot performance across four benchmark datasets to derive scaling laws for data, model size, and compute. Our findings demonstrate predictable power-law improvements in performance with increased scale and confirm that data and compute scaling significantly influence downstream accuracy. We further isolate architectural contributions by comparing GaitPT with GaitFormer under controlled compute budgets. These results provide practical insights into resource allocation and performance estimation for real-world gait recognition systems.","sentences":["Gait recognition from video streams is a challenging problem in computer vision biometrics due to the subtle differences between gaits and numerous confounding factors.","Recent advancements in self-supervised pretraining have led to the development of robust gait recognition models that are invariant to walking covariates.","While neural scaling laws have transformed model development in other domains by linking performance to data, model size, and compute, their applicability to gait remains unexplored.","In this work, we conduct the first empirical study scaling on skeleton-based self-supervised gait recognition to quantify the effect of data quantity, model size and compute on downstream gait recognition performance.","We pretrain multiple variants of GaitPT - a transformer-based architecture - on a dataset of 2.7 million walking sequences collected in the wild.","We evaluate zero-shot performance across four benchmark datasets to derive scaling laws for data, model size, and compute.","Our findings demonstrate predictable power-law improvements in performance with increased scale and confirm that data and compute scaling significantly influence downstream accuracy.","We further isolate architectural contributions by comparing GaitPT with GaitFormer under controlled compute budgets.","These results provide practical insights into resource allocation and performance estimation for real-world gait recognition systems."],"url":"http://arxiv.org/abs/2504.07598v1"}
{"created":"2025-04-10 09:48:56","title":"Boosting Universal LLM Reward Design through the Heuristic Reward Observation Space Evolution","abstract":"Large Language Models (LLMs) are emerging as promising tools for automated reinforcement learning (RL) reward design, owing to their robust capabilities in commonsense reasoning and code generation. By engaging in dialogues with RL agents, LLMs construct a Reward Observation Space (ROS) by selecting relevant environment states and defining their internal operations. However, existing frameworks have not effectively leveraged historical exploration data or manual task descriptions to iteratively evolve this space. In this paper, we propose a novel heuristic framework that enhances LLM-driven reward design by evolving the ROS through a table-based exploration caching mechanism and a text-code reconciliation strategy. Our framework introduces a state execution table, which tracks the historical usage and success rates of environment states, overcoming the Markovian constraint typically found in LLM dialogues and facilitating more effective exploration. Furthermore, we reconcile user-provided task descriptions with expert-defined success criteria using structured prompts, ensuring alignment in reward design objectives. Comprehensive evaluations on benchmark RL tasks demonstrate the effectiveness and stability of the proposed framework. Code and video demos are available at jingjjjjjie.github.io/LLM2Reward.","sentences":["Large Language Models (LLMs) are emerging as promising tools for automated reinforcement learning (RL) reward design, owing to their robust capabilities in commonsense reasoning and code generation.","By engaging in dialogues with RL agents, LLMs construct a Reward Observation Space (ROS) by selecting relevant environment states and defining their internal operations.","However, existing frameworks have not effectively leveraged historical exploration data or manual task descriptions to iteratively evolve this space.","In this paper, we propose a novel heuristic framework that enhances LLM-driven reward design by evolving the ROS through a table-based exploration caching mechanism and a text-code reconciliation strategy.","Our framework introduces a state execution table, which tracks the historical usage and success rates of environment states, overcoming the Markovian constraint typically found in LLM dialogues and facilitating more effective exploration.","Furthermore, we reconcile user-provided task descriptions with expert-defined success criteria using structured prompts, ensuring alignment in reward design objectives.","Comprehensive evaluations on benchmark RL tasks demonstrate the effectiveness and stability of the proposed framework.","Code and video demos are available at jingjjjjjie.github.io/LLM2Reward."],"url":"http://arxiv.org/abs/2504.07596v1"}
{"created":"2025-04-10 09:48:22","title":"High-Level Synthesis using SDF-AP, Template Haskell, QuasiQuotes, and GADTs to Generate Circuits from Hierarchical Input Specification","abstract":"FPGAs provide highly parallel and customizable hardware solutions but are traditionally programmed using low-level Hardware Description Languages (HDLs) like VHDL and Verilog. These languages have a low level of abstraction and require engineers to manage control and scheduling manually. High-Level Synthesis (HLS) tools attempt to lift this level of abstraction by translating C/C++ code into hardware descriptions, but their reliance on imperative paradigms leads to challenges in deriving parallelism due to pointer aliasing and sequential execution models.   Functional programming, with its inherent purity, immutability, and parallelism, presents a more natural abstraction for FPGA design. Existing functional hardware description tools such as Clash enable high-level circuit descriptions but lack automated scheduling and control mechanisms. Prior work by Folmer introduced a framework integrating SDF-AP graphs into Haskell for automatic hardware generation, but it lacked hierarchy and reusability.   This paper extends that framework by introducing hierarchical pattern specification, enabling structured composition and scalable parallelism. Key contributions include: (1) automatic hardware generation, where both data and control paths are derived from functional specifications with hierarchical patterns, (2) parameterized buffers using GADTs, eliminating the need for manual buffer definitions and facilitating component reuse, and (3) provision of a reference \"golden model\" that can be simulated in the integrated environment for validation.   The core focus of this paper is on methodology. But we also evaluate our approach against Vitis HLS, comparing both notation and resulting hardware architectures. Experimental results demonstrate that our method provides greater transparency in resource utilization and scheduling, often outperforming Vitis in both scheduling and predictability.","sentences":["FPGAs provide highly parallel and customizable hardware solutions but are traditionally programmed using low-level Hardware Description Languages (HDLs) like VHDL and Verilog.","These languages have a low level of abstraction and require engineers to manage control and scheduling manually.","High-Level Synthesis (HLS) tools attempt to lift this level of abstraction by translating C/C++ code into hardware descriptions, but their reliance on imperative paradigms leads to challenges in deriving parallelism due to pointer aliasing and sequential execution models.   ","Functional programming, with its inherent purity, immutability, and parallelism, presents a more natural abstraction for FPGA design.","Existing functional hardware description tools such as Clash enable high-level circuit descriptions but lack automated scheduling and control mechanisms.","Prior work by Folmer introduced a framework integrating SDF-AP graphs into Haskell for automatic hardware generation, but it lacked hierarchy and reusability.   ","This paper extends that framework by introducing hierarchical pattern specification, enabling structured composition and scalable parallelism.","Key contributions include: (1) automatic hardware generation, where both data and control paths are derived from functional specifications with hierarchical patterns, (2) parameterized buffers using GADTs, eliminating the need for manual buffer definitions and facilitating component reuse, and (3) provision of a reference \"golden model\" that can be simulated in the integrated environment for validation.   ","The core focus of this paper is on methodology.","But we also evaluate our approach against Vitis HLS, comparing both notation and resulting hardware architectures.","Experimental results demonstrate that our method provides greater transparency in resource utilization and scheduling, often outperforming Vitis in both scheduling and predictability."],"url":"http://arxiv.org/abs/2504.07595v1"}
{"created":"2025-04-10 09:25:47","title":"High-Level Synthesis of Digital Circuits from Template Haskell and SDF-AP","abstract":"Functional languages as input specifications for High-Level Synthesis (HLS) tools allow to specify data dependencies but do not contain a notion of time nor execution order. In this paper, we propose a method to add this notion to the functional description using the dataflow model SDF-AP. SDF-AP consists of patterns that express consumption and production that we can use to enforce resource usage. We created an HLS-tool that can synthesize parallel hardware, both data and control path, based on the repetition, expressed in Higher-Order Functions, combined with specified SDF-AP patterns.   Our HLS-tool, based on Template Haskell, generates an Abstract Syntax Tree based on the given patterns and the functional description uses the Clash-compiler to generate VHDL/Verilog.   Case studies show consistent resource consumption and temporal behavior for our HLS. A comparison with a commercially available HLS-tool shows that our HLS tool outperforms in terms of latency and sometimes in resource consumption.   The method and tool presented in this paper offer more transparency to the developer and allow to specify more accurately the synthesized hardware compared to what is possible with pragmas of the Vitis HLS-tool.","sentences":["Functional languages as input specifications for High-Level Synthesis (HLS) tools allow to specify data dependencies but do not contain a notion of time nor execution order.","In this paper, we propose a method to add this notion to the functional description using the dataflow model SDF-AP.","SDF-AP consists of patterns that express consumption and production that we can use to enforce resource usage.","We created an HLS-tool that can synthesize parallel hardware, both data and control path, based on the repetition, expressed in Higher-Order Functions, combined with specified SDF-AP patterns.   ","Our HLS-tool, based on Template Haskell, generates an Abstract Syntax Tree based on the given patterns and the functional description uses the Clash-compiler to generate VHDL/Verilog.   ","Case studies show consistent resource consumption and temporal behavior for our HLS.","A comparison with a commercially available HLS-tool shows that our HLS tool outperforms in terms of latency and sometimes in resource consumption.   ","The method and tool presented in this paper offer more transparency to the developer and allow to specify more accurately the synthesized hardware compared to what is possible with pragmas of the Vitis HLS-tool."],"url":"http://arxiv.org/abs/2504.07585v1"}
{"created":"2025-04-10 09:20:56","title":"Privacy-Preserving Vertical K-Means Clustering","abstract":"Clustering is a fundamental data processing task used for grouping records based on one or more features. In the vertically partitioned setting, data is distributed among entities, with each holding only a subset of those features. A key challenge in this scenario is that computing distances between records requires access to all distributed features, which may be privacy-sensitive and cannot be directly shared with other parties. The goal is to compute the joint clusters while preserving the privacy of each entity's dataset. Existing solutions using secret sharing or garbled circuits implement privacy-preserving variants of Lloyd's algorithm but incur high communication costs, scaling as O(nkt), where n is the number of data points, k the number of clusters, and t the number of rounds. These methods become impractical for large datasets or several parties, limiting their use to LAN settings only. On the other hand, a different line of solutions rely on differential privacy (DP) to outsource the local features of the parties to a central server. However, they often significantly degrade the utility of the clustering outcome due to excessive noise. In this work, we propose a novel solution based on homomorphic encryption and DP, reducing communication complexity to O(n+kt). In our method, parties securely outsource their features once, allowing a computing party to perform clustering operations under encryption. DP is applied only to the clusters' centroids, ensuring privacy with minimal impact on utility. Our solution clusters 100,000 two-dimensional points into five clusters using only 73MB of communication, compared to 101GB for existing works, and completes in just under 3 minutes on a 100Mbps network, whereas existing works take over 1 day. This makes our solution practical even for WAN deployments, all while maintaining accuracy comparable to plaintext k-means algorithms.","sentences":["Clustering is a fundamental data processing task used for grouping records based on one or more features.","In the vertically partitioned setting, data is distributed among entities, with each holding only a subset of those features.","A key challenge in this scenario is that computing distances between records requires access to all distributed features, which may be privacy-sensitive and cannot be directly shared with other parties.","The goal is to compute the joint clusters while preserving the privacy of each entity's dataset.","Existing solutions using secret sharing or garbled circuits implement privacy-preserving variants of Lloyd's algorithm but incur high communication costs, scaling as O(nkt), where n is the number of data points, k the number of clusters, and t the number of rounds.","These methods become impractical for large datasets or several parties, limiting their use to LAN settings only.","On the other hand, a different line of solutions rely on differential privacy (DP) to outsource the local features of the parties to a central server.","However, they often significantly degrade the utility of the clustering outcome due to excessive noise.","In this work, we propose a novel solution based on homomorphic encryption and DP, reducing communication complexity to O(n+kt).","In our method, parties securely outsource their features once, allowing a computing party to perform clustering operations under encryption.","DP is applied only to the clusters' centroids, ensuring privacy with minimal impact on utility.","Our solution clusters 100,000 two-dimensional points into five clusters using only 73MB of communication, compared to 101GB for existing works, and completes in just under 3 minutes on a 100Mbps network, whereas existing works take over 1 day.","This makes our solution practical even for WAN deployments, all while maintaining accuracy comparable to plaintext k-means algorithms."],"url":"http://arxiv.org/abs/2504.07578v1"}
{"created":"2025-04-10 09:04:58","title":"Exploring Human-Like Thinking in Search Simulations with Large Language Models","abstract":"Simulating user search behavior is a critical task in information retrieval, which can be employed for user behavior modeling, data augmentation, and system evaluation. Recent advancements in large language models (LLMs) have opened up new possibilities for generating human-like actions including querying, browsing, and clicking. In this work, we explore the integration of human-like thinking into search simulations by leveraging LLMs to simulate users' hidden cognitive processes. Specifically, given a search task and context, we prompt LLMs to first think like a human before executing the corresponding action. As existing search datasets do not include users' thought processes, we conducted a user study to collect a new dataset enriched with users' explicit thinking. We investigate the impact of incorporating such human-like thinking on simulation performance and apply supervised fine-tuning (SFT) to teach LLMs to emulate both human thinking and actions. Our experiments span two dimensions in leveraging LLMs for user simulation: (1) with or without explicit thinking, and (2) with or without fine-tuning on the thinking-augmented dataset. The results demonstrate the feasibility and potential of incorporating human-like thinking in user simulations, though performance improvements on some metrics remain modest. We believe this exploration provides new avenues and inspirations for advancing user behavior modeling in search simulations.","sentences":["Simulating user search behavior is a critical task in information retrieval, which can be employed for user behavior modeling, data augmentation, and system evaluation.","Recent advancements in large language models (LLMs) have opened up new possibilities for generating human-like actions including querying, browsing, and clicking.","In this work, we explore the integration of human-like thinking into search simulations by leveraging LLMs to simulate users' hidden cognitive processes.","Specifically, given a search task and context, we prompt LLMs to first think like a human before executing the corresponding action.","As existing search datasets do not include users' thought processes, we conducted a user study to collect a new dataset enriched with users' explicit thinking.","We investigate the impact of incorporating such human-like thinking on simulation performance and apply supervised fine-tuning (SFT) to teach LLMs to emulate both human thinking and actions.","Our experiments span two dimensions in leveraging LLMs for user simulation: (1) with or without explicit thinking, and (2) with or without fine-tuning on the thinking-augmented dataset.","The results demonstrate the feasibility and potential of incorporating human-like thinking in user simulations, though performance improvements on some metrics remain modest.","We believe this exploration provides new avenues and inspirations for advancing user behavior modeling in search simulations."],"url":"http://arxiv.org/abs/2504.07570v1"}
{"created":"2025-04-10 08:56:09","title":"Diffusion Transformers for Tabular Data Time Series Generation","abstract":"Tabular data generation has recently attracted a growing interest due to its different application scenarios. However, generating time series of tabular data, where each element of the series depends on the others, remains a largely unexplored domain. This gap is probably due to the difficulty of jointly solving different problems, the main of which are the heterogeneity of tabular data (a problem common to non-time-dependent approaches) and the variable length of a time series. In this paper, we propose a Diffusion Transformers (DiTs) based approach for tabular data series generation. Inspired by the recent success of DiTs in image and video generation, we extend this framework to deal with heterogeneous data and variable-length sequences. Using extensive experiments on six datasets, we show that the proposed approach outperforms previous work by a large margin.","sentences":["Tabular data generation has recently attracted a growing interest due to its different application scenarios.","However, generating time series of tabular data, where each element of the series depends on the others, remains a largely unexplored domain.","This gap is probably due to the difficulty of jointly solving different problems, the main of which are the heterogeneity of tabular data (a problem common to non-time-dependent approaches) and the variable length of a time series.","In this paper, we propose a Diffusion Transformers (DiTs) based approach for tabular data series generation.","Inspired by the recent success of DiTs in image and video generation, we extend this framework to deal with heterogeneous data and variable-length sequences.","Using extensive experiments on six datasets, we show that the proposed approach outperforms previous work by a large margin."],"url":"http://arxiv.org/abs/2504.07566v1"}
{"created":"2025-04-10 08:38:39","title":"Using LLMs for Analyzing AIS Data","abstract":"Recent research in Large Language Models (LLMs), has had a profound impact across various fields, including mobility data science. This paper explores the and experiment with different approaches to using LLMs for analyzing AIS data. We propose a set of carefully designed queries to assess the reasoning capabilities of LLMs in this kind of tasks. Further, we experiment with four different methods: (1) using LLMs as a natural language interface to a spatial database, (2) reasoning on raw data, (3) reasoning on compressed trajectories, and (4) reasoning on semantic trajectories. We investigate the strengths and weaknesses for the four methods, and discuss the findings. The goal is to provide valuable insights for both researchers and practitioners on selecting the most appropriate LLM-based method depending on their specific data analysis objectives.","sentences":["Recent research in Large Language Models (LLMs), has had a profound impact across various fields, including mobility data science.","This paper explores the and experiment with different approaches to using LLMs for analyzing AIS data.","We propose a set of carefully designed queries to assess the reasoning capabilities of LLMs in this kind of tasks.","Further, we experiment with four different methods: (1) using LLMs as a natural language interface to a spatial database, (2) reasoning on raw data, (3) reasoning on compressed trajectories, and (4) reasoning on semantic trajectories.","We investigate the strengths and weaknesses for the four methods, and discuss the findings.","The goal is to provide valuable insights for both researchers and practitioners on selecting the most appropriate LLM-based method depending on their specific data analysis objectives."],"url":"http://arxiv.org/abs/2504.07557v1"}
{"created":"2025-04-10 08:24:26","title":"STeP: A General and Scalable Framework for Solving Video Inverse Problems with Spatiotemporal Diffusion Priors","abstract":"We study how to solve general Bayesian inverse problems involving videos using diffusion model priors. While it is desirable to use a video diffusion prior to effectively capture complex temporal relationships, due to the computational and data requirements of training such a model, prior work has instead relied on image diffusion priors on single frames combined with heuristics to enforce temporal consistency. However, these approaches struggle with faithfully recovering the underlying temporal relationships, particularly for tasks with high temporal uncertainty. In this paper, we demonstrate the feasibility of practical and accessible spatiotemporal diffusion priors by fine-tuning latent video diffusion models from pretrained image diffusion models using limited videos in specific domains. Leveraging this plug-and-play spatiotemporal diffusion prior, we introduce a general and scalable framework for solving video inverse problems. We then apply our framework to two challenging scientific video inverse problems--black hole imaging and dynamic MRI. Our framework enables the generation of diverse, high-fidelity video reconstructions that not only fit observations but also recover multi-modal solutions. By incorporating a spatiotemporal diffusion prior, we significantly improve our ability to capture complex temporal relationships in the data while also enhancing spatial fidelity.","sentences":["We study how to solve general Bayesian inverse problems involving videos using diffusion model priors.","While it is desirable to use a video diffusion prior to effectively capture complex temporal relationships, due to the computational and data requirements of training such a model, prior work has instead relied on image diffusion priors on single frames combined with heuristics to enforce temporal consistency.","However, these approaches struggle with faithfully recovering the underlying temporal relationships, particularly for tasks with high temporal uncertainty.","In this paper, we demonstrate the feasibility of practical and accessible spatiotemporal diffusion priors by fine-tuning latent video diffusion models from pretrained image diffusion models using limited videos in specific domains.","Leveraging this plug-and-play spatiotemporal diffusion prior, we introduce a general and scalable framework for solving video inverse problems.","We then apply our framework to two challenging scientific video inverse problems--black hole imaging and dynamic MRI.","Our framework enables the generation of diverse, high-fidelity video reconstructions that not only fit observations but also recover multi-modal solutions.","By incorporating a spatiotemporal diffusion prior, we significantly improve our ability to capture complex temporal relationships in the data while also enhancing spatial fidelity."],"url":"http://arxiv.org/abs/2504.07549v1"}
{"created":"2025-04-10 08:20:36","title":"Convexity Helps Iterated Search in 3D","abstract":"Inspired by the classical fractional cascading technique, we introduce new techniques to speed up the following type of iterated search in 3D: The input is a graph $\\mathbf{G}$ with bounded degree together with a set $H_v$ of 3D hyperplanes associated with every vertex of $v$ of $\\mathbf{G}$. The goal is to store the input such that given a query point $q\\in \\mathbb{R}^3$ and a connected subgraph $\\mathbf{H}\\subset \\mathbf{G}$, we can decide if $q$ is below or above the lower envelope of $H_v$ for every $v\\in \\mathbf{H}$. We show that using linear space, it is possible to answer queries in roughly $O(\\log n + |\\mathbf{H}|\\sqrt{\\log n})$ time which improves trivial bound of $O(|\\mathbf{H}|\\log n)$ obtained by using planar point location data structures. Our data structure can in fact answer more general queries (it combines with shallow cuttings) and it even works when $\\mathbf{H}$ is given one vertex at a time. We show that this has a number of new applications and in particular, we give improved solutions to a set of natural data structure problems that up to our knowledge had not seen any improvements.   We believe this is a very surprising result because obtaining similar results for the planar point location problem was known to be impossible.","sentences":["Inspired by the classical fractional cascading technique, we introduce new techniques to speed up the following type of iterated search in 3D: The input is a graph $\\mathbf{G}$ with bounded degree together with a set $H_v$ of 3D hyperplanes associated with every vertex of $v$ of $\\mathbf{G}$. The goal is to store the input such that given a query point $q\\in \\mathbb{R}^3$ and a connected subgraph $\\mathbf{H}\\subset \\mathbf{G}$, we can decide if $q$ is below or above the lower envelope of $H_v$ for every $v\\in \\mathbf{H}$. We show that using linear space, it is possible to answer queries in roughly $O(\\log n + |\\mathbf{H}|\\sqrt{\\log n})$ time which improves trivial bound of $O(|\\mathbf{H}|\\log n)$ obtained by using planar point location data structures.","Our data structure can in fact answer more general queries (it combines with shallow cuttings) and it even works when $\\mathbf{H}$ is given one vertex at a time.","We show that this has a number of new applications and in particular, we give improved solutions to a set of natural data structure problems that up to our knowledge had not seen any improvements.   ","We believe this is a very surprising result because obtaining similar results for the planar point location problem was known to be impossible."],"url":"http://arxiv.org/abs/2504.07545v1"}
{"created":"2025-04-10 08:11:17","title":"SydneyScapes: Image Segmentation for Australian Environments","abstract":"Autonomous Vehicles (AVs) are being partially deployed and tested across various global locations, including China, the USA, Germany, France, Japan, Korea, and the UK, but with limited demonstrations in Australia. The integration of machine learning (ML) into AV perception systems highlights the need for locally labelled datasets to develop and test algorithms in specific environments. To address this, we introduce SydneyScapes - a dataset tailored for computer vision tasks of image semantic, instance, and panoptic segmentation. This dataset, collected from Sydney and surrounding cities in New South Wales (NSW), Australia, consists of 756 images with high-quality pixel-level annotations. It is designed to assist AV industry and researchers by providing annotated data and tools for algorithm development, testing, and deployment in the Australian context. Additionally, we offer benchmarking results using state-of-the-art algorithms to establish reference points for future research and development. The dataset is publicly available at https://hdl.handle.net/2123/33051.","sentences":["Autonomous Vehicles (AVs) are being partially deployed and tested across various global locations, including China, the USA, Germany, France, Japan, Korea, and the UK, but with limited demonstrations in Australia.","The integration of machine learning (ML) into AV perception systems highlights the need for locally labelled datasets to develop and test algorithms in specific environments.","To address this, we introduce SydneyScapes - a dataset tailored for computer vision tasks of image semantic, instance, and panoptic segmentation.","This dataset, collected from Sydney and surrounding cities in New South Wales (NSW), Australia, consists of 756 images with high-quality pixel-level annotations.","It is designed to assist AV industry and researchers by providing annotated data and tools for algorithm development, testing, and deployment in the Australian context.","Additionally, we offer benchmarking results using state-of-the-art algorithms to establish reference points for future research and development.","The dataset is publicly available at https://hdl.handle.net/2123/33051."],"url":"http://arxiv.org/abs/2504.07542v1"}
{"created":"2025-04-10 08:09:34","title":"PoGO: A Scalable Proof of Useful Work via Quantized Gradient Descent and Merkle Proofs","abstract":"We present a design called \\emph{Proof of Gradient Optimization} (PoGO) for blockchain consensus, where miners produce verifiable evidence of training large-scale machine-learning models. Building on previous work, we incorporate \\emph{quantized gradients} (4-bit precision) to reduce storage and computation requirements, while still preserving the ability of verifiers to check that real progress has been made on lowering the model's loss. Additionally, we employ Merkle proofs over the full 32-bit model to handle large parameter sets and to enable random leaf checks with minimal on-chain data. We illustrate these ideas using GPT-3 (175B parameters) as a reference example and also refer to smaller but high-performance models (e.g., \\emph{Gemma~3} with 27B parameters). We provide an empirical cost analysis showing that verification is significantly cheaper than training, thanks in part to quantization and sampling. We also discuss the necessity of longer block times (potentially hours) when incorporating meaningful training steps, the trade-offs when using specialized GPU hardware, and how binary diffs may incrementally optimize updates. Finally, we note that fine-tuning can be handled in a similar manner, merely changing the dataset and the manner of sampling but preserving the overall verification flow. Our protocol allows verifiers to issue either \\emph{positive} or \\emph{negative} attestations; these are aggregated at finalization to either confirm the update or slash the miner.","sentences":["We present a design called \\emph{Proof of Gradient Optimization} (PoGO) for blockchain consensus, where miners produce verifiable evidence of training large-scale machine-learning models.","Building on previous work, we incorporate \\emph{quantized gradients} (4-bit precision) to reduce storage and computation requirements, while still preserving the ability of verifiers to check that real progress has been made on lowering the model's loss.","Additionally, we employ Merkle proofs over the full 32-bit model to handle large parameter sets and to enable random leaf checks with minimal on-chain data.","We illustrate these ideas using GPT-3 (175B parameters) as a reference example and also refer to smaller but high-performance models (e.g., \\emph{Gemma~3} with 27B parameters).","We provide an empirical cost analysis showing that verification is significantly cheaper than training, thanks in part to quantization and sampling.","We also discuss the necessity of longer block times (potentially hours) when incorporating meaningful training steps, the trade-offs when using specialized GPU hardware, and how binary diffs may incrementally optimize updates.","Finally, we note that fine-tuning can be handled in a similar manner, merely changing the dataset and the manner of sampling but preserving the overall verification flow.","Our protocol allows verifiers to issue either \\emph{positive} or \\emph{negative} attestations; these are aggregated at finalization to either confirm the update or slash the miner."],"url":"http://arxiv.org/abs/2504.07540v1"}
{"created":"2025-04-10 07:48:31","title":"Computing gradient vector fields with Morse sequences","abstract":"We rely on the framework of Morse sequences to enable the direct computation of gradient vector fields on simplicial complexes. A Morse sequence is a filtration from a subcomplex L to a complex K via elementary expansions and fillings, naturally encoding critical and regular simplexes. Maximal increasing and minimal decreasing schemes allow constructing these sequences, and are linked to algorithms like Random Discrete Morse and Coreduction. Extending the approach to cosimplicial complexes (S = K \\ L), we define operations -- reductions, perforations, coreductions, and coperforations -- for efficient computation. We further generalize to F -sequences, which are Morse sequences weighted by an arbitrary stack function F , and provide algorithms to compute maximal and minimal sequences. A particular case is when the stack function is given through a vertex map, as it is common in topological data analysis. We show that we retrieve existing methods when the vertex map is injective; in this case, the complex partitions into lower stars, facilitating parallel processing. Thus, this paper proposes simple, flexible, and computationally efficient approaches to obtain Morse sequences from arbitrary stack functions, allowing to generalize previous approaches dedicated to computing gradient vector fields from injective vertex maps.","sentences":["We rely on the framework of Morse sequences to enable the direct computation of gradient vector fields on simplicial complexes.","A Morse sequence is a filtration from a subcomplex L to a complex K via elementary expansions and fillings, naturally encoding critical and regular simplexes.","Maximal increasing and minimal decreasing schemes allow constructing these sequences, and are linked to algorithms like Random Discrete Morse and Coreduction.","Extending the approach to cosimplicial complexes (S = K \\ L), we define operations -- reductions, perforations, coreductions, and coperforations -- for efficient computation.","We further generalize to F -sequences, which are Morse sequences weighted by an arbitrary stack function F , and provide algorithms to compute maximal and minimal sequences.","A particular case is when the stack function is given through a vertex map, as it is common in topological data analysis.","We show that we retrieve existing methods when the vertex map is injective; in this case, the complex partitions into lower stars, facilitating parallel processing.","Thus, this paper proposes simple, flexible, and computationally efficient approaches to obtain Morse sequences from arbitrary stack functions, allowing to generalize previous approaches dedicated to computing gradient vector fields from injective vertex maps."],"url":"http://arxiv.org/abs/2504.07526v1"}
{"created":"2025-04-10 07:40:02","title":"Adversarial Subspace Generation for Outlier Detection in High-Dimensional Data","abstract":"Outlier detection in high-dimensional tabular data is challenging since data is often distributed across multiple lower-dimensional subspaces -- a phenomenon known as the Multiple Views effect (MV). This effect led to a large body of research focused on mining such subspaces, known as subspace selection. However, as the precise nature of the MV effect was not well understood, traditional methods had to rely on heuristic-driven search schemes that struggle to accurately capture the true structure of the data. Properly identifying these subspaces is critical for unsupervised tasks such as outlier detection or clustering, where misrepresenting the underlying data structure can hinder the performance. We introduce Myopic Subspace Theory (MST), a new theoretical framework that mathematically formulates the Multiple Views effect and writes subspace selection as a stochastic optimization problem. Based on MST, we introduce V-GAN, a generative method trained to solve such an optimization problem. This approach avoids any exhaustive search over the feature space while ensuring that the intrinsic data structure is preserved. Experiments on 42 real-world datasets show that using V-GAN subspaces to build ensemble methods leads to a significant increase in one-class classification performance -- compared to existing subspace selection, feature selection, and embedding methods. Further experiments on synthetic data show that V-GAN identifies subspaces more accurately while scaling better than other relevant subspace selection methods. These results confirm the theoretical guarantees of our approach and also highlight its practical viability in high-dimensional settings.","sentences":["Outlier detection in high-dimensional tabular data is challenging since data is often distributed across multiple lower-dimensional subspaces -- a phenomenon known as the Multiple Views effect (MV).","This effect led to a large body of research focused on mining such subspaces, known as subspace selection.","However, as the precise nature of the MV effect was not well understood, traditional methods had to rely on heuristic-driven search schemes that struggle to accurately capture the true structure of the data.","Properly identifying these subspaces is critical for unsupervised tasks such as outlier detection or clustering, where misrepresenting the underlying data structure can hinder the performance.","We introduce Myopic Subspace Theory (MST), a new theoretical framework that mathematically formulates the Multiple Views effect and writes subspace selection as a stochastic optimization problem.","Based on MST, we introduce V-GAN, a generative method trained to solve such an optimization problem.","This approach avoids any exhaustive search over the feature space while ensuring that the intrinsic data structure is preserved.","Experiments on 42 real-world datasets show that using V-GAN subspaces to build ensemble methods leads to a significant increase in one-class classification performance -- compared to existing subspace selection, feature selection, and embedding methods.","Further experiments on synthetic data show that V-GAN identifies subspaces more accurately while scaling better than other relevant subspace selection methods.","These results confirm the theoretical guarantees of our approach and also highlight its practical viability in high-dimensional settings."],"url":"http://arxiv.org/abs/2504.07522v1"}
{"created":"2025-04-10 07:24:23","title":"Enhancements for Developing a Comprehensive AI Fairness Assessment Standard","abstract":"As AI systems increasingly influence critical sectors like telecommunications, finance, healthcare, and public services, ensuring fairness in decision-making is essential to prevent biased or unjust outcomes that disproportionately affect vulnerable entities or result in adverse impacts. This need is particularly pressing as the industry approaches the 6G era, where AI will drive complex functions like autonomous network management and hyper-personalized services. The TEC Standard for Fairness Assessment and Rating of AI Systems provides guidelines for evaluating fairness in AI, focusing primarily on tabular data and supervised learning models. However, as AI applications diversify, this standard requires enhancement to strengthen its impact and broaden its applicability. This paper proposes an expansion of the TEC Standard to include fairness assessments for images, unstructured text, and generative AI, including large language models, ensuring a more comprehensive approach that keeps pace with evolving AI technologies. By incorporating these dimensions, the enhanced framework will promote responsible and trustworthy AI deployment across various sectors.","sentences":["As AI systems increasingly influence critical sectors like telecommunications, finance, healthcare, and public services, ensuring fairness in decision-making is essential to prevent biased or unjust outcomes that disproportionately affect vulnerable entities or result in adverse impacts.","This need is particularly pressing as the industry approaches the 6G era, where AI will drive complex functions like autonomous network management and hyper-personalized services.","The TEC Standard for Fairness Assessment and Rating of AI Systems provides guidelines for evaluating fairness in AI, focusing primarily on tabular data and supervised learning models.","However, as AI applications diversify, this standard requires enhancement to strengthen its impact and broaden its applicability.","This paper proposes an expansion of the TEC Standard to include fairness assessments for images, unstructured text, and generative AI, including large language models, ensuring a more comprehensive approach that keeps pace with evolving AI technologies.","By incorporating these dimensions, the enhanced framework will promote responsible and trustworthy AI deployment across various sectors."],"url":"http://arxiv.org/abs/2504.07516v1"}
{"created":"2025-04-10 07:15:40","title":"GPT Carry-On: Training Foundation Model for Customization Could Be Simple, Scalable and Affordable","abstract":"Modern large language foundation models (LLM) have now entered the daily lives of millions of users. We ask a natural question whether it is possible to customize LLM for every user or every task. From system and industrial economy consideration, general continue-training or fine-tuning still require substantial computation and memory of training GPU nodes, whereas most inference nodes under deployment, possibly with lower-end GPUs, are configured to make forward pass fastest possible. We propose a framework to take full advantages of existing LLMs and systems of online service. We train an additional branch of transformer blocks on the final-layer embedding of pretrained LLMs, which is the base, then a carry-on module merge the base models to compose a customized LLM. We can mix multiple layers, or multiple LLMs specialized in different domains such as chat, coding, math, to form a new mixture of LLM that best fit a new task. As the base model don't need to update parameters, we are able to outsource most computation of the training job on inference nodes, and only train a lightweight carry-on on training nodes, where we consume less than 1GB GPU memory to train a 100M carry-on layer on 30B LLM. We tested Qwen and DeepSeek opensourced models for continue-pretraining and got faster loss convergence. We use it to improve solving math questions with extremely small computation and model size, with 1000 data samples of chain-of-thoughts, and as small as 1 MB parameters of two layer layer carry-on, and the results are promising.","sentences":["Modern large language foundation models (LLM) have now entered the daily lives of millions of users.","We ask a natural question whether it is possible to customize LLM for every user or every task.","From system and industrial economy consideration, general continue-training or fine-tuning still require substantial computation and memory of training GPU nodes, whereas most inference nodes under deployment, possibly with lower-end GPUs, are configured to make forward pass fastest possible.","We propose a framework to take full advantages of existing LLMs and systems of online service.","We train an additional branch of transformer blocks on the final-layer embedding of pretrained LLMs, which is the base, then a carry-on module merge the base models to compose a customized LLM.","We can mix multiple layers, or multiple LLMs specialized in different domains such as chat, coding, math, to form a new mixture of LLM that best fit a new task.","As the base model don't need to update parameters, we are able to outsource most computation of the training job on inference nodes, and only train a lightweight carry-on on training nodes, where we consume less than 1GB GPU memory to train a 100M carry-on layer on 30B LLM.","We tested Qwen and DeepSeek opensourced models for continue-pretraining and got faster loss convergence.","We use it to improve solving math questions with extremely small computation and model size, with 1000 data samples of chain-of-thoughts, and as small as 1 MB parameters of two layer layer carry-on, and the results are promising."],"url":"http://arxiv.org/abs/2504.07513v1"}
{"created":"2025-04-10 07:10:40","title":"Drive in Corridors: Enhancing the Safety of End-to-end Autonomous Driving via Corridor Learning and Planning","abstract":"Safety remains one of the most critical challenges in autonomous driving systems. In recent years, the end-to-end driving has shown great promise in advancing vehicle autonomy in a scalable manner. However, existing approaches often face safety risks due to the lack of explicit behavior constraints. To address this issue, we uncover a new paradigm by introducing the corridor as the intermediate representation. Widely adopted in robotics planning, the corridors represents spatio-temporal obstacle-free zones for the vehicle to traverse. To ensure accurate corridor prediction in diverse traffic scenarios, we develop a comprehensive learning pipeline including data annotation, architecture refinement and loss formulation. The predicted corridor is further integrated as the constraint in a trajectory optimization process. By extending the differentiability of the optimization, we enable the optimized trajectory to be seamlessly trained within the end-to-end learning framework, improving both safety and interpretability. Experimental results on the nuScenes dataset demonstrate state-of-the-art performance of our approach, showing a 66.7% reduction in collisions with agents and a 46.5% reduction with curbs, significantly enhancing the safety of end-to-end driving. Additionally, incorporating the corridor contributes to higher success rates in closed-loop evaluations.","sentences":["Safety remains one of the most critical challenges in autonomous driving systems.","In recent years, the end-to-end driving has shown great promise in advancing vehicle autonomy in a scalable manner.","However, existing approaches often face safety risks due to the lack of explicit behavior constraints.","To address this issue, we uncover a new paradigm by introducing the corridor as the intermediate representation.","Widely adopted in robotics planning, the corridors represents spatio-temporal obstacle-free zones for the vehicle to traverse.","To ensure accurate corridor prediction in diverse traffic scenarios, we develop a comprehensive learning pipeline including data annotation, architecture refinement and loss formulation.","The predicted corridor is further integrated as the constraint in a trajectory optimization process.","By extending the differentiability of the optimization, we enable the optimized trajectory to be seamlessly trained within the end-to-end learning framework, improving both safety and interpretability.","Experimental results on the nuScenes dataset demonstrate state-of-the-art performance of our approach, showing a 66.7% reduction in collisions with agents and a 46.5% reduction with curbs, significantly enhancing the safety of end-to-end driving.","Additionally, incorporating the corridor contributes to higher success rates in closed-loop evaluations."],"url":"http://arxiv.org/abs/2504.07507v1"}
{"created":"2025-04-10 06:58:35","title":"Energy-Efficient UAV Replacement in Software-Defined UAV Networks","abstract":"Unmanned Aerial Vehicles (UAVs) in networked environments face significant challenges due to energy constraints and limited battery life, which necessitate periodic replacements to maintain continuous operation. Efficiently managing the handover of data flows during these replacements is crucial to avoid disruptions in communication and to optimize energy consumption. This paper addresses the complex issue of energy-efficient UAV replacement in software-defined UAV network. We introduce a novel approach based on establishing a strict total ordering relation for UAVs and data flows, allowing us to formulate the problem as an integer linear program. By utilizing the Gurobi solver, we obtain optimal handover schedules for the tested problem instances. Additionally, we propose a heuristic algorithm that significantly reduces computational complexity while maintaining near-optimal performance. Through comprehensive simulations, we demonstrate that our heuristic offers practical and scalable solution, ensuring energy-efficient UAV replacement while minimizing network disruptions. Our results suggest that the proposed approach can enhance UAV battery life and improve overall network reliability in real-world applications.","sentences":["Unmanned Aerial Vehicles (UAVs) in networked environments face significant challenges due to energy constraints and limited battery life, which necessitate periodic replacements to maintain continuous operation.","Efficiently managing the handover of data flows during these replacements is crucial to avoid disruptions in communication and to optimize energy consumption.","This paper addresses the complex issue of energy-efficient UAV replacement in software-defined UAV network.","We introduce a novel approach based on establishing a strict total ordering relation for UAVs and data flows, allowing us to formulate the problem as an integer linear program.","By utilizing the Gurobi solver, we obtain optimal handover schedules for the tested problem instances.","Additionally, we propose a heuristic algorithm that significantly reduces computational complexity while maintaining near-optimal performance.","Through comprehensive simulations, we demonstrate that our heuristic offers practical and scalable solution, ensuring energy-efficient UAV replacement while minimizing network disruptions.","Our results suggest that the proposed approach can enhance UAV battery life and improve overall network reliability in real-world applications."],"url":"http://arxiv.org/abs/2504.07500v1"}
{"created":"2025-04-10 06:46:38","title":"Geological Inference from Textual Data using Word Embeddings","abstract":"This research explores the use of Natural Language Processing (NLP) techniques to locate geological resources, with a specific focus on industrial minerals. By using word embeddings trained with the GloVe model, we extract semantic relationships between target keywords and a corpus of geological texts. The text is filtered to retain only words with geographical significance, such as city names, which are then ranked by their cosine similarity to the target keyword. Dimensional reduction techniques, including Principal Component Analysis (PCA), Autoencoder, Variational Autoencoder (VAE), and VAE with Long Short-Term Memory (VAE-LSTM), are applied to enhance feature extraction and improve the accuracy of semantic relations.   For benchmarking, we calculate the proximity between the ten cities most semantically related to the target keyword and identified mine locations using the haversine equation. The results demonstrate that combining NLP with dimensional reduction techniques provides meaningful insights into the spatial distribution of natural resources. Although the result shows to be in the same region as the supposed location, the accuracy has room for improvement.","sentences":["This research explores the use of Natural Language Processing (NLP) techniques to locate geological resources, with a specific focus on industrial minerals.","By using word embeddings trained with the GloVe model, we extract semantic relationships between target keywords and a corpus of geological texts.","The text is filtered to retain only words with geographical significance, such as city names, which are then ranked by their cosine similarity to the target keyword.","Dimensional reduction techniques, including Principal Component Analysis (PCA), Autoencoder, Variational Autoencoder (VAE), and VAE with Long Short-Term Memory (VAE-LSTM), are applied to enhance feature extraction and improve the accuracy of semantic relations.   ","For benchmarking, we calculate the proximity between the ten cities most semantically related to the target keyword and identified mine locations using the haversine equation.","The results demonstrate that combining NLP with dimensional reduction techniques provides meaningful insights into the spatial distribution of natural resources.","Although the result shows to be in the same region as the supposed location, the accuracy has room for improvement."],"url":"http://arxiv.org/abs/2504.07490v1"}
{"created":"2025-04-10 06:42:19","title":"Rendering Large Volume Datasets in Unreal Engine 5: A Survey","abstract":"In this technical report, we discuss several approaches to in-core rendering of large volumetric datasets in Unreal Engine 5 (UE5). We explore the following methods: the TBRayMarcher Plugin, the Niagara Fluids Plugin , and various approaches using Sparse Volume Textures (SVT), with a particular focus on Heterogeneous Volumes (HV). We found the HV approach to be the most promising. The biggest challenge we encountered with other approaches was the need to chunk datasets so that each fits into volume textures smaller than one gigavoxel. While this enables display of the entire dataset at reasonable frame rates, it introduces noticeable artifacts at chunk borders due to incorrect lighting, as each chunk lacks information about its neighbors. After addressing some (signed) int32 overflows in the Engine's SVT-related source code by converting them to to (unsigned) uint32 or int64, the SVT-based HV system allows us to render sparse datasets up to 32k x 32k x 16k voxels, provided the compressed tile data (including MIP data and padding for correct interpolation) does not exceed 4 gigavoxels. In the future, we intend to extend the existing SVT streaming functionality to support out-of-core rendering, in order to eventually overcome VRAM limitations, graphics API constraints, and the performance issues associated with 64-bit arithmetic in GPU shaders.","sentences":["In this technical report, we discuss several approaches to in-core rendering of large volumetric datasets in Unreal Engine 5 (UE5).","We explore the following methods: the TBRayMarcher Plugin, the Niagara Fluids Plugin , and various approaches using Sparse Volume Textures (SVT), with a particular focus on Heterogeneous Volumes (HV).","We found the HV approach to be the most promising.","The biggest challenge we encountered with other approaches was the need to chunk datasets so that each fits into volume textures smaller than one gigavoxel.","While this enables display of the entire dataset at reasonable frame rates, it introduces noticeable artifacts at chunk borders due to incorrect lighting, as each chunk lacks information about its neighbors.","After addressing some (signed) int32 overflows in the Engine's SVT-related source code by converting them to to (unsigned) uint32 or int64, the SVT-based HV system allows us to render sparse datasets up to 32k x 32k x 16k voxels, provided the compressed tile data (including MIP data and padding for correct interpolation) does not exceed 4 gigavoxels.","In the future, we intend to extend the existing SVT streaming functionality to support out-of-core rendering, in order to eventually overcome VRAM limitations, graphics API constraints, and the performance issues associated with 64-bit arithmetic in GPU shaders."],"url":"http://arxiv.org/abs/2504.07485v1"}
{"created":"2025-04-10 06:25:17","title":"Program Skeletons for Automated Program Translation","abstract":"Translating software between programming languages is a challenging task, for which automated techniques have been elusive and hard to scale up to larger programs. A key difficulty in cross-language translation is that one has to re-express the intended behavior of the source program into idiomatic constructs of a different target language. This task needs abstracting away from the source language-specific details, while keeping the overall functionality the same. In this work, we propose a novel and systematic approach for making such translation amenable to automation based on a framework we call program skeletons. A program skeleton retains the high-level structure of the source program by abstracting away and effectively summarizing lower-level concrete code fragments, which can be mechanically translated to the target programming language. A skeleton, by design, permits many different ways of filling in the concrete implementation for fragments, which can work in conjunction with existing data-driven code synthesizers. Most importantly, skeletons can conceptually enable sound decomposition, i.e., if each individual fragment is correctly translated, taken together with the mechanically translated skeleton, the final translated program is deemed to be correct as a whole. We present a prototype system called Skel embodying the idea of skeleton-based translation from Python to JavaScript. Our results show promising scalability compared to prior works. For 9 real-world Python programs, some with more than about 1k lines of code, 95% of their code fragments can be automatically translated, while about 5% require manual effort. All the final translations are correct with respect to whole-program test suites.","sentences":["Translating software between programming languages is a challenging task, for which automated techniques have been elusive and hard to scale up to larger programs.","A key difficulty in cross-language translation is that one has to re-express the intended behavior of the source program into idiomatic constructs of a different target language.","This task needs abstracting away from the source language-specific details, while keeping the overall functionality the same.","In this work, we propose a novel and systematic approach for making such translation amenable to automation based on a framework we call program skeletons.","A program skeleton retains the high-level structure of the source program by abstracting away and effectively summarizing lower-level concrete code fragments, which can be mechanically translated to the target programming language.","A skeleton, by design, permits many different ways of filling in the concrete implementation for fragments, which can work in conjunction with existing data-driven code synthesizers.","Most importantly, skeletons can conceptually enable sound decomposition, i.e., if each individual fragment is correctly translated, taken together with the mechanically translated skeleton, the final translated program is deemed to be correct as a whole.","We present a prototype system called Skel embodying the idea of skeleton-based translation from Python to JavaScript.","Our results show promising scalability compared to prior works.","For 9 real-world Python programs, some with more than about 1k lines of code, 95% of their code fragments can be automatically translated, while about 5% require manual effort.","All the final translations are correct with respect to whole-program test suites."],"url":"http://arxiv.org/abs/2504.07483v1"}
{"created":"2025-04-10 06:08:04","title":"Intelligent DoS and DDoS Detection: A Hybrid GRU-NTM Approach to Network Security","abstract":"Detecting Denial of Service (DoS) and Distributed Denial of Service (DDoS) attacks remains a critical challenge in cybersecurity. This research introduces a hybrid deep learning model combining Gated Recurrent Units (GRUs) and a Neural Turing Machine (NTM) for enhanced intrusion detection. Trained on the UNSW-NB15 and BoT-IoT datasets, the model employs GRU layers for sequential data processing and an NTM for long-term pattern recognition. The proposed approach achieves 99% accuracy in distinguishing between normal, DoS, and DDoS traffic. These findings offer promising advancements in real-time threat detection and contribute to improved network security across various domains.","sentences":["Detecting Denial of Service (DoS) and Distributed Denial of Service (DDoS) attacks remains a critical challenge in cybersecurity.","This research introduces a hybrid deep learning model combining Gated Recurrent Units (GRUs) and a Neural Turing Machine (NTM) for enhanced intrusion detection.","Trained on the UNSW-NB15 and BoT-IoT datasets, the model employs GRU layers for sequential data processing and an NTM for long-term pattern recognition.","The proposed approach achieves 99% accuracy in distinguishing between normal, DoS, and DDoS traffic.","These findings offer promising advancements in real-time threat detection and contribute to improved network security across various domains."],"url":"http://arxiv.org/abs/2504.07478v1"}
{"created":"2025-04-10 06:04:16","title":"CMEdataset Advancing China Map Detection and Standardization with Digital Image Resources","abstract":"Digital images of Chinas maps play a crucial role in map detection, particularly in ensuring national sovereignty, territorial integrity, and map compliance. However, there is currently no publicly available dataset specifically dedicated to problematic maps the CME dataset. Existing datasets primarily focus on general map data and are insufficient for effectively identifying complex issues such as national boundary misrepresentations, missing elements, and blurred boundaries. Therefore, this study creates a Problematic Map dataset that covers five key problem areas, aiming to provide diverse samples for problematic map detection technologies, support high-precision map compliance detection, and enhance map data quality and timeliness. This dataset not only provides essential resources for map compliance, national security monitoring, and map updates, but also fosters innovation and application of related technologies.","sentences":["Digital images of Chinas maps play a crucial role in map detection, particularly in ensuring national sovereignty, territorial integrity, and map compliance.","However, there is currently no publicly available dataset specifically dedicated to problematic maps the CME dataset.","Existing datasets primarily focus on general map data and are insufficient for effectively identifying complex issues such as national boundary misrepresentations, missing elements, and blurred boundaries.","Therefore, this study creates a Problematic Map dataset that covers five key problem areas, aiming to provide diverse samples for problematic map detection technologies, support high-precision map compliance detection, and enhance map data quality and timeliness.","This dataset not only provides essential resources for map compliance, national security monitoring, and map updates, but also fosters innovation and application of related technologies."],"url":"http://arxiv.org/abs/2504.07476v1"}
{"created":"2025-04-10 05:48:57","title":"Traversal Learning Coordination For Lossless And Efficient Distributed Learning","abstract":"In this paper, we introduce Traversal Learning (TL), a novel approach designed to address the problem of decreased quality encountered in popular distributed learning (DL) paradigms such as Federated Learning (FL), Split Learning (SL), and SplitFed Learning (SFL). Traditional FL experiences from an accuracy drop during aggregation due to its averaging function, while SL and SFL face increased loss due to the independent gradient updates on each split network. TL adopts a unique strategy where the model traverses the nodes during forward propagation (FP) and performs backward propagation (BP) on the orchestrator, effectively implementing centralized learning (CL) principles within a distributed environment. The orchestrator is tasked with generating virtual batches and planning the sequential node visits of the model during FP, aligning them with the ordered index of the data within these batches. We conducted experiments on six datasets representing diverse characteristics across various domains. Our evaluation demonstrates that TL is on par with classic CL approaches in terms of accurate inference, thereby offering a viable and robust solution for DL tasks. TL outperformed other DL methods and improved accuracy by 7.85% for independent and identically distributed (IID) datasets, macro F1-score by 1.06% for non-IID datasets, accuracy by 2.60% for text classification, and AUC by 3.88% and 4.54% for medical and financial datasets, respectively. By effectively preserving data privacy while maintaining performance, TL represents a significant advancement in DL methodologies.","sentences":["In this paper, we introduce Traversal Learning (TL), a novel approach designed to address the problem of decreased quality encountered in popular distributed learning (DL) paradigms such as Federated Learning (FL), Split Learning (SL), and SplitFed Learning (SFL).","Traditional FL experiences from an accuracy drop during aggregation due to its averaging function, while SL and SFL face increased loss due to the independent gradient updates on each split network.","TL adopts a unique strategy where the model traverses the nodes during forward propagation (FP) and performs backward propagation (BP) on the orchestrator, effectively implementing centralized learning (CL) principles within a distributed environment.","The orchestrator is tasked with generating virtual batches and planning the sequential node visits of the model during FP, aligning them with the ordered index of the data within these batches.","We conducted experiments on six datasets representing diverse characteristics across various domains.","Our evaluation demonstrates that TL is on par with classic CL approaches in terms of accurate inference, thereby offering a viable and robust solution for DL tasks.","TL outperformed other DL methods and improved accuracy by 7.85% for independent and identically distributed (IID) datasets, macro F1-score by 1.06% for non-IID datasets, accuracy by 2.60% for text classification, and AUC by 3.88% and 4.54% for medical and financial datasets, respectively.","By effectively preserving data privacy while maintaining performance, TL represents a significant advancement in DL methodologies."],"url":"http://arxiv.org/abs/2504.07471v1"}
{"created":"2025-04-10 05:29:04","title":"Multi-Modal Data Fusion for Moisture Content Prediction in Apple Drying","abstract":"Fruit drying is widely used in food manufacturing to reduce product moisture, ensure product safety, and extend product shelf life. Accurately predicting final moisture content (MC) is critically needed for quality control of drying processes. State-of-the-art methods can build deterministic relationships between process parameters and MC, but cannot adequately account for inherent process variabilities that are ubiquitous in fruit drying. To address this gap, this paper presents a novel multi-modal data fusion framework to effectively fuse two modalities of data: tabular data (process parameters) and high-dimensional image data (images of dried apple slices) to enable accurate MC prediction. The proposed modeling architecture permits flexible adjustment of information portion from tabular and image data modalities. Experimental validation shows that the multi-modal approach improves predictive accuracy substantially compared to state-of-the-art methods. The proposed method reduces root-mean-squared errors by 19.3%, 24.2%, and 15.2% over tabular-only, image-only, and standard tabular-image fusion models, respectively. Furthermore, it is demonstrated that our method is robust in varied tabular-image ratios and capable of effectively capturing inherent small-scale process variabilities. The proposed framework is extensible to a variety of other drying technologies.","sentences":["Fruit drying is widely used in food manufacturing to reduce product moisture, ensure product safety, and extend product shelf life.","Accurately predicting final moisture content (MC) is critically needed for quality control of drying processes.","State-of-the-art methods can build deterministic relationships between process parameters and MC, but cannot adequately account for inherent process variabilities that are ubiquitous in fruit drying.","To address this gap, this paper presents a novel multi-modal data fusion framework to effectively fuse two modalities of data: tabular data (process parameters) and high-dimensional image data (images of dried apple slices) to enable accurate MC prediction.","The proposed modeling architecture permits flexible adjustment of information portion from tabular and image data modalities.","Experimental validation shows that the multi-modal approach improves predictive accuracy substantially compared to state-of-the-art methods.","The proposed method reduces root-mean-squared errors by 19.3%, 24.2%, and 15.2% over tabular-only, image-only, and standard tabular-image fusion models, respectively.","Furthermore, it is demonstrated that our method is robust in varied tabular-image ratios and capable of effectively capturing inherent small-scale process variabilities.","The proposed framework is extensible to a variety of other drying technologies."],"url":"http://arxiv.org/abs/2504.07465v1"}
{"created":"2025-04-10 05:20:29","title":"Learning Universal Features for Generalizable Image Forgery Localization","abstract":"In recent years, advanced image editing and generation methods have rapidly evolved, making detecting and locating forged image content increasingly challenging. Most existing image forgery detection methods rely on identifying the edited traces left in the image. However, because the traces of different forgeries are distinct, these methods can identify familiar forgeries included in the training data but struggle to handle unseen ones. In response, we present an approach for Generalizable Image Forgery Localization (GIFL). Once trained, our model can detect both seen and unseen forgeries, providing a more practical and efficient solution to counter false information in the era of generative AI. Our method focuses on learning general features from the pristine content rather than traces of specific forgeries, which are relatively consistent across different types of forgeries and therefore can be used as universal features to locate unseen forgeries. Additionally, as existing image forgery datasets are still dominated by traditional hand-crafted forgeries, we construct a new dataset consisting of images edited by various popular deep generative image editing methods to further encourage research in detecting images manipulated by deep generative models. Extensive experimental results show that the proposed approach outperforms state-of-the-art methods in the detection of unseen forgeries and also demonstrates competitive results for seen forgeries. The code and dataset are available at https://github.com/ZhaoHengrun/GIFL.","sentences":["In recent years, advanced image editing and generation methods have rapidly evolved, making detecting and locating forged image content increasingly challenging.","Most existing image forgery detection methods rely on identifying the edited traces left in the image.","However, because the traces of different forgeries are distinct, these methods can identify familiar forgeries included in the training data but struggle to handle unseen ones.","In response, we present an approach for Generalizable Image Forgery Localization (GIFL).","Once trained, our model can detect both seen and unseen forgeries, providing a more practical and efficient solution to counter false information in the era of generative AI.","Our method focuses on learning general features from the pristine content rather than traces of specific forgeries, which are relatively consistent across different types of forgeries and therefore can be used as universal features to locate unseen forgeries.","Additionally, as existing image forgery datasets are still dominated by traditional hand-crafted forgeries, we construct a new dataset consisting of images edited by various popular deep generative image editing methods to further encourage research in detecting images manipulated by deep generative models.","Extensive experimental results show that the proposed approach outperforms state-of-the-art methods in the detection of unseen forgeries and also demonstrates competitive results for seen forgeries.","The code and dataset are available at https://github.com/ZhaoHengrun/GIFL."],"url":"http://arxiv.org/abs/2504.07462v1"}
{"created":"2025-04-10 04:59:28","title":"How Can Objects Help Video-Language Understanding?","abstract":"How multimodal large language models (MLLMs) perceive the visual world remains a mystery. To one extreme, object and relation modeling may be implicitly implemented with inductive biases, for example by treating objects as tokens. To the other extreme, empirical results reveal the surprising finding that simply performing visual captioning, which tends to ignore spatial configuration of the objects, serves as a strong baseline for video understanding. We aim to answer the question: how can objects help video-language understanding in MLLMs? We tackle the question from the object representation and adaptation perspectives. Specifically, we investigate the trade-off between representation expressiveness (e.g., distributed versus symbolic) and integration difficulty (e.g., data-efficiency when learning the adapters). Through extensive evaluations on five video question answering datasets, we confirm that explicit integration of object-centric representation remains necessary, and the symbolic objects can be most easily integrated while being performant for question answering. We hope our findings can encourage the community to explore the explicit integration of perception modules into MLLM design. Our code and models will be publicly released.","sentences":["How multimodal large language models (MLLMs) perceive the visual world remains a mystery.","To one extreme, object and relation modeling may be implicitly implemented with inductive biases, for example by treating objects as tokens.","To the other extreme, empirical results reveal the surprising finding that simply performing visual captioning, which tends to ignore spatial configuration of the objects, serves as a strong baseline for video understanding.","We aim to answer the question: how can objects help video-language understanding in MLLMs?","We tackle the question from the object representation and adaptation perspectives.","Specifically, we investigate the trade-off between representation expressiveness (e.g., distributed versus symbolic) and integration difficulty (e.g., data-efficiency when learning the adapters).","Through extensive evaluations on five video question answering datasets, we confirm that explicit integration of object-centric representation remains necessary, and the symbolic objects can be most easily integrated while being performant for question answering.","We hope our findings can encourage the community to explore the explicit integration of perception modules into MLLM design.","Our code and models will be publicly released."],"url":"http://arxiv.org/abs/2504.07454v1"}
{"created":"2025-04-10 04:58:24","title":"Probability Estimation and Scheduling Optimization for Battery Swap Stations via LRU-Enhanced Genetic Algorithm and Dual-Factor Decision System","abstract":"To address the challenges of limited Battery Swap Stations datasets, high operational costs, and fluctuating user charging demand, this research proposes a probability estimation model based on charging pile data and constructs nine scenario-specific battery swap demand datasets. In addition, this study combines Least Recently Used strategy with Genetic Algorithm and incorporates a guided search mechanism, which effectively enhances the global optimization capability. Thus, a dual-factor decision-making based charging schedule optimization system is constructed. Experimental results show that the constructed datasets exhibit stable trend characteristics, adhering to 24-hour and 168-hour periodicity patterns, with outlier ratios consistently below 3.26%, confirming data validity. Compared to baseline, the improved algorithm achieves better fitness individuals in 80% of test regions under the same iterations. When benchmarked against immediate swap-and-charge strategy, our algorithm achieves a peak cost reduction of 13.96%. Moreover, peak user satisfaction reaches 98.57%, while the average iteration time remains below 0.6 seconds, demonstrating good computational efficiency. The complete datasets and optimization algorithm are open-sourced at https://github.com/qingshufan/GA-EVLRU.","sentences":["To address the challenges of limited Battery Swap Stations datasets, high operational costs, and fluctuating user charging demand, this research proposes a probability estimation model based on charging pile data and constructs nine scenario-specific battery swap demand datasets.","In addition, this study combines Least Recently Used strategy with Genetic Algorithm and incorporates a guided search mechanism, which effectively enhances the global optimization capability.","Thus, a dual-factor decision-making based charging schedule optimization system is constructed.","Experimental results show that the constructed datasets exhibit stable trend characteristics, adhering to 24-hour and 168-hour periodicity patterns, with outlier ratios consistently below 3.26%, confirming data validity.","Compared to baseline, the improved algorithm achieves better fitness individuals in 80% of test regions under the same iterations.","When benchmarked against immediate swap-and-charge strategy, our algorithm achieves a peak cost reduction of 13.96%.","Moreover, peak user satisfaction reaches 98.57%, while the average iteration time remains below 0.6 seconds, demonstrating good computational efficiency.","The complete datasets and optimization algorithm are open-sourced at https://github.com/qingshufan/GA-EVLRU."],"url":"http://arxiv.org/abs/2504.07453v1"}
{"created":"2025-04-10 04:16:46","title":"WS-DETR: Robust Water Surface Object Detection through Vision-Radar Fusion with Detection Transformer","abstract":"Robust object detection for Unmanned Surface Vehicles (USVs) in complex water environments is essential for reliable navigation and operation. Specifically, water surface object detection faces challenges from blurred edges and diverse object scales. Although vision-radar fusion offers a feasible solution, existing approaches suffer from cross-modal feature conflicts, which negatively affect model robustness. To address this problem, we propose a robust vision-radar fusion model WS-DETR. In particular, we first introduce a Multi-Scale Edge Information Integration (MSEII) module to enhance edge perception and a Hierarchical Feature Aggregator (HiFA) to boost multi-scale object detection in the encoder. Then, we adopt self-moving point representations for continuous convolution and residual connection to efficiently extract irregular features under the scenarios of irregular point cloud data. To further mitigate cross-modal conflicts, an Adaptive Feature Interactive Fusion (AFIF) module is introduced to integrate visual and radar features through geometric alignment and semantic fusion. Extensive experiments on the WaterScenes dataset demonstrate that WS-DETR achieves state-of-the-art (SOTA) performance, maintaining its superiority even under adverse weather and lighting conditions.","sentences":["Robust object detection for Unmanned Surface Vehicles (USVs) in complex water environments is essential for reliable navigation and operation.","Specifically, water surface object detection faces challenges from blurred edges and diverse object scales.","Although vision-radar fusion offers a feasible solution, existing approaches suffer from cross-modal feature conflicts, which negatively affect model robustness.","To address this problem, we propose a robust vision-radar fusion model WS-DETR.","In particular, we first introduce a Multi-Scale Edge Information Integration (MSEII) module to enhance edge perception and a Hierarchical Feature Aggregator (HiFA) to boost multi-scale object detection in the encoder.","Then, we adopt self-moving point representations for continuous convolution and residual connection to efficiently extract irregular features under the scenarios of irregular point cloud data.","To further mitigate cross-modal conflicts, an Adaptive Feature Interactive Fusion (AFIF) module is introduced to integrate visual and radar features through geometric alignment and semantic fusion.","Extensive experiments on the WaterScenes dataset demonstrate that WS-DETR achieves state-of-the-art (SOTA) performance, maintaining its superiority even under adverse weather and lighting conditions."],"url":"http://arxiv.org/abs/2504.07441v1"}
{"created":"2025-04-10 04:09:47","title":"Revisiting LLM Evaluation through Mechanism Interpretability: a New Metric and Model Utility Law","abstract":"Large Language Models (LLMs) have become indispensable across academia, industry, and daily applications, yet current evaluation methods struggle to keep pace with their rapid development. In this paper, we analyze the core limitations of traditional evaluation pipelines and propose a novel metric, the Model Utilization Index (MUI), which introduces mechanism interpretability techniques to complement traditional performance metrics. MUI quantifies the extent to which a model leverages its capabilities to complete tasks. The core idea is that to assess an LLM's overall ability, we must evaluate not only its task performance but also the effort expended to achieve the outcome. Our extensive experiments reveal an inverse relationship between MUI and performance, from which we deduce a common trend observed in popular LLMs, which we term the Utility Law. Based on this, we derive four corollaries that address key challenges, including training judgement, the issue of data contamination, fairness in model comparison, and data diversity. We hope that our survey, novel metric, and utility law will foster mutual advancement in both evaluation and mechanism interpretability. Our code can be found at https://github.com/ALEX-nlp/MUI-Eva.","sentences":["Large Language Models (LLMs) have become indispensable across academia, industry, and daily applications, yet current evaluation methods struggle to keep pace with their rapid development.","In this paper, we analyze the core limitations of traditional evaluation pipelines and propose a novel metric, the Model Utilization Index (MUI), which introduces mechanism interpretability techniques to complement traditional performance metrics.","MUI quantifies the extent to which a model leverages its capabilities to complete tasks.","The core idea is that to assess an LLM's overall ability, we must evaluate not only its task performance but also the effort expended to achieve the outcome.","Our extensive experiments reveal an inverse relationship between MUI and performance, from which we deduce a common trend observed in popular LLMs, which we term the Utility Law.","Based on this, we derive four corollaries that address key challenges, including training judgement, the issue of data contamination, fairness in model comparison, and data diversity.","We hope that our survey, novel metric, and utility law will foster mutual advancement in both evaluation and mechanism interpretability.","Our code can be found at https://github.com/ALEX-nlp/MUI-Eva."],"url":"http://arxiv.org/abs/2504.07440v1"}
{"created":"2025-04-10 04:00:11","title":"LLM-Enabled Data Transmission in End-to-End Semantic Communication","abstract":"Emerging services such as augmented reality (AR) and virtual reality (VR) have increased the volume of data transmitted in wireless communication systems, revealing the limitations of traditional Shannon theory. To address these limitations, semantic communication has been proposed as a solution that prioritizes the meaning of messages over the exact transmission of bits. This paper explores semantic communication for text data transmission in end-to-end (E2E) systems through a novel approach called KG-LLM semantic communication, which integrates knowledge graph (KG) extraction and large language model (LLM) coding. In this method, the transmitter first utilizes a KG to extract key entities and relationships from sentences. The extracted information is then encoded using an LLM to obtain the semantic meaning. On the receiver side, messages are decoded using another LLM, while a bidirectional encoder representations from transformers (i.e., BERT) model further refines the reconstructed sentences for improved semantic similarity. The KG-LLM semantic communication method reduces the transmitted text data volume by 30% through KG-based compression and achieves 84\\% semantic similarity between the original and received messages. This demonstrates the KG-LLM methods efficiency and robustness in semantic communication systems, outperforming the deep learning-based semantic communication model (DeepSC), which achieves only 63%.","sentences":["Emerging services such as augmented reality (AR) and virtual reality (VR) have increased the volume of data transmitted in wireless communication systems, revealing the limitations of traditional Shannon theory.","To address these limitations, semantic communication has been proposed as a solution that prioritizes the meaning of messages over the exact transmission of bits.","This paper explores semantic communication for text data transmission in end-to-end (E2E) systems through a novel approach called KG-LLM semantic communication, which integrates knowledge graph (KG) extraction and large language model (LLM) coding.","In this method, the transmitter first utilizes a KG to extract key entities and relationships from sentences.","The extracted information is then encoded using an LLM to obtain the semantic meaning.","On the receiver side, messages are decoded using another LLM, while a bidirectional encoder representations from transformers (i.e., BERT) model further refines the reconstructed sentences for improved semantic similarity.","The KG-LLM semantic communication method reduces the transmitted text data volume by 30% through KG-based compression and achieves 84\\% semantic similarity between the original and received messages.","This demonstrates the KG-LLM methods efficiency and robustness in semantic communication systems, outperforming the deep learning-based semantic communication model (DeepSC), which achieves only 63%."],"url":"http://arxiv.org/abs/2504.07431v1"}
{"created":"2025-04-10 03:38:06","title":"Enhancing Player Enjoyment with a Two-Tier DRL and LLM-Based Agent System for Fighting Games","abstract":"Deep reinforcement learning (DRL) has effectively enhanced gameplay experiences and game design across various game genres. However, few studies on fighting game agents have focused explicitly on enhancing player enjoyment, a critical factor for both developers and players. To address this gap and establish a practical baseline for designing enjoyability-focused agents, we propose a two-tier agent (TTA) system and conducted experiments in the classic fighting game Street Fighter II. The first tier of TTA employs a task-oriented network architecture, modularized reward functions, and hybrid training to produce diverse and skilled DRL agents. In the second tier of TTA, a Large Language Model Hyper-Agent, leveraging players' playing data and feedback, dynamically selects suitable DRL opponents. In addition, we investigate and model several key factors that affect the enjoyability of the opponent. The experiments demonstrate improvements from 64. 36% to 156. 36% in the execution of advanced skills over baseline methods. The trained agents also exhibit distinct game-playing styles. Additionally, we conducted a small-scale user study, and the overall enjoyment in the player's feedback validates the effectiveness of our TTA system.","sentences":["Deep reinforcement learning (DRL) has effectively enhanced gameplay experiences and game design across various game genres.","However, few studies on fighting game agents have focused explicitly on enhancing player enjoyment, a critical factor for both developers and players.","To address this gap and establish a practical baseline for designing enjoyability-focused agents, we propose a two-tier agent (TTA) system and conducted experiments in the classic fighting game Street Fighter II.","The first tier of TTA employs a task-oriented network architecture, modularized reward functions, and hybrid training to produce diverse and skilled DRL agents.","In the second tier of TTA, a Large Language Model Hyper-Agent, leveraging players' playing data and feedback, dynamically selects suitable DRL opponents.","In addition, we investigate and model several key factors that affect the enjoyability of the opponent.","The experiments demonstrate improvements from 64.","36% to 156.","36% in the execution of advanced skills over baseline methods.","The trained agents also exhibit distinct game-playing styles.","Additionally, we conducted a small-scale user study, and the overall enjoyment in the player's feedback validates the effectiveness of our TTA system."],"url":"http://arxiv.org/abs/2504.07425v1"}
{"created":"2025-04-10 03:30:15","title":"Routing to the Right Expertise: A Trustworthy Judge for Instruction-based Image Editing","abstract":"Instruction-based Image Editing (IIE) models have made significantly improvement due to the progress of multimodal large language models (MLLMs) and diffusion models, which can understand and reason about complex editing instructions. In addition to advancing current IIE models, accurately evaluating their output has become increasingly critical and challenging. Current IIE evaluation methods and their evaluation procedures often fall short of aligning with human judgment and often lack explainability. To address these limitations, we propose JUdgement through Routing of Expertise (JURE). Each expert in JURE is a pre-selected model assumed to be equipped with an atomic expertise that can provide useful feedback to judge output, and the router dynamically routes the evaluation task of a given instruction and its output to appropriate experts, aggregating their feedback into a final judge. JURE is trustworthy in two aspects. First, it can effortlessly provide explanations about its judge by examining the routed experts and their feedback. Second, experimental results demonstrate that JURE is reliable by achieving superior alignment with human judgments, setting a new standard for automated IIE evaluation. Moreover, JURE's flexible design is future-proof - modular experts can be seamlessly replaced or expanded to accommodate advancements in IIE, maintaining consistently high evaluation quality. Our evaluation data and results are available at https://github.com/Cyyyyyrus/JURE.git.","sentences":["Instruction-based Image Editing (IIE) models have made significantly improvement due to the progress of multimodal large language models (MLLMs) and diffusion models, which can understand and reason about complex editing instructions.","In addition to advancing current IIE models, accurately evaluating their output has become increasingly critical and challenging.","Current IIE evaluation methods and their evaluation procedures often fall short of aligning with human judgment and often lack explainability.","To address these limitations, we propose JUdgement through Routing of Expertise (JURE).","Each expert in JURE is a pre-selected model assumed to be equipped with an atomic expertise that can provide useful feedback to judge output, and the router dynamically routes the evaluation task of a given instruction and its output to appropriate experts, aggregating their feedback into a final judge.","JURE is trustworthy in two aspects.","First, it can effortlessly provide explanations about its judge by examining the routed experts and their feedback.","Second, experimental results demonstrate that JURE is reliable by achieving superior alignment with human judgments, setting a new standard for automated IIE evaluation.","Moreover, JURE's flexible design is future-proof - modular experts can be seamlessly replaced or expanded to accommodate advancements in IIE, maintaining consistently high evaluation quality.","Our evaluation data and results are available at https://github.com/Cyyyyyrus/JURE.git."],"url":"http://arxiv.org/abs/2504.07424v1"}
{"created":"2025-04-10 03:27:25","title":"AgentAda: Skill-Adaptive Data Analytics for Tailored Insight Discovery","abstract":"We introduce AgentAda, the first LLM-powered analytics agent that can learn and use new analytics skills to extract more specialized insights. Unlike existing methods that require users to manually decide which data analytics method to apply, AgentAda automatically identifies the skill needed from a library of analytical skills to perform the analysis. This also allows AgentAda to use skills that existing LLMs cannot perform out of the box. The library covers a range of methods, including clustering, predictive modeling, and NLP techniques like BERT, which allow AgentAda to handle complex analytics tasks based on what the user needs. AgentAda's dataset-to-insight extraction strategy consists of three key steps: (I) a question generator to generate queries relevant to the user's goal and persona, (II) a hybrid Retrieval-Augmented Generation (RAG)-based skill matcher to choose the best data analytics skill from the skill library, and (III) a code generator that produces executable code based on the retrieved skill's documentation to extract key patterns. We also introduce KaggleBench, a benchmark of curated notebooks across diverse domains, to evaluate AgentAda's performance. We conducted a human evaluation demonstrating that AgentAda provides more insightful analytics than existing tools, with 48.78% of evaluators preferring its analyses, compared to 27.67% for the unskilled agent. We also propose a novel LLM-as-a-judge approach that we show is aligned with human evaluation as a way to automate insight quality evaluation at larger scale.","sentences":["We introduce AgentAda, the first LLM-powered analytics agent that can learn and use new analytics skills to extract more specialized insights.","Unlike existing methods that require users to manually decide which data analytics method to apply, AgentAda automatically identifies the skill needed from a library of analytical skills to perform the analysis.","This also allows AgentAda to use skills that existing LLMs cannot perform out of the box.","The library covers a range of methods, including clustering, predictive modeling, and NLP techniques like BERT, which allow AgentAda to handle complex analytics tasks based on what the user needs.","AgentAda's dataset-to-insight extraction strategy consists of three key steps: (I) a question generator to generate queries relevant to the user's goal and persona, (II) a hybrid Retrieval-Augmented Generation (RAG)-based skill matcher to choose the best data analytics skill from the skill library, and (III) a code generator that produces executable code based on the retrieved skill's documentation to extract key patterns.","We also introduce KaggleBench, a benchmark of curated notebooks across diverse domains, to evaluate AgentAda's performance.","We conducted a human evaluation demonstrating that AgentAda provides more insightful analytics than existing tools, with 48.78% of evaluators preferring its analyses, compared to 27.67% for the unskilled agent.","We also propose a novel LLM-as-a-judge approach that we show is aligned with human evaluation as a way to automate insight quality evaluation at larger scale."],"url":"http://arxiv.org/abs/2504.07421v1"}
{"created":"2025-04-10 03:25:56","title":"Emergency Communication: OTFS-Based Semantic Transmission with Diffusion Noise Suppression","abstract":"Due to their flexibility and dynamic coverage capabilities, Unmanned Aerial Vehicles (UAVs) have emerged as vital platforms for emergency communication in disaster-stricken areas. However, the complex channel conditions in high-speed mobile scenarios significantly impact the reliability and efficiency of traditional communication systems. This paper presents an intelligent emergency communication framework that integrates Orthogonal Time Frequency Space (OTFS) modulation, semantic communication, and a diffusion-based denoising module to address these challenges. OTFS ensures robust communication under dynamic channel conditions due to its superior anti-fading characteristics and adaptability to rapidly changing environments. Semantic communication further enhances transmission efficiency by focusing on key information extraction and reducing data redundancy. Moreover, a diffusion-based channel denoising module is proposed to leverage the gradual noise reduction process and statistical noise modeling, optimizing the accuracy of semantic information recovery. Experimental results demonstrate that the proposed solution significantly improves link stability and transmission performance in high-mobility UAV scenarios, achieving at least a 3dB SNR gain over existing methods.","sentences":["Due to their flexibility and dynamic coverage capabilities, Unmanned Aerial Vehicles (UAVs) have emerged as vital platforms for emergency communication in disaster-stricken areas.","However, the complex channel conditions in high-speed mobile scenarios significantly impact the reliability and efficiency of traditional communication systems.","This paper presents an intelligent emergency communication framework that integrates Orthogonal Time Frequency Space (OTFS) modulation, semantic communication, and a diffusion-based denoising module to address these challenges.","OTFS ensures robust communication under dynamic channel conditions due to its superior anti-fading characteristics and adaptability to rapidly changing environments.","Semantic communication further enhances transmission efficiency by focusing on key information extraction and reducing data redundancy.","Moreover, a diffusion-based channel denoising module is proposed to leverage the gradual noise reduction process and statistical noise modeling, optimizing the accuracy of semantic information recovery.","Experimental results demonstrate that the proposed solution significantly improves link stability and transmission performance in high-mobility UAV scenarios, achieving at least a 3dB SNR gain over existing methods."],"url":"http://arxiv.org/abs/2504.07420v1"}
{"created":"2025-04-10 03:25:20","title":"Exploring Vulnerabilities and Concerns in Solana Smart Contracts","abstract":"The Solana blockchain was created by Anatoly Yakovenko of Solana Labs and was introduced in 2017, employing a novel transaction verification method. However, at the same time, the innovation process introduced some new security issues. The frequent security incidents in smart contracts have not only caused enormous economic losses, but also undermined the credit system based on the blockchain. The security and reliability of smart contracts have become a new focus of research both domestically and abroad. This paper studies the current status of security analysis of Solana by researching Solana smart contract security analysis tools. This paper systematically sorts out the vulnerabilities existing in Solana smart contracts and gives examples of some vulnerabilities, summarizes the principles of security analysis tools, and comprehensively summarizes and details the security analysis tools in Solana smart contracts. The data of Solana smart contract security analysis tools are collected and compared with Ethereum, and the differences are analyzed and some tools are selected for practical testing.","sentences":["The Solana blockchain was created by Anatoly Yakovenko of Solana Labs and was introduced in 2017, employing a novel transaction verification method.","However, at the same time, the innovation process introduced some new security issues.","The frequent security incidents in smart contracts have not only caused enormous economic losses, but also undermined the credit system based on the blockchain.","The security and reliability of smart contracts have become a new focus of research both domestically and abroad.","This paper studies the current status of security analysis of Solana by researching Solana smart contract security analysis tools.","This paper systematically sorts out the vulnerabilities existing in Solana smart contracts and gives examples of some vulnerabilities, summarizes the principles of security analysis tools, and comprehensively summarizes and details the security analysis tools in Solana smart contracts.","The data of Solana smart contract security analysis tools are collected and compared with Ethereum, and the differences are analyzed and some tools are selected for practical testing."],"url":"http://arxiv.org/abs/2504.07419v1"}
{"created":"2025-04-10 03:24:21","title":"ThermoStereoRT: Thermal Stereo Matching in Real Time via Knowledge Distillation and Attention-based Refinement","abstract":"We introduce ThermoStereoRT, a real-time thermal stereo matching method designed for all-weather conditions that recovers disparity from two rectified thermal stereo images, envisioning applications such as night-time drone surveillance or under-bed cleaning robots. Leveraging a lightweight yet powerful backbone, ThermoStereoRT constructs a 3D cost volume from thermal images and employs multi-scale attention mechanisms to produce an initial disparity map. To refine this map, we design a novel channel and spatial attention module. Addressing the challenge of sparse ground truth data in thermal imagery, we utilize knowledge distillation to boost performance without increasing computational demands. Comprehensive evaluations on multiple datasets demonstrate that ThermoStereoRT delivers both real-time capacity and robust accuracy, making it a promising solution for real-world deployment in various challenging environments. Our code will be released on https://github.com/SJTU-ViSYS-team/ThermoStereoRT","sentences":["We introduce ThermoStereoRT, a real-time thermal stereo matching method designed for all-weather conditions that recovers disparity from two rectified thermal stereo images, envisioning applications such as night-time drone surveillance or under-bed cleaning robots.","Leveraging a lightweight yet powerful backbone, ThermoStereoRT constructs a 3D cost volume from thermal images and employs multi-scale attention mechanisms to produce an initial disparity map.","To refine this map, we design a novel channel and spatial attention module.","Addressing the challenge of sparse ground truth data in thermal imagery, we utilize knowledge distillation to boost performance without increasing computational demands.","Comprehensive evaluations on multiple datasets demonstrate that ThermoStereoRT delivers both real-time capacity and robust accuracy, making it a promising solution for real-world deployment in various challenging environments.","Our code will be released on https://github.com/SJTU-ViSYS-team/ThermoStereoRT"],"url":"http://arxiv.org/abs/2504.07418v1"}
{"created":"2025-04-10 02:22:23","title":"ClimateBench-M: A Multi-Modal Climate Data Benchmark with a Simple Generative Method","abstract":"Climate science studies the structure and dynamics of Earth's climate system and seeks to understand how climate changes over time, where the data is usually stored in the format of time series, recording the climate features, geolocation, time attributes, etc. Recently, much research attention has been paid to the climate benchmarks. In addition to the most common task of weather forecasting, several pioneering benchmark works are proposed for extending the modality, such as domain-specific applications like tropical cyclone intensity prediction and flash flood damage estimation, or climate statement and confidence level in the format of natural language. To further motivate the artificial general intelligence development for climate science, in this paper, we first contribute a multi-modal climate benchmark, i.e., ClimateBench-M, which aligns (1) the time series climate data from ERA5, (2) extreme weather events data from NOAA, and (3) satellite image data from NASA HLS based on a unified spatial-temporal granularity. Second, under each data modality, we also propose a simple but strong generative method that could produce competitive performance in weather forecasting, thunderstorm alerts, and crop segmentation tasks in the proposed ClimateBench-M. The data and code of ClimateBench-M are publicly available at https://github.com/iDEA-iSAIL-Lab-UIUC/ClimateBench-M.","sentences":["Climate science studies the structure and dynamics of Earth's climate system and seeks to understand how climate changes over time, where the data is usually stored in the format of time series, recording the climate features, geolocation, time attributes, etc.","Recently, much research attention has been paid to the climate benchmarks.","In addition to the most common task of weather forecasting, several pioneering benchmark works are proposed for extending the modality, such as domain-specific applications like tropical cyclone intensity prediction and flash flood damage estimation, or climate statement and confidence level in the format of natural language.","To further motivate the artificial general intelligence development for climate science, in this paper, we first contribute a multi-modal climate benchmark, i.e., ClimateBench-M, which aligns (1) the time series climate data from ERA5, (2) extreme weather events data from NOAA, and (3) satellite image data from NASA HLS based on a unified spatial-temporal granularity.","Second, under each data modality, we also propose a simple but strong generative method that could produce competitive performance in weather forecasting, thunderstorm alerts, and crop segmentation tasks in the proposed ClimateBench-M. The data and code of ClimateBench-M are publicly available at https://github.com/iDEA-iSAIL-Lab-UIUC/ClimateBench-M."],"url":"http://arxiv.org/abs/2504.07394v1"}
{"created":"2025-04-10 02:20:45","title":"State Estimation Using Particle Filtering in Adaptive Machine Learning Methods: Integrating Q-Learning and NEAT Algorithms with Noisy Radar Measurements","abstract":"Reliable state estimation is essential for autonomous systems operating in complex, noisy environments. Classical filtering approaches, such as the Kalman filter, can struggle when facing nonlinear dynamics or non-Gaussian noise, and even more flexible particle filters often encounter sample degeneracy or high computational costs in large-scale domains. Meanwhile, adaptive machine learning techniques, including Q-learning and neuroevolutionary algorithms such as NEAT, rely heavily on accurate state feedback to guide learning; when sensor data are imperfect, these methods suffer from degraded convergence and suboptimal performance. In this paper, we propose an integrated framework that unifies particle filtering with Q-learning and NEAT to explicitly address the challenge of noisy measurements. By refining radar-based observations into reliable state estimates, our particle filter drives more stable policy updates (in Q-learning) or controller evolution (in NEAT), allowing both reinforcement learning and neuroevolution to converge faster, achieve higher returns or fitness, and exhibit greater resilience to sensor uncertainty. Experiments on grid-based navigation and a simulated car environment highlight consistent gains in training stability, final performance, and success rates over baselines lacking advanced filtering. Altogether, these findings underscore that accurate state estimation is not merely a preprocessing step, but a vital component capable of substantially enhancing adaptive machine learning in real-world applications plagued by sensor noise.","sentences":["Reliable state estimation is essential for autonomous systems operating in complex, noisy environments.","Classical filtering approaches, such as the Kalman filter, can struggle when facing nonlinear dynamics or non-Gaussian noise, and even more flexible particle filters often encounter sample degeneracy or high computational costs in large-scale domains.","Meanwhile, adaptive machine learning techniques, including Q-learning and neuroevolutionary algorithms such as NEAT, rely heavily on accurate state feedback to guide learning; when sensor data are imperfect, these methods suffer from degraded convergence and suboptimal performance.","In this paper, we propose an integrated framework that unifies particle filtering with Q-learning and NEAT to explicitly address the challenge of noisy measurements.","By refining radar-based observations into reliable state estimates, our particle filter drives more stable policy updates (in Q-learning) or controller evolution (in NEAT), allowing both reinforcement learning and neuroevolution to converge faster, achieve higher returns or fitness, and exhibit greater resilience to sensor uncertainty.","Experiments on grid-based navigation and a simulated car environment highlight consistent gains in training stability, final performance, and success rates over baselines lacking advanced filtering.","Altogether, these findings underscore that accurate state estimation is not merely a preprocessing step, but a vital component capable of substantially enhancing adaptive machine learning in real-world applications plagued by sensor noise."],"url":"http://arxiv.org/abs/2504.07393v1"}
{"created":"2025-04-10 02:20:18","title":"ID-Booth: Identity-consistent Face Generation with Diffusion Models","abstract":"Recent advances in generative modeling have enabled the generation of high-quality synthetic data that is applicable in a variety of domains, including face recognition. Here, state-of-the-art generative models typically rely on conditioning and fine-tuning of powerful pretrained diffusion models to facilitate the synthesis of realistic images of a desired identity. Yet, these models often do not consider the identity of subjects during training, leading to poor consistency between generated and intended identities. In contrast, methods that employ identity-based training objectives tend to overfit on various aspects of the identity, and in turn, lower the diversity of images that can be generated. To address these issues, we present in this paper a novel generative diffusion-based framework, called ID-Booth. ID-Booth consists of a denoising network responsible for data generation, a variational auto-encoder for mapping images to and from a lower-dimensional latent space and a text encoder that allows for prompt-based control over the generation procedure. The framework utilizes a novel triplet identity training objective and enables identity-consistent image generation while retaining the synthesis capabilities of pretrained diffusion models. Experiments with a state-of-the-art latent diffusion model and diverse prompts reveal that our method facilitates better intra-identity consistency and inter-identity separability than competing methods, while achieving higher image diversity. In turn, the produced data allows for effective augmentation of small-scale datasets and training of better-performing recognition models in a privacy-preserving manner. The source code for the ID-Booth framework is publicly available at https://github.com/dariant/ID-Booth.","sentences":["Recent advances in generative modeling have enabled the generation of high-quality synthetic data that is applicable in a variety of domains, including face recognition.","Here, state-of-the-art generative models typically rely on conditioning and fine-tuning of powerful pretrained diffusion models to facilitate the synthesis of realistic images of a desired identity.","Yet, these models often do not consider the identity of subjects during training, leading to poor consistency between generated and intended identities.","In contrast, methods that employ identity-based training objectives tend to overfit on various aspects of the identity, and in turn, lower the diversity of images that can be generated.","To address these issues, we present in this paper a novel generative diffusion-based framework, called ID-Booth.","ID-Booth consists of a denoising network responsible for data generation, a variational auto-encoder for mapping images to and from a lower-dimensional latent space and a text encoder that allows for prompt-based control over the generation procedure.","The framework utilizes a novel triplet identity training objective and enables identity-consistent image generation while retaining the synthesis capabilities of pretrained diffusion models.","Experiments with a state-of-the-art latent diffusion model and diverse prompts reveal that our method facilitates better intra-identity consistency and inter-identity separability than competing methods, while achieving higher image diversity.","In turn, the produced data allows for effective augmentation of small-scale datasets and training of better-performing recognition models in a privacy-preserving manner.","The source code for the ID-Booth framework is publicly available at https://github.com/dariant/ID-Booth."],"url":"http://arxiv.org/abs/2504.07392v1"}
{"created":"2025-04-10 02:19:03","title":"Task-Circuit Quantization: Leveraging Knowledge Localization and Interpretability for Compression","abstract":"Post-training quantization (PTQ) reduces a model's memory footprint by mapping full precision weights into low bit weights without costly retraining, but can degrade its downstream performance especially in low 2- to 3-bit settings. We develop a new mixed-precision PTQ approach, Task-Circuit Quantization (TaCQ), that draws parallels to automated circuit discovery, directly conditioning the quantization process on specific weight circuits -- which we define as sets of weights associated with downstream task performance. These weights are kept as 16-bit weights, while others are quantized, maintaining performance while only adding a marginal memory cost. Specifically, TaCQ contrasts unquantized model weights with a uniformly-quantized model to estimate the expected change in weights due to quantization and uses gradient information to predict the resulting impact on task performance, allowing us to preserve task-specific weights. We compare TaCQ-based quantization to existing mixed-precision quantization methods when conditioning both on general-purpose and task-specific data. Across QA, math reasoning, and text-to-SQL tasks for both Llama-3 and Qwen2.5, we find that TaCQ outperforms baselines using the same calibration data and a lower weight budget, achieving major improvements in the 2 and 3-bit regime. With only 3.1 bits we are able to recover 96% of Llama-3-8B-Instruct's unquantized 16-bit MMLU performance, obtaining a 5.25% absolute improvement over SPQR. We also observe consistently large gains over existing methods in the 2-bit regime, with an average gain of 14.74% over the strongest baseline, SliM-LLM. Moreover, we observe a 7.20% gain without conditioning on specific tasks, showing TaCQ's ability to identify important weights is not limited to task-conditioned settings.","sentences":["Post-training quantization (PTQ) reduces a model's memory footprint by mapping full precision weights into low bit weights without costly retraining, but can degrade its downstream performance especially in low","2- to 3-bit settings.","We develop a new mixed-precision PTQ approach, Task-Circuit Quantization (TaCQ), that draws parallels to automated circuit discovery, directly conditioning the quantization process on specific weight circuits -- which we define as sets of weights associated with downstream task performance.","These weights are kept as 16-bit weights, while others are quantized, maintaining performance while only adding a marginal memory cost.","Specifically, TaCQ contrasts unquantized model weights with a uniformly-quantized model to estimate the expected change in weights due to quantization and uses gradient information to predict the resulting impact on task performance, allowing us to preserve task-specific weights.","We compare TaCQ-based quantization to existing mixed-precision quantization methods when conditioning both on general-purpose and task-specific data.","Across QA, math reasoning, and text-to-SQL tasks for both Llama-3 and Qwen2.5, we find that TaCQ outperforms baselines using the same calibration data and a lower weight budget, achieving major improvements in the 2 and 3-bit regime.","With only 3.1 bits we are able to recover 96% of Llama-3-8B-Instruct's unquantized 16-bit MMLU performance, obtaining a 5.25% absolute improvement over SPQR.","We also observe consistently large gains over existing methods in the 2-bit regime, with an average gain of 14.74% over the strongest baseline, SliM-LLM.","Moreover, we observe a 7.20% gain without conditioning on specific tasks, showing TaCQ's ability to identify important weights is not limited to task-conditioned settings."],"url":"http://arxiv.org/abs/2504.07389v1"}
{"created":"2025-04-10 01:29:50","title":"Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction","abstract":"Predicting hand motion is critical for understanding human intentions and bridging the action space between human movements and robot manipulations. Existing hand trajectory prediction (HTP) methods forecast the future hand waypoints in 3D space conditioned on past egocentric observations. However, such models are only designed to accommodate 2D egocentric video inputs. There is a lack of awareness of multimodal environmental information from both 2D and 3D observations, hindering the further improvement of 3D HTP performance. In addition, these models overlook the synergy between hand movements and headset camera egomotion, either predicting hand trajectories in isolation or encoding egomotion only from past frames. To address these limitations, we propose novel diffusion models (MMTwin) for multimodal 3D hand trajectory prediction. MMTwin is designed to absorb multimodal information as input encompassing 2D RGB images, 3D point clouds, past hand waypoints, and text prompt. Besides, two latent diffusion models, the egomotion diffusion and the HTP diffusion as twins, are integrated into MMTwin to predict camera egomotion and future hand trajectories concurrently. We propose a novel hybrid Mamba-Transformer module as the denoising model of the HTP diffusion to better fuse multimodal features. The experimental results on three publicly available datasets and our self-recorded data demonstrate that our proposed MMTwin can predict plausible future 3D hand trajectories compared to the state-of-the-art baselines, and generalizes well to unseen environments. The code and pretrained models will be released at https://github.com/IRMVLab/MMTwin.","sentences":["Predicting hand motion is critical for understanding human intentions and bridging the action space between human movements and robot manipulations.","Existing hand trajectory prediction (HTP) methods forecast the future hand waypoints in 3D space conditioned on past egocentric observations.","However, such models are only designed to accommodate 2D egocentric video inputs.","There is a lack of awareness of multimodal environmental information from both 2D and 3D observations, hindering the further improvement of 3D HTP performance.","In addition, these models overlook the synergy between hand movements and headset camera egomotion, either predicting hand trajectories in isolation or encoding egomotion only from past frames.","To address these limitations, we propose novel diffusion models (MMTwin) for multimodal 3D hand trajectory prediction.","MMTwin is designed to absorb multimodal information as input encompassing 2D RGB images, 3D point clouds, past hand waypoints, and text prompt.","Besides, two latent diffusion models, the egomotion diffusion and the HTP diffusion as twins, are integrated into MMTwin to predict camera egomotion and future hand trajectories concurrently.","We propose a novel hybrid Mamba-Transformer module as the denoising model of the HTP diffusion to better fuse multimodal features.","The experimental results on three publicly available datasets and our self-recorded data demonstrate that our proposed MMTwin can predict plausible future 3D hand trajectories compared to the state-of-the-art baselines, and generalizes well to unseen environments.","The code and pretrained models will be released at https://github.com/IRMVLab/MMTwin."],"url":"http://arxiv.org/abs/2504.07375v1"}
{"created":"2025-04-10 01:25:41","title":"ChronoFormer: Time-Aware Transformer Architectures for Structured Clinical Event Modeling","abstract":"The temporal complexity of electronic health record (EHR) data presents significant challenges for predicting clinical outcomes using machine learning. This paper proposes ChronoFormer, an innovative transformer based architecture specifically designed to encode and leverage temporal dependencies in longitudinal patient data. ChronoFormer integrates temporal embeddings, hierarchical attention mechanisms, and domain specific masking techniques. Extensive experiments conducted on three benchmark tasks mortality prediction, readmission prediction, and long term comorbidity onset demonstrate substantial improvements over current state of the art methods. Furthermore, detailed analyses of attention patterns underscore ChronoFormer's capability to capture clinically meaningful long range temporal relationships.","sentences":["The temporal complexity of electronic health record (EHR) data presents significant challenges for predicting clinical outcomes using machine learning.","This paper proposes ChronoFormer, an innovative transformer based architecture specifically designed to encode and leverage temporal dependencies in longitudinal patient data.","ChronoFormer integrates temporal embeddings, hierarchical attention mechanisms, and domain specific masking techniques.","Extensive experiments conducted on three benchmark tasks mortality prediction, readmission prediction, and long term comorbidity onset demonstrate substantial improvements over current state of the art methods.","Furthermore, detailed analyses of attention patterns underscore ChronoFormer's capability to capture clinically meaningful long range temporal relationships."],"url":"http://arxiv.org/abs/2504.07373v1"}
{"created":"2025-04-10 01:15:02","title":"Incremental Planar Nearest Neighbor Queries with Optimal Query Time","abstract":"In this paper we show that two-dimensional nearest neighbor queries can be answered in optimal $O(\\log n)$ time while supporting insertions in $O(\\log^{1+\\varepsilon}n)$ time. No previous data structure was known that supports $O(\\log n)$-time queries and polylog-time insertions. In order to achieve logarithmic queries our data structure uses a new technique related to fractional cascading that leverages the inherent geometry of this problem. Our method can be also used in other semi-dynamic scenarios.","sentences":["In this paper we show that two-dimensional nearest neighbor queries can be answered in optimal $O(\\log n)$ time while supporting insertions in $O(\\log^{1+\\varepsilon}n)$ time.","No previous data structure was known that supports $O(\\log n)$-time queries and polylog-time insertions.","In order to achieve logarithmic queries our data structure uses a new technique related to fractional cascading that leverages the inherent geometry of this problem.","Our method can be also used in other semi-dynamic scenarios."],"url":"http://arxiv.org/abs/2504.07366v1"}
{"created":"2025-04-10 01:06:05","title":"Augmented Shuffle Protocols for Accurate and Robust Frequency Estimation under Differential Privacy","abstract":"The shuffle model of DP (Differential Privacy) provides high utility by introducing a shuffler that randomly shuffles noisy data sent from users. However, recent studies show that existing shuffle protocols suffer from the following two major drawbacks. First, they are vulnerable to local data poisoning attacks, which manipulate the statistics about input data by sending crafted data, especially when the privacy budget epsilon is small. Second, the actual value of epsilon is increased by collusion attacks by the data collector and users.   In this paper, we address these two issues by thoroughly exploring the potential of the augmented shuffle model, which allows the shuffler to perform additional operations, such as random sampling and dummy data addition. Specifically, we propose a generalized framework for local-noise-free protocols in which users send (encrypted) input data to the shuffler without adding noise. We show that this generalized protocol provides DP and is robust to the above two attacks if a simpler mechanism that performs the same process on binary input data provides DP. Based on this framework, we propose three concrete protocols providing DP and robustness against the two attacks. Our first protocol generates the number of dummy values for each item from a binomial distribution and provides higher utility than several state-of-the-art existing shuffle protocols. Our second protocol significantly improves the utility of our first protocol by introducing a novel dummy-count distribution: asymmetric two-sided geometric distribution. Our third protocol is a special case of our second protocol and provides pure epsilon-DP. We show the effectiveness of our protocols through theoretical analysis and comprehensive experiments.","sentences":["The shuffle model of DP (Differential Privacy) provides high utility by introducing a shuffler that randomly shuffles noisy data sent from users.","However, recent studies show that existing shuffle protocols suffer from the following two major drawbacks.","First, they are vulnerable to local data poisoning attacks, which manipulate the statistics about input data by sending crafted data, especially when the privacy budget epsilon is small.","Second, the actual value of epsilon is increased by collusion attacks by the data collector and users.   ","In this paper, we address these two issues by thoroughly exploring the potential of the augmented shuffle model, which allows the shuffler to perform additional operations, such as random sampling and dummy data addition.","Specifically, we propose a generalized framework for local-noise-free protocols in which users send (encrypted) input data to the shuffler without adding noise.","We show that this generalized protocol provides DP and is robust to the above two attacks if a simpler mechanism that performs the same process on binary input data provides DP.","Based on this framework, we propose three concrete protocols providing DP and robustness against the two attacks.","Our first protocol generates the number of dummy values for each item from a binomial distribution and provides higher utility than several state-of-the-art existing shuffle protocols.","Our second protocol significantly improves the utility of our first protocol by introducing a novel dummy-count distribution: asymmetric two-sided geometric distribution.","Our third protocol is a special case of our second protocol and provides pure epsilon-DP.","We show the effectiveness of our protocols through theoretical analysis and comprehensive experiments."],"url":"http://arxiv.org/abs/2504.07362v1"}
{"created":"2025-04-10 01:02:37","title":"Enhancing Time Series Forecasting via Multi-Level Text Alignment with LLMs","abstract":"The adaptation of large language models (LLMs) to time series forecasting poses unique challenges, as time series data is continuous in nature, while LLMs operate on discrete tokens. Despite the success of LLMs in natural language processing (NLP) and other structured domains, aligning time series data with language-based representations while maintaining both predictive accuracy and interpretability remains a significant hurdle. Existing methods have attempted to reprogram time series data into text-based forms, but these often fall short in delivering meaningful, interpretable results. In this paper, we propose a multi-level text alignment framework for time series forecasting using LLMs that not only improves prediction accuracy but also enhances the interpretability of time series representations. Our method decomposes time series into trend, seasonal, and residual components, which are then reprogrammed into component-specific text representations. We introduce a multi-level alignment mechanism, where component-specific embeddings are aligned with pre-trained word tokens, enabling more interpretable forecasts. Experiments on multiple datasets demonstrate that our method outperforms state-of-the-art models in accuracy while providing good interpretability.","sentences":["The adaptation of large language models (LLMs) to time series forecasting poses unique challenges, as time series data is continuous in nature, while LLMs operate on discrete tokens.","Despite the success of LLMs in natural language processing (NLP) and other structured domains, aligning time series data with language-based representations while maintaining both predictive accuracy and interpretability remains a significant hurdle.","Existing methods have attempted to reprogram time series data into text-based forms, but these often fall short in delivering meaningful, interpretable results.","In this paper, we propose a multi-level text alignment framework for time series forecasting using LLMs that not only improves prediction accuracy but also enhances the interpretability of time series representations.","Our method decomposes time series into trend, seasonal, and residual components, which are then reprogrammed into component-specific text representations.","We introduce a multi-level alignment mechanism, where component-specific embeddings are aligned with pre-trained word tokens, enabling more interpretable forecasts.","Experiments on multiple datasets demonstrate that our method outperforms state-of-the-art models in accuracy while providing good interpretability."],"url":"http://arxiv.org/abs/2504.07360v1"}
{"created":"2025-04-10 00:05:35","title":"Quantum-Inspired Genetic Algorithm for Robust Source Separation in Smart City Acoustics","abstract":"The cacophony of urban sounds presents a significant challenge for smart city applications that rely on accurate acoustic scene analysis. Effectively analyzing these complex soundscapes, often characterized by overlapping sound sources, diverse acoustic events, and unpredictable noise levels, requires precise source separation. This task becomes more complicated when only limited training data is available. This paper introduces a novel Quantum-Inspired Genetic Algorithm (p-QIGA) for source separation, drawing inspiration from quantum information theory to enhance acoustic scene analysis in smart cities. By leveraging quantum superposition for efficient solution space exploration and entanglement to handle correlated sources, p-QIGA achieves robust separation even with limited data. These quantum-inspired concepts are integrated into a genetic algorithm framework to optimize source separation parameters. The effectiveness of our approach is demonstrated on two datasets: the TAU Urban Acoustic Scenes 2020 Mobile dataset, representing typical urban soundscapes, and the Silent Cities dataset, capturing quieter urban environments during the COVID-19 pandemic. Experimental results show that the p-QIGA achieves accuracy comparable to state-of-the-art methods while exhibiting superior resilience to noise and limited training data, achieving up to 8.2 dB signal-to-distortion ratio (SDR) in noisy environments and outperforming baseline methods by up to 2 dB with only 10% of the training data. This research highlights the potential of p-QIGA to advance acoustic signal processing in smart cities, particularly for noise pollution monitoring and acoustic surveillance.","sentences":["The cacophony of urban sounds presents a significant challenge for smart city applications that rely on accurate acoustic scene analysis.","Effectively analyzing these complex soundscapes, often characterized by overlapping sound sources, diverse acoustic events, and unpredictable noise levels, requires precise source separation.","This task becomes more complicated when only limited training data is available.","This paper introduces a novel Quantum-Inspired Genetic Algorithm (p-QIGA) for source separation, drawing inspiration from quantum information theory to enhance acoustic scene analysis in smart cities.","By leveraging quantum superposition for efficient solution space exploration and entanglement to handle correlated sources, p-QIGA achieves robust separation even with limited data.","These quantum-inspired concepts are integrated into a genetic algorithm framework to optimize source separation parameters.","The effectiveness of our approach is demonstrated on two datasets: the TAU Urban Acoustic Scenes 2020 Mobile dataset, representing typical urban soundscapes, and the Silent Cities dataset, capturing quieter urban environments during the COVID-19 pandemic.","Experimental results show that the p-QIGA achieves accuracy comparable to state-of-the-art methods while exhibiting superior resilience to noise and limited training data, achieving up to 8.2 dB signal-to-distortion ratio (SDR) in noisy environments and outperforming baseline methods by up to 2 dB with only 10% of the training data.","This research highlights the potential of p-QIGA to advance acoustic signal processing in smart cities, particularly for noise pollution monitoring and acoustic surveillance."],"url":"http://arxiv.org/abs/2504.07345v1"}
{"created":"2025-04-09 23:57:44","title":"Code Generation with Small Language Models: A Deep Evaluation on Codeforces","abstract":"Large Language Models (LLMs) have demonstrated capabilities in code generation, potentially boosting developer productivity. However, their widespread adoption remains limited by high computational costs, significant energy demands, and security risks such as data leakage and adversarial attacks. As a lighter-weight alternative, Small Language Models (SLMs) offer faster inference, lower deployment overhead, and better adaptability to domain-specific tasks, making them an attractive option for real-world applications. While prior research has benchmarked LLMs on competitive programming tasks, such evaluations often focus narrowly on metrics like Elo scores or pass rates, overlooking deeper insights into model behavior, failure patterns, and problem diversity. Furthermore, the potential of SLMs to tackle complex tasks such as competitive programming remains underexplored. In this study, we benchmark five open SLMs - LLAMA 3.2 3B, GEMMA 2 9B, GEMMA 3 12B, DEEPSEEK-R1 14B, and PHI-4 14B - across 280 Codeforces problems spanning Elo ratings from 800 to 2100 and covering 36 distinct topics. All models were tasked with generating Python solutions. PHI-4 14B achieved the best performance among SLMs, with a pass@3 of 63.6%, approaching the proprietary O3-MINI-HIGH (86.8%). In addition, we evaluated PHI-4 14B on C++ and found that combining outputs from both Python and C++ increases its aggregated pass@3 to 73.6%. A qualitative analysis of PHI-4 14B's incorrect outputs revealed that some failures were due to minor implementation issues - such as handling edge cases or correcting variable initialization - rather than deeper reasoning flaws.","sentences":["Large Language Models (LLMs) have demonstrated capabilities in code generation, potentially boosting developer productivity.","However, their widespread adoption remains limited by high computational costs, significant energy demands, and security risks such as data leakage and adversarial attacks.","As a lighter-weight alternative, Small Language Models (SLMs) offer faster inference, lower deployment overhead, and better adaptability to domain-specific tasks, making them an attractive option for real-world applications.","While prior research has benchmarked LLMs on competitive programming tasks, such evaluations often focus narrowly on metrics like Elo scores or pass rates, overlooking deeper insights into model behavior, failure patterns, and problem diversity.","Furthermore, the potential of SLMs to tackle complex tasks such as competitive programming remains underexplored.","In this study, we benchmark five open SLMs - LLAMA 3.2 3B, GEMMA 2 9B, GEMMA 3 12B, DEEPSEEK-R1 14B, and PHI-4 14B - across 280 Codeforces problems spanning Elo ratings from 800 to 2100 and covering 36 distinct topics.","All models were tasked with generating Python solutions.","PHI-4 14B achieved the best performance among SLMs, with a pass@3 of 63.6%, approaching the proprietary O3-MINI-HIGH (86.8%).","In addition, we evaluated PHI-4 14B on C++ and found that combining outputs from both Python and C++ increases its aggregated pass@3 to 73.6%.","A qualitative analysis of PHI-4 14B's incorrect outputs revealed that some failures were due to minor implementation issues - such as handling edge cases or correcting variable initialization - rather than deeper reasoning flaws."],"url":"http://arxiv.org/abs/2504.07343v1"}
{"created":"2025-04-09 23:57:30","title":"Leveraging deep learning for plant disease identification: a bibliometric analysis in SCOPUS from 2018 to 2024","abstract":"This work aimed to present a bibliometric analysis of deep learning research for plant disease identification, with a special focus on generative modeling. A thorough analysis of SCOPUS-sourced bibliometric data from 253 documents was performed. Key performance metrics such as accuracy, precision, recall, and F1-score were analyzed for generative modeling. The findings highlighted significant contributions from some authors Too and Arnal Barbedo, whose works had notable citation counts, suggesting their influence on the academic community. Co-authorship networks revealed strong collaborative clusters, while keyword analysis identified emerging research gaps. This study highlights the role of collaboration and citation metrics in shaping research directions and enhancing the impact of scholarly work in applications of deep learning to plant disease identification. Future research should explore the methodologies of highly cited studies to inform best practices and policy-making.","sentences":["This work aimed to present a bibliometric analysis of deep learning research for plant disease identification, with a special focus on generative modeling.","A thorough analysis of SCOPUS-sourced bibliometric data from 253 documents was performed.","Key performance metrics such as accuracy, precision, recall, and F1-score were analyzed for generative modeling.","The findings highlighted significant contributions from some authors Too and Arnal Barbedo, whose works had notable citation counts, suggesting their influence on the academic community.","Co-authorship networks revealed strong collaborative clusters, while keyword analysis identified emerging research gaps.","This study highlights the role of collaboration and citation metrics in shaping research directions and enhancing the impact of scholarly work in applications of deep learning to plant disease identification.","Future research should explore the methodologies of highly cited studies to inform best practices and policy-making."],"url":"http://arxiv.org/abs/2504.07342v1"}
{"created":"2025-04-09 23:29:08","title":"Objaverse++: Curated 3D Object Dataset with Quality Annotations","abstract":"This paper presents Objaverse++, a curated subset of Objaverse enhanced with detailed attribute annotations by human experts. Recent advances in 3D content generation have been driven by large-scale datasets such as Objaverse, which contains over 800,000 3D objects collected from the Internet. Although Objaverse represents the largest available 3D asset collection, its utility is limited by the predominance of low-quality models. To address this limitation, we manually annotate 10,000 3D objects with detailed attributes, including aesthetic quality scores, texture color classifications, multi-object composition flags, transparency characteristics, etc. Then, we trained a neural network capable of annotating the tags for the rest of the Objaverse dataset. Through experiments and a user study on generation results, we demonstrate that models pre-trained on our quality-focused subset achieve better performance than those trained on the larger dataset of Objaverse in image-to-3D generation tasks. In addition, by comparing multiple subsets of training data filtered by our tags, our results show that the higher the data quality, the faster the training loss converges. These findings suggest that careful curation and rich annotation can compensate for the raw dataset size, potentially offering a more efficient path to develop 3D generative models. We release our enhanced dataset of approximately 500,000 curated 3D models to facilitate further research on various downstream tasks in 3D computer vision. In the near future, we aim to extend our annotations to cover the entire Objaverse dataset.","sentences":["This paper presents Objaverse++, a curated subset of Objaverse enhanced with detailed attribute annotations by human experts.","Recent advances in 3D content generation have been driven by large-scale datasets such as Objaverse, which contains over 800,000 3D objects collected from the Internet.","Although Objaverse represents the largest available 3D asset collection, its utility is limited by the predominance of low-quality models.","To address this limitation, we manually annotate 10,000 3D objects with detailed attributes, including aesthetic quality scores, texture color classifications, multi-object composition flags, transparency characteristics, etc.","Then, we trained a neural network capable of annotating the tags for the rest of the Objaverse dataset.","Through experiments and a user study on generation results, we demonstrate that models pre-trained on our quality-focused subset achieve better performance than those trained on the larger dataset of Objaverse in image-to-3D generation tasks.","In addition, by comparing multiple subsets of training data filtered by our tags, our results show that the higher the data quality, the faster the training loss converges.","These findings suggest that careful curation and rich annotation can compensate for the raw dataset size, potentially offering a more efficient path to develop 3D generative models.","We release our enhanced dataset of approximately 500,000 curated 3D models to facilitate further research on various downstream tasks in 3D computer vision.","In the near future, we aim to extend our annotations to cover the entire Objaverse dataset."],"url":"http://arxiv.org/abs/2504.07334v1"}
{"created":"2025-04-09 22:57:06","title":"MatBase Algorithm for Translating Entity-Relationship Data Models into (Elementary) Mathematical Data Model Schemes","abstract":"This paper presents a pseudocode algorithm for translating Entity-Relationship data models into (Elementary) Mathematical Data Model schemes. We prove that this algorithm is linear, solid, complete, and optimal. We apply this algorithm to an Entity-Relationship data model for a teaching sub-universe. We also provide the main additional features added to the implementation of this algorithm in MatBase, our intelligent knowledge and database management system prototype based on both the Entity-Relationship, (Elementary) Mathematical, and Relational Data Models.","sentences":["This paper presents a pseudocode algorithm for translating Entity-Relationship data models into (Elementary) Mathematical Data Model schemes.","We prove that this algorithm is linear, solid, complete, and optimal.","We apply this algorithm to an Entity-Relationship data model for a teaching sub-universe.","We also provide the main additional features added to the implementation of this algorithm in MatBase, our intelligent knowledge and database management system prototype based on both the Entity-Relationship, (Elementary) Mathematical, and Relational Data Models."],"url":"http://arxiv.org/abs/2504.07326v1"}
{"created":"2025-04-09 22:32:57","title":"Multilingual MFA: Forced Alignment on Low-Resource Related Languages","abstract":"We compare the outcomes of multilingual and crosslingual training for related and unrelated Australian languages with similar phonological inventories. We use the Montreal Forced Aligner to train acoustic models from scratch and adapt a large English model, evaluating results against seen data, unseen data (seen language), and unseen data and language. Results indicate benefits of adapting the English baseline model for previously unseen languages.","sentences":["We compare the outcomes of multilingual and crosslingual training for related and unrelated Australian languages with similar phonological inventories.","We use the Montreal Forced Aligner to train acoustic models from scratch and adapt a large English model, evaluating results against seen data, unseen data (seen language), and unseen data and language.","Results indicate benefits of adapting the English baseline model for previously unseen languages."],"url":"http://arxiv.org/abs/2504.07315v1"}
{"created":"2025-04-09 22:28:21","title":"The Gendered Algorithm: Navigating Financial Inclusion & Equity in AI-facilitated Access to Credit","abstract":"Various companies are developing apps that collect mobile phone data and use machine learning (ML) to provide credit scores - and subsequently, opportunities to access loans - to groups left out of traditional banking. This paper draws on interview data with leaders, investors, and data scientists at fintech companies developing ML-based alternative lending apps in low- and middle-income countries to answer the question: In what ways do the underlying logics, design choices, and management decisions of ML-based alternative lending tools by fintechs embed or challenge gender biases, and how do these choices influence gender equity in access to finance? Findings reveal developers follow 'gender blind' approaches, grounded in beliefs that ML is objective and data reflects the truth. This leads to a lack of grappling with the ways data, features for creditworthiness, and access to apps are gendered. Overall, tools increase access to finance, but not gender equitably: Interviewees report less women access loans and receive lower loan amounts than men, despite women being better repayers. Fintechs identify demand- and supply-side reasons for gender differences, but frame them as outside their responsibility. However, that women are observed as better repayers reveals a market inefficiency and potential discriminatory effect, which can be further linked to profit optimization objectives. This research introduces the concept of 'encoded gender norms', whereby without explicit attention to the gendered nature of data and algorithmic design, AI technologies reproduce existing inequalities. In doing so, they reinforce gender norms as self-fulfilling prophecies. The idea that AI technology is inherently objective and, when left alone, 'fair', is seductive and misleading. In reality, algorithms reflect the perspectives, priorities, and values of the people and institutions that design them.","sentences":["Various companies are developing apps that collect mobile phone data and use machine learning (ML) to provide credit scores - and subsequently, opportunities to access loans - to groups left out of traditional banking.","This paper draws on interview data with leaders, investors, and data scientists at fintech companies developing ML-based alternative lending apps in low- and middle-income countries to answer the question: In what ways do the underlying logics, design choices, and management decisions of ML-based alternative lending tools by fintechs embed or challenge gender biases, and how do these choices influence gender equity in access to finance?","Findings reveal developers follow 'gender blind' approaches, grounded in beliefs that ML is objective and data reflects the truth.","This leads to a lack of grappling with the ways data, features for creditworthiness, and access to apps are gendered.","Overall, tools increase access to finance, but not gender equitably: Interviewees report less women access loans and receive lower loan amounts than men, despite women being better repayers.","Fintechs identify demand- and supply-side reasons for gender differences, but frame them as outside their responsibility.","However, that women are observed as better repayers reveals a market inefficiency and potential discriminatory effect, which can be further linked to profit optimization objectives.","This research introduces the concept of 'encoded gender norms', whereby without explicit attention to the gendered nature of data and algorithmic design, AI technologies reproduce existing inequalities.","In doing so, they reinforce gender norms as self-fulfilling prophecies.","The idea that AI technology is inherently objective and, when left alone, 'fair', is seductive and misleading.","In reality, algorithms reflect the perspectives, priorities, and values of the people and institutions that design them."],"url":"http://arxiv.org/abs/2504.07312v1"}
{"created":"2025-04-09 21:59:31","title":"PAYADOR: A Minimalist Approach to Grounding Language Models on Structured Data for Interactive Storytelling and Role-playing Games","abstract":"Every time an Interactive Storytelling (IS) system gets a player input, it is facing the world-update problem. Classical approaches to this problem consist in mapping that input to known preprogrammed actions, what can severely constrain the free will of the player. When the expected experience has a strong focus on improvisation, like in Role-playing Games (RPGs), this problem is critical. In this paper we present PAYADOR, a different approach that focuses on predicting the outcomes of the actions instead of representing the actions themselves. To implement this approach, we ground a Large Language Model to a minimal representation of the fictional world, obtaining promising results. We make this contribution open-source, so it can be adapted and used for other related research on unleashing the co-creativity power of RPGs.","sentences":["Every time an Interactive Storytelling (IS) system gets a player input, it is facing the world-update problem.","Classical approaches to this problem consist in mapping that input to known preprogrammed actions, what can severely constrain the free will of the player.","When the expected experience has a strong focus on improvisation, like in Role-playing Games (RPGs), this problem is critical.","In this paper we present PAYADOR, a different approach that focuses on predicting the outcomes of the actions instead of representing the actions themselves.","To implement this approach, we ground a Large Language Model to a minimal representation of the fictional world, obtaining promising results.","We make this contribution open-source, so it can be adapted and used for other related research on unleashing the co-creativity power of RPGs."],"url":"http://arxiv.org/abs/2504.07304v1"}
{"created":"2025-04-09 21:51:38","title":"CEC-MMR: Cross-Entropy Clustering Approach to Multi-Modal Regression","abstract":"In practical applications of regression analysis, it is not uncommon to encounter a multitude of values for each attribute. In such a situation, the univariate distribution, which is typically Gaussian, is suboptimal because the mean may be situated between modes, resulting in a predicted value that differs significantly from the actual data. Consequently, to address this issue, a mixture distribution with parameters learned by a neural network, known as a Mixture Density Network (MDN), is typically employed. However, this approach has an important inherent limitation, in that it is not feasible to ascertain the precise number of components with a reasonable degree of accuracy. In this paper, we introduce CEC-MMR, a novel approach based on Cross-Entropy Clustering (CEC), which allows for the automatic detection of the number of components in a regression problem. Furthermore, given an attribute and its value, our method is capable of uniquely identifying it with the underlying component. The experimental results demonstrate that CEC-MMR yields superior outcomes compared to classical MDNs.","sentences":["In practical applications of regression analysis, it is not uncommon to encounter a multitude of values for each attribute.","In such a situation, the univariate distribution, which is typically Gaussian, is suboptimal because the mean may be situated between modes, resulting in a predicted value that differs significantly from the actual data.","Consequently, to address this issue, a mixture distribution with parameters learned by a neural network, known as a Mixture Density Network (MDN), is typically employed.","However, this approach has an important inherent limitation, in that it is not feasible to ascertain the precise number of components with a reasonable degree of accuracy.","In this paper, we introduce CEC-MMR, a novel approach based on Cross-Entropy Clustering (CEC), which allows for the automatic detection of the number of components in a regression problem.","Furthermore, given an attribute and its value, our method is capable of uniquely identifying it with the underlying component.","The experimental results demonstrate that CEC-MMR yields superior outcomes compared to classical MDNs."],"url":"http://arxiv.org/abs/2504.07301v1"}
{"created":"2025-04-09 21:40:46","title":"CiMBA: Accelerating Genome Sequencing through On-Device Basecalling via Compute-in-Memory","abstract":"As genome sequencing is finding utility in a wide variety of domains beyond the confines of traditional medical settings, its computational pipeline faces two significant challenges. First, the creation of up to 0.5 GB of data per minute imposes substantial communication and storage overheads. Second, the sequencing pipeline is bottlenecked at the basecalling step, consuming >40% of genome analysis time. A range of proposals have attempted to address these challenges, with limited success. We propose to address these challenges with a Compute-in-Memory Basecalling Accelerator (CiMBA), the first embedded ($\\sim25$mm$^2$) accelerator capable of real-time, on-device basecalling, coupled with AnaLog (AL)-Dorado, a new family of analog focused basecalling DNNs. Our resulting hardware/software co-design greatly reduces data communication overhead, is capable of a throughput of 4.77 million bases per second, 24x that required for real-time operation, and achieves 17x/27x power/area efficiency over the best prior basecalling embedded accelerator while maintaining a high accuracy comparable to state-of-the-art software basecallers.","sentences":["As genome sequencing is finding utility in a wide variety of domains beyond the confines of traditional medical settings, its computational pipeline faces two significant challenges.","First, the creation of up to 0.5 GB of data per minute imposes substantial communication and storage overheads.","Second, the sequencing pipeline is bottlenecked at the basecalling step, consuming >40% of genome analysis time.","A range of proposals have attempted to address these challenges, with limited success.","We propose to address these challenges with a Compute-in-Memory Basecalling Accelerator (CiMBA), the first embedded ($\\sim25$mm$^2$) accelerator capable of real-time, on-device basecalling, coupled with AnaLog (AL)-Dorado, a new family of analog focused basecalling DNNs.","Our resulting hardware/software co-design greatly reduces data communication overhead, is capable of a throughput of 4.77 million bases per second, 24x that required for real-time operation, and achieves 17x/27x power/area efficiency over the best prior basecalling embedded accelerator while maintaining a high accuracy comparable to state-of-the-art software basecallers."],"url":"http://arxiv.org/abs/2504.07298v1"}
{"created":"2025-04-09 21:40:15","title":"Data Fusion of Deep Learned Molecular Embeddings for Property Prediction","abstract":"Data-driven approaches such as deep learning can result in predictive models for material properties with exceptional accuracy and efficiency. However, in many problems data is sparse, severely limiting their accuracy and applicability. To improve predictions, techniques such as transfer learning and multi-task learning have been used. The performance of multi-task learning models depends on the strength of the underlying correlations between tasks and the completeness of the dataset. We find that standard multi-task models tend to underperform when trained on sparse datasets with weakly correlated properties. To address this gap, we use data fusion techniques to combine the learned molecular embeddings of various single-task models and trained a multi-task model on this combined embedding. We apply this technique to a widely used benchmark dataset of quantum chemistry data for small molecules as well as a newly compiled sparse dataset of experimental data collected from literature and our own quantum chemistry and thermochemical calculations. The results show that the fused, multi-task models outperform standard multi-task models for sparse datasets and can provide enhanced prediction on data-limited properties compared to single-task models.","sentences":["Data-driven approaches such as deep learning can result in predictive models for material properties with exceptional accuracy and efficiency.","However, in many problems data is sparse, severely limiting their accuracy and applicability.","To improve predictions, techniques such as transfer learning and multi-task learning have been used.","The performance of multi-task learning models depends on the strength of the underlying correlations between tasks and the completeness of the dataset.","We find that standard multi-task models tend to underperform when trained on sparse datasets with weakly correlated properties.","To address this gap, we use data fusion techniques to combine the learned molecular embeddings of various single-task models and trained a multi-task model on this combined embedding.","We apply this technique to a widely used benchmark dataset of quantum chemistry data for small molecules as well as a newly compiled sparse dataset of experimental data collected from literature and our own quantum chemistry and thermochemical calculations.","The results show that the fused, multi-task models outperform standard multi-task models for sparse datasets and can provide enhanced prediction on data-limited properties compared to single-task models."],"url":"http://arxiv.org/abs/2504.07297v1"}
{"created":"2025-04-09 21:30:37","title":"Data-Enabled Neighboring Extremal: Case Study on Model-Free Trajectory Tracking for Robotic Arm","abstract":"Data-enabled predictive control (DeePC) has recently emerged as a powerful data-driven approach for efficient system controls with constraints handling capabilities. It performs optimal controls by directly harnessing input-output (I/O) data, bypassing the process of explicit model identification that can be costly and time-consuming. However, its high computational complexity, driven by a large-scale optimization problem (typically in a higher dimension than its model-based counterpart--Model Predictive Control), hinders real-time applications. To overcome this limitation, we propose the data-enabled neighboring extremal (DeeNE) framework, which significantly reduces computational cost while preserving control performance. DeeNE leverages first-order optimality perturbation analysis to efficiently update a precomputed nominal DeePC solution in response to changes in initial conditions and reference trajectories. We validate its effectiveness on a 7-DoF KINOVA Gen3 robotic arm, demonstrating substantial computational savings and robust, data-driven control performance.","sentences":["Data-enabled predictive control (DeePC) has recently emerged as a powerful data-driven approach for efficient system controls with constraints handling capabilities.","It performs optimal controls by directly harnessing input-output (I/O) data, bypassing the process of explicit model identification that can be costly and time-consuming.","However, its high computational complexity, driven by a large-scale optimization problem (typically in a higher dimension than its model-based counterpart--Model Predictive Control), hinders real-time applications.","To overcome this limitation, we propose the data-enabled neighboring extremal (DeeNE) framework, which significantly reduces computational cost while preserving control performance.","DeeNE leverages first-order optimality perturbation analysis to efficiently update a precomputed nominal DeePC solution in response to changes in initial conditions and reference trajectories.","We validate its effectiveness on a 7-DoF KINOVA Gen3 robotic arm, demonstrating substantial computational savings and robust, data-driven control performance."],"url":"http://arxiv.org/abs/2504.07292v1"}
{"created":"2025-04-09 21:28:17","title":"MDIT: A Model-free Data Interpolation Method for Diverse Instruction Tuning","abstract":"As Large Language Models (LLMs) are increasingly applied across various tasks, instruction tuning has emerged as a critical method for enhancing model performance. However, current data management strategies face substantial challenges in generating diverse and comprehensive data, restricting further improvements in model performance. To address this gap, we propose MDIT, a novel model-free data interpolation method for diverse instruction tuning, which generates varied and high-quality instruction data by performing task interpolation. Moreover, it contains diversity-based clustering strategies to ensure the diversity of the training data. Extensive experiments show that our method achieves superior performance in multiple benchmark tasks. The LLMs finetuned with MDIT show significant improvements in numerous tasks such as general question answering, math reasoning, and code generation. MDIT offers an efficient and automatic data synthetic method, generating diverse instruction data without depending on external resources while expanding the application potential of LLMs in complex environments.","sentences":["As Large Language Models (LLMs) are increasingly applied across various tasks, instruction tuning has emerged as a critical method for enhancing model performance.","However, current data management strategies face substantial challenges in generating diverse and comprehensive data, restricting further improvements in model performance.","To address this gap, we propose MDIT, a novel model-free data interpolation method for diverse instruction tuning, which generates varied and high-quality instruction data by performing task interpolation.","Moreover, it contains diversity-based clustering strategies to ensure the diversity of the training data.","Extensive experiments show that our method achieves superior performance in multiple benchmark tasks.","The LLMs finetuned with MDIT show significant improvements in numerous tasks such as general question answering, math reasoning, and code generation.","MDIT offers an efficient and automatic data synthetic method, generating diverse instruction data without depending on external resources while expanding the application potential of LLMs in complex environments."],"url":"http://arxiv.org/abs/2504.07288v1"}
{"created":"2025-04-09 21:24:17","title":"A Scalable Approach to Clustering Embedding Projections","abstract":"Interactive visualization of embedding projections is a useful technique for understanding data and evaluating machine learning models. Labeling data within these visualizations is critical for interpretation, as labels provide an overview of the projection and guide user navigation. However, most methods for producing labels require clustering the points, which can be computationally expensive as the number of points grows. In this paper, we describe an efficient clustering approach using kernel density estimation in the projected 2D space instead of points. This algorithm can produce high-quality cluster regions from a 2D density map in a few hundred milliseconds, orders of magnitude faster than current approaches. We contribute the design of the algorithm, benchmarks, and applications that demonstrate the utility of the algorithm, including labeling and summarization.","sentences":["Interactive visualization of embedding projections is a useful technique for understanding data and evaluating machine learning models.","Labeling data within these visualizations is critical for interpretation, as labels provide an overview of the projection and guide user navigation.","However, most methods for producing labels require clustering the points, which can be computationally expensive as the number of points grows.","In this paper, we describe an efficient clustering approach using kernel density estimation in the projected 2D space instead of points.","This algorithm can produce high-quality cluster regions from a 2D density map in a few hundred milliseconds, orders of magnitude faster than current approaches.","We contribute the design of the algorithm, benchmarks, and applications that demonstrate the utility of the algorithm, including labeling and summarization."],"url":"http://arxiv.org/abs/2504.07285v1"}
{"created":"2025-04-09 21:17:52","title":"RAISE: Reinforenced Adaptive Instruction Selection For Large Language Models","abstract":"In the instruction fine-tuning of large language models (LLMs), it has become a consensus that a few high-quality instructions are superior to a large number of low-quality instructions. At present, many instruction selection methods have been proposed, but most of these methods select instruction based on heuristic quality metrics, and only consider data selection before training. These designs lead to insufficient optimization of instruction fine-tuning, and fixed heuristic indicators are often difficult to optimize for specific tasks. So we designed a dynamic, task-objective-driven instruction selection framework RAISE(Reinforenced Adaptive Instruction SElection), which incorporates the entire instruction fine-tuning process into optimization, selecting instruction at each step based on the expected impact of instruction on model performance improvement. Our approach is well interpretable and has strong task-specific optimization capabilities. By modeling dynamic instruction selection as a sequential decision-making process, we use RL to train our selection strategy. Extensive experiments and result analysis prove the superiority of our method compared with other instruction selection methods. Notably, RAISE achieves superior performance by updating only 1\\% of the training steps compared to full-data training, demonstrating its efficiency and effectiveness.","sentences":["In the instruction fine-tuning of large language models (LLMs), it has become a consensus that a few high-quality instructions are superior to a large number of low-quality instructions.","At present, many instruction selection methods have been proposed, but most of these methods select instruction based on heuristic quality metrics, and only consider data selection before training.","These designs lead to insufficient optimization of instruction fine-tuning, and fixed heuristic indicators are often difficult to optimize for specific tasks.","So we designed a dynamic, task-objective-driven instruction selection framework RAISE(Reinforenced Adaptive Instruction SElection), which incorporates the entire instruction fine-tuning process into optimization, selecting instruction at each step based on the expected impact of instruction on model performance improvement.","Our approach is well interpretable and has strong task-specific optimization capabilities.","By modeling dynamic instruction selection as a sequential decision-making process, we use RL to train our selection strategy.","Extensive experiments and result analysis prove the superiority of our method compared with other instruction selection methods.","Notably, RAISE achieves superior performance by updating only 1\\% of the training steps compared to full-data training, demonstrating its efficiency and effectiveness."],"url":"http://arxiv.org/abs/2504.07282v1"}
{"created":"2025-04-09 21:15:05","title":"Conthereum: Concurrent Ethereum Optimized Transaction Scheduling for Multi-Core Execution","abstract":"Blockchain technology has revolutionized decentralized computation, providing high security through transparent cryptographic protocols and immutable data. However, the Blockchain Trilemma-an inherent trade-off between security, scalability, and performance-limits computational efficiency, resulting in low transactions-per-second (TPS) compared to conventional systems like Visa or PayPal. To address this, we introduce Conthereum, a novel concurrent blockchain solution that enhances multi-core usage in transaction processing through a deterministic scheduling scheme. It reformulates smart contract execution as a variant of the Flexible Job Shop Scheduling Problem (FJSS), optimizing both time and power consumption. Conthereum offers the most efficient open-source implementation compared to existing solutions. Empirical evaluations based on Ethereum, the most widely used blockchain platform, show near-linear throughput increases with available computational power. Additionally, an integrated energy consumption model allows participant to optimize power usage by intelligently distributing workloads across cores. This solution not only boosts network TPS and energy efficiency, offering a scalable and sustainable framework for blockchain transaction processing. The proposed approach also opens new avenues for further optimizations in Ethereum and is adaptable for broader applications in other blockchain infrastructures.","sentences":["Blockchain technology has revolutionized decentralized computation, providing high security through transparent cryptographic protocols and immutable data.","However, the Blockchain Trilemma-an inherent trade-off between security, scalability, and performance-limits computational efficiency, resulting in low transactions-per-second (TPS) compared to conventional systems like Visa or PayPal.","To address this, we introduce Conthereum, a novel concurrent blockchain solution that enhances multi-core usage in transaction processing through a deterministic scheduling scheme.","It reformulates smart contract execution as a variant of the Flexible Job Shop Scheduling Problem (FJSS), optimizing both time and power consumption.","Conthereum offers the most efficient open-source implementation compared to existing solutions.","Empirical evaluations based on Ethereum, the most widely used blockchain platform, show near-linear throughput increases with available computational power.","Additionally, an integrated energy consumption model allows participant to optimize power usage by intelligently distributing workloads across cores.","This solution not only boosts network TPS and energy efficiency, offering a scalable and sustainable framework for blockchain transaction processing.","The proposed approach also opens new avenues for further optimizations in Ethereum and is adaptable for broader applications in other blockchain infrastructures."],"url":"http://arxiv.org/abs/2504.07280v1"}
{"created":"2025-04-09 21:12:29","title":"A Multi-Phase Analysis of Blood Culture Stewardship: Machine Learning Prediction, Expert Recommendation Assessment, and LLM Automation","abstract":"Blood cultures are often over ordered without clear justification, straining healthcare resources and contributing to inappropriate antibiotic use pressures worsened by the global shortage. In study of 135483 emergency department (ED) blood culture orders, we developed machine learning (ML) models to predict the risk of bacteremia using structured electronic health record (EHR) data and provider notes via a large language model (LLM). The structured models AUC improved from 0.76 to 0.79 with note embeddings and reached 0.81 with added diagnosis codes. Compared to an expert recommendation framework applied by human reviewers and an LLM-based pipeline, our ML approach offered higher specificity without compromising sensitivity. The recommendation framework achieved sensitivity 86%, specificity 57%, while the LLM maintained high sensitivity (96%) but over classified negatives, reducing specificity (16%). These findings demonstrate that ML models integrating structured and unstructured data can outperform consensus recommendations, enhancing diagnostic stewardship beyond existing standards of care.","sentences":["Blood cultures are often over ordered without clear justification, straining healthcare resources and contributing to inappropriate antibiotic use pressures worsened by the global shortage.","In study of 135483 emergency department (ED) blood culture orders, we developed machine learning (ML) models to predict the risk of bacteremia using structured electronic health record (EHR) data and provider notes via a large language model (LLM).","The structured models AUC improved from 0.76 to 0.79 with note embeddings and reached 0.81 with added diagnosis codes.","Compared to an expert recommendation framework applied by human reviewers and an LLM-based pipeline, our ML approach offered higher specificity without compromising sensitivity.","The recommendation framework achieved sensitivity 86%, specificity 57%, while the LLM maintained high sensitivity (96%) but over classified negatives, reducing specificity (16%).","These findings demonstrate that ML models integrating structured and unstructured data can outperform consensus recommendations, enhancing diagnostic stewardship beyond existing standards of care."],"url":"http://arxiv.org/abs/2504.07278v1"}
{"created":"2025-04-09 21:02:12","title":"Language Modeling for the Future of Finance: A Quantitative Survey into Metrics, Tasks, and Data Opportunities","abstract":"Recent advances in language modeling have led to growing interest in applying Natural Language Processing (NLP) techniques to financial problems, enabling new approaches to analysis and decision-making. To systematically examine this trend, we review 374 NLP research papers published between 2017 and 2024 across 38 conferences and workshops, with a focused analysis of 221 papers that directly address finance-related tasks. We evaluate these papers across 11 qualitative and quantitative dimensions, identifying key trends such as the increasing use of general-purpose language models, steady progress in sentiment analysis and information extraction, and emerging efforts around explainability and privacy-preserving methods. We also discuss the use of evaluation metrics, highlighting the importance of domain-specific ones to complement standard machine learning metrics. Our findings emphasize the need for more accessible, adaptive datasets and highlight the significance of incorporating financial crisis periods to strengthen model robustness under real-world conditions. This survey provides a structured overview of NLP research applied to finance and offers practical insights for researchers and practitioners working at this intersection.","sentences":["Recent advances in language modeling have led to growing interest in applying Natural Language Processing (NLP) techniques to financial problems, enabling new approaches to analysis and decision-making.","To systematically examine this trend, we review 374 NLP research papers published between 2017 and 2024 across 38 conferences and workshops, with a focused analysis of 221 papers that directly address finance-related tasks.","We evaluate these papers across 11 qualitative and quantitative dimensions, identifying key trends such as the increasing use of general-purpose language models, steady progress in sentiment analysis and information extraction, and emerging efforts around explainability and privacy-preserving methods.","We also discuss the use of evaluation metrics, highlighting the importance of domain-specific ones to complement standard machine learning metrics.","Our findings emphasize the need for more accessible, adaptive datasets and highlight the significance of incorporating financial crisis periods to strengthen model robustness under real-world conditions.","This survey provides a structured overview of NLP research applied to finance and offers practical insights for researchers and practitioners working at this intersection."],"url":"http://arxiv.org/abs/2504.07274v1"}
{"created":"2025-04-09 20:41:03","title":"Fast algorithms for complex-valued discrete Fourier transform with separate real and imaginary inputs/outputs","abstract":"Fast Fourier transform algorithms are an arsenal of effective tools for solving various problems of analysis and high-speed processing of signals of various natures. Almost all of these algorithms are designed to process sequences of complex-valued data when each element of the sequence represents a single whole. However, in some cases, it is more advantageous to represent each element of the input and output sequences by a pair of real numbers. Such a need arises, for example, when further post-processing of spectral coefficients is carried out through two independent channels. Taking into account the noted need, the article proposes an algorithm for fast complex-valued discrete Fourier transform with separate real and imaginary inputs/outputs. A vector-matrix computational procedure is given that allows one to adequately describe and formalize the sequence of calculations when implementing the proposed algorithm.","sentences":["Fast Fourier transform algorithms are an arsenal of effective tools for solving various problems of analysis and high-speed processing of signals of various natures.","Almost all of these algorithms are designed to process sequences of complex-valued data when each element of the sequence represents a single whole.","However, in some cases, it is more advantageous to represent each element of the input and output sequences by a pair of real numbers.","Such a need arises, for example, when further post-processing of spectral coefficients is carried out through two independent channels.","Taking into account the noted need, the article proposes an algorithm for fast complex-valued discrete Fourier transform with separate real and imaginary inputs/outputs.","A vector-matrix computational procedure is given that allows one to adequately describe and formalize the sequence of calculations when implementing the proposed algorithm."],"url":"http://arxiv.org/abs/2504.07264v1"}
