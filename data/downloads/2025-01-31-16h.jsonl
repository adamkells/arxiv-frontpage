{"created":"2025-01-29 18:57:44","title":"rEGGression: an Interactive and Agnostic Tool for the Exploration of Symbolic Regression Models","abstract":"Regression analysis is used for prediction and to understand the effect of independent variables on dependent variables. Symbolic regression (SR) automates the search for non-linear regression models, delivering a set of hypotheses that balances accuracy with the possibility to understand the phenomena. Many SR implementations return a Pareto front allowing the choice of the best trade-off. However, this hides alternatives that are close to non-domination, limiting these choices. Equality graphs (e-graphs) allow to represent large sets of expressions compactly by efficiently handling duplicated parts occurring in multiple expressions. E-graphs allow to store and query all SR solution candidates visited in one or multiple GP runs efficiently and open the possibility to analyse much larger sets of SR solution candidates. We introduce rEGGression, a tool using e-graphs to enable the exploration of a large set of symbolic expressions which provides querying, filtering, and pattern matching features creating an interactive experience to gain insights about SR models. The main highlight is its focus in the exploration of the building blocks found during the search that can help the experts to find insights about the studied phenomena.This is possible by exploiting the pattern matching capability of the e-graph data structure.","sentences":["Regression analysis is used for prediction and to understand the effect of independent variables on dependent variables.","Symbolic regression (SR) automates the search for non-linear regression models, delivering a set of hypotheses that balances accuracy with the possibility to understand the phenomena.","Many SR implementations return a Pareto front allowing the choice of the best trade-off.","However, this hides alternatives that are close to non-domination, limiting these choices.","Equality graphs (e-graphs) allow to represent large sets of expressions compactly by efficiently handling duplicated parts occurring in multiple expressions.","E-graphs allow to store and query all SR solution candidates visited in one or multiple GP runs efficiently and open the possibility to analyse much larger sets of SR solution candidates.","We introduce rEGGression, a tool using e-graphs to enable the exploration of a large set of symbolic expressions which provides querying, filtering, and pattern matching features creating an interactive experience to gain insights about SR models.","The main highlight is its focus in the exploration of the building blocks found during the search that can help the experts to find insights about the studied phenomena.","This is possible by exploiting the pattern matching capability of the e-graph data structure."],"url":"http://arxiv.org/abs/2501.17859v1"}
{"created":"2025-01-29 18:55:07","title":"GRACE: Generalizing Robot-Assisted Caregiving with User Functionality Embeddings","abstract":"Robot caregiving should be personalized to meet the diverse needs of care recipients -- assisting with tasks as needed, while taking user agency in action into account. In physical tasks such as handover, bathing, dressing, and rehabilitation, a key aspect of this diversity is the functional range of motion (fROM), which can vary significantly between individuals. In this work, we learn to predict personalized fROM as a way to generalize robot decision-making in a wide range of caregiving tasks. We propose a novel data-driven method for predicting personalized fROM using functional assessment scores from occupational therapy. We develop a neural model that learns to embed functional assessment scores into a latent representation of the user's physical function. The model is trained using motion capture data collected from users with emulated mobility limitations. After training, the model predicts personalized fROM for new users without motion capture. Through simulated experiments and a real-robot user study, we show that the personalized fROM predictions from our model enable the robot to provide personalized and effective assistance while improving the user's agency in action. See our website for more visualizations: https://emprise.cs.cornell.edu/grace/.","sentences":["Robot caregiving should be personalized to meet the diverse needs of care recipients -- assisting with tasks as needed, while taking user agency in action into account.","In physical tasks such as handover, bathing, dressing, and rehabilitation, a key aspect of this diversity is the functional range of motion (fROM), which can vary significantly between individuals.","In this work, we learn to predict personalized fROM as a way to generalize robot decision-making in a wide range of caregiving tasks.","We propose a novel data-driven method for predicting personalized fROM using functional assessment scores from occupational therapy.","We develop a neural model that learns to embed functional assessment scores into a latent representation of the user's physical function.","The model is trained using motion capture data collected from users with emulated mobility limitations.","After training, the model predicts personalized fROM for new users without motion capture.","Through simulated experiments and a real-robot user study, we show that the personalized fROM predictions from our model enable the robot to provide personalized and effective assistance while improving the user's agency in action.","See our website for more visualizations: https://emprise.cs.cornell.edu/grace/."],"url":"http://arxiv.org/abs/2501.17855v1"}
{"created":"2025-01-29 18:49:34","title":"Improving Genetic Programming for Symbolic Regression with Equality Graphs","abstract":"The search for symbolic regression models with genetic programming (GP) has a tendency of revisiting expressions in their original or equivalent forms. Repeatedly evaluating equivalent expressions is inefficient, as it does not immediately lead to better solutions. However, evolutionary algorithms require diversity and should allow the accumulation of inactive building blocks that can play an important role at a later point. The equality graph is a data structure capable of compactly storing expressions and their equivalent forms allowing an efficient verification of whether an expression has been visited in any of their stored equivalent forms. We exploit the e-graph to adapt the subtree operators to reduce the chances of revisiting expressions. Our adaptation, called eggp, stores every visited expression in the e-graph, allowing us to filter out from the available selection of subtrees all the combinations that would create already visited expressions. Results show that, for small expressions, this approach improves the performance of a simple GP algorithm to compete with PySR and Operon without increasing computational cost. As a highlight, eggp was capable of reliably delivering short and at the same time accurate models for a selected set of benchmarks from SRBench and a set of real-world datasets.","sentences":["The search for symbolic regression models with genetic programming (GP) has a tendency of revisiting expressions in their original or equivalent forms.","Repeatedly evaluating equivalent expressions is inefficient, as it does not immediately lead to better solutions.","However, evolutionary algorithms require diversity and should allow the accumulation of inactive building blocks that can play an important role at a later point.","The equality graph is a data structure capable of compactly storing expressions and their equivalent forms allowing an efficient verification of whether an expression has been visited in any of their stored equivalent forms.","We exploit the e-graph to adapt the subtree operators to reduce the chances of revisiting expressions.","Our adaptation, called eggp, stores every visited expression in the e-graph, allowing us to filter out from the available selection of subtrees all the combinations that would create already visited expressions.","Results show that, for small expressions, this approach improves the performance of a simple GP algorithm to compete with PySR and Operon without increasing computational cost.","As a highlight, eggp was capable of reliably delivering short and at the same time accurate models for a selected set of benchmarks from SRBench and a set of real-world datasets."],"url":"http://arxiv.org/abs/2501.17848v1"}
{"created":"2025-01-29 18:44:48","title":"acoupi: An Open-Source Python Framework for Deploying Bioacoustic AI Models on Edge Devices","abstract":"1. Passive acoustic monitoring (PAM) coupled with artificial intelligence (AI) is becoming an essential tool for biodiversity monitoring. Traditional PAM systems require manual data offloading and impose substantial demands on storage and computing infrastructure. The combination of on-device AI-based processing and network connectivity enables local data analysis and transmission of only relevant information, greatly reducing storage needs. However, programming these devices for robust operation is challenging, requiring expertise in embedded systems and software engineering. Despite the increase in AI-based models for bioacoustics, their full potential remains unrealized without accessible tools to deploy them on custom hardware and tailor device behaviour to specific monitoring goals. 2. To address this challenge, we develop acoupi, an open-source Python framework that simplifies the creation and deployment of smart bioacoustic devices. acoupi integrates audio recording, AI-based data processing, data management, and real-time wireless messaging into a unified and configurable framework. By modularising key elements of the bioacoustic monitoring workflow, acoupi allows users to easily customise, extend, or select specific components to fit their unique monitoring needs. 3. We demonstrate the flexibility of acoupi by integrating two bioacoustic classifiers: BirdNET, for the classification of bird species, and BatDetect2, for the classification of UK bat species. We test the reliability of acoupi over a month-long deployment of two acoupi-powered devices in a UK urban park. 4. acoupi can be deployed on low-cost hardware such as the Raspberry Pi and can be customised for various applications. acoupi standardised framework and simplified tools facilitate the adoption of AI-powered PAM systems for researchers and conservationists. acoupi is on GitHub at https://github.com/acoupi/acoupi.","sentences":["1.","Passive acoustic monitoring (PAM) coupled with artificial intelligence (AI) is becoming an essential tool for biodiversity monitoring.","Traditional PAM systems require manual data offloading and impose substantial demands on storage and computing infrastructure.","The combination of on-device AI-based processing and network connectivity enables local data analysis and transmission of only relevant information, greatly reducing storage needs.","However, programming these devices for robust operation is challenging, requiring expertise in embedded systems and software engineering.","Despite the increase in AI-based models for bioacoustics, their full potential remains unrealized without accessible tools to deploy them on custom hardware and tailor device behaviour to specific monitoring goals.","2.","To address this challenge, we develop acoupi, an open-source Python framework that simplifies the creation and deployment of smart bioacoustic devices.","acoupi integrates audio recording, AI-based data processing, data management, and real-time wireless messaging into a unified and configurable framework.","By modularising key elements of the bioacoustic monitoring workflow, acoupi allows users to easily customise, extend, or select specific components to fit their unique monitoring needs.","3.","We demonstrate the flexibility of acoupi by integrating two bioacoustic classifiers: BirdNET, for the classification of bird species, and BatDetect2, for the classification of UK bat species.","We test the reliability of acoupi over a month-long deployment of two acoupi-powered devices in a UK urban park.","4. acoupi can be deployed on low-cost hardware such as the Raspberry Pi and can be customised for various applications.","acoupi standardised framework and simplified tools facilitate the adoption of AI-powered PAM systems for researchers and conservationists.","acoupi is on GitHub at https://github.com/acoupi/acoupi."],"url":"http://arxiv.org/abs/2501.17841v1"}
{"created":"2025-01-29 18:35:38","title":"Matrix Product Sketching via Coordinated Sampling","abstract":"We revisit the well-studied problem of approximating a matrix product, $\\mathbf{A}^T\\mathbf{B}$, based on small space sketches $\\mathcal{S}(\\mathbf{A})$ and $\\mathcal{S}(\\mathbf{B})$ of $\\mathbf{A} \\in \\R^{n \\times d}$ and $\\mathbf{B}\\in \\R^{n \\times m}$. We are interested in the setting where the sketches must be computed independently of each other, except for the use of a shared random seed. We prove that, when $\\mathbf{A}$ and $\\mathbf{B}$ are sparse, methods based on \\emph{coordinated random sampling} can outperform classical linear sketching approaches, like Johnson-Lindenstrauss Projection or CountSketch. For example, to obtain Frobenius norm error $\\epsilon\\|\\mathbf{A}\\|_F\\|\\mathbf{B}\\|_F$, coordinated sampling requires sketches of size $O(s/\\epsilon^2)$ when $\\mathbf{A}$ and $\\mathbf{B}$ have at most $s \\leq d,m$ non-zeros per row. In contrast, linear sketching leads to sketches of size $O(d/\\epsilon^2)$ and $O(m/\\epsilon^2)$ for $\\mathbf{A}$ and $\\mathbf{B}$. We empirically evaluate our approach on two applications: 1) distributed linear regression in databases, a problem motivated by tasks like dataset discovery and augmentation, and 2) approximating attention matrices in transformer-based language models. In both cases, our sampling algorithms yield an order of magnitude improvement over linear sketching.","sentences":["We revisit the well-studied problem of approximating a matrix product, $\\mathbf{A}^T\\mathbf{B}$, based on small space sketches $\\mathcal{S}(\\mathbf{A})$ and $\\mathcal{S}(\\mathbf{B})$ of $\\mathbf{A} \\in \\R^{n \\times d}$ and $\\mathbf{B}\\in \\R^{n \\times m}$. We are interested in the setting where the sketches must be computed independently of each other, except for the use of a shared random seed.","We prove that, when $\\mathbf{A}$ and $\\mathbf{B}$ are sparse, methods based on \\emph{coordinated random sampling} can outperform classical linear sketching approaches, like Johnson-Lindenstrauss Projection or CountSketch.","For example, to obtain Frobenius norm error $\\epsilon\\|\\mathbf{A}\\|_F\\|\\mathbf{B}\\|_F$, coordinated sampling requires sketches of size $O(s/\\epsilon^2)$ when $\\mathbf{A}$ and $\\mathbf{B}$ have at most $s \\leq d,m$ non-zeros per row.","In contrast, linear sketching leads to sketches of size $O(d/\\epsilon^2)$ and $O(m/\\epsilon^2)$ for $\\mathbf{A}$ and $\\mathbf{B}$. We empirically evaluate our approach on two applications: 1) distributed linear regression in databases, a problem motivated by tasks like dataset discovery and augmentation, and 2) approximating attention matrices in transformer-based language models.","In both cases, our sampling algorithms yield an order of magnitude improvement over linear sketching."],"url":"http://arxiv.org/abs/2501.17836v1"}
{"created":"2025-01-29 18:30:18","title":"Hierarchical Fallback Architecture for High Risk Online Machine Learning Inference","abstract":"Open Banking powered machine learning applications require novel robustness approaches to deal with challenging stress and failure scenarios. In this paper we propose an hierarchical fallback architecture for improving robustness in high risk machine learning applications with a focus in the financial domain. We define generic failure scenarios often found in online inference that depend on external data providers and we describe in detail how to apply the hierarchical fallback architecture to address them. Finally, we offer a real world example of its applicability in the industry for near-real time transactional fraud risk evaluation using Open Banking data and under extreme stress scenarios.","sentences":["Open Banking powered machine learning applications require novel robustness approaches to deal with challenging stress and failure scenarios.","In this paper we propose an hierarchical fallback architecture for improving robustness in high risk machine learning applications with a focus in the financial domain.","We define generic failure scenarios often found in online inference that depend on external data providers and we describe in detail how to apply the hierarchical fallback architecture to address them.","Finally, we offer a real world example of its applicability in the industry for near-real time transactional fraud risk evaluation using Open Banking data and under extreme stress scenarios."],"url":"http://arxiv.org/abs/2501.17834v1"}
{"created":"2025-01-29 18:16:20","title":"SMT-Boosted Security Types for Low-Level MPC","abstract":"Secure Multi-Party Computation (MPC) is an important enabling technology for data privacy in modern distributed applications. We develop a new type theory to automatically enforce correctness,confidentiality, and integrity properties of protocols written in the \\emph{Prelude/Overture} language framework. Judgements in the type theory are predicated on SMT verifications in a theory of finite fields, which supports precise and efficient analysis. Our approach is automated, compositional, scalable, and generalizes to arbitrary prime fields for data and key sizes.","sentences":["Secure Multi-Party Computation (MPC) is an important enabling technology for data privacy in modern distributed applications.","We develop a new type theory to automatically enforce correctness,confidentiality, and integrity properties of protocols written in the \\emph{Prelude/Overture} language framework.","Judgements in the type theory are predicated on SMT verifications in a theory of finite fields, which supports precise and efficient analysis.","Our approach is automated, compositional, scalable, and generalizes to arbitrary prime fields for data and key sizes."],"url":"http://arxiv.org/abs/2501.17824v1"}
{"created":"2025-01-29 18:00:19","title":"Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling","abstract":"In this work, we introduce Janus-Pro, an advanced version of the previous work Janus. Specifically, Janus-Pro incorporates (1) an optimized training strategy, (2) expanded training data, and (3) scaling to larger model size. With these improvements, Janus-Pro achieves significant advancements in both multimodal understanding and text-to-image instruction-following capabilities, while also enhancing the stability of text-to-image generation. We hope this work will inspire further exploration in the field. Code and models are publicly available.","sentences":["In this work, we introduce Janus-Pro, an advanced version of the previous work Janus.","Specifically, Janus-Pro incorporates (1) an optimized training strategy, (2) expanded training data, and (3) scaling to larger model size.","With these improvements, Janus-Pro achieves significant advancements in both multimodal understanding and text-to-image instruction-following capabilities, while also enhancing the stability of text-to-image generation.","We hope this work will inspire further exploration in the field.","Code and models are publicly available."],"url":"http://arxiv.org/abs/2501.17811v1"}
{"created":"2025-01-29 17:44:57","title":"LEKA:LLM-Enhanced Knowledge Augmentation","abstract":"Humans excel in analogical learning and knowledge transfer and, more importantly, possess a unique understanding of identifying appropriate sources of knowledge. From a model's perspective, this presents an interesting challenge. If models could autonomously retrieve knowledge useful for transfer or decision-making to solve problems, they would transition from passively acquiring to actively accessing and learning from knowledge. However, filling models with knowledge is relatively straightforward -- it simply requires more training and accessible knowledge bases. The more complex task is teaching models about which knowledge can be analogized and transferred. Therefore, we design a knowledge augmentation method LEKA for knowledge transfer that actively searches for suitable knowledge sources that can enrich the target domain's knowledge. This LEKA method extracts key information from textual information from the target domain, retrieves pertinent data from external data libraries, and harmonizes retrieved data with the target domain data in feature space and marginal probability measures. We validate the effectiveness of our approach through extensive experiments across various domains and demonstrate significant improvements over traditional methods in reducing computational costs, automating data alignment, and optimizing transfer learning outcomes.","sentences":["Humans excel in analogical learning and knowledge transfer and, more importantly, possess a unique understanding of identifying appropriate sources of knowledge.","From a model's perspective, this presents an interesting challenge.","If models could autonomously retrieve knowledge useful for transfer or decision-making to solve problems, they would transition from passively acquiring to actively accessing and learning from knowledge.","However, filling models with knowledge is relatively straightforward -- it simply requires more training and accessible knowledge bases.","The more complex task is teaching models about which knowledge can be analogized and transferred.","Therefore, we design a knowledge augmentation method LEKA for knowledge transfer that actively searches for suitable knowledge sources that can enrich the target domain's knowledge.","This LEKA method extracts key information from textual information from the target domain, retrieves pertinent data from external data libraries, and harmonizes retrieved data with the target domain data in feature space and marginal probability measures.","We validate the effectiveness of our approach through extensive experiments across various domains and demonstrate significant improvements over traditional methods in reducing computational costs, automating data alignment, and optimizing transfer learning outcomes."],"url":"http://arxiv.org/abs/2501.17802v1"}
{"created":"2025-01-29 17:35:26","title":"An Incremental Multi-Level, Multi-Scale Approach to Assessment of Multifidelity HPC Systems","abstract":"With the growing complexity in architecture and the size of large-scale computing systems, monitoring and analyzing system behavior and events has become daunting. Monitoring data amounting to terabytes per day are collected by sensors housed in these massive systems at multiple fidelity levels and varying temporal resolutions. In this work, we develop an incremental version of multiresolution dynamic mode decomposition (mrDMD), which converts high-dimensional data to spatial-temporal patterns at varied frequency ranges. Our incremental implementation of the mrDMD algorithm (I-mrDMD) promptly reveals valuable information in the massive environment log dataset, which is then visually aligned with the processed hardware and job log datasets through our generalizable rack visualization using D3 visualization integrated into the Jupyter Notebook interface. We demonstrate the efficacy of our approach with two use scenarios on a real-world dataset from a Cray XC40 supercomputer, Theta.","sentences":["With the growing complexity in architecture and the size of large-scale computing systems, monitoring and analyzing system behavior and events has become daunting.","Monitoring data amounting to terabytes per day are collected by sensors housed in these massive systems at multiple fidelity levels and varying temporal resolutions.","In this work, we develop an incremental version of multiresolution dynamic mode decomposition (mrDMD), which converts high-dimensional data to spatial-temporal patterns at varied frequency ranges.","Our incremental implementation of the mrDMD algorithm (I-mrDMD) promptly reveals valuable information in the massive environment log dataset, which is then visually aligned with the processed hardware and job log datasets through our generalizable rack visualization using D3 visualization integrated into the Jupyter Notebook interface.","We demonstrate the efficacy of our approach with two use scenarios on a real-world dataset from a Cray XC40 supercomputer, Theta."],"url":"http://arxiv.org/abs/2501.17796v1"}
{"created":"2025-01-29 17:26:31","title":"Detecting Anomalies Using Rotated Isolation Forest","abstract":"The Isolation Forest (iForest), proposed by Liu, Ting, and Zhou at TKDE 2012, has become a prominent tool for unsupervised anomaly detection. However, recent research by Hariri, Kind, and Brunner, published in TKDE 2021, has revealed issues with iForest. They identified the presence of axis-aligned ghost clusters that can be misidentified as normal clusters, leading to biased anomaly scores and inaccurate predictions. In response, they developed the Extended Isolation Forest (EIF), which effectively solves these issues by eliminating the ghost clusters introduced by iForest. This enhancement results in improved consistency of anomaly scores and superior performance. We reveal a previously overlooked problem in the Extended Isolation Forest (EIF), showing that it is vulnerable to ghost inter-clusters between normal clusters of data points. In this paper, we introduce the Rotated Isolation Forest (RIF) algorithm which effectively addresses both the axis-aligned ghost clusters observed in iForest and the ghost inter-clusters seen in EIF. RIF accomplishes this by randomly rotating the dataset (using random rotation matrices and QR decomposition) before feeding it into the iForest construction, thereby increasing dataset variation and eliminating ghost clusters. Our experiments conclusively demonstrate that the RIF algorithm outperforms iForest and EIF, as evidenced by the results obtained from both synthetic datasets and real-world datasets.","sentences":["The Isolation Forest (iForest), proposed by Liu, Ting, and Zhou at TKDE 2012, has become a prominent tool for unsupervised anomaly detection.","However, recent research by Hariri, Kind, and Brunner, published in TKDE 2021, has revealed issues with iForest.","They identified the presence of axis-aligned ghost clusters that can be misidentified as normal clusters, leading to biased anomaly scores and inaccurate predictions.","In response, they developed the Extended Isolation Forest (EIF), which effectively solves these issues by eliminating the ghost clusters introduced by iForest.","This enhancement results in improved consistency of anomaly scores and superior performance.","We reveal a previously overlooked problem in the Extended Isolation Forest (EIF), showing that it is vulnerable to ghost inter-clusters between normal clusters of data points.","In this paper, we introduce the Rotated Isolation Forest (RIF) algorithm which effectively addresses both the axis-aligned ghost clusters observed in iForest and the ghost inter-clusters seen in EIF.","RIF accomplishes this by randomly rotating the dataset (using random rotation matrices and QR decomposition) before feeding it into the iForest construction, thereby increasing dataset variation and eliminating ghost clusters.","Our experiments conclusively demonstrate that the RIF algorithm outperforms iForest and EIF, as evidenced by the results obtained from both synthetic datasets and real-world datasets."],"url":"http://arxiv.org/abs/2501.17787v1"}
{"created":"2025-01-29 17:15:45","title":"Picard-KKT-hPINN: Enforcing Nonlinear Enthalpy Balances for Physically Consistent Neural Networks","abstract":"Neural networks are widely used as surrogate models but they do not guarantee physically consistent predictions thereby preventing adoption in various applications. We propose a method that can enforce NNs to satisfy physical laws that are nonlinear in nature such as enthalpy balances. Our approach, inspired by Picard successive approximations method, aims to enforce multiplicatively separable constraints by sequentially freezing and projecting a set of the participating variables. We demonstrate our PicardKKThPINN for surrogate modeling of a catalytic packed bed reactor for methanol synthesis. Our results show that the method efficiently enforces nonlinear enthalpy and linear atomic balances at machine-level precision. Additionally, we show that enforcing conservation laws can improve accuracy in data-scarce conditions compared to vanilla multilayer perceptron.","sentences":["Neural networks are widely used as surrogate models but they do not guarantee physically consistent predictions thereby preventing adoption in various applications.","We propose a method that can enforce NNs to satisfy physical laws that are nonlinear in nature such as enthalpy balances.","Our approach, inspired by Picard successive approximations method, aims to enforce multiplicatively separable constraints by sequentially freezing and projecting a set of the participating variables.","We demonstrate our PicardKKThPINN for surrogate modeling of a catalytic packed bed reactor for methanol synthesis.","Our results show that the method efficiently enforces nonlinear enthalpy and linear atomic balances at machine-level precision.","Additionally, we show that enforcing conservation laws can improve accuracy in data-scarce conditions compared to vanilla multilayer perceptron."],"url":"http://arxiv.org/abs/2501.17782v1"}
