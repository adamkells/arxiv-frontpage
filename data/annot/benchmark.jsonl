{"text":"We will release the dataset and code to facilitate future endeavors.","cats":{"new-dataset":1,"benchmark":0}}
{"text":"We release our dataset for others to use and build on.","cats":{"new-dataset":1,"benchmark":0}}
{"text":"We release the generated dataset and used prompts to facilitate future research.","cats":{"new-dataset":1,"benchmark":0}}
{"text":"These datasets included the latest second and third generation deepfake datasets.","cats":{"new-dataset":0,"benchmark":0}}
{"text":"The real-world datasets will be released.","cats":{"new-dataset":1,"benchmark":0}}
{"text":"Experimental results on multiple benchmark datasets demonstrate the effectiveness of our method.","cats":{"new-dataset":0,"benchmark":1}}
{"text":"We perform an exhaustive evaluation in two benchmark datasets.","cats":{"new-dataset":0,"benchmark":1}}
{"text":"We conduct experiments on two benchmark datasets.","cats":{"new-dataset":0,"benchmark":1}}
{"text":"Extensive experiments conducted on two benchmark datasets show that our approach achieves excellent performance compared to its competitors.","cats":{"new-dataset":0,"benchmark":1}}
{"text":"We validate our scheme with some of the most popular benchmarking datasets.","cats":{"new-dataset":0,"benchmark":1}}
{"text":"Our results improve the state-of-the-art on standard benchmarks.","cats":{"new-dataset":0,"benchmark":1}}
{"text":"In addition, we provide extra annotations for used datasets and introduce our new benchmark.","cats":{"new-dataset":1,"benchmark":1}}
{"text":"We then describe the dataset and the results of benchmarking.","cats":{"new-dataset":0,"benchmark":1}}
{"text":"We finally conduct extensive analyses to understand the effectiveness of our method.","cats":{"new-dataset":0,"benchmark":1}}
{"text":"Our method is effective and presents a significant improvement over the original model.","cats":{"new-dataset":0,"benchmark":1}}
{"text":"Additionally, we employ the self-training strategy to improve the performance of our method further.","cats":{"new-dataset":0,"benchmark":0}}
{"text":"Compared to a variety of baselines, our method achieves superior results.","cats":{"new-dataset":0,"data-quality":0,"benchmark":1}}
{"text":"In order to implement the pretraining phase, we curated an expansive tabular dataset comprising approximately 13 billion samples, meticulously gathered from the Kaggle platform.","cats":{"new-dataset":1,"benchmark":0}}
{"text":"Models and the dataset shall be released at https://github.com/OpenGVLab/All-Seeing, and demo can be seen at https://huggingface.co/spaces/OpenGVLab/all-seeing.","cats":{"new-dataset":0,"benchmark":0}}
{"text":"It covers a wide range of 3.5 million common and rare concepts in the real world, and has 132.2 billion tokens that describe the concepts and their attributes.","cats":{"new-dataset":0,"benchmark":0}}
{"text":"Leveraging this new dataset, we develop the All-Seeing model (ASM), a unified framework for panoptic visual recognition and understanding.","cats":{"new-dataset":0,"benchmark":0}}
{"text":"We hope that this project can serve as a foundation for vision-language artificial general intelligence research.","cats":{"new-dataset":0,"benchmark":0}}
{"text":"Extensive experiments demonstrate that the proposed method can surpass all baselines by a large margin.","cats":{"benchmark":1}}
{"text":"The experimental results show that our method strongly outperforms the baselines.","cats":{"benchmark":1}}
{"text":"Empirical results demonstrate the superiority of our method over other baselines.","cats":{"benchmark":1}}
{"text":"We provide results of standard baseline methods.","cats":{"benchmark":1}}
{"text":"Our method outperforms baselines in most tasks by a large margin.","cats":{"benchmark":1}}
{"text":"Thorough comparisons with multiple baseline methods illustrate the strengths of our proposed methods.","cats":{"benchmark":1}}
{"text":"We propose two specific methods and compare them with a baseline method.","cats":{"benchmark":1}}
{"text":"Experimental results show that the method outperforms the baseline.","cats":{"benchmark":1}}
{"text":"Experimental results show that our proposed method can achieve the accuracy@1 of 88.9\\%, which significantly outperforms other baselines by a large margin.","cats":{"benchmark":1}}
{"text":"We benchmark our method as well as several state-of-the-art baselines and demonstrate the effectiveness of the proposed approach.","cats":{"benchmark":1}}
{"text":"Some methods have achieved better results than baseline methods, and the winning methods have demonstrated superior prediction performance.","cats":{"benchmark":1}}
{"text":"To facilitate research reuse, we release our code, trained model weights, and high quality pseudo-labels for the Argoverse 2 and Waymo Open datasets.","cats":{"benchmark":0}}
{"text":"We present the All-Seeing (AS) project: a large-scale data and model for recognizing and understanding everything in the open world.","cats":{"benchmark":0}}
{"text":"The model is trained with open-ended language prompts and locations, which allows it to generalize to various vision and language tasks with remarkable zero-shot performance, including region-text retrieval, region recognition, captioning, and question-answering.","cats":{"benchmark":0}}
{"text":"Large language models (LLMs) have revolutionized NLP by solving downstream tasks with little to no labeled data.","cats":{"benchmark":0}}
{"text":"Despite their versatile abilities, the larger question of their ability to reason remains ill-understood.","cats":{"benchmark":0}}
{"text":"This paper addresses reasoning in math word problems (MWPs) by studying symbolic versions of the numeric problems, since a symbolic expression is a \"concise explanation\" of the numeric answer.","cats":{"benchmark":0}}
{"text":"Our benchmark is available at https://github.com/FudanSELab/ClassEval.","cats":{"benchmark":1}}
{"text":"Our method has attained better classification accuracy over existing methods with notable margins.","cats":{"benchmark":1}}
{"text":"Returned results, show a decent performance of the proposed algorithm (99 % accuracy) in comparison with others.","cats":{"benchmark":1}}
{"text":"For example, compared with several other related methods, UCDFormer improves performance on the Kappa coefficient by more than 12\\%.","cats":{"benchmark":1}}
{"text":"Numerical results show the superiority of the proposed algorithm over state-of-the-art methods.","cats":{"benchmark":1}}
{"text":"Our results reveal both limitations and promising aspects of adapted KGE methods.","cats":{"benchmark":1}}
{"text":"Several numerical results are presented to illustrate the effectiveness of the proposed methodologies.","cats":{"benchmark":1}}
{"text":"Our method achieves substantial improvements of +6% and","cats":{"benchmark":1}}
{"text":"This is a challenging task in which two popular neural network baselines fail.","cats":{"benchmark":1}}
{"text":"On a randomly selected and manually labeled 200 online reviews, CLAA achieved 92% accuracy while the SOTA baseline achieved 81.5%.","cats":{"benchmark":1}}
{"text":"Existing methods under this perspective are also reviewed.   ","cats":{"benchmark":1}}
{"text":"We demonstrate the capabilities of our approach on 11 different benchmarks.","cats":{"benchmark":1}}
{"text":"We evaluate our method on a newly proposed benchmark.","cats":{"benchmark":1}}
{"text":"Comprehensive experiments indicate that our method achieves state-of-the-art performance on widely-used benchmarks.","cats":{"benchmark":1}}
{"text":"We give preliminary evidence suggesting the viability of the approach on a micro-benchmark.","cats":{"benchmark":1}}
{"text":"Experimental results illustrate the effectiveness of our approach, where state-of-the-art performance is achieved on public benchmarks.","cats":{"benchmark":1}}
{"text":"Extensive experiments on benchmark datasets demonstrate the effectiveness of our proposed method.","cats":{"benchmark":1}}
{"text":"We evaluate our method on a wide range of benchmarks in different scales.","cats":{"benchmark":1}}
{"text":"All benchmarks and all raw results are available1 for further analysis.","cats":{"benchmark":1}}
{"text":"The performance of the algorithm is shown for a well-known benchmark.","cats":{"benchmark":1}}
{"text":"Experiments on three benchmarks demonstrate the effectiveness of our method.","cats":{"benchmark":1}}
{"text":"In the experiments, our framework achieves state-of-the-art results on several main benchmarks.","cats":{"benchmark":1}}
{"text":"The results ascertain the efficacy of our technique.","cats":{"benchmark":1}}
{"text":"Yet, much remains to be understood about how best to develop these techniques.","cats":{"benchmark":0}}
{"text":"Method.","cats":{"benchmark":0}}
{"text":"Methods.","cats":{"benchmark":0}}
{"text":"Experiments demonstrate that the proposed method outperforms other methods.","cats":{"benchmark":1}}
{"text":"METHODS:","cats":{"benchmark":0}}
{"text":"A majority of our experiments were toward optimizing this technique, ensuring a proper representation of the technique's potential, since many of the details were new questions.","cats":{"benchmark":0}}
{"text":"The results demonstrate a significant improvement over previous methods.","cats":{"benchmark":1}}
{"text": "Comprehensive experiments on\nbenchmark datasets demonstrate that our proposed framework can achieve\nreasonable interpretation with competitive prediction accuracy.", "meta": {"url": "http://arxiv.org/abs/2312.17624v1"}, "cats": {"benchmark": "1", "new-dataset": "0"}}
{"text": "We provide qualitative and quantitative\nresults for the same over multiple applications, such as selective style\nediting and swapping using test images sampled from several datasets.", "cats": {"new-dataset": "0", "benchmark": "0", "ethical-ai": "0", "reproducibility": "0"}}
{"text": "Remarkably, while maintaining a similar level of accuracy to the conventional\nCPU/CVODE-based solver, the GPU/ML-accelerated approach shows an overall\nspeedup of over two orders of magnitude for both cases.", "cats": {"new-dataset": "0", "benchmark": "0", "ethical-ai": "0", "reproducibility": "0"}}
{"text": "This is achieved through a specialized loss function tailored to\nlearn from the LLM's output probabilities, ensuring that the student model\nclosely mimics the teacher's performance.", "cats": {"new-dataset": "0", "benchmark": "0", "ethical-ai": "0", "reproducibility": "0"}}
{"text": "This study proposes a method for distilling the knowledge of fine-tuned Large\nLanguage Models (LLMs) into a smaller, more efficient, and accurate neural\nnetwork, specifically targeting the challenge of deploying these models on\nresource-constrained devices.", "cats": {"benchmark": "0", "ethical-ai": "0", "reproducibility": "0"}}
{"text": "Experimental evaluation employing\nadapted versions of renowned models (e.g., BERT, RoBERTa) across established\nbenchmark English and multilingual datasets for text classification and\nsequence labeling shows that encrypted models achieve performance parity with\ntheir original counterparts.", "cats": {"explainability": "0", "ethical-ai": "0", "benchmark": "0", "new-dataset": "0", "causality": "0"}}
{"text": "In particular,\nHLA adopts a hierarchical framework and comprises three modules: a proficient\nLLM, referred to as Slow Mind, for intention reasoning and language\ninteraction, a lightweight LLM, referred to as Fast Mind, for generating macro\nactions, and a reactive policy, referred to as Executor, for transforming macro\nactions into atomic actions.", "cats": {"causality": "0", "explainability": "0", "industry": "0", "reproducibility": "0", "ethical-ai": "0", "benchmark": "0", "new-dataset": "0"}}
{"text": "Unlike the existing VLN task that only pays attention to\nvision and instruction (language), the WebVLN agent further considers\nunderlying web-specific content like HTML, which could not be seen on the\nrendered web pages yet contains rich visual and textual information.", "cats": {"causality": "0", "explainability": "0", "industry": "0", "ethical-ai": "0", "benchmark": "0", "new-dataset": "0"}}
{"text": "We first\ncreated the README dataset, an extensive collection of over 20,000 unique\nmedical terms and 300,000 mentions, each offering context-aware lay definitions\nmanually annotated by domain experts.", "cats": {"new-dataset": "1", "benchmark": "0", "industry": "1", "explainability": "0"}}
{"text": "Within this frame, we define the news intent\nidentification task and provide a benchmark dataset with fine-grained labels\nalong with an efficient benchmark method.", "cats": {"benchmark": "1", "ethical-ai": "0"}}
{"text": "The innovative approach of DIVSE lies in its adaptive\nlearning capability, which evolves over time to tailor voice outputs to\nspecific user traits.", "cats": {"causality": "0", "explainability": "0", "industry": "0", "ethical-ai": "0", "benchmark": "0", "new-dataset": "0"}}
{"text": "To operationalise this metric, we engaged in a multi-step process\ninvolving collecting and annotating LLM responses, applying sophisticated\nNatural Language Processing (NLP) techniques for bias detection, and computing\nthe LLMBI score through a specially crafted mathematical formula.", "cats": {"causality": "0", "explainability": "0", "industry": "0", "new-dataset": "0", "benchmark": "0", "ethical-ai": "0", "reproducibility": "0"}}
{"text": "If we focus only later in the pipeline -- making LLMs marginally\nmore fair as they facilitate use cases which intrinsically entrench power -- we\nwill miss an important opportunity to guide them to equitable impacts.", "cats": {"causality": "0", "industry": "0", "benchmark": "0"}}
{"text": "Using the results from the simulated data, we apply CD-NOTS to a broad range of\nindices and factors in order to identify causal connections among the entities,\nthereby showing how causal discovery can serve as a valuable tool for\nfactor-based investing, portfolio diversification, and comprehension of market\ndynamics.", "cats": {"causality": "1", "benchmark": "0", "reproducibility": "0"}}
{"text": "With the estimated (ratios\nof) causal coefficients, we propose a novel approach to identify the existence\nof a causal edge between two observed variables subject to latent variable\ninfluence.", "cats": {"causality": "1", "benchmark": "0", "industry": "0"}}
{"text": "On the other hand, causality can exhibit\nemergence, meaning that new causal laws may arise as we increase the level of\nabstraction.", "cats": {"causality": "1", "new-dataset": "0", "benchmark": "0"}}
{"text": "Further, we prove that the IMA\nprinciple is approximately satisfied with high probability (increasing with the\nnumber of observed mixtures) when the directions along which the latent\ncomponents influence the observations are chosen independently at random.", "cats": {"causality": "0", "explainability": "0", "industry": "0", "reproducibility": "0", "ethical-ai": "0", "benchmark": "0", "new-dataset": "0"}}
{"text": "Our approach aims to\ndetect inter-agent causal influences in specific situations based on the\ncriterion using causal intervention and conditional mutual information.", "cats": {"causality": "1", "explainability": "0", "reproducibility": "0", "benchmark": "0"}}
{"text": "A number of ways\nto construct these many-body solitons (explicitly in the case where the local\nHilbert space dimension $d=2$) are then demonstrated: firstly, via a simple\nconstruction involving products of smaller, constituent solitons; and secondly,\nvia a construction which cannot be understood as simply in terms of products of\nsmaller solitons, but which does have a neat interpretation in terms of\nproducts of fermions under a Jordan-Wigner transformation.", "cats": {"causality": "0", "explainability": "0", "industry": "0", "reproducibility": "0", "ethical-ai": "0", "benchmark": "0"}}
{"text": "We\nalso show that, under certain graphical conditions, RLCD correctly identifies\nthe Markov Equivalence Class of the whole latent causal graph asymptotically.", "cats": {"causality": "1", "benchmark": "0"}}
