<!doctype html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script src="https://cdn.tailwindcss.com"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/@alpinejs/collapse@3.x.x/dist/cdn.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js"></script>
</head>
<body>
  <div class="relative mx-auto h-full max-w-2xl text-md">
    <table class="table-auto">
      <tbody>
        <tr>
          <td></td>
          <td>
            <h1 class="text-4xl pt-4 font-bold"><span class="underline">Adam's</span> Arxiv FrontPage</h1>
            <br>
            <p>Generated on 2024-03-27.</p><br/>
            <p class="text-sm text-gray-500 pt-2">This frontpage is made by scraping arxiv and by runnig a sentence-model that detects if the abstract describes a paper about a topic of interest. One cool feature: it all pretty much runs via Github Actions. </p>
            <br>
          </td>
        </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">New Datasets</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Project MOSLA: Recording Every Moment of Second Language Acquisition
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Second language acquisition (SLA) is a complex and dynamic process.Many SLA studies that have attempted to record and analyze this process have typically focused on a single modality (e.g., textual output of learners), covered only a short period of time, and/or lacked control (e.g., failed to capture every aspect of the learning process).<span class='px-1 mx-1 bg-yellow-200'>In Project MOSLA (Moments of Second Language Acquisition), we have created a longitudinal, multimodal, multilingual, and controlled dataset by inviting participants to learn one of three target languages (Arabic, Spanish, and Chinese) from scratch over a span of two years, exclusively through online instruction, and recording every lesson using Zoom. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.725</span></span><span class='px-1 mx-1 bg-yellow-200'>The dataset is semi-automatically annotated with speaker/language IDs and transcripts by both human annotators and fine-tuned state-of-the-art speech models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.865</span></span>Our experiments reveal linguistic insights into learners' proficiency development over time, as well as the potential for automatically detecting the areas of focus on the screen purely from the unannotated multimodal data.<span class='px-1 mx-1 bg-yellow-200'>Our dataset is freely available for research purposes and can serve as a valuable resource for a wide range of applications, including but not limited to SLA, proficiency assessment, language and speech processing, pedagogy, and multimodal learning analytics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.871</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17314v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RuBia: A Russian Language Bias Detection Dataset
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Warning: this work contains upsetting or disturbing content.   Large language models (LLMs) tend to learn the social and cultural biases present in the raw pre-training data.To test if an LLM's behavior is fair, functional datasets are employed, and due to their purpose, these datasets are highly language and culture-specific.In this paper, we address a gap in the scope of multilingual bias evaluation by presenting a bias detection dataset specifically designed for the Russian language, dubbed as RuBia.<span class='px-1 mx-1 bg-yellow-200'>The RuBia dataset is divided into 4 domains: gender, nationality, socio-economic status, and diverse, each of the domains is further divided into multiple fine-grained subdomains. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.757</span></span>Every example in the dataset consists of two sentences with the first reinforcing a potentially harmful stereotype or trope and the second contradicting it.These sentence pairs were first written by volunteers and then validated by native-speaking crowdsourcing workers.Overall, there are nearly 2,000 unique sentence pairs spread over 19 subdomains in RuBia.To illustrate the dataset's purpose, we conduct a diagnostic evaluation of state-of-the-art or near-state-of-the-art LLMs and discuss the LLMs' predisposition to social biases.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17553v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Panonut360: A Head and Eye Tracking Dataset for Panoramic Video
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the rapid development and widespread application of VR/AR technology, maximizing the quality of immersive panoramic video services that match users' personal preferences and habits has become a long-standing challenge.Understanding the saliency region where users focus, based on data collected with HMDs, can promote multimedia encoding, transmission, and quality assessment.At the same time, large-scale datasets are essential for researchers and developers to explore short/long-term user behavior patterns and train AI models related to panoramic videos.However, existing panoramic video datasets often include low-frequency user head or eye movement data through short-term videos only, lacking sufficient data for analyzing users' Field of View (FoV) and generating video saliency regions.   Driven by these practical factors, in this paper, we present a head and eye tracking dataset involving 50 users (25 males and 25 females) watching 15 panoramic videos.<span class='px-1 mx-1 bg-yellow-200'>The dataset provides details on the viewport and gaze attention locations of users. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.849</span></span><span class='px-1 mx-1 bg-yellow-200'>Besides, we present some statistics samples extracted from the dataset. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.874</span></span>For example, the deviation between head and eye movements challenges the widely held assumption that gaze attention decreases from the center of the FoV following a Gaussian distribution.Our analysis reveals a consistent downward offset in gaze fixations relative to the FoV in experimental settings involving multiple users and videos.That's why we name the dataset Panonut, a saliency weighting shaped like a donut.Finally, we also provide a script that generates saliency distributions based on given head or eye coordinates and pre-generated saliency distribution map sets of each video from the collected eye tracking data.   <span class='px-1 mx-1 bg-yellow-200'>The dataset is available on website: https://dianvrlab.github.io/Panonut360/. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.981</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17708v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Invisible Gas Detection: An RGB-Thermal Cross Attention Network and A New Benchmark
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The widespread use of various chemical gases in industrial processes necessitates effective measures to prevent their leakage during transportation and storage, given their high toxicity.Thermal infrared-based computer vision detection techniques provide a straightforward approach to identify gas leakage areas.However, the development of high-quality algorithms has been challenging due to the low texture in thermal images and the lack of open-source datasets.In this paper, we present the RGB-Thermal Cross Attention Network (RT-CAN), which employs an RGB-assisted two-stream network architecture to integrate texture information from RGB images and gas area information from thermal images.Additionally, to facilitate the research of invisible gas detection, we introduce Gas-DB, an extensive open-source gas detection database including about 1.3K well-annotated RGB-thermal images with eight variant collection scenes.Experimental results demonstrate that our method successfully leverages the advantages of both modalities, achieving state-of-the-art (SOTA) performance among RGB-thermal methods, surpassing single-stream SOTA models in terms of accuracy, Intersection of Union (IoU), and F2 metrics by 4.86%, 5.65%, and 4.88%, respectively.<span class='px-1 mx-1 bg-yellow-200'>The code and data will be made available soon. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.856</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17712v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DataCook: Crafting Anti-Adversarial Examples for Healthcare Data Copyright Protection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In the realm of healthcare, the challenges of copyright protection and unauthorized third-party misuse are increasingly significant.Traditional methods for data copyright protection are applied prior to data distribution, implying that models trained on these data become uncontrollable.This paper introduces a novel approach, named DataCook, designed to safeguard the copyright of healthcare data during the deployment phase.DataCook operates by "cooking" the raw data before distribution, enabling the development of models that perform normally on this processed data.However, during the deployment phase, the original test data must be also "cooked" through DataCook to ensure normal model performance.This process grants copyright holders control over authorization during the deployment phase.The mechanism behind DataCook is by crafting anti-adversarial examples (AntiAdv), which are designed to enhance model confidence, as opposed to standard adversarial examples (Adv) that aim to confuse models.Similar to Adv, AntiAdv introduces imperceptible perturbations, ensuring that the data processed by DataCook remains easily understandable.We conducted extensive experiments on MedMNIST datasets, encompassing both 2D/3D data and the high-resolution variants.The outcomes indicate that DataCook effectively meets its objectives, preventing models trained on AntiAdv from analyzing unauthorized data effectively, without compromising the validity and accuracy of the data in legitimate scenarios.<span class='px-1 mx-1 bg-yellow-200'>Code and data are available at https://github.com/MedMNIST/DataCook. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.826</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17755v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                TWOLAR: a TWO-step LLM-Augmented distillation method for passage Reranking
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this paper, we present TWOLAR: a two-stage pipeline for passage reranking based on the distillation of knowledge from Large Language Models (LLM).TWOLAR introduces a new scoring strategy and a distillation process consisting in the creation of a novel and diverse training dataset.The dataset consists of 20K queries, each associated with a set of documents retrieved via four distinct retrieval methods to ensure diversity, and then reranked by exploiting the zero-shot reranking capabilities of an LLM.Our ablation studies demonstrate the contribution of each new component we introduced.Our experimental results show that TWOLAR significantly enhances the document reranking ability of the underlying model, matching and in some cases even outperforming state-of-the-art models with three orders of magnitude more parameters on the TREC-DL test sets and the zero-shot evaluation benchmark BEIR.<span class='px-1 mx-1 bg-yellow-200'>To facilitate future work we release our data set, finetuned models, and code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.816</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17759v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Assessment of Multimodal Large Language Models in Alignment with Human Values
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) aim to serve as versatile assistants aligned with human values, as defined by the principles of being helpful, honest, and harmless (hhh).However, in terms of Multimodal Large Language Models (MLLMs), despite their commendable performance in perception and reasoning tasks, their alignment with human values remains largely unexplored, given the complexity of defining hhh dimensions in the visual world and the difficulty in collecting relevant data that accurately mirrors real-world situations.To address this gap, we introduce Ch3Ef, a Compreh3ensive Evaluation dataset and strategy for assessing alignment with human expectations.<span class='px-1 mx-1 bg-yellow-200'>Ch3Ef dataset contains 1002 human-annotated data samples, covering 12 domains and 46 tasks based on the hhh principle. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.743</span></span>We also present a unified evaluation strategy supporting assessment across various scenarios and different perspectives.Based on the evaluation results, we summarize over 10 key findings that deepen the understanding of MLLM capabilities, limitations, and the dynamic relationships between evaluation levels, guiding future advancements in the field.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17830v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>High Dynamic Range (HDR) content (i.e., images and videos) has a broad range of applications.However, capturing HDR content from real-world scenes is expensive and time- consuming.Therefore, the challenging task of reconstructing visually accurate HDR images from their Low Dynamic Range (LDR) counterparts is gaining attention in the vision research community.<span class='px-1 mx-1 bg-yellow-200'>A major challenge in this research problem is the lack of datasets, which capture diverse scene conditions (e.g., lighting, shadows, weather, locations, landscapes, objects, humans, buildings) and various image features (e.g., color, contrast, saturation, hue, luminance, brightness, radiance). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.844</span></span><span class='px-1 mx-1 bg-yellow-200'>To address this gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset of photo-realistic HDR images sampled from the GTA-V video game. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.786</span></span>We perform thorough evaluation of the proposed dataset, which demonstrates significant qualitative and quantitative improvements of the state-of-the-art HDR image reconstruction methods.Furthermore, we demonstrate the effectiveness of the proposed dataset and its impact on additional computer vision tasks including 3D human pose estimation, human body part segmentation, and holistic scene segmentation.<span class='px-1 mx-1 bg-yellow-200'>The dataset, data collection pipeline, and evaluation code are available at: https://github.com/HrishavBakulBarua/GTA-HDR. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.763</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17837v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features.While these maps allow for the prediction of point-wise saliency maps when queried for a certain language concept, large-scale environments and abstract queries beyond the object level still pose a considerable hurdle, ultimately limiting language-grounded robotic navigation.In this work, we present HOV-SG, a hierarchical open-vocabulary 3D scene graph mapping approach for language-grounded robot navigation.Leveraging open-vocabulary vision foundation models, we first obtain state-of-the-art open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene graph hierarchy consisting of floor, room, and object concepts, each enriched with open-vocabulary features.Our approach is able to represent multi-story buildings and allows robotic traversal of those using a cross-floor Voronoi graph.HOV-SG is evaluated on three distinct datasets and surpasses previous baselines in open-vocabulary semantic accuracy on the object, room, and floor level while producing a 75% reduction in representation size compared to dense open-vocabulary maps.In order to prove the efficacy and generalization capabilities of HOV-SG, we showcase successful long-horizon language-conditioned robot navigation within real-world multi-storage environments.<span class='px-1 mx-1 bg-yellow-200'>We provide code and trial video data at http://hovsg.github.io/. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.818</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17846v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MIND Your Language: A Multilingual Dataset for Cross-lingual News Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Digital news platforms use news recommenders as the main instrument to cater to the individual information needs of readers.Despite an increasingly language-diverse online community, in which many Internet users consume news in multiple languages, the majority of news recommendation focuses on major, resource-rich languages, and English in particular.Moreover, nearly all news recommendation efforts assume monolingual news consumption, whereas more and more users tend to consume information in at least two languages.Accordingly, the existing body of work on news recommendation suffers from a lack of publicly available multilingual benchmarks that would catalyze development of news recommenders effective in multilingual settings and for low-resource languages.<span class='px-1 mx-1 bg-yellow-200'>Aiming to fill this gap, we introduce xMIND, an open, multilingual news recommendation dataset derived from the English MIND dataset using machine translation, covering a set of 14 linguistically and geographically diverse languages, with digital footprints of varying sizes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.831</span></span>Using xMIND, we systematically benchmark several state-of-the-art content-based neural news recommenders (NNRs) in both zero-shot (ZS-XLT) and few-shot (FS-XLT) cross-lingual transfer scenarios, considering both monolingual and bilingual news consumption patterns.Our findings reveal that (i) current NNRs, even when based on a multilingual language model, suffer from substantial performance losses under ZS-XLT and that (ii) inclusion of target-language data in FS-XLT training has limited benefits, particularly when combined with a bilingual news consumption.Our findings thus warrant a broader research effort in multilingual and cross-lingual news recommendation.<span class='px-1 mx-1 bg-yellow-200'>The xMIND dataset is available at https://github.com/andreeaiana/xMIND. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.922</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17876v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Sen2Fire: A Challenging Benchmark Dataset for Wildfire Detection using Sentinel Data
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Utilizing satellite imagery for wildfire detection presents substantial potential for practical applications.<span class='px-1 mx-1 bg-yellow-200'>To advance the development of machine learning algorithms in this domain, our study introduces the \textit{Sen2Fire} dataset--a challenging satellite remote sensing dataset tailored for wildfire detection. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.705</span></span><span class='px-1 mx-1 bg-yellow-200'>This dataset is curated from Sentinel-2 multi-spectral data and Sentinel-5P aerosol product, comprising a total of 2466 image patches. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.961</span></span>Each patch has a size of 512$\times$512 pixels with 13 bands.Given the distinctive sensitivities of various wavebands to wildfire responses, our research focuses on optimizing wildfire detection by evaluating different wavebands and employing a combination of spectral indices, such as normalized burn ratio (NBR) and normalized difference vegetation index (NDVI).The results suggest that, in contrast to using all bands for wildfire detection, selecting specific band combinations yields superior performance.Additionally, our study underscores the positive impact of integrating Sentinel-5 aerosol data for wildfire detection.<span class='px-1 mx-1 bg-yellow-200'>The code and dataset are available online (https://zenodo.org/records/10881058). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.887</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17884v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AgentStudio: A Toolkit for Building General Virtual Agents
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Creating autonomous virtual agents capable of using arbitrary software on any digital device remains a major challenge for artificial intelligence.Two key obstacles hinder progress: insufficient infrastructure for building virtual agents in real-world environments, and the need for in-the-wild evaluation of fundamental agent abilities.To address this, we introduce AgentStudio, an online, realistic, and multimodal toolkit that covers the entire lifecycle of agent development.<span class='px-1 mx-1 bg-yellow-200'>This includes environment setups, data collection, agent evaluation, and visualization. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.782</span></span>The observation and action spaces are highly generic, supporting both function calling and human-computer interfaces.This versatility is further enhanced by AgentStudio's graphical user interfaces, which allow efficient development of datasets and benchmarks in real-world settings.<span class='px-1 mx-1 bg-yellow-200'>To illustrate, we introduce a visual grounding dataset and a real-world benchmark suite, both created with our graphical interfaces. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.891</span></span>Furthermore, we present several actionable insights derived from AgentStudio, e.g., general visual grounding, open-ended tool creation, learning from videos, etc.We have open-sourced the environments, datasets, benchmarks, and interfaces to promote research towards developing general virtual agents for the future.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17918v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Gestures play a key role in human communication.Recent methods for co-speech gesture generation, while managing to generate beat-aligned motions, struggle generating gestures that are semantically aligned with the utterance.Compared to beat gestures that align naturally to the audio signal, semantically coherent gestures require modeling the complex interactions between the language and human motion, and can be controlled by focusing on certain words.Therefore, we present ConvoFusion, a diffusion-based approach for multi-modal gesture synthesis, which can not only generate gestures based on multi-modal speech inputs, but can also facilitate controllability in gesture synthesis.Our method proposes two guidance objectives that allow the users to modulate the impact of different conditioning modalities (e.g. audio vs text) as well as to choose certain words to be emphasized during gesturing.Our method is versatile in that it can be trained either for generating monologue gestures or even the conversational gestures.<span class='px-1 mx-1 bg-yellow-200'>To further advance the research on multi-party interactive gestures, the DnD Group Gesture dataset is released, which contains 6 hours of gesture data showing 5 people interacting with one another. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.961</span></span>We compare our method with several recent works and demonstrate effectiveness of our method on a variety of tasks.We urge the reader to watch our supplementary video at our website.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17936v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Spike-NeRF: Neural Radiance Field Based On Spike Camera
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As a neuromorphic sensor with high temporal resolution, spike cameras offer notable advantages over traditional cameras in high-speed vision applications such as high-speed optical estimation, depth estimation, and object tracking.Inspired by the success of the spike camera, we proposed Spike-NeRF, the first Neural Radiance Field derived from spike data, to achieve 3D reconstruction and novel viewpoint synthesis of high-speed scenes.Instead of the multi-view images at the same time of NeRF, the inputs of Spike-NeRF are continuous spike streams captured by a moving spike camera in a very short time.To reconstruct a correct and stable 3D scene from high-frequency but unstable spike data, we devised spike masks along with a distinctive loss function.We evaluate our method qualitatively and numerically on several challenging synthetic scenes generated by blender with the spike camera simulator.Our results demonstrate that Spike-NeRF produces more visually appealing results than the existing methods and the baseline we proposed in high-speed scenes.<span class='px-1 mx-1 bg-yellow-200'>Our code and data will be released soon. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.909</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16410v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Human Understanding AI Paper Challenge 2024 -- Dataset Design
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In 2024, we will hold a research paper competition (the third Human Understanding AI Paper Challenge) for the research and development of artificial intelligence technologies to understand human daily life.<span class='px-1 mx-1 bg-yellow-200'>This document introduces the datasets that will be provided to participants in the competition, and summarizes the issues to consider in data processing and learning model development. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.766</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16509v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                FineWAVE: Fine-Grained Warning Verification of Bugs for Automated Static Analysis Tools
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The continual expansion of software size and complexity has led to an increased focus on reducing defects and bugs during development.Although Automated Static Analysis Tools (ASATs) offer help, in practice, the significant number of false positives can impede developers' productivity and confidence in the tools.Therefore, previous research efforts have explored learning-based methods to validate the reported warnings.Nevertheless, there are still some limitations.(1) The granularity of prior research is coarse, as it focuses on identifying either actionable warnings throughout extensive development histories or potential true warnings at the function level.These approaches lack specificity regarding individual bugs and warnings.(2) Machine learning-based approaches need much manual effort for feature engineering while existing deep learning-based approaches ignore key semantics between source code and warnings.(3) The small number of selected projects hinders the comprehensive evaluation of these approaches.In this paper, we proposed a fine-grained warning verification approach that is sensitive to bugs for improving the results of ASATs, namely \ourtool.Specifically, we design a novel LSTM-based model that captures both fine-grained semantics of source code and warnings from ASATs and highlights their correlations with cross-attention.<span class='px-1 mx-1 bg-yellow-200'>To tackle the data scarcity of training and evaluation, we collected a large-scale dataset of 280,273 warnings, namely FineWA. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.78</span></span>It is ten times larger than the existing largest dataset.Then, we conducted extensive experiments on the dataset to evaluate FineWAVE.The experimental results demonstrate the effectiveness of our approach, with an F1-score of 97.79% for reducing false alarms and 67.06% for confirming actual warnings, which also significantly outperforms all baselines.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16032v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                V2X-Real: a Largs-Scale Dataset for Vehicle-to-Everything Cooperative Perception
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent advancements in Vehicle-to-Everything (V2X) technologies have enabled autonomous vehicles to share sensing information to see through occlusions, greatly boosting the perception capability.However, there are no real-world datasets to facilitate the real V2X cooperative perception research -- existing datasets either only support Vehicle-to-Infrastructure cooperation or Vehicle-to-Vehicle cooperation.In this paper, we propose a dataset that has a mixture of multiple vehicles and smart infrastructure simultaneously to facilitate the V2X cooperative perception development with multi-modality sensing data.Our V2X-Real is collected using two connected automated vehicles and two smart infrastructures, which are all equipped with multi-modal sensors including LiDAR sensors and multi-view cameras.<span class='px-1 mx-1 bg-yellow-200'>The whole dataset contains 33K LiDAR frames and 171K camera data with over 1.2M annotated bounding boxes of 10 categories in very challenging urban scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.92</span></span>According to the collaboration mode and ego perspective, we derive four types of datasets for Vehicle-Centric, Infrastructure-Centric, Vehicle-to-Vehicle, and Infrastructure-to-Infrastructure cooperative perception.Comprehensive multi-class multi-agent benchmarks of SOTA cooperative perception methods are provided.<span class='px-1 mx-1 bg-yellow-200'>The V2X-Real dataset and benchmark codes will be released. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.709</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16034v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic Human Modeling
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>High-quality human reconstruction and photo-realistic rendering of a dynamic scene is a long-standing problem in computer vision and graphics.Despite considerable efforts invested in developing various capture systems and reconstruction algorithms, recent advancements still struggle with loose or oversized clothing and overly complex poses.In part, this is due to the challenges of acquiring high-quality human datasets.<span class='px-1 mx-1 bg-yellow-200'>To facilitate the development of these fields, in this paper, we present PKU-DyMVHumans, a versatile human-centric dataset for high-fidelity reconstruction and rendering of dynamic human scenarios from dense multi-view videos. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.84</span></span>It comprises 8.2 million frames captured by more than 56 synchronized cameras across diverse scenarios.These sequences comprise 32 human subjects across 45 different scenarios, each with a high-detailed appearance and realistic human motion.Inspired by recent advancements in neural radiance field (NeRF)-based scene representations, we carefully set up an off-the-shelf framework that is easy to provide those state-of-the-art NeRF-based implementations and benchmark on PKU-DyMVHumans dataset.It is paving the way for various applications like fine-grained foreground/background decomposition, high-quality human reconstruction and photo-realistic novel view synthesis of a dynamic scene.Extensive studies are performed on the benchmark, demonstrating new observations and challenges that emerge from using such high-fidelity dynamic data.<span class='px-1 mx-1 bg-yellow-200'>The dataset is available at: https://pku-dymvhumans.github.io. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.969</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16080v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                WangchanLion and WangchanX MRC Eval
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This technical report describes the development of WangchanLion, an instruction fine-tuned model focusing on Machine Reading Comprehension (MRC) in the Thai language.<span class='px-1 mx-1 bg-yellow-200'>Our model is based on SEA-LION and a collection of instruction following datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.858</span></span>To promote open research and reproducibility, we publically release all training data, code, and the final model weights under the Apache-2 license.To assess the contextual understanding capability, we conducted extensive experimental studies using two Thai MRC datasets, XQuAD and Iapp_wiki_qa_squad.Experimental results demonstrate the model's ability to comprehend the context and produce an answer faithful to the reference one in 0-shot and 1-shot settings.In addition, our evaluation goes beyond the traditional MRC.We propose a new evaluation scheme assessing the answer's correctness, helpfulness, conciseness, and contextuality.Evaluation results provide insight into how we can improve our model in the future.Our code is public at https://github.com/vistec-AI/WangchanLion.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16127v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                On the Equivalency, Substitutability, and Flexibility of Synthetic Data
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We study, from an empirical standpoint, the efficacy of synthetic data in real-world scenarios.Leveraging synthetic data for training perception models has become a key strategy embraced by the community due to its efficiency, scalability, perfect annotations, and low costs.<span class='px-1 mx-1 bg-yellow-200'>Despite proven advantages, few studies put their stress on how to efficiently generate synthetic datasets to solve real-world problems and to what extent synthetic data can reduce the effort for real-world data collection. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.708</span></span>To answer the questions, we systematically investigate several interesting properties of synthetic data -- the equivalency of synthetic data to real-world data, the substitutability of synthetic data for real data, and the flexibility of synthetic data generators to close up domain gaps.Leveraging the M3Act synthetic data generator, we conduct experiments on DanceTrack and MOT17.Our results suggest that synthetic data not only enhances model performance but also demonstrates substitutability for real data, with 60% to 80% replacement without performance loss.In addition, our study of the impact of synthetic data distributions on downstream performance reveals the importance of flexible data generators in narrowing domain gaps for improved model adaptability.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16244v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio Benchmarks and Novel Data
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The NeurIPS 2023 Machine Learning for Audio Workshop brings together machine learning (ML) experts from various audio domains.There are several valuable audio-driven ML tasks, from speech emotion recognition to audio event detection, but the community is sparse compared to other ML areas, e.g., computer vision or natural language processing.A major limitation with audio is the available data; with audio being a time-dependent modality, high-quality data collection is time-consuming and costly, making it challenging for academic groups to apply their often state-of-the-art strategies to a larger, more generalizable dataset.<span class='px-1 mx-1 bg-yellow-200'>In this short white paper, to encourage researchers with limited access to large-datasets, the organizers first outline several open-source datasets that are available to the community, and for the duration of the workshop are making several propriety datasets available. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.946</span></span><span class='px-1 mx-1 bg-yellow-200'>Namely, three vocal datasets, Hume-Prosody, Hume-VocalBurst, an acted emotional speech dataset Modulate-Sonata, and an in-game streamer dataset Modulate-Stream. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.83</span></span>We outline the current baselines on these datasets but encourage researchers from across audio to utilize them outside of the initial baseline tasks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14048v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain Multi-Hop Dense Sentence Retrieval
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In recent research, contrastive learning has proven to be a highly effective method for representation learning and is widely used for dense retrieval.However, we identify that relying solely on contrastive learning can lead to suboptimal retrieval performance.On the other hand, despite many retrieval datasets supporting various learning objectives beyond contrastive learning, combining them efficiently in multi-task learning scenarios can be challenging.In this paper, we introduce M3, an advanced recursive Multi-hop dense sentence retrieval system built upon a novel Multi-task Mixed-objective approach for dense text representation learning, addressing the aforementioned challenges.Our approach yields state-of-the-art performance on a large-scale open-domain fact verification benchmark dataset, FEVER.<span class='px-1 mx-1 bg-yellow-200'>Code and data are available at: https://github.com/TonyBY/M3 <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.792</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14074v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Training point-based deep learning networks for forest segmentation with synthetic data
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Remote sensing through unmanned aerial systems (UAS) has been increasing in forestry in recent years, along with using machine learning for data processing.Deep learning architectures, extensively applied in natural language and image processing, have recently been extended to the point cloud domain.However, the availability of point cloud datasets for training and testing remains limited.Creating forested environment point cloud datasets is expensive, requires high-precision sensors, and is time-consuming as manual point classification is required.Moreover, forest areas could be inaccessible or dangerous for humans, further complicating data collection.Then, a question arises whether it is possible to use synthetic data to train deep learning networks without the need to rely on large volumes of real forest data.To answer this question, we developed a realistic simulator that procedurally generates synthetic forest scenes.Thanks to this, we have conducted a comparative study of different state-of-the-art point-based deep learning networks for forest segmentation.Using created datasets, we determined the feasibility of using synthetic data to train deep learning networks to classify point clouds from real forest datasets.<span class='px-1 mx-1 bg-yellow-200'>Both the simulator and the datasets are released as part of this work. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.841</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14115v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Leveraging Large Language Model-based Room-Object Relationships Knowledge for Enhancing Multimodal-Input Object Goal Navigation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Object-goal navigation is a crucial engineering task for the community of embodied navigation; it involves navigating to an instance of a specified object category within unseen environments.Although extensive investigations have been conducted on both end-to-end and modular-based, data-driven approaches, fully enabling an agent to comprehend the environment through perceptual knowledge and perform object-goal navigation as efficiently as humans remains a significant challenge.Recently, large language models have shown potential in this task, thanks to their powerful capabilities for knowledge extraction and integration.<span class='px-1 mx-1 bg-yellow-200'>In this study, we propose a data-driven, modular-based approach, trained on a dataset that incorporates common-sense knowledge of object-to-room relationships extracted from a large language model. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.777</span></span>We utilize the multi-channel Swin-Unet architecture to conduct multi-task learning incorporating with multimodal inputs.The results in the Habitat simulator demonstrate that our framework outperforms the baseline by an average of 10.6% in the efficiency metric, Success weighted by Path Length (SPL).The real-world demonstration shows that the proposed approach can efficiently conduct this task by traversing several rooms.For more details and real-world demonstrations, please check our project webpage (https://sunleyuan.github.io/ObjectNav).</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14163v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Chain-of-Thought (CoT) prompting can enhance the reasoning capabilities of large language models (LLMs), establishing itself as a primary approach to solving complex reasoning tasks.Existing CoT synthesis approaches usually focus on simpler reasoning tasks and thus result in low-quality and inconsistent CoT prompts.In response to this challenge, we present an empirical investigation of CoT prompting and introduce CoTGenius, a novel framework designed for the automatic generation of superior CoT prompts.CoTGenius is developed based on three major evolution strategies, i.e., complicate, diversify, and specify-alongside two filtering mechanisms: evolutionary success judgement and correctness verification.<span class='px-1 mx-1 bg-yellow-200'>We further employ CoTGenius to create an extensive CoT dataset, and subsequently fine-tune the Llama 2-Chat 7B and 13B models on this dataset. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.792</span></span>We call the resulting model ChainLM.To deal with the cumulative error issue in reasoning steps, we propose a step-level debating method, wherein multiple debaters discuss each reasoning step to arrive at the correct answer.Extensive experiments demonstrate that our ChainLM models exhibit enhanced proficiency in addressing a spectrum of complex reasoning problems compared to existing models.In addition, we conduct an in-depth analysis of the impact of data categories within CoTGenius on the model performance.<span class='px-1 mx-1 bg-yellow-200'>We release our dataset and code at https://github.com/RUCAIBox/ChainLM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.814</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14312v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring 3D Human Pose Estimation and Forecasting from the Robot's Perspective: The HARPER Dataset
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We introduce HARPER, a novel dataset for 3D body pose estimation and forecast in dyadic interactions between users and \spot, the quadruped robot manufactured by Boston Dynamics.The key-novelty is the focus on the robot's perspective, i.e., on the data captured by the robot's sensors.These make 3D body pose analysis challenging because being close to the ground captures humans only partially.The scenario underlying HARPER includes 15 actions, of which 10 involve physical contact between the robot and users.The Corpus contains not only the recordings of the built-in stereo cameras of Spot, but also those of a 6-camera OptiTrack system (all recordings are synchronized).This leads to ground-truth skeletal representations with a precision lower than a millimeter.<span class='px-1 mx-1 bg-yellow-200'>In addition, the Corpus includes reproducible benchmarks on 3D Human Pose Estimation, Human Pose Forecasting, and Collision Prediction, all based on publicly available baseline approaches. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.757</span></span>This enables future HARPER users to rigorously compare their results with those we provide in this work.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14447v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                TiBiX: Leveraging Temporal Information for Bidirectional X-ray and Report Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the emergence of vision language models in the medical imaging domain, numerous studies have focused on two dominant research activities: (1) report generation from Chest X-rays (CXR), and (2) synthetic scan generation from text or reports.Despite some research incorporating multi-view CXRs into the generative process, prior patient scans and reports have been generally disregarded.This can inadvertently lead to the leaving out of important medical information, thus affecting generation quality.To address this, we propose TiBiX: Leveraging Temporal information for Bidirectional X-ray and Report Generation.Considering previous scans, our approach facilitates bidirectional generation, primarily addressing two challenging problems: (1) generating the current image from the previous image and current report and (2) generating the current report based on both the previous and current images.<span class='px-1 mx-1 bg-yellow-200'>Moreover, we extract and release a curated temporal benchmark dataset derived from the MIMIC-CXR dataset, which focuses on temporal data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.767</span></span>Our comprehensive experiments and ablation studies explore the merits of incorporating prior CXRs and achieve state-of-the-art (SOTA) results on the report generation task.Furthermore, we attain on-par performance with SOTA image generation efforts, thus serving as a new baseline in longitudinal bidirectional CXR-to-report generation.The code is available at https://github.com/BioMedIA-MBZUAI/TiBiX.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13343v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                An Entropy-based Text Watermarking Detection Method
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Currently, text watermarking algorithms for large language models (LLMs) can embed hidden features to texts generated by LLMs to facilitate subsequent detection, thus alleviating the problem of misuse of LLMs.Although the current text watermarking algorithms perform well in most high-entropy scenarios, its performance in low-entropy scenarios still needs to be improved.In this work, we proposed that the influence of token entropy should be fully considered in the watermark detection process, that is, the weight of each token should be adjusted according to its entropy during watermark detection, rather than setting the weight of all tokens to the same value as in previous methods.Specifically, we proposed an Entropy-based Watermark Detection (EWD) that gives higher-entropy tokens higher weights during watermark detection, so as to better reflect the degree of watermarking.Furthermore, the proposed detection process is training-free and fully automated.%In actual detection, we use a proxy-LLM to calculate the entropy of each token, without the need to use the original LLM.In the experiment, we found that our method can achieve better detection performance in low-entropy scenarios, and our method is also general and can be applied to texts with different entropy distributions.<span class='px-1 mx-1 bg-yellow-200'>Our code and data will be available online. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.808</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13485v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                eRST: A Signaled Graph Theory of Discourse Relations and Organization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this article we present Enhanced Rhetorical Structure Theory (eRST), a new theoretical framework for computational discourse analysis, based on an expansion of Rhetorical Structure Theory (RST).The framework encompasses discourse relation graphs with tree-breaking, nonprojective and concurrent relations, as well as implicit and explicit signals which give explainable rationales to our analyses.We survey shortcomings of RST and other existing frameworks, such as Segmented Discourse Representation Theory (SDRT), the Penn Discourse Treebank (PDTB) and Discourse Dependencies, and address these using constructs in the proposed theory.<span class='px-1 mx-1 bg-yellow-200'>We provide annotation, search and visualization tools for data, and present and evaluate a freely available corpus of English annotated according to our framework, encompassing 12 spoken and written genres with over 200K tokens. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.938</span></span>Finally, we discuss automatic parsing, evaluation metrics and applications for data in our framework.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13560v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Does Differentially Private Synthetic Data Lead to Synthetic Discoveries?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Background: Synthetic data has been proposed as a solution for sharing anonymized versions of sensitive biomedical datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.74</span></span>Ideally, synthetic data should preserve the structure and statistical properties of the original data, while protecting the privacy of the individual subjects.Differential privacy (DP) is currently considered the gold standard approach for balancing this trade-off.   Objectives:The aim of this study is to evaluate the Mann-Whitney U test on DP-synthetic biomedical data in terms of Type I and Type II errors, in order to establish whether statistical hypothesis testing performed on privacy preserving synthetic data is likely to lead to loss of test's validity or decreased power.   Methods: We evaluate the Mann-Whitney U test on DP-synthetic data generated from real-world data, including a prostate cancer dataset (n=500) and a cardiovascular dataset (n=70 000), as well as on data drawn from two Gaussian distributions.Five different DP-synthetic data generation methods are evaluated, including two basic DP histogram release methods and MWEM, Private-PGM, and DP GAN algorithms.   Conclusion: Most of the tested DP-synthetic data generation methods showed inflated Type I error, especially at privacy budget levels of $\epsilon\leq 1$.This result calls for caution when releasing and analyzing DP-synthetic data: low p-values may be obtained in statistical tests simply as a byproduct of the noise added to protect privacy.A DP smoothed histogram-based synthetic data generation method was shown to produce valid Type I error for all privacy levels tested but required a large original dataset size and a modest privacy budget ($\epsilon\geq 5$) in order to have reasonable Type II error levels.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13612v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Choreographers determine what the dances look like, while cameramen determine the final presentation of dances.Recently, various methods and datasets have showcased the feasibility of dance synthesis.However, camera movement synthesis with music and dance remains an unsolved challenging problem due to the scarcity of paired data.<span class='px-1 mx-1 bg-yellow-200'>Thus, we present DCM, a new multi-modal 3D dataset, which for the first time combines camera movement with dance motion and music audio. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.737</span></span><span class='px-1 mx-1 bg-yellow-200'>This dataset encompasses 108 dance sequences (3.2 hours) of paired dance-camera-music data from the anime community, covering 4 music genres. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.941</span></span>With this dataset, we uncover that dance camera movement is multifaceted and human-centric, and possesses multiple influencing factors, making dance camera synthesis a more challenging task compared to camera or dance synthesis alone.To overcome these difficulties, we propose DanceCamera3D, a transformer-based diffusion model that incorporates a novel body attention loss and a condition separation strategy.For evaluation, we devise new metrics measuring camera movement quality, diversity, and dancer fidelity.Utilizing these metrics, we conduct extensive experiments on our DCM dataset, providing both quantitative and qualitative evidence showcasing the effectiveness of our DanceCamera3D model.Code and video demos are available at https://github.com/Carmenw1203/DanceCamera3D-Official.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13667v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Overview of Publicly Available Degradation Data Sets for Tasks within Prognostics and Health Management
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Central to the efficacy of prognostics and health management methods is the acquisition and analysis of degradation data, which encapsulates the evolving health condition of engineering systems over time.Degradation data serves as a rich source of information, offering invaluable insights into the underlying degradation processes, failure modes, and performance trends of engineering systems.<span class='px-1 mx-1 bg-yellow-200'>This paper provides an overview of publicly available degradation data sets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.807</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13694v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                When Cars meet Drones: Hyperbolic Federated Learning for Source-Free Domain Adaptation in Adverse Weather
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In Federated Learning (FL), multiple clients collaboratively train a global model without sharing private data.In semantic segmentation, the Federated source Free Domain Adaptation (FFreeDA) setting is of particular interest, where clients undergo unsupervised training after supervised pretraining at the server side.While few recent works address FL for autonomous vehicles, intrinsic real-world challenges such as the presence of adverse weather conditions and the existence of different autonomous agents are still unexplored.To bridge this gap, we address both problems and introduce a new federated semantic segmentation setting where both car and drone clients co-exist and collaborate.Specifically, we propose a novel approach for this setting which exploits a batch-norm weather-aware strategy to dynamically adapt the model to the different weather conditions, while hyperbolic space prototypes are used to align the heterogeneous client representations.<span class='px-1 mx-1 bg-yellow-200'>Finally, we introduce FLYAWARE, the first semantic segmentation dataset with adverse weather data for aerial vehicles. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.782</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13762v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Hierarchical NeuroSymbolic Approach for Action Quality Assessment
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Action quality assessment (AQA) applies computer vision to quantitatively assess the performance or execution of a human action.Current AQA approaches are end-to-end neural models, which lack transparency and tend to be biased because they are trained on subjective human judgements as ground-truth.To address these issues, we introduce a neuro-symbolic paradigm for AQA, which uses neural networks to abstract interpretable symbols from video data and makes quality assessments by applying rules to those symbols.We take diving as the case study.We found that domain experts prefer our system and find it more informative than purely neural approaches to AQA in diving.Our system also achieves state-of-the-art action recognition and temporal segmentation, and automatically generates a detailed report that breaks the dive down into its elements and provides objective scoring with visual evidence.As verified by a group of domain experts, this report may be used to assist judges in scoring, help train judges, and provide feedback to divers.<span class='px-1 mx-1 bg-yellow-200'>We will open-source all of our annotated training data and code for ease of reproducibility. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.775</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13798v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Bounding Box Stability against Feature Dropout Reflects Detector Generalization across Environments
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Bounding boxes uniquely characterize object detection, where a good detector gives accurate bounding boxes of categories of interest.However, in the real-world where test ground truths are not provided, it is non-trivial to find out whether bounding boxes are accurate, thus preventing us from assessing the detector generalization ability.In this work, we find under feature map dropout, good detectors tend to output bounding boxes whose locations do not change much, while bounding boxes of poor detectors will undergo noticeable position changes.We compute the box stability score (BoS score) to reflect this stability.Specifically, given an image, we compute a normal set of bounding boxes and a second set after feature map dropout.To obtain BoS score, we use bipartite matching to find the corresponding boxes between the two sets and compute the average Intersection over Union (IoU) across the entire test set.We contribute to finding that BoS score has a strong, positive correlation with detection accuracy measured by mean average precision (mAP) under various test environments.This relationship allows us to predict the accuracy of detectors on various real-world test sets without accessing test ground truths, verified on canonical detection tasks such as vehicle detection and pedestrian detection.<span class='px-1 mx-1 bg-yellow-200'>Code and data are available at https://github.com/YangYangGirl/BoS. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.75</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13803v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Data Quality</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ChatGPT Rates Natural Language Explanation Quality Like Humans: But on Which Scales?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As AI becomes more integral in our lives, the need for transparency and responsibility grows.While natural language explanations (NLEs) are vital for clarifying the reasoning behind AI decisions, evaluating them through human judgments is complex and resource-intensive due to subjectivity and the need for fine-grained ratings.This study explores the alignment between ChatGPT and human assessments across multiple scales (i.e., binary, ternary, and 7-Likert scale).<span class='px-1 mx-1 bg-yellow-200'>We sample 300 data instances from three NLE datasets and collect 900 human annotations for both informativeness and clarity scores as the text quality measurement. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.607</span></span>We further conduct paired comparison experiments under different ranges of subjectivity scores, where the baseline comes from 8,346 human annotations.Our results show that ChatGPT aligns better with humans in more coarse-grained scales.Also, paired comparisons and dynamic prompting (i.e., providing semantically similar examples in the prompt) improve the alignment.This research advances our understanding of large language models' capabilities to assess the text explanation quality in different configurations for responsible AI development.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17368v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                "You are an expert annotator": Automatic Best-Worst-Scaling Annotations for Emotion Intensity Modeling
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Labeling corpora constitutes a bottleneck to create models for new tasks or domains.<span class='px-1 mx-1 bg-yellow-200'>Large language models mitigate the issue with automatic corpus labeling methods, particularly for categorical annotations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.665</span></span>Some NLP tasks such as emotion intensity prediction, however, require text regression, but there is no work on automating annotations for continuous label assignments.Regression is considered more challenging than classification: The fact that humans perform worse when tasked to choose values from a rating scale lead to comparative annotation methods, including best-worst scaling.This raises the question if large language model-based annotation methods show similar patterns, namely that they perform worse on rating scale annotation tasks than on comparative annotation tasks.To study this, we automate emotion intensity predictions and compare direct rating scale predictions, pairwise comparisons and best-worst scaling.We find that the latter shows the highest reliability.A transformer regressor fine-tuned on these data performs nearly on par with a model trained on the original manual annotations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17612v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Learning from Reduced Labels for Long-Tailed Data
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Long-tailed data is prevalent in real-world classification tasks and heavily relies on supervised information, which makes the annotation process exceptionally labor-intensive and time-consuming.<span class='px-1 mx-1 bg-yellow-200'>Unfortunately, despite being a common approach to mitigate labeling costs, existing weakly supervised learning methods struggle to adequately preserve supervised information for tail samples, resulting in a decline in accuracy for the tail classes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.608</span></span><span class='px-1 mx-1 bg-yellow-200'>To alleviate this problem, we introduce a novel weakly supervised labeling setting called Reduced Label. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.705</span></span>The proposed labeling setting not only avoids the decline of supervised information for the tail samples, but also decreases the labeling costs associated with long-tailed data.Additionally, we propose an straightforward and highly efficient unbiased framework with strong theoretical guarantees to learn from these Reduced Labels.Extensive experiments conducted on benchmark datasets including ImageNet validate the effectiveness of our approach, surpassing the performance of state-of-the-art weakly supervised methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16469v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Determined Multi-Label Learning via Similarity-Based Prompt
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In multi-label classification, each training instance is associated with multiple class labels simultaneously.Unfortunately, collecting the fully precise class labels for each training instance is time- and labor-consuming for real-world applications.To alleviate this problem, a novel labeling setting termed \textit{Determined Multi-Label Learning} (DMLL) is proposed, aiming to effectively alleviate the labeling cost inherent in multi-label tasks.In this novel labeling setting, each training instance is associated with a \textit{determined label} (either "Yes" or "No"), which indicates whether the training instance contains the provided class label.The provided class label is randomly and uniformly selected from the whole candidate labels set.Besides, each training instance only need to be determined once, which significantly reduce the annotation cost of the labeling task for multi-label datasets.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we theoretically derive an risk-consistent estimator to learn a multi-label classifier from these determined-labeled training data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.716</span></span>Additionally, we introduce a similarity-based prompt learning method for the first time, which minimizes the risk-consistent loss of large-scale pre-trained models to learn a supplemental prompt with richer semantic information.Extensive experimental validation underscores the efficacy of our approach, demonstrating superior performance compared to existing state-of-the-art methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16482v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Federated Learning (FL) heavily depends on label quality for its performance.<span class='px-1 mx-1 bg-yellow-200'>However, the label distribution among individual clients is always both noisy and heterogeneous. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.607</span></span><span class='px-1 mx-1 bg-yellow-200'>The high loss incurred by client-specific samples in heterogeneous label noise poses challenges for distinguishing between client-specific and noisy label samples, impacting the effectiveness of existing label noise learning approaches. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.827</span></span>To tackle this issue, we propose FedFixer, where the personalized model is introduced to cooperate with the global model to effectively select clean client-specific samples.In the dual models, updating the personalized model solely at a local level can lead to overfitting on noisy data due to limited samples, consequently affecting both the local and global models' performance.To mitigate overfitting, we address this concern from two perspectives.<span class='px-1 mx-1 bg-yellow-200'>Firstly, we employ a confidence regularizer to alleviate the impact of unconfident predictions caused by label noise. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.688</span></span>Secondly, a distance regularizer is implemented to constrain the disparity between the personalized and global models.We validate the effectiveness of FedFixer through extensive experiments on benchmark datasets.<span class='px-1 mx-1 bg-yellow-200'>The results demonstrate that FedFixer can perform well in filtering noisy label samples on different clients, especially in highly heterogeneous label noise scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.689</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16561v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                HPL-ESS: Hybrid Pseudo-Labeling for Unsupervised Event-based Semantic Segmentation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Event-based semantic segmentation has gained popularity due to its capability to deal with scenarios under high-speed motion and extreme lighting conditions, which cannot be addressed by conventional RGB cameras.Since it is hard to annotate event data, previous approaches rely on event-to-image reconstruction to obtain pseudo labels for training.<span class='px-1 mx-1 bg-yellow-200'>However, this will inevitably introduce noise, and learning from noisy pseudo labels, especially when generated from a single source, may reinforce the errors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.817</span></span><span class='px-1 mx-1 bg-yellow-200'>This drawback is also called confirmation bias in pseudo-labeling. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.612</span></span>In this paper, we propose a novel hybrid pseudo-labeling framework for unsupervised event-based semantic segmentation, HPL-ESS, to alleviate the influence of noisy pseudo labels.In particular, we first employ a plain unsupervised domain adaptation framework as our baseline, which can generate a set of pseudo labels through self-training.Then, we incorporate offline event-to-image reconstruction into the framework, and obtain another set of pseudo labels by predicting segmentation maps on the reconstructed images.<span class='px-1 mx-1 bg-yellow-200'>A noisy label learning strategy is designed to mix the two sets of pseudo labels and enhance the quality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.797</span></span>Moreover, we propose a soft prototypical alignment module to further improve the consistency of target domain features.Extensive experiments show that our proposed method outperforms existing state-of-the-art methods by a large margin on the DSEC-Semantic dataset (+5.88% accuracy, +10.32% mIoU), which even surpasses several supervised methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16788v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                What Happens to a Dataset Transformed by a Projection-based Concept Removal Method?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We investigate the behavior of methods that use linear projections to remove information about a concept from a language representation, and we consider the question of what happens to a dataset transformed by such a method.A theoretical analysis and experiments on real-world and synthetic data show that these methods inject strong statistical dependencies into the transformed datasets.After applying such a method, the representation space is highly structured: in the transformed space, an instance tends to be located near instances of the opposite label.<span class='px-1 mx-1 bg-yellow-200'>As a consequence, the original labeling can in some cases be reconstructed by applying an anti-clustering method. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.638</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16142v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Hypothesis-Driven Deep Learning for Out of Distribution Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Predictions of opaque black-box systems are frequently deployed in high-stakes applications such as healthcare.For such applications, it is crucial to assess how models handle samples beyond the domain of training data.While several metrics and tests exist to detect out-of-distribution (OoD) data from in-distribution (InD) data to a deep neural network (DNN), their performance varies significantly across datasets, models, and tasks, which limits their practical use.In this paper, we propose a hypothesis-driven approach to quantify whether a new sample is InD or OoD.Given a trained DNN and some input, we first feed the input through the DNN and compute an ensemble of OoD metrics, which we term latent responses.We then formulate the OoD detection problem as a hypothesis test between latent responses of different groups, and use permutation-based resampling to infer the significance of the observed latent responses under a null hypothesis.We adapt our method to detect an unseen sample of bacteria to a trained deep learning model, and show that it reveals interpretable differences between InD and OoD latent responses.<span class='px-1 mx-1 bg-yellow-200'>Our work has implications for systematic novelty detection and informed decision-making from classifiers trained on a subset of labels. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.676</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14058v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Automatic detection of multimodal misinformation has gained a widespread attention recently. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.635</span></span>However, the potential of powerful Large Language Models (LLMs) for multimodal misinformation detection remains underexplored.Besides, how to teach LLMs to interpret multimodal misinformation in cost-effective and accessible way is still an open question.To address that, we propose MMIDR, a framework designed to teach LLMs in providing fluent and high-quality textual explanations for their decision-making process of multimodal misinformation.To convert multimodal misinformation into an appropriate instruction-following format, we present a data augmentation perspective and pipeline.This pipeline consists of a visual information processing module and an evidence retrieval module.Subsequently, we prompt the proprietary LLMs with processed contents to extract rationales for interpreting the authenticity of multimodal misinformation.Furthermore, we design an efficient knowledge distillation approach to distill the capability of proprietary LLMs in explaining multimodal misinformation into open-source LLMs.To explore several research questions regarding the performance of LLMs in multimodal misinformation detection tasks, we construct an instruction-following multimodal misinformation dataset and conduct comprehensive experiments.The experimental findings reveal that our MMIDR exhibits sufficient detection performance and possesses the capacity to provide compelling rationales to support its assessments.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14171v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SoftPatch: Unsupervised Anomaly Detection with Noisy Data
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Although mainstream unsupervised anomaly detection (AD) algorithms perform well in academic datasets, their performance is limited in practical application due to the ideal experimental setting of clean training data.Training with noisy data is an inevitable problem in real-world anomaly detection but is seldom discussed.<span class='px-1 mx-1 bg-yellow-200'>This paper considers label-level noise in image sensory anomaly detection for the first time. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.635</span></span>To solve this problem, we proposed a memory-based unsupervised AD method, SoftPatch, which efficiently denoises the data at the patch level.Noise discriminators are utilized to generate outlier scores for patch-level noise elimination before coreset construction.The scores are then stored in the memory bank to soften the anomaly detection boundary.Compared with existing methods, SoftPatch maintains a strong modeling ability of normal data and alleviates the overconfidence problem in coreset.Comprehensive experiments in various noise scenes demonstrate that SoftPatch outperforms the state-of-the-art AD methods on the MVTecAD and BTAD benchmarks and is comparable to those methods under the setting without noise.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14233v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Tackling Noisy Labels with Network Parameter Additive Decomposition
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Given data with noisy labels, over-parameterized deep networks suffer overfitting mislabeled data, resulting in poor generalization. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.65</span></span>The memorization effect of deep networks shows that although the networks have the ability to memorize all noisy data, they would first memorize clean training data, and then gradually memorize mislabeled training data.<span class='px-1 mx-1 bg-yellow-200'>A simple and effective method that exploits the memorization effect to combat noisy labels is early stopping. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.689</span></span>However, early stopping cannot distinguish the memorization of clean data and mislabeled data, resulting in the network still inevitably overfitting mislabeled data in the early training stage.In this paper, to decouple the memorization of clean data and mislabeled data, and further reduce the side effect of mislabeled data, we perform additive decomposition on network parameters.Namely, all parameters are additively decomposed into two groups, i.e., parameters $\mathbf{w}$ are decomposed as $\mathbf{w}=\bm{\sigma}+\bm{\gamma}$. Afterward, the parameters $\bm{\sigma}$ are considered to memorize clean data, while the parameters $\bm{\gamma}$ are considered to memorize mislabeled data.Benefiting from the memorization effect, the updates of the parameters $\bm{\sigma}$ are encouraged to fully memorize clean data in early training, and then discouraged with the increase of training epochs to reduce interference of mislabeled data.The updates of the parameters $\bm{\gamma}$ are the opposite.In testing, only the parameters $\bm{\sigma}$ are employed to enhance generalization.Extensive experiments on both simulated and real-world benchmarks confirm the superior performance of our method.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13241v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Unified Optimal Transport Framework for Cross-Modal Retrieval with Noisy Labels
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Cross-modal retrieval (CMR) aims to establish interaction between different modalities, among which supervised CMR is emerging due to its flexibility in learning semantic category discrimination.Despite the remarkable performance of previous supervised CMR methods, much of their success can be attributed to the well-annotated data.However, even for unimodal data, precise annotation is expensive and time-consuming, and it becomes more challenging with the multimodal scenario.In practice, massive multimodal data are collected from the Internet with coarse annotation, which inevitably introduces noisy labels.<span class='px-1 mx-1 bg-yellow-200'>Training with such misleading labels would bring two key challenges -- enforcing the multimodal samples to \emph{align incorrect semantics} and \emph{widen the heterogeneous gap}, resulting in poor retrieval performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.66</span></span>To tackle these challenges, this work proposes UOT-RCL, a Unified framework based on Optimal Transport (OT) for Robust Cross-modal Retrieval.First, we propose a semantic alignment based on partial OT to progressively correct the noisy labels, where a novel cross-modal consistent cost function is designed to blend different modalities and provide precise transport cost.Second, to narrow the discrepancy in multi-modal data, an OT-based relation alignment is proposed to infer the semantic-level cross-modal matching.Both of these two components leverage the inherent correlation among multi-modal data to facilitate effective cost function.The experiments on three widely-used cross-modal retrieval datasets demonstrate that our UOT-RCL surpasses the state-of-the-art approaches and significantly improves the robustness against noisy labels.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13480v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Have You Poisoned My Data? Defending Neural Networks against Data Poisoning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The unprecedented availability of training data fueled the rapid development of powerful neural networks in recent years.However, the need for such large amounts of data leads to potential threats such as poisoning attacks: adversarial manipulations of the training data aimed at compromising the learned model to achieve a given adversarial goal.   <span class='px-1 mx-1 bg-yellow-200'>This paper investigates defenses against clean-label poisoning attacks and proposes a novel approach to detect and filter poisoned datapoints in the transfer learning setting. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.679</span></span>We define a new characteristic vector representation of datapoints and show that it effectively captures the intrinsic properties of the data distribution.Through experimental analysis, we demonstrate that effective poisons can be successfully differentiated from clean points in the characteristic vector space.We thoroughly evaluate our proposed approach and compare it to existing state-of-the-art defenses using multiple architectures, datasets, and poison budgets.Our evaluation shows that our proposal outperforms existing approaches in defense rate and final trained model performance across all experimental settings.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13523v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Graph Partial Label Learning with Potential Cause Discovering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Graph Neural Networks (GNNs) have gained considerable attention for their potential in addressing challenges posed by complex graph-structured data in diverse domains.However, accurately annotating graph data for training is difficult due to the inherent complexity and interconnectedness of graphs.To tackle this issue, we propose a novel graph representation learning method that enables GNN models to effectively learn discriminative information even in the presence of noisy labels within the context of Partially Labeled Learning (PLL).PLL is a critical weakly supervised learning problem, where each training instance is associated with a set of candidate labels, including both the true label and additional noisy labels.Our approach leverages potential cause extraction to obtain graph data that exhibit a higher likelihood of possessing a causal relationship with the labels.<span class='px-1 mx-1 bg-yellow-200'>By incorporating auxiliary training based on the extracted graph data, our model can effectively filter out the noise contained in the labels. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.718</span></span>We support the rationale behind our approach with a series of theoretical analyses.Moreover, we conduct extensive evaluations and ablation studies on multiple datasets, demonstrating the superiority of our proposed method.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.11449v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Uncertainty-Aware Pseudo-Label Filtering for Source-Free Unsupervised Domain Adaptation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Source-free unsupervised domain adaptation (SFUDA) aims to enable the utilization of a pre-trained source model in an unlabeled target domain without access to source data.Self-training is a way to solve SFUDA, where confident target samples are iteratively selected as pseudo-labeled samples to guide target model learning.However, prior heuristic noisy pseudo-label filtering methods all involve introducing extra models, which are sensitive to model assumptions and may introduce additional errors or mislabeling.<span class='px-1 mx-1 bg-yellow-200'>In this work, we propose a method called Uncertainty-aware Pseudo-label-filtering Adaptation (UPA) to efficiently address this issue in a coarse-to-fine manner. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.634</span></span><span class='px-1 mx-1 bg-yellow-200'>Specially, we first introduce a sample selection module named Adaptive Pseudo-label Selection (APS), which is responsible for filtering noisy pseudo labels. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.601</span></span>The APS utilizes a simple sample uncertainty estimation method by aggregating knowledge from neighboring samples and confident samples are selected as clean pseudo-labeled.Additionally, we incorporate Class-Aware Contrastive Learning (CACL) to mitigate the memorization of pseudo-label noise by learning robust pair-wise representation supervised by pseudo labels.Through extensive experiments conducted on three widely used benchmarks, we demonstrate that our proposed method achieves competitive performance on par with state-of-the-art SFUDA methods.Code is available at https://github.com/chenxi52/UPA.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.11256v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Dial-insight: Fine-tuning Large Language Models with High-Quality Domain-Specific Data Preventing Capability Collapse
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The efficacy of large language models (LLMs) is heavily dependent on the quality of the underlying data, particularly within specialized domains.A common challenge when fine-tuning LLMs for domain-specific applications is the potential degradation of the model's generalization capabilities.To address these issues, we propose a two-stage approach for the construction of production prompts designed to yield high-quality data.This method involves the generation of a diverse array of prompts that encompass a broad spectrum of tasks and exhibit a rich variety of expressions.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, we introduce a cost-effective, multi-dimensional quality assessment framework to ensure the integrity of the generated labeling data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.739</span></span>Utilizing a dataset comprised of service provider and customer interactions from the real estate sector, we demonstrate a positive correlation between data quality and model performance.Notably, our findings indicate that the domain-specific proficiency of general LLMs can be enhanced through fine-tuning with data produced via our proposed method, without compromising their overall generalization abilities, even when exclusively domain-specific data is employed for fine-tuning.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.09167v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MGIC: A Multi-Label Gradient Inversion Attack based on Canny Edge Detection on Federated Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As a new distributed computing framework that can protect data privacy, federated learning (FL) has attracted more and more attention in recent years.It receives gradients from users to train the global model and releases the trained global model to working users.Nonetheless, the gradient inversion (GI) attack reflects the risk of privacy leakage in federated learning.Attackers only need to use gradients through hundreds of thousands of simple iterations to obtain relatively accurate private data stored on users' local devices.For this, some works propose simple but effective strategies to obtain user data under a single-label dataset.However, these strategies induce a satisfactory visual effect of the inversion image at the expense of higher time costs.Due to the semantic limitation of a single label, the image obtained by gradient inversion may have semantic errors.We present a novel gradient inversion strategy based on canny edge detection (MGIC) in both the multi-label and single-label datasets.<span class='px-1 mx-1 bg-yellow-200'>To reduce semantic errors caused by a single label, we add new convolution layers' blocks in the trained model to obtain the image's multi-label. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.691</span></span><span class='px-1 mx-1 bg-yellow-200'>Through multi-label representation, serious semantic errors in inversion images are reduced. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.657</span></span>Then, we analyze the impact of parameters on the difficulty of input image reconstruction and discuss how image multi-subjects affect the inversion performance.Our proposed strategy has better visual inversion image results than the most widely used ones, saving more than 78% of time costs in the ImageNet dataset.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.08284v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                From Weak to Strong Sound Event Labels using Adaptive Change-Point Detection and Active Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In this work we propose an audio recording segmentation method based on an adaptive change point detection (A-CPD) for machine guided weak label annotation of audio recording segments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.742</span></span>The goal is to maximize the amount of information gained about the temporal activation's of the target sounds.For each unlabeled audio recording, we use a prediction model to derive a probability curve used to guide annotation.The prediction model is initially pre-trained on available annotated sound event data with classes that are disjoint from the classes in the unlabeled dataset.The prediction model then gradually adapts to the annotations provided by the annotator in an active learning loop.<span class='px-1 mx-1 bg-yellow-200'>The queries used to guide the weak label annotator towards strong labels are derived using change point detection on these probabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.742</span></span>We show that it is possible to derive strong labels of high quality even with a limited annotation budget, and show favorable results for A-CPD when compared to two baseline query strategies.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.08525v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Verifix: Post-Training Correction to Improve Label Noise Robustness with Verified Samples
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Label corruption, where training samples have incorrect labels, can significantly degrade the performance of machine learning models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.883</span></span>This corruption often arises from non-expert labeling or adversarial attacks.Acquiring large, perfectly labeled datasets is costly, and retraining large models from scratch when a clean dataset becomes available is computationally expensive.<span class='px-1 mx-1 bg-yellow-200'>To address this challenge, we propose Post-Training Correction, a new paradigm that adjusts model parameters after initial training to mitigate label noise, eliminating the need for retraining. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.729</span></span>We introduce Verifix, a novel Singular Value Decomposition (SVD) based algorithm that leverages a small, verified dataset to correct the model weights using a single update.Verifix uses SVD to estimate a Clean Activation Space and then projects the model's weights onto this space to suppress activations corresponding to corrupted data.<span class='px-1 mx-1 bg-yellow-200'>We demonstrate Verifix's effectiveness on both synthetic and real-world label noise. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.624</span></span>Experiments on the CIFAR dataset with 25% synthetic corruption show 7.36% generalization improvements on average.Additionally, we observe generalization improvements of up to 2.63% on naturally corrupted datasets like WebVision1.0 and Clothing1M.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.08618v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Benchmarks</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Federated Learning (FL) emerged as a practical approach to training a model from decentralized data.The proliferation of FL led to the development of numerous FL algorithms and mechanisms.<span class='px-1 mx-1 bg-yellow-200'>Many prior efforts have given their primary focus on accuracy of those approaches, but there exists little understanding of other aspects such as computational overheads, performance and training stability, etc. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.642</span></span>To bridge this gap, we conduct extensive performance evaluation on several canonical FL algorithms (FedAvg, FedProx, FedYogi, FedAdam, SCAFFOLD, and FedDyn) by leveraging an open-source federated learning framework called Flame.<span class='px-1 mx-1 bg-yellow-200'>Our comprehensive measurement study reveals that no single algorithm works best across different performance metrics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.822</span></span><span class='px-1 mx-1 bg-yellow-200'>A few key observations are: (1) While some state-of-the-art algorithms achieve higher accuracy than others, they incur either higher computation overheads (FedDyn) or communication overheads (SCAFFOLD). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.622</span></span><span class='px-1 mx-1 bg-yellow-200'>(2) Recent algorithms present smaller standard deviation in accuracy across clients than FedAvg, indicating that the advanced algorithms' performances are stable. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.701</span></span>(3) However, algorithms such as FedDyn and SCAFFOLD are more prone to catastrophic failures without the support of additional techniques such as gradient clipping.<span class='px-1 mx-1 bg-yellow-200'>We hope that our empirical study can help the community to build best practices in evaluating FL algorithms. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.669</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17287v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Neural Multimodal Topic Modeling: A Comprehensive Evaluation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Neural topic models can successfully find coherent and diverse topics in textual data.However, they are limited in dealing with multimodal datasets (e.g., images and text).This paper presents the first systematic and comprehensive evaluation of multimodal topic modeling of documents containing both text and images.In the process, we propose two novel topic modeling solutions and two novel evaluation metrics.Overall, our evaluation on an unprecedented rich and diverse collection of datasets indicates that both of our models generate coherent and diverse topics.<span class='px-1 mx-1 bg-yellow-200'>Nevertheless, the extent to which one method outperforms the other depends on the metrics and dataset combinations, which suggests further exploration of hybrid solutions in the future. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.669</span></span>Notably, our succinct human evaluation aligns with the outcomes determined by our proposed metrics.This alignment not only reinforces the credibility of our metrics but also highlights the potential for their application in guiding future multimodal topic modeling endeavors.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17308v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MESIA: Understanding and Leveraging Supplementary Nature of Method-level Comments for Automatic Comment Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Code comments are important for developers in program comprehension.In scenarios of comprehending and reusing a method, developers expect code comments to provide supplementary information beyond the method signature.However, the extent of such supplementary information varies a lot in different code comments.In this paper, we raise the awareness of the supplementary nature of method-level comments and propose a new metric named MESIA (Mean Supplementary Information Amount) to assess the extent of supplementary information that a code comment can provide.With the MESIA metric, we conduct experiments on a popular code-comment dataset and three common types of neural approaches to generate method-level comments.<span class='px-1 mx-1 bg-yellow-200'>Our experimental results demonstrate the value of our proposed work with a number of findings. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.601</span></span>(1) Small-MESIA comments occupy around 20% of the dataset and mostly fall into only the WHAT comment category.(2) Being able to provide various kinds of essential information, large-MESIA comments in the dataset are difficult for existing neural approaches to generate.(3) We can improve the capability of existing neural approaches to generate large-MESIA comments by reducing the proportion of small-MESIA comments in the training set.(4) The retrained model can generate large-MESIA comments that convey essential meaningful supplementary information for methods in the small-MESIA test set, but will get a lower BLEU score in evaluation.These findings indicate that with good training data, auto-generated comments can sometimes even surpass human-written reference comments, and having no appropriate ground truth for evaluation is an issue that needs to be addressed by future work on automatic comment generation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17357v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Bridging Textual and Tabular Worlds for Fact Verification: A Lightweight, Attention-Based Model
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>FEVEROUS is a benchmark and research initiative focused on fact extraction and verification tasks involving unstructured text and structured tabular data.In FEVEROUS, existing works often rely on extensive preprocessing and utilize rule-based transformations of data, leading to potential context loss or misleading encodings.This paper introduces a simple yet powerful model that nullifies the need for modality conversion, thereby preserving the original evidence's context.By leveraging pre-trained models on diverse text and tabular datasets and by incorporating a lightweight attention-based mechanism, our approach efficiently exploits latent connections between different data types, thereby yielding comprehensive and reliable verdict predictions.The model's modular structure adeptly manages multi-modal information, ensuring the integrity and authenticity of the original evidence are uncompromised.<span class='px-1 mx-1 bg-yellow-200'>Comparative analyses reveal that our approach exhibits competitive performance, aligning itself closely with top-tier models on the FEVEROUS benchmark. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.764</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17361v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SSF3D: Strict Semi-Supervised 3D Object Detection with Switching Filter
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>SSF3D modified the semi-supervised 3D object detection (SS3DOD) framework, which designed specifically for point cloud data.Leveraging the characteristics of non-coincidence and weak correlation of target objects in point cloud, we adopt a strategy of retaining only the truth-determining pseudo labels and trimming the other fuzzy labels with points, instead of pursuing a balance between the quantity and quality of pseudo labels.Besides, we notice that changing the filter will make the model meet different distributed targets, which is beneficial to break the training bottleneck.Two mechanism are introduced to achieve above ideas: strict threshold and filter switching.<span class='px-1 mx-1 bg-yellow-200'>The experiments are conducted to analyze the effectiveness of above approaches and their impact on the overall performance of the system. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.712</span></span>Evaluating on the KITTI dataset, SSF3D exhibits superior performance compared to the current state-of-the-art methods.The code will be released here.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17390v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LM-Combiner: A Contextual Rewriting Model for Chinese Grammatical Error Correction
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Over-correction is a critical problem in Chinese grammatical error correction (CGEC) task.Recent work using model ensemble methods based on voting can effectively mitigate over-correction and improve the precision of the GEC system.However, these methods still require the output of several GEC systems and inevitably lead to reduced error recall.In this light, we propose the LM-Combiner, a rewriting model that can directly modify the over-correction of GEC system outputs without a model ensemble.Specifically, we train the model on an over-correction dataset constructed through the proposed K-fold cross inference method, which allows it to directly generate filtered sentences by combining the original and the over-corrected text.In the inference stage, we directly take the original sentences and the output results of other systems as input and then obtain the filtered sentences through LM-Combiner.<span class='px-1 mx-1 bg-yellow-200'>Experiments on the FCGEC dataset show that our proposed method effectively alleviates the over-correction of the original system (+18.2 Precision) while ensuring the error recall remains unchanged. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.604</span></span>Besides, we find that LM-Combiner still has a good rewriting performance even with small parameters and few training data, and thus can cost-effectively mitigate the over-correction of black-box GEC systems (e.g., ChatGPT).</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17413v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Incorporating Exponential Smoothing into MLP: A Simple but Effective Sequence Model
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Modeling long-range dependencies in sequential data is a crucial step in sequence learning.A recently developed model, the Structured State Space (S4), demonstrated significant effectiveness in modeling long-range sequences.However, It is unclear whether the success of S4 can be attributed to its intricate parameterization and HiPPO initialization or simply due to State Space Models (SSMs).To further investigate the potential of the deep SSMs, we start with exponential smoothing (ETS), a simple SSM, and propose a stacked architecture by directly incorporating it into an element-wise MLP.We augment simple ETS with additional parameters and complex field to reduce the inductive bias.<span class='px-1 mx-1 bg-yellow-200'>Despite increasing less than 1\% of parameters of element-wise MLP, our models achieve comparable results to S4 on the LRA benchmark. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.612</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17445v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SeNM-VAE: Semi-Supervised Noise Modeling with Hierarchical Variational Autoencoder
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The data bottleneck has emerged as a fundamental challenge in learning based image restoration methods.Researchers have attempted to generate synthesized training data using paired or unpaired samples to address this challenge.This study proposes SeNM-VAE, a semi-supervised noise modeling method that leverages both paired and unpaired datasets to generate realistic degraded data.Our approach is based on modeling the conditional distribution of degraded and clean images with a specially designed graphical model.Under the variational inference framework, we develop an objective function for handling both paired and unpaired data.We employ our method to generate paired training samples for real-world image denoising and super-resolution tasks.Our approach excels in the quality of synthetic degraded images compared to other unpaired and paired noise modeling methods.Furthermore, our approach demonstrates remarkable performance in downstream image restoration tasks, even with limited paired data.<span class='px-1 mx-1 bg-yellow-200'>With more paired data, our method achieves the best performance on the SIDD dataset. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.658</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17502v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Boosting Few-Shot Learning with Disentangled Self-Supervised Learning and Meta-Learning for Medical Image Classification
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Background and objective: Employing deep learning models in critical domains such as medical imaging poses challenges associated with the limited availability of training data.We present a strategy for improving the performance and generalization capabilities of models trained in low-data regimes.Methods: The proposed method starts with a pre-training phase, where features learned in a self-supervised learning setting are disentangled to improve the robustness of the representations for downstream tasks.We then introduce a meta-fine-tuning step, leveraging related classes between meta-training and meta-testing phases but varying the granularity level.This approach aims to enhance the model's generalization capabilities by exposing it to more challenging classification tasks during meta-training and evaluating it on easier tasks but holding greater clinical relevance during meta-testing.We demonstrate the effectiveness of the proposed approach through a series of experiments exploring several backbones, as well as diverse pre-training and fine-tuning schemes, on two distinct medical tasks, i.e., classification of prostate cancer aggressiveness from MRI data and classification of breast cancer malignity from microscopic images.Results:Our results indicate that the proposed approach consistently yields superior performance w.r.t. ablation experiments, maintaining competitiveness even when a distribution shift between training and evaluation data occurs.<span class='px-1 mx-1 bg-yellow-200'>Conclusion: Extensive experiments demonstrate the effectiveness and wide applicability of the proposed approach. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.638</span></span>We hope that this work will add another solution to the arsenal of addressing learning issues in data-scarce imaging domains.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17530v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Sparse Logistic Regression with High-order Features for Automatic Grammar Rule Extraction from Treebanks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Descriptive grammars are highly valuable, but writing them is time-consuming and difficult.Furthermore, while linguists typically use corpora to create them, grammar descriptions often lack quantitative data.As for formal grammars, they can be challenging to interpret.In this paper, we propose a new method to extract and explore significant fine-grained grammar patterns and potential syntactic grammar rules from treebanks, in order to create an easy-to-understand corpus-based grammar.More specifically, we extract descriptions and rules across different languages for two linguistic phenomena, agreement and word order, using a large search space and paying special attention to the ranking order of the extracted rules.For that, we use a linear classifier to extract the most salient features that predict the linguistic phenomena under study.<span class='px-1 mx-1 bg-yellow-200'>We associate statistical information to each rule, and we compare the ranking of the model's results to those of other quantitative and statistical measures. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.626</span></span>Our method captures both well-known and less well-known significant grammar rules in Spanish, French, and Wolof.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17534v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Deep functional multiple index models with an application to SER
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Speech Emotion Recognition (SER) plays a crucial role in advancing human-computer interaction and speech processing capabilities.We introduce a novel deep-learning architecture designed specifically for the functional data model known as the multiple-index functional model.Our key innovation lies in integrating adaptive basis layers and an automated data transformation search within the deep learning framework.Simulations for this new model show good performances.This allows us to extract features tailored for chunk-level SER, based on Mel Frequency Cepstral Coefficients (MFCCs).<span class='px-1 mx-1 bg-yellow-200'>We demonstrate the effectiveness of our approach on the benchmark IEMOCAP database, achieving good performance compared to existing methods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.692</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17562v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the emergence of pre-trained vision-language models like CLIP, how to adapt them to various downstream classification tasks has garnered significant attention in recent research.The adaptation strategies can be typically categorized into three paradigms: zero-shot adaptation, few-shot adaptation, and the recently-proposed training-free few-shot adaptation.Most existing approaches are tailored for a specific setting and can only cater to one or two of these paradigms.In this paper, we introduce a versatile adaptation approach that can effectively work under all three settings.Specifically, we propose the dual memory networks that comprise dynamic and static memory components.The static memory caches training data knowledge, enabling training-free few-shot adaptation, while the dynamic memory preserves historical test features online during the testing process, allowing for the exploration of additional data insights beyond the training set.This novel capability enhances model performance in the few-shot setting and enables model usability in the absence of training data.The two memory networks employ the same flexible memory interactive strategy, which can operate in a training-free mode and can be further enhanced by incorporating learnable projection layers.Our approach is tested across 11 datasets under the three task settings.<span class='px-1 mx-1 bg-yellow-200'>Remarkably, in the zero-shot scenario, it outperforms existing methods by over 3\% and even shows superior results against methods utilizing external training data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.622</span></span>Additionally, our method exhibits robust performance against natural distribution shifts.Codes are available at \url{https://github.com/YBZh/DMN}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17589v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Uncertainty-aware Distributional Offline Reinforcement Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Offline reinforcement learning (RL) presents distinct challenges as it relies solely on observational data.A central concern in this context is ensuring the safety of the learned policy by quantifying uncertainties associated with various actions and environmental stochasticity.Traditional approaches primarily emphasize mitigating epistemic uncertainty by learning risk-averse policies, often overlooking environmental stochasticity.In this study, we propose an uncertainty-aware distributional offline RL method to simultaneously address both epistemic uncertainty and environmental stochasticity.We propose a model-free offline RL algorithm capable of learning risk-averse policies and characterizing the entire distribution of discounted cumulative rewards, as opposed to merely maximizing the expected value of accumulated discounted returns.<span class='px-1 mx-1 bg-yellow-200'>Our method is rigorously evaluated through comprehensive experiments in both risk-sensitive and risk-neutral benchmarks, demonstrating its superior performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.753</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17646v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Generalising the maximum independent set algorithm via Boolean networks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>A simple greedy algorithm to find a maximal independent set (MIS) in a graph starts with the empty set and visits every vertex, adding it to the set if and only if none of its neighbours are already in the set.In this paper, we consider the generalisation of this MIS algorithm by letting it start with any set of vertices and we prove the hardness of many decision problems related to this generalisation.<span class='px-1 mx-1 bg-yellow-200'>Our results are based on two main strategies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span>Firstly, we view the MIS algorithm as a sequential update of a Boolean network, which we refer to as the MIS network, according to a permutation of the vertex set.The set of fixed points of the MIS network corresponds to the set of MIS of the graph.Our generalisation then consists in starting from any configuration and following a sequential update given by a word of vertices.Secondly, we introduce the concept of a colony of a graph, that is a set of vertices that is dominated by an independent set.Deciding whether a set of vertices is a colony is NP-complete; decision problems related to the MIS algorithm will be reduced from the Colony problem.We first show that deciding whether a configuration can reach all maximal independent sets is coNP-complete.Second, we consider so-called fixing words, that allow to reach a MIS for any initial configuration, and fixing permutations, which we call permises; deciding whether a permutation is fixing is coNP-complete.Third, we show that deciding whether a graph has a permis is coNP-hard.Finally, we generalise the MIS algorithm to digraphs.The algorithm then uses the so-called kernel network, whose fixed points are the kernels of the digraph.Deciding whether the kernel network of a given digraph is fixable is coNP-hard, even for digraphs that have a kernel.Alternatively, we introduce two fixable Boolean networks whose sets of fixed points contain all kernels.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17658v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Solution for the CVPR 2023 1st foundation model challenge-Track2
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this paper, we propose a solution for cross-modal transportation retrieval.Due to the cross-domain problem of traffic images, we divide the problem into two sub-tasks of pedestrian retrieval and vehicle retrieval through a simple strategy.In pedestrian retrieval tasks, we use IRRA as the base model and specifically design an Attribute Classification to mine the knowledge implied by attribute labels.More importantly, We use the strategy of Inclusion Relation Matching to make the image-text pairs with inclusion relation have similar representation in the feature space.For the vehicle retrieval task, we use BLIP as the base model.Since aligning the color attributes of vehicles is challenging, we introduce attribute-based object detection techniques to add color patch blocks to vehicle images for color data augmentation.This serves as strong prior information, helping the model perform the image-text alignment.At the same time, we incorporate labeled attributes into the image-text alignment loss to learn fine-grained alignment and prevent similar images and texts from being incorrectly separated.<span class='px-1 mx-1 bg-yellow-200'>Our approach ranked first in the final B-board test with a score of 70.9. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.643</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17702v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>To capture user preference, transformer models have been widely applied to model sequential user behavior data.The core of transformer architecture lies in the self-attention mechanism, which computes the pairwise attention scores in a sequence.Due to the permutation-equivariant nature, positional encoding is used to enhance the attention between token representations.In this setting, the pairwise attention scores can be derived by both semantic difference and positional difference.However, prior studies often model the two kinds of difference measurements in different ways, which potentially limits the expressive capacity of sequence modeling.To address this issue, this paper proposes a novel transformer variant with complex vector attention, named EulerFormer, which provides a unified theoretical framework to formulate both semantic difference and positional difference.The EulerFormer involves two key technical improvements.First, it employs a new transformation function for efficiently transforming the sequence tokens into polar-form complex vectors using Euler's formula, enabling the unified modeling of both semantic and positional information in a complex rotation form.Secondly, it develops a differential rotation mechanism, where the semantic rotation angles can be controlled by an adaptation function, enabling the adaptive integration of the semantic and positional information according to the semantic contexts.Furthermore, a phase contrastive learning task is proposed to improve the anisotropy of contextual representations in EulerFormer.Our theoretical framework possesses a high degree of completeness and generality.It is more robust to semantic variations and possesses moresuperior theoretical properties in principle.<span class='px-1 mx-1 bg-yellow-200'>Extensive experiments conducted on four public datasets demonstrate the effectiveness and efficiency of our approach. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.603</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17729v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating Prediction
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Cold-start rating prediction is a fundamental problem in recommender systems that has been extensively studied.Many methods have been proposed that exploit explicit relations among existing data, such as collaborative filtering, social recommendations and heterogeneous information network, to alleviate the data insufficiency issue for cold-start users and items.However, the explicit relations constructed based on data between different roles may be unreliable and irrelevant, which limits the performance ceiling of the specific recommendation task.Motivated by this, in this paper, we propose a flexible framework dubbed heterogeneous interaction rating network (HIRE).HIRE dose not solely rely on the pre-defined interaction pattern or the manually constructed heterogeneous information network.Instead, we devise a Heterogeneous Interaction Module (HIM) to jointly model the heterogeneous interactions and directly infer the important interactions via the observed data.In the experiments, we evaluate our model under three cold-start settings on three real-world datasets.<span class='px-1 mx-1 bg-yellow-200'>The experimental results show that HIRE outperforms other baselines by a large margin. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.668</span></span>Furthermore, we visualize the inferred interactions of HIRE to confirm the contribution of our model.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17740v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Graph Language Model (GLM): A new graph-based approach to detect social instabilities
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This scientific report presents a novel methodology for the early prediction of important political events using News datasets.The methodology leverages natural language processing, graph theory, clique analysis, and semantic relationships to uncover hidden predictive signals within the data.<span class='px-1 mx-1 bg-yellow-200'>Initially, we designed a preliminary version of the method and tested it on a few events. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.606</span></span>This analysis revealed limitations in the initial research phase.We then enhanced the model in two key ways: first, we added a filtration step to only consider politically relevant news before further processing; second, we adjusted the input features to make the alert system more sensitive to significant spikes in the data.After finalizing the improved methodology, we tested it on eleven events including US protests, the Ukraine war, and French protests.<span class='px-1 mx-1 bg-yellow-200'>Results demonstrate the superiority of our approach compared to baseline methods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.887</span></span>Through targeted refinements, our model can now provide earlier and more accurate predictions of major political events based on subtle patterns in news data.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17816v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                GPFL: A Gradient Projection-Based Client Selection Framework for Efficient Federated Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Federated learning client selection is crucial for determining participant clients while balancing model accuracy and communication efficiency.Existing methods have limitations in handling data heterogeneity, computational burdens, and independent client treatment.To address these challenges, we propose GPFL, which measures client value by comparing local and global descent directions.We also employ an Exploit-Explore mechanism to enhance performance.<span class='px-1 mx-1 bg-yellow-200'>Experimental results on FEMINST and CIFAR-10 datasets demonstrate that GPFL outperforms baselines in Non-IID scenarios, achieving over 9\% improvement in FEMINST test accuracy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.638</span></span>Moreover, GPFL exhibits shorter computation times through pre-selection and parameter reuse in federated learning.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17833v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ReMamber: Referring Image Segmentation with Mamba Twister
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Referring Image Segmentation (RIS) leveraging transformers has achieved great success on the interpretation of complex visual-language tasks.However, the quadratic computation cost makes it resource-consuming in capturing long-range visual-language dependencies.Fortunately, Mamba addresses this with efficient linear complexity in processing.However, directly applying Mamba to multi-modal interactions presents challenges, primarily due to inadequate channel interactions for the effective fusion of multi-modal data.In this paper, we propose ReMamber, a novel RIS architecture that integrates the power of Mamba with a multi-modal Mamba Twister block.The Mamba Twister explicitly models image-text interaction, and fuses textual and visual features through its unique channel and spatial twisting mechanism.<span class='px-1 mx-1 bg-yellow-200'>We achieve the state-of-the-art on three challenging benchmarks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.821</span></span>Moreover, we conduct thorough analyses of ReMamber and discuss other fusion designs using Mamba.These provide valuable perspectives for future research.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17839v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Sen2Fire: A Challenging Benchmark Dataset for Wildfire Detection using Sentinel Data
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Utilizing satellite imagery for wildfire detection presents substantial potential for practical applications.To advance the development of machine learning algorithms in this domain, our study introduces the \textit{Sen2Fire} dataset--a challenging satellite remote sensing dataset tailored for wildfire detection.This dataset is curated from Sentinel-2 multi-spectral data and Sentinel-5P aerosol product, comprising a total of 2466 image patches.Each patch has a size of 512$\times$512 pixels with 13 bands.Given the distinctive sensitivities of various wavebands to wildfire responses, our research focuses on optimizing wildfire detection by evaluating different wavebands and employing a combination of spectral indices, such as normalized burn ratio (NBR) and normalized difference vegetation index (NDVI).<span class='px-1 mx-1 bg-yellow-200'>The results suggest that, in contrast to using all bands for wildfire detection, selecting specific band combinations yields superior performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.645</span></span>Additionally, our study underscores the positive impact of integrating Sentinel-5 aerosol data for wildfire detection.The code and dataset are available online (https://zenodo.org/records/10881058).</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17884v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Compressed Multi-task embeddings for Data-Efficient Downstream training and inference in Earth Observation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As repositories of large scale data in earth observation (EO) have grown, so have transfer and storage costs for model training and inference, expending significant resources.We introduce Neural Embedding Compression (NEC), based on the transfer of compressed embeddings to data consumers instead of raw data.We adapt foundation models (FM) through learned neural compression to generate multi-task embeddings while navigating the tradeoff between compression rate and embedding utility.We update only a small fraction of the FM parameters (10%) for a short training period (1% of the iterations of pre-training).We evaluate NEC on two EO tasks: scene classification and semantic segmentation.<span class='px-1 mx-1 bg-yellow-200'>Compared with applying traditional compression to the raw data, NEC achieves similar accuracy with a 75% to 90% reduction in data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.631</span></span>Even at 99.7% compression, performance drops by only 5% on the scene classification task.Overall, NEC is a data-efficient yet performant approach for multi-task EO modelling.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17886v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Gestures play a key role in human communication.Recent methods for co-speech gesture generation, while managing to generate beat-aligned motions, struggle generating gestures that are semantically aligned with the utterance.Compared to beat gestures that align naturally to the audio signal, semantically coherent gestures require modeling the complex interactions between the language and human motion, and can be controlled by focusing on certain words.Therefore, we present ConvoFusion, a diffusion-based approach for multi-modal gesture synthesis, which can not only generate gestures based on multi-modal speech inputs, but can also facilitate controllability in gesture synthesis.Our method proposes two guidance objectives that allow the users to modulate the impact of different conditioning modalities (e.g. audio vs text) as well as to choose certain words to be emphasized during gesturing.Our method is versatile in that it can be trained either for generating monologue gestures or even the conversational gestures.To further advance the research on multi-party interactive gestures, the DnD Group Gesture dataset is released, which contains 6 hours of gesture data showing 5 people interacting with one another.<span class='px-1 mx-1 bg-yellow-200'>We compare our method with several recent works and demonstrate effectiveness of our method on a variety of tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.824</span></span>We urge the reader to watch our supplementary video at our website.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17936v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CLHA: A Simple yet Effective Contrastive Learning Framework for Human Alignment
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Reinforcement learning from human feedback (RLHF) is a crucial technique in aligning large language models (LLMs) with human preferences, ensuring these LLMs behave in beneficial and comprehensible ways to users.However, a longstanding challenge in human alignment techniques based on reinforcement learning lies in their inherent complexity and difficulty in training.To address this challenge, we present a simple yet effective Contrastive Learning Framework for Human Alignment (CLHA) to align LLMs with human preferences directly.CLHA employs a novel rescoring strategy to evaluate the noise within the data by considering its inherent quality and dynamically adjusting the training process.Simultaneously, CLHA utilizes pairwise contrastive loss and adaptive supervised fine-tuning loss to adaptively modify the likelihood of generating responses, ensuring enhanced alignment with human preferences.<span class='px-1 mx-1 bg-yellow-200'>Using advanced methods, CLHA surpasses other algorithms, showcasing superior performance in terms of reward model scores, automatic evaluations, and human assessments on the widely used ``Helpful and Harmless'' dataset. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.613</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16649v2' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric, Data, and Algorithm
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) are gaining increasing interests to improve clinical efficiency for medical diagnosis, owing to their unprecedented performance in modelling natural language.Ensuring the safe and reliable clinical applications, the evaluation of LLMs indeed becomes critical for better mitigating the potential risks, e.g., hallucinations.However, current evaluation methods heavily rely on labor-intensive human participation to achieve human-preferred judgements.To overcome this challenge, we propose an automatic evaluation paradigm tailored to assess the LLMs' capabilities in delivering clinical services, e.g., disease diagnosis and treatment.The evaluation paradigm contains three basic elements: metric, data, and algorithm.Specifically, inspired by professional clinical practice pathways, we formulate a LLM-specific clinical pathway (LCP) to define the clinical capabilities that a doctor agent should possess.Then, Standardized Patients (SPs) from the medical education are introduced as the guideline for collecting medical data for evaluation, which can well ensure the completeness of the evaluation procedure.Leveraging these steps, we develop a multi-agent framework to simulate the interactive environment between SPs and a doctor agent, which is equipped with a Retrieval-Augmented Evaluation (RAE) to determine whether the behaviors of a doctor agent are in accordance with LCP.The above paradigm can be extended to any similar clinical scenarios to automatically evaluate the LLMs' medical capabilities.<span class='px-1 mx-1 bg-yellow-200'>Applying such paradigm, we construct an evaluation benchmark in the field of urology, including a LCP, a SPs dataset, and an automated RAE. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.671</span></span>Extensive experiments are conducted to demonstrate the effectiveness of the proposed approach, providing more insights for LLMs' safe and reliable deployments in clinical practice.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16446v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                FedAC: A Adaptive Clustered Federated Learning Framework for Heterogeneous Data
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Clustered federated learning (CFL) is proposed to mitigate the performance deterioration stemming from data heterogeneity in federated learning (FL) by grouping similar clients for cluster-wise model training.However, current CFL methods struggle due to inadequate integration of global and intra-cluster knowledge and the absence of an efficient online model similarity metric, while treating the cluster count as a fixed hyperparameter limits flexibility and robustness.In this paper, we propose an adaptive CFL framework, named FedAC, which (1) efficiently integrates global knowledge into intra-cluster learning by decoupling neural networks and utilizing distinct aggregation methods for each submodule, significantly enhancing performance; (2) includes a costeffective online model similarity metric based on dimensionality reduction; (3) incorporates a cluster number fine-tuning module for improved adaptability and scalability in complex, heterogeneous environments.<span class='px-1 mx-1 bg-yellow-200'>Extensive experiments show that FedAC achieves superior empirical performance, increasing the test accuracy by around 1.82% and 12.67% on CIFAR-10 and CIFAR-100 datasets, respectively, under different non-IID settings compared to SOTA methods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.649</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16460v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Determined Multi-Label Learning via Similarity-Based Prompt
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In multi-label classification, each training instance is associated with multiple class labels simultaneously.Unfortunately, collecting the fully precise class labels for each training instance is time- and labor-consuming for real-world applications.To alleviate this problem, a novel labeling setting termed \textit{Determined Multi-Label Learning} (DMLL) is proposed, aiming to effectively alleviate the labeling cost inherent in multi-label tasks.In this novel labeling setting, each training instance is associated with a \textit{determined label} (either "Yes" or "No"), which indicates whether the training instance contains the provided class label.The provided class label is randomly and uniformly selected from the whole candidate labels set.Besides, each training instance only need to be determined once, which significantly reduce the annotation cost of the labeling task for multi-label datasets.In this paper, we theoretically derive an risk-consistent estimator to learn a multi-label classifier from these determined-labeled training data.Additionally, we introduce a similarity-based prompt learning method for the first time, which minimizes the risk-consistent loss of large-scale pre-trained models to learn a supplemental prompt with richer semantic information.<span class='px-1 mx-1 bg-yellow-200'>Extensive experimental validation underscores the efficacy of our approach, demonstrating superior performance compared to existing state-of-the-art methods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.857</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16482v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Spatially temporally distributed informative path planning for multi-robot systems
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper investigates the problem of informative path planning for a mobile robotic sensor network in spatially temporally distributed mapping.The robots are able to gather noisy measurements from an area of interest during their movements to build a Gaussian Process (GP) model of a spatio-temporal field.The model is then utilized to predict the spatio-temporal phenomenon at different points of interest.To spatially and temporally navigate the group of robots so that they can optimally acquire maximal information gains while their connectivity is preserved, we propose a novel multistep prediction informative path planning optimization strategy employing our newly defined local cost functions.By using the dual decomposition method, it is feasible and practical to effectively solve the optimization problem in a distributed manner.<span class='px-1 mx-1 bg-yellow-200'>The proposed method was validated through synthetic experiments utilizing real-world data sets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.623</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16489v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Two Algorithms for Computing Rational Univariate Representations of Zero-Dimensional Ideals with Parameters
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Two algorithms for computing the rational univariate representation of zero-dimensional ideals with parameters are presented in the paper.Different from the rational univariate representation of zero-dimensional ideals without parameters, the number of zeros of zero-dimensional ideals with parameters under various specializations is different, which leads to choosing and checking the separating element, the key to computing the rational univariate representation, is difficult.In order to pick out the separating element, by partitioning the parameter space we can ensure that under each branch the ideal has the same number of zeros.Subsequently with the help of the extended subresultant theorem for parametric cases, two ideas are given to conduct the further partition of parameter space for choosing and checking the separating element.Based on these, we give two algorithms for computing rational univariate representations of zero-dimensional ideals with parameters.Furthermore, the two algorithms have been implemented on the computer algebra system Singular.<span class='px-1 mx-1 bg-yellow-200'>Experimental data show that the second algorithm has the better performance in contrast to the first one. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.782</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16519v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Differentially Private Online Federated Learning with Correlated Noise
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We propose a novel differentially private algorithm for online federated learning that employs temporally correlated noise to improve the utility while ensuring the privacy of the continuously released models.To address challenges stemming from DP noise and local updates with streaming noniid data, we develop a perturbed iterate analysis to control the impact of the DP noise on the utility.Moreover, we demonstrate how the drift errors from local updates can be effectively managed under a quasi-strong convexity condition.Subject to an $(\epsilon, \delta)$-DP budget, we establish a dynamic regret bound over the entire time horizon that quantifies the impact of key parameters and the intensity of changes in dynamic environments.<span class='px-1 mx-1 bg-yellow-200'>Numerical experiments validate the efficacy of the proposed algorithm. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.776</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16542v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Accelerating Federated Learning by Selecting Beneficial Herd of Local Gradients
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Federated Learning (FL) is a distributed machine learning framework in communication network systems.However, the systems' Non-Independent and Identically Distributed (Non-IID) data negatively affect the convergence efficiency of the global model, since only a subset of these data samples are beneficial for model convergence.<span class='px-1 mx-1 bg-yellow-200'>In pursuit of this subset, a reliable approach involves determining a measure of validity to rank the samples within the dataset. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.725</span></span>In this paper, We propose the BHerd strategy which selects a beneficial herd of local gradients to accelerate the convergence of the FL model.Specifically, we map the distribution of the local dataset to the local gradients and use the Herding strategy to obtain a permutation of the set of gradients, where the more advanced gradients in the permutation are closer to the average of the set of gradients.These top portion of the gradients will be selected and sent to the server for global aggregation.We conduct experiments on different datasets, models and scenarios by building a prototype system, and experimental results demonstrate that our BHerd strategy is effective in selecting beneficial local gradients to mitigate the effects brought by the Non-IID dataset, thus accelerating model convergence.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16557v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Federated Learning (FL) heavily depends on label quality for its performance.However, the label distribution among individual clients is always both noisy and heterogeneous.The high loss incurred by client-specific samples in heterogeneous label noise poses challenges for distinguishing between client-specific and noisy label samples, impacting the effectiveness of existing label noise learning approaches.To tackle this issue, we propose FedFixer, where the personalized model is introduced to cooperate with the global model to effectively select clean client-specific samples.In the dual models, updating the personalized model solely at a local level can lead to overfitting on noisy data due to limited samples, consequently affecting both the local and global models' performance.To mitigate overfitting, we address this concern from two perspectives.Firstly, we employ a confidence regularizer to alleviate the impact of unconfident predictions caused by label noise.Secondly, a distance regularizer is implemented to constrain the disparity between the personalized and global models.<span class='px-1 mx-1 bg-yellow-200'>We validate the effectiveness of FedFixer through extensive experiments on benchmark datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.636</span></span>The results demonstrate that FedFixer can perform well in filtering noisy label samples on different clients, especially in highly heterogeneous label noise scenarios.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16561v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Graph Augmentation for Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Graph augmentation with contrastive learning has gained significant attention in the field of recommendation systems due to its ability to learn expressive user representations, even when labeled data is limited.However, directly applying existing GCL models to real-world recommendation environments poses challenges.There are two primary issues to address.Firstly, the lack of consideration for data noise in contrastive learning can result in noisy self-supervised signals, leading to degraded performance.Secondly, many existing GCL approaches rely on graph neural network (GNN) architectures, which can suffer from over-smoothing problems due to non-adaptive message passing.To address these challenges, we propose a principled framework called GraphAug.This framework introduces a robust data augmentor that generates denoised self-supervised signals, enhancing recommender systems.The GraphAug framework incorporates a graph information bottleneck (GIB)-regularized augmentation paradigm, which automatically distills informative self-supervision information and adaptively adjusts contrastive view generation.Through rigorous experimentation on real-world datasets, we thoroughly assessed the performance of our novel GraphAug model.<span class='px-1 mx-1 bg-yellow-200'>The outcomes consistently unveil its superiority over existing baseline methods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.772</span></span>The source code for our model is publicly available at: https://github.com/HKUDS/GraphAug.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16656v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Adaptive Frequency Bin Interval in FFT via Dense Sampling Factor $α$
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The Fast Fourier Transform (FFT) is a fundamental tool for signal analysis, widely used across various fields.However, traditional FFT methods encounter challenges in adjusting the frequency bin interval, which may impede accurate spectral analysis.In this study, we propose a method for adjusting the frequency bin interval in FFT by introducing a parameter $\alpha$. We elucidate the underlying principles of the proposed method and discuss its potential applications across various contexts.<span class='px-1 mx-1 bg-yellow-200'>Our findings suggest that the proposed method offers a promising approach to overcome the limitations of traditional FFT methods and enhance spectral analysis accuracy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.612</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16665v2' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A General and Efficient Federated Split Learning with Pre-trained Image Transformers for Heterogeneous Data
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Federated Split Learning (FSL) is a promising distributed learning paradigm in practice, which gathers the strengths of both Federated Learning (FL) and Split Learning (SL) paradigms, to ensure model privacy while diminishing the resource overhead of each client, especially on large transformer models in a resource-constrained environment, e.g., Internet of Things (IoT).However, almost all works merely investigate the performance with simple neural network models in FSL.Despite the minor efforts focusing on incorporating Vision Transformers (ViT) as model architectures, they train ViT from scratch, thereby leading to enormous training overhead in each device with limited resources.Therefore, in this paper, we harness Pre-trained Image Transformers (PITs) as the initial model, coined FES-PIT, to accelerate the training process and improve model robustness.Furthermore, we propose FES-PTZO to hinder the gradient inversion attack, especially having the capability compatible with black-box scenarios, where the gradient information is unavailable.Concretely, FES-PTZO approximates the server gradient by utilizing a zeroth-order (ZO) optimization, which replaces the backward propagation with just one forward process.<span class='px-1 mx-1 bg-yellow-200'>Empirically, we are the first to provide a systematic evaluation of FSL methods with PITs in real-world datasets, different partial device participations, and heterogeneous data splits. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.603</span></span><span class='px-1 mx-1 bg-yellow-200'>Our experiments verify the effectiveness of our algorithms. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.705</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16050v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic Human Modeling
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>High-quality human reconstruction and photo-realistic rendering of a dynamic scene is a long-standing problem in computer vision and graphics.Despite considerable efforts invested in developing various capture systems and reconstruction algorithms, recent advancements still struggle with loose or oversized clothing and overly complex poses.In part, this is due to the challenges of acquiring high-quality human datasets.To facilitate the development of these fields, in this paper, we present PKU-DyMVHumans, a versatile human-centric dataset for high-fidelity reconstruction and rendering of dynamic human scenarios from dense multi-view videos.It comprises 8.2 million frames captured by more than 56 synchronized cameras across diverse scenarios.These sequences comprise 32 human subjects across 45 different scenarios, each with a high-detailed appearance and realistic human motion.Inspired by recent advancements in neural radiance field (NeRF)-based scene representations, we carefully set up an off-the-shelf framework that is easy to provide those state-of-the-art NeRF-based implementations and benchmark on PKU-DyMVHumans dataset.It is paving the way for various applications like fine-grained foreground/background decomposition, high-quality human reconstruction and photo-realistic novel view synthesis of a dynamic scene.<span class='px-1 mx-1 bg-yellow-200'>Extensive studies are performed on the benchmark, demonstrating new observations and challenges that emerge from using such high-fidelity dynamic data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.672</span></span>The dataset is available at: https://pku-dymvhumans.github.io.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16080v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing Visual Continual Learning with Language-Guided Supervision
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Continual learning (CL) aims to empower models to learn new tasks without forgetting previously acquired knowledge.Most prior works concentrate on the techniques of architectures, replay data, regularization, \etc.However, the category name of each class is largely neglected.Existing methods commonly utilize the one-hot labels and randomly initialize the classifier head.We argue that the scarce semantic information conveyed by the one-hot labels hampers the effective knowledge transfer across tasks.In this paper, we revisit the role of the classifier head within the CL paradigm and replace the classifier with semantic knowledge from pretrained language models (PLMs).Specifically, we use PLMs to generate semantic targets for each class, which are frozen and serve as supervision signals during training.Such targets fully consider the semantic correlation between all classes across tasks.Empirical studies show that our approach mitigates forgetting by alleviating representation drifting and facilitating knowledge transfer across tasks.<span class='px-1 mx-1 bg-yellow-200'>The proposed method is simple to implement and can seamlessly be plugged into existing methods with negligible adjustments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.604</span></span>Extensive experiments based on eleven mainstream baselines demonstrate the effectiveness and generalizability of our approach to various protocols.For example, under the class-incremental learning setting on ImageNet-100, our method significantly improves the Top-1 accuracy by 3.2\% to 6.1\% while reducing the forgetting rate by 2.6\% to 13.1\%.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16124v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Complementary Recommendation in E-commerce: Definition, Approaches, and Future Directions
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In recent years, complementary recommendation has received extensive attention in the e-commerce domain.In this paper, we comprehensively summarize and compare 34 representative studies conducted between 2009 and 2024.Firstly, we compare the data and methods used for modeling complementary relationships between products, including simple complementarity and more complex scenarios such as asymmetric complementarity, the coexistence of substitution and complementarity relationships between products, and varying degrees of complementarity between different pairs of products.Next, we classify and compare the models based on the research problems of complementary recommendation, such as diversity, personalization, and cold-start.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, we provide a comparative analysis of experimental results from different studies conducted on the same dataset, which helps identify the strengths and weaknesses of the research. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.701</span></span>Compared to previous surveys, this paper provides a more updated and comprehensive summary of the research, discusses future research directions, and contributes to the advancement of this field.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16135v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Fusion of Minutia Cylinder Codes and Minutia Patch Embeddings for Latent Fingerprint Recognition
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Latent fingerprints are one of the most widely used forensic evidence by law enforcement agencies.However, latent recognition performance is far from the exemplary performance of sensor fingerprint recognition due to deformations and artifacts within these images.In this study, we propose a fusion based local matching approach towards latent fingerprint recognition.Recent latent recognition studies typically relied on local descriptor generation methods, in which either handcrafted minutiae features or deep neural network features are extracted around a minutia of interest, in the latent recognition process.Proposed approach would integrate these handcrafted features with a recently proposed deep neural network embedding features in a multi-stage fusion approach to significantly improve latent recognition results.<span class='px-1 mx-1 bg-yellow-200'>Effectiveness of the proposed approach has been shown on several public and private data sets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.624</span></span><span class='px-1 mx-1 bg-yellow-200'>As demonstrated in our experimental results, proposed method improves rank-1 identification accuracy by considerably for real-world datasets when compared to either the single usage of these features or existing state-of-the-art methods in the literature. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.663</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16172v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Blur2Blur: Blur Conversion for Unsupervised Image Deblurring on Unknown Domains
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper presents an innovative framework designed to train an image deblurring algorithm tailored to a specific camera device.This algorithm works by transforming a blurry input image, which is challenging to deblur, into another blurry image that is more amenable to deblurring.The transformation process, from one blurry state to another, leverages unpaired data consisting of sharp and blurry images captured by the target camera device.Learning this blur-to-blur transformation is inherently simpler than direct blur-to-sharp conversion, as it primarily involves modifying blur patterns rather than the intricate task of reconstructing fine image details.<span class='px-1 mx-1 bg-yellow-200'>The efficacy of the proposed approach has been demonstrated through comprehensive experiments on various benchmarks, where it significantly outperforms state-of-the-art methods both quantitatively and qualitatively. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.871</span></span>Our code and data are available at https://zero1778.github.io/blur2blur/</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16205v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Thermal Analysis for NVIDIA GTX480 Fermi GPU Architecture
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this project, we design a four-layer (Silicon|TIM|Silicon|TIM), 3D floor plan for NVIDIA GTX480 Fermi GPU architecture and compare heat dissipation and power trends for matrix multiplication and Needleman-Wunsch kernels.First, cuda kernels for the two algorithms are written.These kernels are compiled and executed with the GPGPU Simulator to extract power logs for varying tensor sizes.These power logs are converted to ptrace files with an automation script written in Python.The 3D floor plan, along with the generated ptrace files are given to HotSpot, which generates thermal heat maps to show heat dissipation for various components of the Fermi architecture.These heat dissipation trends for both the kernels are observed for multiple tensor sizes to draw qualitative conclusions.The behavioral and execution patterns of both kernels are also observed with these varying heat dissipation trends.With this project, we observe that an increase in tensor size results in an increase of heat dissipation in components of the Fermi Architecture.However, the temperature of the chip remains saturated after a particular tensor size and remains constant thereafter.Heat dissipation is non-uniform with smaller tensor sizes, and becomes more uniform after a certain tensor size.This means, that after a particular tensor size, more cores of the architecture get activated in the computations, thereby resulting in an almost constant temperature.We also observe that Needleman Wunsch uses more data movement between DRAM and caches, thereby showing higher heat dissipation patterns in DRAMs when compared to Matrix multiplication for the same tensor size.<span class='px-1 mx-1 bg-yellow-200'>Our observations are in accordance with the theoretical concepts behind the working of the two algorithms, thereby making our results consistent. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.699</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16239v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal Contrastive Learning via Local Token Unlearning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Multimodal contrastive learning has emerged as a powerful paradigm for building high-quality features using the complementary strengths of various data modalities.However, the open nature of such systems inadvertently increases the possibility of backdoor attacks.These attacks subtly embed malicious behaviors within the model during training, which can be activated by specific triggers in the inference phase, posing significant security risks.Despite existing countermeasures through fine-tuning that reduce the adverse impacts of such attacks, these defenses often degrade the clean accuracy and necessitate the construction of extensive clean training pairs.In this paper, we explore the possibility of a less-cost defense from the perspective of model unlearning, that is, whether the model can be made to quickly \textbf{u}nlearn \textbf{b}ackdoor \textbf{t}hreats (UBT) by constructing a small set of poisoned samples.Specifically, we strengthen the backdoor shortcuts to discover suspicious samples through overfitting training prioritized by weak similarity samples.Building on the initial identification of suspicious samples, we introduce an innovative token-based localized forgetting training regime.This technique specifically targets the poisoned aspects of the model, applying a focused effort to unlearn the backdoor associations and trying not to damage the integrity of the overall model.<span class='px-1 mx-1 bg-yellow-200'>Experimental results show that our method not only ensures a minimal success rate for attacks, but also preserves the model's high clean accuracy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.628</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16257v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Object Detectors in the Open Environment:Challenges, Solutions, and Outlook
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the emergence of foundation models, deep learning-based object detectors have shown practical usability in closed set scenarios.However, for real-world tasks, object detectors often operate in open environments, where crucial factors (\eg, data distribution, objective) that influence model learning are often changing.The dynamic and intricate nature of the open environment poses novel and formidable challenges to object detectors.Unfortunately, current research on object detectors in open environments lacks a comprehensive analysis of their distinctive characteristics, challenges, and corresponding solutions, which hinders their secure deployment in critical real-world scenarios.This paper aims to bridge this gap by conducting a comprehensive review and analysis of object detectors in open environments.We initially identified limitations of key structural components within the existing detection pipeline and propose the open environment object detector challenge framework that includes four quadrants (\ie, out-of-domain, out-of-category, robust learning, and incremental learning) based on the dimensions of the data / target changes.<span class='px-1 mx-1 bg-yellow-200'>For each quadrant of challenges in the proposed framework, we present a detailed description and systematic analysis of the overarching goals and core difficulties, systematically review the corresponding solutions, and benchmark their performance over multiple widely adopted datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.675</span></span>In addition, we engage in a discussion of open problems and potential avenues for future research.This paper aims to provide a fresh, comprehensive, and systematic understanding of the challenges and solutions associated with open-environment object detectors, thus catalyzing the development of more solid applications in real-world scenarios.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16271v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Developer Research</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MESIA: Understanding and Leveraging Supplementary Nature of Method-level Comments for Automatic Comment Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Code comments are important for developers in program comprehension. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.703</span></span><span class='px-1 mx-1 bg-yellow-200'>In scenarios of comprehending and reusing a method, developers expect code comments to provide supplementary information beyond the method signature. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.623</span></span>However, the extent of such supplementary information varies a lot in different code comments.In this paper, we raise the awareness of the supplementary nature of method-level comments and propose a new metric named MESIA (Mean Supplementary Information Amount) to assess the extent of supplementary information that a code comment can provide.With the MESIA metric, we conduct experiments on a popular code-comment dataset and three common types of neural approaches to generate method-level comments.Our experimental results demonstrate the value of our proposed work with a number of findings.(1) Small-MESIA comments occupy around 20% of the dataset and mostly fall into only the WHAT comment category.(2) Being able to provide various kinds of essential information, large-MESIA comments in the dataset are difficult for existing neural approaches to generate.(3) We can improve the capability of existing neural approaches to generate large-MESIA comments by reducing the proportion of small-MESIA comments in the training set.(4) The retrained model can generate large-MESIA comments that convey essential meaningful supplementary information for methods in the small-MESIA test set, but will get a lower BLEU score in evaluation.These findings indicate that with good training data, auto-generated comments can sometimes even surpass human-written reference comments, and having no appropriate ground truth for evaluation is an issue that needs to be addressed by future work on automatic comment generation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17357v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing Software Effort Estimation through Reinforcement Learning-based Project Management-Oriented Feature Selection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Purpose:The study aims to investigate the application of the data element market in software project management, focusing on improving effort estimation by addressing challenges faced by traditional methods.Design/methodology/approach: This study proposes a solution based on feature selection, utilizing the data element market and reinforcement learning-based algorithms to enhance the accuracy of software effort estimation.It explores the application of the MARLFS algorithm, customizing improvements to the algorithm and reward function.Findings:<span class='px-1 mx-1 bg-yellow-200'>This study demonstrates that the proposed approach achieves more precise estimation compared to traditional methods, leveraging feature selection to guide project management in software development. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.621</span></span>Originality/value: This study contributes to the field by offering a novel approach that combines the data element market, machine learning, and feature selection to improve software effort estimation, addressing limitations of traditional methods and providing insights for future research in project management.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16749v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have shown remarkable progress in automated code generation.Yet, incorporating LLM-based code generation into real-life software projects poses challenges, as the generated code may contain errors in API usage, class, data structure, or missing project-specific information.As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context.<span class='px-1 mx-1 bg-yellow-200'>To this end, this paper puts forward a novel approach, termed ProCoder, which iteratively refines the project-level code context for precise code generation, guided by the compiler feedback. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.615</span></span>In particular, ProCoder first leverages compiler techniques to identify a mismatch between the generated code and the project's context.It then iteratively aligns and fixes the identified errors using information extracted from the code repository.We integrate ProCoder with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation.Experimental results show that ProCoder significantly improves the vanilla LLMs by over 80% in generating code dependent on project context, and consistently outperforms the existing retrieval-based code generation baselines.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16792v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                FineWAVE: Fine-Grained Warning Verification of Bugs for Automated Static Analysis Tools
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The continual expansion of software size and complexity has led to an increased focus on reducing defects and bugs during development. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.653</span></span>Although Automated Static Analysis Tools (ASATs) offer help, in practice, the significant number of false positives can impede developers' productivity and confidence in the tools.Therefore, previous research efforts have explored learning-based methods to validate the reported warnings.Nevertheless, there are still some limitations.(1) The granularity of prior research is coarse, as it focuses on identifying either actionable warnings throughout extensive development histories or potential true warnings at the function level.These approaches lack specificity regarding individual bugs and warnings.(2) Machine learning-based approaches need much manual effort for feature engineering while existing deep learning-based approaches ignore key semantics between source code and warnings.(3) The small number of selected projects hinders the comprehensive evaluation of these approaches.In this paper, we proposed a fine-grained warning verification approach that is sensitive to bugs for improving the results of ASATs, namely \ourtool.Specifically, we design a novel LSTM-based model that captures both fine-grained semantics of source code and warnings from ASATs and highlights their correlations with cross-attention.To tackle the data scarcity of training and evaluation, we collected a large-scale dataset of 280,273 warnings, namely FineWA.It is ten times larger than the existing largest dataset.Then, we conducted extensive experiments on the dataset to evaluate FineWAVE.The experimental results demonstrate the effectiveness of our approach, with an F1-score of 97.79% for reducing false alarms and 67.06% for confirming actual warnings, which also significantly outperforms all baselines.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16032v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Code Revert Prediction with Graph Neural Networks: A Case Study at J.P. Morgan Chase
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Code revert prediction, a specialized form of software defect detection, aims to forecast or predict the likelihood of code changes being reverted or rolled back in software development.<span class='px-1 mx-1 bg-yellow-200'>This task is very important in practice because by identifying code changes that are more prone to being reverted, developers and project managers can proactively take measures to prevent issues, improve code quality, and optimize development processes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.734</span></span>However, compared to code defect detection, code revert prediction has been rarely studied in previous research.Additionally, many previous methods for code defect detection relied on independent features but ignored relationships between code scripts.Moreover, new challenges are introduced due to constraints in an industry setting such as company regulation, limited features and large-scale codebase.To overcome these limitations, this paper presents a systematic empirical study for code revert prediction that integrates the code import graph with code features.Different strategies to address anomalies and data imbalance have been implemented including graph neural networks with imbalance classification and anomaly detection.We conduct the experiments on real-world code commit data within J.P. Morgan Chase which is extremely imbalanced in order to make a comprehensive comparison of these different approaches for the code revert prediction problem.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.09507v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Log Summarisation for Defect Evolution Analysis
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Log analysis and monitoring are essential aspects in software maintenance and identifying defects.In particular, the temporal nature and vast size of log data leads to an interesting and important research question: How can logs be summarised and monitored over time?<span class='px-1 mx-1 bg-yellow-200'>While this has been a fundamental topic of research in the software engineering community, work has typically focused on heuristic-, syntax-, or static-based methods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.654</span></span>In this work, we suggest an online semantic-based clustering approach to error logs that dynamically updates the log clusters to enable monitoring code error life-cycles.We also introduce a novel metric to evaluate the performance of temporal log clusters.We test our system and evaluation metric with an industrial dataset and find that our solution outperforms similar systems.We hope that our work encourages further temporal exploration in defect datasets.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.08358v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Software Vulnerability and Functionality Assessment using LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>While code review is central to the software development process, it can be tedious and expensive to carry out. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.645</span></span>In this paper, we investigate whether and how Large Language Models (LLMs) can aid with code reviews.Our investigation focuses on two tasks that we argue are fundamental to good reviews: (i) flagging code with security vulnerabilities and (ii) performing software functionality validation, i.e., ensuring that code meets its intended functionality.To test performance on both tasks, we use zero-shot and chain-of-thought prompting to obtain final ``approve or reject'' recommendations.As data, we employ seminal code generation datasets (HumanEval and MBPP) along with expert-written code snippets with security vulnerabilities from the Common Weakness Enumeration (CWE).Our experiments consider a mixture of three proprietary models from OpenAI and smaller open-source LLMs.We find that the former outperforms the latter by a large margin.Motivated by promising results, we finally ask our models to provide detailed descriptions of security vulnerabilities.Results show that 36.7% of LLM-generated descriptions can be associated with true CWE vulnerabilities.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.08429v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Data Annotation Techniques</h2>
          </td>
        </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Causality Research</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Counterfactual Fairness through Transforming Data Orthogonal to Bias
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Machine learning models have shown exceptional prowess in solving complex issues across various domains.Nonetheless, these models can sometimes exhibit biased decision-making, leading to disparities in treatment across different groups.Despite the extensive research on fairness, the nuanced effects of multivariate and continuous sensitive variables on decision-making outcomes remain insufficiently studied.We introduce a novel data pre-processing algorithm, Orthogonal to Bias (OB), designed to remove the influence of a group of continuous sensitive variables, thereby facilitating counterfactual fairness in machine learning applications.<span class='px-1 mx-1 bg-yellow-200'>Our approach is grounded in the assumption of a jointly normal distribution within a structural causal model (SCM), proving that counterfactual fairness can be achieved by ensuring the data is uncorrelated with sensitive variables. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.729</span></span>The OB algorithm is model-agnostic, catering to a wide array of machine learning models and tasks, and includes a sparse variant to enhance numerical stability through regularization.Through empirical evaluation on simulated and real-world datasets - including the adult income and the COMPAS recidivism datasets - our methodology demonstrates its capacity to enable fairer outcomes without compromising accuracy.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17852v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Hierarchical Multi-label Classification for Fine-level Event Extraction from Aviation Accident Reports
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>A large volume of accident reports is recorded in the aviation domain, which greatly values improving aviation safety.To better use those reports, we need to understand the most important events or impact factors according to the accident reports.However, the increasing number of accident reports requires large efforts from domain experts to label those reports.<span class='px-1 mx-1 bg-yellow-200'>In order to make the labeling process more efficient, many researchers have started developing algorithms to identify the underlying events from accident reports automatically. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.554</span></span><span class='px-1 mx-1 bg-yellow-200'>This article argues that we can identify the events more accurately by leveraging the event taxonomy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.57</span></span>More specifically, we consider the problem a hierarchical classification task where we first identify the coarse-level information and then predict the fine-level information.We achieve this hierarchical classification process by incorporating a novel hierarchical attention module into BERT.To further utilize the information from event taxonomy, we regularize the proposed model according to the relationship and distribution among labels.The effectiveness of our framework is evaluated with the data collected by National Transportation Safety Board (NTSB).It has been shown that fine-level prediction accuracy is highly improved, and the regularization term can be beneficial to the rare event identification problem.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17914v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                EXPLORA: A teacher-apprentice methodology for eliciting natural child-computer interactions
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Investigating child-computer interactions within their contexts is vital for designing technology that caters to children's needs.However, determining what aspects of context are relevant for designing child-centric technology remains a challenge.We introduce EXPLORA, a multimodal, multistage online methodology comprising three pivotal stages: (1) building a teacher-apprentice relationship,(2) learning from child-teachers, and (3) assessing and reinforcing researcher-apprentice learning.Central to EXPLORA is the collection of attitudinal data through pre-observation interviews, offering researchers a deeper understanding of children's characteristics and contexts.<span class='px-1 mx-1 bg-yellow-200'>This informs subsequent online observations, allowing researchers to focus on frequent interactions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.519</span></span>Furthermore, researchers can validate preliminary assumptions with children.A means-ends analysis framework aids in the systematic analysis of data, shedding light on context, agency and homework-information searching processes children employ in their activities.To illustrate EXPLORA's capabilities, we present nine single case studies investigating Brazilian child-caregiver dyads' (children ages 9-11) use of technology in homework information-searching.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17264v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Skews in the Phenomenon Space Hinder Generalization in Text-to-Image Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The literature on text-to-image generation is plagued by issues of faithfully composing entities with relations.But there lacks a formal understanding of how entity-relation compositions can be effectively learned.Moreover, the underlying phenomenon space that meaningfully reflects the problem structure is not well-defined, leading to an arms race for larger quantities of data in the hope that generalization emerges out of large-scale pretraining.<span class='px-1 mx-1 bg-yellow-200'>We hypothesize that the underlying phenomenological coverage has not been proportionally scaled up, leading to a skew of the presented phenomenon which harms generalization. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.516</span></span>We introduce statistical metrics that quantify both the linguistic and visual skew of a dataset for relational learning, and show that generalization failures of text-to-image generation are a direct result of incomplete or unbalanced phenomenological coverage.We first perform experiments in a synthetic domain and demonstrate that systematically controlled metrics are strongly predictive of generalization performance.Then we move to natural images and show that simple distribution perturbations in light of our theories boost generalization without enlarging the absolute data size.This work informs an important direction towards quality-enhancing the data diversity or balance orthogonal to scaling up the absolute size.Our discussions point out important open questions on 1) Evaluation of generated entity-relation compositions, and 2) Better models for reasoning with abstract relations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16394v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Rethinking the Representation in Federated Unsupervised Learning with Non-IID Data
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Federated learning achieves effective performance in modeling decentralized data.In practice, client data are not well-labeled, which makes it potential for federated unsupervised learning (FUSL) with non-IID data.However, the performance of existing FUSL methods suffers from insufficient representations, i.e., (1) representation collapse entanglement among local and global models, and (2) inconsistent representation spaces among local models.The former indicates that representation collapse in local model will subsequently impact the global model and other local models.<span class='px-1 mx-1 bg-yellow-200'>The latter means that clients model data representation with inconsistent parameters due to the deficiency of supervision signals. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.506</span></span>In this work, we propose FedU2 which enhances generating uniform and unified representation in FUSL with non-IID data.Specifically, FedU2 consists of flexible uniform regularizer (FUR) and efficient unified aggregator (EUA).FUR in each client avoids representation collapse via dispersing samples uniformly, and EUA in server promotes unified representation by constraining consistent client model updating.To extensively validate the performance of FedU2, we conduct both cross-device and cross-silo evaluation experiments on two benchmark datasets, i.e., CIFAR10 and CIFAR100.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16398v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A comparative analysis of embedding models for patent similarity
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper makes two contributions to the field of text-based patent similarity.First, it compares the performance of different kinds of patent-specific pretrained embedding models, namely static word embeddings (such as word2vec and doc2vec models) and contextual word embeddings (such as transformers based models), on the task of patent similarity calculation.Second, it compares specifically the performance of Sentence Transformers (SBERT) architectures with different training phases on the patent similarity task.<span class='px-1 mx-1 bg-yellow-200'>To assess the models' performance, we use information about patent interferences, a phenomenon in which two or more patent claims belonging to different patent applications are proven to be overlapping by patent examiners. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.508</span></span>Therefore, we use these interferences cases as a proxy for maximum similarity between two patents, treating them as ground-truth to evaluate the performance of the different embedding models.Our results point out that, first, Patent SBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer architecture proposed in this research, outperforms the current state-of-the-art in patent similarity.Second, they show that, in some cases, large static models performances are still comparable to contextual ones when trained on extensive data; thus, we believe that the superiority in the performance of contextual embeddings may not be related to the actual architecture but rather to the way the training phase is performed.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16630v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                An early warning indicator trained on stochastic disease-spreading models with different noises
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The timely detection of disease outbreaks through reliable early warning signals (EWSs) is indispensable for effective public health mitigation strategies.Nevertheless, the intricate dynamics of real-world disease spread, often influenced by diverse sources of noise and limited data in the early stages of outbreaks, pose a significant challenge in developing reliable EWSs, as the performance of existing indicators varies with extrinsic and intrinsic noises.Here, we address the challenge of modeling disease when the measurements are corrupted by additive white noise, multiplicative environmental noise, and demographic noise into a standard epidemic mathematical model.To navigate the complexities introduced by these noise sources, we employ a deep learning algorithm that provides EWS in infectious disease outbreak by training on noise-induced disease-spreading models.The indicator's effectiveness is demonstrated through its application to real-world COVID-19 cases in Edmonton and simulated time series derived from diverse disease spread models affected by noise.<span class='px-1 mx-1 bg-yellow-200'>Notably, the indicator captures an impending transition in a time series of disease outbreaks and outperforms existing indicators. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.564</span></span>This study contributes to advancing early warning capabilities by addressing the intricate dynamics inherent in real-world disease spread, presenting a promising avenue for enhancing public health preparedness and response efforts.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16233v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Causal knowledge engineering: A case study from COVID-19
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>COVID-19 appeared abruptly in early 2020, requiring a rapid response amid a context of great uncertainty.<span class='px-1 mx-1 bg-yellow-200'>Good quality data and knowledge was initially lacking, and many early models had to be developed with causal assumptions and estimations built in to supplement limited data, often with no reliable approach for identifying, validating and documenting these causal assumptions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.757</span></span><span class='px-1 mx-1 bg-yellow-200'>Our team embarked on a knowledge engineering process to develop a causal knowledge base consisting of several causal BNs for diverse aspects of COVID-19. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.744</span></span><span class='px-1 mx-1 bg-yellow-200'>The unique challenges of the setting lead to experiments with the elicitation approach, and what emerged was a knowledge engineering method we call Causal Knowledge Engineering (CKE). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.732</span></span><span class='px-1 mx-1 bg-yellow-200'>The CKE provides a structured approach for building a causal knowledge base that can support the development of a variety of application-specific models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.752</span></span>Here we describe the CKE method, and use our COVID-19 work as a case study to provide a detailed discussion and analysis of the method.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14100v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Existence Is Chaos: Enhancing 3D Human Motion Prediction with Uncertainty Consideration
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Human motion prediction is consisting in forecasting future body poses from historically observed sequences.It is a longstanding challenge due to motion's complex dynamics and uncertainty.Existing methods focus on building up complicated neural networks to model the motion dynamics.The predicted results are required to be strictly similar to the training samples with L2 loss in current training pipeline.However, little attention has been paid to the uncertainty property which is crucial to the prediction task.<span class='px-1 mx-1 bg-yellow-200'>We argue that the recorded motion in training data could be an observation of possible future, rather than a predetermined result. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.546</span></span>In addition, existing works calculate the predicted error on each future frame equally during training, while recent work indicated that different frames could play different roles.In this work, a novel computationally efficient encoder-decoder model with uncertainty consideration is proposed, which could learn proper characteristics for future frames by a dynamic function.Experimental results on benchmark datasets demonstrate that our uncertainty consideration approach has obvious advantages both in quantity and quality.Moreover, the proposed method could produce motion sequences with much better quality that avoids the intractable shaking artefacts.We believe our work could provide a novel perspective to consider the uncertainty quality for the general motion prediction task and encourage the studies in this field.The code will be available in https://github.com/Motionpre/Adaptive-Salient-Loss-SAGGB.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14104v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Stitching for Neuroevolution: Recombining Deep Neural Networks without Breaking Them
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Traditional approaches to neuroevolution often start from scratch.This becomes prohibitively expensive in terms of computational and data requirements when targeting modern, deep neural networks.Using a warm start could be highly advantageous, e.g., using previously trained networks, potentially from different sources.This moreover enables leveraging the benefits of transfer learning (in particular vastly reduced training effort).However, recombining trained networks is non-trivial because architectures and feature representations typically differ.Consequently, a straightforward exchange of layers tends to lead to a performance breakdown.<span class='px-1 mx-1 bg-yellow-200'>We overcome this by matching the layers of parent networks based on their connectivity, identifying potential crossover points. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.524</span></span>To correct for differing feature representations between these layers we employ stitching, which merges the networks by introducing new layers at crossover points.To train the merged network, only stitching layers need to be considered.New networks can then be created by selecting a subnetwork by choosing which stitching layers to (not) use.Assessing their performance is efficient as only their evaluation on data is required.We experimentally show that our approach enables finding networks that represent novel trade-offs between performance and computational cost, with some even dominating the original networks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14224v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Impact Assessment of Missing Data in Model Predictions for Earth Observation Applications
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Earth observation (EO) applications involving complex and heterogeneous data sources are commonly approached with machine learning models.<span class='px-1 mx-1 bg-yellow-200'>However, there is a common assumption that data sources will be persistently available. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.504</span></span>Different situations could affect the availability of EO sources, like noise, clouds, or satellite mission failures.In this work, we assess the impact of missing temporal and static EO sources in trained models across four datasets with classification and regression tasks.We compare the predictive quality of different methods and find that some are naturally more robust to missing data.The Ensemble strategy, in particular, achieves a prediction robustness up to 100%.We evidence that missing scenarios are significantly more challenging in regression than classification tasks.Finally, we find that the optical view is the most critical view when it is missing individually.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14297v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Investigating the validity of structure learning algorithms in identifying risk factors for intervention in patients with diabetes
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Diabetes, a pervasive and enduring health challenge, imposes significant global implications on health, financial healthcare systems, and societal well-being.<span class='px-1 mx-1 bg-yellow-200'>This study undertakes a comprehensive exploration of various structural learning algorithms to discern causal pathways amongst potential risk factors influencing diabetes progression. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.629</span></span><span class='px-1 mx-1 bg-yellow-200'>The methodology involves the application of these algorithms to relevant diabetes data, followed by the conversion of their output graphs into Causal Bayesian Networks (CBNs), enabling predictive analysis and the evaluation of discrepancies in the effect of hypothetical interventions within our context-specific case study.    <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.741</span></span>This study highlights the substantial impact of algorithm selection on intervention outcomes.<span class='px-1 mx-1 bg-yellow-200'>To consolidate insights from diverse algorithms, we employ a model-averaging technique that helps us obtain a unique causal model for diabetes derived from a varied set of structural learning algorithms. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.616</span></span>We also investigate how each of those individual graphs, as well as the average graph, compare to the structures elicited by a domain expert who categorised graph edges into high confidence, moderate, and low confidence types, leading into three individual graphs corresponding to the three levels of confidence.   <span class='px-1 mx-1 bg-yellow-200'>The resulting causal model and data are made available online, and serve as a valuable resource and a guide for informed decision-making by healthcare practitioners, offering a comprehensive understanding of the interactions between relevant risk factors and the effect of hypothetical interventions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.748</span></span>Therefore, this research not only contributes to the academic discussion on diabetes, but also provides practical guidance for healthcare professionals in developing efficient intervention and risk management strategies.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14327v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                History-Independent Concurrent Objects
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>A data structure is called history independent if its internal memory representation does not reveal the history of operations applied to it, only its current state.<span class='px-1 mx-1 bg-yellow-200'>In this paper we study history independence for concurrent data structures, and establish foundational possibility and impossibility results. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.626</span></span>We show that a large class of concurrent objects cannot be implemented from smaller base objects in a manner that is both wait-free and history independent; but if we settle for either lock-freedom instead of wait-freedom or for a weak notion of history independence, then at least one object in the class, multi-valued single-reader single-writer registers, can be implemented from smaller base objects, binary registers.   On the other hand, using large base objects, we give a strong possibility result in the form of a universal construction: an object with $s$ possible states can be implemented in a wait-free, history-independent manner from compare-and-swap base objects that each have $O(s + 2^n)$ possible memory states, where $n$ is the number of processes in the system.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14445v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A survey on Concept-based Approaches For Model Improvement
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The focus of recent research has shifted from merely increasing the Deep Neural Networks (DNNs) performance in various tasks to DNNs, which are more interpretable to humans.The field of eXplainable Artificial Intelligence (XAI) has observed various techniques, including saliency-based and concept-based approaches.Concept-based approaches explain the model's decisions in simple human understandable terms called Concepts.Concepts are human interpretable units of data and are the thinking ground of humans.<span class='px-1 mx-1 bg-yellow-200'>Explanations in terms of concepts enable detecting spurious correlations, inherent biases, or clever-hans. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.566</span></span>With the advent of concept-based explanations, there have been various concept representation methods and automatic concept discovery algorithms.Some recent methods use concepts for post-hoc model disentanglement evaluation, while others use them for ante-hoc training.The concept-based approaches are new, with many representations coming up, and there is very limited work on Concept-based Model improvement.We provide a systematic review and taxonomy of various concept representations and their discovery algorithms in DNNs, specifically in vision.We also provide details on concept-based model improvement literature, which is the first to survey concept-based model improvement methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14566v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Conceptualizing predictive conceptual model for unemployment rates in the implementation of Industry 4.0: Exploring machine learning techniques
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Although there are obstacles related to obtaining data, ensuring model precision, and upholding ethical standards, the advantages of utilizing machine learning to generate predictive models for unemployment rates in developing nations amid the implementation of Industry 4.0 (I4.0) are noteworthy.This research delves into the concept of utilizing machine learning techniques through a predictive conceptual model to understand and address factors that contribute to unemployment rates in developing nations during the implementation of I4.0.A thorough examination of the literature was carried out through a literature review to determine the economic and social factors that have an impact on the unemployment rates in developing nations.<span class='px-1 mx-1 bg-yellow-200'>The examination of the literature uncovered that considerable influence on unemployment rates in developing nations is attributed to elements such as economic growth, inflation, population increase, education levels, and technological progress. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.543</span></span>A predictive conceptual model was developed that indicates factors that contribute to unemployment in developing nations can be addressed by using techniques of machine learning like regression analysis and neural networks when adopting I4.0.The study's findings demonstrated the effectiveness of the proposed predictive conceptual model in accurately understanding and addressing unemployment rate factors within developing nations when deploying I4.0.The model serves a dual purpose of predicting future unemployment rates and tracking the advancement of reducing unemployment rates in emerging economies.By persistently conducting research and improvements, decision-makers and enterprises can employ these patterns to arrive at more knowledgeable judgments that can advance the growth of the economy, generation of employment, and alleviation of poverty specifically in emerging nations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13536v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                eRST: A Signaled Graph Theory of Discourse Relations and Organization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this article we present Enhanced Rhetorical Structure Theory (eRST), a new theoretical framework for computational discourse analysis, based on an expansion of Rhetorical Structure Theory (RST).<span class='px-1 mx-1 bg-yellow-200'>The framework encompasses discourse relation graphs with tree-breaking, nonprojective and concurrent relations, as well as implicit and explicit signals which give explainable rationales to our analyses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.554</span></span>We survey shortcomings of RST and other existing frameworks, such as Segmented Discourse Representation Theory (SDRT), the Penn Discourse Treebank (PDTB) and Discourse Dependencies, and address these using constructs in the proposed theory.We provide annotation, search and visualization tools for data, and present and evaluate a freely available corpus of English annotated according to our framework, encompassing 12 spoken and written genres with over 200K tokens.Finally, we discuss automatic parsing, evaluation metrics and applications for data in our framework.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13560v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AUD-TGN: Advancing Action Unit Detection with Temporal Convolution and GPT-2 in Wild Audiovisual Contexts
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Leveraging the synergy of both audio data and visual data is essential for understanding human emotions and behaviors, especially in in-the-wild setting.Traditional methods for integrating such multimodal information often stumble, leading to less-than-ideal outcomes in the task of facial action unit detection.To overcome these shortcomings, we propose a novel approach utilizing audio-visual multimodal data.This method enhances audio feature extraction by leveraging Mel Frequency Cepstral Coefficients (MFCC) and Log-Mel spectrogram features alongside a pre-trained VGGish network.Moreover, this paper adaptively captures fusion features across modalities by modeling the temporal relationships, and ultilizes a pre-trained GPT-2 model for sophisticated context-aware fusion of multimodal information.Our method notably improves the accuracy of AU detection by understanding the temporal and contextual nuances of the data, showcasing significant advancements in the comprehension of intricate scenarios.<span class='px-1 mx-1 bg-yellow-200'>These findings underscore the potential of integrating temporal dynamics and contextual interpretation, paving the way for future research endeavors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.513</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13678v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Principled Representation Learning from Videos for Reinforcement Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We study pre-training representations for decision-making using video data, which is abundantly available for tasks such as game agents and software testing.Even though significant empirical advances have been made on this problem, a theoretical understanding remains absent.We initiate the theoretical investigation into principled approaches for representation learning and focus on learning the latent state representations of the underlying MDP using video data.<span class='px-1 mx-1 bg-yellow-200'>We study two types of settings: one where there is iid noise in the observation, and a more challenging setting where there is also the presence of exogenous noise, which is non-iid noise that is temporally correlated, such as the motion of people or cars in the background. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.542</span></span>We study three commonly used approaches: autoencoding, temporal contrastive learning, and forward modeling.We prove upper bounds for temporal contrastive learning and forward modeling in the presence of only iid noise.We show that these approaches can learn the latent state and use it to do efficient downstream RL with polynomial sample complexity.When exogenous noise is also present, we establish a lower bound result showing that the sample complexity of learning from video data can be exponentially worse than learning from action-labeled trajectory data.This partially explains why reinforcement learning with video pre-training is hard.We evaluate these representational learning methods in two visual domains, yielding results that are consistent with our theoretical findings.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13765v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Large Collection of Model-generated Contradictory Responses for Consistency-aware Dialogue Systems
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Mitigating the generation of contradictory responses poses a substantial challenge in dialogue response generation.The quality and quantity of available contradictory response data play a vital role in suppressing these contradictions, offering two significant benefits.First, having access to large contradiction data enables a comprehensive examination of their characteristics.Second, data-driven methods to mitigate contradictions may be enhanced with large-scale contradiction data for training.Nevertheless, no attempt has been made to build an extensive collection of model-generated contradictory responses.In this paper, we build a large dataset of response generation models' contradictions for the first time.<span class='px-1 mx-1 bg-yellow-200'>Then, we acquire valuable insights into the characteristics of model-generated contradictions through an extensive analysis of the collected responses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.537</span></span>Lastly, we also demonstrate how this dataset substantially enhances the performance of data-driven contradiction suppression methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12500v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Has Approximate Machine Unlearning been evaluated properly? From Auditing to Side Effects
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The growing concerns surrounding data privacy and security have underscored the critical necessity for machine unlearning--aimed at fully removing data lineage from machine learning models.MLaaS providers expect this to be their ultimate safeguard for regulatory compliance.Despite its critical importance, the pace at which privacy communities have been developing and implementing strong methods to verify the effectiveness of machine unlearning has been disappointingly slow, with this vital area often receiving insufficient focus.This paper seeks to address this shortfall by introducing well-defined and effective metrics for black-box unlearning auditing tasks.<span class='px-1 mx-1 bg-yellow-200'>We transform the auditing challenge into a question of non-membership inference and develop efficient metrics for auditing. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.502</span></span>By relying exclusively on the original and unlearned models--eliminating the need to train additional shadow models--our approach simplifies the evaluation of unlearning at the individual data point level.Utilizing these metrics, we conduct an in-depth analysis of current approximate machine unlearning algorithms, identifying three key directions where these approaches fall short: utility, resilience, and equity.Our aim is that this work will greatly improve our understanding of approximate machine unlearning methods, taking a significant stride towards converting the theoretical right to data erasure into a auditable reality.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12830v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Neural Differential Algebraic Equations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Differential-Algebraic Equations (DAEs) describe the temporal evolution of systems that obey both differential and algebraic constraints.<span class='px-1 mx-1 bg-yellow-200'>Of particular interest are systems that contain implicit relationships between their components, such as conservation relationships. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.549</span></span>Here, we present Neural Differential-Algebraic Equations (NDAEs) suitable for data-driven modeling of DAEs.This methodology is built upon the concept of the Universal Differential Equation; that is, a model constructed as a system of Neural Ordinary Differential Equations informed by theory from particular science domains.In this work, we show that the proposed NDAEs abstraction is suitable for relevant system-theoretic data-driven modeling tasks.Presented examples include (i) the inverse problem of tank-manifold dynamics and (ii) discrepancy modeling of a network of pumps, tanks, and pipes.Our experiments demonstrate the proposed method's robustness to noise and extrapolation ability to (i) learn the behaviors of the system components and their interaction physics and (ii) disambiguate between data trends and mechanistic relationships contained in the system.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12938v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Training Survival Models using Scoring Rules
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Survival Analysis provides critical insights for partially incomplete time-to-event data in various domains. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.556</span></span>It is also an important example of probabilistic machine learning.The probabilistic nature of the predictions can be exploited by using (proper) scoring rules in the model fitting process instead of likelihood-based optimization.Our proposal does so in a generic manner and can be used for a variety of model classes.We establish different parametric and non-parametric sub-frameworks that allow different degrees of flexibility.Incorporated into neural networks, it leads to a computationally efficient and scalable optimization routine, yielding state-of-the-art predictive performance.Finally, we show that using our framework, we can recover various parametric models and demonstrate that optimization works equally well when compared to likelihood-based methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13150v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhanced Detection of Transdermal Alcohol Levels Using Hyperdimensional Computing on Embedded Devices
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Alcohol consumption has a significant impact on individuals' health, with even more pronounced consequences when consumption becomes excessive.One approach to promoting healthier drinking habits is implementing just-in-time interventions, where timely notifications indicating intoxication are sent during heavy drinking episodes.<span class='px-1 mx-1 bg-yellow-200'>However, the complexity or invasiveness of an intervention mechanism may deter an individual from using them in practice. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.533</span></span>Previous research tackled this challenge using collected motion data and conventional Machine Learning (ML) algorithms to classify heavy drinking episodes, but with impractical accuracy and computational efficiency for mobile devices.Consequently, we have elected to use Hyperdimensional Computing (HDC) to design a just-in-time intervention approach that is practical for smartphones, smart wearables, and IoT deployment.HDC is a framework that has proven results in processing real-time sensor data efficiently.This approach offers several advantages, including low latency, minimal power consumption, and high parallelism.We explore various HDC encoding designs and combine them with various HDC learning models to create an optimal and feasible approach for mobile devices.Our findings indicate an accuracy rate of 89\%, which represents a substantial 12\% improvement over the current state-of-the-art.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12323v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Methods for Generating Drift in Text Streams
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Systems and individuals produce data continuously.On the Internet, people share their knowledge, sentiments, and opinions, provide reviews about services and products, and so on.<span class='px-1 mx-1 bg-yellow-200'>Automatically learning from these textual data can provide insights to organizations and institutions, thus preventing financial impacts, for example. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.506</span></span>To learn from textual data over time, the machine learning system must account for concept drift.Concept drift is a frequent phenomenon in real-world datasets and corresponds to changes in data distribution over time.For instance, a concept drift occurs when sentiments change or a word's meaning is adjusted over time.Although concept drift is frequent in real-world applications, benchmark datasets with labeled drifts are rare in the literature.To bridge this gap, this paper provides four textual drift generation methods to ease the production of datasets with labeled drifts.These methods were applied to Yelp and Airbnb datasets and tested using incremental classifiers respecting the stream mining paradigm to evaluate their ability to recover from the drifts.Results show that all methods have their performance degraded right after the drifts, and the incremental SVM is the fastest to run and recover the previous performance levels regarding accuracy and Macro F1-Score.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12328v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle Decision-Making
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the advent of universal function approximators in the domain of reinforcement learning, the number of practical applications leveraging deep reinforcement learning (DRL) has exploded.Decision-making in automated driving tasks has emerged as a chief application among them, taking the sensor data or the higher-order kinematic variables as the input and providing a discrete choice or continuous control output.However, the black-box nature of the models presents an overwhelming limitation that restricts the real-world deployment of DRL in autonomous vehicles (AVs).Therefore, in this research work, we focus on the interpretability of an attention-based DRL framework.We use a continuous proximal policy optimization-based DRL algorithm as the baseline model and add a multi-head attention framework in an open-source AV simulation environment.<span class='px-1 mx-1 bg-yellow-200'>We provide some analytical techniques for discussing the interpretability of the trained models in terms of explainability and causality for spatial and temporal correlations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.578</span></span>We show that the weights in the first head encode the positions of the neighboring vehicles while the second head focuses on the leader vehicle exclusively.<span class='px-1 mx-1 bg-yellow-200'>Also, the ego vehicle's action is causally dependent on the vehicles in the target lane spatially and temporally. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.575</span></span>Through these findings, we reliably show that these techniques can help practitioners decipher the results of the DRL algorithms.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.11432v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Graph Partial Label Learning with Potential Cause Discovering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Graph Neural Networks (GNNs) have gained considerable attention for their potential in addressing challenges posed by complex graph-structured data in diverse domains.However, accurately annotating graph data for training is difficult due to the inherent complexity and interconnectedness of graphs.To tackle this issue, we propose a novel graph representation learning method that enables GNN models to effectively learn discriminative information even in the presence of noisy labels within the context of Partially Labeled Learning (PLL).PLL is a critical weakly supervised learning problem, where each training instance is associated with a set of candidate labels, including both the true label and additional noisy labels.<span class='px-1 mx-1 bg-yellow-200'>Our approach leverages potential cause extraction to obtain graph data that exhibit a higher likelihood of possessing a causal relationship with the labels. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.833</span></span>By incorporating auxiliary training based on the extracted graph data, our model can effectively filter out the noise contained in the labels.We support the rationale behind our approach with a series of theoretical analyses.Moreover, we conduct extensive evaluations and ablation studies on multiple datasets, demonstrating the superiority of our proposed method.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.11449v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SeisFusion: Constrained Diffusion Model with Input Guidance for 3D Seismic Data Interpolation and Reconstruction
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Geographical, physical, or economic constraints often result in missing traces within seismic data, making the reconstruction of complete seismic data a crucial step in seismic data processing.Traditional methods for seismic data reconstruction require the selection of multiple empirical parameters and struggle to handle large-scale continuous missing data.With the development of deep learning, various neural networks have demonstrated powerful reconstruction capabilities.However, these convolutional neural networks represent a point-to-point reconstruction approach that may not cover the entire distribution of the dataset.Consequently, when dealing with seismic data featuring complex missing patterns, such networks may experience varying degrees of performance degradation.In response to this challenge, we propose a novel diffusion model reconstruction framework tailored for 3D seismic data.<span class='px-1 mx-1 bg-yellow-200'>To constrain the results generated by the diffusion model, we introduce conditional supervision constraints into the diffusion model, constraining the generated data of the diffusion model based on the input data to be reconstructed. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.521</span></span>We introduce a 3D neural network architecture into the diffusion model, successfully extending the 2D diffusion model to 3D space.Additionally, we refine the model's generation process by incorporating missing data into the generation process, resulting in reconstructions with higher consistency.Through ablation studies determining optimal parameter values, our method exhibits superior reconstruction accuracy when applied to both field datasets and synthetic datasets, effectively addressing a wide range of complex missing patterns.Our implementation is available at https://github.com/WAL-l/SeisFusion.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.11482v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Expanding the Resolution Boundary of Outcome-Based Imperfect-Recall Abstraction in Games with Ordered Signals
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In the development of advanced Texas Hold'em AI systems, abstraction technology has garnered widespread attention due to its significant effect in simplifying game complexity.This study adopts a more specific model, the games of ordered signal, to describe Texas Hold'em-style games and optimizes this model to streamline its mathematical representation and broaden its applicability.By transitioning from a broad imperfect information game model to a game with ordered signals model, we have separated the previously intertwined infoset abstraction and action abstraction into independent signal abstraction and action abstraction.Importantly, this signal abstraction provides a mathematical framework for the hand abstraction task, which is emphatically discussed in this paper.Additionally, a novel common refinement principle is introduced, revealing the limit performance of hand abstraction algorithms.We introduce potential outcome isomorphism (POI) and pinpoint that it suffers from the issue of excessive abstraction.Futher, We demonstrate that POI serves as a common refinement for leading outcome-based hand abstraction algorithms, such as E[HS] and PA\&PAEMD.Consequently, excessive abstraction also inherently affects these algorithms, leading to suboptimal performance.<span class='px-1 mx-1 bg-yellow-200'>Our investigation reveals the omission of historical data as a primary contributor to excessive abstraction. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.523</span></span>To remedy this, we propose the K-Recall Outcome Isomorphism (KROI) to incorporate the missing information.Compared with POI, KROI more accurately mirrors lossless isomorphism (LI), the ground truth, offering enhanced signal abstraction resolution.Experimental results in the Numeral211 Hold'em indicate that strategies developed through KROI approximate the exploitability of those developed through LI more closely than those trained through POI.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.11486v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Transformer language models are neural networks used for a wide variety of tasks concerning natural language, including some that also require logical reasoning.However, a transformer model may easily learn spurious patterns in the data, short-circuiting actual reasoning.In this paper we investigate to what extent transformers can be trained to a) approximate reasoning in propositional logic while b) avoiding known reasoning shortcuts via spurious correlations in the training data.<span class='px-1 mx-1 bg-yellow-200'>To do so, we use a dataset with known spurious correlation between truth and e.g. the number of rules in the problem. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.524</span></span>We augment the data with proofs, and train two models: a generative transformer, WP-BART, trained on problems and their whole proofs, and a neuro-symbolic model, SIP-BART, trained on individual proof steps and combining the generative transformer model BART with a symbolic proof checker.We find that SIP-BART succeeds in avoiding reasoning shortcuts, while WP-BART does not.For SIP-BART, we then identify a few remaining reasoning errors, not previously described in the literature, arising from using a pre-trained language model.These are qualitatively analysed to create a taxonomy of four different types of additional pitfalls.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.11314v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Graph Neural Network based Double Machine Learning Estimator of Network Causal Effects
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Our paper addresses the challenge of inferring causal effects in social network data, characterized by complex interdependencies among individuals resulting in challenges such as non-independence of units, interference (where a unit's outcome is affected by neighbors' treatments), and introduction of additional confounding factors from neighboring units. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.829</span></span><span class='px-1 mx-1 bg-yellow-200'>We propose a novel methodology combining graph neural networks and double machine learning, enabling accurate and efficient estimation of direct and peer effects using a single observational social network. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.503</span></span><span class='px-1 mx-1 bg-yellow-200'>Our approach utilizes graph isomorphism networks in conjunction with double machine learning to effectively adjust for network confounders and consistently estimate the desired causal effects. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.751</span></span>We demonstrate that our estimator is both asymptotically normal and semiparametrically efficient.<span class='px-1 mx-1 bg-yellow-200'>A comprehensive evaluation against four state-of-the-art baseline methods using three semi-synthetic social network datasets reveals our method's on-par or superior efficacy in precise causal effect estimation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.701</span></span>Further, we illustrate the practical application of our method through a case study that investigates the impact of Self-Help Group participation on financial risk tolerance.The results indicate a significant positive direct effect, underscoring the potential of our approach in social network analysis.Additionally, we explore the effects of network sparsity on estimation performance.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.11332v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Generating Feasible and Plausible Counterfactual Explanations for Outcome Prediction of Business Processes
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In recent years, various machine and deep learning architectures have been successfully introduced to the field of predictive process analytics.Nevertheless, the inherent opacity of these algorithms poses a significant challenge for human decision-makers, hindering their ability to understand the reasoning behind the predictions.<span class='px-1 mx-1 bg-yellow-200'>This growing concern has sparked the introduction of counterfactual explanations, designed as human-understandable what if scenarios, to provide clearer insights into the decision-making process behind undesirable predictions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.598</span></span><span class='px-1 mx-1 bg-yellow-200'>The generation of counterfactual explanations, however, encounters specific challenges when dealing with the sequential nature of the (business) process cases typically used in predictive process analytics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.547</span></span><span class='px-1 mx-1 bg-yellow-200'>Our paper tackles this challenge by introducing a data-driven approach, REVISEDplus, to generate more feasible and plausible counterfactual explanations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.577</span></span><span class='px-1 mx-1 bg-yellow-200'>First, we restrict the counterfactual algorithm to generate counterfactuals that lie within a high-density region of the process data, ensuring that the proposed counterfactuals are realistic and feasible within the observed process data distribution. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.572</span></span>Additionally, we ensure plausibility by learning sequential patterns between the activities in the process cases, utilising Declare language templates.<span class='px-1 mx-1 bg-yellow-200'>Finally, we evaluate the properties that define the validity of counterfactuals. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.559</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.09232v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Recursive Causal Discovery
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Causal discovery, i.e., learning the causal graph from data, is often the first step toward the identification and estimation of causal effects, a key requirement in numerous scientific domains. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.885</span></span><span class='px-1 mx-1 bg-yellow-200'>Causal discovery is hampered by two main challenges: limited data results in errors in statistical testing and the computational complexity of the learning task is daunting. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.879</span></span>This paper builds upon and extends four of our prior publications (Mokhtarian et al., 2021; Akbari et al., 2021; Mokhtarian et al., 2022, 2023a).<span class='px-1 mx-1 bg-yellow-200'>These works introduced the concept of removable variables, which are the only variables that can be removed recursively for the purpose of causal discovery. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.784</span></span><span class='px-1 mx-1 bg-yellow-200'>Presence and identification of removable variables allow recursive approaches for causal discovery, a promising solution that helps to address the aforementioned challenges by reducing the problem size successively. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.83</span></span>This reduction not only minimizes conditioning sets in each conditional independence (CI) test, leading to fewer errors but also significantly decreases the number of required CI tests.The worst-case performances of these methods nearly match the lower bound.In this paper, we present a unified framework for the proposed algorithms, refined with additional details and enhancements for a coherent presentation.A comprehensive literature review is also included, comparing the computational complexity of our methods with existing approaches, showcasing their state-of-the-art efficiency.Another contribution of this paper is the release of RCD, a Python package that efficiently implements these algorithms.This package is designed for practitioners and researchers interested in applying these methods in practical scenarios.The package is available at github.com/ban-epfl/rcd, with comprehensive documentation provided at rcdpackage.com.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.09300v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Mitigating social biases typically requires identifying the social groups associated with each data sample. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.505</span></span>In this paper, we present DAFair, a novel approach to address social bias in language models.Unlike traditional methods that rely on explicit demographic labels, our approach does not require any such information.Instead, we leverage predefined prototypical demographic texts and incorporate a regularization term during the fine-tuning process to mitigate bias in the model's representations.Our empirical results across two tasks and two models demonstrate the effectiveness of our method compared to previous approaches that do not rely on labeled data.Moreover, with limited demographic-annotated data, our approach outperforms common debiasing approaches.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.09516v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The deployment of autonomous agents in environments involving human interaction has increasingly raised security concerns.<span class='px-1 mx-1 bg-yellow-200'>Consequently, understanding the circumstances behind an event becomes critical, requiring the development of capabilities to justify their behaviors to non-expert users. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.561</span></span>Such explanations are essential in enhancing trustworthiness and safety, acting as a preventive measure against failures, errors, and misunderstandings.Additionally, they contribute to improving communication, bridging the gap between the agent and the user, thereby improving the effectiveness of their interactions.This work presents an accountability and explainability architecture implemented for ROS-based mobile robots.The proposed solution consists of two main components.Firstly, a black box-like element to provide accountability, featuring anti-tampering properties achieved through blockchain technology.Secondly, a component in charge of generating natural language explanations by harnessing the capabilities of Large Language Models (LLMs) over the data contained within the previously mentioned black box.The study evaluates the performance of our solution in three different scenarios, each involving autonomous agent navigation functionalities.This evaluation includes a thorough examination of accountability and explainability metrics, demonstrating the effectiveness of our approach in using accountable data from robot actions to obtain coherent, accurate and understandable explanations, even when facing challenges inherent in the use of autonomous agents in real-world scenarios.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.09567v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Iterative Forgetting: Online Data Stream Regression Using Database-Inspired Adaptive Granulation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Many modern systems, such as financial, transportation, and telecommunications systems, are time-sensitive in the sense that they demand low-latency predictions for real-time decision-making.Such systems often have to contend with continuous unbounded data streams as well as concept drift, which are challenging requirements that traditional regression techniques are unable to cater to.<span class='px-1 mx-1 bg-yellow-200'>There exists a need to create novel data stream regression methods that can handle these scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.535</span></span>We present a database-inspired datastream regression model that (a) uses inspiration from R*-trees to create granules from incoming datastreams such that relevant information is retained, (b) iteratively forgets granules whose information is deemed to be outdated, thus maintaining a list of only recent, relevant granules, and (c) uses the recent data and granules to provide low-latency predictions.The R*-tree-inspired approach also makes the algorithm amenable to integration with database systems.Our experiments demonstrate that the ability of this method to discard data produces a significant order-of-magnitude improvement in latency and training time when evaluated against the most accurate state-of-the-art algorithms, while the R*-tree-inspired granulation technique provides competitively accurate predictions</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.09588v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Explainability Research</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ChatGPT Rates Natural Language Explanation Quality Like Humans: But on Which Scales?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As AI becomes more integral in our lives, the need for transparency and responsibility grows.While natural language explanations (NLEs) are vital for clarifying the reasoning behind AI decisions, evaluating them through human judgments is complex and resource-intensive due to subjectivity and the need for fine-grained ratings.This study explores the alignment between ChatGPT and human assessments across multiple scales (i.e., binary, ternary, and 7-Likert scale).We sample 300 data instances from three NLE datasets and collect 900 human annotations for both informativeness and clarity scores as the text quality measurement.We further conduct paired comparison experiments under different ranges of subjectivity scores, where the baseline comes from 8,346 human annotations.Our results show that ChatGPT aligns better with humans in more coarse-grained scales.Also, paired comparisons and dynamic prompting (i.e., providing semantically similar examples in the prompt) improve the alignment.<span class='px-1 mx-1 bg-yellow-200'>This research advances our understanding of large language models' capabilities to assess the text explanation quality in different configurations for responsible AI development. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.502</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17368v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Explainable Graph Neural Networks for Observation Impact Analysis in Atmospheric State Estimation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper investigates the impact of observations on atmospheric state estimation in weather forecasting systems using graph neural networks (GNNs) and explainability methods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.512</span></span>We integrate observation and Numerical Weather Prediction (NWP) points into a meteorological graph, extracting $k$-hop subgraphs centered on NWP points.Self-supervised GNNs are employed to estimate the atmospheric state by aggregating data within these $k$-hop radii.The study applies gradient-based explainability methods to quantify the significance of different observations in the estimation process.Evaluated with data from 11 satellite and land-based observations, the results highlight the effectiveness of visualizing the importance of observation types, enhancing the understanding and optimization of observational data in weather forecasting.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17384v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Survey on Deep Learning and State-of-the-arts Applications
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Deep learning, a branch of artificial intelligence, is a computational model that uses multiple layers of interconnected units (neurons) to learn intricate patterns and representations directly from raw input data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.515</span></span>Empowered by this learning capability, it has become a powerful tool for solving complex problems and is the core driver of many groundbreaking technologies and innovations.<span class='px-1 mx-1 bg-yellow-200'>Building a deep learning model is a challenging task due to the algorithm`s complexity and the dynamic nature of real-world problems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.511</span></span>Several studies have reviewed deep learning concepts and applications.<span class='px-1 mx-1 bg-yellow-200'>However, the studies mostly focused on the types of deep learning models and convolutional neural network architectures, offering limited coverage of the state-of-the-art of deep learning models and their applications in solving complex problems across different domains. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.524</span></span>Therefore, motivated by the limitations, this study aims to comprehensively review the state-of-the-art deep learning models in computer vision, natural language processing, time series analysis and pervasive computing.We highlight the key features of the models and their effectiveness in solving the problems within each domain.Furthermore, this study presents the fundamentals of deep learning, various deep learning model types and prominent convolutional neural network architectures.Finally, challenges and future directions in deep learning research are discussed to offer a broader perspective for future researchers.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17561v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DeepMachining: Online Prediction of Machining Errors of Lathe Machines
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>We describe DeepMachining, a deep learning-based AI system for online prediction of machining errors of lathe machine operations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.501</span></span>We have built and evaluated DeepMachining based on manufacturing data from factories.Specifically, we first pretrain a deep learning model for a given lathe machine's operations to learn the salient features of machining states.Then, we fine-tune the pretrained model to adapt to specific machining tasks.We demonstrate that DeepMachining achieves high prediction accuracy for multiple tasks that involve different workpieces and cutting tools.To the best of our knowledge, this work is one of the first factory experiments using pre-trained deep-learning models to predict machining errors of lathe machines.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16451v2' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Model-less Is the Best Model: Generating Pure Code Implementations to Replace On-Device DL Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent studies show that deployed deep learning (DL) models such as those of Tensor Flow Lite (TFLite) can be easily extracted from real-world applications and devices by attackers to generate many kinds of attacks like adversarial attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.573</span></span>Although securing deployed on-device DL models has gained increasing attention, no existing methods can fully prevent the aforementioned threats.Traditional software protection techniques have been widely explored, if on-device models can be implemented using pure code, such as C++, it will open the possibility of reusing existing software protection techniques.However, due to the complexity of DL models, there is no automatic method that can translate the DL models to pure code.To fill this gap, we propose a novel method, CustomDLCoder, to automatically extract the on-device model information and synthesize a customized executable program for a wide range of DL models.CustomDLCoder first parses the DL model, extracts its backend computing units, configures the computing units to a graph, and then generates customized code to implement and deploy the ML solution without explicit model representation.The synthesized program hides model information for DL deployment environments since it does not need to retain explicit model representation, preventing many attacks on the DL model.In addition, it improves ML performance because the customized code removes model parsing and preprocessing steps and only retains the data computing process.Our experimental results show that CustomDLCoder improves model security by disabling on-device model sniffing.Compared with the original on-device platform (i.e., TFLite), our method can accelerate model inference by 21.0% and 24.3% on x86-64 and ARM64 platforms, respectively.Most importantly, it can significantly reduce memory consumption by 68.8% and 36.0% on x86-64 and ARM64 platforms, respectively.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16479v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                FineWAVE: Fine-Grained Warning Verification of Bugs for Automated Static Analysis Tools
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The continual expansion of software size and complexity has led to an increased focus on reducing defects and bugs during development.Although Automated Static Analysis Tools (ASATs) offer help, in practice, the significant number of false positives can impede developers' productivity and confidence in the tools.Therefore, previous research efforts have explored learning-based methods to validate the reported warnings.Nevertheless, there are still some limitations.(1) The granularity of prior research is coarse, as it focuses on identifying either actionable warnings throughout extensive development histories or potential true warnings at the function level.These approaches lack specificity regarding individual bugs and warnings.<span class='px-1 mx-1 bg-yellow-200'>(2) Machine learning-based approaches need much manual effort for feature engineering while existing deep learning-based approaches ignore key semantics between source code and warnings. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.541</span></span>(3) The small number of selected projects hinders the comprehensive evaluation of these approaches.In this paper, we proposed a fine-grained warning verification approach that is sensitive to bugs for improving the results of ASATs, namely \ourtool.Specifically, we design a novel LSTM-based model that captures both fine-grained semantics of source code and warnings from ASATs and highlights their correlations with cross-attention.To tackle the data scarcity of training and evaluation, we collected a large-scale dataset of 280,273 warnings, namely FineWA.It is ten times larger than the existing largest dataset.Then, we conducted extensive experiments on the dataset to evaluate FineWAVE.The experimental results demonstrate the effectiveness of our approach, with an F1-score of 97.79% for reducing false alarms and 67.06% for confirming actual warnings, which also significantly outperforms all baselines.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16032v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Systematic construction of continuous-time neural networks for linear dynamical systems
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Discovering a suitable neural network architecture for modeling complex dynamical systems poses a formidable challenge, often involving extensive trial and error and navigation through a high-dimensional hyper-parameter space.In this paper, we discuss a systematic approach to constructing neural architectures for modeling a subclass of dynamical systems, namely, Linear Time-Invariant (LTI) systems.<span class='px-1 mx-1 bg-yellow-200'>We use a variant of continuous-time neural networks in which the output of each neuron evolves continuously as a solution of a first-order or second-order Ordinary Differential Equation (ODE). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.511</span></span>Instead of deriving the network architecture and parameters from data, we propose a gradient-free algorithm to compute sparse architecture and network parameters directly from the given LTI system, leveraging its properties.We bring forth a novel neural architecture paradigm featuring horizontal hidden layers and provide insights into why employing conventional neural architectures with vertical hidden layers may not be favorable.We also provide an upper bound on the numerical errors of our neural networks.Finally, we demonstrate the high accuracy of our constructed networks on three numerical examples.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16215v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Science based AI model certification for untrained operational environments with application in traffic state estimation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The expanding role of Artificial Intelligence (AI) in diverse engineering domains highlights the challenges associated with deploying AI models in new operational environments, involving substantial investments in data collection and model training.Rapid application of AI necessitates evaluating the feasibility of utilizing pre-trained models in unobserved operational settings with minimal or no additional data.<span class='px-1 mx-1 bg-yellow-200'>However, interpreting the opaque nature of AI's black-box models remains a persistent challenge. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.624</span></span>Addressing this issue, this paper proposes a science-based certification methodology to assess the viability of employing pre-trained data-driven models in untrained operational environments.The methodology advocates a profound integration of domain knowledge, leveraging theoretical and analytical models from physics and related disciplines, with data-driven AI models.<span class='px-1 mx-1 bg-yellow-200'>This novel approach introduces tools to facilitate the development of secure engineering systems, providing decision-makers with confidence in the trustworthiness and safety of AI-based models across diverse environments characterized by limited training data and dynamic, uncertain conditions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.593</span></span>The paper demonstrates the efficacy of this methodology in real-world safety-critical scenarios, particularly in the context of traffic state estimation.<span class='px-1 mx-1 bg-yellow-200'>Through simulation results, the study illustrates how the proposed methodology efficiently quantifies physical inconsistencies exhibited by pre-trained AI models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.583</span></span>By utilizing analytical models, the methodology offers a means to gauge the applicability of pre-trained AI models in new operational environments.<span class='px-1 mx-1 bg-yellow-200'>This research contributes to advancing the understanding and deployment of AI models, offering a robust certification framework that enhances confidence in their reliability and safety across a spectrum of operational conditions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.542</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14093v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Existence Is Chaos: Enhancing 3D Human Motion Prediction with Uncertainty Consideration
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Human motion prediction is consisting in forecasting future body poses from historically observed sequences.It is a longstanding challenge due to motion's complex dynamics and uncertainty.<span class='px-1 mx-1 bg-yellow-200'>Existing methods focus on building up complicated neural networks to model the motion dynamics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.541</span></span>The predicted results are required to be strictly similar to the training samples with L2 loss in current training pipeline.However, little attention has been paid to the uncertainty property which is crucial to the prediction task.We argue that the recorded motion in training data could be an observation of possible future, rather than a predetermined result.In addition, existing works calculate the predicted error on each future frame equally during training, while recent work indicated that different frames could play different roles.In this work, a novel computationally efficient encoder-decoder model with uncertainty consideration is proposed, which could learn proper characteristics for future frames by a dynamic function.Experimental results on benchmark datasets demonstrate that our uncertainty consideration approach has obvious advantages both in quantity and quality.Moreover, the proposed method could produce motion sequences with much better quality that avoids the intractable shaking artefacts.We believe our work could provide a novel perspective to consider the uncertainty quality for the general motion prediction task and encourage the studies in this field.The code will be available in https://github.com/Motionpre/Adaptive-Salient-Loss-SAGGB.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14104v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Adversary-Robust Graph-Based Learning of WSIs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Enhancing the robustness of deep learning models against adversarial attacks is crucial, especially in critical domains like healthcare where significant financial interests heighten the risk of such attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.554</span></span>Whole slide images (WSIs) are high-resolution, digitized versions of tissue samples mounted on glass slides, scanned using sophisticated imaging equipment.The digital analysis of WSIs presents unique challenges due to their gigapixel size and multi-resolution storage format.In this work, we aim at improving the robustness of cancer Gleason grading classification systems against adversarial attacks, addressing challenges at both the image and graph levels.As regards the proposed algorithm, we develop a novel and innovative graph-based model which utilizes GNN to extract features from the graph representation of WSIs.A denoising module, along with a pooling layer is incorporated to manage the impact of adversarial attacks on the WSIs.The process concludes with a transformer module that classifies various grades of prostate cancer based on the processed data.To assess the effectiveness of the proposed method, we conducted a comparative analysis using two scenarios.Initially, we trained and tested the model without the denoiser using WSIs that had not been exposed to any attack.We then introduced a range of attacks at either the image or graph level and processed them through the proposed network.The performance of the model was evaluated in terms of accuracy and kappa scores.<span class='px-1 mx-1 bg-yellow-200'>The results from this comparison showed a significant improvement in cancer diagnosis accuracy, highlighting the robustness and efficiency of the proposed method in handling adversarial challenges in the context of medical imaging. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.501</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14489v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Universal Differential Equations as a Common Modeling Language for Neuroscience
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The unprecedented availability of large-scale datasets in neuroscience has spurred the exploration of artificial deep neural networks (DNNs) both as empirical tools and as models of natural neural systems.Their appeal lies in their ability to approximate arbitrary functions directly from observations, circumventing the need for cumbersome mechanistic modeling.However, without appropriate constraints, DNNs risk producing implausible models, diminishing their scientific value.Moreover, the interpretability of DNNs poses a significant challenge, particularly with the adoption of more complex expressive architectures.In this perspective, we argue for universal differential equations (UDEs) as a unifying approach for model development and validation in neuroscience.<span class='px-1 mx-1 bg-yellow-200'>UDEs view differential equations as parameterizable, differentiable mathematical objects that can be augmented and trained with scalable deep learning techniques. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.64</span></span><span class='px-1 mx-1 bg-yellow-200'>This synergy facilitates the integration of decades of extensive literature in calculus, numerical analysis, and neural modeling with emerging advancements in AI into a potent framework. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.559</span></span>We provide a primer on this burgeoning topic in scientific machine learning and demonstrate how UDEs fill in a critical gap between mechanistic, phenomenological, and data-driven models in neuroscience.We outline a flexible recipe for modeling neural systems with UDEs and discuss how they can offer principled solutions to inherent challenges across diverse neuroscience applications such as understanding neural computation, controlling neural systems, neural decoding, and normative modeling.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14510v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A survey on Concept-based Approaches For Model Improvement
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The focus of recent research has shifted from merely increasing the Deep Neural Networks (DNNs) performance in various tasks to DNNs, which are more interpretable to humans.<span class='px-1 mx-1 bg-yellow-200'>The field of eXplainable Artificial Intelligence (XAI) has observed various techniques, including saliency-based and concept-based approaches. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.539</span></span>Concept-based approaches explain the model's decisions in simple human understandable terms called Concepts.Concepts are human interpretable units of data and are the thinking ground of humans.Explanations in terms of concepts enable detecting spurious correlations, inherent biases, or clever-hans.With the advent of concept-based explanations, there have been various concept representation methods and automatic concept discovery algorithms.Some recent methods use concepts for post-hoc model disentanglement evaluation, while others use them for ante-hoc training.The concept-based approaches are new, with many representations coming up, and there is very limited work on Concept-based Model improvement.We provide a systematic review and taxonomy of various concept representations and their discovery algorithms in DNNs, specifically in vision.We also provide details on concept-based model improvement literature, which is the first to survey concept-based model improvement methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14566v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Threats, Attacks, and Defenses in Machine Unlearning: A Survey
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recently, Machine Unlearning (MU) has gained considerable attention for its potential to improve AI safety by removing the influence of specific data from trained Machine Learning (ML) models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.511</span></span>This process, known as knowledge removal, addresses concerns about data such as sensitivity, copyright restrictions, obsolescence, or low quality.This capability is also crucial for ensuring compliance with privacy regulations such as the Right To Be Forgotten (RTBF).Therefore, strategic knowledge removal mitigates the risk of harmful outcomes, safeguarding against biases, misinformation, and unauthorized data exploitation, thereby enhancing the ethical use and reliability of AI systems.Efforts have been made to design efficient unlearning approaches, with MU services being examined for integration with existing machine learning as a service (MLaaS), allowing users to submit requests to erase data.However, recent research highlights vulnerabilities in machine unlearning systems, such as information leakage and malicious unlearning requests, that can lead to significant security and privacy concerns.Moreover, extensive research indicates that unlearning methods and prevalent attacks fulfill diverse roles within MU systems.For instance, unlearning can act as a mechanism to recover models from backdoor attacks, while backdoor attacks themselves can serve as an evaluation metric for unlearning effectiveness.This underscores the intricate relationship and complex interplay between these elements in maintaining system functionality and safety.Therefore, this survey seeks to bridge the gap between the extensive number of studies on threats, attacks, and defenses in machine unlearning and the absence of a comprehensive review that categorizes their taxonomy, methods, and solutions, thus offering valuable insights for future research directions and practical implementations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13682v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Generative AI (GAI) offers unprecedented possibilities but its commercialization has raised concerns about transparency, reproducibility, bias, and safety. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.508</span></span>Many "open-source" GAI models lack the necessary components for full understanding and reproduction, and some use restrictive licenses, a practice known as "openwashing."We propose the Model Openness Framework (MOF), a ranked classification system that rates machine learning models based on their completeness and openness, following principles of open science, open source, open data, and open access.The MOF requires specific components of the model development lifecycle to be included and released under appropriate open licenses.This framework aims to prevent misrepresentation of models claiming to be open, guide researchers and developers in providing all model components under permissive licenses, and help companies, academia, and hobbyists identify models that can be safely adopted without restrictions.Wide adoption of the MOF will foster a more open AI ecosystem, accelerating research, innovation, and adoption.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13784v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Existing neural operator architectures face challenges when solving multiphysics problems with coupled partial differential equations (PDEs), due to complex geometries, interactions between physical variables, and the lack of large amounts of high-resolution training data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.585</span></span>To address these issues, we propose Codomain Attention Neural Operator (CoDA-NO), which tokenizes functions along the codomain or channel space, enabling self-supervised learning or pretraining of multiple PDE systems.Specifically, we extend positional encoding, self-attention, and normalization layers to the function space.CoDA-NO can learn representations of different PDE systems with a single model.We evaluate CoDA-NO's potential as a backbone for learning multiphysics PDEs over multiple systems by considering few-shot learning settings.On complex downstream tasks with limited data, such as fluid flow simulations and fluid-structure interactions, we found CoDA-NO to outperform existing methods on the few-shot learning task by over $36\%$. The code is available at https://github.com/ashiq24/CoDA-NO.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12553v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Physics-embedded Deep Learning Framework for Cloth Simulation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Delicate cloth simulations have long been desired in computer graphics.Various methods were proposed to improve engaged force interactions, collision handling, and numerical integrations.Deep learning has the potential to achieve fast and real-time simulation, but common neural network structures often demand many parameters to capture cloth dynamics.This paper proposes a physics-embedded learning framework that directly encodes physical features of cloth simulation.The convolutional neural network is used to represent spatial correlations of the mass-spring system, after which three branches are designed to learn linear, nonlinear, and time derivate features of cloth physics.<span class='px-1 mx-1 bg-yellow-200'>The framework can also integrate with other external forces and collision handling through either traditional simulators or sub neural networks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.5</span></span>The model is tested across different cloth animation cases, without training with new data.Agreement with baselines and predictive realism successfully validate its generalization ability.Inference efficiency of the proposed model also defeats traditional physics simulation.This framework is also designed to easily integrate with other visual refinement techniques like wrinkle carving, which leaves significant chances to incorporate prevailing macing learning techniques in 3D cloth amination.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12820v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Neural Differential Algebraic Equations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Differential-Algebraic Equations (DAEs) describe the temporal evolution of systems that obey both differential and algebraic constraints.Of particular interest are systems that contain implicit relationships between their components, such as conservation relationships.Here, we present Neural Differential-Algebraic Equations (NDAEs) suitable for data-driven modeling of DAEs.<span class='px-1 mx-1 bg-yellow-200'>This methodology is built upon the concept of the Universal Differential Equation; that is, a model constructed as a system of Neural Ordinary Differential Equations informed by theory from particular science domains. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.503</span></span>In this work, we show that the proposed NDAEs abstraction is suitable for relevant system-theoretic data-driven modeling tasks.Presented examples include (i) the inverse problem of tank-manifold dynamics and (ii) discrepancy modeling of a network of pumps, tanks, and pipes.Our experiments demonstrate the proposed method's robustness to noise and extrapolation ability to (i) learn the behaviors of the system components and their interaction physics and (ii) disambiguate between data trends and mechanistic relationships contained in the system.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12938v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Robust NAS under adversarial training: benchmark, theory, and beyond
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent developments in neural architecture search (NAS) emphasize the significance of considering robust architectures against malicious data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.51</span></span>However, there is a notable absence of benchmark evaluations and theoretical guarantees for searching these robust architectures, especially when adversarial training is considered.In this work, we aim to address these two challenges, making twofold contributions.First, we release a comprehensive data set that encompasses both clean accuracy and robust accuracy for a vast array of adversarially trained networks from the NAS-Bench-201 search space on image datasets.Then, leveraging the neural tangent kernel (NTK) tool from deep learning theory, we establish a generalization theory for searching architecture in terms of clean accuracy and robust accuracy under multi-objective adversarial training.We firmly believe that our benchmark and theoretical insights will significantly benefit the NAS community through reliable reproducibility, efficient assessment, and theoretical foundation, particularly in the pursuit of robust architectures.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13134v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Deep reinforcement learning (DRL) has shown significant promise for uncovering sophisticated control policies that interact in environments with complicated dynamics, such as stabilizing the magnetohydrodynamics of a tokamak fusion reactor or minimizing the drag force exerted on an object in a fluid flow.However, these algorithms require an abundance of training examples and may become prohibitively expensive for many applications.<span class='px-1 mx-1 bg-yellow-200'>In addition, the reliance on deep neural networks often results in an uninterpretable, black-box policy that may be too computationally expensive to use with certain embedded systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.52</span></span>Recent advances in sparse dictionary learning, such as the sparse identification of nonlinear dynamics (SINDy), have shown promise for creating efficient and interpretable data-driven models in the low-data regime.In this work we introduce SINDy-RL, a unifying framework for combining SINDy and DRL to create efficient, interpretable, and trustworthy representations of the dynamics model, reward function, and control policy.We demonstrate the effectiveness of our approaches on benchmark control environments and challenging fluids problems.SINDy-RL achieves comparable performance to state-of-the-art DRL algorithms using significantly fewer interactions in the environment and results in an interpretable control policy orders of magnitude smaller than a deep neural network policy.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.09110v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Adversarial Fine-tuning of Compressed Neural Networks for Joint Improvement of Robustness and Efficiency
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As deep learning (DL) models are increasingly being integrated into our everyday lives, ensuring their safety by making them robust against adversarial attacks has become increasingly critical. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.508</span></span>DL models have been found to be susceptible to adversarial attacks which can be achieved by introducing small, targeted perturbations to disrupt the input data.Adversarial training has been presented as a mitigation strategy which can result in more robust models.This adversarial robustness comes with additional computational costs required to design adversarial attacks during training.The two objectives -- adversarial robustness and computational efficiency -- then appear to be in conflict of each other.In this work, we explore the effects of two different model compression methods -- structured weight pruning and quantization -- on adversarial robustness.We specifically explore the effects of fine-tuning on compressed models, and present the trade-off between standard fine-tuning and adversarial fine-tuning.Our results show that compression does not inherently lead to loss in model robustness and adversarial fine-tuning of a compressed model can yield large improvement to the robustness performance of models.We present experiments on two benchmark datasets showing that adversarial fine-tuning of compressed models can achieve robustness performance comparable to adversarially trained models, while also improving computational efficiency.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.09441v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Machine learning for structural design models of continuous beam systems via influence zones
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This work develops a machine learned structural design model for continuous beam systems from the inverse problem perspective.After demarcating between forward, optimisation and inverse machine learned operators, the investigation proposes a novel methodology based on the recently developed influence zone concept which represents a fundamental shift in approach compared to traditional structural design methods.The aim of this approach is to conceptualise a non-iterative structural design model that predicts cross-section requirements for continuous beam systems of arbitrary system size.<span class='px-1 mx-1 bg-yellow-200'>After generating a dataset of known solutions, an appropriate neural network architecture is identified, trained, and tested against unseen data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.55</span></span>The results show a mean absolute percentage testing error of 1.6% for cross-section property predictions, along with a good ability of the neural network to generalise well to structural systems of variable size.The CBeamXP dataset generated in this work and an associated python-based neural network training script are available at an open-source data repository to allow for the reproducibility of results and to encourage further investigations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.09454v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Architectural Implications of Neural Network Inference for High Data-Rate, Low-Latency Scientific Applications
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>With more scientific fields relying on neural networks (NNs) to process data incoming at extreme throughputs and latencies, it is crucial to develop NNs with all their parameters stored on-chip. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.508</span></span>In many of these applications, there is not enough time to go off-chip and retrieve weights.Even more so, off-chip memory such as DRAM does not have the bandwidth required to process these NNs as fast as the data is being produced (e.g., every 25 ns).As such, these extreme latency and bandwidth requirements have architectural implications for the hardware intended to run these NNs: 1) all NN parameters must fit on-chip, and 2) codesigning custom/reconfigurable logic is often required to meet these latency and bandwidth constraints.In our work, we show that many scientific NN applications must run fully on chip, in the extreme case requiring a custom chip to meet such stringent constraints.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.08980v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Effect of Different Optimization Strategies to Physics-Constrained Deep Learning for Soil Moisture Estimation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Soil moisture is a key hydrological parameter that has significant importance to human society and the environment.Accurate modeling and monitoring of soil moisture in crop fields, especially in the root zone (top 100 cm of soil), is essential for improving agricultural production and crop yield with the help of precision irrigation and farming tools.Realizing the full sensor data potential depends greatly on advanced analytical and predictive domain-aware models.<span class='px-1 mx-1 bg-yellow-200'>In this work, we propose a physics-constrained deep learning (P-DL) framework to integrate physics-based principles on water transport and water sensing signals for effective reconstruction of the soil moisture dynamics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.501</span></span>We adopt three different optimizers, namely Adam, RMSprop, and GD, to minimize the loss function of P-DL during the training process.In the illustrative case study, we demonstrate the empirical convergence of Adam optimizers outperforms the other optimization methods in both mini-batch and full-batch training.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.08154v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Advancing Security in AI Systems: A Novel Approach to Detecting Backdoors in Deep Neural Networks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In the rapidly evolving landscape of communication and network security, the increasing reliance on deep neural networks (DNNs) and cloud services for data processing presents a significant vulnerability: the potential for backdoors that can be exploited by malicious actors.Our approach leverages advanced tensor decomposition algorithms Independent Vector Analysis (IVA), Multiset Canonical Correlation Analysis (MCCA), and Parallel Factor Analysis (PARAFAC2) to meticulously analyze the weights of pre-trained DNNs and distinguish between backdoored and clean models effectively.The key strengths of our method lie in its domain independence, adaptability to various network architectures, and ability to operate without access to the training data of the scrutinized models.This not only ensures versatility across different application scenarios but also addresses the challenge of identifying backdoors without prior knowledge of the specific triggers employed to alter network behavior.We have applied our detection pipeline to three distinct computer vision datasets, encompassing both image classification and object detection tasks.The results demonstrate a marked improvement in both accuracy and efficiency over existing backdoor detection methods.<span class='px-1 mx-1 bg-yellow-200'>This advancement enhances the security of deep learning and AI in networked systems, providing essential cybersecurity against evolving threats in emerging technologies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.517</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.08208v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DeepCSHAP: Utilizing Shapley Values to Explain Deep Complex-Valued Neural Networks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Deep Neural Networks are widely used in academy as well as corporate and public applications, including safety critical applications such as health care and autonomous driving. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.518</span></span>The ability to explain their output is critical for safety reasons as well as acceptance among applicants.<span class='px-1 mx-1 bg-yellow-200'>A multitude of methods have been proposed to explain real-valued neural networks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.522</span></span><span class='px-1 mx-1 bg-yellow-200'>Recently, complex-valued neural networks have emerged as a new class of neural networks dealing with complex-valued input data without the necessity of projecting them onto $\mathbb{R}^2$. This brings up the need to develop explanation algorithms for this kind of neural networks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.525</span></span>In this paper we provide these developments.<span class='px-1 mx-1 bg-yellow-200'>While we focus on adapting the widely used DeepSHAP algorithm to the complex domain, we also present versions of four gradient based explanation methods suitable for use in complex-valued neural networks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.561</span></span><span class='px-1 mx-1 bg-yellow-200'>We evaluate the explanation quality of all presented algorithms and provide all of them as an open source library adaptable to most recent complex-valued neural network architectures. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.604</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.08428v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can physical information aid the generalization ability of Neural Networks for hydraulic modeling?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Application of Neural Networks to river hydraulics is fledgling, despite the field suffering from data scarcity, a challenge for machine learning techniques.Consequently, many purely data-driven Neural Networks proved to lack predictive capabilities.In this work, we propose to mitigate such problem by introducing physical information into the training phase.<span class='px-1 mx-1 bg-yellow-200'>The idea is borrowed from Physics-Informed Neural Networks which have been recently proposed in other contexts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.58</span></span><span class='px-1 mx-1 bg-yellow-200'>Physics-Informed Neural Networks embed physical information in the form of the residual of the Partial Differential Equations (PDEs) governing the phenomenon and, as such, are conceived as neural solvers, i.e. an alternative to traditional numerical solvers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.691</span></span>Such approach is seldom suitable for environmental hydraulics, where epistemic uncertainties are large, and computing residuals of PDEs exhibits difficulties similar to those faced by classical numerical methods.<span class='px-1 mx-1 bg-yellow-200'>Instead, we envisaged the employment of Neural Networks as neural operators, featuring physical constraints formulated without resorting to PDEs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.568</span></span>The proposed novel methodology shares similarities with data augmentation and regularization.We show that incorporating such soft physical information can improve predictive capabilities.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.08589v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Ethics Research</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                InternLM2 Technical Report
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI).However, replicating such advancements in open-source models has been challenging.This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques.The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data.InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack" test.<span class='px-1 mx-1 bg-yellow-200'>InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.501</span></span>By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17297v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ChatGPT Rates Natural Language Explanation Quality Like Humans: But on Which Scales?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As AI becomes more integral in our lives, the need for transparency and responsibility grows. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.773</span></span><span class='px-1 mx-1 bg-yellow-200'>While natural language explanations (NLEs) are vital for clarifying the reasoning behind AI decisions, evaluating them through human judgments is complex and resource-intensive due to subjectivity and the need for fine-grained ratings. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.537</span></span>This study explores the alignment between ChatGPT and human assessments across multiple scales (i.e., binary, ternary, and 7-Likert scale).We sample 300 data instances from three NLE datasets and collect 900 human annotations for both informativeness and clarity scores as the text quality measurement.We further conduct paired comparison experiments under different ranges of subjectivity scores, where the baseline comes from 8,346 human annotations.Our results show that ChatGPT aligns better with humans in more coarse-grained scales.Also, paired comparisons and dynamic prompting (i.e., providing semantically similar examples in the prompt) improve the alignment.<span class='px-1 mx-1 bg-yellow-200'>This research advances our understanding of large language models' capabilities to assess the text explanation quality in different configurations for responsible AI development. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.525</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17368v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AIDE: An Automatic Data Engine for Object Detection in Autonomous Driving
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Autonomous vehicle (AV) systems rely on robust perception models as a cornerstone of safety assurance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.508</span></span>However, objects encountered on the road exhibit a long-tailed distribution, with rare or unseen categories posing challenges to a deployed perception model.This necessitates an expensive process of continuously curating and annotating data with significant human effort.We propose to leverage recent advances in vision-language and large language models to design an Automatic Data Engine (AIDE) that automatically identifies issues, efficiently curates data, improves the model through auto-labeling, and verifies the model through generation of diverse scenarios.This process operates iteratively, allowing for continuous self-improvement of the model.We further establish a benchmark for open-world detection on AV datasets to comprehensively evaluate various learning paradigms, demonstrating our method's superior performance at a reduced cost.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17373v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Practical Applications of Advanced Cloud Services and Generative AI Systems in Medical Image Analysis
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The medical field is one of the important fields in the application of artificial intelligence technology. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.584</span></span><span class='px-1 mx-1 bg-yellow-200'>With the explosive growth and diversification of medical data, as well as the continuous improvement of medical needs and challenges, artificial intelligence technology is playing an increasingly important role in the medical field. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.582</span></span><span class='px-1 mx-1 bg-yellow-200'>Artificial intelligence technologies represented by computer vision, natural language processing, and machine learning have been widely penetrated into diverse scenarios such as medical imaging, health management, medical information, and drug research and development, and have become an important driving force for improving the level and quality of medical services. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.553</span></span>The article explores the transformative potential of generative AI in medical imaging, emphasizing its ability to generate syntheticACM-2 data, enhance images, aid in anomaly detection, and facilitate image-to-image translation.Despite challenges like model complexity, the applications of generative models in healthcare, including Med-PaLM 2 technology, show promising results.By addressing limitations in dataset size and diversity, these models contribute to more accurate diagnoses and improved patient outcomes.<span class='px-1 mx-1 bg-yellow-200'>However, ethical considerations and collaboration among stakeholders are essential for responsible implementation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.527</span></span>Through experiments leveraging GANs to augment brain tumor MRI datasets, the study demonstrates how generative AI can enhance image quality and diversity, ultimately advancing medical diagnostics and patient care.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17549v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recently, significant progress has been achieved in sensing real large-scale outdoor 3D environments, particularly by using modern acquisition equipment such as LiDAR sensors.Unfortunately, they are fundamentally limited in their ability to produce dense, complete 3D scenes.To address this issue, recent learning-based methods integrate neural implicit representations and optimizable feature grids to approximate surfaces of 3D scenes.However, naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results due to the nature of sparse, conflicting LiDAR measurements.Instead, in this work we depart from fitting LiDAR data exactly, instead letting the network optimize a non-metric monotonic implicit field defined in 3D space.To fit our field, we design a learning system integrating a monotonicity loss that enables optimizing neural monotonic fields and leverages recent progress in large-scale 3D mapping.Our algorithm achieves high-quality dense 3D mapping performance as captured by multiple quantitative and perceptual measures and visual results obtained for Mai City, Newer College, and KITTI benchmarks.<span class='px-1 mx-1 bg-yellow-200'>The code of our approach will be made publicly available. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.536</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17550v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Coimagining the Future of Voice Assistants with Cultural Sensitivity
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Voice assistants (VAs) are becoming a feature of our everyday life.Yet, the user experience (UX) is often limited, leading to underuse, disengagement, and abandonment.Co-designing interactions for VAs with potential end-users can be useful.Crowdsourcing this process online and anonymously may add value.However, most work has been done in the English-speaking West on dialogue data sets.<span class='px-1 mx-1 bg-yellow-200'>We must be sensitive to cultural differences in language, social interactions, and attitudes towards technology. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.536</span></span><span class='px-1 mx-1 bg-yellow-200'>Our aims were to explore the value of co-designing VAs in the non-Western context of Japan and demonstrate the necessity of cultural sensitivity. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.512</span></span>We conducted an online elicitation study (N = 135) where Americans (n = 64) and Japanese people (n = 71) imagined dialogues (N = 282) and activities (N = 73) with future VAs.We discuss the implications for coimagining interactions with future VAs, offer design guidelines for the Japanese and English-speaking US contexts, and suggest opportunities for cultural plurality in VA design and scholarship.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17599v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Healthcare Data Governance, Privacy, and Security - A Conceptual Framework
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The abundance of data has transformed the world in every aspect.It has become the core element in decision making, problem solving, and innovation in almost all areas of life, including business, science, healthcare, education, and many others.<span class='px-1 mx-1 bg-yellow-200'>Despite all these advances, privacy and security remain critical concerns of the healthcare industry. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.535</span></span>It is important to note that healthcare data can also be a liability if it is not managed correctly.This data mismanagement can have severe consequences for patients and healthcare organisations, including patient safety, legal liability, damage to reputation, financial loss, and operational inefficiency.Healthcare organisations must comply with a range of regulations to protect patient data.We perform a classification of data governance elements or components in a manner that thoroughly assesses the healthcare data chain from a privacy and security standpoint.After deeply analysing the existing literature, we propose a conceptual privacy and security driven healthcare data governance framework.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17648v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The advent of Generative Artificial Intelligence (GenAI) models, including GitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content creation, enabling non-professionals to produce high-quality content across various domains.This transformative technology has led to a surge of synthetic content and sparked legal disputes over copyright infringement.To address these challenges, this paper introduces a novel approach that leverages the learning capacity of GenAI models for copyright legal analysis, demonstrated with GPT2 and Stable Diffusion models.Copyright law distinguishes between original expressions and generic ones (Sc\`enes \`a faire), protecting the former and permitting reproduction of the latter.However, this distinction has historically been challenging to make consistently, leading to over-protection of copyrighted works.GenAI offers an unprecedented opportunity to enhance this legal analysis by revealing shared patterns in preexisting works.We propose a data-driven approach to identify the genericity of works created by GenAI, employing "data-driven bias" to assess the genericity of expressive compositions.This approach aids in copyright scope determination by utilizing the capabilities of GenAI to identify and prioritize expressive elements and rank them according to their frequency in the model's dataset.The potential implications of measuring expressive genericity for copyright law are profound.Such scoring could assist courts in determining copyright scope during litigation, inform the registration practices of Copyright Offices, allowing registration of only highly original synthetic works, and help copyright owners signal the value of their works and facilitate fairer licensing deals.<span class='px-1 mx-1 bg-yellow-200'>More generally, this approach offers valuable insights to policymakers grappling with adapting copyright law to the challenges posed by the era of GenAI. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.501</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17691v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DataCook: Crafting Anti-Adversarial Examples for Healthcare Data Copyright Protection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In the realm of healthcare, the challenges of copyright protection and unauthorized third-party misuse are increasingly significant. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.522</span></span>Traditional methods for data copyright protection are applied prior to data distribution, implying that models trained on these data become uncontrollable.This paper introduces a novel approach, named DataCook, designed to safeguard the copyright of healthcare data during the deployment phase.DataCook operates by "cooking" the raw data before distribution, enabling the development of models that perform normally on this processed data.However, during the deployment phase, the original test data must be also "cooked" through DataCook to ensure normal model performance.This process grants copyright holders control over authorization during the deployment phase.The mechanism behind DataCook is by crafting anti-adversarial examples (AntiAdv), which are designed to enhance model confidence, as opposed to standard adversarial examples (Adv) that aim to confuse models.Similar to Adv, AntiAdv introduces imperceptible perturbations, ensuring that the data processed by DataCook remains easily understandable.We conducted extensive experiments on MedMNIST datasets, encompassing both 2D/3D data and the high-resolution variants.The outcomes indicate that DataCook effectively meets its objectives, preventing models trained on AntiAdv from analyzing unauthorized data effectively, without compromising the validity and accuracy of the data in legitimate scenarios.Code and data are available at https://github.com/MedMNIST/DataCook.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17755v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AgentStudio: A Toolkit for Building General Virtual Agents
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Creating autonomous virtual agents capable of using arbitrary software on any digital device remains a major challenge for artificial intelligence. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.685</span></span><span class='px-1 mx-1 bg-yellow-200'>Two key obstacles hinder progress: insufficient infrastructure for building virtual agents in real-world environments, and the need for in-the-wild evaluation of fundamental agent abilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.634</span></span><span class='px-1 mx-1 bg-yellow-200'>To address this, we introduce AgentStudio, an online, realistic, and multimodal toolkit that covers the entire lifecycle of agent development. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.501</span></span>This includes environment setups, data collection, agent evaluation, and visualization.The observation and action spaces are highly generic, supporting both function calling and human-computer interfaces.This versatility is further enhanced by AgentStudio's graphical user interfaces, which allow efficient development of datasets and benchmarks in real-world settings.To illustrate, we introduce a visual grounding dataset and a real-world benchmark suite, both created with our graphical interfaces.Furthermore, we present several actionable insights derived from AgentStudio, e.g., general visual grounding, open-ended tool creation, learning from videos, etc.<span class='px-1 mx-1 bg-yellow-200'>We have open-sourced the environments, datasets, benchmarks, and interfaces to promote research towards developing general virtual agents for the future. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.555</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17918v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                EXPLORA: A teacher-apprentice methodology for eliciting natural child-computer interactions
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Investigating child-computer interactions within their contexts is vital for designing technology that caters to children's needs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.515</span></span>However, determining what aspects of context are relevant for designing child-centric technology remains a challenge.We introduce EXPLORA, a multimodal, multistage online methodology comprising three pivotal stages: (1) building a teacher-apprentice relationship,(2) learning from child-teachers, and (3) assessing and reinforcing researcher-apprentice learning.Central to EXPLORA is the collection of attitudinal data through pre-observation interviews, offering researchers a deeper understanding of children's characteristics and contexts.This informs subsequent online observations, allowing researchers to focus on frequent interactions.Furthermore, researchers can validate preliminary assumptions with children.A means-ends analysis framework aids in the systematic analysis of data, shedding light on context, agency and homework-information searching processes children employ in their activities.To illustrate EXPLORA's capabilities, we present nine single case studies investigating Brazilian child-caregiver dyads' (children ages 9-11) use of technology in homework information-searching.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17264v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Human Stress Response and Perceived Safety during Encounters with Quadruped Robots
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Despite the rise of mobile robot deployments in home and work settings, perceived safety of users and bystanders is understudied in the human-robot interaction (HRI) literature. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.62</span></span>To address this, we present a study designed to identify elements of a human-robot encounter that correlate with observed stress response.Stress is a key component of perceived safety and is strongly associated with human physiological response.In this study a Boston Dynamics Spot and a Unitree Go1 navigate autonomously through a shared environment occupied by human participants wearing multimodal physiological sensors to track their electrocardiography (ECG) and electrodermal activity (EDA).The encounters are varied through several trials and participants self-rate their stress levels after each encounter.<span class='px-1 mx-1 bg-yellow-200'>The study resulted in a multidimensional dataset archiving various objective and subjective aspects of a human-robot encounter, containing insights for understanding perceived safety in such encounters. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.575</span></span>To this end, acute stress responses were decoded from the human participants' ECG and EDA and compared across different human-robot encounter conditions.Statistical analysis of data indicate that on average (1) participants feel more stress during encounters compared to baselines, (2) participants feel more stress encountering multiple robots compared to a single robot and (3) participants stress increases during navigation behavior compared with search behavior.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.17270v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ColonyOS -- A Meta-Operating System for Distributed Computing Across Heterogeneous Platform
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper presents ColonyOS, an open-source meta-operating system designed to improve integration and utilization of diverse computing platforms, including IoT, edge, cloud, and HPC.Operating as an overlay, ColonyOS can interface with a wide range of computing environments, fostering creation of so-called compute continuums.<span class='px-1 mx-1 bg-yellow-200'>This makes it possible to develop AI workflows and applications that can operate across platforms. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.603</span></span>At its core, ColonyOS consists of distributed executors that integrate with various underlying platforms based on a distributed microservice architecture.These executors collectively form a colony, serving as a unified computing unit.To enable secure integration of various platforms, each colony is provisioned with precisely the resources needed, and all communication is confined within the colony governed by a strict zero-trust security protocol.Interaction with ColonyOS is done by submitting functional meta-descriptions of computational tasks, called function specifications.These are sent to a Colonies server, which acts as intermediary between applications and the executors.Upon assignment, an executor interprets the meta-description and translates it into an executable format, e.g. a Kubernetes deployment description, a Slurm script, or a direct function call within the executor.Furthermore, a built-in meta-file system enables data synchronization directives to be included in meta-descriptions, enabling seamless data management across platforms.Ultimately, ColonyOS paves the way for development of hyper-distributed applications and workflows, which can seamlessly operate in a computing continuum.The paper describes design principles and implementation details of ColonyOS.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16486v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Human Understanding AI Paper Challenge 2024 -- Dataset Design
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In 2024, we will hold a research paper competition (the third Human Understanding AI Paper Challenge) for the research and development of artificial intelligence technologies to understand human daily life. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.665</span></span>This document introduces the datasets that will be provided to participants in the competition, and summarizes the issues to consider in data processing and learning model development.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16509v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Norm Violation Detection in Multi-Agent Systems using Large Language Models: A Pilot Study
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Norms are an important component of the social fabric of society by prescribing expected behaviour. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.544</span></span><span class='px-1 mx-1 bg-yellow-200'>In Multi-Agent Systems (MAS), agents interacting within a society are equipped to possess social capabilities such as reasoning about norms and trust. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.623</span></span><span class='px-1 mx-1 bg-yellow-200'>Norms have long been of interest within the Normative Multi-Agent Systems community with researchers studying topics such as norm emergence, norm violation detection and sanctioning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.575</span></span>However, these studies have some limitations: they are often limited to simple domains, norms have been represented using a variety of representations with no standard approach emerging, and the symbolic reasoning mechanisms generally used may suffer from a lack of extensibility and robustness.In contrast, Large Language Models (LLMs) offer opportunities to discover and reason about norms across a large range of social situations.This paper evaluates the capability of LLMs to detecting norm violations.Based on simulated data from 80 stories in a household context, with varying complexities, we investigated whether 10 norms are violated.For our evaluations we first obtained the ground truth from three human evaluators for each story.Then, the majority result was compared against the results from three well-known LLM models (Llama 2 7B, Mixtral 7B and ChatGPT-4).Our results show the promise of ChatGPT-4 for detecting norm violations, with Mixtral some distance behind.Also, we identify areas where these models perform poorly and discuss implications for future work.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16517v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Counter-example guided Imitation Learning of Feedback Controllers from Temporal Logic Specifications
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We present a novel method for imitation learning for control requirements expressed using Signal Temporal Logic (STL).More concretely we focus on the problem of training a neural network to imitate a complex controller.The learning process is guided by efficient data aggregation based on counter-examples and a coverage measure.Moreover, we introduce a method to evaluate the performance of the learned controller via parameterization and parameter estimation of the STL requirements.<span class='px-1 mx-1 bg-yellow-200'>We demonstrate our approach with a flying robot case study. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.535</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16593v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Virtual Co-Pilot: Multimodal Large Language Model-enabled Quick-access Procedures for Single Pilot Operations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Advancements in technology, pilot shortages, and cost pressures are driving a trend towards single-pilot and even remote operations in aviation.Considering the extensive workload and huge risks associated with single-pilot operations, the development of a Virtual Co-Pilot (V-CoP) is expected to be a potential way to ensure aviation safety.<span class='px-1 mx-1 bg-yellow-200'>This study proposes a V-CoP concept and explores how humans and virtual assistants can effectively collaborate. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.561</span></span>A preliminary case study is conducted to explore a critical role of V-CoP, namely automated quick procedures searching, using the multimodal large language model (LLM).The LLM-enabled V-CoP integrates the pilot instruction and real-time cockpit instrumental data to prompt applicable aviation manuals and operation procedures.The results showed that the LLM-enabled V-CoP achieved high accuracy in situational analysis and effective retrieval of procedure information.The results showed that the LLM-enabled V-CoP achieved high accuracy in situational analysis (90.5%) and effective retrieval of procedure information (86.5%).The proposed V-CoP is expected to provide a foundation for future virtual intelligent assistant development, improve the performance of single pilots, and reduce the risk of human errors in aviation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16645v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                "We Have No Idea How Models will Behave in Production until Production": How Engineers Operationalize Machine Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Organizations rely on machine learning engineers (MLEs) to deploy models and maintain ML pipelines in production.Due to models' extensive reliance on fresh data, the operationalization of machine learning, or MLOps, requires MLEs to have proficiency in data science and engineering.When considered holistically, the job seems staggering -- how do MLEs do MLOps, and what are their unaddressed challenges?<span class='px-1 mx-1 bg-yellow-200'>To address these questions, we conducted semi-structured ethnographic interviews with 18 MLEs working on various applications, including chatbots, autonomous vehicles, and finance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.548</span></span>We find that MLEs engage in a workflow of (i) data preparation, (ii) experimentation, (iii) evaluation throughout a multi-staged deployment, and (iv) continual monitoring and response.Throughout this workflow, MLEs collaborate extensively with data scientists, product stakeholders, and one another, supplementing routine verbal exchanges with communication tools ranging from Slack to organization-wide ticketing and reporting systems.We introduce the 3Vs of MLOps: velocity, visibility, and versioning -- three virtues of successful ML deployments that MLEs learn to balance and grow as they mature.Finally, we discuss design implications and opportunities for future work.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16795v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Little Leak Will Sink a Great Ship: Survey of Transparency for Large Language Models from Start to Finish
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) are trained on massive web-crawled corpora.This poses risks of leakage, including personal information, copyrighted texts, and benchmark datasets.<span class='px-1 mx-1 bg-yellow-200'>Such leakage leads to undermining human trust in AI due to potential unauthorized generation of content or overestimation of performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.711</span></span>We establish the following three criteria concerning the leakage issues: (1) leakage rate: the proportion of leaked data in training data, (2) output rate: the ease of generating leaked data, and (3) detection rate: the detection performance of leaked versus non-leaked data.Despite the leakage rate being the origin of data leakage issues, it is not understood how it affects the output rate and detection rate.In this paper, we conduct an experimental survey to elucidate the relationship between the leakage rate and both the output rate and detection rate for personal information, copyrighted texts, and benchmark data.Additionally, we propose a self-detection approach that uses few-shot learning in which LLMs detect whether instances are present or absent in their training data, in contrast to previous methods that do not employ explicit learning.To explore the ease of generating leaked information, we create a dataset of prompts designed to elicit personal information, copyrighted text, and benchmarks from LLMs.Our experiments reveal that LLMs produce leaked information in most cases despite less such data in their training set.This indicates even small amounts of leaked data can greatly affect outputs.Our self-detection method showed superior performance compared to existing detection methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16139v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric View of Procedural Activities in Real World
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Being able to map the activities of others into one's own point of view is one fundamental human skill even from a very early age. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.556</span></span>Taking a step toward understanding this human ability, we introduce EgoExoLearn, a large-scale dataset that emulates the human demonstration following process, in which individuals record egocentric videos as they execute tasks guided by demonstration videos.Focusing on the potential applications in daily assistance and professional support, EgoExoLearn contains egocentric and demonstration video data spanning 120 hours captured in daily life scenarios and specialized laboratories.Along with the videos we record high-quality gaze data and provide detailed multimodal annotations, formulating a playground for modeling the human ability to bridge asynchronous procedural actions from different viewpoints.To this end, we present benchmarks such as cross-view association, cross-view action planning, and cross-view referenced skill assessment, along with detailed analysis.<span class='px-1 mx-1 bg-yellow-200'>We expect EgoExoLearn can serve as an important resource for bridging the actions across views, thus paving the way for creating AI agents capable of seamlessly learning by observing humans in the real world. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.57</span></span>Code and data can be found at: https://github.com/OpenGVLab/EgoExoLearn</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16182v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Cyber-Security Knowledge Graph Generation by Hierarchical Nonnegative Matrix Factorization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Much of human knowledge in cybersecurity is encapsulated within the ever-growing volume of scientific papers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.536</span></span>As this textual data continues to expand, the importance of document organization methods becomes increasingly crucial for extracting actionable insights hidden within large text datasets.Knowledge Graphs (KGs) serve as a means to store factual information in a structured manner, providing explicit, interpretable knowledge that includes domain-specific information from the cybersecurity scientific literature.One of the challenges in constructing a KG from scientific literature is the extraction of ontology from unstructured text.In this paper, we address this topic and introduce a method for building a multi-modal KG by extracting structured ontology from scientific papers.We demonstrate this concept in the cybersecurity domain.One modality of the KG represents observable information from the papers, such as the categories in which they were published or the authors.The second modality uncovers latent (hidden) patterns of text extracted through hierarchical and semantic non-negative matrix factorization (NMF), such as named entities, topics or clusters, and keywords.We illustrate this concept by consolidating more than two million scientific papers uploaded to arXiv into the cyber-domain, using hierarchical and semantic NMF, and by building a cyber-domain-specific KG.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16222v2' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The recent advancements in artificial intelligence highlight the potential of language models in psychological health support. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.544</span></span>While models trained on data from mental health service platform have achieved preliminary success, challenges persist in areas such as data scarcity, quality, and ensuring a solid foundation in psychological techniques.To address these challenges, this study introduces a novel approach to enhance the precision and efficacy of psychological support through large language models.Specifically, we design a specific prompt derived from principles of Cognitive Behavioral Therapy (CBT) and have generated the CBT QA dataset, specifically for Chinese psychological health Q&A based on CBT structured intervention strategies.Unlike previous methods, our dataset emphasizes professional and structured response.Utilizing this dataset, we fine-tuned the large language model, giving birth to CBT-LLM, the large-scale language model specifically designed for Cognitive Behavioral Therapy techniques.Empirical evaluations demonstrate that CBT-LLM excels in generating structured, professional, and highly relevant responses in psychological health support tasks, showcasing its practicality and quality.The model is available on Hugging Face: https://huggingface.co/Hongbin37/CBT-LLM.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.16008v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Efficient Model Learning and Adaptive Tracking Control of Magnetic Micro-Robots for Non-Contact Manipulation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Magnetic microrobots can be navigated by an external magnetic field to autonomously move within living organisms with complex and unstructured environments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.507</span></span>Potential applications include drug delivery, diagnostics, and therapeutic interventions.<span class='px-1 mx-1 bg-yellow-200'>Existing techniques commonly impart magnetic properties to the target object,or drive the robot to contact and then manipulate the object, both probably inducing physical damage. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.507</span></span>This paper considers a non-contact formulation, where the robot spins to generate a repulsive field to push the object without physical contact.Under such a formulation, the main challenge is that the motion model between the input of the magnetic field and the output velocity of the target object is commonly unknown and difficult to analyze.To deal with it, this paper proposes a data-driven-based solution.A neural network is constructed to efficiently estimate the motion model.Then, an approximate model-based optimal control scheme is developed to push the object to track a time-varying trajectory, maintaining the non-contact with distance constraints.Furthermore, a straightforward planner is introduced to assess the adaptability of non-contact manipulation in a cluttered unstructured environment.Experimental results are presented to show the tracking and navigation performance of the proposed scheme.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14414v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                FHAUC: Privacy Preserving AUC Calculation for Federated Learning using Fully Homomorphic Encryption
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Ensuring data privacy is a significant challenge for machine learning applications, not only during model training but also during evaluation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.523</span></span>Federated learning has gained significant research interest in recent years as a result.Current research on federated learning primarily focuses on preserving privacy during the training phase.However, model evaluation has not been adequately addressed, despite the potential for significant privacy leaks during this phase as well.In this paper, we demonstrate that the state-of-the-art AUC computation method for federated learning systems, which utilizes differential privacy, still leaks sensitive information about the test data while also requiring a trusted central entity to perform the computations.More importantly, we show that the performance of this method becomes completely unusable as the data size decreases.In this context, we propose an efficient, accurate, robust, and more secure evaluation algorithm capable of computing the AUC in horizontal federated learning systems.Our approach not only enhances security compared to the current state-of-the-art but also surpasses the state-of-the-art AUC computation method in both approximation performance and computational robustness, as demonstrated by experimental results.To illustrate, our approach can efficiently calculate the AUC of a federated learning system involving 100 parties, achieving 99.93% accuracy in just 0.68 seconds, regardless of data size, while providing complete data privacy.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14428v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring 3D Human Pose Estimation and Forecasting from the Robot's Perspective: The HARPER Dataset
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We introduce HARPER, a novel dataset for 3D body pose estimation and forecast in dyadic interactions between users and \spot, the quadruped robot manufactured by Boston Dynamics.<span class='px-1 mx-1 bg-yellow-200'>The key-novelty is the focus on the robot's perspective, i.e., on the data captured by the robot's sensors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.504</span></span>These make 3D body pose analysis challenging because being close to the ground captures humans only partially.The scenario underlying HARPER includes 15 actions, of which 10 involve physical contact between the robot and users.The Corpus contains not only the recordings of the built-in stereo cameras of Spot, but also those of a 6-camera OptiTrack system (all recordings are synchronized).This leads to ground-truth skeletal representations with a precision lower than a millimeter.In addition, the Corpus includes reproducible benchmarks on 3D Human Pose Estimation, Human Pose Forecasting, and Collision Prediction, all based on publicly available baseline approaches.This enables future HARPER users to rigorously compare their results with those we provide in this work.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14447v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Ethics of ChatGPT in Medicine and Healthcare: A Systematic Review on Large Language Models (LLMs)
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the introduction of ChatGPT, Large Language Models (LLMs) have received enormous attention in healthcare.<span class='px-1 mx-1 bg-yellow-200'>Despite their potential benefits, researchers have underscored various ethical implications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.593</span></span>While individual instances have drawn much attention, the debate lacks a systematic overview of practical applications currently researched and ethical issues connected to them.<span class='px-1 mx-1 bg-yellow-200'>Against this background, this work aims to map the ethical landscape surrounding the current stage of deployment of LLMs in medicine and healthcare. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.516</span></span>Electronic databases and preprint servers were queried using a comprehensive search strategy.Studies were screened and extracted following a modified rapid review approach.Methodological quality was assessed using a hybrid approach.For 53 records, a meta-aggregative synthesis was performed.Four fields of applications emerged and testify to a vivid exploration phase.Advantages of using LLMs are attributed to their capacity in data analysis, personalized information provisioning, support in decision-making, mitigating information loss and enhancing information accessibility.<span class='px-1 mx-1 bg-yellow-200'>However, we also identifies recurrent ethical concerns connected to fairness, bias, non-maleficence, transparency, and privacy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.593</span></span>A distinctive concern is the tendency to produce harmful misinformation or convincingly but inaccurate content.<span class='px-1 mx-1 bg-yellow-200'>A recurrent plea for ethical guidance and human oversight is evident. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.603</span></span><span class='px-1 mx-1 bg-yellow-200'>Given the variety of use cases, it is suggested that the ethical guidance debate be reframed to focus on defining what constitutes acceptable human oversight across the spectrum of applications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.622</span></span>This involves considering diverse settings, varying potentials for harm, and different acceptable thresholds for performance and certainty in healthcare.In addition, a critical inquiry is necessary to determine the extent to which the current experimental use of LLMs is necessary and justified.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14473v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Building a Language-Learning Game for Brazilian Indigenous Languages: A Case of Study
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this paper we discuss a first attempt to build a language learning game for brazilian indigenous languages and the challenges around it.We present a design for the tool with gamification aspects.Then we describe a process to automatically generate language exercises and questions from a dependency treebank and a lexical database for Tupian languages.<span class='px-1 mx-1 bg-yellow-200'>We discuss the limitations of our prototype highlighting ethical and practical implementation concerns. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.69</span></span>Finally, we conclude that new data gathering processes should be established in partnership with indigenous communities and oriented for educational purposes.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14515v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Dynamic Explanation Emphasis in Human-XAI Interaction with Communication Robot
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Communication robots have the potential to contribute to effective human-XAI interaction as an interface that goes beyond textual or graphical explanations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.585</span></span>One of their strengths is that they can use physical and vocal expressions to add detailed nuances to explanations.However, it is not clear how a robot can apply such expressions, or in particular, how we can develop a strategy to adaptively use such expressions depending on the task and user in dynamic interactions.<span class='px-1 mx-1 bg-yellow-200'>To address this question, this paper proposes DynEmph, a method for a communication robot to decide where to emphasize XAI-generated explanations with physical expressions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.512</span></span>It predicts the effect of emphasizing certain points on a user and aims to minimize the expected difference between predicted user decisions and AI-suggested ones.DynEmph features a strategy for deciding where to emphasize in a data-driven manner, relieving engineers from the need to manually design a strategy.We further conducted experiments to investigate how emphasis selection strategies affect the performance of user decisions.The results suggest that, while a naive strategy (emphasizing explanations for an AI's most probable class) does not necessarily work better, DynEmph effectively guides users to better decisions under the condition that the performance of the AI suggestion is high.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14550v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A survey on Concept-based Approaches For Model Improvement
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The focus of recent research has shifted from merely increasing the Deep Neural Networks (DNNs) performance in various tasks to DNNs, which are more interpretable to humans.<span class='px-1 mx-1 bg-yellow-200'>The field of eXplainable Artificial Intelligence (XAI) has observed various techniques, including saliency-based and concept-based approaches. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.54</span></span>Concept-based approaches explain the model's decisions in simple human understandable terms called Concepts.Concepts are human interpretable units of data and are the thinking ground of humans.Explanations in terms of concepts enable detecting spurious correlations, inherent biases, or clever-hans.With the advent of concept-based explanations, there have been various concept representation methods and automatic concept discovery algorithms.Some recent methods use concepts for post-hoc model disentanglement evaluation, while others use them for ante-hoc training.The concept-based approaches are new, with many representations coming up, and there is very limited work on Concept-based Model improvement.We provide a systematic review and taxonomy of various concept representations and their discovery algorithms in DNNs, specifically in vision.We also provide details on concept-based model improvement literature, which is the first to survey concept-based model improvement methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14566v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Envisioning the Next-Generation AI Coding Assistants: Insights & Proposals
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As a research-product hybrid group in AI for Software Engineering (AI4SE), we present four key takeaways from our experience developing in-IDE AI coding assistants. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.549</span></span><span class='px-1 mx-1 bg-yellow-200'>AI coding assistants should set clear expectations for usage, integrate with advanced IDE capabilities and existing extensions, use extendable backend designs, and collect app data responsibly for downstream analyses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.547</span></span><span class='px-1 mx-1 bg-yellow-200'>We propose open questions and challenges that academia and industry should address to realize the vision of next-generation AI coding assistants. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.607</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14592v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr></tbody>
  </table>
  <br><br>
</div>
</div>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      // customised options
      // • auto-render specific keys, e.g.:
      delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false},
      {left: '\\(', right: '\\)', display: false},
      {left: '\\[', right: '\\]', display: true}
      ],
      // • rendering keys, e.g.:
      throwOnError : false
    });
  });
</script>
</body>
</html>